<doc id="7566" url="http://en.wikipedia.org/wiki?curid=7566" title="Carousel (musical)">
Carousel (musical)

Carousel is the second musical by the team of Richard Rodgers (music) and Oscar Hammerstein II (book and lyrics). The 1945 work was adapted from Ferenc Molnár's 1909 play "Liliom", transplanting its Budapest setting to the Maine coastline. The story revolves around carousel barker Billy Bigelow, whose romance with millworker Julie Jordan comes at the price of both their jobs. He attempts a robbery to provide for Julie and their unborn child; after it goes wrong, he is given a chance to make things right. A secondary plot line deals with millworker Carrie Pipperidge and her romance with ambitious fisherman Enoch Snow. The show includes the well-known songs "If I Loved You", "June Is Bustin' Out All Over" and "You'll Never Walk Alone". Richard Rodgers later wrote that "Carousel" was his favorite of all his musicals.
Following the spectacular success of the first Rodgers and Hammerstein musical, "Oklahoma!" (1943), the pair sought to collaborate on another piece, knowing that any resulting work would be compared with "Oklahoma!", most likely unfavorably. They were initially reluctant to seek the rights to "Liliom"; Molnár had refused permission for the work to be adapted in the past, and the original ending was considered too depressing for the musical theatre. After acquiring the rights, the team created a work with lengthy sequences of music and made the ending more hopeful.
The musical required considerable modification during out-of-town tryouts, but once it opened on Broadway on April 19, 1945, it was an immediate hit with both critics and audiences. "Carousel" initially ran for 890 performances and duplicated its success in the West End in 1950. Though it has never achieved as much commercial success as "Oklahoma!", the piece has been repeatedly revived, and has been recorded several times. A production by Nicholas Hytner enjoyed success in 1992 in London, in 1994 in New York and on tour. In 1999, "Time" magazine named "Carousel" the best musical of the 20th century.
Background.
"Liliom".
Ferenc Molnár's Hungarian-language drama, "Liliom", premiered in Budapest in 1909. The audience was puzzled by the work, and it lasted only thirty-odd performances before being withdrawn, the first shadow on Molnár's successful career as a playwright. "Liliom" was not presented again until after World War I. When it reappeared on the Budapest stage, it was a tremendous hit.
Except for the ending, the plots of "Liliom" and "Carousel" are very similar. Andreas Zavocky (nicknamed Liliom, the Hungarian word for "lily", a slang term for "tough guy"), a carnival barker, falls in love with Julie Zeller, a servant girl, and they begin living together. With both discharged from their jobs, Liliom is discontented and contemplates leaving Julie, but decides not to do so on learning that she is pregnant. A subplot involves Julie's friend Marie, who has fallen in love with Wolf Biefeld, a hotel porter—after the two marry, he becomes the owner of the hotel. Desperate to make money so that he, Julie and their child can escape to America and a better life, Liliom conspires with lowlife Ficsur to commit a robbery, but it goes badly, and Liliom stabs himself. He dies, and his spirit is taken to heaven's police court. As Ficsur suggested while the two waited to commit the crime, would-be robbers like them do not come before God Himself. Liliom is told by the magistrate that he may go back to Earth for one day to attempt to redeem the wrongs he has done to his family, but must first spend sixteen years in a fiery purgatory.
On his return to Earth, Liliom encounters his daughter, Louise, who like her mother is now a factory worker. Saying that he knew her father, he tries to give her a star he stole from the heavens. When Louise refuses to take it, he strikes her. Not realizing who he is, Julie confronts him, but finds herself unable to be angry with him. Liliom is ushered off to his fate, presumably Hell, and Louise asks her mother if it is possible to feel a hard slap as if it was a kiss. Julie reminiscently tells her daughter that it is very possible for that to happen.
An English translation of "Liliom" was credited to Benjamin "Barney" Glazer, though there is a story that the actual translator, uncredited, was Rodgers' first major partner Lorenz Hart. The Theatre Guild presented it in New York City in 1921, with Joseph Schildkraut as Liliom, and the play was a success, running 300 performances. A 1940 revival, with Burgess Meredith and Ingrid Bergman was seen by both Hammerstein and Rodgers. Glazer, in introducing the English translation of "Liliom", wrote of the play's appeal:
And where in modern dramatic literature can such pearls be matched—Julie incoherently confessing to her dead lover the love she had always been ashamed to tell; Liliom crying out to the distant carousel the glad news that he is to be a father; the two thieves gambling for the spoils of their prospective robbery; Marie and Wolf posing for their portrait while the broken-hearted Julie stands looking after the vanishing Liliom, the thieves' song ringing in her ears; the two policemen grousing about pay and pensions while Liliom lies bleeding to death; Liliom furtively proffering his daughter the star he has stolen for her in heaven. ... The temptation to count the whole scintillating string is difficult to resist.
Inception.
In the 1920s and 1930s, Rodgers and Hammerstein both became well known for creating Broadway hits with other partners. Rodgers, with Lorenz Hart, had produced a string of over two dozen musicals, including such popular successes as "Babes in Arms" (1937), "The Boys from Syracuse" (1938) and "Pal Joey" (1940). Some of Rodgers' work with Hart broke new ground in musical theatre: "On Your Toes" was the first use of ballet to sustain the plot (in the "Slaughter on Tenth Avenue" scene), while "Pal Joey" flouted Broadway tradition by presenting a knave as its hero. Hammerstein had written or co-written the words for such hits as "Rose-Marie" (1924), "The Desert Song" (1926), "The New Moon" (1927) and "Show Boat" (1927). Though less productive in the 1930s, he wrote material for musicals and films, sharing an Academy Award for his song with Jerome Kern, "The Last Time I Saw Paris", which was included in the 1941 film "Lady Be Good".
By the early 1940s, Hart had sunk into alcoholism and emotional turmoil, becoming unreliable and prompting Rodgers to approach Hammerstein to ask if he would consider working with him. Hammerstein was eager to do so, and their first collaboration was "Oklahoma!" (1943). Thomas Hischak states, in his "The Rodgers and Hammerstein Encyclopedia", that "Oklahoma!" is "the single most influential work in the American musical theatre. In fact, the history of the Broadway musical can accurately be divided into what came before "Oklahoma!" and what came after it." An innovation for its time in integrating song, character, plot and dance, "Oklahoma!" would serve, according to Hischak, as "the model for Broadway shows for decades", and proved a huge popular and financial success. Once it was well-launched, what to do as an encore was a daunting challenge for the pair. Movie producer Sam Goldwyn saw "Oklahoma!" and advised Rodgers to shoot himself, which according to Rodgers "was Sam's blunt but funny way of telling me that I'd never create another show as good as "Oklahoma!"" As they considered new projects, Hammerstein wrote, "We're such fools. No matter what we do, everyone is bound to say, 'This is not another "Oklahoma!"' "
"Oklahoma!" had been a struggle to finance and produce. Hammerstein and Rodgers met weekly in 1943 with Theresa Helburn and Lawrence Langner of the Theatre Guild, producers of the blockbuster musical, who together formed what they termed "the Gloat Club". At one such luncheon, Helburn and Langner proposed to Rodgers and Hammerstein that they turn Molnár's "Liliom" into a musical. Both men refused—they had no feeling for the Budapest setting and thought that the unhappy ending was unsuitable for musical theatre. In addition, given the unstable wartime political situation, they might need to change the setting from Hungary while in rehearsal. At the next luncheon, Helburn and Langner again proposed "Liliom", suggesting that they move the setting to Louisiana and make Liliom a Creole. Rodgers and Hammerstein played with the idea over the next few weeks, but decided that Creole dialect, filled with "zis" and "zose" would sound corny and would make it difficult to write effective lyrics.
A breakthrough came when Rodgers, who owned a house in Connecticut, proposed a New England setting. Hammerstein wrote of this suggestion in 1945,
I began to see an attractive ensemble—sailors, whalers, girls who worked in the mills up the river, clambakes on near-by islands, an amusement park on the seaboard, things people could do in crowds, people who were strong and alive and lusty, people who had always been depicted on the stage as thin-lipped puritans—a libel I was anxious to refute ... as for the two leading characters, Julie with her courage and inner strength and outward simplicity seemed more indigenous to Maine than to Budapest. Liliom is, of course, an international character, indigenous to nowhere.
Rodgers and Hammerstein were also concerned about what they termed "the tunnel" of Molnár's second act—a series of gloomy scenes leading up to Liliom's suicide—followed by a dark ending. They also felt it would be difficult to set Liliom's motivation for the robbery to music. Molnár's opposition to having his works adapted was also an issue; he had famously turned down Giacomo Puccini when the great composer wished to transform "Liliom" into an opera, stating that he wanted the piece to be remembered as his, not Puccini's. In 1937, Molnár, who had recently emigrated to the United States, had declined another offer from Kurt Weill to adapt the play into a musical.
The pair continued to work on the preliminary ideas for a "Liliom" adaptation while pursuing other projects in late 1943 and early 1944—writing the film musical "State Fair" and producing "I Remember Mama" on Broadway. Meanwhile, the Theatre Guild took Molnár to see "Oklahoma!" Molnár stated that if Rodgers and Hammerstein could adapt "Liliom" as beautifully as they had modified "Green Grow the Lilacs" into "Oklahoma!", he would be pleased to have them do it. The Guild obtained the rights from Molnár in October 1943. The playwright received one percent of the gross and $2,500 for "personal services". The duo insisted, as part of the contract, that Molnár permit them to make changes in the plot. At first, the playwright refused, but eventually yielded. Hammerstein later stated that if this point had not been won, "we could never have made "Carousel"."
In seeking to establish through song Liliom's motivation for the robbery, Rodgers remembered that he and Hart had a similar problem in "Pal Joey". Rodgers and Hart had overcome the problem with a song that Joey sings to himself, "I'm Talking to My Pal". This inspired "Soliloquy". Both partners later told a story that "Soliloquy" was only intended to be a song about Liliom's dreams of a son, but that Rodgers, who had two daughters, insisted that Liliom consider that Julie might have a girl. However, the notes taken at their meeting of December 7, 1943 state: "Mr. Rodgers suggested a fine musical number for the end of the scene where Liliom discovers he is to be a father, in which he sings first with pride of the growth of a boy, and then suddenly realizes it might be a girl and changes completely."
Hammerstein and Rodgers returned to the "Liliom" project in mid-1944. Hammerstein was uneasy as he worked, fearing that no matter what they did, Molnár would disapprove of the results. "Green Grow the Lilacs" had been a little-known work; "Liliom" was a theatrical standard. Molnár's text also contained considerable commentary on the Hungarian politics of 1909 and the rigidity of that society. A dismissed carnival barker who hits his wife, attempts a robbery and commits suicide seemed an unlikely central character for a musical comedy. Hammerstein decided to use the words and story to make the audience sympathize with the lovers. He also built up the secondary couple, who are incidental to the plot in "Liliom"; they became Enoch Snow and Carrie Pipperidge. "This Was a Real Nice Clambake" was repurposed from a song, "A Real Nice Hayride", written for "Oklahoma!" but not used.
Molnár's ending was unsuitable, and after a couple of false starts, Hammerstein conceived the graduation scene that ends the musical. According to Frederick Nolan in his book on the team's works: "From that scene the song "You'll Never Walk Alone" sprang almost naturally." In spite of Hammerstein's simple lyrics for "You'll Never Walk Alone", Rodgers had great difficulty in setting it to music. Rodgers explained his rationale for the changed ending,
"Liliom" was a tragedy about a man who cannot learn to live with other people. The way Molnár wrote it, the man ends up hitting his daughter and then having to go back to purgatory, leaving his daughter helpless and hopeless. We couldn't accept that. The way we ended "Carousel" it may still be a tragedy but it's a hopeful one because in the final scene it is clear that the child has at last learned how to express herself and communicate with others.
When the pair decided to make "This Was a Real Nice Clambake" into an ensemble number, Hammerstein realized he had no idea what a clambake was like, and researched the matter. Based on his initial findings, he wrote the line, "First came codfish chowder". However, further research convinced him the proper term was "codhead chowder", a term unfamiliar to many playgoers. He decided to keep it as "codfish". When the song proceeded to discuss the lobsters consumed at the feast, Hammerstein wrote the line "We slit 'em down the back/And peppered 'em good". He was grieved to hear from a friend that lobsters are always slit down the front. The lyricist sent a researcher to a seafood restaurant and heard back that lobsters are always slit down the back. Hammerstein concluded that there is disagreement about which side of a lobster is the back. One error not caught involved the song "June Is Bustin' Out All Over", in which sheep are depicted as seeking to mate in late spring—they actually do so in the winter. Whenever this was brought to Hammerstein's attention, he told his informant that 1873 was a special year, in which sheep mated in the spring.
Rodgers early decided to dispense with an overture, feeling that the music was hard to hear over the banging of seats as latecomers settled themselves. In his autobiography, Rodgers complained that only the brass section can be heard during an overture because there are never enough strings in a musical's small orchestra. He determined to force the audience to concentrate from the beginning by opening with a pantomime scene accompanied by what became known as "The Carousel Waltz". The pantomime paralleled one in the Molnár play, which was also used to introduce the characters and situation to the audience. Author Ethan Mordden described the effectiveness of this opening:
Other characters catch our notice—Mr. Bascombe, the pompous mill owner, Mrs. Mullin, the widow who runs the carousel and, apparently, Billy; a dancing bear; an acrobat. But what draws us in is the intensity with which Julie regards Billy—the way she stands frozen, staring at him, while everyone else at the fair is swaying to the rhythm of Billy's spiel. And as Julie and Billy ride together on the swirling carousel, and the stage picture surges with the excitement of the crowd, and the orchestra storms to a climax, and the curtain falls, we realize that R & H have not only skipped the overture "and" the opening number but the exposition as well. They have plunged into the story, right into the middle of it, in the most intense first scene any musical ever had.
Casting and tryouts.
The casting for "Carousel" began when "Oklahoma!"'s production team, including Rodgers and Hammerstein, was seeking a replacement for the part of Curly (the male lead in "Oklahoma!"). Lawrence Langner had heard, through a relative, of a California singer named John Raitt, who might be suitable for the part. Langner went to hear Raitt, then urged the others to bring Raitt to New York for an audition. Raitt asked to sing "Largo al factotum", Figaro's song from "The Barber of Seville", to warm up. The warmup was sufficient to convince the producers that not only had they found a Curly, they had found a Liliom (or Billy Bigelow, as the part was renamed). Theresa Helburn made another California discovery, Jan Clayton, a singer/actress who had made a few minor films for MGM. She was brought east and successfully auditioned for the part of Julie.
The producers sought to cast unknowns. Though many had played in previous Hammerstein or Rodgers works, only one, Jean Casto (cast as carousel owner Mrs. Mullin, and a veteran of "Pal Joey"), had ever played on Broadway before. It proved harder to cast the ensemble than the leads, due to the war—Rodgers told his casting director, John Fearnley, that the sole qualification for a dancing boy was that he be alive. Rodgers and Hammerstein reassembled much of the creative team that had made "Oklahoma!" a success, including director Rouben Mamoulian and choreographer Agnes de Mille. Miles White was the costume designer while Jo Mielziner (who had not worked on "Oklahoma!") was the scenic and lighting designer. Even though "Oklahoma!" orchestrator Russell Bennett had informed Rodgers that he was unavailable to work on "Carousel" due to a radio contract, Rodgers insisted he do the work in his spare time. He orchestrated "The Carousel Waltz" and "(When I Marry) Mister Snow" before finally being replaced by Don Walker. A new member of the creative team was Trude Rittmann, who arranged the dance music. Rittmann initially felt that Rodgers mistrusted her because she was a woman, and found him difficult to work with, but the two worked together on Rodgers' shows until the 1970s.
Rehearsals began in January 1945; either Rodgers or Hammerstein was always present. Raitt was presented with the lyrics for "Soliloquy" on a five-foot long sheet of paper—the piece ran nearly eight minutes. Staging such a long solo number presented problems, and Raitt later stated that he felt that they were never fully addressed. At some point during rehearsals, Molnár came to see what they had done to his play. There are a number of variations on the story. As Rodgers told it, while watching rehearsals with Hammerstein, the composer spotted Molnár in the rear of the theatre and whispered the news to his partner. Both sweated through an afternoon of rehearsal in which nothing seemed to go right. At the end, the two walked to the back of the theatre, expecting an angry reaction from Molnár. Instead, the playwright said enthusiastically, "What you have done is so beautiful. And you know what I like best? The ending!" Hammerstein wrote that Molnár became a regular attendee at rehearsals after that.
Like most of the pair's works, "Carousel" contains a lengthy ballet, "Billy Makes a Journey", in the second act, as Billy looks down to the Earth from "Up There" and observes his daughter. In the original production the ballet was choreographed by de Mille. As originally written, de Mille's ballet lasted an hour and fifteen minutes. It began with Billy looking down from heaven at his wife in labor, with the village women gathered for a "birthing". The ballet involved every character in the play, some of whom spoke lines of dialogue, and contained a number of subplots. The focus was on Louise, played by Bambi Linn, who at first almost soars in her dance, expressing the innocence of childhood. She is teased and mocked by her schoolmates, and Louise becomes attracted to the rough carnival people, who symbolize Billy's world. A youth from the carnival attempts to seduce Louise, as she discovers her own sexuality, but he decides she is more girl than woman, and he leaves her. After Julie comforts her, Louise goes to a children's party, where she is shunned. The carnival people reappear and form a ring around the children's party, with Louise lost between the two groups. At the end, the performers form a huge carousel with their bodies.
The play opened for tryouts in New Haven, Connecticut on March 22, 1945. The first act was well-received; the second act was not. Casto recalled that the second act finished about 1:30 a.m. The staff immediately sat down for a two-hour conference. Five scenes, half a ballet and two songs were cut from the show as the result. John Fearnley commented, "Now I see why these people have hits. I never witnessed anything so brisk and brave in my life." De Mille said of this conference, "not three minutes had been wasted pleading for something cherished. Nor was there any idle joking. ... We cut and cut and cut and then we went to bed." By the time the company left New Haven, de Mille's ballet was down to forty minutes.
A major concern with the second act was the effectiveness of the characters He and She (later called by Rodgers "Mr. and Mrs. God"), before whom Billy appeared after his death. Mr. and Mrs. God were depicted as a New England minister and his wife, seen in their parlor. The couple was still part of the show at the Boston opening. Rodgers said to Hammerstein, "We've got to get God out of that parlor". When Hammerstein inquired where he should put the deity, Rodgers replied, "I don't care where you put Him. Put Him on a ladder for all I care, only get Him out of that parlor!" Hammerstein duly put Mr. God (renamed the Starkeeper) atop a ladder, and Mrs. God was removed from the show. Rodgers biographer Meryle Secrest terms this change a mistake, leading to a more fantastic afterlife, which was later criticized by "The New Republic" as "a Rotarian atmosphere congenial to audiences who seek not reality but escape from reality, not truth but escape from truth".
Hammerstein wrote that Molnár's advice, to combine two scenes into one, was key to pulling together the second act and represented "a more radical departure from the original than any change we had made". A reprise of "If I Loved You" was added in the second act, which Rodgers felt needed more music. Three weeks of tryouts in Boston followed the brief New Haven run, and the audience there gave the musical a warm reception. An even shorter version of the ballet was presented the final two weeks in Boston, but on the final night there, de Mille expanded it back to forty minutes, and it brought the house down, causing both Rodgers and Hammerstein to embrace her.
Plot.
Act 1.
Two young female millworkers in 1873 Maine visit the town's carousel after work. One of them, Julie Jordan, attracts the attention of the barker, Billy Bigelow ("The Carousel Waltz"). When Julie lets Billy put his arm around her during the ride, Mrs. Mullin, the widowed owner of the carousel, tells Julie never to return. Julie and her friend, Carrie Pipperidge, argue with Mrs. Mullin. Billy arrives and, seeing that Mrs. Mullin is jealous, mocks her; he is fired from his job. Billy, unconcerned, invites Julie to join him for a drink. As he goes to get his belongings, Carrie presses Julie about her feelings toward him, but Julie is evasive ("You're a Queer One, Julie Jordan"). Carrie has a beau too, fisherman Enoch Snow ("(When I Marry) Mister Snow"), to whom she is newly engaged. Billy returns for Julie as the departing Carrie warns that staying out late means the loss of Julie's job. Mr. Bascombe, owner of the mill, happens by along with a policeman, and offers to escort Julie to her home, but she refuses and is fired. Left alone, she and Billy talk about what life might be like if they were in love, but neither quite confesses to the growing attraction they feel for each other ("If I Loved You").
Over a month passes, and preparations for the summer clambake are under way ("June Is Bustin' Out All Over"). Julie and Billy, now married, live at Julie's cousin Nettie's spa. Julie confides in Carrie that Billy, frustrated over being unemployed, hit her. Carrie has happier news—she is engaged to Enoch, who enters as she discusses him ("(When I Marry) Mister Snow (reprise))". Billy arrives with his ne'er-do-well whaler friend, Jigger. The former barker is openly rude to Enoch and Julie, then leaves with Jigger, followed by a distraught Julie. Enoch tells Carrie that he expects to become rich selling herring and to have a large family, larger perhaps than Carrie is comfortable having ("When the Children Are Asleep").
Jigger and his shipmates, joined by Billy, then sing about life on the sea ("Blow High, Blow Low"). The whaler tries to recruit Billy to help with a robbery, but Billy declines, as the victim—Julie's former boss, Mr. Bascombe—might have to be killed. Mrs. Mullin enters and tries to tempt Billy back to the carousel (and to her). He would have to abandon Julie; a married barker cannot evoke the same sexual tension as one who is single. Billy reluctantly mulls it over as Julie arrives and the others leave. She tells him that she is pregnant, and Billy is overwhelmed with happiness, ending all thoughts of returning to the carousel. Once alone, Billy imagines the fun he will have with Bill Jr.—until he realizes that his child might be a girl, and reflects soberly that "you've got to be a "father" to a girl" ("Soliloquy"). Determined to provide financially for his future child, whatever the means, Billy decides to be Jigger's accomplice.
The whole town leaves for the clambake. Billy, who had earlier refused to go, agrees to join in, to Julie's delight, as he realizes that being seen at the clambake is integral to his and Jigger's alibi ("Act I Finale").
Act 2.
Everyone reminisces about the huge meal and much fun ("This Was a Real Nice Clambake"). Jigger tries to seduce Carrie; Enoch walks in at the wrong moment, and declares that he is finished with her ("Geraniums In the Winder"), as Jigger jeers ("There's Nothin' So Bad for a Woman"). The girls try to comfort Carrie, but for Julie all that matters is that "he's your feller and you love him" ("What's the Use of Wond'rin'?"). Julie sees Billy trying to sneak away with Jigger and, trying to stop him, feels the knife hidden in his shirt. She begs him to give it to her, but he refuses and leaves to commit the robbery.
As they wait, Jigger and Billy gamble with cards. They stake their shares of the anticipated robbery spoils. Billy loses: his participation is now pointless. Unknown to Billy and Jigger, Mr. Bascombe, the intended victim, has already deposited the mill's money. The robbery fails: Bascombe pulls a gun on Billy while Jigger escapes. Billy stabs himself with his knife; Julie arrives just in time for him to say his last words to her and die. Julie strokes his hair, finally able to tell him that she loved him. Carrie and Enoch, reunited by the crisis, attempt to console Julie; Nettie arrives and gives Julie the resolve to keep going despite her despair ("You'll Never Walk Alone").
Billy's defiant spirit ("The Highest Judge of All") is taken Up There to see the Starkeeper, a heavenly official. The Starkeeper tells Billy that the good he did in life was not enough to get into heaven, but so long as there is a person alive who remembers him, he can return for a day to try to do good to redeem himself. He informs Billy that fifteen years have passed on Earth since the former barker's suicide, and suggests that Billy can get himself into heaven if he helps his daughter, Louise. He helps Billy look down from heaven to see her (instrumental ballet: "Billy Makes a Journey"). Louise has grown up to be lonely and bitter. The local children ostracize her because her father was a thief and a wife-beater. In the dance, a young ruffian, much like her father at that age, flirts with her and abandons her as too young. The dance concludes, and Billy is anxious to return to Earth and help his daughter. He steals a star to take with him, as the Starkeeper pretends not to notice.
Outside Julie's cottage, Carrie describes her visit to New York with the now-wealthy Enoch. Carrie's husband and their many children enter to fetch her—the family must get ready for the high school graduation later that day. Enoch Jr., the oldest son, remains behind to talk with Louise, as Billy and the Heavenly Friend escorting him enter, invisible to the other characters. Louise confides in Enoch Jr. that she plans to run away from home with an acting troupe. He says that he will stop her by marrying her, but that his father will think her an unsuitable match. Louise is outraged: each insults the other's father, and Louise orders Enoch Jr. to go away. Billy, able to make himself visible at will, reveals himself to the sobbing Louise, pretending to be a friend of her father. He offers her a gift—the star he stole from heaven. She refuses it and, frustrated, he slaps her hand. He makes himself invisible, and Louise tells Julie what happened, stating that the slap miraculously felt like a kiss, not a blow—and Julie understands her perfectly. Louise retreats to the house, as Julie notices the star that Billy dropped; she picks it up and seems to feel Billy's presence ("If I Loved You (Reprise)").
Billy invisibly attends Louise's graduation, hoping for one last chance to help his daughter and redeem himself. The beloved town physician, Dr. Seldon (who resembles the Starkeeper) advises the graduating class not to rely on their parents' success or be held back by their failure (words directed at Louise). Seldon prompts everyone to sing an old song, "You'll Never Walk Alone". Billy, still invisible, whispers to Louise, telling her to believe Seldon's words, and when she tentatively reaches out to another girl, she learns she does not have to be an outcast. Billy goes to Julie, telling her at last that he loved her. As his widow and daughter join in the singing, Billy is taken to his heavenly reward.
Principal roles and notable performers.
° denotes original Broadway cast
Musical numbers.
Act I
Act II
Productions.
Early productions.
The original Broadway production opened at the Majestic Theatre on April 19, 1945. The dress rehearsal the day before had gone badly, and the pair feared the new work would not be well received. One successful last-minute change was to have de Mille choreograph the pantomime. The movement of the carnival crowd in the pantomime had been entrusted to Mamoulian, and his version was not working. Rodgers had injured his back the previous week, and he watched the opening from a stretcher propped in a box behind the curtain. Sedated with morphine, he could see only part of the stage. As he could not hear the audience's applause and laughter, he assumed the show was a failure. It was not until friends congratulated him later that evening that he realized that the curtain had been met by wild applause. Bambi Linn, who played Louise, was so enthusiastically received by the audience during her ballet that she was forced to break character, when she next appeared, and bow. Rodgers' daughter Mary caught sight of her friend, Stephen Sondheim, both teenagers then, across several rows; both had eyes wet with tears.
The original production ran for 890 performances, closing on May 24, 1947. The original cast included John Raitt (Billy), Jan Clayton (Julie), Jean Darling (Carrie), Eric Mattson (Enoch Snow), Christine Johnson (Nettie Fowler), Murvyn Vye (Jigger), Bambi Linn (Louise) and Russell Collins (Starkeeper). In December 1945, Clayton left to star in the Broadway revival of "Show Boat" and was replaced by Iva Withers; Raitt was replaced by Henry Michel in January 1947; Darling was replaced by Margot Moser.
After closing on Broadway, the show went on a national tour for two years. It played for five months in Chicago alone, visited twenty states and two Canadian cities, covered and played to nearly two million people. The touring company had a four-week run at New York City Center in January 1949.<ref name="NYT/Calta 1949-01-25">Calta, Louis. . "The New York Times", January 25, 1949, p. 27. Retrieved on December 21, 2010.</ref> Following the City Center run, the show was moved back to the Majestic Theatre in the hopes of filling the theatre until "South Pacific" opened in early April. However, ticket sales were mediocre, and the show closed almost a month early.<ref name="NYT/Calta 1949-02-28">Calta, Louis. . "The New York Times", February 28, 1949, p. 15. Retrieved on December 21, 2010.</ref>
The musical premiered in the West End, London, at the Theatre Royal, Drury Lane on June 7, 1950. The production was restaged by Jerome Whyte, with a cast that included Stephen Douglass (Billy), Iva Withers (Julie) and Margot Moser (Carrie). "Carousel" ran in London for 566 performances, remaining there for over a year and a half.
Subsequent productions.
"Carousel" was revived in 1954 and 1957 at City Center, presented by the New York City Center Light Opera Company. Both times, the production featured Barbara Cook, though she played Carrie in 1954 and Julie in 1957 (playing alongside Howard Keel as Billy). The production was then taken to Belgium to be performed at the 1958 Brussels World's Fair, with David Atkinson as Billy, Ruth Kobart as Nettie, and Clayton reprising the role of Julie, which she had originated.
In August 1965, Rodgers and the Music Theater of Lincoln Center produced "Carousel" for 47 performances. John Raitt reprised the role of Billy, with Jerry Orbach as Jigger and Reid Shelton as Enoch Snow. The roles of the Starkeeper and Dr. Seldon were played by Edward Everett Horton in his final stage appearance. The following year, New York City Center Light Opera Company brought "Carousel" back to City Center for 22 performances, with Bruce Yarnell as Billy and Constance Towers as Julie.
Nicholas Hytner directed a new production of "Carousel" in 1992, at London's Royal National Theatre, with choreography by Sir Kenneth MacMillan and designs by Bob Crowley. In this staging, the story begins at the mill, where Julie and Carrie work, with the music slowed down to emphasize the drudgery. After work ends, they move to the shipyards and then to the carnival. As they proceed on a revolving stage, carnival characters appear, and at last the carousel is assembled onstage for the girls to ride. Louise is seduced by the ruffian boy during her Act 2 ballet, set around the ruins of a carousel. Michael Hayden played Billy not as a large, gruff man, but as a frustrated smaller one, a time bomb waiting to explode. Hayden, Joanna Riding (Julie) and Janie Dee (Carrie) all won Olivier Awards for their performances. Patricia Routledge played Nettie. Enoch and Carrie were cast as an interracial couple whose eight children, according to the review in "The New York Times", looked like "a walking United Colors of Benetton ad". The production's limited run from December 1992 through March 1993 was a sellout. It re-opened at the Shaftesbury Theatre in London in September 1993, presented by Cameron Mackintosh, where it continued until May 1994.
The Hytner production moved to New York's Vivian Beaumont Theater, where it opened on March 24, 1994 and ran for 322 performances. This won five Tony Awards, including best musical revival, as well as awards for Hytner, MacMillan, Crowley and Audra McDonald (as Carrie). The cast also included Sally Murphy as Julie, Shirley Verrett as Nettie, Fisher Stevens as Jigger and Eddie Korbich as Enoch. One change made from the London to the New York production was to have Billy strike Louise across the face, rather than on the hand. According to Hayden, "He does the one unpardonable thing, the thing we can't forgive. It's a challenge for the audience to like him after that."
The Hytner "Carousel" was presented in Japan in May 1995. A U.S. national tour with a scaled-down production began in February 1996 in Houston and closed in May 1997 in Providence, Rhode Island. Producers sought to feature young talent on the tour; Hayden was replaced by Patrick Wilson as Billy; Sarah Uriarte Berry and later Jennifer Laura Thompson played Julie.
A revival opened at London's Savoy Theatre on December 2, 2008, after a week of previews, starring Jeremiah James (Billy), Alexandra Silber (Julie) and Lesley Garrett (Nettie). The production received warm to mixed reviews. It closed in June 2009, a month early. Michael Coveney, writing in "The Independent", admired Rodgers' music but stated, "Lindsay Posner's efficient revival doesn't hold a candle to the National Theatre 1992 version".
Film, television and concert versions.
A film version of the musical was made in 1956, starring Gordon MacRae and Shirley Jones. It follows the musical's story fairly closely, although a prologue, set in the Starkeeper's heaven, was added. The film was released only a few months after the release of the film version of "Oklahoma!". It garnered some good reviews, and the soundtrack recording was a best seller. As the same stars appeared in both pictures, however, the two films were often compared, generally to the disadvantage of "Carousel". Thomas Hischak, in "The Rodgers and Hammerstein Encyclopedia", later wondered "if the smaller number of "Carousel" stage revivals is the product of this often-lumbering [film] musical".
There was also an abridged (100 minute) 1967 network television version that starred Robert Goulet, with choreography by Edward Villella.
The New York Philharmonic presented a staged concert version of the musical from February 28, 2013 to March 2, 2013 at Avery Fisher Hall. Kelli O'Hara played Julie, with Nathan Gunn as Billy, Stephanie Blythe as Nettie, Jessie Mueller as Carrie, Jason Danieley as Enoch, Shuler Hensley as Jigger, John Cullum as the Starkeeper, and Kate Burton as Mrs. Mullin. Tiler Peck danced the role of Louise to choreography by Warren Carlyle. The production was directed by John Rando. Charles Isherwood of "The New York Times" wrote, "this is as gorgeously sung a production of this sublime 1945 Broadway musical as you are ever likely to hear." It was broadcast as part of the PBS "Live from Lincoln Center" series, premiering on April 26, 2013.
Music and recordings.
Musical treatment.
Rodgers designed "Carousel" to be an almost continuous stream of music, especially in Act 1. In later years, Rodgers was asked if he had considered writing an opera. He stated that he had been sorely tempted to, but saw "Carousel" in operatic terms. He remembered, "We came very close to opera in the Majestic Theatre. ... There's much that is operatic in the music."
Rodgers uses music in "Carousel" in subtle ways to differentiate characters and tell the audience of their emotional state. In "You're a Queer One, Julie Jordan", the music for the placid Carrie is characterized by even eighth-note rhythms, whereas the emotionally restless Julie's music is marked by dotted eighths and sixteenths; this rhythm will characterize her throughout the show. When Billy whistles a snatch of the song, he selects Julie's dotted notes rather than Carrie's. Reflecting the close association in the music between Julie and the as-yet unborn Louise, when Billy sings in "Soliloquy" of his daughter, who "gets hungry every night", he uses Julie's dotted rhythms. Such rhythms also characterize Julie's Act 2 song, "What's the Use of Wond'rin'". The stable love between Enoch and Carrie is strengthened by her willingness to let Enoch not only plan his entire life, but hers as well. This is reflected in "When the Children Are Asleep", where the two sing in close harmony, but Enoch musically interrupts his intended's turn at the chorus with the words "Dreams that won't be interrupted". Rodgers biographer Geoffrey Block, in his book on the Broadway musical, points out that though Billy may strike his wife, he allows her musical themes to become a part of him and never interrupts her music. Block suggests that, as reprehensible as Billy may be for his actions, Enoch requiring Carrie to act as "the little woman", and his having nine children with her (more than she had found acceptable in "When the Children are Asleep") can be considered to be even more abusive.
The twelve-minute "bench scene", in which Billy and Julie get to know each other and which culminates with "If I Loved You", according to Hischak, "is considered the most completely integrated piece of music-drama in the American musical theatre". The scene is almost entirely drawn from Molnár and is one extended musical piece; Stephen Sondheim described it as "probably the single most important moment in the revolution of contemporary musicals". "If I Loved You" has been recorded many times, by such diverse artists as Frank Sinatra, Barbra Streisand, Sammy Davis Jr., Mario Lanza and Chad and Jeremy. The D-flat major theme that dominates the music for the second act ballet seems like a new melody to many audience members. It is, however, a greatly expanded development of a theme heard during "Soliloquy" at the line "I guess he'll call me 'The old man' ".
When the pair discussed the song that would become "Soliloquy", Rodgers improvised at the piano to give Hammerstein an idea of how he envisioned the song. When Hammerstein presented his collaborator with the lyrics after two weeks of work (Hammerstein always wrote the words first, then Rodgers would write the melodies), Rodgers wrote the music for the eight-minute song in two hours. "What's the Use of Wond'rin' ", one of Julie's songs, worked well in the show but was never as popular on the radio or for recording, and Hammerstein believed that the lack of popularity was because he had concluded the final line, "And all the rest is talk" with a hard consonant, which does not allow the singer a vocal climax.
Irving Berlin later stated that "You'll Never Walk Alone" had the same sort of effect on him as the 23rd Psalm. When singer Mel Tormé told Rodgers that "You'll Never Walk Alone" had made him cry, Rodgers nodded impatiently. "You're supposed to." The frequently recorded song has become a universally accepted hymn. The cast recording of "Carousel" proved popular in Liverpool, like many Broadway albums, and in 1963, the Brian Epstein-managed band, Gerry and the Pacemakers had a number-one hit with the song. At the time, the top ten hits were played before Liverpool F.C. home matches; even after "You'll Never Walk Alone" dropped out of the top ten, fans continued to sing it, and it has become closely associated with the soccer team and the city of Liverpool. A BBC program, "Soul Music", ranked it alongside "Silent Night" and "Abide With Me" in terms of its emotional impact and iconic status.
Recordings.
The cast album of the 1945 Broadway production was issued on 78s, and the score was significantly cut—as was the 1950 London cast recording. Theatre historian John Kenrick notes of the 1945 recording that a number of songs had to be abridged to fit the 78 format, but that there is a small part of "Soliloquy" found on no other recording, as Rodgers cut it from the score immediately after the studio recording was made.
A number of songs were cut for the 1956 film, but two of the deleted numbers had been recorded and were ultimately retained on the soundtrack album. The expanded CD version of the soundtrack, issued in 2001, contains all of the singing recorded for the film, including the cut portions, and nearly all of the dance music. The recording of the 1965 Lincoln Center revival featured Raitt reprising the role of Billy. Studio recordings of "Carousel"'s songs were released in 1956 (with Robert Merrill as Billy, Patrice Munsel as Julie, and Florence Henderson as Carrie), 1962 and 1987. The last featured a mix of opera and musical stars, including Samuel Ramey, Barbara Cook and Sarah Brightman. Kenrick recommends the 1962 studio recording, though it is not yet available on CD, for its outstanding cast, including Alfred Drake and Roberta Peters.
Both the London (1993) and New York (1994) cast albums of the Hytner production contain portions of dialogue that, according to Hischak, speak to the power of Michael Hayden's portrayal of Billy. Kenrick judges the 1994 recording the best all-around performance of "Carousel" on disc, despite uneven singing by Hayden, due to Sally Murphy's Julie and the strong supporting cast (calling Audra McDonald the best Carrie he has heard).
Critical reception and legacy.
The musical received almost unanimous rave reviews after its opening in 1945. According to Hischak, reviews were not as exuberant as for "Oklahoma!" as the critics were not taken by surprise this time. John Chapman of the "Daily News" termed it "one of the finest musical plays I have ever seen and I shall remember it always". "The New York Times"'s reviewer, Lewis Nichols, stated that "Richard Rodgers and Oscar Hammerstein 2d, who can do no wrong, have continued doing no wrong in adapting "Liliom" into a musical play. Their "Carousel" is on the whole delightful." Wilella Waldorf of the "New York Post", however, complained, ""Carousel" seemed to us a rather long evening. The "Oklahoma!" formula is becoming a bit monotonous and so are Miss de Mille's ballets. All right, go ahead and shoot!" "Dance Magazine" gave Linn plaudits for her role as Louise, stating, "Bambi doesn't come on until twenty minutes before eleven, and for the next forty minutes, she practically holds the audience in her hand". Howard Barnes in the "New York Herald Tribune" also applauded the dancing: "It has waited for Miss de Mille to come through with peculiarly American dance patterns for a musical show to become as much a dance as a song show."
When the musical returned to New York in 1949, "The New York Times" reviewer Brooks Atkinson described "Carousel" as "a conspicuously superior musical play ... "Carousel", which was warmly appreciated when it opened, seems like nothing less than a masterpiece now." In 1954, when "Carousel" was revived at City Center, Atkinson discussed the musical in his review:
"Carousel" has no comment to make on anything of topical importance. The theme is timeless and universal: the devotion of two people who love each other through thick and thin, complicated in this case by the wayward personality of the man, who cannot fulfill the responsibilities he has assumed.  ... Billy is a bum, but "Carousel" recognizes the decency of his motives and admires his independence. There are no slick solutions in "Carousel".
Stephen Sondheim noted the duo's ability to take the innovations of "Oklahoma!" and apply them to a serious setting: ""Oklahoma!" is about a picnic, "Carousel" is about life and death." Critic Eric Bentley, on the other hand, wrote that "the last scene of "Carousel" is an impertinence: I refuse to be lectured to by a musical comedy scriptwriter on the education of children, the nature of the good life, and the contribution of the American small town to the salvation of souls."
"New York Times" critic Frank Rich said of the 1992 London production: "What is remarkable about Mr. Hytner's direction, aside from its unorthodox faith in the virtues of simplicity and stillness, is its ability to make a 1992 audience believe in Hammerstein's vision of redemption, which has it that a dead sinner can return to Earth to do godly good." The Hytner production in New York was hailed by many critics as a grittier "Carousel", which they deemed more appropriate for the 1990s. Clive Barnes of the "New York Post" called it a "defining "Carousel"—hard-nosed, imaginative, and exciting."
Rodgers considered "Carousel" his favorite of all his musicals and wrote, "it affects me deeply every time I see it performed". In 1999, "Time" magazine, in its "Best of the Century" list, named "Carousel" the Best Musical of the 20th century, writing that Rodgers and Hammerstein "set the standards for the 20th century musical, and this show features their most beautiful score and the most skillful and affecting example of their musical storytelling". Hammerstein's grandson, Oscar Andrew Hammerstein, in his book about his family, suggested that the wartime situation made "Carousel"'s ending especially poignant to its original viewers, "Every American grieved the loss of a brother, son, father, or friend ... the audience empathized with [Billy's] all-too-human efforts to offer advice, to seek forgiveness, to complete an unfinished life, and to bid a proper good-bye from beyond the grave." Author and composer Ethan Mordden agreed with that perspective:
If "Oklahoma!" developed the moral argument for sending American boys overseas, "Carousel" offered consolation to those wives and mothers whose boys would only return in spirit. The meaning lay not in the tragedy of the present, but in the hope for a future where no one walks alone.
Awards and nominations.
Original 1945 Broadway.
"Note: The Tony Awards were not established until 1947, and so "Carousel" was not eligible to win any Tonys at its premiere.

</doc>
<doc id="7572" url="http://en.wikipedia.org/wiki?curid=7572" title="Christian alternative rock">
Christian alternative rock

Christian alternative rock is a form of alternative rock music that is lyrically grounded in a Christian worldview. Some critics have suggested that unlike CCM and older Christian rock, Christian alternative rock generally emphasizes musical style over lyrical content as a defining genre characteristic, though the degree to which the faith appears in the music varies from artist to artist.
History.
Christian alternative music has its roots in the early 1980s, as the earliest efforts at Christian punk and new wave were recorded by artists like Andy McCarroll and Moral Support, Undercover, The 77s, Steve Scott, Adam Again, Quickflight, Daniel Amos, Youth Choir (later renamed The Choir), Lifesavers Underground, Michael Knott, The Altar Boys, Breakfast with Amy, Steve Taylor, 4-4-1, David Edwards and Vector. Early labels, most now-defunct, included Blonde Vinyl, Frontline, Exit, and Refuge.
By the 1990s, many of these bands and artists had disbanded, were no longer performing, or were being carried by independent labels because their music tended to be more lyrically complex (and often more controversial) than mainstream Christian pop. The modern market is currently supported by labels such as Tooth & Nail, Gotee and Floodgate. These companies are often children of or partially owned by general market labels such as Warner, EMI, and Capitol Records, giving successful artists an opportunity to "cross over" into mainstream markets.

</doc>
<doc id="7573" url="http://en.wikipedia.org/wiki?curid=7573" title="Clive Barker">
Clive Barker

Clive Barker (born 5 October 1952) is an English author, film director, video game designer and visual artist best known for his work in both fantasy and horror fiction. Barker came to prominence in the mid-1980s with a series of short stories known as the "Books of Blood" which established him as a leading young horror writer. He has since written many novels and other works, and his fiction has been adapted into films, notably the "Hellraiser" and "Candyman" series. He was the Executive Producer of the film "Gods and Monsters", which won an Academy Award for Best Adapted Screenplay.
Barker's paintings and illustrations have been featured in galleries in the United States, as well as within his own books. He has also created original characters and series for comic books, and some of his more popular horror stories have been adapted to the medium.
His archives have been a source of material for biographies and non-fiction books containing his personal essays, discussions of his fringe theater work, interviews, and other content.
Early life.
Clive Barker was born in Liverpool, Merseyside, the son of Joan Ruby (née Revill), a painter and school welfare officer, and Leonard Barker, a personnel director for an industrial relations firm. He was educated at Dovedale Primary School, Quarry Bank High School and the University of Liverpool, where he studied English and Philosophy.
When he was four years old, Barker witnessed the French skydiver Léo Valentin plummet to his death during a performance at an air show in Liverpool. Barker would later allude to Valentin in many of his stories.
Personal life.
In 2003, Barker received the Davidson/Valentini Award at the 15th GLAAD Media Awards. Barker has been critical of organised religion throughout his career, but he has stated that the Bible influences his work and spirituality. In 2003, Barker remarked "I am, [a Christian]" during an episode of "Real Time With Bill Maher" when Ann Coulter implied he was not a Christian.
Barker said in a December 2008 online interview (published in March 2009) that he had polyps in his throat which were so severe that a doctor told him he was taking in ten percent of the air he was supposed to have been getting. He has had two surgeries to remove them and believes his resultant voice is an improvement over how it was prior to the surgeries. He said he did not have cancer and has given up cigars. On 27 August 2010, Barker underwent surgery yet again to remove new polyp growths from his throat.
In early February 2012, Barker fell into a coma after a visit to a dentist led to blood poisoning. Barker remained in a coma for eleven days but eventually came out of it. Fans were notified on his Twitter page about some of the experience and that Barker was recovering after the ordeal, but was left with many strange visions.
Relationships.
In a 20 August 1996 appearance on the radio call-in show "Loveline", Barker stated that during his teens he had several relationships with older women, and came to identify himself as homosexual by 18 or 19 years old. Barker has been openly gay lasted from 1975 until 1986. It was during this period, with the support that John provided, that Clive was able to write the "Books of Blood" series and "The Damnation Game". He later spent fourteen years with David Armstrong; they separated in 2009.
Writing career.
Barker is an author of contemporary horror/fantasy. He began writing in the horror genre early in his career, mostly in the form of short stories (collected in "Books of Blood" 1 – 6) and the Faustian novel "The Damnation Game" (1985). Later he moved towards modern-day fantasy and urban fantasy with horror elements in "Weaveworld" (1987), "The Great and Secret Show" (1989), the world-spanning "Imajica" (1991), and "Sacrament" (1996), bringing in the deeper, richer concepts of reality, the nature of the mind and dreams, and the power of words and memories.
Barker's distinctive style is characterised by the notion of hidden fantastical worlds coexisting with our own, the role of sexuality in the supernatural, and the construction of coherent, complex and detailed universes. Barker has referred to this style as "dark fantasy" or the "fantastique". His stories are notable for a deliberate blurring of the distinction between binary opposites such as Hell and Heaven, or pleasure and pain (the latter particularly so in "The Hellbound Heart").
When the "Books of Blood" were first published in the United States in paperback, Stephen King was quoted on the book covers: "I have seen the future of horror, his name is Clive Barker." As influences on his writing, Barker lists Herman Melville, Edgar Allan Poe, Ray Bradbury, William S. Burroughs, William Blake and Jean Cocteau, among others.
He is also the writer of the best-selling Abarat series, and plans on producing two more novels in the series.
Barker's basic philosophy and approach are revealed in his foreword to H.R. Giger's illustrated work, "Necronomicon" (1977).
Film work.
Barker has a keen interest in film production, although his films have received mixed receptions. He wrote the screenplays for "Underworld" (aka "Transmutations" – 1985) and "Rawhead Rex" (1986), both directed by George Pavlou. Displeased by how his material was handled, he moved to directing with "Hellraiser" (1987), based on his novella "The Hellbound Heart". His early films, the shorts "The Forbidden" and "Salome", are experimental art films with surrealist elements, which have been re-released together to moderate critical acclaim. After his film "Nightbreed" (1990), which was widely considered to be a flop, Barker returned to write and direct "Lord of Illusions" (1995). A short story titled "The Forbidden", from Barker's "Books of Blood", provided the basis for the 1992 film "Candyman" and its two sequels (whereof he was also producer of the two first). Barker was an executive producer of the film "Gods and Monsters" (1998), which received major critical acclaim. He had been working on a series of film adaptations of his "The Abarat Quintet" books under Disney's management, but has admitted that because of creative differences, this project will not go ahead.
In 2005, Barker created the film production company Midnight Picture Show together with horror film producer Jorge Saralegui, with the intent of producing two horror films per year. Since then, the company has produced four films: "The Plague" (2006), "The Midnight Meat Train" (2008), "Book of Blood" (2009) and "Dread" (2009).
In October 2006, Barker announced through his official website that he will be writing the script to a forthcoming remake of the original "Hellraiser" film. He is also developing a film based on his "Tortured Souls" line of toys from McFarlane Toys.
Visual art and plays.
Barker is a prolific visual artist working in a variety of media, often illustrating his own books. His paintings have been seen first on the covers of his official fan club magazine, "Dread", published by Fantaco in the early '90s; on the covers of the collections of his plays, "Incarnations" (1995) and "Forms of Heaven" (1996); and on the second printing of the original British publications of his "Books of Blood" series. Barker also provided the artwork for his young adult novel "The Thief of Always" and for the "Abarat" series. His artwork has been exhibited at Bert Green Fine Art in Los Angeles and Chicago, at the Bess Cutler Gallery in New York and La Luz De Jesus in Los Angeles. Many of his sketches and paintings can be found in the collection "Clive Barker, Illustrator", published in 1990 by Arcane/Eclipse Books, and in "Visions of Heaven and Hell", published in 2005 by Rizzoli Books. The most complete selection of Clive Barker's paintings and drawings are available to view in a gallery setting on the website.
He worked on the creative side of a horror video game, "Clive Barker's Undying", providing the voice for the character Ambrose. "Undying" was developed by DreamWorks Interactive and released in 2001. He also worked on "Clive Barker's Jericho" for Codemasters, which was released in late 2007.
Barker created Halloween costume designs for Disguise Costumes
Comic books.
A longtime comics fan, Barker achieved his dream of publishing his own superhero books when Marvel Comics launched the Razorline imprint in 1993. Based on detailed premises, titles and lead characters he created specifically for this, the four interrelated titles — set outside the Marvel universe — were "Ectokid" (written first by James Robinson, then by future "Matrix" co-creator Lana Wachowski, with art by Steve Skroce), "Hokum & Hex" (written by Frank Lovece, art by Anthony Williams), "Hyperkind" (written by Fred Burke, art by Paris Cullins and Bob Petrecca) and "Saint Sinner" (written by Elaine Lee, art by Max Douglas). A 2002 Barker telefilm titled "Saint Sinner" bore no relation to the comic.
Barker horror adaptations and spinoffs in comics include the Marvel/Epic Comics series "Hellraiser", "Nightbreed", "Pinhead", "The Harrowers", "Book of the Damned", and "Jihad"; Eclipse Books' series and graphic novels "Tapping The Vein", "Dread", "Son of Celluloid", "Revelations" "The Life of Death", "Rawhead Rex" and "The Yattering and Jack", and Dark Horse Comics' "Primal", among others. Barker served as a consultant and wrote issues of the Hellraiser anthology comic book.
In 2005, IDW published a three-issue adaptation of Barker's children's fantasy novel "The Thief of Always", written and painted by Kris Oprisko and Gabriel Hernandez. IDW is publishing a 12 issue adaptation of Barker's novel "The Great and Secret Show".
In December 2007, Chris Ryall and Clive Barker announced an upcoming collaboration of an original comic book series, "Torakator", to be published by IDW.
In October 2009, IDW published "Seduth" (Written by Clive Barker and Chris Monfette; art by Gabriel Rodriguez; colours by Jay Fotos; letters by Neil Uyetake; edits by Chris Ryall; and 3-D conversion by Ray Zone), the first time Barker has created a world specifically for the comic book medium in two decades. The work was released with three variant covers; cover a featuring art by Gabriel Rodriguez and cover b with art by Clive Barker and the third is a "retailer incentive signed edition cover" with art by Clive Barker.
In 2011, Boom! Studios began publishing an original Hellraiser comic book series. The comic book picks up 2 decades after the events of , and from there, builds its own mythology. The book has several credited writers: Chris Monfette, Anthony Diblasi, Mark Miller and most recently Witch Doctor creator Brandon Seifert. The series is ongoing and has just celebrated its second anniversary in print.
In 2013, Boom! Studios announced the first original story by Barker to be published in comic book format: "Next Testament". The story concerns a man named Julian Demond who unearths the God of the Old Testament and discovers that he has bitten off more than he can chew. The series is co-written by Seraphim Films Vice President Mark Miller.
Critical studies of Clive Barker's work.
British Fantasy Society, 1994, ISBN 0952415305.
New York: Thomson/Gale, 2003, ISBN 0684312506. 
in: Darren Harris-Fain (ed.) "British Fantasy and Science Fiction Writers Since 1960". Farmington Hills, MI: Thomson/Gale, 2002,
ISBN 0787660051.

</doc>
<doc id="7574" url="http://en.wikipedia.org/wiki?curid=7574" title="Comic fantasy">
Comic fantasy

Comic fantasy is a subgenre of fantasy that is primarily humorous in intent and tone. Usually set in imaginary worlds, comic fantasy often includes puns on and parodies of other works of fantasy. It is sometimes known as low fantasy in contrast to high fantasy, which is primarily serious in intent and tone. The term "low fantasy" is used to represent other types of fantasy, however, so while comic fantasies may also correctly be classified as low fantasy, many examples of low fantasy are not comic in nature.
History.
The subgenre rose in the nineteenth century. Elements of comic fantasy can be found in such nineteenth century works
as some of Hans Christian Andersen's fairy tales, Charles Dickens' "Christmas Books", and Lewis Carroll's Alice books. The first writer to specialize in the sub-genre was "F. Anstey" in novels such as "Vice Versa" (1882), where magic disrupts Victorian society with humorous results. Anstey's work was popular enough to inspire several imitations, including E. Nesbit's light-hearted children's fantasies, "The Phoenix and the Carpet" (1904) and "The Story of the Amulet" (1906). The United States had several writers of comic fantasy, including James Branch Cabell, whose satirical fantasy "Jurgen, A Comedy of Justice" (1919) was the subject of an unsuccessful prosecution for obscenity. 
Another American writer in a similar vein was Thorne Smith,whose works (such as "Topper" and "The Night Life of the Gods") were popular and influential, and often adapted for film and television.
Humorous fantasies narrated in a "gentleman's club" setting are common; they include John Kendrick Bangs' "A Houseboat on the Styx" (1895), Lord Dunsany's "Jorkens" stories, and Maurice Richardson's 
"The Exploits of Englebrecht" (1950).
According to Lin Carter, T. H. White's works exemplify comic fantasy, L. Sprague de Camp and Fletcher Pratt's Harold Shea stories are early exemplars. The overwhelming bulk of de Camp's fantasy was comic. Pratt and de Camp were among several contributors to "Unknown Worlds", a pulp magazine which emphasized fantasy with a comedic element. The work of Fritz Leiber also appeared in "Unknown Worlds", including his Fafhrd and the Gray Mouser stories, a jocose take on the sword and sorcery subgenre.
In more modern times, Piers Anthony's "Xanth" books, Robert Asprin's "MythAdventures" of Skeeve and Aahz books, Terry Pratchett's "Discworld", and Tom Holt's books provide good examples, as do many of the works by Christopher Moore. There are also comic-strips/graphic novels in the humorous fantasy genre, including Chuck Whelon's Pewfell series and the webcomics "8-Bit Theater" and "The Order of the Stick". Other recent authors in the genre include Toby Frost, Stuart Sharp, Nicholas Andrews, and DC Farmer, and the writing team of John P. Logsdon and Christopher P. Young.
Television.
The subgenre has also been represented in television, such as in the television series "I Dream of Jeannie", "Kröd Mändoon".
Radio.
Examples on radio are the BBC's "Hordes of the Things" and "ElvenQuest".
Film.
Comic fantasy films can either be parodies (Monty Python and the Holy Grail), comedies with fantastical elements (Being John Malkovich) or animated (Shrek).

</doc>
<doc id="7575" url="http://en.wikipedia.org/wiki?curid=7575" title="CLU (programming language)">
CLU (programming language)

CLU is a programming language created at MIT by Barbara Liskov and her students between 1974 and 1975. It was notable for its use of constructors for abstract data types that included the code that operated on them, a key step in the direction of object-oriented programming (OOP). However many of the other features of OOP are (intentionally) missing, notably inheritance. CLU Is therefore referred to as an "object-based" language, but not fully "object-oriented".
Clusters.
The syntax of CLU was based on ALGOL, then the starting point for most new language designs. The key addition was the concept of a "cluster", CLU's type extension system and the root of the language's name (CLUster). Clusters correspond generally to the concept of an "class" in an OO language, and have similar syntax. For instance, here is the CLU syntax for a cluster that implements complex numbers:
A cluster is a module that encapsulates all of its components except for those explicitly named in the "is" clause. These correspond to the public components of a class in recent OO languages. A cluster also defines a type that can be named outside the cluster (in this case, "complex_number"), but its representation type (rep) is hidden from external clients.
Cluster names are global, and no namespace mechanism was provided to group clusters or allow them to be created "locally" inside other clusters.
CLU does not perform implicit type conversions. In a cluster, the explicit type conversions 'up' and 'down' change between the abstract type and the representation. There is a universal type 'any', and a procedure force[] to check that an object is a certain type. Objects may be mutable or immutable, the latter being "base types" such as integers, booleans, characters and strings.
Other features.
Another key feature of the CLU type system are "iterators", which return objects from a collection one after the other. Iterators offer an identical API no matter what data they are being used with. Thus the iterator for a collection of codice_1s can be used interchangeably with that for an array of codice_2s. A distinctive feature of CLU iterators is that they are implemented as coroutines, with each value being provided to the caller via a "yield" statement. Iterators like those in CLU are now a common feature of many modern languages, such as C#, Ruby, and Python, though recently they are often referred to as generators."(See Iterator)".
CLU also includes exception handling, based on various attempts in other languages; exceptions are raised using codice_3 and handled with codice_4. Unlike most other languages with exception handling, exceptions are not implicitly resignaled up the calling chain; exceptions that are neither caught nor resignaled explicitly are immediately converted into a special failure exception that typically terminates the program.
CLU is often credited as being the first language with type-safe variant types (called oneofs), preceding ML in this respect.
A final distinctive feature in CLU is multiple assignment, where more than one variable can appear on the left hand side of an assignment operator. For instance, writing codice_5 would exchange values of codice_6 and codice_7. In the same way, functions could return several values, like codice_8.
All objects in a CLU program live in the heap, and memory management is automatic.
CLU supported type parameterized user-defined data abstractions. It was the first language to offer type-safe bounded parameterized types, using structure "where clauses" to express constraints on actual type arguments.

</doc>
<doc id="7577" url="http://en.wikipedia.org/wiki?curid=7577" title="History of the Soviet Union (1982–91)">
History of the Soviet Union (1982–91)

The history of the Soviet Union from 1982 through 1991, spans the period from Leonid Brezhnev's death and funeral until the dissolution of the Soviet Union. Due to the years of Soviet military buildup at the expense of domestic development, economic growth stagnated . Failed attempts at reform, a standstill economy, and the success of the United States against the Soviet Union's forces in the war in Afghanistan led to a general feeling of discontent, especially in the Baltic republics and Eastern Europe.
Greater political and social freedoms, instituted by the last Soviet leader, Mikhail Gorbachev, created an atmosphere of open criticism of the communist regime. The dramatic drop of the price of oil in 1985 and 1986 profoundly influenced actions of the Soviet leadership.
Nikolai Tikhonov, the Chairman of the Council of Ministers, was succeeded by Nikolai Ryzhkov, and Vasili Kuznetsov, the acting Chairman of the Presidium of the Supreme Soviet, was succeeded by Andrei Gromyko, the former Minister of Foreign Affairs.
Several Soviet Socialist Republics began resisting central control, and increasing democratization led to a weakening of the central government. The USSR's trade gap progressively emptied the coffers of the union, leading to eventual bankruptcy. The Soviet Union finally collapsed in 1991 when Boris Yeltsin seized power in the aftermath of a failed coup that had attempted to topple reform-minded Gorbachev.
Leadership transition.
By 1982 the stagnation of the Soviet economy was obvious, as evidenced by the fact that the Soviet Union had been importing grain from the U.S. throughout the 1970s, but the system was so firmly entrenched that any real change seemed impossible. A huge rate of defense spending consumed large parts of the economy. The transition period that separated the Brezhnev and Gorbachev eras resembled the former much more than the latter, although hints of reform emerged as early as 1983.
The Andropov interregnum.
Brezhnev died on 10 November 1982. Two days passed between his death and the announcement of the election of Yuri Andropov as the new general secretary, suggesting to many outsiders that a power struggle had occurred in the Kremlin. Andropov maneuvered his way into power both through his KGB connections and by gaining the support of the military by promising not to cut defense spending. For comparison, some of his rivals such as Konstantin Chernenko were skeptical of a continued high military budget. Aged 69, he was the oldest person ever appointed as General Secretary and 11 years older than Brezhnev when he acquired that post. In June 1983, he assumed the post of chairman of the Presidium of the Supreme Soviet, thus becoming the ceremonial head of state. It had taken Brezhnev 13 years to acquire this post. Andropov began a thorough house-cleaning throughout the party and state bureaucracy, a decision made easy by the fact that the Central Committee had an average age of 69. He replaced more than one-fifth of the Soviet ministers and regional party first secretaries and more than one-third of the department heads within the Central Committee apparatus. As a result, he replaced the aging leadership with younger, more vigorous administrators. But Andropov's ability to reshape the top leadership was constrained by his own age and poor health and the influence of his rival (and longtime ally of Leonid Brezhnev) Konstantin Chernenko, who had previously supervised personnel matters in the Central Committee.
The transition of power from Brezhnev to Andropov was notably the first one in Soviet history to occur completely peacefully with no one being imprisoned, killed, or forced from office.
Andropov's domestic policy leaned heavily towards restoring discipline and order to Soviet society. He eschewed radical political and economic reforms, promoting instead a small degree of candor in politics and mild economic experiments similar to those that had been associated with the late Premier Alexei Kosygin's initiatives in the mid−1960s. In tandem with such economic experiments, Andropov launched an anti-corruption drive that reached high into the government and party ranks. Unlike Brezhnev, who possessed several mansions and a fleet of luxury cars, he lived quite simply. While visiting Budapest in early 1983, he expressed interest in Hungary's goulash communism and that the sheer size of the Soviet economy made strict top-down planning impractical. Changes were needed in a hurry for 1982 had witnessed the country's worst economic performance since WWII, with real GDP growth at almost zero percent.
In foreign affairs, Andropov continued Brezhnev's policies. US−Soviet relations deteriorated rapidly beginning in March 1983, when US President Ronald Reagan dubbed the Soviet Union an "evil empire". The official press agency TASS accused Reagan of "thinking only in terms of confrontation and bellicose, lunatic anti-communism". Further deterioration occurred as a result of the 1 Sep 1983 Soviet shootdown of Korean Air Lines Flight 007 near Moneron Island carrying 269 people including a sitting US congressman, Larry McDonald, and over Reagan's stationing of intermediate-range nuclear missiles in Western Europe. In Afghanistan, Angola, Nicaragua and elsewhere, under the Reagan Doctrine, the US began undermining Soviet-supported governments by supplying arms to anti-communist resistance movements in these nations.
President Reagan's decision to deploy medium-range MX missiles in Western Europe met with mass protests in countries such as France and West Germany, sometimes numbering 1 million people at a time. A skillful KGB propaganda campaign had succeeded in convincing many Europeans that the US and not the Soviet Union was the real aggressive warmongering nation, but much was also genuine terror over the prospect of a war, especially since there was a widespread conviction in Europe that the US, being separated from the Red Army by two oceans as opposed to a short land border, was insensitive to the people of Germany and other countries. Moreover, the memory of WWII was still strong and many Germans could not forget the destruction and mass rapes committed by Soviet troops in the closing days of that conflict. This attitude was helped along by the Reagan Administration's comments that a war between NATO and the Warsaw Pact would not necessarily result in the use of nuclear weapons.
Andropov's health declined rapidly during the tense summer and fall of 1983, and he became the first Soviet leader to miss the anniversary celebrations of the 1917 revolution that November. He died in February 1984 of kidney failure after disappearing from public view for several months. His most significant legacy to the Soviet Union was his discovery and promotion of Mikhail Gorbachev. Beginning in 1978, Gorbachev advanced in two years through the Kremlin hierarchy to full membership in the Politburo. His responsibilities for the appointment of personnel allowed him to make the contacts and distribute the favors necessary for a future bid to become general secretary. At this point, Western experts believed that Andropov was grooming Gorbachev as his successor. However, although Gorbachev acted as a deputy to the general secretary throughout Andropov's illness, Gorbachev's time had not yet arrived when his patron died early in 1984.
The Chernenko interregnum.
At 71, Konstantin Chernenko was in poor health, suffering from emphysema, and unable to play an active role in policy making when he was chosen, after lengthy discussion, to succeed Andropov. But Chernenko's short time in office did bring some significant policy changes. The personnel changes and investigations into corruption undertaken under Andropov's tutelage came to an end. Chernenko advocated more investment in consumer goods and services and in agriculture. He also called for a reduction in the CPSU's micromanagement of the economy and greater attention to public opinion. However, KGB repression of Soviet dissidents also increased. In February 1983, Soviet representatives withdrew from the World Psychiatric Organization in protest of that group's continued complaints about the use of psychiatry to suppress dissent. This policy was underlined in June when Vladimir Danchev, a broadcaster for Radio Moscow, referred to the Soviet troops in Afghanistan as "invaders" while conducting English-language broadcasts. After refusing to retract this statement, he was sent to a mental institution for several months. Valery Senderov, a leader of an unofficial union of professional workers, was sentenced to seven years in a labor camp early in the year for speaking out on discrimination practiced against Jews in education and the professions.
Although Chernenko had called for renewed "détente" with the West, little progress was made towards closing the rift in East−West relations during his rule. The Soviet Union boycotted the 1984 Summer Olympics in Los Angeles, retaliating for the United States boycott of the 1980 Summer Olympics in Moscow. In the late summer of 1984, the Soviet Union also prevented a visit to West Germany by East German leader Erich Honecker. Fighting in Afghanistan also intensified, but in the late autumn of 1984 the United States and the Soviet Union did agree to resume arms control talks in early 1985.
The poor state of Chernenko's health made the question of succession an acute one. Chernenko gave Gorbachev high party positions that provided significant influence in the Politburo, and Gorbachev was able to gain the vital support of Foreign Minister Andrei Gromyko in the struggle for succession. When Chernenko died in March 1985, Gorbachev assumed power unopposed.
Rise of Gorbachev.
The war in Afghanistan, often referred to as the Soviet Union's "Vietnam War", led to increased public dissatisfaction with the Communist regime. Also, the Chernobyl disaster in 1986 added motive force to Gorbachev's glasnost and perestroika reforms, which eventually spiraled out of control and caused the Soviet system to collapse.
Changing of the guard.
After years of stagnation, the "new thinking" (Anatoli Cherniaev, 2008: 131) of younger Communist apparatchik began to emerge. Following the death of terminally ill Konstantin Chernenko, the Politburo elected Mikhail Gorbachev to the position of General Secretary of the Communist Party of the Soviet Union (CPSU) in March 1985. At 54, Gorbachev was the youngest man since Stalin to become General Secretary and the nation's first head of state born a Soviet citizen instead of a subject of the tsar. During his official confirmation on March 11, Foreign Minister Andrei Gromyko spoke of how the new Soviet leader had filled in for Chernenko as CC Secretariat, and praised his intelligence and flexible, pragmatic ideas instead of rigid adherence to party ideology. Gorbachev was aided by a lack of serious competition in the Politburo. He immediately began appointing younger men of his generation to important party posts, including Nikolai Ryzhkov, Secretary of Economics, Viktor Cherbrikov, KGB Chief, Foreign Minister Eduard Shevardnazde (replacing the 75-year old Gromyko), Secretary of Defense Industries Lev Zaikov, and Secretary of Construction Boris Yeltsin. Removed from the Politburo and Secretariat was Grigory Romanov, who had been Gorbachev's most significant rival for the position of General Secretary. Gromyko's removal as Foreign Minister was the most unexpected change given his decades of unflinching, faithful service compared to the unknown, inexperienced Shevardnazde.
More predictably, the 80-year old Nikolai Tikhonov, the Chairman of the Council of Ministers, was succeeded by Nikolai Ryzhkov, and Vasili Kuznetsov, the acting Chairman of the Presidium of the Supreme Soviet, was succeeded by Andrei Gromyko, the former Minister of Foreign Affairs.
Further down the chain, up to 40% of the first secretaries of the "oblasts" (provinces) were replaced with younger, better educated, and more competent men. The defense establishment was also given a thorough shakeup with the commanders of all 16 military districts replaced along with all theaters of military operation, as well as the three Soviet fleets. Not since WWII had the Soviet military had such a rapid turnover of officers. 68-year old Marshal Nikolai Ogarkov was fully rehabilitated after having fallen from favor in 1983-84 due to his handling of the KAL 007 shootdown and his ideas about improving Soviet strategic and tactical doctrines were made into an official part of defense policy, although some of his other ambitions such as developing the military into a smaller, tighter force based on advanced technology were not considered feasible for the time being. Many, but not all, of the younger army officers appointed during 1985 were proteges of Ogarkov.
Gorbachev got off to an excellent start during his first months in power. He projected an aura of youth and dynamism compared to his aged predecessors and made frequent walks in the streets of the major cities answering questions from ordinary citizens. When he made public speeches, he made clear that he was interested in constructive exchanges of ideas instead of merely reciting lengthy platitudes about the excellence of the Soviet system. He also spoke candidly about the slackness and run-down condition of Soviet society in recent years, blaming alcohol abuse, poor workplace discipline, and other factors for these situations. Alcohol was a particular nag of Gorbachev's, especially as he himself did not drink, and he made one of his major policy aims curbing the consumption of it.
In terms of foreign policy, the most important one, relations with the United States, remained twitchy through 1985. In March, a US military liaison office, Maj. Arthur Nicholson, was shot dead by a Soviet guard in East Germany. This was the first American fatality in 40 years of liaison work with the Soviet military. The Reagan Administration also accused Moscow of wiretapping and tracking US diplomats in the Soviet Union. In November, KGB officer Vitaly Yurchenko appeared in the Soviet embassy in Washington DC claiming that American authorities had involuntarily detained him. After returning to Moscow, Yurchenko claimed that he had been kidnapped in Rome three months earlier and pressured to defect. The CIA meanwhile argued that Yurchenko was merely a defector who changed his mind. It was also theorized that he'd been sent on an intelligence-gathering mission to learn about CIA interrogation techniques and possibly use them to charge the US with human rights violations, an area that the Soviet Union was clearly weak on.
In October, Gorbachev made his first visit to a non-communist country when he traveled to France and was warmly received. The fashion-conscious French were also captivated by his wife Raisa and political pundits widely believed that the comparatively young Soviet leader would have a PR advantage over President Reagan, who was 20 years his senior.
Reagan and Gorbachev met for the first time in Geneva, Switzerland in November. The three weeks preceding the summit meeting were marked by an unprecedented Soviet media campaign against the Strategic Defense Initiative (SDI), taking advantage of opposition at home in the US to the program. When it finally took place, the two superpower leaders established a solid rapport that boded well for the future despite Reagan's refusal to compromise on abandonment of SDI. A joint communique by both parties stated that they were in agreement that nuclear war could not be won by either side and must never be allowed to happen. It was also agreed that Reagan and Gorbachev would carry out two more summit meetings in 1986-87.
Jimmy Carter had officially ended the policy of Détente, by financially aiding the Mujahideen movement in neighboring Afghanistan, which served as a pretext for the Soviet intervention in Afghanistan six months later, with the aims of supporting the Afghan government, controlled by the People's Democratic Party of Afghanistan. Tensions between the superpowers increased during this time, when Carter placed trade embargoes on the Soviet Union and stated that the Soviet invasion of Afghanistan was "the most serious threat to the peace since the Second World War."
East-West tensions increased during the first term of U.S. President Ronald Reagan (1981–1985), reaching levels not seen since the 1962 Cuban missile crisis as Reagan increased US military spending to 7% of the GDP. To match the USA's military buildup, the Soviet Union increased its own military spending to 27% of its GDP and froze production of civilian goods at 1980 levels, causing a sharp economic decline in the already failing Soviet economy. However, it is not clear where the number 27% of the GDP came from. This thesis is not confirmed by the extensive study on the causes of the dissolution of the Soviet Union by two prominent economists from the World Bank—William Easterly and Stanley Fischer from the Massachusetts Institute of Technology. “… the study concludes that the increased Soviet defense spending provoked by Mr. Reagan's policies was not the straw that broke the back of the Empire. The Afghan war and the Soviet response to Mr. Reagan's Star Wars program caused only a relatively small rise in defense costs. And the defense effort throughout the period from 1960 to 1987 contributed only marginally to economic decline."
If economic premises are taken into account, it is not clear why the Soviet leaders did not adopt the Chinese option—economic liberalization with preservation of political system. Instead Gorbachev chose political liberalization during the years leading to the collapse of the USSR, while not implementing any significant economic reforms.
US Financed the training for the Mujahideen warlords such as Jalaluddin Haqqani, Gulbudin Hekmatyar and Burhanuddin Rabbani eventually culminated to the fall of the Soviet satellite the Democratic Republic of Afghanistan. While the CIA and MI6 and the People's Liberation Army of China financed the operation along with the Pakistan government against the Soviet Union Eventually the Soviet Union began looking for a withdrawal route and in 1988 Geneva Accords were signed between Communist-Afghanistan and the Islamic Republic of Pakistan; under the terms Soviet troops were to withdraw. Once the withdrawal was complete the Pakistan ISI continued to support the Mujahideen against the Communist Government, by 1992 the government collapsed. US President Reagan also actively hindered the Soviet Union's ability to sell natural gas to Europe whilst simultaneously actively working to keep gas prices low, which kept the price of Soviet oil low and further starved the Soviet Union of foreign capital. This "long-term strategic offensive," which "contrasts with the essentially reactive and defensive strategy of "containment", accelerated the fall of the Soviet Union by encouraging it to overextend its economic base. The proposition that special operations by the CIA in Saudi Arabia affected the prices of Soviet oil was refuted by Marshall Goldman—one of the leading experts on the economy of the Soviet Union—in his latest book. He pointed out that the Saudis decreased their production of oil in 1985 (it reached a 16-year low), whereas the peak of oil production was reached in 1980. They increased the production of oil in 1986, reduced it in 1987 with a subsequent increase in 1988, but not to the levels of 1980 when production reached its highest level. The real increase happened in 1990, by which time the Cold War was almost over. In his book he asked why, if Saudi Arabia had such an effect on Soviet oil prices, did prices not fall in 1980 when the production of oil by Saudi Arabia reached its highest level—three times as much oil as in the mid-eighties—and why did the Saudis wait till 1990 to increase their production, five years after the CIA's supposed intervention? Why didn't the Soviet Union collapse in 1980 then?
However this theory ignores the fact that the Soviet Union had already suffered several important setbacks during “reactive and defensive strategy” of “containment”. In 1972, Nixon normalized American relationship with China, thus creating pressure on the Soviet Union. Egyptian president Sadat in 1979 after signing of Camp David peace accord severed military and economic relations with the USSR (by that time the USSR provided a lot of assistance to Egypt and supported it in all its military operations against Israel).
By the time Gorbachev ushered in the process that would lead to the dismantling of the Soviet administrative command economy through his programs of "glasnost" (political openness), "uskoreniye" (speed-up of economic development) and "perestroika" (political and economic restructuring) announced in 1986, the Soviet economy suffered from both hidden inflation and pervasive supply shortages aggravated by an increasingly open black market that undermined the official economy. Additionally, the costs of superpower status—the military, space program, subsidies to client states—were out of proportion to the Soviet economy. The new wave of industrialization based upon information technology had left the Soviet Union desperate for Western technology and credits in order to counter its increasing backwardness.
Reforms.
The Law on Cooperatives enacted in May 1989 was perhaps the most radical of the economic reforms during the early part of the Gorbachev era. For the first time since Vladimir Lenin's New Economic Policy, the law permitted private ownership of businesses in the services, manufacturing, and foreign-trade sectors. Under this provision, cooperative restaurants, shops, and manufacturers became part of the Soviet scene.
"Glasnost" resulted in greater freedom of speech and the press becoming far less controlled. Thousands of political prisoners and many dissidents were also released. Soviet social science became free to explore and publish on many subjects that had previously been off limits, including conducting public opinion polls. The All−Union Center for Public Opinion Research (VCIOM) — the most prominent of several polling organizations that were started then – was opened. State archives became more accessible, and some social statistics that had been kept secret became open for research and publication on sensitive subjects such as income disparities, crime, suicide, abortion, and infant mortality. The first center for gender studies was opened within a newly formed Institute for the Socio−Economic Study of Human Population.
In January 1987, Gorbachev called for democratization: the infusion of democratic elements such as multi−candidate elections into the Soviet political process. A 1987 conference convened by Soviet economist and Gorbachev adviser Leonid Abalkin, concluded: "Deep transformations in the management of the economy cannot be realised without corresponding changes in the political system."
In June 1988, at the CPSU's Nineteenth Party Conference, Gorbachev launched radical reforms meant to reduce party control of the government apparatus. On 1 December 1988, the Supreme Soviet amended the Soviet constitution to allow for the establishment of a Congress of People's Deputies as the Soviet Union's new supreme legislative body.
Elections to the new Congress of People's Deputies were held throughout the USSR in March and April 1989. Gorbachev, as General Secretary of the Communist Party, could be forced to resign at any moment if the communist elite became dissatisfied with him. To proceed with reforms opposed by the majority of the communist party, Gorbachev aimed to consolidate power in a new position, President of the Soviet Union, which was independent from the CPSU and the soviets (councils) and whose holder could be impeached only in case of direct violation of the law. On 15 March 1990, Gorbachev was elected as the first executive president. At the same time, Article 6 of the constitution was changed to deprive the CPSU of a monopoly on political power.
Unintended consequences.
Gorbachev's efforts to streamline the Communist system offered promise, but ultimately proved uncontrollable and resulted in a cascade of events that eventually concluded with the dissolution of the Soviet Union. Initially intended as tools to bolster the Soviet economy, the policies of "perestroika" and "glasnost" soon led to unintended consequences.
Relaxation under "glasnost" resulted in the Communist Party losing its absolute grip on the media. Before long, and much to the embarrassment of the authorities, the media began to expose severe social and economic problems the Soviet government had long denied and actively concealed. Problems receiving increased attention included poor housing, alcoholism, drug abuse, pollution, outdated Stalin-era factories, and petty to large−scale corruption, all of which the official media had ignored. Media reports also exposed crimes committed by Joseph Stalin and the Soviet regime, such as the gulags, his treaty with Adolf Hitler, and the Great Purges, which had been ignored by the official media. Moreover, the ongoing war in Afghanistan, and the mishandling of the 1986 Chernobyl disaster, which Gorbachev tried to cover up, further damaged the credibility of the Soviet government at a time when dissatisfaction was increasing.
In all, the positive view of Soviet lifelong presented to the public by the official media was rapidly fading, and the negative aspects of life in the Soviet Union were brought into the spotlight. This undermined the faith of the public in the Soviet system and eroded the Communist Party's social power base, threatening the identity and integrity of the Soviet Union itself.
Fraying amongst the members of the Warsaw Pact nations and instability of its western allies, first indicated by Lech Wałęsa's 1980 rise to leadership of the trade union Solidarity, accelerated, leaving the Soviet Union unable to depend upon its Eastern European satellite states for protection as a buffer zone. By 1989, Moscow had repudiated the Brezhnev Doctrine in favor of non−intervention in the internal affairs of its Warsaw Pact allies. Gradually, each of the Warsaw Pact nations saw their communist governments fall to popular elections and, in the case of Romania, a violent uprising. By 1991 the communist governments of Bulgaria, Czechoslovakia, East Germany, Hungary, Poland and Romania, all of which had been imposed after World War II, were brought down as revolution swept Eastern Europe.
The Soviet Union also began experiencing upheaval as the political consequences of "glasnost" reverberated throughout the country. Despite efforts at containment, the upheaval in Eastern Europe inevitably spread to nationalities within the USSR. In elections to the regional assemblies of the Soviet Union's constituent republics, nationalists as well as radical reformers swept the board. As Gorbachev had weakened the system of internal political repression, the ability of the USSR's central Moscow government to impose its will on the USSR's constituent republics had been largely undermined. Massive peaceful protests in the Baltic Republics such as The Baltic Way and the Singing Revolution drew international attention and bolstered independence movements in various other regions.
The rise of nationalism under "freedom of speech" soon reawakened simmering ethnic tensions in various Soviet republics, further discrediting the ideal of a unified Soviet people. One instance occurred in February 1988, when the government in Nagorno-Karabakh, a predominantly ethnic Armenian region in the Azerbaijan SSR, passed a resolution calling for unification with the Armenian SSR. Violence against local Azerbaijanis was reported on Soviet television, provoking massacres of Armenians in the Azerbaijani city of Sumgait.
Emboldened by the liberalized atmosphere of "glasnost", public dissatisfaction with economic conditions was much more overt than ever before in the Soviet period. Although "perestroika" was considered bold in the context of Soviet history, Gorbachev's attempts at economic reform were not radical enough to restart the country's chronically sluggish economy in the late 1980s. The reforms made some inroads in decentralization, but Gorbachev and his team left intact most of the fundamental elements of the Stalinist system, including price controls, inconvertibility of the ruble, exclusion of private property ownership, and the government monopoly over most means of production.
By 1990 the Soviet government had lost control over economic conditions. Government spending increased sharply as an increasing number of unprofitable enterprises required state support and consumer price subsidies to continue. Tax revenues declined as republic and local governments withheld tax revenues from the central government under the growing spirit of regional autonomy. The anti−alcohol campaign reduced tax revenues as well, which in 1982 accounted for about 12% of all state revenue. The elimination of central control over production decisions, especially in the consumer goods sector, led to the breakdown in traditional supplier−producer relationships without contributing to the formation of new ones. Thus, instead of streamlining the system, Gorbachev's decentralization caused new production bottlenecks.
Dissolution of the USSR.
The dissolution of the Soviet Union was a process of systematic disintegration, which occurred in the economy, social structure and political structure. It resulted in the abolition of the Soviet Federal Government ("the Union center") and independence of the USSR's republics on 25 December 1991. The process was caused by a weakening of the Soviet government, which led to disintegration and took place from about 19 January 1990 to 31 December 1991. The process was characterized by many of the republics of the Soviet Union declaring their independence and being recognized as sovereign nation-states.
Andrei Grachev, the Deputy Head of the Intelligence Department of the Central Committee, summed up the denouement of the downfall quite cogently:
"Gorbachev actually put the sort of final blow to the resistance of the Soviet Union by killing the fear of the people. It was still that this country was governed and kept together, as a structure, as a government structure, by the fear from Stalinist times."
Summary.
The principal elements of the old Soviet political system were Communist Party dominance, the hierarchy of soviets, state socialism, and ethnic federalism. Gorbachev's programs of "perestroika" (restructuring) and "glasnost" (openness) produced radical unforeseen effects that brought that system down. As a means of reviving the Soviet state, Gorbachev repeatedly attempted to build a coalition of political leaders supportive of reform and created new arenas and bases of power. He implemented these measures because he wanted to resolve serious economic problems and political inertia that clearly threatened to put the Soviet Union into a state of long−term stagnation.
But by using structural reforms to widen opportunities for leaders and popular movements in the union republics to gain influence, Gorbachev also made it possible for nationalist, orthodox communist, and populist forces to oppose his attempts to liberalize and revitalize Soviet communism. Although some of the new movements aspired to replace the Soviet system altogether with a liberal democratic one, others demanded independence for the national republics. Still others insisted on the restoration of the old Soviet ways. Ultimately, Gorbachev could not forge a compromise among these forces and the consequence was the dissolution of the Soviet Union.
Post-Soviet restructuring.
To restructure the Soviet administrative command system and implement a transition to a market-based economy, Yeltsin's shock program was employed within days of the dissolution of the Soviet Union. The subsidies to money-losing farms and industries were cut, price controls abolished, and the ruble moved towards convertibility. New opportunities for Yeltsin's circle and other entrepreneurs to seize former state property were created, thus restructuring the old state-owned economy within a few months.
After obtaining power, the vast majority of "idealistic" reformers gained huge possessions of state property using their positions in the government and became business oligarchs in a manner that appeared antithetical to an emerging democracy. Existing institutions were conspicuously abandoned prior to the establishment of new legal structures of the market economy such as those governing private property, overseeing financial markets, and enforcing taxation.
Market economists believed that the dismantling of the administrative command system in Russia would raise GDP and living standards by allocating resources more efficiently. They also thought the collapse would create new production possibilities by eliminating central planning, substituting a decentralized market system, eliminating huge macroeconomic and structural distortions through liberalization, and providing incentives through privatization.
Since the USSR's collapse, Russia faced many problems that free market proponents in 1992 did not expect. Among other things, 25% of the population lived below the poverty line, life expectancy had fallen, birthrates were low, and the GDP was halved. These problems led to a series of crises in the 1990s, which nearly led to the election of Yeltsin's Communist challenger, Gennady Zyuganov, in the 1996 presidential election. In recent years, the economy of Russia has begun to improve greatly, due to major investments and business development and also due to high prices of natural resources.
Further reading.
Anatoli Cherniaev, ‘Gorbachev’s Foreign Policy: The Concept’ in Skinner, Kiron (ed.) Turning Points in Ending the Cold War, (Hoover Institution Press: 2008), pp. 111–140, p. 111; [online] <http://www.hoover.org/publications/books/8237> [accessed 22–23 February 2012]. Also for ‘new thinking’ see Ibid., p. 131.

</doc>
<doc id="7578" url="http://en.wikipedia.org/wiki?curid=7578" title="Corsican language">
Corsican language

Corsican ("corsu" or "lingua corsa") is an Italo-Dalmatian Romance language spoken and written on the islands of Corsica (France) and northern Sardinia (Italy). Corsican was long the vernacular language alongside Italian, the official language in Corsica until 1859; afterwards Italian was replaced by French, owing to the acquisition of the island by France from Genoa in 1768. Over the next two centuries, the use of French grew to the extent that, by the Liberation in 1945, all islanders had a working knowledge of French. The twentieth century saw a wholesale language shift, with islanders changing their language practices to the extent that there were no monolingual Corsican speakers left by the 1960s. By 1990, an estimated 50% of islanders had some degree of proficiency in Corsican, and a small minority, perhaps 10%, used Corsican as a first language.
Number of speakers.
The January 2007 estimated population of the island was 281,000, while the figure for the March 1999 census, when most of the studies – though not the linguistic survey work referenced in this article – were performed, was about 261,000 (see under Corsica). Only a fraction of the population at either time spoke Corsican with any fluency.
The use of Corsican over French has been declining. In 1980 about 70% of the population "had some command of the Corsican language". In 1990 out of a total population of about 254,000 the percentage had declined to 50%, with only 10% using it as a first language. (These figures do not count varieties of Corsican spoken in Sardinia.) The language appeared to be in serious decline when the French government reversed its unsupportive stand and initiated some strong measures to save it.
UNESCO classifies Corsican as a "definitely endangered language". The Corsican language is a key vehicle for Corsican culture, which is notably rich in proverbs and in polyphonic song.
Governmental support.
The 1991 "Joxe Statute", in setting up the Collectivité Territoriale de Corse, also provided for the Corsican Assembly, and charged it with developing a plan for the optional teaching of Corsican. The University of Corsica Pascal Paoli at Corte took a central role in the planning.
At the primary school level Corsican is taught up to a fixed number of hours per week (three in the year 2000) and is a voluntary subject at the secondary school level, but is required at the University of Corsica. It is available through adult education. It can be spoken in court or in the conduct of other government business if the officials concerned speak it. The Cultural Council of the Corsican Assembly advocates for its use; for example, on public signs.
Sources.
According to the anthropologist Dumenica Verdoni, writing new literature in modern Corsican, known as the "Riacquistu", is an integral part of affirming Corsican identity. Persons who had a notable career in France returned to Corsica to write in Corsican, such as the musical producers, Dumenicu Togniotti, director of the Teatru Paisanu, which produced polyphonic musicals, 1973–1982, followed in 1980 by Michel Raffaelli's Teatru di a Testa Mora, and Saveriu Valentini's Teatru Cupabbia in 1984. The list of prose writers includes Alanu di Meglio, Ghjacumu Fusina, Lucia Santucci, Marcu Biancarelli,and many others.
A mythology concerning the Corsican language is to some degree current among foreigners, that it was a spoken language only or was only recently written. Omniglot goes so far as to assert "Corsican first appeared in writing towards the end of the 19th century ..." Throughout the 1700s and 1800s there was a steady stream of writers in Corsican, many of whom wrote also in other languages.
Ferdinand Gregorovius, 1800's traveller and enthusiast of Corsican culture, reports that the preferred form of the literary tradition of his time was the "vocero", a type of polyphonic ballad originating from funeral obsequies. These laments were similar in form to the chorales of Greek drama except that the leader could improvise. Some performers were noted at this, such as the 1700s Mariola della Piazzole and Clorinda Franseschi.
The trail of written popular literature of known date in Corsican currently goes no further back than the 1600s. An undated corpus of proverbs from communes may well precede it (see under "External links" below). Corsican has also left a trail of legal documents ending in the late 1100s. At that time the monasteries held considerable land on Corsica and many of the churchmen were notaries.
Between 1200 and 1425 the monastery of Gorgona, Benedictine for much of that time and in the territory of Pisa, acquired about 40 legal papers of various sorts written on Corsica. As the church was replacing Pisan prelates with Corsican ones there the legal language shows a transition from entirely Latin through partially Latin, partially Corsican to entirely Corsican. The first known surviving document containing some Corsican is a bill of sale from Patrimonio dated to 1220. These documents were moved to Pisa before the monastery closed its doors and were published there. Research into earlier evidence of Corsican is ongoing.
Origins.
The Corsican language has been influenced by the languages of the major powers taking an interest in Corsican affairs; earlier by those of the Medieval Italian powers: Tuscany (828–1077), Pisa (1077–1282) and Genoa (1282–1768), more recently by France (1768–present), which, since 1789, has promulgated the official Parisian French. The term "gallicised Corsican" refers to Corsican up to about the year 1950. The term "distanciated Corsican" refers to an idealized Corsican from which various agents have removed French or other elements.
The general classification of Corsican as a Romance language allows two possibilities as to the identity of the speakers of the first distinct Corsican, or Proto-Corsican. They created the language either from Proto-Romance or from a subsequent Romance language.
In 40 AD neither a Romance nor an Italic language were spoken by the natives of Corsica. The Roman exile, Seneca the younger, reports that both coast and interior were occupied by natives whose language he did not understand (see under Prehistory of Corsica). Latin at that time was generally spoken only in the Roman colonies. There was probably a substratic language that is still visible in the toponymy or in some words, for instance Gallurese "zerru" 'pig'. The same is valid for Sardinian. The occupation of the island by Vandals about 469 AD marks the end of authoritative influence by Latin-speaking Romans (see under Medieval Corsica). If the natives of that time were speaking Latin they must have acquired it during the late empire. The documents of the early Christian church concerning Corsica are in Latin, but they are only communications between church officials (see under Ajaccio).
The next window of opportunity for the predecessor of a Proto-Corsican was the administration of Corsica by Tuscany, then speaking the Tuscan dialect, an immediate predecessor of Italian. The first Italian documents date from the 900's but Italian must have developed earlier and Tuscan even earlier. Tuscan would have come from the latest phases of Vulgar Latin; Proto-Corsican from the Tuscan spoken on Corsica.
The last historical possibility is that Proto-Corsican came from the Tuscan dialect of Pisa; its period of Corsican administration, however, was relatively short. Genoese is not a likely possibility as Corsican is attested before the presence of Genoa on Corsica, and the linguistic features of Corsican do not match well with those of Genoese. Historical circumstances alone reduce the window of opportunity only to within several hundred years.
Classification by subjective analysis.
One of the main sources of confusion in popular classifications is the difference between a dialect and a language. Typically it is not possible to ascertain what an author means by these terms. For example, one might read that Corsican is a "central southern Italian dialect" along with Tuscan, Neapolitan, Sicilian and others or that it is "closely related to the Tuscan dialect of Italian,".
One of the characteristics of Italian, but not of modern Tuscan, is the retention of the -"re" infinitive ending as in Latin "mittere", "send", which is lost in Corsican (and in modern Tuscan), which has "mette"/"metta", "to put." The Latin relative pronoun, "who," "qui," "quae," and "what," "quod," are inflected in Latin, while relative pronoun in Italian for "who" and "what" is "chi", "che cosa" and in Corsican is uninflected "chì."
Dialects in Corsica.
The two most widely spoken forms of the Corsican language are "Supranacciu", spoken in the Bastia and Corte area (generally throughout the northern half of the island, in Corsican "Corsica suprana"), and "Suttanacciu", spoken around Sartène and Porto-Vecchio (generally throughout the southern half of the island, in Corsican "Corsica suttana"). The dialect of Ajaccio has been described as in transition. The dialects spoken at Calvi and Bonifacio are closer to the genoese dialect, also known as Ligurian.
On Maddalena archipelago the local dialect (called "Isulanu, Maddaleninu, Maddalenino") was brought by fishermen and shepherds from Bonifacio during immigration in the 17th-18th centuries. Though influenced by Gallurese it has maintained the original characteristics of Corsican. There are also numerous words of Genoese and Ponzese origin.
Corsican in Sardinia.
Gallurese is spoken in the extreme north of Sardinia, including the region of Gallura and the archipelago of La Maddalena. Sassarese is spoken in Sassari and in its neighbourhood, in the north-west of Sardinia. Whether these two languages should be included in Corsican as dialects, included in Sardinian as dialects, or considered as independent languages, is debatable.
Article 2 Item 4 of Law Number 26, October 15, 1997, of the Autonomous Region of Sardinia grants "al dialetto sassarese e a quello gallurese" equal legal status with the other languages on Sardinia. They are thus legally defined as different languages from Sardinian by the Sardinian government.
Alphabet and spelling.
Corsican is written in the standard Latin script, using 22 of the letters for native words. The letters k, w, x, and y are found only in foreign names and French vocabulary. The multigraphs chj, ghj, sc, and sg are also defined as "letters" of the alphabet in its modern scholarly form (compare the presence of "ch" or "ll" in the Spanish alphabet), and appear respectively after c, g, and s.
The primary diacritic used is the grave accent, indicating word stress when not penultimate. In scholarly contexts disyllables may be distinguished from diphthongs by use of the diaeresis on the "first" of the two vowels (as in Italian and distinct from French and English). In older writing the acute accent is sometimes found on stressed , and circumflex on stressed , indicating respectively the Close E () and Close O () phonemes.
Corsican has been regarded as a dialect of Italian historically, similarly to the regional Romance languages in Italy proper, and in writing often resembles Italian (with the substitution of -u for final -o, and the articles "u" and "a" for "il/lo" and "la" respectively). However the phonemes of the modern spoken forms of Corsican undergo complex and sometimes irregular phenomena depending on phonological context, thus pronunciation of the language for foreigners familiar with other Romance languages is not straightforward.
Phonology.
Vowel inventory.
As in Italian, the grapheme appears in some digraphs and trigraphs in which it does not represent the phonemic vowel. All vowels are pronounced except in a few well-defined instances. is not pronounced before , , after , , and : "sciarpa" ; or initially in some words: "istu" .
Vowels may be nasalized before (which is assimilated to before or ) and the palatal nasal consonant represented by . The nasal vowels are represented by the vowel plus , or . The combination is a digraph or trigraph indicating the nasalized vowel. The consonant is pronounced in weakened form. The same combination of letters might not be the digraph or trigraph but might be just the non-nasal vowel followed by the consonant at full weight. The speaker must know the difference. Example of nasal: is pronounced and not .
The vowel inventory, or collection of phonemic vowels (and the major allophones), transcribed in IPA symbols, is:

</doc>
<doc id="7580" url="http://en.wikipedia.org/wiki?curid=7580" title="Commodore International">
Commodore International

Commodore International (or Commodore International Limited) was a North American home computer and electronics manufacturer. Commodore International (CI) along with its subsidiary Commodore Business Machines (CBM) participated in the development of the home–personal computer industry in the 1970s and 1980s. The company developed and marketed one of the world's best-selling desktop computers, the Commodore 64 (1982) and released its Amiga computer line in 1985.
History.
Founding and early years.
The company that would become Commodore Business Machines, Inc. was founded in 1954 in Toronto as the Commodore Portable Typewriter Company by Polish immigrant and Auschwitz survivor Jack Tramiel. For a few years he had been living in New York, driving a taxicab and running a small business repairing typewriters, when he managed to sign a deal with a Czechoslovakian company to manufacture their designs in Canada. He moved to Toronto to start production. By the late 1950s a wave of Japanese machines forced most North American typewriter companies to cease business, but Tramiel instead turned to adding machines.
In 1955, the company was formally incorporated as Commodore Business Machines, Inc. (CBM) in Canada. In 1962, Commodore went public on the New York Stock Exchange (NYSE) under the name of Commodore International Limited. In the late 1960s history repeated itself when Japanese firms started producing and exporting adding machines. The company's main investor and chairman, Irving Gould, suggested that Tramiel travel to Japan to understand how to compete. Instead, he returned with the new idea to produce electronic calculators, which were just coming on the market.
Commodore soon had a profitable calculator line and was one of the more popular brands in the early 1970s, producing both consumer as well as scientific/programmable calculators. However, in 1975, Texas Instruments, the main supplier of calculator parts, entered the market directly and put out a line of machines priced at less than Commodore's cost for the parts. Commodore obtained an infusion of cash from Gould, which Tramiel used beginning in 1976 to purchase several second-source chip suppliers, including MOS Technology, Inc., in order to assure his supply. He agreed to buy MOS, which was having troubles of its own, only on the condition that its chip designer Chuck Peddle join Commodore directly as head of engineering.
Through the 1970s, Commodore also produced numerous peripherals and consumer electronic products such as the Chessmate, a chess computer based around a MOS 6504 chip, released in 1978.
In December 2007 when Tramiel was visiting the Computer History Museum in Mountain View, California, for the 25th anniversary of the Commodore 64, he was asked why he called his company Commodore. He said: "I wanted to call my company General, but there's so many Generals in the U.S.: General Electric, General Motors. Then I went to Admiral, but that was taken. So I wind up in Berlin, Germany, with my wife, and we were in a cab, and the cab made a short stop, and in front of us was an Opel Commodore." Tramiel gave this account in many interviews, but Opel's Commodore didn't debut until 1967, years after the company had been named.
"Computers for the masses, not the classes".
Once Chuck Peddle had taken over engineering at Commodore, he convinced Jack Tramiel that calculators were already a dead end and that they should turn their attention to home computers. Peddle packaged his single-board computer design in a metal case, initially with a keyboard using calculator keys, later with a full-travel QWERTY keyboard, monochrome monitor, and tape recorder for program and data storage, to produce the Commodore PET (Personal Electronic Transactor). From PET's 1977 debut, Commodore would be a computer company.
Commodore had been reorganized the year before into Commodore International, Ltd., moving its financial headquarters to the Bahamas and its operational headquarters to West Chester, Pennsylvania, near to the MOS Technology site. The operational headquarters, where research and development of new products occurred, retained the name Commodore Business Machines, Inc.
The PET computer line was used primarily in schools, due to its tough all-metal construction (some models were given away as part of a "buy 2 get 1 free" promotion aimed at schools and were labeled "Teacher's PET"), but did not compete well in the home setting where graphics and sound were important. This was addressed with the introduction of the VIC-20 in 1981, which was introduced at a cost of US$299 and sold in retail stores. Commodore took out aggressive ads featuring William Shatner asking consumers "Why buy just a video game?" The strategy worked and the VIC-20 became the first computer to ship more than one million units. A total of 2.5 million units were sold over the machine's lifetime.
In 1982, Commodore introduced the Commodore 64 as the successor to the VIC-20. Thanks to a well-designed set of chips designed by MOS Technology, the Commodore 64, (also referred to as C64), possessed remarkable sound and graphics for its time and is often credited with starting the computer demo scene. Its US$595 price was high compared with that of the VIC-20, but it was still much less expensive than any other 64K computer on the market. Early C64 ads boasted, "You can't buy a better computer at twice the price."
Australian adverts in the mid-1980s used a tune speaking the words "Are you keeping up with the Commodore? Because the Commodore is keeping up with you."
In 1983, Tramiel decided to focus on market share and cut the price of the VIC-20 and C64 dramatically, starting what would be called the "home computer war". TI responded by cutting prices on its TI-99/4A, which had been introduced in 1981. Soon there was an all-out price war involving Commodore, TI, Atari and practically every vendor other than Apple Computer. Commodore began selling the VIC-20 and C64 through mass-market retailers such as K-Mart, in addition to traditional computer stores. By the end of this conflict, Commodore had shipped somewhere around 22 million C64s—making the C64 the best selling computer of all time.
Commodore's strategy was to, according to a spokesman, devote 50% of its efforts to the under-$500 market, 30% on the $500–1000 market, and 20% on the over-$1000 market. Its vertical integration and Tramiel's focus on cost control helped Commodore do well during the price war, with $1 billion in 1983 sales.
Tramiel quits; the Amiga vs. ST battle.
Commodore's board of directors were as impacted as anyone else by the price spiral and decided they wanted out. An internal power struggle resulted; in January 1984, Tramiel resigned due to intense disagreement with the chairman of the board, Irving Gould. Gould replaced Tramiel with Marshall F. Smith, a steel executive who had no experience with computers or consumer marketing. Tramiel founded a new company, Tramel Technology (spelled differently so people would pronounce it correctly), and hired away a number of Commodore engineers to begin work on a next-generation computer design.
Now it was left to the remaining Commodore management to salvage the company's fortunes and plan for the future. It did so by buying a small startup company called Amiga Corporation in August 1984, for $25 million ($12.8 million in cash & 550,000 in common shares) which became a subsidiary of Commodore, called Commodore-Amiga, Inc. Commodore brought this new 32-bit computer design (initially codenamed "Lorraine", later dubbed the Amiga 1000) to market in the fall of 1985 for US $1295.
But Tramiel had beaten Commodore to the punch. His design was 95% completed by June (which fueled speculation that his engineers had taken technology with them from Commodore). In July 1984 he bought the consumer side of Atari Inc. from Warner Communications which allowed him to strike back and release the Atari ST earlier in 1985 for about $800.
During development in 1983, Amiga had exhausted venture capital and was desperate for more financing. Jay Miner and company had approached former employer Atari, and the "Warner-owned" Atari had paid Amiga to continue development work. In return Atari was to get one-year exclusive use of the design as a video game console. After one year Atari would have the right to add a keyboard and market the complete Amiga computer. The Atari Museum has acquired the Atari-Amiga contract and Atari engineering logs revealing that the Atari Amiga was originally designated as the 1850XLD. As Atari was heavily involved with Disney at the time, it was later code-named "Mickey", and the 256K memory expansion board was codenamed "Minnie".
The following year, Tramiel discovered that Warner Communications wanted to sell Atari, which was rumored to be losing about $10,000 a day. Interested in Atari's overseas manufacturing and worldwide distribution network for his new computer, he approached Atari and entered negotiations. After several on-again/off-again talks with Atari in May and June 1984, Tramiel had secured his funding and bought Atari's Consumer Division (which included the console and home computer departments) in July.
As more execs and researchers left Commodore after the announcement to join up with Tramiel's new company Atari Corp., Commodore followed by filing lawsuits against four former engineers for theft of trade secrets in late July. This was intended, in effect, to bar Tramiel from releasing his new computer.
One of Tramiel's first acts after forming Atari Corp. was to fire most of Atari's remaining staff, and to cancel almost all ongoing projects, in order to review their continued viability. In late July/early August, Tramiel representatives discovered the original Amiga contract from the previous fall. Seeing a chance to gain some leverage, Tramiel immediately used the contract to counter-sue Commodore through its new subsidiary, Amiga, on August 13.
The Amiga crew, still suffering serious financial problems, had sought more monetary support from investors that entire spring. At around the same time that Tramiel was in negotiations with Atari, Amiga entered into discussions with Commodore. The discussions ultimately led to Commodore's intentions to purchase Amiga outright, which would (from Commodore's viewpoint) cancel any outstanding contracts - including Atari Inc.'s. This "interpretation" is what Tramiel used to counter-sue, and sought damages and an injunction to bar Amiga (and effectively Commodore) from producing any resembling technology. This was an attempt to render Commodore's new acquisition (and the source for its next generation of computers) useless. The resulting court case lasted for several years, with both companies releasing their respective products. By March 1987 they had settled out of court, with all suits against Tramiel's engineers dropped. His "Business is War" tactics had succeeded again.
Throughout the life of the ST and Amiga platforms, a ferocious Atari-Commodore rivalry raged. While this rivalry was in many ways a holdover from the days when the Commodore 64 had first challenged the Atari 800 (among others) in a series of scathing television commercials, the events leading to the launch of the ST and Amiga only served to further alienate fans of each computer, who fought vitriolic holy wars on the question of which platform was superior. This was reflected in sales numbers for the two platforms until the release of the Amiga 500 in 1987 which led the Amiga sales to exceed the ST by about 1.5 to 1, despite reaching the market later. However, the battle was in vain, as neither platform captured a significant share of the world computer market and only the Apple Macintosh would survive the industry-wide shift to Microsoft Windows running on PC clones.
Demise and bankruptcy.
One columnist stated in April 1981 that "the microcomputer industry abounds with horror stories describing the way Commodore treats its dealers and its customers." After Tramiel's departure, another journalist wrote that he "had never been able to establish very good relations with computer dealers ... computer retailers have accused Commodore of treating them as harshly as if they were suppliers or competitors, and as a result, many have become disenchanted with Commodore and dropped the product line". After Tramiel's departure, Commodore executives shied away from mass advertising and other marketing ploys, fearful of repeating past mistakes. Commodore also retreated from its earlier strategy of selling its computers to discount outlets and toy stores, and now favored authorized dealers.
Commodore faced the problem when marketing the Amiga of still being perceived as a company that makes cheap, disposable computers like the 64 and Vic 20. By the late 1980s, the personal computer market had become dominated by the IBM PC and Apple Macintosh platforms. Commodore's marketing efforts for the Amiga were less competitive. The company also concentrated on consumer products that would not see a demand for another few years, such as a digital TV system called CDTV. As early as 1986, the mainstream press was predicting Commodore's demise, and in 1990 "Computer Gaming World" wrote of its "abysmal record of customer and technical support in the past". Nevertheless, "The Philadelphia Inquirer's" Top 100 Businesses annual continued to list several Commodore executives among the highest-paid in the region.
In the early 1990s, CBM continued selling Amigas with 7–14 MHz 68000-family CPUs, even though the Amiga 3000 with 25 MHz 68030 was in the market. At this time, PCs had finally caught up with the Amiga's performance, but only when fitted with high-color graphics cards and SoundBlaster (or compatible) sound cards, consequently at a far higher cost. The Amiga was not successful in the business market competing with PCs, where high-performance sound and graphics were irrelevant for most routine business word-processing and data-processing requirements. Commodore introduced a range of PC compatible systems, and while the Commodore name was better known in the US than some of its competition, the systems' price and specs were only average.
In 1992, the production of the A600 replaced the A500. It removed the numeric keypad, Zorro expansion slot, SCSI capability, and other functionality in favor of PCMCIA and a theoretically cost-reduced design. It was basically unexpandable and lasted less than a year. Productivity developers moved to PC and Macintosh, while the console wars took over the gaming market. David Pleasance, managing director of Commodore UK, described the A600 as a 'complete and utter screw-up'.
In late 1992, Commodore released the A4000 and A1200 computers, which featured an improved graphics chipset, the AGA. The custom-designed and custom-built AGA chipset cost Commodore more than the commodity chips used in IBM PCs, reducing Commodore's competitive edge.
In 1994, the 'make or break' system, according to Pleasance, was a 32-bit CD-ROM-based game console called the Amiga CD32, but it was not sufficiently profitable to put Commodore back in the black.
In the early 1990s, all servicing and warranty repairs were outsourced to Wang Laboratories. By 1994, only its operations in Germany and the United Kingdom were still profitable. Commodore declared bankruptcy on April 29, 1994, and the board of directors "authorized the transfer of its assets to trustees for the benefit of its creditors," according to an official statement.
The company's computer systems, especially the C64 and Amiga series, retained a cult-following among their users decades after its demise.
Post-Commodore International, Ltd..
Following its liquidation, Commodore's former assets went their separate ways, with none of the descendant companies repeating Commodore's early success. Both Commodore and Amiga product lines were produced in the 21st century, but separately with Amiga, Inc. being its own company and "Commodore" computers being produced by Commodore USA, an unrelated Florida based company that had purchased the brand name. Other companies develop operating systems and manufacture computers for both Commodore and Amiga brands as well as software.
Commodore UK was the only subsidiary to survive the bankruptcy and even placed a bid to buy out the rest of the operation, or at least the former parent company. For a time it was considered the front runner in the bid, and numerous reports surfaced during the 1994–1995 time frame that Commodore UK had made the purchase. Commodore UK stayed in business by selling old inventory and making computer speakers and some other types of computer peripherals. However, Commodore UK lost its financial backing after several larger companies, including Gateway Computers and Dell Inc., became interested, primarily for Commodore's 47 patents relating to the Amiga. Ultimately, the successful bidder was German PC conglomerate Escom, and Commodore UK was absorbed into Escom in mid-1995.
In 1995 Escom paid US$14 million for the assets of Commodore International. It separated the Commodore and Amiga operations into separate divisions and quickly started using the Commodore brand name on a line of PCs sold in Europe. However, it soon started losing money due to over-expansion, went bankrupt on July 15, 1996, and was liquidated.
In September 1997, the Commodore brand name was acquired by Dutch computer maker Tulip Computers NV.
The Commodore brand name resurfaced in late 2003 on an inexpensive portable MP3 player made in the People's Republic of China by Tai Guen Enterprise, sold mostly in Europe. However, the device's connection to Tulip, the legal owners of the name, is unclear.
In July 2004, Tulip announced a new series of products using the Commodore name: fPET, a flash memory-based USB Flash drive; mPET, a flash-based MP3 Player and digital recorder; eVIC, a 20 GB music player. Also, it licensed the Commodore trademark and "chicken lips" logo to the producers of the C64 DTV.
In late 2004, Tulip sold the Commodore trademarks to Yeahronimo Media Ventures for €22 million. The sale was completed in March 2005 after months of negotiations. Yeahronimo Media Ventures soon renamed itself to "Commodore International Corporation" and started an operation intended to relaunch the Commodore brand. The company launched its "Gravel" line of products: personal multimedia players equipped with Wi-Fi, with the hope the Commodore brand would help them take off. The "Gravel" was never a success and was discontinued. On June 24, 2009, CIC renamed itself to Reunite Investments. CIC's founder, Ben van Wijhe, bought a Hong Kong-based company called Asiarim. The brand is now owned by C= Holdings (formerly Commodore International B.V.): Reunite became the sole owner of it in 2010, after buying the remaining shares from the bankrupt Nedfield, then sold it to Commodore Licensing BV, a subsidiary of Asiarim, later in 2010. It was sold again on 7 November 2011: this transaction became the basis of a legal dispute between Asiarim (which, even after that date, made commercial use of the Commodore trademark, among others by advertising for sale Commodore-branded computers, and dealing licensing agreements for the trademarks) and the new owners, that was resolved by the United States District Court for the Southern District of New York on 16 December 2013 in favour of the new owners.
The Commodore Semiconductor Group (formerly MOS Technology, Inc.) was bought by its former management and in 1995, resumed operations under the name GMT Microelectronics, utilizing a troubled facility in Norristown, Pennsylvania that Commodore had closed in 1992. By 1999 it had $21 million in revenues and 183 employees. However, in 2001 the United States Environmental Protection Agency shut the plant down. GMT ceased operations and was liquidated.
Ownership of the remaining assets of Commodore International, including the copyrights and patents, and the Amiga trademarks, passed from Escom to U.S. PC clone maker Gateway 2000 in 1997, who retained the patents and sold the copyrights and trademarks, together with a license to use the patents, to Amiga, Inc., a Washington company founded, among others, by former Gateway subcontractors Bill McEwen and Fleecy Moss in 2000. On March 15, 2004, Amiga, Inc. announced that on April 23, 2003 it had transferred its rights over past and future versions of the Amiga OS (but not yet over other intellectual property) to Itec, LLC, later acquired by KMOS, Inc., a Delaware company. Shortly afterwards, on the basis of some loans and security agreements between Amiga, Inc. and Itec, LLC, the remaining intellectual property assets were also transferred from Amiga, Inc. to KMOS, Inc. On March 16, 2005, KMOS, Inc. announced that it had completed all registrations with the State of Delaware to change its corporate name to Amiga, Inc. AmigaOS (as well as spin-offs MorphOS and AROS) is still maintained and updated. Several companies produce related hardware and software today.
Product line.
This product line consists of original Commodore products.
Calculators.
774D, 9R23, C108, C110, F4146R, F4902, MM3, Minuteman 6, P50, PR100, SR1800, SR4120D, SR4120R, SR4148D, SR4148R, SR4190R, SR4212, SR4912, SR4921RPN, SR5120D, SR5120R, SR5148D, SR5148R, SR5190R, SR59, SR7919, SR7949, SR9150R, SR9190R, US*3, and The Specialist series: M55 (The Mathematician), N60 (The Navigator), S61 (The Statistician).
Computers.
"(listed chronologically)"

</doc>
<doc id="7581" url="http://en.wikipedia.org/wiki?curid=7581" title="Commodore (rank)">
Commodore (rank)

Commodore is a naval rank used in many navies that is superior to a navy captain, but below a rear admiral. Non-English-speaking nations often use the rank of flotilla admiral or counter admiral or senior captain as an equivalent, although the latter may also correspond to rear admiral.
Traditionally, "commodore" is the title for any officer assigned to command more than one ship at a time, even temporarily, much as "captain" is the traditional title for the commanding officer of a single ship even if the officer's official title in the service is a lower rank. As an official rank, a commodore typically commands a flotilla or squadron of ships as part of a larger task force or naval fleet commanded by an admiral.
It is often regarded as a one-star rank with a NATO code of OF-6 (which is known in the U.S. as "rear admiral (lower half)"), but whether it is regarded as a flag rank varies between countries.
It is sometimes abbreviated: as "Cdre" in British Royal Navy, "CDRE" in the US Navy, "Cmdre" in the Royal Canadian Navy, "COMO" in the Spanish Navy and in the some navies speaking the Spanish language, or "CMDE" as used in the Indian Navy or in some other country's navies. 
Etymology.
The rank of commodore derives from the French "commandeur", which was one of the highest ranks in orders of knighthood, and in military orders the title of the knight in charge of a "commenda" (a local part of the order's territorial possessions).
History.
The Dutch Navy also used the rank of "commandeur" from the end of the 16th century for a variety of temporary positions, until it became a conventional permanent rank in 1955. The Royal Netherlands Air Force has adopted the English spelling of "commodore" for an equivalent rank.
The rank of commodore was at first a position created as a temporary title to be bestowed upon captains who commanded squadrons of more than one vessel. In many navies, the rank of commodore was merely viewed as a senior captain position, whereas other naval services bestowed upon the rank of commodore the prestige of flag officer status; commodore is the highest rank in the Irish Naval Service, for example, and is held by only one person. In the Royal Navy, the position was introduced to combat the cost of appointing more admirals—a costly business with a fleet as large as the Royal Navy's at that time.
United States.
In 1899 the substantive rank of commodore was discontinued in the United States Navy, but revived during World War II. It was discontinued as a rank in these services during the postwar period, but as an appointment, the title "commodore" was then used to identify senior U.S. Navy captains who commanded squadrons of more than one vessel or functional air wings or air groups that were not part of a carrier air wing or air group. Concurrently, until the early 1980s, U.S. Navy and U.S. Coast Guard captains selected for promotion to the rank of rear admiral (lower half), would wear the same insignia as rear admiral (upper half), i.e., two silver stars for collar insignia or sleeve braid of one wide and one narrow gold stripe, even though they were actually only equivalent to one-star officers and paid at the one-star rate.
To correct this inequity, the rank of commodore as a single star flag officer was reinstated by both services in the early 1980s. This immediately caused confusion with those senior U.S. Navy captains commanding destroyer squadrons, submarine squadrons, functional air wings and air groups, and so on, who held the temporary "title" of commodore while in their major command billet. As a result of this confusion, the services soon renamed the new one-star rank as commodore admiral (CADM) within the first six months following the rank's reintroduction. However, this was considered an awkward title and the one-star flag rank was renamed a few months later to its current title of rear admiral (lower half), later abbreviated by the U.S. Navy and U.S. Coast Guard as RDML.
The "title" of commodore continues to be used in the U.S. Navy and Coast Guard for those senior captains in command of organizations consisting of groups of ships or submarines organized into squadrons; air wings or air groups of multiple aviation squadrons other than carrier air wings (the latter whose commanders still use the title "CAG"); explosive ordnance disposal (EOD), mine warfare and special warfare (SEAL) groups; and construction battalion (SeaBee) regiments. Although not flag officers, modern day commodores in the U.S. Navy rate a blue and white command pennant, also known as a broad pennant, that is normally flown at their headquarters facilities ashore or from ships that they are embarked aboard when they are the senior officer present afloat (SOPA).
Argentina.
In the Argentine Navy, the rank of commodore was created in the late 1990s, and is usually, but not always, issued to senior captains holding rear-admirals' positions. Its equivalent in the army is colonel-major, and has no equivalent in the air force. It is usually—but incorrectly—referred to as "navy commodore", to avoid confusion with the "air force commodore", which is equivalent to the navy's captain and army's colonel. The sleeve lace is identical to that of the Royal Navy, and wears one star on the shoulder strap.
Naval rank.
The following articles deal with the rank of commodore (or its equivalent) as it is employed in various countries.
Air force ranks.
Commodore, in Spanish "comodoro", is a rank in the Argentine Air Force. This rank is the equivalent of a colonel in the Argentine Army, and a colonel or group captain in other air forces of the world. The Argentine rank below commodore is the rank of vice-commodore (Spanish "vicecomodoro") equivalent to a lieutenant-colonel in the Argentine Army, and a lieutenant-colonel or wing commander in other air forces.
Commodore is a rank in the Royal Netherlands Air Force. It is a one-star rank and has essentially the same rank insignia as the British air commodore.
Many air forces, use the rank of air commodore. This rank was first used by the Royal Air Force and is now used in many countries such as Australia, Bangladesh, Greece, India, New Zealand, Nigeria, Pakistan, Thailand and Zimbabwe. It is the equivalent rank to the navy rank of "commodore", and the army ranks of brigadier and brigadier general.
The German air force used the concept of a unit commodore, although this was a unit command appointment rather than a rank.
Merchant and boating rank.
Commodore is also a title held by captains with high grade of navigation and seagoing seniority within a shipping companies and by the senior officers of many yacht clubs and boating associations. Commodores wears particular rank ensignia and particular cap's golden ensignia.
Convoy commodore.
During wartime, a shipping convoy will have a ranking officer—sometimes an active-duty naval officer, at other times a civilian master or retired naval officer—designated as the "convoy commodore". This title is not related to the individuals military rank (if any), but instead is the title of the senior individual responsible for the overall operation of the merchant ships and naval auxiliary ships that make up the convoy. The convoy commodore does not command the convoy escort forces (if any), which are commanded by a naval officer who serves as escort commander.
Other uses.
In the United States Coast Guard Auxiliary, the senior elected officer of the organization is the National Commodore, also there are commodores elected for the individual district commands of the Coast Guard Auxiliary. They hold the title of District Commodore. There are also appointed commodores who are the senior office holder for the areas within the organizational directorate.
In the Philippine Coast Guard Auxiliary—PCGA—each of the directors in command of the ten Coast Guard Auxiliary districts are commodores, as well as most of the Deputy National Directors (some may be rear admirals). Commodore is appreviated to COMMO in the PCGA.
Vanderbilt University's intercollegiate athletics teams are nicknamed the "Commodores", a reference to Cornelius Vanderbilt's self-appointed title (he was the master of a large shipping fleet).
In the U.S. Sea Scouting program (which is part of the Boy Scouts of America), all National, Regional, Area, and Council committee chairs are titled as commodore, while senior committee members are addressed as vice commodore. Ship committee chairs do not hold this recognition.

</doc>
<doc id="7583" url="http://en.wikipedia.org/wiki?curid=7583" title="Cauchy–Riemann equations">
Cauchy–Riemann equations

In the field of complex analysis in mathematics, the Cauchy–Riemann equations, named after Augustin Cauchy and Bernhard Riemann, consist of a system of two partial differential equations which, together with certain continuity and differentiability criteria, form a necessary and sufficient condition for a complex function to be complex differentiable, that is holomorphic. This system of equations first appeared in the work of Jean le Rond d'Alembert . Later, Leonhard Euler connected this system to the analytic functions . then used these equations to construct his theory of functions. Riemann's dissertation on the theory of functions appeared in 1851.
The Cauchy–Riemann equations on a pair of real-valued functions of two real variables u(x,y) and v(x,y) are the two equations:
Typically u and v are taken to be the real and imaginary parts respectively of a complex-valued function of a single complex variable z = x + iy, f(x + iy) = u(x,y) + iv(x,y). Suppose that u and v are real-differentiable at a point in an open subset of C ( C is the set of complex numbers), which can be considered as functions from R2 to R. This implies that the partial derivatives of u and v exist (although they need not be continuous) and we can approximate small variations of f linearly. Then f = u + iv is complex-differentiable at that point if and only if the partial derivatives of u and v satisfy the Cauchy–Riemann equations (1a) and (1b) at that point. The sole existence of partial derivatives satisfying the Cauchy–Riemann equations is not enough to ensure complex differentiability at that point. It is necessary that u and v be real differentiable, which is a stronger condition than the existence of the partial derivatives, but it is not necessary that these partial derivatives be continuous.
Holomorphy is the property of a complex function of being differentiable at every point of an open and connected subset of C (this is called a domain in C). Consequently, we can assert that a complex function f, whose real and imaginary parts u and v are real-differentiable functions, is holomorphic if and only if, equations (1a) and (1b) are satisfied throughout the domain we are dealing with.
The reason why Euler and some other authors relate the Cauchy–Riemann equations with analyticity is that a major theorem in complex analysis says that holomorphic functions are analytic and vice versa. This means that, in complex analysis, a function that is complex-differentiable in a whole domain (holomorphic) is the same as an analytic function. This is not true for real differentiable functions.
Interpretation and reformulation.
The equations are one way of looking at the condition on a function to be differentiable in the sense of complex analysis: in other words they encapsulate the notion of function of a complex variable by means of conventional differential calculus. In the theory there are several other major ways of looking at this notion, and the translation of the condition into other language is often needed.
Conformal mappings.
First, the Cauchy–Riemann equations may be written in complex form
In this form, the equations correspond structurally to the condition that the Jacobian matrix is of the form
where formula_5 and formula_6. A matrix of this form is the matrix representation of a complex number. Geometrically, such a matrix is always the composition of a rotation with a scaling, and in particular preserves angles. Consequently, a function satisfying the Cauchy–Riemann equations, with a nonzero derivative, preserves the angle between curves in the plane. That is, the Cauchy–Riemann equations are the conditions for a function to be conformal.
Complex differentiability.
Suppose that
is a function of a complex number "z". Then the complex derivative of "f" at a point "z"0 is defined by
provided this limit exists.
If this limit exists, then it may be computed by taking the limit as "h" → 0 along the real axis or imaginary axis; in either case it should give the same result. Approaching along the real axis, one finds
On the other hand, approaching along the imaginary axis,
The equality of the derivative of "f" taken along the two axes is
which are the Cauchy–Riemann equations (2) at the point "z"0.
Conversely, if "f" : C → C is a function which is differentiable when regarded as a function on R2, then "f" is complex differentiable if and only if the Cauchy–Riemann equations hold. In other words, if u and v are real-differentiable functions of two real variables, obviously "u" + "iv" is a (complex-valued) real-differentiable function, but "u" + "iv" is complex-differentiable if and only if the Cauchy–Riemann equations hold.
Indeed, following , suppose "f" is a complex function defined in an open set Ω ⊂ C. Then, writing for every "z" ∈ Ω, one can also regard Ω as an open subset of R2, and "f" as a function of two real variables "x" and "y", which maps Ω ⊂ R2 to C. We consider the Cauchy–Riemann equations at "z" = 0 assuming "f"("z") = 0, just for notational simplicity – the proof is identical in general case. So assume "f" is differentiable at 0, as a function of two real variables from Ω to C. This is equivalent to the existence of two complex numbers α and β (which are the partial derivatives of "f") such that we have the linear approximation
where "z" = "x" + "iy" and η("z") → 0 as "z" → "z"0 = 0. Since formula_13 and formula_14, the above can be re-written as
Defining the two Wirtinger derivatives as
the above equality can be written as
For real values of "z", we have formula_18 and for purely imaginary "z" we have formula_19 hence "f"("z")/"z" has a limit at 0 ("i.e.", "f" is complex differentiable at 0) if and only if formula_20. But this is exactly the Cauchy–Riemann equations, thus "f" is differentiable at 0 if and only if the Cauchy–Riemann equations hold at 0.
Independence of the complex conjugate.
The above proof suggests another interpretation of the Cauchy–Riemann equations. The complex conjugate of "z", denoted formula_21, is defined by
for real "x" and "y". The Cauchy–Riemann equations can then be written as a single equation
by using the Wirtinger derivative with respect to the conjugate variable. In this form, the Cauchy–Riemann equations can be interpreted as the statement that "f" is independent of the variable formula_21. As such, we can view analytic functions as true functions of "one" complex variable as opposed to complex functions of "two" real variables.
Physical interpretation.
One interpretation of the Cauchy–Riemann equations does not involve complex variables directly. Suppose that "u" and "v" satisfy the Cauchy–Riemann equations in an open subset of R2, and consider the vector field
regarded as a (real) two-component vector. Then the second Cauchy–Riemann equation (1b) asserts that formula_26 is irrotational (its curl is 0):
The first Cauchy–Riemann equation (1a) asserts that the vector field is solenoidal (or divergence-free):
Owing respectively to Green's theorem and the divergence theorem, such a field is necessarily a conservative one, and it is free from sources or sinks, having net flux equal to zero through any open domain without holes. (These two observations combine as real and imaginary parts in Cauchy's integral theorem.) In fluid dynamics, such a vector field is a potential flow . In magnetostatics, such vector fields model static magnetic fields on a region of the plane containing no current. In electrostatics, they model static electric fields in a region of the plane containing no electric charge.
Other representations.
Other representations of the Cauchy–Riemann equations occasionally arise in other coordinate systems. If (1a) and (1b) hold for a differentiable pair of functions "u" and "v", then so do
for any coordinate system such that the pair (∇"n", ∇"s") is orthonormal and positively oriented. As a consequence, in particular, in the system of coordinates given by the polar representation , the equations then take the form
Combining these into one equation for "f" gives
The inhomogeneous Cauchy–Riemann equations consist of the two equations for a pair of unknown functions "u"("x","y") and "v"("x","y") of two real variables
for some given functions α("x","y") and β("x","y") defined in an open subset of R2. These equations are usually combined into a single equation
where "f" = "u" + i"v" and φ = (α + iβ)/2.
If φ is "C""k", then the inhomogeneous equation is explicitly solvable in any bounded domain "D", provided φ is continuous on the closure of "D". Indeed, by the Cauchy integral formula,
for all ζ ∈ "D".
Generalizations.
Goursat's theorem and its generalizations.
Suppose that is a complex-valued function which is differentiable as a function . Then Goursat's theorem asserts that "f" is analytic in an open complex domain Ω if and only if it satisfies the Cauchy–Riemann equation in the domain . In particular, continuous differentiability of "f" need not be assumed .
The hypotheses of Goursat's theorem can be weakened significantly. If is continuous in an open set Ω and the partial derivatives of "f" with respect to "x" and "y" exist in Ω, and satisfies the Cauchy–Riemann equations throughout Ω, then "f" is holomorphic (and thus analytic). This result is the Looman–Menchoff theorem.
The hypothesis that "f" obey the Cauchy–Riemann equations throughout the domain Ω is essential. It is possible to construct a continuous function satisfying the Cauchy–Riemann equations at a point, but which is not analytic at the point (e.g., "f"("z") = . Similarly, some additional assumption is needed besides the Cauchy–Riemann equations (such as continuity), as the following example illustrates 
which satisfies the Cauchy–Riemann equations everywhere, but fails to be continuous at "z" = 0.
Nevertheless, if a function satisfies the Cauchy–Riemann equations in an open set in a weak sense, then the function is analytic. More precisely :
This is in fact a special case of a more general result on the regularity of solutions of hypoelliptic partial differential equations.
Several variables.
There are Cauchy–Riemann equations, appropriately generalized, in the theory of several complex variables. They form a significant overdetermined system of PDEs. As often formulated, the "d-bar operator" 
annihilates holomorphic functions. This generalizes most directly the formulation
where
Bäcklund transform.
Viewed as conjugate harmonic functions, the Cauchy–Riemann equations are a simple example of a Bäcklund transform. More complicated, generally non-linear Bäcklund transforms, such as in the sine-Gordon equation, are of great interest in the theory of solitons and integrable systems.

</doc>
<doc id="7585" url="http://en.wikipedia.org/wiki?curid=7585" title="Chaim Topol">
Chaim Topol

Chaim Topol (; born September 9, 1935), often billed simply as Topol, is an Israeli theatrical and film performer, singer, actor, writer and producer. He has been nominated for an Academy Award and a Tony Award, and has won two Golden Globe Awards.
Early life.
Topol was born in Tel Aviv in 1935 in what was then the British Mandate of Palestine, to Rel (née Goldman) and Jacob Topol. He first practised acting in amateur theatrical plays staged by the Israeli Army. Subsequently he established his own theatre troupe in Tel Aviv, and in 1961 he significantly contributed to the foundation of the Haifa Municipal Theatre.
Acting career.
Among Topol's earliest film appearances was the lead role in the 1964 film "Sallah Shabati" by Ephraim Kishon — a play, later adapted for film, depicting the hardships of a Mizrahi Jewish immigrant family in Israel of the early 1960s. The film was nominated for the Academy Award for Best Foreign Language Film and earned the actor the Golden Globe Award for New Star of the Year - Actor. In 1966, Topol made his first English-language screen appearance as Abou Ibn Kaqden in the big-budget Mickey Marcus biopic "Cast a Giant Shadow".
He came to greatest prominence in the role of Tevye the milkman in the long-running musical show "Fiddler on the Roof", at Her Majesty's Theatre. After a major success on the West End stage, he later starred in the 1971 film version.
In 1972, Topol won a Golden Globe Award and was nominated for a Best Actor Oscar for his performance in the film. He was on active service with the Israeli Army at the time, but was granted permission to attend the awards ceremonies.
In 1976, Topol originated the leading role of the baker, Amiable, in the new musical "The Baker's Wife", but was fired after eight months by producer David Merrick. In her autobiography, "Patti LuPone: A Memoir", his co-star in the production relayed that Topol behaved unprofessionally in front of paying audiences, sometimes speaking gibberish instead of his lines, and other times responding to the director's instructions by grossly overacting on purpose. Her account was echoed by the show's composer, Stephen Schwartz, in the book "Defying Gravity: The Creative Career of Stephen Schwartz, From Godspell to Wicked", in which he claimed that Topol's behavior greatly disturbed the cast and directors and resulted in the production not reaching Broadway as planned.
Some of Topol's other notable film appearances were the title role in "Galileo" (1975), Dr. Hans Zarkov in "Flash Gordon" (1980), and as Milos Columbo in the James Bond movie "For Your Eyes Only" (1981).
In 1983, he reprised the role of Tevye in a London revival of "Fiddler on the Roof". In the late 1980s, he played the role in a touring United States production. He was by then the approximate age of the character. Also, the actress playing his wife, Golde, in that production—Rosalind Harris—had played his eldest daughter, Tzeitel, in the film. In 1990, he again played the part in a Broadway revival of "Fiddler", and was nominated in 1991 for a Tony Award for Best Performance by a Leading Actor in a Musical, losing to Jonathan Pryce. He played the part again in a 1994 London revival, which became a touring production. He has since played the part in various productions including stages in Europe, Australia and Japan. His most recent film roles were in "Left Luggage" (1998) in the role of Mr. Apfelschnitt, and "Time Elevator" (1998) as Shalem.
In November 2005, Topol had a two-month season once again playing Tevye in "Fiddler On the Roof" at Capitol Theatre in Sydney, Australia and in April 2007, played the role in Wellington, New Zealand. In September 2008, Topol played the part of Honore in "Gigi" at the Open Air Theatre in Regent's Park, London.
On January 20, 2009, Topol began a farewell tour of "Fiddler on the Roof" as Tevye, opening in Wilmington, Delaware, USA. He was forced to withdraw from the tour owing to a shoulder injury, and made his last appearance as Tevye in Boston, Massachusetts on November 15, 2009. (Theodore Bikel and Harvey Fierstein, both of whom have portrayed Tevye on Broadway, replaced him in scheduled appearances.) 
Author.
His autobiography, "Chaim Topol on Topol", was published in London and Israel.
Topol is also an illustrator, responsible for drawings in several books, including "A Treasury of Jewish Humour".
Charitable work.
Topol serves as chairman of the board of Jordan River Village.
Filmography.
Source: 

</doc>
<doc id="7586" url="http://en.wikipedia.org/wiki?curid=7586" title="Christadelphians">
Christadelphians

The Christadelphians are a millenarian Christian group who hold a view of Biblical Unitarianism. The movement developed in the United Kingdom and North America in the 19th century around the teachings of John Thomas, who coined the name "Christadelphian" from the Greek for "Brethren in Christ".
Basing their beliefs solely on the Bible, Christadelphians differ from mainstream Christianity in a number of doctrinal areas. For example, they reject the Trinity and the immortality of the soul, believing these to be corruptions of original Christian teaching.
They were initially found predominantly in the developed English-speaking world, but expanded in developing countries after the Second World War. There are estimated to be 60,000 Christadelphians in around 120 countries worldwide. Congregations are usually referred to as "ecclesias".
History.
19th century.
The Christadelphian religious group traces its origins to Dr John Thomas (1805–1871), who migrated to North America from England in 1832. Following a near shipwreck he vowed to find out the truth about life and God through personal Biblical study. Initially he sought to avoid the kind of sectarianism he had seen in England. In this he found sympathy with the rapidly emerging Restoration Movement in the United States of America at the time. This movement sought for a reform based upon the Bible alone as a sufficient guide and rejected all creeds. However this liberality eventually led to dissent as John Thomas developed in his personal beliefs and started to question mainstream orthodox Christian beliefs. Whilst the Restoration Movement accepted Thomas's right to have his own beliefs, when he started preaching that they were essential to salvation, it led to a fierce series of debates with a notable leader of the movement, Alexander Campbell. John Thomas believed that scripture, as God's word, did not support a multiplicity of differing beliefs, and challenged the leaders to continue with the process of restoring 1st-century Christian beliefs and correct interpretation through a process of debate. The history of this process appears in the book "Dr. Thomas, His Life and Work" (1873) by a Christadelphian, Robert Roberts.
During this period of formulating his ideas John Thomas was baptised twice, the second time after renouncing the beliefs he previously held. He based his new position on a new appreciation for the reign of Christ on David's throne. The abjuration of his former beliefs eventually led to the Restoration Movement disfellowshipping him when he toured England and they became aware of his abjuration in the United States of America.
The Christadelphian community in Britain effectively dates from Thomas's first lecturing tour (May 1848 – October 1850). His message was particularly welcomed in Scotland, and Campbellite, Unitarian and Adventist friends separated to form groups of "Baptised Believers". Two thirds of ecclesias, and members, in Britain before 1864 were in Scotland. In 1849, during his tour of Britain, he completed (a decade and a half before the name "Christadelphian" was conceived) "Elpis Israel" in which he laid out his understanding of the main doctrines of the Bible. Since his medium for bringing change was print and debate, it was natural for the origins of the Christadelphian body to be associated with books and journals, such as Thomas's "Herald of the Kingdom".
In his desire to seek to establish Biblical truth and test out orthodox Christian beliefs through independent scriptural study he was not alone and, amongst other churches, he also had links with Adventist movement and with Benjamin Wilson (who later set up the Church of God of the Abrahamic Faith in the 1860s).
Although the Christadelphian movement originated through the activities of John Thomas, he never saw himself as making his own disciples. He believed rather that he had rediscovered 1st-century beliefs from the Bible alone, and sought to prove that through a process of challenge and debate and writing journals. Through that process a number of people became convinced and set up various fellowships that had sympathy with that position. Groups associated with John Thomas met under various names, including Believers, Baptised Believers, the Royal Association of Believers, Baptised Believers in the Kingdom of God, Nazarines (or Nazarenes) and The Antipas until the time of the American Civil War (1861–1865). At that time, church affiliation was required in the United States and in the Confederacy in order to register for conscientious objector status, and in 1864 Thomas chose for registration purposes the name "Christadelphian".
Through the teaching of John Thomas and the need in the American Civil War for a name, the Christadelphians emerged as a denomination, but they were formed into a lasting structure through a passionate follower of Thomas's interpretation of the Bible, Robert Roberts. In 1864 he began to publish "The Ambassador of the Coming Age" magazine. This was renamed "The Christadelphian" in 1869 and continues to be published under that name. Roberts was prominent in the period following the death of John Thomas in 1871, and helped craft the structures of the Christadelphian body.
Robert Roberts was certain that John Thomas had rediscovered the truth. Robert Robert's life was characterised by debates over issues that arose within the fledgling organisation; some of these debates can be found in the book "Robert Roberts—A study of his life and character" by Islip Collyer.
Initially the denomination grew in the English-speaking world, particularly in the English Midlands and in parts of North America. In the early days after the death of John Thomas the group could have moved in a number of directions. Doctrinal issues arose, debates took place and statements of faith were created and amended as other issues arose. These attempts were felt necessary by many to both settle and define a doctrinal stance for the newly emerging denomination and to keep out error. As a result of these debates, several groups separated from the main body of Christadelphians, most notably the Suffolk Street fellowship and the Unamended fellowship.
20th century.
The Christadelphian position on conscientious objection came to the fore with the introduction of conscription during the First World War. Varying degrees of exemption from military service were granted to Christadelphians in the United Kingdom, Canada, Australia, New Zealand and the United States. In the Second World War, this frequently required the person seeking exemption to undertake civilian work under the direction of the authorities.
During the Second World War the Christadelphians in Britain assisted in the Kindertransport, helping to relocate several hundred Jewish children away from Nazi persecution and founding a hostel Elpis Lodge. In Germany the small Christadelphian community founded by Albert Maier went underground from 1940–1945, and a leading brother, Albert Merz, was imprisoned as a conscientious objector and later executed.
After the Second World War, moves were taken to try to reunite various of the earlier divisions. By the end of the 1950s, most Christadelphians had united into one community, but there are still a number of small groups who remain separate.
Today.
The post-war, and post-reunions, period saw an increase in co-operation and interaction between ecclesias, resulting in the establishment of a number of week-long Bible schools and the formation of national and international organisations such as the Christadelphian Bible Mission (for preaching and pastoral support overseas), the Christadelphian Support Network (for counselling), and the Christadelphian Meal-A-Day Fund (for charity and humanitarian work).
The period following the reunions was accompanied by expansion in the developing world, which now accounts for around 40% of Christadelphians.
Organisation.
General organisation.
In the absence of centralised organisation, some differences exist amongst Christadelphians on matters of belief and practice. This is because each congregation (commonly styled 'ecclesias') is organised autonomously, typically following common practices which have altered little since the 19th century. Most ecclesias have a constitution, which includes a 'Statement of Faith', a list of 'Doctrines to be Rejected' and a formalized list of 'The Commandments of Christ'. With no central authority individual congregations are responsible for maintaining orthodoxy in belief and practice, and the statement of faith is seen by many as useful to this end. The statement of faith acts as the official standard of most ecclesias to determine fellowship within and between ecclesias, and as the basis for co-operation between ecclesias. Congregational discipline and conflict resolution are applied using various forms of consultation, mediation, and discussion, with disfellowship (similar to excommunication) being the final response to those with unorthodox practices or beliefs.
The relative uniformity of organisation and practice is undoubtedly due to the influence of a booklet, written early in Christadelphian history by Robert Roberts, called "A Guide to the Formation and Conduct of Christadelphian Ecclesias". It recommends a basically democratic arrangement by which congregational members elect 'brothers' to arranging and serving duties, and includes guidelines for the organisation of committees, as well as conflict resolution between congregational members and between congregations. Christadelphians do not have paid ministers. Male members are assessed by the congregation for their eligibility to teach and perform other duties, which are usually assigned on a rotation basis, as opposed to having a permanently appointed preacher. Congregational governance typically follows a democratic model, with an elected arranging committee for each individual ecclesia. This unpaid committee is responsible for the day-to-day running of the ecclesia and is answerable to the rest of the ecclesia's members.
Inter-ecclesial organisations co-ordinate the running of, among other things, Christadelphian schools and elderly care homes, the Christadelphian Isolation League (which cares for those prevented by distance or infirmity from attending an ecclesia regularly) and the publication of .
Adherents.
No official membership figures are published, but the Columbia Encyclopedia gives an estimated figure of 50,000 Christadelphians. They are spread across approximately 120 countries; there are established churches (often referred to as "ecclesias") in many of those countries, along with isolated members. Estimates for the main centers of Christadelphian population are as follows: United Kingdom (18,000), Australia (10,653), Mozambique (7,500), Malawi (7,000), United States (6,500), Canada (3,375), New Zealand (1,785), India (1,750), Kenya (1,700), Tanzania (1,000). and Pakistan (900). Combining the estimates from the Christadelphian Bible Mission with the figures above, the numbers for each continent are as follows: Africa (21,400), Americas (10,500), Asia (4,150), Australasia (12,600), Europe (18,950). This puts the total figure at around 67,000.
Fellowships.
The Christadelphian body consists of a number of "fellowships" - groups of ecclesias which associate with one another, often to the exclusion of ecclesias outside their group. They are to some degree localised. The Unamended Fellowship, for example, exists only in North America. Christadelphian fellowships have often been named after ecclesias or magazines who took a lead in developing a particular stance.
The majority of Christadelphians (around 60,000) belong to "Central fellowship", named after the Birmingham Central ecclesia. This was formed in 1957–1958 as a result of a reunion between the Temperance Hall and Suffolk Street fellowships in the UK. The "Suffolk Street fellowship" had formed in 1885 over the inspiration of the Bible. Robert Ashcroft, a leading member, wrote an article which challenged Christadelphian belief in inspiration and which, although he himself left, led to a division in the main body. One group formed a new ecclesia which later met in Suffolk Street, Birmingham. Other ecclesias throughout the world which supported them became known as the "Suffolk Street fellowship" to distinguish them from the group they had separated from, which became known as the "Temperance Hall fellowship". The main magazine of this group from 1884–1957 was "The Fraternal Visitor", whose editors included J.J. Bishop and J.J. Hadley (d. 1912), then Thomas Turner, and finally Cyril Cooper (till reunion in 1957). The Temperance Hall-Suffolk Street reunion was closely followed by union in 1958 with the Shield fellowship (allied to the Suffolk Street fellowship) through an understanding expressed in a document called the Cooper-Carter Addendum. The Central fellowship in North America is often referred to as the "Amended fellowship".
The "Unamended fellowship", consisting of around 1,850 members, is found in East Coast and Midwest USA and Ontario, Canada. This group separated in 1898 as a result of differing views on who would be raised to judgment at the return of Christ. The majority of Christadelphians believe that the judgment will include anyone who had sufficient knowledge of the gospel message, and is not limited to baptized believers. The majority in Britain, Australia and North America amended their statement of faith accordingly. Those who opposed the amendment became known as the "Unamended fellowship" and allowed the teaching that God either could not or would not raise those who had no covenant relationship with him. Opinions vary as to what the established position was on this subject prior to the controversy. Prominent in the formation of the Unamended fellowship was Thomas Williams, editor of the Christadelphian Advocate magazine. The majority of the Unamended Fellowship outside North America joined the Suffolk Street fellowship before its eventual incorporation into Central fellowship. There is also some co-operation between the Central (Amended) and Unamended Fellowships in North America – most recently in the Great Lakes region, where numerous Amended and Unamended ecclesias have opened fellowship to one another despite the failure of wider attempts at re-union under the North American Statement of Understanding (NASU).
The "Berean Fellowship" was formed in 1923 as a result of varying views on military service in Britain, and on the atonement in North America. The majority of the North American Bereans re-joined the main body of Christadelphians in 1952. A number continue as a separate community, numbering around 200 in Texas, 100 in Kenya and 30 in Wales. Most of the divisions still in existence within the Christadelphian community today stem from further divisions of the Berean fellowship.
In 1942 the Berean fellowship divided over marriage and divorce with the stricter party forming the "Dawn fellowship". Following union with the Lightstand fellowship in Australia in November 2007, there are now 800 members in the UK, Australia, Canada, India, Jamaica, Poland, the Philippines and Russia.
The "Old Paths fellowship" was formed in the 1957 by those in the Temperance Hall fellowship who held that the reasons for separation from the Suffolk Street fellowship remained and opposed the re-union. There are around 250 members in the UK, and 150 in Australasia.
Other small groups, numbering no more than 300 members in total, include the "Watchman fellowship", the "Companion fellowship" and the "Pioneer Fellowship".
The Church of God of the Abrahamic Faith (CGAF) has common origins with Christadelphians and shares Christadelphian beliefs. Numbering around 400 (primarily Ohio and Florida, USA), they are welcomed into fellowship by some Christadelphians and are currently involved in unity talks.
According to Bryan Wilson, functionally the definition of a "fellowship" within Christadelphian history has been mutual or unilateral exclusion of groupings of ecclesias from the breaking of bread. This functional definition still holds true in North America, where the Unamended fellowship and the Church of God of the Abrahamic Faith are not received by most North American Amended ecclesias. But outside North America this functional definition no longer holds. Many articles and books on the doctrine and practice of fellowship now reject the notion itself of separate "fellowships" among those who recognise the same baptism, viewing such separations as schismatic. Many ecclesias in the Central fellowship would not refuse a baptised Christadelphian from a minority fellowship from breaking bread; the exclusion is more usually the other way.
They tend to operate organisationally fairly similarly, although there are different emphases. Despite their differences, the Central, Old Paths, Dawn and Berean fellowships generally subscribe to the "Birmingham Amended Statement of Faith" (BASF), though the latter two have additional clauses or supporting documents to explain their position. Most Unamended ecclesias use the "Birmingham Unamended Statement of Faith" (BUSF) with one clause being different. Within the Central fellowship individual ecclesias also may have their own statement of faith, whilst still accepting the statement of faith of the larger community. Some ecclesias have statements around their positions, especially on divorce and re-marriage, making clear that offence would be caused by anyone in that position seeking to join them at the 'Breaking of Bread' service. Others tolerate a degree of divergence from commonly held Christadelphian views.
For each fellowship, anyone who publicly assents to the doctrines described in the statement and is in good standing in their "home ecclesia" is generally welcome to participate in the activities of any other ecclesia.
Beliefs.
Due to the way the Christadelphian body is organised there is no central authority to establish and maintain a standardised set of beliefs and it depends what statement of faith is adhered to and how liberal the ecclesia is, but there are core doctrines most Christadelphians would accept. In the formal statements of faith a more complete list is found. For instance in the Central fellowship, the BASF, the standard statement of faith, has 30 doctrines to be accepted and 35 to be rejected.
The Bible.
Christadelphians state that their beliefs are based wholly on the Bible, and they do not see other works as inspired by God. They regard the Bible as inspired by God and, therefore, believe that, in its original form, it was error free (errors in later copies are thought to be due to 'errors of transcription or translation'). Based on this, Christadelphians teach what they believe to be true Bible teaching.
God.
Christadelphians believe that God is the creator of all things and the father of true believers, that he is a separate being from his son, Jesus Christ, and that the Holy Spirit is the power of God used in creation and for salvation. They also believe that the phrase "Holy Spirit" sometimes refers to God's character/mind, depending on the context in which the phrase appears, but reject the orthodox Christian view that we need strength, guidance and power from the Holy Spirit to live the Christian life, believing instead that the spirit a believer needs within themselves is the mind/character of God, which is developed in a believer by their reading of the Bible (which, they believe, contains words God gave by his Spirit) and trying to live by what it says during the events of their lives which God uses to help shape their character.
Jesus.
Christadelphians believe that Jesus is the promised Jewish Messiah, in whom the prophecies and promises of the Old Testament find their fulfilment. They believe he is the Son of Man, in that he inherited human nature (with its inclination to sin) from his mother, and the Son of God by virtue of his miraculous conception by the power of God. Although he was tempted, Jesus committed no sin, and was therefore a perfect representative sacrifice to bring salvation to sinful humankind. They believe that God raised Jesus from death and gave him immortality, and he ascended to Heaven, God's dwelling place. Christadelphians believe that he will return to the earth in person to set up the Kingdom of God in fulfilment of the promises made to Abraham and David. This includes the belief that the coming Kingdom will be the restoration of God's first Kingdom of Israel, which was under David and Solomon. For Christadelphians, this is the focal point of the gospel taught by Jesus and the apostles.
Salvation.
Christadelphians believe that people are separated from God because of their sins but that mankind can be reconciled to him by becoming disciples of Jesus Christ. This is by belief in the gospel, through repentance, and through baptism by total immersion in water. They do not believe we can be sure of being saved, believing instead that salvation comes as a result of a life of obedience to the commands of Christ After death, believers are in a state of non-existence, knowing nothing until the Resurrection at the return of Christ. Following the judgement at that time, the accepted receive the gift of immortality, and live with Christ on a restored Earth, assisting him to establish the Kingdom of God and to rule over the mortal population for a thousand years (the Millennium). Christadelphians believe that the Kingdom will be centred upon Israel, but Jesus Christ will also reign over all the other nations on the earth. Some believe that the Kingdom itself is not worldwide but limited to the land of Israel promised to Abraham and ruled over in the past by David, with a worldwide empire.
Life in Christ.
The historic "" demonstrates the community's recognition of the importance of Biblical teaching on morality. Marriage and family life are important. Christadelphians believe that sexual relationships are limited to heterosexual marriage, ideally between baptised believers.
Similarities and differences with other Christians.
Disagreement with some mainstream doctrines.
Christadelphians reject a number of doctrines held by many other Christians, notably the immortality of the soul (see also mortalism; conditionalism), trinitarianism, the personal pre-existence of Christ, the baptism of infants, the personhood of the Holy Spirit the divinity of Jesus and the present-day possession of the gifts of the Holy Spirit (see cessationism). They believe that the word "devil" is a reference in the scriptures to sin and human nature in opposition to God, while the word "satan" is merely a reference to an adversary (be it good or bad). According to Christadelphians, these terms are used in reference to specific political systems or individuals in opposition or conflict. "Hell" (Hebrew: Sheol; Greek: Hades, Gehenna) is understood to refer exclusively to death and the grave, rather than being a place of everlasting torment (see also annihilationism). Christadelphians do not believe that anyone will "go to Heaven" upon death. Instead, they believe that only Christ Jesus went to Heaven, and when he comes back to the earth there will be a resurrection and God's kingdom will be established on earth, starting in the land of Israel. Christadelphians believe the doctrines they reject were introduced into Christendom after the 1st century in large part through exposure to pagan Greek philosophy, and cannot be substantiated from the Biblical texts.
Other historical groups and individuals with some shared doctrines.
One criticism of the Christadelphian movement has been over the claim of John Thomas and Robert Roberts to have "rediscovered" scriptural truth. However, although both men believed that they had "recovered" the true doctrines for themselves and contemporaries, they also believed there had always existed a group of true believers throughout the ages, albeit marred by the apostasy.
The most notable Christadelphian attempts to find a continuity of those with doctrinal similarities since that point have been geographer Alan Eyre's two books "The Protesters" (1975) and "Brethren in Christ" (1982) in which he shows that many individual Christadelphian doctrines had been previously believed. Eyre focused in particular on the Radical Reformation, and also among the Socinians and other early Unitarians and the English Dissenters. In this way, Eyre was able to demonstrate substantial historical precedents for individual Christadelphian teachings and practices, and believed that the Christadelphian community was the 'inheritor of a noble tradition, by which elements of the Truth were from century to century hammered out on the anvil of controversy, affliction and even anguish'. Although noting in the introduction to 'The Protestors' that 'Some recorded herein perhaps did not have "all the truth" — so the writer has been reminded', Eyre nevertheless claimed that the purpose of the work was to 'tell how a number of little-known individuals, groups and religious communities strove to preserve or revive the original Christianity of apostolic times', and that 'In faith and outlook they were far closer to the early springing shoots of 1st-century Christianity and the penetrating spiritual challenge of Jesus himself than much that has passed for the religion of the Nazarene in the last nineteen centuries'.
Eyre's research has been criticized by some of his Christadelphian peers, and as a result Christadelphian commentary on the subject has subsequently been more cautious and circumspect, with caveats being issued concerning Eyre's claims, and the two books less used and publicized than in previous years.
Nevertheless, even with most source writings of those later considered "heretics" destroyed, evidence can be provided that since the 1st century CE there have been various groups and individuals who have held certain individual Christadelphian beliefs or similar ones. For example, all the distinctive Christadelphian doctrines (with the exception of the non-literal devil), down to interpretations of specific verses, can be found particularly among 16th century Socinian writers (e.g. the rejection of the doctrines of the trinity, pre-existence of Christ, immortal souls, a literal hell of fire, original sin). Early English Unitarian writings also correspond closely to those of Christadelphians. Also, recent discoveries and research have shown a large similarity between Christadelphian beliefs and those held by Isaac Newton who, among other things, rejected the doctrines of the trinity, immortal souls, a personal devil and literal demons. Further examples are as follows:
Organised worship in England for those whose beliefs anticipated those of Christadelphians only truly became possible in 1779 when the Act of Toleration 1689 was amended to permit denial of the Trinity, and only fully when property penalties were removed in the Doctrine of the Trinity Act 1813. This is only 35 years before John Thomas' 1849 lecture tour in Britain which attracted significant support from an existing non-Trinitarian Adventist base, particularly, initially, in Scotland where Arian Socinian and unitarian (with a small 'u' as distinct from the Unitarian Church of Theophilus Lindsey) views were prevalent.
Modern mainstream theology developing similar beliefs.
While Christadelphians have beliefs that are different from the majority of other Christian denominations, over the last 100 years some mainstream Christian theologians and Biblical scholars have gradually been developing beliefs which the Christadelphian community has historically held. Example areas are the atonement; justification; heaven and hell; the state of the dead.
Practices and worship.
Christadelphians are organised into local congregations, that commonly call themselves "ecclesias", which is taken from usage in the New Testament and is Greek for "gathering of those summoned". Congregational worship, which usually takes place on Sunday, centres on the remembrance of the death and celebration of the resurrection of Jesus Christ by the taking part in the "memorial service". Additional meetings are often organised for worship, prayer, preaching and Bible study.
Ecclesias are typically involved in preaching the gospel (evangelism) in the form of public lectures on Bible teaching, college-style seminars on reading the Bible, and Bible Reading Groups. Correspondence courses are also used widely, particularly in areas where there is no established Christadelphian presence. Some ecclesias, organisations or individuals also preach through other media like video, podcasts and internet forums. There are also a number of Bible Education/Learning Centres around the world.
Only baptised (by complete immersion in water) believers are considered members of the ecclesia. Ordinarily, baptism follows someone making a "good confession" (cf. 1 Tim. 6:12) of their faith before two or three nominated elders of the ecclesia they are seeking to join. The good confession has to demonstrate a basic understanding of the main elements - "first principles" - of the faith of the community. The children of members are encouraged to attend Christadelphian Sunday Schools and youth groups. Interaction between youth from different ecclesias is encouraged through regional and national youth gatherings. Many ecclesias organise holidays for young people, the most popular form in the UK being camping holidays and Youth Weekends such as Swanwick and others locally organised by different ecclesias.
Christadelphians understand the Bible to teach that male and female believers are equal in God's sight, and also that there is a distinction between the roles of male and female members. Women are typically not eligible to teach in formal gatherings of the ecclesia when male believers are present, are expected to cover their heads (using hat or scarf, etc.) during formal services, and do not sit on the main ecclesial arranging (organising) committees. They do, however: participate in other ecclesial and inter-ecclesial committees; participate in discussions; teach children in Sunday Schools as well as at home, teach other women and non-members; perform music; discuss and vote on business matters; and engage in the majority of other activities. Generally, at formal ecclesial and inter-ecclesial meetings the women wear head coverings when there are acts of worship and prayer.
There are ecclesially-accountable committees for co-ordinated preaching, youth and Sunday School work, conscientious objection issues, care of the elderly, and humanitarian work. These do not have any legislative authority, and are wholly dependent upon ecclesial support. Ecclesias in an area may regularly hold joint activities combining youth groups, fellowship, preaching, and Bible study.
Christadelphians refuse to participate in any military (and Police forces) because they are conscientious objectors.
Christadelphians do not vote in political elections, as they take direction from Romans 13:1-4, which they interpret as meaning that God puts into power those leaders He deems worthy. To vote for a candidate that does not win an election would be considered to vote against God's will. To avoid the risk of such conflict, Christadelphians abstain from voting.
There is a strong emphasis on personal Bible reading and study and many Christadelphians use the Bible Companion to help them systematically read the Bible each year.
Hymnody and music.
Christadelphians are a non-liturgical denomination. Christadelphian ecclesias are autonomous and free to adopt whatever pattern of worship they choose. However, in the English-speaking world, there tends to be a great deal of uniformity in order of service and hymnody.
Christadelphian hymnody makes considerable use of the hymns of the Anglican and British Protestant traditions (even in US ecclesias the hymnody is typically more British than American). In many Christadelphian hymn books a sizeable proportion of hymns are drawn from the Scottish Psalter and non-Christadelphian hymn-writers including Isaac Watts, Charles Wesley, William Cowper and John Newton. Despite incorporating non-Christadelphian hymns however, Christadelphian hymnody preserves the essential teachings of the community.
The earliest hymn book published was the "Sacred Melodist" which was published by Benjamin Wilson in Geneva, Illinois in 1860. The next was the hymn book published for the use of "Baptised Believers in the Kingdom of God" (an early name for Christadelphians) by George Dowie in Edinburgh in 1864. In 1865 Robert Roberts published a collection of Scottish psalms and hymns called "The Golden Harp" (which was subtitled "Psalms, Hymns, and Spiritual Songs, compiled for the use of Immersed Believers in 'The Things concerning the Kingdom of God and the Name of Jesus Christ'"). This was replaced only five years later by the first "Christadelphian Hymn Book" (1869), compiled by J. J. and A. Andrew, and this was revised and expanded in 1874, 1932 and 1964. A thorough revision by the Christadelphian Magazine and Publishing Association resulted in the latest (2002) edition which is almost universally used by English-speaking Christadelphian ecclesias. In addition some Christadelphian fellowships have published their own hymn books.
Some ecclesias use the "Praise the Lord" songbook. It was produced with the aim of making contemporary songs which are consistent with Christadelphian theology more widely available. Another publication, the "Worship" book is a compilation of songs and hymns that have been composed only by members of the Christadelphian community. This book was produced with the aim of providing extra music for non-congregational music items within services (e.g. voluntaries, meditations, etc) but has been adopted by congregations worldwide and is now used to supplement congregational repertoire.
In the English-speaking world, worship is typically accompanied by organ or piano, though in recent years a few ecclesias have promoted the use of other instruments (e.g. strings, wind and brass as mentioned in the Psalms). This trend has also seen the emergence of some Christadelphian bands and the establishment of the Christadelphian Art Trust to support performing, visual and dramatic arts within the Christadelphian community.
In other countries, hymnbooks have been produced in local languages, sometimes resulting in styles of worship which reflect the local culture. It has been noted that Christadelphian hymnody has historically been a consistent witness to Christadelphian beliefs, and that hymnody occupies a significant role in the community.

</doc>
<doc id="7587" url="http://en.wikipedia.org/wiki?curid=7587" title="Cable television">
Cable television

Cable television is a system of broadcasting television programming to paying subscribers via radio frequency (RF) signals transmitted through coaxial cables or light pulses through fiber-optic cables. This contrasts with traditional terrestrial television, in which the television signal is transmitted over the air by radio waves and received by a television antenna attached to the television. FM radio programming, high-speed Internet, telephone service, and similar non-television services may also be provided through these cables.
The abbreviation CATV is often used for cable television. It originally stood for "Community Access Television" or "Community Antenna Television", from cable television's origins in 1948: in areas where over-the-air reception was limited by distance from transmitters or mountainous terrain, large "community antennas" were constructed, and cable was run from them to individual homes. The origins of cable "broadcasting" are even older as radio programming was distributed by cable in some European cities as far back as 1924.
Distribution.
In order to receive cable television at a given location, cable distribution lines must be available on the local utility poles or underground utility lines. Coaxial cable brings the signal to the customer's building through a "service drop", an overhead or underground cable. If the subscriber's building does not have a cable service drop, the cable company will install one. The standard cable used in the U.S. is RG-6, which has a 75 ohm impedance, and connects with a type F connector.
The cable company's portion of the wiring usually ends at a distribution box on the building exterior, and built-in cable wiring in the walls usually distributes the signal to jacks in different rooms to which televisions are connected. Multiple cables to different rooms are split off the incoming cable with a small device called a splitter.
There are two standards for cable television; older analog cable, and newer digital cable which is capable of carrying high definition signals used by newer digital HDTV televisions. Many cable companies have upgraded to digital cable infrastructures since it was first introduced in the late 1990s. To receive digital cable, most television sets require a digital television adapter (set-top box or cable converter box) supplied by the cable provider. A cable from the jack in the wall is attached to the input of the box, and an output cable from the box is attached to the "Antenna In" or "RF In" connector on the back of the television. Different converter boxes are required for newer digital high definition televisions and older legacy analog televisions. The box must be "activated" by a signal from the cable company before use.
To receive older analog cable, most television sets sold are "cable-ready" and have an analog cable television tuner. This means that the cable from the wall can be directly attached to the back of the television, without the need of a separate decoder, or "set-top box".
Principle of operation.
In the most common system, multiple television channels (as many as 500, although this varies depending on the provider's available channel capacity) are distributed to subscriber residences through a coaxial cable, which comes from a trunkline supported on utility poles originating at the cable company's local distribution facility, called the headend. Multiple channels are transmitted through the cable by a technique called frequency division multiplexing. At the headend, each television channel is translated to a different frequency. By giving each channel a different frequency "slot" on the cable, the separate television signals do not interfere. At the subscriber's residence, either the subscriber's television or a set-top box provided by the cable company translates the desired channel back to its original frequency (baseband), and it is displayed on-screen. Due to widespread cable theft in earlier analog systems, the signals are encrypted on modern digital cable systems, and the set-top box must be activated by an activation code sent by the cable company before it will function, which is only sent after the subscriber signs up. There are also usually "upstream" channels on the cable, to send data from the customer box to the cable headend, for advanced features such as requesting pay-per-view shows, cable internet access, and cable telephone service. The "downstream" channels occupy a band of frequencies from approximately 50 MHz to 1 GHz, while the "upstream" channels occupy frequencies of 5 to 42 MHz. Subscribers pay with a monthly fee. Subscribers can choose from several levels of service, with "premium" packages including more channels but costing a higher rate.
At the local headend, the feed signals from the individual television channels are received by dish antennas from communication satellites. Additional local channels, such as local broadcast television stations, educational channels from local colleges, and community access channels devoted to local governments (PEG channels) are usually included on the cable service. Commercial advertisements for local business are also inserted in the programming at the headend (the individual channels, which are distributed nationally, also have their own nationally oriented commercials).
Hybrid fiber-coaxial.
Modern cable systems are large, with a single network and headend often serving an entire metropolitan area. Most systems use hybrid fiber-coaxial (HFC) distribution; this means the trunklines that carry the signal from the headend to local neighborhoods are optical fiber to provide greater bandwidth and also extra capacity for future expansion. At the headend, the radio frequency electrical signal carrying all the channels is modulated on a light beam and sent through the fiber. The fiber trunkline goes to several "distribution hubs", from which multiple fibers fan out to carry the signal to boxes called "optical nodes" in local communities. At the optical node, the light beam from the fiber is translated back to an electrical signal and carried by coaxial cable distribution lines on utility poles, from which cables branch out to subscriber residences.
Deployments by country.
Cable television is mostly available in North America, Europe, Australia and East Asia, and less so in South America and the Middle East. Cable television has had little success in Africa, as it is not cost-effective to lay cables in sparsely populated areas. So-called "wireless cable" or microwave-based systems are used instead.
Other cable-based services.
Coaxial cables are capable of bi-directional carriage of signals as well as the transmission of large amounts of data. Cable television signals use only a portion of the bandwidth available over coaxial lines. This leaves plenty of space available for other digital services such as cable internet, cable telephony and wireless services, using both unlicensed and licensed spectrum.
Broadband internet access is achieved over coaxial cable by using cable modems to convert the network data into a type of digital signal that can be transferred over coaxial cable. One problem with some cable systems is the older amplifiers placed along the cable routes are unidirectional thus in order to allow for uploading of data the customer would need to use an analog telephone modem to provide for the upstream connection. This limited the upstream speed to 31.2k and prevented the always-on convenience broadband internet typically provides. Many large cable systems have upgraded or are upgrading their equipment to allow for bi-directional signals, thus allowing for greater upload speed and always-on convenience, though these upgrades are expensive.
In North America, Australia and Europe, many cable operators have already introduced cable telephone service, which operates just like existing fixed line operators. This service involves installing a special telephone interface at the customer's premises that converts the analog signals from the customer's in-home wiring into a digital signal, which is then sent on the local loop (replacing the analog last mile, or plain old telephone service (POTS)) to the company's switching center, where it is connected to the public switched telephone network (PSTN). The biggest obstacle to cable telephone service is the need for nearly 100% reliable service for emergency calls. One of the standards available for digital cable telephony, PacketCable, seems to be the most promising and able to work with the Quality of Service (QOS) demands of traditional analog plain old telephone service (POTS) service. The biggest advantage to digital cable telephone service is similar to the advantage of digital cable, namely that data can be compressed, resulting in much less bandwidth used than a dedicated analog circuit-switched service. Other advantages include better voice quality and integration to a Voice over Internet Protocol (VoIP) network providing cheap or unlimited nationwide and international calling. In many cases, digital cable telephone service is separate from cable modem service being offered by many cable companies and does not rely on Internet Protocol (IP) traffic or the Internet.
Beginning in 2004 in the United States, the traditional cable television providers and traditional telecommunication companies increasingly compete in providing voice, video and data services to residences. The combination of television, telephone and Internet access is commonly called "triple play", regardless of whether CATV or telcos offer it.
More recently, several U.S. cable operators have begun offering wireless services to their subscribers. Most notably was the September 2008 launch of Optimum Wi-Fi by Cablevision. This service is made available, at no additional cost, to Optimum Broadband subscribers, and is available at over 14,000 locations across Long Island, New York, and parts of New Jersey and Connecticut. Cablevision has reported a double digit reduction in subscriber churn since launching Optimum Wi-Fi, even as Verizon has rolled out FiOS, a competitive residential broadband service in the Cablevision footprint. Other Tier 1 cable operators, including Comcast, have announced trials of a similar service in sections of the Northeastern United States.
History.
During the 1980s, in the United States, mandated regulations not unlike public, educational, and government access (PEG) channels created the beginning of the cable-originated live television program that evolved into what is known today in the 2010s where many cable networks provide live cable-only broadcasts of many varieties, cable-only produced television movies and miniseries. Various live local programs with local interests were rapidly being created all over the United States in most major television markets in the early 1980s. One of the first was in Columbus, Ohio where Richard Sillman became the nation's youngest cable television Director at age 16.
With the development of the internet, by the late 1990s and early 2000, much of that regulation had been replaced where newer industry technologies developed, offering viewers alternate choices for local events and programming leading to what is today, that being digital cable, Internet and telephone being offered to consumers, bundled, by 2010.

</doc>
<doc id="7591" url="http://en.wikipedia.org/wiki?curid=7591" title="Cholera">
Cholera

Cholera is an infection of the small intestine caused by the bacterium "Vibrio cholerae".
The main symptoms are watery diarrhea and vomiting. This may result in dehydration and in severe cases grayish-bluish skin. Transmission occurs primarily by drinking water or eating food that has been contaminated by the feces (waste product) of an infected person, including one with no apparent symptoms.
The severity of the diarrhea and vomiting can lead to rapid dehydration and electrolyte imbalance, and death in some cases. The primary treatment is oral rehydration therapy, typically with oral rehydration solution, to replace water and electrolytes. If this is not tolerated or does not provide improvement fast enough, intravenous fluids can also be used. Antibacterial drugs are beneficial in those with severe disease to shorten its duration and severity.
Worldwide, it affects 3–5 million people and causes 100,000–130,000 deaths a year . Cholera was one of the earliest infections to be studied by epidemiological methods.
Signs and symptoms.
The primary symptoms of cholera are profuse diarrhea and vomiting of clear fluid. These symptoms usually start suddenly, half a day to five days after ingestion of the bacteria. The diarrhea is frequently described as "rice water" in nature and may have a fishy odor. An untreated person with cholera may produce of diarrhea a day. Severe cholera kills about half of affected individuals. Estimates of the ratio of asymptomatic to symptomatic infections have ranged from 3 to 100. Cholera has been nicknamed the "blue death" because a person's skin may turn bluish-gray from extreme loss of fluids.
If the severe diarrhea is not treated, it can result in life-threatening dehydration and electrolyte imbalances.
Fever is rare and should raise suspicion for secondary infection. Patients can be lethargic, and might have sunken eyes, dry mouth, cold clammy skin, decreased skin turgor, or wrinkled hands and feet. Kussmaul breathing, a deep and labored breathing pattern, can occur because of acidosis from stool bicarbonate losses and lactic acidosis associated with poor perfusion. Blood pressure drops due to dehydration, peripheral pulse is rapid and thready, and urine output decreases with time. Muscle cramping and weakness, altered consciousness, seizures, or even coma due to electrolyte losses and ion shifts are common, especially in children.
Cause.
Transmission is mostly from the fecal contamination of food and water caused by poor sanitation.
Susceptibility.
About 100 million bacteria must typically be ingested to cause cholera in a normal healthy adult. This dose, however, is less in those with lowered gastric acidity (for instance those using proton pump inhibitors). Children are also more susceptible, with two- to four-year-olds having the highest rates of infection. Individuals' susceptibility to cholera is also affected by their blood type, with those with type O blood being the most susceptible. Persons with lowered immunity, such as persons with AIDS or children who are malnourished, are more likely to experience a severe case if they become infected. Any individual, even a healthy adult in middle age, can experience a severe case, and each person's case should be measured by the loss of fluids, preferably in consultation with a professional health care provider.
The cystic fibrosis genetic mutation in humans has been said to maintain a selective advantage: heterozygous carriers of the mutation (who are thus not affected by cystic fibrosis) are more resistant to "V. cholerae" infections. In this model, the genetic deficiency in the cystic fibrosis transmembrane conductance regulator channel proteins interferes with bacteria binding to the gastrointestinal epithelium, thus reducing the effects of an infection.
Transmission.
Cholera is typically transmitted by either contaminated food or water. In the developed world, seafood is the usual cause, while in the developing world it is more often water. Most cholera cases in developed countries are a result of transmission by food. This occurs when people harvest oysters in waters infected with sewage, as "Vibrio cholerae" accumulates in zooplankton and the oysters eat the zooplankton.
Cholera has been found in two animal populations: shellfish and plankton.
People infected with cholera often have diarrhea, and if this highly liquid stool, colloquially referred to as "rice-water", contaminates water used by others, disease transmission may occur. The source of the contamination is typically other cholera sufferers when their untreated diarrheal discharge is allowed to get into waterways, groundwater or drinking water supplies. Drinking any infected water and eating any foods washed in the water, as well as shellfish living in the affected waterway, can cause a person to contract an infection. Cholera is rarely spread directly from person to person. Both toxic and nontoxic strains exist. Nontoxic strains can acquire toxicity through a temperate bacteriophage. Coastal cholera outbreaks typically follow zooplankton blooms, thus making cholera a zoonotic disease.
Mechanism.
When consumed, most bacteria do not survive the acidic conditions of the human stomach. The few surviving bacteria conserve their energy and stored nutrients during the passage through the stomach by shutting down much protein production. When the surviving bacteria exit the stomach and reach the small intestine, they need to propel themselves through the thick mucus that lines the small intestine to get to the intestinal walls where they can thrive. "V. cholerae" bacteria start up production of the hollow cylindrical protein flagellin to make flagella, the cork-screw helical fibers they rotate to propel themselves through the mucus of the small intestine.
Once the cholera bacteria reach the intestinal wall they no longer need the flagella to move. The bacteria stop producing the protein flagellin to conserve energy and nutrients by changing the mix of proteins which they express in response to the changed chemical surroundings. On reaching the intestinal wall, "V. cholerae" start producing the toxic proteins that give the infected person a watery diarrhea. This carries the multiplying new generations of "V. cholerae" bacteria out into the drinking water of the next host if proper sanitation measures are not in place.
The cholera toxin (CTX or CT) is an oligomeric complex made up of six protein subunits: a single copy of the A subunit (part A), and five copies of the B subunit (part B), connected by a disulfide bond. The five B subunits form a five-membered ring that binds to GM1 gangliosides on the surface of the intestinal epithelium cells. The A1 portion of the A subunit is an enzyme that ADP-ribosylates G proteins, while the A2 chain fits into the central pore of the B subunit ring. Upon binding, the complex is taken into the cell via receptor-mediated endocytosis. Once inside the cell, the disulfide bond is reduced, and the A1 subunit is freed to bind with a human partner protein called ADP-ribosylation factor 6 (Arf6). Binding exposes its active site, allowing it to permanently ribosylate the Gs alpha subunit of the heterotrimeric G protein. This results in constitutive cAMP production, which in turn leads to secretion of H2O, Na+, K+, Cl−, and HCO3− into the lumen of the small intestine and rapid dehydration. The gene encoding the cholera toxin was introduced into "V. cholerae" by horizontal gene transfer. Virulent strains of "V. cholerae" carry a variant of temperate bacteriophage called CTXf or CTXφ.
Microbiologists have studied the genetic mechanisms by which the "V. cholerae" bacteria turn off the production of some proteins and turn on the production of other proteins as they respond to the series of chemical environments they encounter, passing through the stomach, through the mucous layer of the small intestine, and on to the intestinal wall. Of particular interest have been the genetic mechanisms by which cholera bacteria turn on the protein production of the toxins that interact with host cell mechanisms to pump chloride ions into the small intestine, creating an ionic pressure which prevents sodium ions from entering the cell. The chloride and sodium ions create a salt-water environment in the small intestines, which through osmosis can pull up to six litres of water per day through the intestinal cells, creating the massive amounts of diarrhea. The host can become rapidly dehydrated if an appropriate mixture of dilute salt water and sugar is not taken to replace the blood's water and salts lost in the diarrhea.
By inserting separate, successive sections of "V. cholerae" DNA into the DNA of other bacteria, such as "E. coli" that would not naturally produce the protein toxins, researchers have investigated the mechanisms by which "V. cholerae" responds to the changing chemical environments of the stomach, mucous layers, and intestinal wall. Researchers have discovered a complex cascade of regulatory proteins controls expression of "V. cholerae" virulence determinants. In responding to the chemical environment at the intestinal wall, the "V. cholerae" bacteria produce the TcpP/TcpH proteins, which, together with the ToxR/ToxS proteins, activate the expression of the ToxT regulatory protein. ToxT then directly activates expression of virulence genes that produce the toxins, causing diarrhea in the infected person and allowing the bacteria to colonize the intestine. Current research aims at discovering "the signal that makes the cholera bacteria stop swimming and start to colonize (that is, adhere to the cells of) the small intestine."
Genetic structure.
Amplified fragment length polymorphism fingerprinting of the pandemic isolates of "V. cholerae" has revealed variation in the genetic structure. Two clusters have been identified: Cluster I and Cluster II. For the most part, Cluster I consists of strains from the 1960s and 1970s, while Cluster II largely contains strains from the 1980s and 1990s, based on the change in the clone structure. This grouping of strains is best seen in the strains from the African continent.
Diagnosis.
A rapid dipstick test is available to determine the presence of "V. cholerae". In those samples that test positive, further testing should be done to determine antibiotic resistance. In epidemic situations, a clinical diagnosis may be made by taking a patient history and doing a brief examination. Treatment is usually started without or before confirmation by laboratory analysis.
Stool and swab samples collected in the acute stage of the disease, before antibiotics have been administered, are the most useful specimens for laboratory diagnosis. If an epidemic of cholera is suspected, the most common causative agent is "V. cholerae" O1. If "V. cholerae" serogroup O1 is not isolated, the laboratory should test for "V. cholerae" O139. However, if neither of these organisms is isolated, it is necessary to send stool specimens to a reference laboratory.
Infection with "V. cholerae" O139 should be reported and handled in the same manner as that caused by "V. cholerae" O1. The associated diarrheal illness should be referred to as cholera and must be reported in the United States.
Prevention.
The World Health Organization recommends focusing on prevention, preparedness, and response to combat the spread of cholera. They also stress the importance of an effective surveillance system. Governments can play a role in all of these areas, and in preventing cholera or indirectly facilitating its spread.
Although cholera may be life-threatening, prevention of the disease is normally straightforward if proper sanitation practices are followed. In developed countries, due to nearly universal advanced water treatment and sanitation practices, cholera is no longer a major health threat. The last major outbreak of cholera in the United States occurred in 1910–1911. Effective sanitation practices, if instituted and adhered to in time, are usually sufficient to stop an epidemic. There are several points along the cholera transmission path at which its spread may be halted:
Surveillance.
Surveillance and prompt reporting allow for containing cholera epidemics rapidly. Cholera exists as a seasonal disease in many endemic countries, occurring annually mostly during rainy seasons. Surveillance systems can provide early alerts to outbreaks, therefore leading to coordinated response and assist in preparation of preparedness plans. Efficient surveillance systems can also improve the risk assessment for potential cholera outbreaks. Understanding the seasonality and location of outbreaks provides guidance for improving cholera control activities for the most vulnerable. For prevention to be effective, it is important that cases be reported to national health authorities.
Vaccine.
A number of safe and effective oral vaccines for cholera are available. Dukoral, an orally administered, inactivated whole cell vaccine, has an overall efficacy of about 52% during the first year after being given and 62% in the second year, with minimal side effects. It is available in over 60 countries. However, it is not currently recommended by the Centers for Disease Control and Prevention (CDC) for most people traveling from the United States to endemic countries. One injectable vaccine was found to be effective for two to three years. The protective efficacy was 28% lower in children less than 5 years old. However, as of 2010, it has limited availability. Work is under way to investigate the role of mass vaccination. The World Health Organization (WHO) recommends immunization of high-risk groups, such as children and people with HIV, in countries where this disease is endemic. If people are immunized broadly, herd immunity results, with a decrease in the amount of contamination in the environment.
Sari filtration.
An effective and relatively cheap method to prevent the transmission of cholera is the use of a folded "sari" (a long cloth garment) to filter drinking water. In Bangladesh this practice was found to decrease rates of cholera by nearly half. It involves folding a "sari" four to eight times. Between uses the cloth should be rinsed in clean water and dried in the sun to kill any bacteria on it. A nylon cloth appears to work as well.
Treatment.
Continued eating speeds the recovery of normal intestinal function. The World Health Organization recommends this generally for cases of diarrhea no matter what the underlying cause. A CDC training manual specifically for cholera states: “Continue to breastfeed your baby if the baby has watery diarrhea, even when traveling to get treatment. Adults and older children should continue to eat frequently.”
Fluids.
The most common error in caring for patients with cholera is to underestimate the speed
and volume of fluids required. In most cases, cholera can be successfully treated with oral rehydration therapy (ORT), which is highly effective, safe, and simple to administer. Rice-based solutions are preferred to glucose-based ones due to greater efficiency. In severe cases with significant dehydration, intravenous rehydration may be necessary. Ringer's lactate is the preferred solution, often with added potassium. Large volumes and continued replacement until diarrhea has subsided may be needed. Ten percent of a person's body weight in fluid may need to be given in the first two to four hours. This method was first tried on a mass scale during the Bangladesh Liberation War, and was found to have much success.
If commercially produced oral rehydration solutions are too expensive or difficult to obtain, solutions can be made. One such recipe calls for 1 liter of boiled water, 1/2 teaspoon of salt, 6 teaspoons of sugar, and added mashed banana for potassium and to improve taste.
Electrolytes.
As there frequently is initially acidosis, the potassium level may be normal, even though large losses have occurred. As the dehydration is corrected, potassium levels may decrease rapidly, and thus need to be replaced. This may be done by eating foods high in potassium like bananas or green coconut water.
Antibiotics.
Antibiotic treatments for one to three days shorten the course of the disease and reduce the severity of the symptoms. Use of antibiotics also reduces fluid requirements. People will recover without them, however, if sufficient hydration is maintained. The World Health Organization only recommends antibiotics in those with severe dehydration.
Doxycycline is typically used first line, although some strains of "V. cholerae" have shown resistance. Testing for resistance during an outbreak can help determine appropriate future choices. Other antibiotics proven to be effective include cotrimoxazole, erythromycin, tetracycline, chloramphenicol, and furazolidone. Fluoroquinolones, such as ciprofloxacin, also may be used, but resistance has been reported.
In many areas of the world, antibiotic resistance is increasing. In Bangladesh, for example, most cases are resistant to tetracycline, trimethoprim-sulfamethoxazole, and erythromycin. Rapid diagnostic assay methods are available for the identification of multiple drug-resistant cases. New generation antimicrobials have been discovered which are effective against in "in vitro" studies.
Zinc supplementation.
In Bangladesh zinc supplementation reduced the duration and severity of diarrhea in children with cholera when given with antibiotics and rehydration therapy as needed. It reduced the length of disease by eight hours and the amount of diarrheal stool by 10%. Supplementation appears to be also effective in both treating and preventing infectious diarrhea due to other causes among children in the developing world.
Prognosis.
If people with cholera are treated quickly and properly, the mortality rate is less than 1%; however, with untreated cholera, the mortality rate rises to 50–60%. For certain genetic strains of cholera, such as the one present during the 2010 epidemic in Haiti and the 2004 outbreak in India, death can occur within two hours of becoming ill.
Epidemiology.
Cholera affects an estimated 3–5 million people worldwide, and causes 58,000–130,000 deaths a year as of 2010. This occurs mainly in the developing world. In the early 1980s, death rates are believed to have been greater than 3 million a year. It is difficult to calculate exact numbers of cases, as many go unreported due to concerns that an outbreak may have a negative impact on the tourism of a country. Cholera remains both epidemic and endemic in many areas of the world.
Although much is known about the mechanisms behind the spread of cholera, this has not led to a full understanding of what makes cholera outbreaks happen some places and not others. Lack of treatment of human feces and lack of treatment of drinking water greatly facilitate its spread, but bodies of water can serve as a reservoir, and seafood shipped long distances can spread the disease. Cholera was not known in the Americas for most of the 20th century, but it reappeared towards the end of that century.
History.
The word cholera is from "kholera" from χολή "kholē" "bile". Cholera likely has its origins in the Indian subcontinent; it has been prevalent in the Ganges delta since ancient times. Choleras first origins, within the Indian subcontinent, are believed to have occurred due to the result of poor living conditions as well as the presence of pools of still water; both of which are ideal living conditions for cholera to thrive. The disease first spread by trade routes (land and sea) to Russia in 1817, then, through technological advancements, to the rest of Europe, and from Europe to North America and the rest of the world. Seven cholera pandemics have occurred in the past 200 years, with the seventh pandemic originating in Indonesia in 1961.
The first cholera pandemic occurred in the Bengal region of India starting in 1817 through 1824. The disease dispersed from India to Southeast Asia, China, Japan, the Middle East, and southern Russia. The second pandemic lasted from 1827 to 1835 and affected the United States and Europe particularly due to the result of advancements in transportation and global trade, and increased human migration, including soldiers. The third pandemic erupted in 1839, persisted until 1856, extended to North Africa, and reached South America, for the first time specifically infringing upon Brazil. Cholera hit the sub-Saharan African region during the fourth pandemic from 1863 to 1875. The fifth and sixth pandemics raged from 1881–1896 and 1899-1923. These epidemics were less fatal due to a greater understanding of the cholera bacteria. Egypt, the Arabian peninsula, Persia, India, and the Philippines were hit hardest during these epidemics, while other areas, like Germany in 1892 and Naples from 1910–1911, experienced severe outbreaks. The final pandemic originated in 1961 in Indonesia and is marked by the emergence of a new strain, nicknamed "El Tor", which still persists today in developing countries.
Since it became widespread in the 19th century, cholera has killed tens of millions of people. In Russia alone, between 1847 and 1851, more than one million people perished of the disease. It killed 150,000 Americans during the second pandemic. Between 1900 and 1920, perhaps eight million people died of cholera in India.
Cholera became the first reportable disease in the United States due to the significant effects it had on health. John Snow, in England, was the first to identify the importance of contaminated water in its cause in 1854. Cholera is now no longer considered a pressing health threat in Europe and North America due to filtering and chlorination of water supplies, but still heavily affects populations in developing countries.
In the past, vessels flew a yellow quarantine flag if any crew members or passengers were suffering from cholera. No one aboard a vessel flying a yellow flag would be allowed ashore for an extended period, typically 30 to 40 days. In modern sets of international maritime signal flags, the quarantine flag is yellow and black.
Historically many different claimed remedies have existed in folklore. In the 1854–1855 outbreak in Naples homeopathic Camphor was used according to Hahnemann. While T. J. Ritter's "Mother's Remedies" book lists tomato syrup as a home remedy from northern America. While elecampagne was recommended in the United Kingdom according to William Thomas Fernie
Cholera cases are much less frequent in developed countries where governments have helped to establish water sanitation practices and effective medical treatments. The United States, for example, used to have a severe cholera problem similar to those in some developing countries. There were three large cholera outbreaks in the 1800s, which can be attributed to "Vibrio cholerae"'s spread through interior waterways like the Erie Canal and routes along the Eastern Seaboard. The island of Manhattan in New York City touched the Atlantic Ocean, where cholera collected just off the coast. At this time, New York City did not have as effective a sanitation system as it does today, so cholera was able to spread.
Research.
The bacterium was isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.
Spanish physician Jaume Ferran i Clua developed a cholera vaccine in 1885, the first to immunize humans against a bacterial disease.
Russian-Jewish bacteriologist Waldemar Haffkine developed a cholera vaccine in July 1892.
One of the major contributions to fighting cholera was made by the physician and pioneer medical scientist John Snow (1813–1858), who in 1854 found a link between cholera and contaminated drinking water. Dr. Snow proposed a microbial origin for epidemic cholera in 1849. In his major "state of the art" review of 1855, he proposed a substantially complete and correct model for the etiology of the disease. In two pioneering epidemiological field studies, he was able to demonstrate human sewage contamination was the most probable disease vector in two major epidemics in London in 1854. His model was not immediately accepted, but it was seen to be the more plausible, as medical microbiology developed over the next 30 years or so.
Cities in developed nations made massive investment in clean water supply and well-separated sewage treatment infrastructures between the mid-1850s and the 1900s. This eliminated the threat of cholera epidemics from the major developed cities in the world. In 1883, Robert Koch identified "V. cholerae" with a microscope as the bacillus causing the disease.
Robert Allan Phillips, working at the US Naval Medical Research Unit Two in Southeast Asia, evaluated the pathophysiology of the disease using modern laboratory chemistry techniques and developed a protocol for rehydration. His research led the Lasker Foundation to award him its prize in 1967.
Cholera has been a laboratory for the study of evolution of virulence. The province of Bengal in British India was partitioned into West Bengal and East Pakistan in 1947. Prior to partition, both regions had cholera pathogens with similar characteristics. After 1947, India made more progress on public health than East Pakistan (now Bangladesh). As a consequence, the strains of the pathogen that succeeded in India had a greater incentive in the longevity of the host. They have become less virulent than the strains prevailing in Bangladesh. These draw upon the resources of the host population and rapidly kill many victims.
More recently, in 2002, Alam, "et al.", studied stool samples from patients at the International Centre for Diarrhoeal Disease in Dhaka, Bangladesh. From the various experiments they conducted, the researchers found a correlation between the passage of "V. cholerae" through the human digestive system and an increased infectivity state. Furthermore, the researchers found the bacterium creates a hyperinfected state where genes that control biosynthesis of amino acids, iron uptake systems, and formation of periplasmic nitrate reductase complexes were induced just before defecation. These induced characteristics allow the cholera vibrios to survive in the "rice water" stools, an environment of limited oxygen and iron, of patients with a cholera infection.
Society and culture.
In many developing countries, cholera still reaches its victims through contaminated water sources, and countries without proper sanitation techniques have greater incidence of the disease. Governments can play a role in this. In 2008, for example, the Zimbabwean cholera outbreak was due partly to the government's role, according to a report from the James Baker Institute. The Haitian government’s inability to provide safe drinking water after the 2010 earthquake led to an increase in cholera cases as well.
Similarly, South Africa’s cholera outbreak was exacerbated by the government’s policy of privatizing water programs. The wealthy elite of the country were able to afford safe water while others had to use water from cholera-infected rivers.
According to Rita R. Colwell of the James Baker Institute, if cholera does begin to spread, government preparedness is crucial. A government's ability to contain the disease before it extends to other areas can prevent a high death toll and the development of an epidemic or even pandemic. Effective disease surveillance can ensure that cholera outbreaks are recognized as soon as possible and dealt with appropriately. Oftentimes, this will allow public health programs to determine and control the cause of the cases, whether it is unsanitary water or seafood that have accumulated a lot of "Vibrio cholerae" specimens. Having an effective surveillance program contributes to a government’s ability to prevent cholera from spreading. In the year 2000 in the state of Kerala in India, the Kottayam district was determined to be "cholera-affected"; this pronouncement led to task forces that concentrated on educating citizens with 13,670 information sessions about human health. These task forces promoted the boiling of water to obtain safe water, and provided chlorine and oral rehydration salts. Ultimately, this helped to control the spread of the disease to other areas and minimize deaths. On the other hand, researchers have shown that most of the citizens infected during the 1991 cholera outbreak in Bangladesh lived in rural areas, and were not recognized by the government's surveillance program. This inhibited physicians' abilities to detect cholera cases early.
According to Colwell, the quality and inclusiveness of a country's health care system affects the control of cholera, as it did in the Zimbabwean cholera outbreak. While sanitation practices are important, when governments respond quickly and have readily available vaccines, the country will have a lower cholera death toll. Affordability of vaccines can be a problem; if the governments do not provide vaccinations, only the wealthy may be able to afford them and there will be a greater toll on the country's poor. The speed with which government leaders respond to cholera outbreaks is important.
Besides contributing to an effective or declining public health care system and water sanitation treatments, government can have indirect effects on cholera control and the effectiveness of a response to cholera. A country's government can impact its ability to prevent disease and control its spread. A speedy government response backed by a fully functioning health care system and financial resources can prevent cholera's spread. This limits cholera's ability to cause death, or at the very least a decline in education, as children are kept out of school to minimize the risk of infection.
Notable cases.
Other famous people believed to have died of cholera include:

</doc>
<doc id="7592" url="http://en.wikipedia.org/wiki?curid=7592" title="Caldera">
Caldera

A caldera is a cauldron-like volcanic feature usually formed by the collapse of land following a volcanic eruption. They are sometimes confused with volcanic craters. The word comes from Spanish "caldera", and this from Latin "caldaria", meaning "cooking pot". In some texts the English term "cauldron" is also used.
The term "caldera" was introduced into the geological vocabulary by the German geologist Leopold von Buch when he published his memoirs of an 1815 visit to the Canary Islands, where he saw the Las Cañadas caldera on Tenerife, with Teide dominating the scene, and the Caldera de Taburiente on La Palma.
Caldera formation.
A collapse is triggered by the emptying of the magma chamber beneath the volcano, usually as the result of a large volcanic eruption. If enough magma is ejected, the emptied chamber is unable to support the weight of the "volcanic edifice" above it. A roughly circular fracture, the "ring fault", develops around the edge of the chamber. Ring fractures serve as feeders for fault intrusions which are also known as ring dykes. Secondary volcanic vents may form above the ring fracture. As the magma chamber empties, the center of the volcano within the ring fracture begins to collapse. The collapse may occur as the result of a single cataclysmic eruption, or it may occur in stages as the result of a series of eruptions. The total area that collapses may be hundreds or thousands of square kilometers.
Mineralization.
Some calderas are known to host rich ore deposits. One of the world's best preserved mineralized calderas is the Neoarchean Sturgeon Lake Caldera in northeastern Ontario, Canada.
Explosive caldera.
If the magma is rich in silica, the caldera is often filled in with ignimbrite, tuff, rhyolite, and other igneous rocks. Silica-rich magma has a high viscosity, and therefore does not flow easily like basalt. As a result, gases tend to become trapped at high pressure within the magma. When the magma approaches the surface of the Earth, the rapid off-loading of overlying material causes the trapped gases to decompress rapidly, thus triggering explosive destruction of the magma and spreading volcanic ash over wide areas. Further lava flows may be erupted.
If volcanic activity continues, the centre of the caldera may be uplifted in the form of a "resurgent dome" such as is seen at Cerro Galán, Lake Toba, Yellowstone, and so on, by subsequent intrusion of magma. A "silicic" or "rhyolitic caldera" may erupt hundreds or even thousands of cubic kilometers of material in a single event. Even small caldera-forming eruptions, such as Krakatoa in 1883 or Mount Pinatubo in 1991, may result in significant local destruction and a noticeable drop in temperature around the world. Large calderas may have even greater effects.
When Yellowstone Caldera last erupted some 650,000 years ago, it released about 1,000 km3 of material (as measured in dense rock equivalent (DRE)), covering a substantial part of North America in up to two metres of debris. By comparison, when Mount St. Helens erupted in 1980, it released ~1.2 km3 (DRE) of ejecta. The ecological effects of the eruption of a large caldera can be seen in the record of the Lake Toba eruption in Indonesia.
Toba.
About 74,000 years ago, this Indonesian volcano released about 2,800 km3 DRE of ejecta, the largest known eruption within the Quaternary Period (last 1.8 million years) and the largest known explosive eruption within the last 25 million years. In the late 1990s, anthropologist Stanley Ambrose proposed that a volcanic winter induced by this eruption reduced the human population to about 2,000 - 20,000 individuals, resulting in a population bottleneck ("see" Toba catastrophe theory). More recently several geneticists, including Lynn Jorde and Henry Harpending have proposed that the human race was reduced to approximately five to ten thousand people. However, there is no direct evidence that the theory is correct. And there is no evidence for any other animal decline or extinction, even in environmentally sensitive species. There is evidence that human habitation continued in India after the eruption. The theory in its strongest form may be incorrect.
Eruptions forming even larger calderas are known, especially La Garita Caldera in the San Juan Mountains of Colorado, where the 5,000-km3 Fish Canyon Tuff was blasted out in a single major eruption about 27.8 million years ago.
At some points in geological time, rhyolitic calderas have appeared in distinct clusters. The remnants of such clusters may be found in places such as the San Juan Mountains of Colorado (formed during the Oligocene, Miocene, and Pliocene periods) or the Saint Francois Mountain Range of Missouri (erupted during the Proterozoic).
Non-explosive calderas.
Some volcanoes, such as shield volcanoes Kīlauea and Mauna Loa (respectively the most active and second largest on Earth, are both on the island of Hawaii), form calderas in a different fashion. The magma feeding these volcanoes is basalt which is silica poor. As a result, the magma is much less viscous than the magma of a rhyolitic volcano, and the magma chamber is drained by large lava flows rather than by explosive events. The resulting calderas are also known as subsidence calderas, and can form more gradually than explosive calderas. For instance, the caldera atop Fernandina Island underwent a collapse in 1968, when parts of the caldera floor dropped 350 meters. Kilauea Caldera has an inner crater known as Halema‘uma‘u, which has often been filled by a lava lake.
In April 2007, during the eruption, the summit floor of the Piton de la Fournaise on the island of Réunion the floor of the main crater suddenly dropped about 300 m. This was attributed to the withdrawal of magma which was being erupted through a vent lower down on the southern flank of the volcano.
It is very frequent for a caldera to become emptied by drainage of melted lava through a breach on the caldera's rim.
Extraterrestrial calderas.
Since the early 1960s, it has been known that volcanism has occurred on other planets and moons in the Solar System. Through the use of manned and unmanned spacecraft, volcanism has been discovered on Venus, Mars, the Moon, and Io, a satellite of Jupiter. None of these worlds have plate tectonics, which contributes approximately 60% of the Earth's volcanic activity (the other 40% is attributed to hotspot volcanism). Caldera structure is similar on all of these planetary bodies, though the size varies considerably. The average caldera diameter on Venus is 68 km. The average caldera diameter of Io is close to 40 km, and the mode is 6 km. Tvashtar Paterae is likely the largest caldera on Io with a diameter of 290 km. The average caldera diameter of Mars is 48 km, smaller than Venus. Calderas on Earth are the smallest of all planetary bodies and vary from 1.6 to 80 km as a maximum.
The Moon.
The Moon has an outer shell of low-density crystalline rock that is a few hundred kilometers thick, which formed due to a rapid creation. The craters of the moon have been well preserved through time and were once thought to have been the result of extreme volcanic activity, but instead were formed by meteorites, nearly all of which took place in the first few hundred million years after the Moon formed. Around 500 million years afterward, the Moon's mantle was able to be extensively melted due to the decay of radioactive elements. Massive basaltic eruptions took place generally at the base of large impact craters. Also, eruptions may have taken place due to a magma reservoir at the base of the crust. This forms a dome, possibly the same morphology of a shield volcano where calderas universally are known to form.
Mars.
The volcanic activity of Mars is concentrated in two major provinces: Tharsis and Elysium. Each province contains a series of giant shield volcanoes that are similar to what we see on Earth and likely are the result of mantle hot spots. The surfaces are dominated by lava flows, and all have one or more collapse calderas. Mars has the largest volcano in the Solar System, Olympus Mons, which is more than three times the height of Mount Everest, with a diameter of 520 km (323 miles). The summit of the mountain has six nested calderas.
Venus.
Because there are no plate tectonics on Venus, heat is only lost by conduction through the lithosphere. This causes enormous lava flows, accounting for 80% of Venus' surface area. Many of the mountains are large shield volcanoes that range in size from 150–400 km in diameter and 2–4 km high. More than 80 of these large shield volcanoes have summit calderas averaging 60 km across.
Io.
Io, unusually, is heated by solid flexing due to the tidal influence of Jupiter and Io's orbital resonance with neighboring large moons Europa and Ganymede, which keeps its orbit slightly eccentric. Unlike any of the planets mentioned, Io is continuously volcanically active and contains many calderas with diameters tens of kilometers across. For example, in 1979, "Voyager 1" and "Voyager 2" spotted nine erupting volcanoes while passing Io.
List of volcanic calderas.
"See also 

</doc>
<doc id="7593" url="http://en.wikipedia.org/wiki?curid=7593" title="Calculator">
Calculator

An electronic calculator is a small, portable electronic device used to perform both basic and complex operations of arithmetic. In 2014, basic calculators can be very inexpensive. Scientific calculators tend to be higher-priced.
The first solid state electronic calculator was created in the 1960s, building on the extensive history of tools such as the abacus, developed around 2000 BC, and the mechanical calculator, developed in the 17th century. It was developed in parallel with the analog computers of the day.
Pocket sized devices became available in the 1970s, especially after the invention of the microprocessor developed by Intel for the Japanese calculator company Busicom.
Modern electronic calculators vary from cheap, give-away, credit-card-sized models to sturdy desktop models with built-in printers. They became popular in the mid-1970s as integrated circuits made their size and cost small. By the end of that decade, calculator prices had reduced to a point where a basic calculator was affordable to most and they became common in schools.
Computer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all PDA-type devices (save a few dedicated address book and dictionary devices).
In addition to general purpose calculators, there are those designed for specific markets; for example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space.
In 1986, calculators still represented an estimated 41% of the world's general-purpose hardware capacity to compute information. This diminished to less than 0.05% by 2007.
Design.
Modern electronic calculators contain a keyboard with buttons for digits and arithmetical operations. Some even contain 00 and 000 buttons to make large numbers easier to enter. Most basic calculators assign only one digit or operation on each button. However, in more specific calculators, a button can perform multi-function working with key combination or current reckoning mode.
Calculators usually have liquid-crystal displays as output in place of historical vacuum fluorescent displays. See more details in technical improvements. Fractions such as are displayed as decimal approximations, for example rounded to . Also, some fractions such as which is (to 14 significant figures) can be difficult to recognize in decimal form; as a result, many scientific calculators are able to work in vulgar fractions or mixed numbers.
Calculators also have the ability to store numbers into memory. Basic types of these store only one number at a time. More specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulae. Some models have the ability to extend memory capacity to store more numbers; the extended address is referred to as an array index.
Power sources of calculators are batteries, solar cells or electricity (for old models) turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off, for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid. Crank-powered calculators were also common in the early computer era.
Use in education.
In most countries, students use calculators for schoolwork. There was some initial resistance to the idea out of fear that basic arithmetic skills would suffer. There remains disagreement about the importance of the ability to perform calculations "in the head", with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching estimation techniques and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become "too dependent" on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum.
Internal workings.
In general, a basic electronic calculator consists of the following components:
Example.
A basic explanation as to how calculations are performed in a simple 4-function calculator:
To perform the calculation 25 + 9, one presses keys in the following sequence on most calculators:     .
All other functions are usually carried out using repeated additions. Where calculators have additional functions such as square root, or trigonometric functions, software algorithms are required to produce high precision results. Sometimes significant design effort is required to fit all the desired functions in the limited memory space available in the calculator chip, with acceptable calculation time.
Calculators compared to computers.
The fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions such as addition, multiplication, and logarithms built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functionality, sometimes with support for programming languages such as RPL or TI-BASIC.
Typically the user buys the least expensive model having a specific feature set, but does not care much about speed (since speed is constrained by how fast the user can press the buttons). Thus designers of calculators strive to minimize the number of logic elements on the chip, not the number of clock cycles needed to do a computation.
For instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in ROM, and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require hardware floating-point. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. (Again, the line blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, particularly the Z80, MC68000, and ARM architectures, as well as some custom designs specifically made for the calculator market.)
History.
Precursors to the electronic calculator.
The first known tools used to aid arithmetic calculations were bones (used to tally items), pebbles and counting boards, and the Abacus, known to have been used by Sumerians and Egyptians before 2000 BC. Except for the Antikythera mechanism, an "out of the time" astronomical device, development of computing tools arrived near the beginning of the 17th century: Geometric-military compass by Galileo, Logarithms and Napier Bones by Napier, slide rule by Edmund Gunter.
In 1642, the Renaissance saw the invention of the mechanical calculator by Wilhelm Schickard and several decades later Blaise Pascal, a device that was at times somewhat over-promoted as being able to perform all four arithmetic operations minimal human intervention. Pascal's Calculator could add and subtract two numbers directly and thus, if the tedium could be borne, multiply and divide by repetition. Schickard's machine, constructed several decades earlier, used a clever set of mechanised multiplication tables to ease the process of multiplication and division with the adding machine as a means of completing this operation. (Because they were different inventions with different aims a debate about whether Pascal or Schickard should be credited as the "inventor" of the adding machine (or calculating machine) is probably pointless.) Schickard and Pascal were followed by Gottfried Leibniz who spent forty years designing a four-operation mechanical calculator, inventing in the process his leibniz wheel, but who couldn't design a fully operational machine. There were also five unsuccessful attempts to design a calculating clock in the 17th century.
The 18th century saw the arrival of some interesting improvements, first by Poleni with the first fully functional calculating clock and four-operation machine, but these machines were almost always "one of the kind". It was not until the 19th century and the Industrial Revolution that real developments began to occur. Although machines capable of performing all four arithmetic functions existed prior to the 19th century, the refinement of manufacturing and fabrication processes during the eve of the industrial revolution made large scale production of more compact and modern units possible. The Arithmometer, invented in 1820 as a four-operation mechanical calculator, was released to production in 1851 as an adding machine and became the first commercially successful unit; forty years later, by 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883) and Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers.
It wasn't until 1902 that the familiar push-button user interface was developed, with the introduction of the Dalton Adding Machine, developed by James L. Dalton in the United States.
The Curta calculator was developed in 1948 and, although costly, became popular for its portability. This purely mechanical hand-held device could do addition, subtraction, multiplication and division. By the early 1970s electronic pocket calculators ended manufacture of mechanical calculators, although the Curta remains a popular collectable item.
Development of electronic calculators.
The first mainframe computers, using firstly vacuum tubes and later transistors in the logic circuits, appeared in the 1940s and 1950s. This technology was to provide a stepping stone to the development of electronic calculators.
The Casio Computer Company, in Japan, released the Model "14-A" calculator in 1957, which was the world's first all-electric (relatively) "compact" calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.
In October 1961, the world's first "all-electronic desktop" calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced. This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly 33 pounds due to its large tube system. Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names "Plus" and "Sumlock", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.
The tube technology of the ANITA was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a CRT, and introduced reverse Polish notation (RPN) to the calculator market for a price of $2200, which was about three times the cost of an electromechanical calculator of the time. Like Bell Punch, Friden was a manufacturer of mechanical calculators that had decided that the future lay in electronics. In 1964 more all-transistor electronic calculators were introduced: Sharp introduced the CS-10A, which weighed 25 kg (55 lb) and cost 500,000 yen (~US$2500), and Industria Macchine Elettroniche of Italy introduced the IME 84, to which several extra keyboard and display units could be connected so that several people could make use of it (but apparently not at the same time).
There followed a series of electronic calculator models from these and other manufacturers, including Canon, Mathatronics, Olivetti, SCM (Smith-Corona-Marchant), Sony, Toshiba, and Wang. The early calculators used hundreds of germanium transistors, which were cheaper than silicon transistors, on multiple circuit boards. Display types used were CRT, cold-cathode Nixie tubes, and filament lamps. Memory technology was usually based on the delay line memory or the magnetic core memory, though the Toshiba "Toscal" BC-1411 appears to have used an early form of dynamic RAM built from discrete components. Already there was a desire for smaller and less power-hungry machines.
The Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.
Another calculator introduced in 1965 was Bulgaria's ELKA 6521, developed by the Central Institute for Calculation Technologies and built at the Elektronika factory in Sofia. The name derives from "ELektronen KAlkulator", and it weighed around 8 kg. It is the first calculator in the world which includes the square root function. Later that same year were released the ELKA 22 (with a luminescent display) and the ELKA 25, with an in-built printer. Several other models were developed until the first pocket model, the ELKA 101, was released in 1974. The writing on it was in Roman script, and it was exported to western countries.
The "Monroe Epic" programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only "branch" instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.
The first handheld calculator, a prototype called "Cal Tech", was developed by Texas Instruments in 1967. It could add, multiply, subtract, and divide, and its output device was a paper tape.
1970s to mid-1980s.
The electronic calculators of the mid-1960s were large and heavy desktop machines due to their use of hundreds of transistors on several circuit boards with a large power consumption that required an AC power supply. There were great efforts to put the logic required for a calculator into fewer and fewer integrated circuits (chips) and calculator electronics was one of the leading edges of semiconductor development. U.S. semiconductor manufacturers led the world in Large Scale Integration (LSI) semiconductor development, squeezing more and more functions into individual integrated circuits. This led to alliances between Japanese calculator manufacturers and U.S. semiconductor companies: Canon Inc. with Texas Instruments, Hayakawa Electric (later known as Sharp Corporation) with North-American Rockwell Microelectronics, Busicom with Mostek and Intel, and General Instrument with Sanyo.
Pocket calculators.
By 1970, a calculator could be made using just a few chips of low power consumption, allowing portable models powered from rechargeable batteries. The first portable calculators appeared in Japan in 1970, and were soon marketed around the world. These included the Sanyo ICC-0081 "Mini Calculator", the Canon Pocketronic, and the Sharp QT-8B "micro Compet". The Canon Pocketronic was a development of the "Cal-Tech" project which had been started at Texas Instruments in 1965 as a research project to produce a portable calculator. The Pocketronic has no traditional display; numerical output is on thermal paper tape. As a result of the "Cal-Tech" project, Texas Instruments was granted master patents on portable calculators.
Sharp put in great efforts in size and power reduction and introduced in January 1971 the Sharp EL-8, also marketed as the Facit 1111, which was close to being a pocket calculator. It weighed about 482 grams or 1.0652 pound, had a vacuum fluorescent display, rechargeable NiCad batteries, and initially sold for $395.
However, the efforts in integrated circuit development culminated in the introduction in early 1971 of the first "calculator on a chip", the MK6010 by Mostek, followed by Texas Instruments later in the year. Although these early hand-held calculators were very expensive, these advances in electronics, together with developments in display technology (such as the vacuum fluorescent display, LED, and LCD), led within a few years to the cheap pocket calculator available to all.
In 1971 Pico Electronics. and General Instrument also introduced their first collaboration in ICs, a complete single chip calculator IC for the Monroe Royal Digital III calculator. Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. Pico and GI went on to have significant success in the burgeoning handheld calculator market.
The first truly pocket-sized electronic calculator was the Busicom LE-120A "HANDY", which was marketed early in 1971. Made in Japan, this was also the first calculator to use an LED display, the first hand-held calculator to use a single integrated circuit (then proclaimed as a "calculator on a chip"), the Mostek MK6010, and the first electronic calculator to run off replaceable batteries. Using four AA-size cells the LE-120A measures 4.9x2.8x0.9 in (124x72x24 mm).
The first American-made pocket-sized calculator, the Bowmar 901B (popularly referred to as "The Bowmar Brain"), measuring 5.2 × 3.0 × 1.5 in (131 × 77 × 37 mm), came out in the Autumn of 1971, with four functions and an eight-digit red LED display, for $240, while in August 1972 the four-function Sinclair Executive became the first slimline pocket calculator measuring 5.4 × 2.2 × 0.35 in (138 × 56 × 9 mm) and weighing 2.5 oz (70g). It retailed for around £79. By the end of the decade, similar calculators were priced less than £5.
The first Soviet-made pocket-sized calculator, the "Elektronika B3-04" was developed by the end of 1973 and sold at the beginning of 1974.
One of the first low-cost calculators was the Sinclair Cambridge, launched in August 1973. It retailed for £29.95, or £5 less in kit form. The Sinclair calculators were successful because they were far cheaper than the competition; however, their design led to slow and inaccurate computations of transcendental functions.
Meanwhile Hewlett Packard (HP) had been developing a pocket calculator. Launched in early 1972 it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with "scientific" functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, used reverse Polish notation (RPN), also called postfix notation. A calculation like "8 plus 5" is, using RPN, performed by pressing "8", "Enter↑", "5", and "+"; instead of the algebraic infix notation: "8", "+", "5", "=".
The first Soviet "scientific" pocket-sized calculator the "B3-18" was completed by the end of 1975.
In 1973, Texas Instruments (TI) introduced the SR-10, ("SR" signifying slide rule) an "algebraic entry" pocket calculator using scientific notation for $150. Shortly after the SR-11 featured an additional key for entering "π". It was followed the next year by the SR-50 which added log and trig functions to compete with the HP-35, and in 1977 the mass-marketed TI-30 line which is still produced.
In 1978 a new company, Calculated Industries, came onto the scene, focusing on specific markets. Their first calculator, the Loan Arranger (1978) was a pocket calculator marketed to the Real Estate industry with preprogrammed functions to simplify the process of calculating payments and future values. In 1985, CI launched a calculator for the construction industry called the Construction Master which came preprogrammed with common construction calculations (such as angles, stairs, roofing math, pitch, rise, run, and feet-inch fraction conversions). This would be the first in a line of construction related calculators.
Programmable calculators.
The first desktop "programmable calculators" were produced in the mid-1960s by Mathatronics and Casio (AL-1000). These machines were, however, very heavy and expensive. The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced "continuous memory", i.e. programs and data were retained in CMOS memory during power-off. In 1979, HP released the first "alphanumeric", programmable, "expandable" calculator, the HP-41C. It could be expanded with RAM (memory) and ROM (software) modules, as well as peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).
The first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the beginning of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika "B3-21", was developed by the end of 1977 and released at the beginning of 1978. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited capabilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.
This series of calculators was also noted for a large number of highly counter-intuitive mysterious undocumented features, somewhat similar to "synthetic programming" of the American HP-41, which were exploited by applying normal arithmetic operations to error messages, jumping to non-existent addresses and other techniques. A number of respected monthly publications, including the popular science magazine "Наука и жизнь" ("Science and Life"), featured special columns, dedicated to optimization techniques for calculator programmers and updates on undocumented features for hackers, which grew into a whole esoteric science with many branches, known as "yeggogology" ("еггогология"). The error messages on those calculators appear as a Russian word "YEGGOG" ("ЕГГОГ") which, unsurprisingly, is translated to "Error".
A similar hacker culture in the USA revolved around the HP-41, which was also noted for a large number of undocumented features and was much more powerful than B3-34.
Technical improvements.
Through the 1970s the hand-held electronic calculator underwent rapid development. The red LED and blue/green vacuum fluorescent displays consumed a lot of power and the calculators either had a short battery life (often measured in hours, so rechargeable nickel-cadmium batteries were common) or were large so that they could take larger, higher capacity batteries. In the early 1970s liquid-crystal displays (LCDs) were in their infancy and there was a great deal of concern that they only had a short operating lifetime. Busicom introduced the Busicom "LE-120A "HANDY"" calculator, the first pocket-sized calculator and the first with an LED display, and announced the Busicom "LC" with LCD display. However, there were problems with this display and the calculator never went on sale. The first successful calculators with LCDs were manufactured by Rockwell International and sold from 1972 by other companies under such names as: Dataking "LC-800", Harden "DT/12", Ibico "086", Lloyds "40", Lloyds "100", Prismatic "500" (aka "P500"), Rapid Data "Rapidman 1208LC". The LCDs were an early form using the "Dynamic Scattering Mode DSM" with the numbers appearing as bright against a dark background. To present a high-contrast display these models illuminated the LCD using a filament lamp and solid plastic light guide, which negated the low power consumption of the display. These models appear to have been sold only for a year or two.
A more successful series of calculators using a reflective DSM-LCD was launched in 1972 by Sharp Inc with the Sharp "EL-805", which was a slim pocket calculator. This, and another few similar models, used Sharp's "COS" (Calculator On Substrate) technology. An extension of one glass plate needed for the Liquid Crystal Display was used as a substrate to mount the required chips based on a new hybrid technology. The "COS" technology may have been too expensive since it was only used in a few models before Sharp reverted to conventional circuit boards.
In the mid-1970s the first calculators appeared with field-effect, "Twisted Nematic TN" LCDs with dark numerals against a grey background, though the early ones often had a yellow filter over them to cut out damaging ultraviolet rays. The advantage of LCDs is that they are passive light modulators reflecting light, which require much less power than light-emitting displays such as LEDs or VFDs. This led the way to the first credit-card-sized calculators, such as the Casio "Mini Card LC-78" of 1978, which could run for months of normal use on button cells.
There were also improvements to the electronics inside the calculators. All of the logic functions of a calculator had been squeezed into the first "Calculator on a chip" integrated circuits in 1971, but this was leading edge technology of the time and yields were low and costs were high. Many calculators continued to use two or more integrated circuits (ICs), especially the scientific and the programmable ones, into the late 1970s.
The power consumption of the integrated circuits was also reduced, especially with the introduction of CMOS technology. Appearing in the Sharp "EL-801" in 1972, the transistors in the logic cells of CMOS ICs only used any appreciable power when they changed state. The LED and VFD displays often required additional driver transistors or ICs, whereas the LCD displays were more amenable to being driven directly by the calculator IC itself.
With this low power consumption came the possibility of using solar cells as the power source, realised around 1978 by such calculators as the Royal "Solar 1", Sharp "EL-8026", and Teal "Photon".
A pocket calculator for everyone.
At the beginning of the 1970s hand-held electronic calculators were very expensive, costing two or three weeks' wages, and so were a luxury item. The high price was due to their construction requiring many mechanical and electronic components which were expensive to produce, and production runs were not very large. Many companies saw that there were good profits to be made in the calculator business with the margin on these high prices. However, the cost of calculators fell as components and their production techniques improved, and the effect of economies of scale was felt.
By 1976 the cost of the cheapest four-function pocket calculator had dropped to a few dollars, about one 20th of the cost five years earlier. The consequences of this were that the pocket calculator was affordable, and that it was now difficult for the manufacturers to make a profit out of calculators, leading to many companies dropping out of the business or closing down altogether. The companies that survived making calculators tended to be those with high outputs of higher quality calculators, or producing high-specification scientific and programmable calculators.
Mid-1980s to present.
The first calculator capable of symbolic computation was the HP-28C, released in 1987. It was able to, for example, solve quadratic equations symbolically. The first graphing calculator was the Casio FX-7000G released in 1985.
The two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.
The HP 12c financial calculator is still produced. It was introduced in 1981 and is still being made with few changes. The HP 12c featured the reverse Polish notation mode of data entry. In 2003 several new models were released, including an improved version of the HP 12c, the "HP 12c platinum edition" which added more memory, more built-in functions, and the addition of the algebraic mode of data entry.
Calculated Industries competed with the HP 12c in the mortgage and real estate markets by differentiating the key labeling; changing the “I”, “PV”, “FV” to easier labeling terms such as "Int", "Term", "Pmt", and not using the reverse Polish notation. However, CI's more successful calculators involved a line of construction calculators, which evolved and expanded in the 1990s to present. According to Mark Bollman, a mathematics and calculator historian and associate professor of mathematics at Albion College, the "Construction Master is the first in a long and profitable line of CI construction calculators" which carried them through the 1980s, 1990s, and to the present.
Personal computers often come with a calculator utility program that emulates the appearance and functionality of a calculator, using the graphical user interface to portray a calculator. One such example is Windows Calculator. Most personal data assistants (PDA) and smartphones also have such a feature.
Manufacturers.
These are some of the manufacturers which made a notable contribution to calculator development:

</doc>
<doc id="7594" url="http://en.wikipedia.org/wiki?curid=7594" title="Cash register">
Cash register

A cash register, also referred to as a till in the United Kingdom and other Commonwealth countries, is a mechanical or electronic device for registering and calculating transactions. It is usually attached to a drawer for storing cash and other valuables. The cash register is also usually attached to a printer, that can print out receipts for record keeping purposes.
History.
An early mechanical cash register was invented by James Ritty and John Birch following the American Civil War. James was the owner of a saloon in Dayton, Ohio, USA, and wanted to stop employees from pilfering his profits. The Ritty Model I was invented in 1879 after seeing a tool that counted the revolutions of the propeller on a steamship. With the help of James' brother John Ritty, they patented it in 1883. It was called "Ritty's Incorruptible Cashier" and it was invented for the purpose to stop cashiers of pilfering and eliminating employee theft or embezzlement.
Early mechanical registers were entirely mechanical, without receipts. The employee was required to ring up every transaction on the register, and when the total key was pushed, the drawer opened and a bell would ring, alerting the manager to a sale taking place. Those original machines were nothing but simple adding machines.
Since the registration is done with the process of returning change, according to Bill Bryson the odd pricing came about because by charging odd amounts like 49 and 99 cents (or 45 and 95 cents when Nickels are more used than Pennies), the cashier very probably had to open the till for the penny change and thus announce the sale.
Shortly after the patent, Ritty became overwhelmed with the responsibilities of running two businesses, so he sold all of his interests in the cash register business to Jacob H. Eckert of Cincinnati, a china and glassware salesman, who formed the National Manufacturing Company. In 1884 Eckert sold the company to John H. Patterson, who renamed the company the National Cash Register Company and improved the cash register by adding a paper roll to record sales transactions, thereby creating the journal for internal bookkeeping purposes, and the receipt for external bookkeeping purposes. The original purpose of the receipt was enhanced fraud protection. The business owner could read the receipts to ensure that cashiers charged customers the correct amount for each transaction and did not embezzle the cash drawer. It also prevents customer from defrauding the business by falsely claiming of receiving a less amount of change or a transaction that never happened in the first place.
In 1906, while working at the National Cash Register company, inventor Charles F. Kettering designed a cash register with an electric motor.
A leading designer, builder, manufacturer, seller and exporter of cash registers in the 1950s until the 1970s was London-based (and later Brighton-based) Gross Cash Registers Ltd., founded by brothers Sam and Henry Gross. Their cash registers were particularly popular around the time of decimalisation in Britain in early 1971, Henry having designed one of the few known models of cash register which could switch currencies from £sd to £p so that retailers could easily change from one to the other on or after Decimal Day. Sweda also had decimal ready registers where the retailer used a special key on decimal day for the conversion.
In current use.
In some jurisdictions the law also requires customers to collect the receipt and keep it at least for a short while after leaving the shop, again to check that the shop records sales, so that it cannot evade sales taxes.
Often cash registers are attached to scales, barcode scanners, checkstands, and debit card or credit card terminals. Increasingly, dedicated cash registers are being replaced with general purpose computers with POS software. Cash registers use bitmap characters for printing.
Today, point of sale systems scan the barcode (usually EAN or UPC) for each item, retrieve the price from a database, calculate deductions for items on sale (or, in British retail terminology, "special offer", "multibuy" or "buy one, get one free"), calculate the sales tax or VAT, calculate differential rates for preferred customers, actualize inventory, time and date stamp the transaction, record the transaction in detail including each item purchased, record the method of payment, keep totals for each product or type of product sold as well as total sales for specified periods, and do other tasks as well. These POS terminals will often also identify the cashier on the receipt, and carry additional information or offers.
Currently, many cash registers are individual computers. They may be running traditionally in-house software or general purpose software such as DOS. Many of the newer ones have touch screens. They may be connected to computerized Point of sale networks using any type of protocol. Such systems may be accessed remotely for the purpose of obtaining records or troubleshooting. Many businesses also use tablet computers as cash registers, utilizing the sale system as downloadable app-software.
Cash drawer.
Cash registers include a key labeled "No Sale", abbreviated into "NS" in modern electronic cash register. It functions is to open the drawer, printing a receipt stating "No Sale" and recording it in the register log that the register was opened. Some other cash registers require a numeric password or physical key to be used when attempting to open the till. 
A cash register's drawer can only be opened by an instruction from the cash register. Except when using special keys, generally hold by the owner and some employees (e.g. manager). This reduces the amount of contact most employees had with cash and other valuables, therefore freeing them from the responsibility of handling cash and valuables. It is also reduced risks of an employee taking money from the drawer without a record and the owner's consent, such as when a customer does not expressively ask for a receipt but still has to be given change (cash is more easily checked against recorded sales than inventory). 
A cash drawer is usually compartment underneath a cash register in which the cash from transactions is kept. The drawer typically contains a removable till. The till is usually a plastic or wooden tray divided into compartments used to store each denomination of bank notes and coins separately in order to make counting easier. The removable till allows money to be removed from the sales floor to a more secure location for counting and creating bank deposits. Some modern cash drawers are individual units separate from the rest of the cash register.
A cash drawer is usually of strong construction and may be integral with the register or a separate piece that the register sits atop. It slides in and out of its lockable box and is secured by a spring-loaded catch. When a transaction that involves cash is completed, the register sends an electrical impulse to a solenoid to release the catch and open the drawer.
Cash drawers that are integral to a stand-alone register often have a manual release catch underneath to open the drawer in the event of a power failure. More advanced cash drawers have eliminated the manual release in favor of a cylinder lock, requiring a key to manually open the drawer. The cylinder lock usually has several positions: locked, unlocked, online (will open if an impulse is given), and release. The release position is an intermittent position with a spring to push the cylinder back to the unlocked position. In the "locked" position, the drawer will remain latched even when an electric impulse is sent to the solenoid.
Due the increasing amount of notes and varieties of notes, many cash drawers have opted to store notes in a vertical side facing position instead of the traditional horizontal upward facing position. It enables faster access to each note and allows more varieties of notes to be stored. Sometimes the cashier will even divide the notes without any physical divider at all. Some cash drawers are also the Flip Top in design, where they flip open instead of sliding out like an ordinary drawer, resembling a cashbox instead.
Manual input.
Registers will typically feature a numerical pad, QWERTY or custom keyboard, touch screen interface, or a combination of these input methods for the cashier to enter products and fees by hand and access information necessary to complete the sale. For older registers as well as at restaurants and other establishments that do not sell barcoded items, the manual input may be the only method of interacting with the register. While customization was previously limited to larger chains that could afford to have physical keyboards custom-built for their needs, the customization of register inputs is now more widespread with the use of touch screens that can display a variety of point of sale software.
Scanner.
Modern cash registers may be connected to a handheld or stationary barcode reader so that a customer's purchases can be more rapidly scanned than would be possible by keying numbers into the register by hand. The use of scanners should also help prevent errors that result from manually entering the product's barcode or pricing. At grocers, the register's scanner may be combined with a scale for measuring product that is sold by weight.
Receipt printer.
Cashiers are often required to provide a receipt to the customer after a purchase has been made. Registers typically use thermal printers to print receipts, although older dot matrix printers are still in use at some retailers. Alternatively, retailers can forgo issuing paper receipts in some jurisdictions by instead asking the customer for an email to which their receipt can be sent. The receipts of larger retailers tend to include unique barcodes or other information identifying the transaction so that the receipt can be scanned to facilitate returns or other customer services.
Security deactivation.
In stores that use electronic article surveillance, a pad or other surface will be attached to the register that deactivates security devices embedded or attached to the items being purchased. This will prevent a customer's purchase from setting off security alarms at the store's exit.
Self Service Cash Register.
Some corporations and supermarkets have introduced self-checkout machines, where the customer is trusted to scan the barcodes (or manually identify uncoded items like fruit), and place the items into a bagging area. The bag is weighed, and the machine halts the checkout when the weight of something in the bag does not match the weight in the inventory database. Normally, an employee is watching over several such checkouts to prevent theft or exploitation of the machines' weaknesses (for example, intentional misidentification of expensive produce or dry goods). Payment on these machines is accepted by debit card/credit card, or cash via coin slot and bank note scanner. Store employees are also needed to authorize "age-restricted" purchases, such as alcohol, solvents or knives, which can either be done remotely by the employee observing the self-checkout, or by means of a "store login" which the operator has to enter.

</doc>
<doc id="7595" url="http://en.wikipedia.org/wiki?curid=7595" title="Chronometer">
Chronometer

Chronometer or Chronoscope may refer to:

</doc>
<doc id="7597" url="http://en.wikipedia.org/wiki?curid=7597" title="Processor design">
Processor design

Processor design is the design engineering task of creating a microprocessor, a component of computer hardware. It is a subfield of electronics engineering and computer engineering. The design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture described in e.g. VHDL or Verilog. This description is then manufactured employing some of the various semiconductor device fabrication processes. This results in a die which is bonded onto some chip carrier. This chip carrier is then soldered onto some printed circuit board (PCB).
The mode of operation of any microprocessor is the execution of lists of instructions, such as the addition or subtraction of registers.
Details.
CPU design focuses on six main areas:
CPUs designed for high-performance markets might require custom designs for each of these items to achieve frequency, power-dissipation, and chip-area goals whereas CPUs designed for lower performance markets might lessen the implementation burden by acquiring some of these items by purchasing them as intellectual property. Control logic implementation techniques (logic synthesis using CAD tools) can be used to implement datapaths, register files, and clocks. Common logic styles used in CPU design include unstructured random logic, finite-state machines, microprogramming (common from 1965 to 1985), and Programmable logic arrays (common in the 1980s, no longer common).
Device types used to implement the logic include:
A CPU design project generally has these major tasks:
Re-designing a CPU core to a smaller die-area helps to shrink everything (a "photomask shrink"), resulting in the same number of transistors on a smaller die. It improves performance (smaller transistors switch faster), reduces power (smaller wires have less parasitic capacitance) and reduces cost (more CPUs fit on the same wafer of silicon). Releasing a CPU on the same size die, but with a smaller CPU core, keeps the cost about the same but allows higher levels of integration within one very-large-scale integration chip (additional cache, multiple CPUs, or other components), improving performance and reducing overall system cost.
As with most complex electronic designs, the logic verification effort (proving that the design does
not have bugs) now dominates the project schedule of a CPU.
Key CPU architectural innovations include index register, cache, virtual memory, instruction pipelining, superscalar, CISC, RISC, virtual machine, emulators, microprogram, and stack.
Research topics.
A variety of have been proposed,
including reconfigurable logic, clockless CPUs, computational RAM, and optical computing.
Performance analysis and benchmarking.
Benchmarking is a way of testing CPU speed. Examples include SPECint and SPECfp, developed by Standard Performance Evaluation Corporation, and ConsumerMark developed by the Embedded Microprocessor Benchmark Consortium EEMBC.
Measurements include:
Some of these measures conflict. In particular, many design techniques that make a CPU run faster make the "performance per watt", "performance per dollar", and "deterministic response" much worse, and vice versa.
Markets.
There are several different markets in which CPUs are used. Since each of these markets differ in their requirements for CPUs, the devices designed for one market are in most cases inappropriate for the other markets.
General purpose computing.
The vast majority of revenues generated from CPU sales is for general purpose computing, that is, desktop, laptop, and server computers commonly used in businesses and homes. In this market, the Intel IA-32 architecture dominates, with its rivals PowerPC and SPARC maintaining much smaller customer bases. Yearly, hundreds of millions of IA-32 architecture CPUs are used by this market. A growing percentage of these processors are for mobile implementations such as netbooks and laptops.
Since these devices are used to run countless different types of programs, these CPU designs are not specifically targeted at one type of application or one function. The demands of being able to run a wide range of programs efficiently has made these CPU designs among the more advanced technically, along with some disadvantages of being relatively costly, and having high power consumption.
High-end processor economics.
In 1984, most high-performance CPUs required four to five years to develop.
Scientific computing.
Scientific computing is a much smaller niche market (in revenue and units shipped). It is used in government research labs and universities. Before 1990, CPU design was often done for this market, but mass market CPUs organized into large clusters have proven to be more affordable. The main remaining area of active hardware design and research for scientific computing is for high-speed data transmission systems to connect mass market CPUs.
Embedded design.
As measured by units shipped, most CPUs are embedded in other machinery, such as telephones, clocks, appliances, vehicles, and infrastructure. Embedded processors sell in the volume of many billions of units per year, however, mostly at much lower price points than that of the general purpose processors.
These single-function devices differ from the more familiar general-purpose CPUs in several ways:
Embedded processor economics.
The embedded CPU family with the largest number of total units shipped is the 8051, averaging nearly a billion units per year. The 8051 is widely used because it is very inexpensive. The design time is now roughly zero, because it is widely available as commercial intellectual property. It is now often embedded as a small part of a larger system on a chip. The silicon cost of an 8051 is now as low as US$0.001, because some implementations use as few as 2,200 logic gates and take 0.0127 square millimeters of silicon.
As of 2009, more CPUs are produced using the ARM architecture instruction set than any other 32-bit instruction set.
The ARM architecture and the first ARM chip were designed in about one and a half years and 5 human years of work time.
The 32-bit Parallax Propeller microcontroller architecture and the first chip were designed by two people in about 10 human years of work time.
The 8-bit AVR architecture and first AVR microcontroller was conceived and designed by two students at the Norwegian Institute of Technology.
The 8-bit 6502 architecture and the first MOS Technology 6502 chip were designed in 13 months by a group of about 9 people.
Research and educational CPU design.
The 32 bit Berkeley RISC I and RISC II architecture and the first chips were mostly designed by a series of students as part of a four quarter sequence of graduate courses.
This design became the basis of the commercial SPARC processor design.
For about a decade, every student taking the 6.004 class at MIT was part of a team—each team had one semester to design and build a simple 8 bit CPU out of 7400 series integrated circuits.
One team of 4 students designed and built a simple 32 bit CPU during that semester.
Some undergraduate courses require a team of 2 to 5 students to design, implement, and test a simple CPU in a FPGA in a single 15 week semester.
Soft microprocessor cores.
For embedded systems, the highest performance levels are often not needed or desired due to the power consumption requirements. This allows for the use of processors which can be totally implemented by logic synthesis techniques. These synthesized processors can be implemented in a much shorter amount of time, giving quicker time-to-market.

</doc>
<doc id="7598" url="http://en.wikipedia.org/wiki?curid=7598" title="Carinatae">
Carinatae

"This article is about bird taxonomy; for the topic in pottery and glassware design, see Carinate."
The Carinatae are, in phylogenetic taxonomy, the last common ancestor of the Aves or Neornithes (modern birds) and "Ichthyornis" (an extinct seabird of the Cretaceous), and all its descendants. Defined in this way, the group includes all modern birds, both living and recently extinct, and a few Mesozoic forms.
Classification.
Early definitions.
Traditionally, Carinatae were defined as all birds whose sternum (breast bone) has a keel ("carina"). The keel is a strong median ridge running down the length of the sternum. This is an important area for the attachment of flight muscles. Thus, all flying birds have a pronounced keel. Ratites, all of whom are flightless, lack a strong keel. Thus, living birds were divided into carinates (keeled) and ratites (from "ratis", "raft", referring to the flatness of the sternum). 
The difficulty with this scheme phylogenetically was that some flightless birds, without strong carinae, are descended directly from ordinary flying birds with carinae. Examples include the Kakapo, a flightless parrot, and the dodo, a columbiform (the pigeon family). None of these birds are ratites. Thus, this supposedly distinctive feature was easy to use, but had nothing to do with actual phylogenic relationship.
Current definition.
The use of a term for keeled sternum to describe the Ichthyornis–Neornithine group turned out to be equally inapt. Various dinosaurs – apparently, remote ancestors and cousins of the Carinatae – "do" possess a keeled sternum. So, evidently the presence of this structure does not necessarily imply its use in flight. This sort of definitional problem is one reason why the use of physical characteristics to name taxonomic groups is now discouraged. 
The characteristics that actually are unique to the Carinatae have little to do with the sternum. Rather, carinates are unique in having, for example, a globe-shaped, convex head on the humerus and fully fused bones in the lower leg and outer arm.
They also have a pterygoid bone that articulates with the palatine by means of a joint. The vomer is reduced or absent.
Relationships.
The cladogram below follows a 2004 analysis by Julia Clarke..
<br>

</doc>
<doc id="7599" url="http://en.wikipedia.org/wiki?curid=7599" title="Cocktail">
Cocktail

When used to refer to any generic alcoholic mixed drink, cocktail may mean any beverage that contains two or more ingredients if at least one of them contains alcohol.
Usage and related terms.
When a cocktail contains only a distilled spirit and a mixer, such as soda or fruit juice, it is a highball; many of the International Bartenders Association Official Cocktails are highballs. When a cocktail contains only a distilled spirit and a liqueur, it is a duo and when it adds a mixer, it is a trio. Additional ingredients may be sugar, honey, milk, cream, and various herbs.
Etymology.
The origin of the word "cocktail" is disputed.
The first recorded use of the word "cocktail" not referring to a horse is found in "The Morning Post and Gazetteer in London, England" on March 20, 1798:
<poem>
Mr. Pitt,
two petit vers of “L’huile de Venus”
Ditto, one of “perfeit amour”
Ditto, “cock-tail” (vulgarly called ginger)
</poem>
The Oxford English dictionary cites the word as originating in the U.S. The first recorded use of the word "cocktail" as a beverage (possibly non-alcoholic) in the United States appears in "The Farmer's Cabinet" on April 28, 1803:
The first definition of "cocktail" known to be an alcoholic beverage appeared in the May 13, 1806, edition of "The Balance and Columbian Repository", a publication in Hudson, New York, in which an answer was provided to the question, "What is a cocktail?". The editor Harry Croswell replied:
Development.
There is a lack of clarity on the origins of cocktails. Traditionally cocktails were a mixture of spirits, sugar, water, and bitters. But by the 1860s, a cocktail frequently included a liqueur.
The first publication of a bartenders' guide which included cocktail recipes was in 1862 — "How to Mix Drinks; or, The Bon Vivant's Companion", by "Professor" Jerry Thomas. In addition to listings of recipes for punches, sours, slings, cobblers, shrubs, toddies, flips, and a variety of other types of mixed drinks were 10 recipes for drinks referred to as "cocktails". A key ingredient which differentiated cocktails from other drinks in this compendium was the use of bitters as an ingredient. Mixed drinks popular today that conform to this original meaning of "cocktail" include the Old Fashioned whiskey cocktail, the Sazerac cocktail, and the Manhattan cocktail. 
The ingredients listed (spirits, sugar, water, and bitters) match the ingredients of an Old Fashioned, which originated as a term used by late 19th century bar patrons to distinguish cocktails made the “old-fashioned” way from newer, more complex cocktails.
The term highball appears during the 1890s to distinguish a drink composed only of a distilled spirit and a mixer.
The first "cocktail party" ever thrown was allegedly by Mrs. Julius S. Walsh Jr. of St. Louis, Missouri, in May 1917. Mrs. Walsh invited 50 guests to her home at noon on a Sunday. The party lasted an hour, until lunch was served at 1 pm. The site of this first cocktail party still stands. In 1924, the Roman Catholic Archdiocese of St. Louis bought the Walsh mansion at 4510 Lindell Boulevard, and it has served as the local archbishop's residence ever since.
During Prohibition in the United States (1919–1933), when alcoholic beverages were illegal, cocktails were still consumed illegally in establishments known as speakeasies. The quality of liquor available during Prohibition was much worse than previously. There was a shift from whiskey to gin, which does not require aging and is therefore easier to produce illicitly. Honey, fruit juices, and other flavorings served to mask the foul taste of the inferior liquors. Sweet cocktails were easier to drink quickly, an important consideration when the establishment might be raided at any moment.
Cocktails became less popular in the late 1960s and through the 1970s, until resurging in the 1980s with vodka often substituting the original gin in drinks such as the martini. Traditional cocktails began to make a comeback in the 2000s and by the mid-2000s there was a renaissance of cocktail culture in a style typically referred to as mixology that draws on traditional cocktails for inspiration but utilizes novel ingredients and often complex flavors.
See also.
Lists
Devices for Producing and Imbibing
Media

</doc>
<doc id="7601" url="http://en.wikipedia.org/wiki?curid=7601" title="Coptic Orthodox Church of Alexandria">
Coptic Orthodox Church of Alexandria

The Coptic Orthodox Church of Alexandria is the official name for the largest Christian church in Egypt and the Middle East.
According to tradition, the church was established by Saint Mark, an apostle and evangelist, in the middle of the 1st century (approximately AD 42). The head of the church and the See of Alexandria is the Patriarch of Alexandria on the Holy See of Saint Mark. The See of Alexandria is titular and nowadays the Coptic Pope seat is Saint Mark's Coptic Orthodox Cathedral in the Abbassia District in Cairo.
The Church belongs to the Oriental Orthodox family of churches, which has been a distinct church body since the Council of Chalcedon in AD 451, when it took a different position over Christological theology from that of the Eastern Orthodox Church. The precise differences in theology that caused the split with the Coptic Christians are still disputed, highly technical, and mainly concerned with the nature of Christ. The foundational roots of the Church are based in Egypt, but it has a worldwide following.
As of 2012, about 10% of Egyptians belonged to the Coptic Orthodox Church of Alexandria.
History.
Apostolic foundation.
Egypt is identified in the Bible as the place of refuge that the Holy Family sought in its flight from Judea: When he [Joseph] arose, he took the young Child and His mother by night and departed for Egypt, and was there until the death of Herod the Great, that it might be fulfilled which was spoken by the Lord through the prophet, saying, "Out of Egypt I called My Son" (Matthew 2:12–23).
The Egyptian Church, which is more than 1,900 years old, regards itself as the subject of many prophecies in the Old Testament. Isaiah the prophet, in Chapter 19, Verse 19 says "In that day there will be an altar to the LORD in the midst of the land of Egypt, and a pillar to the LORD at its border."
The first Christians in Egypt were common people who spoke Egyptian Coptic. There were also Alexandrian Jews such as Theophilus, whom Saint Luke the Evangelist addresses in the introductory chapter of his gospel. When the church was founded by Saint Mark during the reign of the Roman emperor Nero, a great multitude of native Egyptians (as opposed to Greeks or Jews) embraced the Christian faith.
Christianity spread throughout Egypt within half a century of Saint Mark's arrival in Alexandria, as is clear from the New Testament writings found in Bahnasa, in Middle Egypt, which date around the year AD 200, and a fragment of the Gospel of John, written in Coptic, which was found in Upper Egypt and can be dated to the first half of the 2nd century. In the 2nd century, Christianity began to spread to the rural areas, and scriptures were translated into the local languages, namely Coptic.
Contributions to Christianity.
The Catechetical School of Alexandria.
The Catechetical School of Alexandria is the oldest catechetical school in the world. St. Jerome records that the Christian School of Alexandria was founded by Saint Mark himself. Around 190 AD under the leadership of the scholar Pantanaeus, the school of Alexandria became an important institution of religious learning, where students were taught by scholars such as Athenagoras, Clement, Didymus, and the native Egyptian Origen, who was considered the father of theology and who was also active in the field of commentary and comparative Biblical studies. Origen wrote over 6,000 commentaries of the Bible in addition to his famous Hexapla.
Many scholars such as Jerome visited the school of Alexandria to exchange ideas and to communicate directly with its scholars. The scope of this school was not limited to theological subjects; science, mathematics and humanities were also taught there. The question-and-answer method of commentary began there, and 15 centuries before Braille, wood-carving techniques were in use there by blind scholars to read and write.
The Theological college of the catechetical school was re-established in 1893. The new school currently has campuses in Ireland, Cairo, New Jersey, and Los Angeles, where Coptic priests-to-be and other qualified men and women are taught among other subjects Christian theology, history, the Coptic language and art – including chanting, music, iconography, and tapestry.
The cradle of monasticism and its missionary work.
Many Egyptian Christians went to the desert during the 3rd century, and remained there to pray and work and dedicate their lives to seclusion and worship of God. This was the beginning of the monastic movement, which was organized by Anthony the Great, Saint Paul, the world's first anchorite, Saint Macarius the Great and Saint Pachomius the Cenobite in the 4th century.
Christian monasticism was born in Egypt and was instrumental in the formation of the Coptic Orthodox Church character of submission, simplicity and humility, thanks to the teachings and writings of the Great Fathers of Egypt's Deserts. By the end of the 5th century, there were hundreds of monasteries, and thousands of cells and caves scattered throughout the Egyptian desert. A great number of these monasteries are still flourishing and have new vocations to this day.
All Christian monasticism stems, either directly or indirectly, from the Egyptian example: Saint Basil the Great Archbishop of Caesaria of Cappadocia, founder and organizer of the monastic movement in Asia Minor, visited Egypt around AD 357 and his rule is followed by the Eastern Orthodox Churches; Saint Jerome who translated the Bible into Latin, came to Egypt, while en route to Jerusalem, around AD 400 and left details of his experiences in his letters; Benedict founded the Benedictine Order in the 6th century on the model of Saint Pachomius, but in a stricter form. Countless pilgrims have visited the "Desert Fathers" to emulate their spiritual, disciplined lives.
Role and participation in the Ecumenical Councils.
Council of Nicea.
In the 4th century, an Alexandrian presbyter named Arius began a theological dispute about the nature of Christ that spread throughout the Christian world and is now known as Arianism. The Ecumenical Council of Nicea AD 325 was convened by Constantine under the presidency of Saint Hosius of Cordova and Pope Saint Alexander I of Alexandria to resolve the dispute and eventually led to the formulation of the Symbol of Faith, also known as the Nicene Creed. The Creed, which is now recited throughout the Christian world, was based largely on the teaching put forth by a man who eventually would become Pope Saint Athanasius of Alexandria, the chief opponent of Arius.
Council of Constantinople.
In the year AD 381, Pope Timothy I of Alexandria presided over the second ecumenical council known as the Ecumenical Council of Constantinople, to judge Macedonius, who denied the Divinity of the Holy Spirit. This council completed the Nicene Creed with this confirmation of the divinity of the Holy Spirit:
We believe in the Holy Spirit, the Lord, the Giver of Life, who proceeds from the Father, who with the Father through the Son is worshiped and glorified who spoke by the Prophets and in One, Holy, Catholic, and Apostolic church. We confess one Baptism for the remission of sins and we look for the resurrection of the dead and the life of the coming age, Amen.
Council of Ephesus.
Another theological dispute in the 5th century occurred over the teachings of Nestorius, the Patriarch of Constantinople who taught that God the Word was not hypostatically joined with human nature, but rather dwelt in the man Jesus. As a consequence of this, he denied the title "Mother of God" "(Theotokos)" to the Virgin Mary, declaring her instead to be "Mother of Christ" "Christotokos".
When reports of this reached the Apostolic Throne of Saint Mark, Pope Saint Cyril I of Alexandria acted quickly to correct this breach with orthodoxy, requesting that Nestorius repent. When he would not, the Synod of Alexandria met in an emergency session and a unanimous agreement was reached. Pope Cyril I of Alexandria, supported by the entire See, sent a letter to Nestorius known as "The Third Epistle of Saint Cyril to Nestorius." This epistle drew heavily on the established Patristic Constitutions and contained the most famous article of Alexandrian Orthodoxy: "The Twelve Anathemas of Saint Cyril." In these anathemas, Cyril excommunicated anyone who followed the teachings of Nestorius. For example, "Anyone who dares to deny the Holy Virgin the title "Theotokos" is Anathema!" Nestorius however, still would not repent and so this led to the convening of the First Ecumenical Council of Ephesus (431), over which Cyril presided.
The Council confirmed the teachings of Saint Athanasius and confirmed the title of Mary as "Mother of God". It also clearly stated that anyone who separated Christ into two hypostases was anathema, as Athanasius had said that there is "One Nature and One Hypostasis for God the Word Incarnate" ("Mia Physis tou Theou Loghou Sesarkomeni"). Also, the introduction to the creed was formulated as follows:
We magnify you O Mother of the True Light and we glorify you O saint and Mother of God "(Theotokos)" for you have borne unto us the Saviour of the world. Glory to you O our Master and King: Christ, the pride of the Apostles, the crown of the martyrs, the rejoicing of the righteous, firmness of the churches and the forgiveness of sins. We proclaim the Holy Trinity in One Godhead: we worship Him, we glorify Him, Lord have mercy, Lord have mercy, Lord bless us, Amen. [not dissimilar to the "Axion Estin" Chant still used in Orthodoxy]
Council of Chalcedon.
When in AD 451, Emperor Marcianus attempted to heal divisions in the Church, the response of Pope Dioscorus – the Pope of Alexandria who was later exiled – was that the emperor should not intervene in the affairs of the Church. It was at Chalcedon that the emperor, through the Imperial delegates, enforced harsh disciplinary measures against Pope Dioscorus in response to his boldness.
The Council of Chalcedon, from the perspective of the Alexandrine Christology, has deviated from the approved Cyrillian terminology and declared that Christ was one hypostasis in two natures. However, in the Nicene-Constantinopolitan Creed, "Christ was conceived of the Holy Spirit and of the Virgin Mary," thus the foundation of the definition according to the Non-Chalcedonian adherents, according to the Christology of Cyril of Alexandria is valid. There is a change in the Non-Chalcedonian definition here, as the Nicene creed clearly uses the terms "of", rather than "in".
In terms of Christology, the Oriental Orthodox (Non-Chalcedonians) understanding is that Christ is "One Nature—the Logos Incarnate," "of" the full humanity and full divinity. The Chalcedonians' understanding is that Christ is "in" two natures, full humanity and full divinity. Just as humans are of their mothers and fathers and not in their mothers and fathers, so too is the nature of Christ according to Oriental Orthodoxy. If Christ is in full humanity and in full divinity, then He is separate in two persons as the Nestorians teach. This is the doctrinal perception that makes the apparent difference which separated the Oriental Orthodox from the Eastern Orthodox.
The Council's findings were rejected by many of the Christians on the fringes of the Byzantine Empire, including Egyptians, Syrians, Armenians, and others.
From that point onward, Alexandria would have two patriarchs: the non-Chalcedonian native Egyptian one, now known as the Coptic Pope of Alexandria and Patriarch of All Africa on the Holy Apostolic See of St. Mark and the "Melkite" or Imperial Patriarch, now known as the Greek Orthodox Patriarch of Alexandria.
Almost the entire Egyptian population rejected the terms of the Council of Chalcedon and remained faithful to the native Egyptian Church (now known as the Coptic Orthodox Church of Alexandria). Those who supported the Chalcedonian definition remained in communion with the other leading churches of Rome and Constantinople. The non-Chalcedonian party became what is today called the Oriental Orthodox Church.
The Coptic Orthodox Church of Alexandria regards itself as having been misunderstood at the Council of Chalcedon. There was an opinion in the Church that viewed that perhaps the Council understood the Church of Alexandria correctly, but wanted to curtail the existing power of the Alexandrine Hierarch, especially after the events that happened several years before at Constantinople from Pope Theophilus of Alexandria towards Patriarch John Chrysostom and the unfortunate turnouts of the Second Council of Ephesus in AD 449, where Eutichus misled Pope Dioscorus and the Council in confessing the Orthodox Faith in writing and then renouncing it after the Council, which in turn, had upset Rome, especially that the Tome which was sent was not read during the Council sessions.
To make things even worse, the Tome of Pope Leo of Rome was, according to the Alexandria School of Theology, particularly in regards to the definition of Christology, considered influenced by Nestorian heretical teachings. So, due to the above-mentioned, especially in the consecutive sequences of events, the Hierarchs of Alexandria were considered holding too much of power from one hand, and on the other hand, due to the conflict of the Schools of Theology, there would be an impasse and a scapegoat, i.e. Pope Dioscorus. The Tome of Leo has been widely criticized (surprisingly by Roman Catholic and Eastern Orthodox scholars) in the past 50 years as a much less than perfect orthodox theological doctrine.
It is also to be noted that by anathemizing Pope Leo because of the tone and content of his tome, as per Alexandrine Theology perception, Pope Dioscorus was found guilty of doing so without due process; in other words, the Tome of Leo was not a subject of heresy in the first place, but it was a question of questioning the reasons behind not having it either acknowledged or read at the Second Council of Ephesus in AD 449. Pope Dioscorus of Alexandria was never labeled as heretic by the council's canons.
Copts also believe that the Pope of Alexandria was forcibly prevented from attending the third congregation of the council from which he was ousted, apparently the result of a conspiracy tailored by the Roman delegates.
Before the current positive era of Eastern and Oriental Orthodox dialogues, Chalcedonians sometimes used to call the non-Chalcedonians "monophysites", though the Coptic Orthodox Church in reality regards monophysitism as a heresy. The Chalcedonian doctrine in turn came to be known as "dyophysite".
A term that comes closer to Coptic Orthodoxy is miaphysite, which refers to a conjoined nature for Christ, both human and divine, united indivisibly in the Incarnate Logos. The Coptic Orthodox Church of Alexandria believes that Christ is perfect in His divinity, and He is perfect in His humanity, but His divinity and His humanity were united in one nature called "the nature of the incarnate word", which was reiterated by Saint Cyril of Alexandria.
Copts, thus, believe in two natures "human" and "divine" that are united in one hypostasis "without mingling, without confusion, and without alteration". These two natures "did not separate for a moment or the twinkling of an eye" (Coptic Liturgy of Saint Basil of Caesarea).
From Chalcedon to the Arab conquest of Egypt.
Copts suffered under the rule of the Byzantine Eastern Roman Empire. The Melkite Patriarchs, appointed by the emperors as both spiritual leaders and civil governors, massacred the Egyptian population whom they considered heretics. Many Egyptians were tortured and martyred to accept the terms of Chalcedon, but Egyptians remained loyal to the faith of their fathers and to the Cyrillian view of Christology. One of the most renowned Egyptian saints of that period is Saint Samuel the Confessor.
Muslim conquest of Egypt.
The Muslim invasion of Egypt took place in AD 639. Despite the political upheaval, the Egyptian population remained mainly Christian. However, the gradual conversions to Islam over the centuries changed Egypt from a Christian to a largely Muslim country by the end of the 12th century. Egypt's Umayyad rulers taxed Christians at a higher rate, driving merchants towards Islam and undermining the economic base of the Coptic Church. Although the Coptic Church did not disappear, the Umayyad tax policies made it difficult for the church to retain elites.
From the 19th century to the 1952 revolution.
The position of the Copts began to improve early in the 19th century under the stability and tolerance of the Muhammad Ali Dynasty. The Coptic community ceased to be regarded by the state as an administrative unit. In 1855 the "jizya" tax was abolished. Shortly thereafter, the Copts started to serve in the Egyptian army.
Towards the end of the 19th century, the Coptic Church underwent phases of new development. In 1853, Pope Cyril IV established the first modern Coptic schools, including the first Egyptian school for girls. He also founded a printing press, which was the second national press in the country. Pope Cyril IV established very friendly relations with other denominations, to the extent that when the Greek Patriarch in Egypt had to absent himself for a long period of time outside the country, he left his Church under the guidance of the Coptic Patriarch.
The Theological College of the School of Alexandria was reestablished in 1893. It began its new history with five students, one of whom was later to become its dean. Today it has campuses in Alexandria, Cairo, and various dioceses throughout Egypt, as well as outside Egypt, in New Jersey, Los Angeles, Sydney, Melbourne and London, where potential clergymen and other qualified men and women are taught many subjects, among which are theology, church history, missionary studies, and Coptic language.
Present day.
On 4 November 2012, Bishop Tawadros was chosen as the 118th Pope of Alexandria and the Patriarch of All Africa on the Holy See of Saint Mark. His predecessor was Pope Shenouda III, who died on 17 March 2012.
There are about 18 million Coptic Orthodox Christians in the world. Between 10 and 14 million of them are found in Egypt under the jurisdiction of the Coptic Orthodox Church of Alexandria.<ref name="The world factbook/Egypt/"></ref> Estimates of the size of Egypt's Christian population vary from the low government figures of 6 to 7 million to the 12 million reported by some Christian leaders. The actual numbers may be in the 11 to 13 million range, out of an Egyptian population of more than 85 million.<ref name="FCO/Egypt/"></ref>
There are also significant numbers in the diaspora in countries such as the United States of America, Canada, Australia, France, Germany, and Sudan. The number of Coptic Orthodox Christians in the diaspora is roughly 4 million. In addition, there are between 350,000 and 400,000 native African adherents in East, Central and South Africa, most in Sudan. Although under the jurisdiction of the Coptic Orthodox Church, these adherents are not considered Copts, since they are not ethnic Egyptians. Some accounts regard members of the Ethiopian Orthodox Tewahedo Church (roughly 45 million), the Eritrean Orthodox Tewahedo Church (roughly 2.5 million), as members of the Coptic Orthodox Church. This is however a misnomer, since both the Ethiopian and the Eritrean Churches, although daughter churches of the Church of Alexandria, are currently autocephalous churches. In 1959, the Ethiopian Orthodox Tewahedo Church was granted its first own Patriarch by Pope Cyril VI. Furthermore, the Eritrean Orthodox Tewahedo Church similarly became independent of the Ethiopian Orthodox Tewahedo Church in 1994, when four bishops were consecrated by Pope Shenouda III of Alexandria to form the basis of a local Holy Synod of the Eritrean Church. In 1998, the Eritrean Orthodox Tewahedo Church gained its autocephelacy from the Coptic Orthodox Church when its first Patriarch was enthroned by Pope Shenouda III of Alexandria.
These three churches remain in full communion with each other and with the other Oriental Orthodox churches. The Ethiopian Orthodox Tewahedo Church and the Eritrean Orthodox Tewahedo Church do acknowledge the Honorary Supremacy of the Coptic Orthodox Patriarch of Alexandria, since the Church of Alexandria is technically their Mother Church. Upon their selection, both Patriarchs (Ethiopian & Eritrean) must receive the approval and communion from the Holy Synod of the Apostolic See of Alexandria before their enthronement.
Since the 1980s theologians from the Oriental (non-Chalcedonian) Orthodox and Eastern (Chalcedonian) Orthodox churches have been meeting in a bid to resolve theological differences, and have concluded that many of the differences are caused by the two groups using different terminology to describe the same thing (see ).
In the summer of 2001, the Coptic Orthodox and Greek Orthodox Patriarchates of Alexandria agreed to mutually recognize baptisms performed in each other's churches, making re-baptisms unnecessary, and to recognize the sacrament of marriage as celebrated by the other. Previously, if a Coptic Orthodox and Greek Orthodox wanted to get married, the marriage had to be performed twice, once in each church, for it to be recognized by both. Now it can be done in only one church and be recognized by both.
According to Christian Tradition and Canon Law, the Coptic Orthodox Church of Alexandria only ordains men to the priesthood and episcopate, and if they wish to be married, they must be married before they are ordained. In this respect they follow the same practices as does the Eastern Orthodox Church.
Traditionally, the Coptic language was used in church services, and the scriptures were written in the Coptic alphabet. However, due to the Arabisation of Egypt, service in churches started to witness increased use of Arabic, while preaching is done entirely in Arabic. Native languages are used, in conjunction with Coptic, during services outside of Egypt.
Coptic Orthodox Christians celebrate Christmas on 7 January (Gregorian Calendar), which coincides with 25 December according to the Julian Calendar. The Coptic Orthodox Church uses the Julian Calendar as its Ecclesiastical Calendar. It is known as the Coptic calendar or the Alexandrian Calendar. This calendar is in turn based on the old Egyptian calendar of Ancient Egypt. The Coptic Orthodox Church is thus considered an Old Calendrist Church. Christmas according to the Coptic calendar was adopted as an official national holiday in Egypt since 2002.
Current events.
A 2010 New Year's Eve attack by Islamic fundamentalists on the Coptic Orthodox Church in the city of Alexandria left 21 dead and many more injured. One week later, thousands of Muslims stood as human shields outside churches as Coptic Christians attended Christmas Masses on 6 & 7 January 2011.
On 30 Jan, just days after the demonstrations to reform the Egyptian government, Muslims in southern Egypt broke into two homes belonging to Coptic Christians. The Muslim assailants murdered 11 people and wounded four others.
In Tahrir Square, Cairo, on Wednesday 2 February 2011, Coptic Christians joined hands to provide a protective cordon around their Muslim neighbors during salat (prayers) in the midst of the 2011 Egyptian Revolution.
On 4 October 2011, military and police squads used force late at night to disperse hundreds of angry Coptic demonstrators and their supporters who were attempting to stage a sit-in outside the Maspero TV headquarters in downtown Cairo to protest attacks on a Christian church in Upper Egypt.
On 17 March 2012 the Coptic Orthodox Pope, Pope Shenouda III died leaving many Copts mourning and worrying as tensions rise with Muslims. Pope Shenouda III constantly met with Muslim leaders in order to create peace. Many now worry of Muslims controlling Egypt as the Muslim Brotherhood won 70% of the parliamentary elections.
On 4 November 2012, Bishop Tawadros was chosen as the 118th Pope. In a ritual filled with prayer, chants and incense at Abbasiya cathedral in Cairo, the 60-year-old bishop's name was picked by a blindfolded child from a glass bowl in which the names of two other candidates had also been placed. The enthronement was scheduled on 18 November 2012.
Jurisdiction outside of Egypt.
Besides Egypt, the Church of Alexandria has jurisdiction over Pentapolis, Libya, Nubia, Sudan, Ethiopia, Eritrea and all Africa.
Both the Patriarchate of Addis Ababa and all Ethiopia, and the Patriarchate of Asmara and all Eritrea do acknowledge the supremacy of honor and dignity of the Pope and Patriarch of Alexandria on the basis that both Patriarchates were established by the Throne of Alexandria and that they have their roots in the Apostolic Church of Alexandria, and acknowledge that Saint Mark the Apostle is the founder of their Churches through the heritage and Apostolic evangelization of the Fathers of Alexandria.
In other words, the Patriarchates of Ethiopia and Eritrea are daughter Churches of the Holy Apostolic Patriarchate of Alexandria.
In addition to the above, the countries of Uganda, Kenya, Tanzania, Zambia, Zimbabwe, the Congo, Cameroon, Nigeria, Ghana, Botswana, Malawi, Angola, Namibia and South Africa are under the jurisdiction and the evangelization of the Throne of Alexandria. It is still expanding in the continent of Africa.
Daughter churches.
Ethiopian Orthodox Tewahedo Church.
Ethiopia received Christianity next to Jerusalem only a year after Jesus was crucified through its own apostle (Acts 8: 26-39). Since Christianity became a national religion of Ethiopia in the 4th century, the Church of Ethiopia became under the dominion of the Church of Alexandria until 1950. The first bishop of Ethiopia, Saint Frumentius, was consecrated as Bishop of Axum by Pope Athanasius of Alexandria in 328 AD. From then on, until 1959, the Pope of Alexandria, as Patriarch of All Africa, always named an Egyptian (a Copt) to be the Archbishop of the Ethiopian Church. On 13 July 1948, the Coptic Church of Alexandria and the Ethiopian Orthodox Tewahido Church reached an agreement concerning the relationship between the two churches. In 1950, the Ethiopian Orthodox Tewahido Church was granted autocephaly by Pope Joseph II of Alexandria, head of the Coptic Orthodox Church. Five Ethiopian bishops were immediately consecrated by the Pope of Alexandria and Patriarch of All Africa, and were empowered to elect a new Patriarch for their church. This promotion was completed when Joseph II consecrated the first Ethiopian-born Archbishop, Abuna Basilios, as head of the Ethiopian Church on 14 January 1951. In 1959, Pope Cyril VI of Alexandria crowned Abuna Basilios as the first Patriarch of Ethiopia.
Patriarch Basilios died in 1971, and was succeeded on the same year by Abuna Theophilos. With the fall of Emperor Haile Selassie I of Ethiopia in 1974, the new Marxist government arrested Abuna Theophilos and secretly executed him in 1979. The Ethiopian government then ordered the Ethiopian Church to elect Abuna Takla Haymanot as Patriarch of Ethiopia. The Coptic Orthodox Church refused to recognize the election and enthronement of Abuna Takla Haymanot on the grounds that the Synod of the Ethiopian Church had not removed Abuna Theophilos, and that the Ethiopian government had not publicly acknowledged his death, and he was thus still legitimate Patriarch of Ethiopia. Formal relations between the two churches were halted, although they remained in communion with each other.
After the death of Abuna Takla Haymanot in 1988, Abune Merkorios who had close ties to the Derg (Communist) government was elected Patriarch of Ethiopia. Following the fall of the Derg regime in 1991, Abune Merkorios abdicated under public and governmental pressure and went to exile in the United States. The newly elected Patriarch, Abune Paulos was officially recognized by the Coptic Orthodox Church of Alexandria in 1992 as the legitimate Patriarch of Ethiopia. Formal relations between the Coptic Church of Alexandria and the Ethiopian Orthodox Tewahedo Church were resumed on 13 July 2007.
His Holiness Abune Paulos died in August 2012 and transitionally a new pope is elected until an election is made next year.
Eritrean Orthodox Tewahedo Church.
Following the independence of Eritrea from Ethiopia in 1993, the newly independent Eritrean government appealed to Pope Shenouda III of Alexandria for Eritrean Orthodox autocephaly. In 1994, Pope Shenouda ordained Abune Phillipos as first Archbishop of Eritrea. The Eritrean Orthodox Tewahedo Church obtained autocephaly on 7 May 1998, and Abune Phillipos was subsequently consecrated as first Patriarch of Eritrea. The two churches remain in full communion with each other and with the other Oriental Orthodox Churches, although the Coptic Orthodox Church of Alexandria, along with the Ethiopian Orthodox Tewahedo Church does not recognize the deposition of the third Patriarch of Eritrea, Abune Antonios.
Coptic Orthodox churches around the world.
There are several and institutions outside of Egypt, including churches and institutions in:
Official titles of the Patriarch of Alexandria.
Historical evolution of the ecclesiastical title.
The Bishop of Alexandria was first known just as the "Bishop of Alexandria". It continued to be so, until the Church grew within and all over the Egyptian Province, and many Bishops were consecrated for the newly founded parishes all over the towns and cities.
The Bishop of Alexandria, being the successor of the first Bishop in Egypt consecrated by Saint Mark, was honored by the other Bishops, as first among equals "Primus inter Pares,". This was in addition to the appropriate honorary dignity, which was due by virtue of being the Senior Bishop of the main Metropolis of the Province, Alexandria, which also the Capital and the main Port of the Province. This honor was bestowed by making the Senior Bishop an "“Archbishop,”" thus presiding in dignity of honor over all the Alexandrine and Egyptian Bishops.
The appellation of "“Pope”" has been attributed to the Bishop of Alexandria since the Episcopate of Heraclas, the 13th Bishop of Alexandria. All the clergy of Alexandria and Lower Egypt honored him with the appellation "“Papas,”" which means “Our Father,” as the Senior and Elder Bishop among all bishops, within the Egyptian Province, who are under his jurisdiction. This is because Alexandria was the Capital of the Province, and the preaching center and the place of martyrdom of Saint Mark the Evangelist and Apostle.
The title "“Patriarch”" means the Head or the Leader of a Tribe or a Community. Ecclesiastically it means the Head of the Fathers (Bishops) and their congregation of faithful. This title is historically known as “Patriarch of Alexandria and all Africa on the Holy Apostolic Throne of Saint Mark the Evangelist,” that is “of Egypt.” The title of “Patriarch” was first used around the time of the Third Ecumenical Council of Ephesus, convened in AD 431, and ratified at Chalcedon in AD 451.
It is to be noted that only the Patriarch of Alexandria has the double title of Pope and Patriarch among the Eastern Orthodox and the Oriental Orthodox Thrones.
Administrative divisions of the Coptic Orthodox Patriarchate of Alexandria.
The Holy Synod of the Coptic Orthodox Patriarchate of Alexandria.
The Holy Synod of the Coptic Orthodox Patriarchate of Alexandria is headed by the Patriarch of Alexandria and the members are the Metropolitan Archbishops, Metropolitan Bishops, Diocesan Bishops, Patriarchal Exarchs, Missionary Bishops, Auxiliary Bishops, Suffragan Bishops, Assistant Bishops, Chorbishops and the Patriarchal Vicars for the Church of Alexandria.
For the list of the members of the Holy Synod and their official titles see main article The Holy Synod of the Coptic Orthodox Patriarchate of Alexandria

</doc>
<doc id="7602" url="http://en.wikipedia.org/wiki?curid=7602" title="Family International">
Family International

The Family International, formed as the Children of God (COG), renamed Family of Love and later The Family, is a new religious movement often referred to as a cult (by such academics as Benjamin Beit-Hallahmi and John Huxley) started in 1968 in Huntington Beach, California, United States, with many of its early converts drawn from the hippie movement.
Overview.
TFI initially spread a message of salvation, apocalypticism, and spiritual "revolution" against the outside world, which the members called "the System". In 1976, it began a method of evangelism called Flirty Fishing, using sex to "show God's love" and win converts, resulting in controversy. TFI's founder and prophetic leader, David Berg (who was first called "Moses David" in the Texas press), took the titles of "King", "The Last Endtime Prophet", "Moses", and "David". He communicated with his followers via "Mo Letters"—letters of instruction and counsel on myriad spiritual and practical subjects—until his death in late 1994. After his death, his widow Karen Zerby became the leader of TFI, taking the title of "Queen" and "prophetess". She married Steve Kelly, an assistant of Berg's whom he had handpicked as her "consort". Kelly took the title of "King Peter" and became the public face of TFI, speaking in a more public capacity than either David Berg or Karen Zerby.
History.
The Children of God (1968–1977).
Members of the Children of God founded communes, first called "colonies" (now referred to as "homes") in various cities. They would proselytize in the streets and distribute pamphlets.
New converts memorized MO letters, took Bible classes, and were expected to emulate the lives of early Christians while rejecting mainstream denominational Christianity. In common with converts to some other religions, most incoming members adopted a new "Bible" name.
The founder of the movement was a former Baptist and Christian and Missionary Alliance pastor, David Brandt Berg (1919–1994), also known within the group as Moses David, Mo, Father David, and Dad to adult group members, and eventually as Grandpa to the group's youngest members.
Berg communicated with his followers through more than 3,000 published letters written over 24 years, referred to as "Mo Letters" by members of the group. By January 1972, Berg introduced through his letters that he was God's prophet for this time, further establishing his spiritual authority within the group. Despite this teaching, Berg freely acknowledged his failings and weaknesses.
By the end of 1972, COG members had printed and distributed approximately 42 million Christian tracts, mostly on God's salvation and America's doom. Street distribution of Berg's Letters (called "litnessing") became the COG's predominant method of both outreach and support for the next five years.
The Children of God ended as an organizational entity in February 1978. Berg reorganized the movement amid reports of serious misconduct, financial mismanagement, and established leaders having abused their positions (and others having opposed flirty fishing). He dismissed more than 300 of the movement's leaders, known as "The Chain", and declared the general dissolution of the COG structure. This shift was known as the "Reorganization Nationalization Revolution" (RNR). A third of the total membership left the movement, and those who remained became part of the reorganized movement, dubbed the "Family of Love", and later "the Family". Most of the group's beliefs, however, remained the same.
The Family of Love (1978–1981).
The "Family of Love" era was characterized by expansion into more countries. Regular proselytizer methods included door-to-door distributing tracts and other gospel literature, and organized classes on various aspects of Christian life, with heavy use of TFI-created music.
In 1976, David Berg introduced a new proselytizing method called Flirty Fishing (or FFing), encouraging female members to "show God's love" through sexual activity with potential converts. Flirty Fishing was practiced by members of Berg's inner circle starting in 1973, and was introduced to the general membership in 1976, when it became widely practiced by members of the group. In some areas, Flirty Fishers used escort agencies to meet people. According to TFI, as a result of Flirty Fishing, "over 100,000 received God's gift of salvation through Jesus, and some chose to live the life of a disciple and missionary". According to data provided by TFI to researcher Bill Bainbridge, from 1974 until 1987, members had sexual contact with 223,989 people while practicing Flirty Fishing. Flirty Fishing also resulted in the births of many children, including Karen Zerby's son, Ricky Rodriguez (aka "Davidito"), who committed suicide after murdering a female member of the cult who had sexually abused him as a toddler. Children born as result of Flirty Fishing were referred to as "Jesus Babies". By the end of 1981, more than 300 "Jesus Babies" had been born.
In an official statement on its origins, TFI partly describes the practice of Flirty Fishing as follows:
In his judgment of a child custody court case in England in 1994-95, after extensively researching COG publications and hearing the testimony of numerous witnesses, Lord Justice Sir Alan Ward said the following about Flirty Fishing:
A judge in Italy came to a different conclusion in 1991, deciding that Flirty Fishing was not prostitution (see Tribunale Penale di Roma (Criminal Court of Rome), 15 November 1991, re: Berg and others, and in the archives of the Criminal Court of Rome (RG 3841/84)). The judge concluded that it was only in "the last months of 1977 Berg started counseling the members that it was permissible for proselyting reasons to offer sexual contacts and services to perspective members, the more so when the latter were potentially good financial contributors to the cult". Among the Children of God, the judge argued, Flirty Fishing was not understood as prostitution but "as a personal contribution to the humanitarian aims that the sect always claimed to pursue".
Flirty Fishing was officially abandoned in 1987, though the principles and theory were retained, in favor of other witnessing methods and also to avoid contracting and spreading HIV within the group. In 1987, new rules banned, under penalty of excommunication, sexual contact with non-members. However, the new rules also stated that exceptions would be allowed. For example, one publication stated: "All sex with outsiders is banned!--Unless they are already close and well-known friends!"
Many of the Mo Letters also promoted sharing, including the sharing of one's physical body in love. Women and female children were led to believe that it was their duty to share with a man anytime he wanted. After some complaints from people who had been abused as children by adults, a new rule, that sexual interactions should not occur between an adult and a minor, was promoted but not enforced.
The Family (1982–1994).
By 1982, many Family members had moved to southern and eastern parts of the world. At the end of 1983, TF was reporting 10,000 full-time members living in 1,642 TF Homes. Additionally, TF's Music With Meaning radio club had by this time grown to almost 20,000 members. According to statistics by TF, at this time evangelistic efforts were resulting in an average of 200,000 conversions to Christ and distribution of nearly 30 million pages of literature per month.
In March 1989, TF issued a statement that, in "early 1985" an urgent memorandum had been sent to all members "reminding them that any such activities" [adult-child sexual contact] "are strictly forbidden within our group". (emphasis in original). In January 2005, Claire Borowik, a spokesperson for TFI, issued a statement stating that "[d]ue to the fact that our current zero-tolerance policy regarding sexual interaction between adults and underage minors was not clearly stated in our literature published before 1986, we came to the realization that during a transitional stage of our movement, from 1978 until 1986, there were cases when some minors were subject to sexually inappropriate advances... This was corrected officially in 1986, when any contact between an adult and minor (any person under 21 years of age) was declared an excommunicable offense".
During the 1990s, numerous allegations of child sexual abuse were brought against TF around the world, in locations including Argentina, Australia, Brazil, France, Italy, Japan, Norway, Peru, Spain, Sweden, the UK, the USA, and Venezuela. TFI leadership has maintained that they did not sanction or condone the sexual abuse of children. The UK's High Court of Justice found that not only did widespread sexual abuse occur but that publications printed by church leaders promoted such activities. Berg published a document in which he said, in par. 69, "[T]here's nothing in the world at all wrong with sex as long as it's practised in love, whatever it is or whoever it's with, "no matter who or what age or what relative or what manner!" And you don't hardly dare even say these words in private!" (emphasis added) Some court documents can be found in the Court Cases section below.
Transformation in the 1990s.
In the early 1990s, TF members took advantage of the newly opened Eastern Europe (following the fall of Communism) and expanded their evangelism campaigns eastward, alongside many other religious groups. The production and dissemination of millions of pieces of literature earned them the colloquial name "the poster people".
The early 1990s also saw the launch of what TF termed their Consider the Poor (CTP) ministries. Expanding their outreach beyond evangelism, adherents provided material aid to the poor and disadvantaged. TF members became active in disaster relief efforts, providing and distributing humanitarian aid, organizing musical shows to benefit refugees, visiting hospitals, and similar activities.
The Family (1995–2003).
After Berg's death in October 1994, Karen Zerby (known in the group as Mama Maria, Queen Maria, Maria David, or Maria Fontaine), took over leadership of the group. She married her longtime partner, Steven Douglas Kelly, an American known in the group as Peter Amsterdam or King Peter, who legally changed his name to Christopher Smith. He became her traveling representative due to Zerby's reclusive separation from most of her followers.
In February 1995, the group introduced the "Love Charter", which defined the rights and responsibilities of Charter members and Homes. The Charter also includes the "Fundamental Family Rules", a summary of rules and guidelines from past TF publications which were still in effect.
The Charter established a new way of living within the organization, allowing members greater freedom to choose and follow their pursuits. The rights referred to in the Charter were what a member could expect to receive from the group and how members were to be treated by leaders and fellow members. The responsibilities were what members were expected to give to the group if they wished to remain full-time members, including tithing 10% of their income to World Services, giving 3% to the "Family Aid Fund" set up to support needy field situations, and 1% to regional "common pots", used for local projects, activities, and fellowships. The Charter has been amended over the years according to changes within the group. TFI's 2010 policies state that all members must tithe (give 10% of their income) or give a monthly contribution in order to retain membership.
In the 1994-95 British court case, the Rt. Hon. Lord Justice Alan Ward decided that the group, including some of its top leaders, had engaged in abusive sexual practices involving minors and had also used severe corporal punishment and sequestration of minors. However, he found that TF had abandoned these practices and concluded that they were a safe environment for children. Nevertheless, he did require that the group cease all corporal punishment of children in the United Kingdom and denounce any of Berg's writings that were "responsible for children in TF having been subjected to sexually inappropriate behaviour".
The Family International (2004–present).
In 2004, the movement's name was changed to The Family International. However, TFI members were told that they could retain their former names so long as they do not conceal their affiliation with TFI.
In 2004, there were also major changes in the group. Internal publications spoke of arresting a general trend towards a less dedicated lifestyle, and the need for re-commitment to the group's mission of fervent evangelism. In the second half of 2004, a six-month period was held to help members refocus their priorities (known as "The Renewal"). The group was reorganized, with new levels of membership defined into the following categories: Family Disciples (FD), Missionary Members (MM), Fellow Members (FM), Active Members (AM), and General Members (GM).
The "Love Charter" governs FDs, while the "Missionary Member Statutes" and "Fellow Member Statutes" were written for the governance of TFI's Missionary member and Fellow Member circles, respectively. FD Homes are reviewed every six months against a published set of criteria.
According to TFI statistics, at the beginning of 2005 there were 1,238 TFI Homes and 10,202 members worldwide. Of those, 266 Homes and 4,884 members were FD, 255 Homes and 1,769 members were MM, and 717 Homes and 3,549 members were FM. Statistics on AM and GM categories were unavailable.
Beliefs.
To some extent, TFI identifies itself with fundamentalist Christianity, though their more radical beliefs and practices are generally regarded as non-traditional, even heretical, by many conservative Christians. TFI teaches that the Bible and MO letters are the inspired Word of God and sacred revelation. David Berg is regarded as the last and most important prophet of the end times, predicted in the Old and New Testaments, specifically in the reference to "a prophet like Moses" (although Peter the Apostle uses this passage to refer to Christ) . Berg is regarded as a prophet who passed on God's message, and his writing are seen as "filling in the gaps" (par.24 ) in the Bible. If they contradict or are irreconcilable with Scripture, they take precedence over it . The group believes Berg's spiritual "mantle" passed to his wife, Karen Zerby, at his death. The couple's officially published writings are regarded as part of the "Word of God," equal in weight and importance to the Bible as divine revelations. These beliefs have been re-addressed in recent publications issued in 2010, which say they are no longer requirements of membership. However neither Berg's nor Zerby's prophetic status has been retracted.
TFI members believe that the Great Commission to evangelize the world is every Christian' duty, and that their lives should be dedicated to serving God and others. Among their several levels of membership, the most committed -- "Family Disciples" (FD) -- live communally. The group encourages having children. While birth control was at first sharply discouraged as "ungodly", the choice is now left to the individual; the practice is not uncommon, though it was officially regarded as indicating lack of trust in God's plan. Birth-control views were among those re-adressed in 2010.
A central tenet of TFI theology is the "Law of Love" which, stated simply, claims that if a person's actions are motivated by unselfish, sacrificial love and are not intentionally hurtful, they are in accordance with Scripture and thus lawful in the eyes of God. Though the romantic and sexual implication of this principle is polyamory, the "Law of Love" emphasizes unselfishness, giving, caring, respect, honesty, and other essential Christian values that should be enacted in every facet of life (the Scriptural basis used for this teaching can be found in Matthew 22:37 - 40 and in Galatians 5:14). The members believe that this law supersedes all other Biblical laws, except those forbidding male homosexuality, which they believe is a sin. Female bisexuality is allowed, though a lesbian life that completely excludes men is not. TFI teaches that God created human sexuality, that it is a natural, emotional, and physical need, and that heterosexual relations between consenting adults constitute a pure and natural wonder of God's creation, and are therefore permissible according to Scripture.
The re-statements issued in 2010 express the need for more tolerance toward varying sexual choices. Since 2010, the age of consent in TFI is determined by local laws and regulations. Since 1986, sex between minors and adults has been forbidden. Adult members may have sex with any other adult member of the opposite sex, and are encouraged to do so, regardless of marital status, as a way to foster unity and combat loneliness of those "in need". This is commonly called "sharing", or "sacrificial sex". While TFI policy states that members should not be pressured into sex against their will, numerous former members have alleged they were coerced to "share" or cast as selfish or unloving if they did not. These issues were also re-addressed in 2010, reflecting a need to change this aspect of TFI culture to respect personal sexual decisions and become more inclusive of differing personal views.
TFI members believe they are living in the period known the Bible calls the "Last Days" or the "Time of the End", the era immediately preceding Christ's return. Before that event, they believe, the Earth will be ruled for seven years by the Antichrist, who will create a world government. Halfway through his rule, he will be possessed by Satan, precipitating a time of troubles known as the Great Tribulation. This will bring intense persecution of Christians, as well as stupendous natural and unnatural disasters. At the end of this period, faithful Christians will be taken to heaven in an event known as the Rapture, shortly followed by a battle between Christ and the Antichrist commonly known as the "Battle of Armageddon", in which the Antichrist will be defeated. Then, they say, Christ will reign on Earth for 1,000 years, a period they call the Millennium.
Recent teachings.
TFI's recent teachings center around beliefs they term the "new [spiritual] weapons". TFI members believe that they are soldiers in the spiritual war of good versus evil for the souls and hearts of men. Although some of the following beliefs are not new to TFI, they have assumed greater importance in recent years.
Loving Jesus.
TFI continues to stress the imminent Second Coming of Christ, preceded by the rise of a worldwide government led by the "Antichrist". Doctrines of the "end times" influence virtually all long-term decision-making. However, documents issued in 2010 have changed this view to reflect a need for long-term plans and projects.
Issues.
Child abduction.
Since the late 1970s, there have been reports of ex-members' children being abducted to other countries to hide them from their parents, law enforcement authorities and child welfare agencies. In the early 1990s, one investigation -- seeking the four children of Ruth Frouman, who was expelled in July 1987 after being diagnosed with breast cancer and died four years later -- resulted in police raids on 10 TFI homes in Buenos Aires, Argentina. After holding a large number of TFI children in custody and conducting physical and psychological tests, the court returned them to their parents, citing lack of evidence. Two of the Frouman children were returned to their father that year, and the others were brought to him four years later. 
Although TFI has rarely made public statements about specific child abduction cases, the group's policies and practices on child custody were re-defined in the February 1995 "Love Charter", TFI's governing document after the death of its founder. Section 60, "Permanent Marital Separation Rules", states that couples with children must come to a mutual written agreement about the separation and the custody of their children, but that obtaining a legal divorce and custody order is optional. Amendments published in June 2003 state that if the parties cannot agree and "opt to use the court system", they must "relinquish Charter membership until the matter is settled". This clause was revoked by TFI's 2010 policy re-statement, and members now may retain membership while seeking a court divorce.
One TFI member, Peter Bevan Riddell, is known to have been convicted of crimes relating to child abduction. In 1984, the Australian government canceled Riddell's passport and he was extradited from Japan and convicted of committing forgery and making false statements to facilitate unlawful abduction. He later returned to Japan, where he continued working on behalf of Berg and Zerby in World Services. Another TFI member, Brian Edward Pickus, has been wanted for decades on an Interpol warrant issued by the United States and the state of Hawaii for kidnapping, burglary and unlawful flight to avoid prosecution.
The second generation.
Second-generation adults (known as "SGAs") are adults born or reared in TFI. They have assumed many leadership positions in the organization, including chairmanships of international, regional, and national boards.
However, many second-generation members have left to pursue secular careers or higher education, and to rear their children in an a different environment. There is much anti-TFI sentiment among those who have left (examples include Rose McGowan, as well as sisters Celeste Jones, Kristina Jones, and Julianna Buhring, who wrote a book on their lives in TFI). Several former members have legally pursued alleged physical and sexual abusers, who, they claim, are shielded by the group's leaders.
Many former Missionary Kids have returned to the country of their citizenship, where they are Third Culture Kids (TCKs). Many have kept in communication with each other. A notable example of this is the site MovingOn.org, created by a former second-generation member in 2001 (closed down as of February 2009).
Some SGAs who remained in the group were vocal defenders of TFI's lifestyle. One outlet for their views was MyConclusion.com, a site opened shortly after the 2005 murder-suicide of SGA Rick Rodriguez and Angela Smith. Yet in subsequent years, many of those same second-generation adults left and became just as vocal, or more so, in opposing TFI. This seems to illustrate the effects of mental conditioning and group-think on TFI youth, including the radical rejection of (and by) the group upon their exit.
Members are encouraged to remain friendly with relatives who have left. However, they are discouraged from associating with anyone considered enemies of TFI, including ex-members who appear on television programs or publish books and articles denouncing the group.
Several former SGAs have reported crimes to law enforcement, testified against TFI in court, and publicly criticized the group's members and practices. In the past, TFI used the term "apostate" for such former members, and argued that their testimony was unreliable. Some TFI members have claimed that SGAs who alleged abuse were mentally unstable, demonically possessed, or paid by anti-cult movements. Former SGAs often resent the "apostate" label, noting they never chose to join the group. Since 2009, TFI documents have discouraged using pejorative terms for former members. They express the need to understand and respect the decisions of former members, and to support them in establishing themselves outside the group. TFI's past literature, however, and its general culture, continue to make life difficult for second- and third-generation members who leave.
Secrecy.
TFI members are expected to respect legal and civil authorities where they live. Members have typically cooperated with appointed authorities, even during the police and social-service raids of their communities in the early 1990s. However, a controversial belief taught and practiced by many members holds that it is right to lie to non-members (or "unbelievers") to protect God's work. This belief is commonly referred to as "deceivers yet true".
Extreme secrecy surrounding leadership and finances, and aversion to government oversight, have been consistent throughout TFI's history. World Services (WS), its central administrative wing, operates in seclusion, very few members even knowing its whereabouts. Since 2010, workers' families have been told the location of specific WS centers, but the information is not otherwise available.
It is not uncommon for senior leaders to legally change their names. There have been allegations that members -- including senior leaders -- use forged or fraudulently obtained passports from Australia, Canada, the United States, and other countries. Senior leaders typically try not to publish their legal names; this has become more difficult because of legal action in many countries. In particular, a major court case in England brought to light many formerly guarded names of senior members.
In TFI's older publications, printed photographs of WS members were typically censored by means of a rudimentary pencil drawing over the person's face. In TFI-produced art, Berg's head was often replaced with that of a lion.
After Berg's death in 1994, members and the public were finally allowed to see photographs of him. In 2005, several current photos of Karen Zerby, Steven Kelly, and leading WS members were leaked online. This marked the first time in nearly 30 years that images of Zerby were publicly available. Since the TFI policy changes of 2009-10, the level of secrecy has changed somewhat. Pictures of Zerby and Kelly can now be found at http://karenzerby.org/, and Kelly carries pictures of Zerby with him to show members. But Zerby's and Kelly's whereabouts are still heavily guarded secrets, and TFI's structure and organization remain closed to any public accounting or government oversight.
Finances.
TFI finances are based on a system of tithing. All members are required to donate 10% of their income to World Services. A further 3% is required for regional offices of locally administered projects and a community lending program, and a final 1% is demanded for regional literature publishing. Supplementary giving to TFI offices and leadership, beyond the 14% is encouraged and fairly common. 
Additional income comes from marketing products such as children's videos and music (under varied names such as Treasure Attic and Kiddy Viddy) and selling posters on the street. In recent years, many TFI members have established associations and foundations subject to the accounting and auditing regulations where they live; since 2008, TFI documents have emphasized the need for members' charity works to be transparent and sustainable.
How TFI channels its funds around the world depends largely on the trust of carefully placed, non-senior members, who typically manage bank accounts that contain organization funds in their own names. Despite this practice, TFI says it has experienced very little graft; the publicized cases have involved insubstantial amounts of money.
In TFI's literature, impending global financial doom is a common theme. Accordingly, the group has gone to considerable lengths to avoid investments it deems unstable in the event of a crash. Typically, its reserves are stored in Japanese yen, Swiss francs, and gold. TFI has consistently avoided investing in real estate, stocks or bonds. Since many members are now creating businesses and charitable enterprises, and the group's end-time beliefs have been modified, long-term planning and investments are more common.
Reception.
The group has been heavily criticized by the press and the anti-cult movement. In 1971, an organization called FREECOG was founded by concerned family members and followers, including deprogrammer Ted Patrick, to "free" them from their involvement in the group.
Frequently, critics cite Berg's writings, and incidents of alleged criminal behavior by individuals. TFI members, meanwhile, argue that not all of Berg's writings reflect the group's fundamental beliefs (contained in the "Statement of Faith") or policies (contained in the 1995 "Love Charter"). They also reject judging the entire group for the wrongdoing of individuals, even when those individuals are at the highest leadership levels.
Due to the high commitment nature of the group and its controversial beliefs, the movement tends to generate strong feelings in both current and former members.
Programs, projects, and productions.
TFI has numerous programs, local foundations, and projects through which it operates around the world. The largest of these are the "Family Care Foundation" (FCF), "Aurora Production AG", and "Activated Ministries", a California-based nonprofit organization that heavily supports TFI projects.
Leadership and management.
The leadership of TFI is headed by:
Under them, management is divided into "World Services", "Creations", and a "Family Care Foundation". Each region is managed by a team of Continental Officers (COs), usually having five to seven members. Management structures beneath the CO team are more variable, change members frequently.
Statistics.
In 1972, the Children of God reported 130 communes or "colonies" in 15 countries. In 1993, 7,000 of TFI's 10,000 members were under 18 years of age. Recent changes have resulted in the majority of members leaving.

</doc>
<doc id="7603" url="http://en.wikipedia.org/wiki?curid=7603" title="CIT">
CIT

CIT may refer to:
Other.
Cit may refer to:

</doc>
<doc id="7604" url="http://en.wikipedia.org/wiki?curid=7604" title="Code of Hammurabi">
Code of Hammurabi

The Code of Hammurabi is a well-preserved Babylonian law code of ancient Mesopotamia, dating back to about 1772 BC. It is one of the oldest deciphered writings of significant length in the world. The sixth Babylonian king, Hammurabi, enacted the code, and partial copies exist on a human-sized stone "stele" and various clay tablets. The Code consists of 282 laws, with scaled punishments, adjusting "an eye for an eye, a tooth for a tooth" ("lex talionis") as graded depending on social status, of slave versus free man.
Nearly one-half of the Code deals with matters of contract, establishing, for example, the wages to be paid to an ox driver or a surgeon. Other provisions set the terms of a transaction, establishing the liability of a builder for a house that collapses, for example, or property that is damaged while left in the care of another. A third of the code addresses issues concerning household and family relationships such as inheritance, divorce, paternity and sexual behavior. Only one provision appears to impose obligations on an official; this provision establishes that a judge who reaches an incorrect decision is to be fined and removed from the bench permanently. A handful of provisions address issues related to military service.
One nearly complete example of the Code survives today, on a diorite stele in the shape of a huge index finger, tall ("see images at right"). The Code is inscribed in the Akkadian language, using cuneiform script carved into the stele. It is currently on display in the Louvre, with exact replicas in the Oriental Institute at the University of Chicago, the library of the Theological University of the Reformed Churches (Dutch: Theologische Universiteit Kampen voor de Gereformeerde Kerken) in The Netherlands, the Pergamon Museum of Berlin and the National Museum of Iran in Tehran.
History.
Hammurabi ruled for nearly 46 years, c. 1792 to 1750 BC according to the Middle chronology. In the preface to the law, he states, "Anu and Bel called by name me, Hammurabi, the exalted prince, who feared Marduk, the patron god of Babylon (The Human Record, Andrea & Overfield 2005), to bring about the rule in the land." On the stone slab there are 44 columns and 28 paragraphs that contained 282 laws.
The stele was probably erected at Sippar, city of the sun god Shamash, god of justice, who is depicted handing authority to the king in the image at the top of the stele.
In 1901, Egyptologist Gustave Jéquier, a member of an expedition headed by Jacques de Morgan, found the stele containing the Code of Hammurabi in what is now Khūzestān, Iran (ancient Susa, Elam), where it had been taken as plunder by the Elamite king Shutruk-Nahhunte in the 12th century BC.
Law.
The Code of Hammurabi was one of several sets of laws in the ancient Near East.
The code of laws was arranged in orderly groups, so that everyone who read the laws would know what was required of them.
Earlier collections of laws include the Code of Ur-Nammu, king of Ur (c. 2050 BC), the Laws of Eshnunna (c. 1930 BC) and the codex of Lipit-Ishtar of Isin (c. 1870 BC), while later ones include the Hittite laws, the Assyrian laws, and Mosaic Law.
These codes come from similar cultures in a relatively small geographical area, and they have passages which resemble each other.
The Code of Hammurabi is the longest surviving text from the Old Babylonian period.
The code has been seen as an early example of a fundamental law regulating a government — i.e., a primitive constitution. The code is also one of the earliest examples of the idea of presumption of innocence, and it also suggests that both the accused and accuser have the opportunity to provide evidence. The occasional nature of many provisions suggests that the Code may be better understood as a codification of Hammurabi's supplementary judicial decisions, and that, by memorializing his wisdom and justice, its purpose may have been the self-glorification of Hammurabi rather than a modern legal code or constitution. However, its copying in subsequent generations indicates that it was used as a model of legal and judicial reasoning.
Other copies.
Various copies of portions of the Code of Hammurabi have been found on baked clay tablets, some possibly older than the celebrated diorite stele now in the Louvre. The Prologue of the Code of Hammurabi (the first 305 inscribed squares on the stele) is on such a tablet, also at the Louvre (Inv #AO 10237). Some gaps in the list of benefits bestowed on cities recently annexed by Hammurabi may imply that it is older than the famous stele (it is currently dated to the early 18th century BC). Likewise, the Museum of the Ancient Orient, part of the Istanbul Archaeology Museums, also has a "Code of Hammurabi" clay tablet, dated to 1750 BC, in (Room 5, Inv # Ni 2358).
In July, 2010, archaeologists reported that a fragmentary Akkadian cuneiform tablet was discovered at Tel Hazor, Israel, containing a c. 1700 BC text that was said to be partly parallel to portions of the Hammurabi code. The Hazor law code fragments are currently being prepared for publication by a team from the Hebrew University of Jerusalem.
Laws covered.
The laws covered such subjects as:
One of the most well known of Hammurabi's laws was
Ex. Law #196. "If a man destroy the eye of another man, they shall destroy his eye. If one break a man's bone, they shall break his bone. If one destroy the eye of a freeman or break the bone of a freeman he shall pay one mana of silver. If one destroy the eye of a man's slave or break a bone of a man's slave he shall pay one-half his price."
Hammurabi had many other punishments as well. If a boy struck his father they would cut off the boy's hand or fingers (translations vary).

</doc>
<doc id="7605" url="http://en.wikipedia.org/wiki?curid=7605" title="Cuba Libre">
Cuba Libre

The Cuba Libre (; , "Free Cuba") is a highball made of cola, lime, and white rum. This highball is often referred to as a Rum and Coke in the United States, Canada, the UK, Ireland, Australia and New Zealand where the lime juice may or may not be included.
History.
Accounts of the invention of the Cuba Libre vary. One account claims that the drink (Spanish for "Free Cuba") was invented in Havana, Cuba around 1901/1902. Patriots aiding Cuba during the Spanish-American War—and, later, expatriates avoiding Prohibition—regularly mixed rum and cola as a highball and a toast to this Caribbean island.
According to Bacardi:
The world's second most popular drink was born in a collision between the United States and Spain. It happened during the Spanish-American War at the turn of the century when Teddy Roosevelt, the Rough Riders, and Americans in large numbers arrived in Cuba. One afternoon, a group of off-duty soldiers from the U.S. Signal Corps were gathered in a bar in Old Havana. Fausto Rodriguez, a young messenger, later recalled that Captain Russell came in and ordered Bacardi (Gold) rum and Coca-Cola on ice with a wedge of lime. The captain drank the concoction with such pleasure that it sparked the interest of the soldiers around him. They had the bartender prepare a round of the captain's drink for them. The Bacardi rum and Coke was an instant hit. As it does to this day, the drink united the crowd in a spirit of fun and good fellowship. When they ordered another round, one soldier suggested that they toast "¡Por Cuba Libre!" in celebration of the newly freed Cuba. The captain raised his glass and sang out the battle cry that had inspired Cuba's victorious soldiers in the War of Independence.
However, there are some problems with Bacardi's account, as the Spanish-American war was fought in 1898, Cuba's liberation was in 1898, and the Rough Riders left Cuba in September 1898, but Coca-Cola was not available in Cuba until 1900. According to a 1965 deposition by Fausto Rodriguez, the Cuba Libre was first mixed at a Cuban bar in August 1900 by a member of the U.S. Signal Corps, referred to as "John Doe".
According to Havana Club:
Along with the Mojito and the Daiquiri, the Cuba Libre shares the mystery of its exact origin. The only certainty is that this cocktail was first sipped in Cuba. The year? 1900. 1900 is generally said to be the year that cola first came to Cuba, introduced to the island by American troops. But “Cuba Libre!” was the battle cry of the Cuba Liberation Army during the war of independence that ended in 1898.
Popularity.
This drink was once viewed as exotic, with its dark syrup, made (at that time) from kola nuts and coca.
Soon, as Charles H. Baker, Jr. points out in his "Gentlemen's Companion" of 1934, the Cuba Libre "caught on everywhere throughout the [American] South ... filtered through the North and West," aided by the ample supply of its ingredients. In "The American Language", 1921, H.L. Mencken writes of an early variation of the drink: "The troglodytes of western South Carolina coined 'jump stiddy' for a mixture of Coca-Cola and denatured alcohol (usually drawn from automobile radiators); connoisseurs reputedly preferred the taste of what had been aged in Model-T Fords."
The drink gained further popularity in the United States after The Andrews Sisters recorded a song (in 1945) named after the drink's ingredients, "Rum and Coca-Cola". Cola and rum were both cheap at the time and this also contributed to the widespread popularity of the concoction.
Recipe variations.
Cubata is a Cuba Libre made from Bacardi Gold instead of Bacardi Superior, giving it a deeper, more complex flavour.
The Cuba Pintada ("stained Cuba") is one part rum with two parts club soda and just enough cola so that it tints the club soda. The Cuba Campechana ("half-and-half Cuba") contains one part rum topped off with equal parts of club soda and cola. They are both popular refreshments, especially among young people.
Other recent variations are the Cuba Light made with rum and Diet Coke, and the Witch Doctor made with dark rum and Dr. Pepper.
Another variation of the Cuba Libre is the Cuban Missile Crisis. Compared to a normal Cuba Libre, it uses a higher proof rum, such as Bacardi 151 (75.5%).
A variation of the Cuba Libre popular in the West Indies is a “Hot” Cuba Libre which includes a splash of Caribbean hot sauce (for example, Capt'n Sleepy's Quintessential Habanero, or Matouk's).
Some people substitute Cream Soda and spiced rum to create a bright gold drink, often referred to as a Midas.
Another common variation is the use of "golden" or "dark" rum as opposed to white rum. This variation is the most commonly used in Venezuela.
Another variation is the use of White Rum (Captain Morgan White Rum) and cherry or vanilla Coke. This is most commonly known in the U.S. as "The Gringo".
Local variations.
The drink's name has evolved somewhat in both Cuba and the United States, where some choose to refer to it as a Mentirita ("a little lie"), in an opinionated reference to Cuban politics.
References.
Notes

</doc>
<doc id="7607" url="http://en.wikipedia.org/wiki?curid=7607" title="Collagen helix">
Collagen helix

In collagen, the collagen helix, or type-2 helix, is a major shape in secondary structure. It consists of a triple helix made of the repetitious amino acid sequence glycine - X - Y, where X and Y are frequently proline or hydroxyproline.
Each of the three chains is stabilized by the steric repulsion due to the pyrrolidine rings of proline and hydroxyproline residues. The pyrrolidine rings keep out of each other’s way when the polypeptide chain assumes this extended helical form, which is much more open than the tightly coiled form of the alpha helix.
The three chains are hydrogen bonded to each other. The hydrogen bond donors are the peptide NH groups of glycine residues. The hydrogen bond acceptors are the CO groups of residues on the other chains. The OH group of hydroxyproline also participates in hydrogen bonding. The rise of the collagen helix (superhelix) is 2.9 Å (0.29 nm) per residue.

</doc>
<doc id="7609" url="http://en.wikipedia.org/wiki?curid=7609" title="Cosmic censorship hypothesis">
Cosmic censorship hypothesis

The weak and the strong cosmic censorship hypotheses are two mathematical conjectures about the structure of singularities arising in general relativity.
Singularities that arise in the solutions of Einstein's equations are typically hidden within event horizons, and therefore cannot be seen from the rest of spacetime. Singularities that are not so hidden are called "naked". The weak cosmic censorship hypothesis was conceived by Roger Penrose in 1969 and posits that no naked singularities, other than the Big Bang singularity, exist in the universe.
Basics.
Since the physical behavior of singularities is unknown, if singularities can be observed from the rest of spacetime, causality may break down, and physics may lose its predictive power. The issue cannot be avoided, since according to the Penrose-Hawking singularity theorems, singularities are inevitable in physically reasonable situations. Still, in the absence of naked singularities, the universe is deterministic —it is possible to predict the entire evolution of the universe (possibly excluding some finite regions of space hidden inside event horizons of singularities), knowing only its condition at a certain moment of time (more precisely, everywhere on a spacelike three-dimensional hypersurface, called the Cauchy surface). Failure of the cosmic censorship hypothesis leads to the failure of determinism, because it is yet impossible to predict the behavior of spacetime in the causal future of a singularity. Cosmic censorship is not merely a problem of formal interest; some form of it is assumed whenever black hole event horizons are mentioned.
The hypothesis was first formulated by Roger Penrose in 1969, and it is not stated in a completely formal way. In a sense it is more of a research program proposal: part of the research is to find a proper formal statement that is physically reasonable and that can be proved to be true or false (and that is sufficiently general to be interesting). Because the statement is not a strictly formal one, there is sufficient latitude for (at least) two independent formulations, a weak form, and a strong form.
Weak and strong cosmic censorship hypothesis.
The weak and the strong cosmic censorship hypothesis are two conjectures concerned with the global geometry of spacetimes.
Mathematically, the conjecture states that, for generic initial data, the maximal Cauchy development possesses a complete future null infinity.
The two conjectures are mathematically independent, as there exist spacetimes for which the weak cosmic censorship is valid but the strong cosmic censorship is violated and, conversely, there exist spacetimes for which the weak cosmic censorship is violated but the strong cosmic censorship is valid.
Example.
The Kerr Metric, corresponding to a black hole of mass formula_1 and angular momentum formula_2, can be used to derive the effective potential for particle orbits restricted to the equator (as defined by rotation). This potential looks like:
where formula_4 is the coordinate radius, formula_5 and formula_6 are the test-particle's conserved energy and angular momentum respectively (constructed from the killing vectors).
To preserve "cosmic censorship", the black hole is restricted to the case of formula_7. For there to exist an event horizon around the singularity, the requirement: formula_7 must be satisfied:. This amounts to the angular momentum of the black hole being constrained to below a critical value, outside of which the horizon would disappear. 
The following thought experiment is reproduced from Hartle's "Gravity":
Imagine specifically trying to violate the censorship conjecture. This could be done by somehow imparting an angular momentum upon the black hole, making it exceed the critical value (assume it starts infinitesimally below it). This could be done by sending a particle of angular momentum formula_9. Because this particle has angular momentum, it can only be captured by the black hole if the maximum potential of the black hole is less than formula_10.
Solving the above effective potential equation for the maximum under the given conditions results in a maximum potential of exactly formula_10! Testing other values shows that no particle with enough angular momentum to violate the censorship conjecture would be able to enter the black hole, because they have too much angular momentum to fall in.
Problems with the concept.
There are a number of difficulties in formalizing the hypothesis:
In 1991, John Preskill and Kip Thorne bet against Stephen Hawking that the hypothesis was false. Hawking conceded the bet in 1997, due to the discovery of the special situations just mentioned, which he characterized as "technicalities". Hawking later reformulated the bet to exclude those technicalities. The revised bet is still open, the prize being "clothing to cover the winner's nakedness".
Counter-example.
An exact solution to the scalar-Einstein equations formula_14 which forms a counter example to many formulations of the 
cosmic censorship hypothesis was found by Mark D. Roberts in 1985:
where formula_16 is a constant.

</doc>
<doc id="7610" url="http://en.wikipedia.org/wiki?curid=7610" title="History of the term &quot;Catholic&quot;">
History of the term &quot;Catholic&quot;

The word catholic (with lowercase "c"; derived via Late Latin "catholicus", from the Greek adjective ("katholikos"), meaning "universal") comes from the Greek phrase ("katholou"), meaning "on the whole", "according to the whole" or "in general", and is a combination of the Greek words meaning "about" and meaning "whole". The word in English can mean either "including a wide variety of things; all-embracing" or "of the Roman Catholic faith" as "relating to the historic doctrine and practice of the Western Church.". ("Catholicos, the title used for the head of some churches in Eastern Christian traditions, is derived from the same linguistic origin).
The term Catholic (usually written with uppercase "C" in English) was first used to describe the Christian Church in the early 2nd century to emphasize its universal scope. In the context of Christian ecclesiology, it has a rich history and several usages. In non-ecclesiastical use, it derives its English meaning directly from its root, and is currently used to mean the following:
The term has been incorporated into the name of the largest Christian communion, the Catholic Church (also called the Roman Catholic Church). However, many other Christians use the term "Catholic" (sometimes with a lower-case letter "c") to refer more broadly to the whole Christian Church or to all believers in Jesus Christ regardless of denominational affiliation. Theologians writing in English will sometimes use the term "Church Catholic" or "Church catholic" to avoid confusion between this concept and the Catholic Church.
The Eastern Orthodox, Oriental Orthodox, Anglicans, Lutherans, and some Methodists believe that their churches are "Catholic" in the sense that they are in continuity with the original universal church founded by the Apostles. However, each church defines the scope of the "Catholic Church" differently. For instance, the Roman Catholic, Eastern Orthodox, and Oriental Orthodox churches each maintain that their own denomination is identical with the original universal church, from which all other denominations broke away.
Almost all Christians who call themselves "Catholic" believe that bishops are considered the highest order of ministers within the Christian religion. Along with unity, sanctity, and apostolicity, catholicity is considered one of Four Marks of the Church, in line with the Nicene Creed of 381: "I believe in one holy catholic and apostolic Church."
History of ecclesiastical use of the term.
Ignatius of Antioch.
The earliest recorded evidence of the use of the term "Catholic Church" is the "Letter to the Smyrnaeans" that Ignatius of Antioch wrote in about 107 to Christians in Smyrna. Exhorting Christians to remain closely united with their bishop, he wrote: "Wherever the bishop shall appear, there let the multitude [of the people] also be; even as, wherever Jesus Christ is, there is the Catholic Church."
Of the meaning for Ignatius of this phrase J.H. Srawley wrote:
This is the earliest occurrence in Christian literature of the phrase 'the Catholic Church' (ἡ καθολικὴ ἐκκλησία). The original sense of the word is 'universal'. Thus Justin Martyr ("Dial". 82) speaks of the 'universal or general resurrection', using the words ἡ καθολικὴ ἀνάστασις. Similarly here the Church universal is contrasted with the particular Church of Smyrna. Ignatius means by the Catholic Church 'the aggregate of all the Christian congregations' (Swete, "Apostles Creed", p. 76). So too the letter of the Church of Smyrna is addressed to all the congregations of the Holy Catholic Church in every place. And this primitive sense of 'universal' the word has never lost, although in the latter part of the second century it began to receive the secondary sense of 'orthodox' as opposed to 'heretical'. Thus it is used in an early Canon of Scripture, the Muratorian fragment ("circa" 170 A.D.), which refers to certain heretical writings as 'not received in the Catholic Church'. So too Cyril of Jerusalem, in the fourth century, says that the Church is called Catholic not only 'because it is spread throughout the world', but also 'because it teaches completely and without defect all the doctrines which ought to come to the knowledge of men'. This secondary sense arose out of the original meaning because Catholics claimed to teach the whole truth, and to represent the whole Church, while heresy arose out of the exaggeration of some one truth and was essentially partial and local.
By "Catholic Church" Ignatius designated the universal church. Ignatius considered that certain heretics of his time, who disavowed that Jesus was a material being who actually suffered and died, saying instead that "he only seemed to suffer" (Smyrnaeans, 2), were not really Christians.
Other second-century uses.
The term is also used in the "Martyrdom of Polycarp" (155) and in the Muratorian fragment (about 177).
Cyril of Jerusalem.
As mentioned in the above quotation from J.H. Srawley, Cyril of Jerusalem (c. 315–386), who is venerated as a saint by the Roman Catholic Church, the Eastern Orthodox Church, and the Anglican Communion, distinguished what he called the "Catholic Church" from other groups who could also refer to themselves as an ἐκκλησία (assembly or church):
Since the word Ecclesia is applied to different things (as also it is written of the multitude in the theatre of the Ephesians, "And when he had thus spoken, he dismissed the Assembly" (Acts 19:14), and since one might properly and truly say that there is a "Church of evil doers", I mean the meetings of the heretics, the Marcionists and Manichees, and the rest, for this cause the Faith has securely delivered to you now the Article, "And in one Holy Catholic Church"; that you may avoid their wretched meetings, and ever abide with the Holy Church Catholic in which you were regenerated. And if ever you are sojourning in cities, inquire not simply where the Lord's House is (for the other sects of the profane also attempt to call their own dens houses of the Lord), nor merely where the Church is, but where is the Catholic Church. For this is the peculiar name of this Holy Church, the mother of us all, which is the spouse of our Lord Jesus Christ, the Only-begotten Son of God(Catechetical Lectures, XVIII, 26).
Theodosius I.
Theodosius I, Emperor from 379 to 395, declared "Catholic" Christianity the official religion of the Roman Empire, declaring in the Edict of Thessalonica of 27 February 380:
It is our desire that all the various nations which are subject to our clemency and moderation, should continue the profession of that religion which was delivered to the Romans by the divine Apostle Peter, as it has been preserved by faithful tradition and which is now professed by the Pontiff Damasus and by Peter, Bishop of Alexandria, a man of apostolic holiness. According to the apostolic teaching and the doctrine of the Gospel, let us believe in the one Deity of the Father, Son and Holy Spirit, in equal majesty and in a holy Trinity. We authorize the followers of this law to assume the title "Catholic" Christians; but as for the others, since in our judgment they are foolish madmen, we decree that they shall be branded with the ignominious name of heretics, and shall not presume to give their conventicles the name of churches. They will suffer in the first place the chastisement of the divine condemnation, and in the second the punishment which our authority, in accordance with the will of heaven, will decide to inflict. Theodosian Code XVI.i.2
Augustine of Hippo.
Only slightly later, Saint Augustine of Hippo (354–430) also used the term "Catholic" to distinguish the "true" church from heretical groups:
In the Catholic Church, there are many other things which most justly keep me in her bosom. The consent of peoples and nations keeps me in the Church; so does her authority, inaugurated by miracles, nourished by hope, enlarged by love, established by age. The succession of priests keeps me, beginning from the very seat of the Apostle Peter, to whom the Lord, after His resurrection, gave it in charge to feed His sheep (Jn 21:15–19), down to the present episcopate.
And so, lastly, does the very name of Catholic, which, not without reason, amid so many heresies, the Church has thus retained; so that, though all heretics wish to be called Catholics, yet when a stranger asks where the Catholic Church meets, no heretic will venture to point to his own chapel or house.
Such then in number and importance are the precious ties belonging to the Christian name which keep a believer in the Catholic Church, as it is right they should ... With you, where there is none of these things to attract or keep me... No one shall move me from the faith which binds my mind with ties so many and so strong to the Christian religion... For my part, I should not believe the gospel except as moved by the authority of the Catholic Church. —St. Augustine (354–430): "Against the Epistle of Manichaeus called Fundamental", chapter 4: Proofs of the Catholic Faith.
St Vincent of Lerins.
A contemporary of Augustine, St. Vincent of Lerins, wrote in 434 (under the pseudonym Peregrinus) a work known as the "Commonitoria" ("Memoranda"). While insisting that, like the human body, church doctrine develops while truly keeping its identity (sections 54-59, chapter XXIII), he stated:
Divergent usages.
Use by the Catholic Church.
In the English language, the first known use of the term is in Andrew of Wyntoun's "Orygynale Cronykil of Scotland", "He was a constant Catholic/All Lollard he hated and heretic."
The term "Catholic" is commonly associated with the whole of the church led by the Roman Pontiff, the Catholic Church, and whose over one billion adherents are about half of the estimated 2.1 billion Christians. Other Christian churches also laying claim to the description "Catholic" include the Eastern Orthodox Church and those churches possessing the historic episcopate (bishops), such as those of the Anglican Communion. Some of them claim to be the one true Catholic Church from which, in their view, other Christians, including those in communion with the Pope, have fallen away.
Many of those who apply the term "Catholic Church" to all Christians object to the use of the term to designate what they view as only one church within what they understand as the "whole" Catholic Church. However, the church in communion with the Bishop of Rome, both in its Western form and in that of the Eastern Catholic Churches, has always considered itself to be the historic Catholic Church, with all others as "non-Catholics" and regularly refers to itself as "the Catholic Church". This practice is an application of the belief that not all who claim to be Christians are part of the Catholic Church, as Ignatius of Antioch, the earliest known writer to use the term "Catholic Church", considered that certain heretics who called themselves Christians only seemed to be such.
Though normally distinguishing itself from other churches by calling itself the "Catholic Church", it also uses the description "Roman Catholic Church". Even apart from documents drawn up jointly with other churches, it has sometimes, in view of the central position it attributes to the See of Rome, adopted the adjective "Roman" for the whole church, Eastern as well as Western, as in the papal encyclicals and Another example is its self-description as the "Holy, Catholic, Apostolic and Roman Church" in the 24 April 1870 Dogmatic Constitution on the Catholic Faith of the First Vatican Council. In all of these documents it also refers to itself both simply as the Catholic Church and by other names. The Eastern Catholic Churches, while united with Rome in the faith, have their own traditions and laws, differing from those of the Latin Rite and those of other Eastern Catholic Churches.
Orthodox Christians.
The Eastern Orthodox Church also identifies itself as "Catholic", as in the title of . So does the Coptic Church, which, being part of Oriental Orthodoxy, is not in communion with the Eastern Orthodox Church and considers itself "the True Church of the Lord Jesus Christ".
After the East-West Schism, conventionally dated to 1054, a brief reunification was agreed to between the Pope and a number of Eastern Orthodox bishops at the Council of Florence. However, this agreement was denied by one of the Orthodox bishops present, namely Mark of Ephesus, and the common people of the Orthodox churches generally rejected the agreement as well. Pope Benedict XVI stated his wish to restore full unity with the Orthodox. The Roman Catholic Church considers that almost all of the ancient theological differences have been satisfactorily addressed (the Filioque clause, the nature of purgatory, etc.), and has declared that differences in traditional customs, observances and discipline are no obstacle to unity.
Recent historic ecumenical efforts on the part of the Catholic Church have focused on healing the rupture between the Western ("Catholic") and the Eastern ("Orthodox") churches. Pope John Paul II often spoke of his great desire that the Catholic Church "once again breathe with both lungs", thus emphasizing that the Roman Catholic Church seeks to restore full communion with the separated Eastern churches.
Anglican and Old Catholic.
Anglicans and Old Catholics see themselves as a communion within the Catholic Church and Lutherans see themselves as "a reform movement within the greater church catholic".
Some Anglicans and Old Catholics accept that, among bishops, that of Rome is "primus inter pares", and hold that conciliarism is a necessary check against ultramontanism.
Other Western Christians.
Methodists and Presbyterians believe their denominations owe their origins to the Apostles and the early church, but do not claim descent from ancient church structures such as the episcopate. However, both of these churches hold that they are a part of the catholic (universal) church. According to "Harper's New Monthly Magazine": As such, according to one viewpoint, for those who "belong to the Church," the term Methodist Catholic, or Presbyterian Catholic, or Baptist Catholic, is as proper as the term Roman Catholic. It simply means that body of Christian believers over the world who agree in their religious views, and accept the same ecclesiastical forms.
Avoidance of usage.
Some Protestant churches avoid using the term completely, to the extent among many Lutherans of reciting the Creed with the word "Christian" in place of "catholic". The Orthodox churches share some of the concerns about Roman Catholic papal claims, but disagree with some Protestants about the nature of the church as one body.

</doc>
<doc id="7611" url="http://en.wikipedia.org/wiki?curid=7611" title="Crystal Eastman">
Crystal Eastman

Crystal Catherine Eastman (June 25, 1881 – July 8, 1928) was an American lawyer, antimilitarist, feminist, socialist, and journalist. She is best remembered as a leader in the fight for women's suffrage, as a co-founder and co-editor with her brother Max Eastman of the radical arts and politics magazine "The Liberator," co-founder of the Women's International League for Peace and Freedom, and co-founder in 1920 of the American Civil Liberties Union. In 2000 she was inducted into the National Women's Hall of Fame in Seneca Falls, New York.
Early life and education.
Crystal Eastman was born in Marlborough, Massachusetts, on June 25, 1881, the third of four children. In 1883 their parents, Samuel Elijah Eastman and Annis Bertha Ford, moved the family to Canandaigua, New York, where her brother Max was born. The following year their older brother died at age seven. In 1889, their mother became one of the first women ordained as a Protestant minister in America when she became a minister of the Congregationalist Church. Her father was also a Congregation minister, and the two served as pastors at the church of Thomas K. Beecher near Elmira. This part of New York was in the so-called "Burnt Over District." During the Second Great Awakening earlier in the 19th century, its frontier had been a center of evangelizing and much religious excitement, which resulted in the founding of the Shakers and Mormonism. During the antebellum period, some were inspired by religious ideals to support such progressive social causes as abolitionism and the Underground Railroad.
Crystal and her brother Max Eastman were influenced by this progressive tradition. Their parents were friendly with the writer Mark Twain. From this association young Crystal also became acquainted with him.
She was the sister of the socialist activist Max Eastman, with whom she was quite close throughout her life. The two lived together for several years on 11th Street in Greenwich Village among other radical activists. The group, including Ida Rauh, Inez Milholland, Floyd Dell, and Doris Stevens, also spent summers and weekends in Croton-on-Hudson.
Eastman graduated from Vassar College in 1903 and received an M.A. in sociology (a relatively new field) from Columbia University in 1904. Gaining her law degree from New York University Law School, she graduated second in the class of 1907.
Social efforts.
Social work pioneer and journal editor Paul Kellogg offered Eastman her first job, investigating labor conditions for The Pittsburgh Survey sponsored by the Russell Sage Foundation. Her report, "Work Accidents and the Law" (1910), became a classic and resulted in the first workers' compensation law, which she drafted while serving on a New York state commission.
She continued to campaign for occupational safety and health while working as an investigating attorney for the U.S. Commission on Industrial Relations during Woodrow Wilson's presidency. She was at one time called the "most dangerous woman in America," due to her free-love idealism and outspoken nature.
Emancipation.
During a brief marriage to Wallace J. Benedict which ended in divorce, Eastman moved to Milwaukee and managed the unsuccessful 1912 Wisconsin suffrage campaign.
When she returned east in 1913, she joined Alice Paul, Lucy Burns, and others in founding the militant Congressional Union, which became the National Woman's Party. After the passage of the 19th Amendment gave women the vote in 1920, Eastman and three others wrote the Equal Rights Amendment, first introduced in 1923. One of the few socialists to endorse the ERA, she warned that protective legislation for women would mean only discrimination against women. Eastman claimed that one could assess the importance of the ERA by the intensity of the opposition to it, but she felt that it was still a struggle worth fighting. She also delivered the speech, "Now We Can Begin" following the ratification of the Nineteenth Amendment, outlining the work that needed to be done in the political and economic spheres to achieve gender equality.
Peace efforts.
During World War I, Eastman was one of the founders of the Woman's Peace Party, soon joined by Jane Addams, Lillian D. Wald, and others. She served as president of the New York branch. Renamed the Women's International League for Peace and Freedom in 1921, it remains the oldest extant women's peace organization. Eastman also became executive director of the American Union Against Militarism, which lobbied against America's entrance into the European war and more successfully against war with Mexico in 1916, sought to remove profiteering from arms manufacturing, and campaigned against conscription and imperial adventures.
When the United States entered World War I, Eastman organized with Roger Baldwin and Norman Thomas the National Civil Liberties Bureau to protect conscientious objectors, or in her words: "To maintain something over here that will be worth coming back to when the weary war is over." The NCLB grew into the American Civil Liberties Union, with Baldwin at the head and Eastman functioning as attorney-in-charge. Eastman is credited as a founding member of the ACLU, but her role as founder of the NCLB may have been largely ignored by posterity due to her personal differences with Baldwin.
Marriage and family.
In 1916 Eastman married the British editor and antiwar activist Walter Fuller (British editor), who had come to the United States to escape the Great War. They had two children, Jeffrey and Annis. They worked together as activists until the end of the war, when he returned to England to find paying work.
In 1917, Eastman co-founded a radical journal of politics, art, and literature, "The Liberator", with her brother Max. She served as managing editor from 1917 to 1921.
Post-War.
After the war, Eastman organized the First Feminist Congress in 1919.
She traveled by ship to London to be with her husband at times. In New York, her activities led to her being blacklisted during the Red Scare of 1919-1920. She struggled to find paying work.
During the 1920s her only paid work was as a columnist for feminist journals, notably "Equal Rights" and "Time and Tide". Eastman claimed that "life was a big battle for the complete feminist," but she was convinced that the complete feminist would someday achieve total victory.
Her husband died in 1927.
Death.
Crystal Eastman died on July 8, 1928, of nephritis. Her friends were entrusted with her two children, then orphans, to rear them until adulthood.
Legacy.
Eastman has been called one of the United States' most neglected leaders, because, although she wrote pioneering legislation and created long-lasting political organizations, she disappeared from history for fifty years. Freda Kirchwey, then editor of "The Nation", wrote at the time of her death: "When she spoke to people—whether it was to a small committee or a swarming crowd—hearts beat faster. She was for thousands a symbol of what the free woman might be."
In 2000 Eastman was inducted in the (American) National Women's Hall of Fame in Seneca Falls, New York.
Work.
Papers.
Eastman's papers are housed at Harvard University.
Publications.
The Library of Congress has the following publications by Eastman in its collection, much of it published posthumously:

</doc>
<doc id="7612" url="http://en.wikipedia.org/wiki?curid=7612" title="Christopher Alexander">
Christopher Alexander

Christopher Wolfgang Alexander (born October 4, 1936 in Vienna, Austria) is an architect noted for his theories about design, and for more than 200 building projects in California, Japan, Mexico and around the world. Reasoning that users know more about the buildings they need than any architect could, he produced and validated (in collaboration with Sarah Ishikawa and Murray Silverstein) a "pattern language" designed to empower anyone to design and build at any scale. Alexander is often overlooked by texts in the history and theory of architecture because his work intentionally disregards contemporary architectural discourse. As such, Alexander is widely considered to occupy a place outside the discipline, the discourse, and the practice of Architecture. 
In 1958, he moved from England to the United States, living and teaching in Berkeley, California from 1963. He is professor emeritus at the University of California, Berkeley. Now retired (though still active), he is based in Arundel, Sussex, UK.
Education.
Alexander grew up in England and started his education in sciences. In 1954, he was awarded the top open scholarship to Trinity College, Cambridge University in chemistry and physics, and went on to read mathematics. He earned a Bachelor's degree in Architecture and a Master's degree in Mathematics. He took his doctorate at Harvard (the first Ph.D. in Architecture ever awarded at Harvard University), and was elected fellow at Harvard. During the same period he worked at MIT in transportation theory and in computer science, and worked at Harvard in cognition and cognitive studies.
Honors.
Alexander was awarded the First Gold Medal for Research by the American Institute of Architects in 1972. The ACSA (Association of Collegiate Schools of Architecture) honored Alexander with the ACSA Distinguished Professor Award in 1986 and 1987. He was awarded the Seaside Prize in 1994. He was elected a Fellow of the American Academy of Arts and Sciences in 1996. In 2006, he was one of the two inaugural recipients of the Athena Award, given by the Congress for the New Urbanism (CNU). On 5 November 2009, at a ceremony in Washington D.C., he was awarded ("in absentia") the Vincent Scully Prize by the National Building Museum. In 2011, he was awarded the lifetime achievement award by the Urban Design Group.
Career.
Writings.
"The Timeless Way of Building" (1979) described the perfection of use to which buildings could aspire:
"A Pattern Language: Towns, Buildings, Construction" (1977) described a practical architectural system in a form that a theoretical mathematician or computer scientist might call a generative grammar.
The work originated from an observation that many medieval cities are attractive and harmonious. The authors said that this occurs because they were built to local regulations that required specific features, but freed the architect to adapt them to particular situations.
The book provides rules and pictures, and leaves decisions to be taken from the precise environment of the project. It describes exact methods for constructing practical, safe and attractive designs at every scale, from entire regions, through cities, neighborhoods, gardens, buildings, rooms, built-in furniture, and fixtures down to the level of doorknobs.
A notable value is that the architectural system consists only of classic patterns tested in the real world and reviewed by multiple architects for beauty and practicality.
The book includes all needed surveying and structural calculations, and a novel simplified building system that copes with regional shortages of wood and steel, uses easily stored inexpensive materials, and produces long-lasting classic buildings with small amounts of materials, design and labor. It first has users prototype a structure on-site in temporary materials. Once accepted, these are finished by filling them with very-low-density concrete. It uses vaulted construction to build as high as three stories, permitting very high densities.
This book's method was adopted by the University of Oregon, as described in "The Oregon Experiment" (1975), and remains the official planning instrument. It has also been adopted in part by some cities as a building code.
The idea of a pattern language appears to apply to any complex engineering task, and has been applied to some of them. It has been especially influential in software engineering where patterns have been used to document collective knowledge in the field.
"A New Theory of Urban Design" (1987) coincided with a renewal of interest in urbanism among architects, but stood apart from most other expressions of this by assuming a distinctly anti-masterplanning stance. An account of a design studio conducted with Berkeley students, it shows how convincing urban networks can be generated by requiring individual actors to respect only "local" rules, in relation to neighbours. A vastly undervalued part of the Alexander canon, "A New Theory" is important in understanding the generative processes which give rise to the shanty towns latterly championed by Stewart Brand, Robert Neuwirth, and the Prince of Wales.
"The Nature of Order: An Essay on the Art of Building and the Nature of the Universe" (2003–04), which includes The "Phenomenon of Life", "The Process of Creating Life", "A Vision of a Living World" and "The Luminous Ground", is Alexander's latest, and most comprehensive and elaborate work. In it, he puts forth a new theory about the nature of space and describes how this theory influences thinking about architecture, building, planning, and the way in which we view the world in general. The mostly static patterns from "A Pattern Language" have been amended by more dynamic sequences, which describe how to work towards patterns (which can roughly be seen as the end result of sequences). Sequences, like patterns, promise to be tools of wider scope than building (just as his theory of space goes beyond architecture).
The online publication (September 2004) includes several essays by Christopher Alexander, as well as the legendary between Alexander and Peter Eisenman from 1982.
Buildings.
Among Alexander's most notable built works are the Eishin Campus near Tokyo (the building process of which is outlined in his 2012 book "The Battle for the Life and Beauty of the Earth"); the West Dean Visitors Centre in West Sussex, England; the Julian Street Inn (a homeless shelter) in San Jose, California (both described in "Nature of Order"); the Martinez House (an experimental house in Martinez, California made of lightweight concrete); the low-cost housing in Mexicali, Mexico (described in "The Production of Houses"); and several private houses (described and illustrated in "The Nature of Order"). Alexander's built work is characterized by a special quality (which he used to call "the quality without a name", but named "wholeness" in "Nature of Order") that relates to human beings and induces feelings of belonging to the place and structure. This quality is found in the most loved traditional and historic buildings and urban spaces, and is precisely what Alexander has tried to capture with his sophisticated mathematical design theories. Paradoxically, achieving this connective human quality has also moved his buildings away from the abstract imageability valued in contemporary architecture, and this is one reason why his buildings are under-appreciated at present.
Michael Mehaffy wrote an on Christopher Alexander's built work in the online publication , which includes a of Alexander's major built projects to date (September 2004).
Teaching.
Apart from his lengthy teaching career at Berkeley (during which a number of international students began to appreciate and apply his methods), Alexander was a key member of faculty both of The Prince of Wales's Summer Schools in Civil Architecture (1990–1994) and The Prince of Wales's Institute of Architecture
Influence.
Architecture.
Alexander's work has widely influenced architects; those that acknowledge his influence include Sarah Susanka, among others.
Architecture critic Peter Buchanan, in an essay for The Architectural Review's 2012 campaign "The Big Rethink", depicts the challenge posed by Alexander's work as follows:
In the UK the developers Living Villages have been highly influenced by Alexander's work and used A Pattern Language as the basis for the design of The Wintles in Bishops Castle, Shropshire.
Computer science.
Alexander's "Notes on the Synthesis of Form" was required reading for researchers in computer science throughout the 1960s. It had an influence in the 1960s and 1970s on programming language design, modular programming, object-oriented programming, software engineering and other design methodologies. Alexander's mathematical concepts and orientation were similar to Edsger Dijkstra's influential "A Discipline of Programming".
"A Pattern Language"‘s greatest influence in computer science is the design patterns movement. Alexander's philosophy of incremental, organic, coherent design also influenced the extreme programming movement. The Wiki was invented to allow the Hillside Group to work on programming design patterns. More recently, The Nature of Order's "deep geometrical structures" have been cited as having importance for object-oriented programming, particularly in C++.
Will Wright wrote that Alexander's work was influential in the origin of "The Sims" computer game, and in his later game "Spore".
Alexander often leads his own software research, far from mainstream computing, such as the 1996 Gatemaker project with Greg Bryant.
Religion.
The fourth volume of "The Nature of Order" approaches religious questions from a scientific rather than mystical direction. In it, Alexander describes deep ties between the nature of matter, human perception of the universe, and the geometries people construct in buildings, cities, and artifacts, and he suggests a crucial link between traditional beliefs and recent scientific advances. Despite his leanings toward Deism, Alexander has retained a respect for the Catholic Church, believing it to embody a great deal of accumulated human truth within its rituals.
Published works.
Alexander's published works include:
Unpublished:

</doc>
<doc id="7614" url="http://en.wikipedia.org/wiki?curid=7614" title="Clabbers">
Clabbers

Clabbers is a game played by tournament Scrabble players for fun, or occasionally at Scrabble variant tournaments. The name derives from the fact that the words CLABBERS and SCRABBLE form an anagram pair.
Rules.
The rules are identical to those of Scrabble, except that valid plays are only required to form anagrams of acceptable words; in other words, the letters in a word do not need to be placed in the correct order. If a word is challenged, the player who played the word must then name an acceptable word that anagrams to the tiles played.
Because the number of "words" that can be formed is vastly larger than in standard English, the board usually ends up tightly packed in places, and necessarily quite empty in others. Game scores will often be much higher than in standard Scrabble, due to the relative ease of making high-scoring overlap plays and easier access to premium squares.
Web Version.
The Internet Scrabble Club offers the ability to play Clabbers online.
Example game (SOWPODS).
Horizontal words from top to bottom (# denotes words that exist in the Collins English Dictionary but not the TWL). Some of the words below have multiple anagrams:
Vertical words from left to right

</doc>
<doc id="7616" url="http://en.wikipedia.org/wiki?curid=7616" title="Canopus (disambiguation)">
Canopus (disambiguation)

Canopus may refer to:

</doc>
<doc id="7617" url="http://en.wikipedia.org/wiki?curid=7617" title="Corum Jhaelen Irsei">
Corum Jhaelen Irsei

Corum Jhaelen Irsei ("the Prince in the Scarlet Robe") is the name of a fictional fantasy hero in a series of two trilogies written by author Michael Moorcock.
Plot summary.
Corum is the last survivor of the Vadhagh race and an incarnation aspect of the Eternal Champion, a being that exists in all worlds to ensure there is "Cosmic Balance".
"Corum: The Prince in the Scarlet Robe" (The Swords Trilogy).
Corum is a Vadhagh, one of a race of long-lived beings with limited magical abilities dedicated to peaceful pursuits such as art and poetry. A group of "Mabden" (men) led by the savage Earl Glandyth-a-Krae raid the family castle and slaughter everyone with the exception of Corum, who escapes. Arming himself, Corum attacks and kills several of the Mabden before being captured and tortured. After having his left hand cut off and right eye put out, Corum escapes by moving into another plane of existence, becoming invisible to the Mabden. They depart and Corum is found by The Brown Man, a dweller of the forest of Laar able to see Corum while out of phase. The Brown Man takes Corum to a being called Arkyn, who treats his wounds and explains he has a higher purpose. 
Travelling to Moidel's Castle, Corum encounters his future lover, the Margravine Rhalina. Rhalina uses sorcery (a ship summoned from the depths of the ocean and manned by her drowned dead husband and crew) to ward off an attack by Glandyth-a-Krae. Determined to restore himself, Corum and Rhalina travel to the island of Shool, a near immortal and mad sorcerer. During the journey Corum observes a mysterious giant who trawls the ocean with a net. On arrival at the island Shool takes Rhalina hostage, and then provides Corum with two artifacts to replace his lost hand and eye: the Hand of Kwll and the Eye of Rhynn. The Eye of Rhynn allows Corum to see into an undead netherworld where the last beings killed by Corum exist until summoned by the Hand of Kwll. 
Shool then explains that Corum's ill fortune has been caused by the Chaos God Arioch, the Knight of the Swords. When Arioch and his fellow Chaos Lords conquered the Fifteen Planes, the balance between the forces of Law and Chaos tipped in favor of Chaos, and their minions - such as Glandyth-a-Krae - embarked on a bloody rampage. Shool sends Corum to Arioch's fortress to steal the Heart of Arioch, which the sorcerer intends to use to attain greater power. Corum confronts Arioch, and learns Shool is nothing more than a pawn of the Chaos God. Arioch then ignores Corum, who discovers the location of the Heart. Corum is then attacked by Arioch, but the Hand of Kwll crushes the Heart and banishes the Chaos God forever. Before fading from existence, Arioch warns Corum that he has now earned the enmity of the Sword Rulers. Corum returns to the island to rescue Rhalina, and observes Shool has become a powerless moron, and is devoured by his own creations soon afterwards. Corum learns Arkyn is in fact a Lord of Law, and that this is the first step towards Law regaining control of the Fifteen Planes.
On another five planes, the forces of Chaos - led by Xiombarg, Queen of the Swords - reign supreme and are on the verge on eradicating the last resistance from the forces of Law. The avatars of the Bear and Dog gods plot with Earl Glandyth-a-Krae to murder Corum and return Arioch to the Fifteen Planes. Guided by Arkyn, Corum, Rhalina and companion Jhary-a-Conel cross the planes and encounter the King Without A Country, the last of his people who in turn is seeking the City in the Pyramid. The group locate the City, which is in fact a floating arsenal powered by advanced technology and inhabited by a people originally from Corum's world and his distant kin. 
Besieged by the forces of Chaos, the City requires certain rare minerals to continue to power their weapons. Corum and Jhary attempt to locate the minerals and also encounter Xiombarg, who learns of Corum's identity. Corum slows Xiombarg's forces by defeating their leader, Prince Gaynor the Damned. Xiombarg is goaded into attacking the City directly in revenge for Arioch's banishment. Arkyn provides the minerals and confronts Xiombarg, who has manifested in a vulnerable state. As Arkyn banishes Xiombarg, Corum and his allies devastate the forces of Chaos. Glandyth-a-Krae, however, escapes, and seeks revenge.
A spell - determined to have been cast by the forces of Chaos - forces the inhabitants of Corum's plane to war with each other (including the City in the Pyramid). Desperate to stop the slaughter, Corum, Rhalina and Jhary Corum travel to the last five planes, ruled by Mabelode, the King of the Swords. Rhalina is taken hostage by the forces of Chaos and Corum has several encounters with the forces of Chaos, including Earl Glandyth-a-Krae. 
Corum also meets two other aspects of the Eternal Champion: Elric and Erekose, with all three seeking the mystical city of Tanelorn for their own purposes. After a brief adventure in the "Vanishing Tower", the other heroes depart and Corum and Jhary arrive at their version of Tanelorn. Corum discovers one of the "Lost Gods", the being Kwll, who is imprisoned and cannot be freed until whole. Corum offers Kwll his hand, on the condition that he aid them against Mabelode. Kwll accepts the terms, but reneges on the bargain until persuaded to assist. Corum is also stripped of his artificial eye, which belongs to Rhynn - actually the mysterious giant Corum had previously encountered. Kwll transports Corum and Jhary to the court of Mabelode, with the pair fleeing with Rhalina when Kwll directly challenges the Chaos God.
Having found out Corum's location by torturing and killing the Brown Man of Laar, Glandyth-a-Krae marshalled his allies to Moidel's Castle. Glandyth had kept Corum's former hand and eye as souvenirs, and showed them to Corum to provoke a reaction.
In a final battle Corum avenged his family by killing Glandyth-a-Krae and decimating the last of Chaos' mortal forces. Kwll later located Corum and revealed that all the gods - of both Chaos and Law - have been slain in order to free humanity and allow it to shape its own destiny.
"Corum: The Prince with the Silver Hand".
Set eighty years after the defeat of the Sword Rulers, Corum has become despondent and alone since the death of his Mabden bride Rhalina. Plagued by voices at night, Corum believes he has gone insane until old friend Jhary-a-Conel advises Corum it is in fact a summons from another world. Listening to the voices allows Corum to pass to the other world, which is in fact the distant future. Rhalina's descendants, the Tuha-na-Cremm Croich (who call Corum "Corum Llew Ereint") face extinction from the Fhoi Myore: seven giants who with their allies conquered the land and plunged it into eternal winter. Allying himself with King Mannach, Corum falls in love with his daughter Medhbh.
Corum also hears the prophecy of a seeress, who claims Corum should fear a brother (who will apparently slay him), a harp and above all, beauty. Corum seeks the lost artifacts of the Tuha-na-Cremm Croich - a sacred Bull, a spear, an oak, a ram and a stallion - which will restore the land. Together with new allies Goffanon (a blacksmith and diminutive giant) and Goffanon's cousin and true giant Illbrec battles the Fhoi Myore and their own allies, a returned Prince Gaynor, the wizard Calatin and his clone of Corum, the Brothers of the Pine, the undead Ghoolegh and a host of giant wolves. After being instrumental in the death of two of the Fhoi Myore and restoring the High King of the Tuha-na-Cremm Croich, Corum and his allies have a final battle in which all their foes are destroyed.
Corum decides not to return his own world, and is attacked by his clone, whom he defeats with the aid of a spell placed on his silver hand by Medhbh. Medhbh, however, attacks and wounds Corum, having been told by the being the Dagdah that their world must be free of all gods and demi-gods if they are to flourish as a people. Corum is then killed with his own sword by his animated silver hand, thereby fulfilling the prophecy.
In other media.
First Comics published "The Chronicles of Corum", a twelve issue limited series (Jan. 1986 - Dec. 1988) that adapted the "Swords Trilogy", and was followed by the four issue limited series "Corum: The Bull and the Spear" (Jan. - July (bi-monthly) 1989), which adapted the first book in the second trilogy. 
Darcsyde Productions produced a supplement for use with Chaosium's "Stormbringer" (2001) role-playing game adapting the characters and settings from the "Corum" series for role-playing.
Gollancz have announced plans to release the entire Corum stories in both print and ebook form, commencing in 2013. The ebooks will be available via Gollancz's SF Gateway site.
Bibliography.
"First trilogy":
"Second trilogy":
"Additional appearances":
Note: In the United Kingdom the first trilogy has been collected as an omnibus edition titled "Corum", "Swords of Corum" and most recently "Corum: The Prince in the Scarlet Robe" (vol. 30 of Orion's Fantasy Masterworks series). In the United States the first trilogy has been published as "Corum: The Coming of Chaos". The second trilogy was titled "The Prince with the Silver Hand" (United Kingdom) and "The Chronicles of Corum" (United States) respectively.

</doc>
<doc id="7618" url="http://en.wikipedia.org/wiki?curid=7618" title="Cumberland (disambiguation)">
Cumberland (disambiguation)

Cumberland is one of the historic counties of England. 
Cumberland may also refer to:
Places.
In Australia:
In Canada:
In England:
In Saint Vincent and the Grenadines:
In the United States:
There are also:
Other geographical features.
In Canada:
In Chile:
In the United States:

</doc>
<doc id="7619" url="http://en.wikipedia.org/wiki?curid=7619" title="Capella (disambiguation)">
Capella (disambiguation)

Capella is a bright star in the constellation of Auriga – from the classical Latin for "she goat".
Capella may also refer to:

</doc>
<doc id="7622" url="http://en.wikipedia.org/wiki?curid=7622" title="Complex instruction set computing">
Complex instruction set computing

Complex instruction set computing (CISC ) is a CPU design where single instructions can execute several low-level operations (such as a load from memory, an arithmetic operation, and a memory store) or are capable of multi-step operations or addressing modes within single instructions. The term was retroactively coined in contrast to reduced instruction set computer (RISC).
Examples of CISC instruction set architectures are System/360 through z/Architecture, PDP-11, VAX, Motorola 68k, and x86.
Historical design context.
Incitements and benefits.
Before the RISC philosophy became prominent, many computer architects tried to bridge the so-called semantic gap, i.e. to design instruction sets that directly supported high-level programming constructs such as procedure calls, loop control, and complex addressing modes, allowing data structure and array accesses to be combined into single instructions. Instructions are also typically highly encoded in order to further enhance the code density. The compact nature of such instruction sets results in smaller program sizes and fewer (slow) main memory accesses, which at the time (early 1960s and onwards) resulted in a tremendous savings on the cost of computer memory and disc storage, as well as faster execution. It also meant good programming productivity even in assembly language, as high level languages such as Fortran or Algol were not always available or appropriate (microprocessors in this category are sometimes still programmed in assembly language for certain types of critical applications).
New instructions.
In the 1970s, analysis of high level languages indicated some complex machine language implementations and it was determined that new instructions could improve performance. Some instructions were added that were never intended to be used in assembly language but fit well with compiled high level languages. Compilers were updated to take advantage of these instructions. The benefits of semantically rich instructions with compact encodings can be seen in modern processors as well, particularly in the high performance segment where caches are a central component (as opposed to most embedded systems). This is because these fast, but complex and expensive, memories are inherently limited in size, making compact code beneficial. Of course, the fundamental reason they are needed is that main memories (i.e. dynamic RAM today) remain slow compared to a (high performance) CPU-core.
Design issues.
While many designs achieved the aim of higher throughput at lower cost and also allowed high-level language constructs to be expressed by fewer instructions, it was observed that this was not "always" the case. For instance, low-end versions of complex architectures (i.e. using less hardware) could lead to situations where it was possible to improve performance by "not" using a complex instruction (such as a procedure call or enter instruction), but instead using a sequence of simpler instructions.
One reason for this was that architects (microcode writers) sometimes "over-designed" assembler language instructions, i.e. including features which were not possible to implement efficiently on the basic hardware available. This could, for instance, be "side effects" (above conventional flags), such as the setting of a register or memory location that was perhaps seldom used; if this was done via ordinary (non duplicated) internal buses, or even the "external" bus, it would demand extra cycles every time, and thus be quite inefficient.
Even in balanced high performance designs, highly encoded and (relatively) high-level instructions could be complicated to decode and execute efficiently within a limited transistor budget. Such architectures therefore required a great deal of work on the part of the processor designer in cases where a simpler, but (typically) slower, solution based on decode tables and/or microcode sequencing is not appropriate. At a time when transistors and other components were a limited resource, this also left fewer components and less opportunity for other types of performance optimizations.
The RISC idea.
The circuitry that performs the actions defined by the microcode in many (but not all) CISC processors is, in itself, a processor which in many ways is reminiscent in structure to very early CPU designs. In the early 1970s, this gave rise to ideas to return to simpler processor designs in order to make it more feasible to cope without ("then" relatively large and expensive) ROM tables and/or PLA structures for sequencing and/or decoding. The first (retroactively) RISC-"labeled" processor (IBM 801 - IBM's Watson Research Center, mid-1970s) was a tightly pipelined simple machine originally intended to be used as an internal microcode kernel, or engine, in CISC designs, but also became the processor that introduced the RISC idea to a somewhat larger public. Simplicity and regularity also in the visible instruction set would make it easier to implement overlapping processor stages (pipelining) at the machine code level (i.e. the level seen by compilers.) However, pipelining at that level was already used in some high performance CISC "supercomputers" in order to reduce the instruction cycle time (despite the complications of implementing within the limited component count and wiring complexity feasible at the time). Internal microcode execution in CISC processors, on the other hand, could be more or less pipelined depending on the particular design, and therefore more or less akin to the basic structure of RISC processors.
Superscalar.
In a more modern context, the complex variable length encoding used by some of the typical CISC architectures makes it complicated, but still feasible, to build a superscalar implementation of a CISC programming model "directly"; the in-order superscalar original Pentium and the out-of-order superscalar Cyrix 6x86 are well known examples of this. The frequent memory accesses for operands of a typical CISC machine may limit the instruction level parallelism that can be extracted from the code, although this is strongly mediated by the fast cache structures used in modern designs, as well as by other measures. Due to inherently compact and semantically rich instructions, the average amount of work performed per machine code unit (i.e. per byte or bit) is higher for a CISC than a RISC processor, which may give it a significant advantage in a modern cache based implementation.
Transistors for logic, PLAs, and microcode are no longer scarce resources; only large high-speed cache memories are limited by the maximum number of transistors today. Although complex, the transistor count of CISC decoders do not grow exponentially like the total number of transistors per processor (the majority typically used for caches). Together with better tools and enhanced technologies, this has led to new implementations of highly encoded and variable length designs without load-store limitations (i.e. non-RISC). This governs re-implementations of older architectures such as the ubiquitous x86 (see below) as well as new designs for microcontrollers for embedded systems, and similar uses. The superscalar complexity in the case of modern x86 was solved by converting instructions into one or more micro-operations and dynamically issuing those micro-operations, i.e. indirect and dynamic superscalar execution; the Pentium Pro and AMD K5 are early examples of this. It allows a fairly simple superscalar design to be located after the (fairly complex) decoders (and buffers), giving, so to speak, the best of both worlds in many respects.
CISC and RISC terms.
The terms CISC and RISC have become less meaningful with the continued evolution of both CISC and RISC designs and implementations. The first highly (or tightly) pipelined x86 implementations, the 486 designs from Intel, AMD, Cyrix, and IBM, supported every instruction that their predecessors did, but achieved "maximum efficiency" only on a fairly simple x86 subset that was only a little more than a typical RISC instruction set (i.e. without typical RISC "load-store" limitations). The Intel P5 Pentium generation was a superscalar version of these principles. However, modern x86 processors also (typically) decode and split instructions into dynamic sequences of internally buffered micro-operations, which not only helps execute a larger subset of instructions in a pipelined (overlapping) fashion, but also facilitates more advanced extraction of parallelism out of the code stream, for even higher performance.
Contrary to popular simplifications (present also in some academic texts), not all CISCs are microcoded or have "complex" instructions. As CISC became a catch-all term meaning anything that's not a load-store (RISC) architecture, it's not the number of instructions, nor the complexity of the implementation or of the instructions themselves, that define CISC, but the fact that arithmetic instructions also perform memory accesses. Compared to a small 8-bit CISC processor, a RISC floating-point instruction is complex. CISC does not even need to have complex addressing modes; 32 or 64-bit RISC processors may well have more complex addressing modes than small 8-bit CISC processors.
A PDP-10, a PDP-8, an Intel 386, an Intel 4004, a Motorola 68000, a System z mainframe, a Burroughs B5000, a VAX, a Zilog Z80000, and a MOS Technology 6502 all vary wildly in the number, sizes, and formats of instructions, the number, types, and sizes of registers, and the available data types. Some have hardware support for operations like scanning for a substring, arbitrary-precision BCD arithmetic, or transcendental functions, while others have only 8-bit addition and subtraction. But they are all in the CISC category because they have "load-operate" instructions that load and/or store memory contents within the same instructions that perform the actual calculations. For instance, the PDP-8, having only 8 fixed-length instructions and no microcode at all, is a CISC because of "how" the instructions work, PowerPC, which has over 230 instructions (more than some VAXes) and complex internals like register renaming and a reorder buffer is a RISC, while has 8 instructions, but is clearly a CISC because it combines memory access and computation in the same instructions.
Some of the problems and contradictions in this terminology will perhaps disappear as more systematic terms, such as (non) load/store, become more popular and eventually replace the imprecise and slightly counter-intuitive RISC/CISC terms.

</doc>
<doc id="7624" url="http://en.wikipedia.org/wiki?curid=7624" title="CISC">
CISC

CISC may refer to:

</doc>
<doc id="7626" url="http://en.wikipedia.org/wiki?curid=7626" title="Cetacea">
Cetacea

The order Cetacea includes the marine mammals commonly known as whales, dolphins, and porpoises. "Cetus" is Latin and is used in biological names to mean 'whale'. Its original meaning, 'large sea animal', was more general. It comes from Ancient Greek (""), meaning 'whale' or "any huge fish or sea monster". In Greek mythology, the monster Keto defeated was A daughter of Gaia and Pontus, Keto was a hideous sea monster in Greek mythology. She was considered the personification of the dangers of the sea. Her husband was Phorcys, and their children were called the Phorcydes.was never defeated by a hero, but which is depicted by the constellation of Cetus. Cetology is the branch of marine science associated with the study of cetaceans.An ancient ancestor of the whale, Basilosaurus was thought to be a reptile until vestigial parts were recognized.
Fossil evidence suggests that the cetaceans share a common ancestor with land-dwelling mammals that began living in marine environments around 50 million years ago. Today, they are the mammals best adapted to aquatic life. The body of a cetacean is fusiform (spindle-shaped). The forelimbs are modified into flippers. The tiny hindlimbs are vestigial; they do not attach to the backbone and are hidden within the body. The tail has horizontal flukes. Cetaceans are nearly hairless, and are insulated from the cooler water they inhabit by a thick layer of blubber.
Some species are attributed with high levels of intelligence. At the 2012 meeting of the American Association for the Advancement of Science, support was reiterated for a cetacean bill of rights, listing cetaceans as non-human persons.
Physical characteristics.
Respiration.
Cetaceans breathe air, and surface periodically to exhale carbon dioxide and inhale a fresh supply of oxygen. During diving, a muscular action closes the blowholes (nostrils), which remain closed until the cetacean comes to the surface; when it surfaces, the muscles open the blowholes and warm air is exhaled.
Cetaceans' blowholes have evolved to a position at the top of the head, simplifying breathing in sometimes rough seas. When the stale air, warmed from the lungs, is exhaled, it condenses as it meets colder external air. As with a terrestrial mammal breathing out on a cold day, a small cloud of 'steam' appears. This is called the 'blow' or 'spout' and varies by species in terms of shape, angle, and height. Species can be identified at a distance using this characteristic.
Cetaceans can remain under water for much longer periods than most other mammals, (about seven to 120 minutes, varying by species) due to large physiological differences. Two studied advantages of cetacean physiology let this order (and other marine mammals) forage underwater for extended periods without breathing:
Vision, hearing and echolocation.
Cetacean eyes are set on the sides rather than the front of the head. This means only cetaceans with pointed 'beaks' (such as dolphins) have good binocular vision forward and downward. Tear glands secrete greasy tears, which protect the eyes from the salt in the water. The lens is almost spherical, which is most efficient at focusing the minimal light that reaches deep water. Cetaceans make up for their generally poor vision (with the exception of the dolphin) with excellent hearing.
The external ear of cetaceans has lost the pinna (visible ear), but still retains an extremely narrow external auditory meatus. To register sounds, instead, the posterior part of the mandible has a thin lateral wall (the pan bone) behind which a concavity houses a large fat pad. The fat pad passes anteriorly into the greatly enlarged mandibular foramen to reach in under the teeth, and posteriorly to reach the thin lateral wall of the ectotympanic. The ectotympanic only offers a reduced attachment area for the tympanic membrane and the connection between this auditory complex and the rest of the skull is reduced in cetaceans — to a single, small cartilage in oceanic dolphins. In odontocetes, the complex is surrounded by spongy tissue filled with air spaces, while in mysticetes it is integrated into the skull similar to land mammals. In odontocetes, the tympanic membrane (or ligament) has the shape of a folded-in umbrella that stretches from the ectotympanic ring and narrows off to the malleus (quite unlike the flat, circular membrane found in land mammals.) In mysticetes, it also forms a large protrusion (known as the "glove finger"), which stretches into the external meatus, and the stapes are larger than in odontocetes. In some small sperm whales, the malleus is fused with the ectotympanic. The ear ossicles are pachyosteosclerotic (dense and compact) in cetaceans and very different in shape compared to land mammals (other aquatic mammals, such as sirenians and earless seals, have also lost their pinnae). In modern cetaceans, the semicircular canals are much smaller relative to body size than in other mammals.
In modern cetaceans, the auditory bulla is separated from the skull and composed of two compact and dense bones (the periotic and tympanic) referred to as the tympano-periotic complex. This complex is located in a cavity in the middle ear, which, in Mysticeti, is divided by a bony projection and compressed between the exoccipital and squamosal but, in Odontoceti, is large and completely surrounds the bulla (hence called "peribullar"), which is therefore not connected to the skull except in physeterids. In odontoceti, the cavity is filled with a dense foam in which the bulla hangs suspended in five or more sets of ligaments. The pterygoid and peribullar sinuses that form the cavity tend to be more developed in shallow water and riverine species than in pelagic mysticeti. In odontoceti, the composite auditory structure is thought to serve as an acoustic isolator, analogous to the lamellar construction found in the temporal bone in bats.
Odontoceti (toothed whales, which includes dolphins and porpoises) are generally capable of echolocation. From this, Odontoceti can discern the size, shape, surface characteristics, distance and movement of an object. With this ability, cetaceans can search for, chase and catch fast-swimming prey in total darkness. Echolocation is so advanced in most Odontoceti, they can distinguish between prey and nonprey (such as humans or boats); captive Odontoceti can be trained to distinguish between, for example, balls of different sizes or shapes. Mysticeti (baleen whales) have exceptionally thin, wide basilar membranes in their cochleae without stiffening agents, making their ears adapted for processing low to infrasonic frequencies.
Cetaceans also use sound to communicate, whether it be groans, moans, whistles, clicks, or the complex 'singing' of the humpback whale. Besides hearing and vision, at least one species, the tucuxi or Guiana dolphin, is able to use electroreception to sense prey.
Feeding.
The toothed whales, such as the sperm whale, beluga, dolphins, and porpoises, have teeth they use for catching fish, squid or other marine life. They do not chew, but swallow prey whole. When they catch large prey, such as when the killer whale ("Orcinus orca") catches a seal, they bite off and swallow one chunk at a time.
Instead of teeth, the Mysticeti have baleen plates made of keratin (the same substance as human fingernails), which hang from the upper jaw. These plates filter small animals (such as krill and fish) from the seawater. Cetaceans included in this group include the blue, humpback, bowhead, and minke whales.
Not all Mysticeti feed on plankton; the larger species eat small shoaling fish, such as herring and sardines, called micronecton. The gray whale ("Eschrichtius robustus"), is a benthic feeder, primarily eating sea-floor crustaceans.
Taxonomy.
The order Cetacea contains about 90 species, all marine except for four species of freshwater dolphins. The order contains two suborders: Mysticeti (baleen whales) and Odontoceti (toothed whales, which includes dolphins and porpoises). The species range in size from Commerson's dolphin, smaller than a human, to the blue whale, the largest animal ever known to have lived.
Cetaceans are mammals, that is, members of the class Mammalia. Their closest living relatives are the even-toed ungulates, such as the hippopotamus and deer.
Mammalian characteristics include warm-bloodedness, breathing air through their lungs, suckling their young, and growing hair, although very little of it.
Another way of distinguishing a cetacean from a fish is the shape of the tail. Fish tails are vertical and move from side to side when the fish swims. A cetacean's tail — called a fluke — is horizontal and moves up and down, because cetacean spines bend in the same manner as a human spine.
Evolution.
The cetaceans (whales, dolphins and porpoises) are marine mammal descendants of the artiodactyl family Raoellidae, a group of land mammals characterized by an even-toed ungulate skull, slim limbs, and an ear with significant similarities to that of early whales. The terrestrial origins of cetaceans are indicated first by their need to breathe air from the surface, the bones of their fins, which resemble the limbs of land mammals, and by the vestigial remains of hind legs inherited from their four-legged ancestors.
The question of how land animals became ocean-going was a mystery until discoveries starting in the late 1970s in Pakistan revealed several stages in the transition of cetaceans from land to sea:
This image does not capture the true phylogenetic evolution of a particular species, but it is an illustrative representation of the evolution of cetaceans from terrestrial four-legged mammals, from their probable ancestor, through different stages of adaptation to aquatic life to modern cetaceans type, hydrodynamic body shape, fully developed caudal fin and vestigial hind legs. The separation of cetaceans in suborder baleen whales and suborder toothed whales, occurred during the Oligocene ("Janjucetus" and "Squalodon" represent the early forms of their suborders).
Mysticeti vs Odontoceti.
Fossils indicate, before evolving baleen, the Mysticeti also had teeth, so defining the Odontoceti by teeth alone is problematic. Instead, paleontologists have identified other features uniting fossil and modern odontocetes that are not shared by Mysticeti. It was also assumed that toothed whales evolved their asymmetrical skulls as an adaptation to their echolocation, but newer discoveries indicate the common ancestor of the present whales actually had a contorted skull, as well. Cranial asymmetry is now known to have evolved in ancient whales as part of a set of traits linked to directional hearing, including pan-bone thinning of the lower jaws, the development of mandibular fat pads, and the isolation of the ear region. This likely means, while the asymmetry in the Odontoceti skull has increased over time, the Mysticeti skull has evolved from asymmetrical to symmetrical.
Classification.
The classification here closely follows Dale W. Rice, "Marine Mammals of the World: Systematics and Distribution" (1998), which has become the standard taxonomy reference in the field. There is very close agreement between this classification and that of "Mammal Species of the World: 3rd Edition" (Wilson and Reeder eds., 2005). Any differences are noted using the abbreviations "Rice" and "MSW3" respectively. Further differences due to recent discoveries are also noted.
Discussion of synonyms and subspecies are relegated to the relevant genus and species articles.
†Recently extinct

</doc>
<doc id="7627" url="http://en.wikipedia.org/wiki?curid=7627" title="The Canterbury Tales">
The Canterbury Tales

The Canterbury Tales (Middle English: "Tales of Caunterbury") is a collection of over 20 stories written in Middle English by Geoffrey Chaucer at the end of the 14th century, during the time of the Hundred Years' War. The tales (mostly written in verse, although some are in prose) are presented as part of a story-telling contest by a group of pilgrims as they travel together on a journey from Southwark to the shrine of Saint Thomas Becket at Canterbury Cathedral. The prize for this contest is a free meal at the Tabard Inn at Southwark on their return.
After a long list of works written earlier in his career, including "Troilus and Criseyde", "House of Fame", and "Parliament of Fowls", "The Canterbury Tales" is near-unanimously seen as Chaucer's magnum opus. He uses the tales and the descriptions of its characters to paint an ironic and critical portrait of English society at the time, and particularly of the Church. Structurally, the collection resembles "The Decameron", which Chaucer may have read during his first diplomatic mission to Italy in 1372.
It is sometimes argued that the greatest contribution "The Canterbury Tales" made to English literature was in popularising the literary use of vernacular, English, rather than French or Latin. English had, however, been used as a literary language centuries before Chaucer's life, and several of Chaucer's contemporaries — John Gower, William Langland, the Pearl Poet, and Julian of Norwich — also wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend as opposed to simply being part of it.
While Chaucer clearly states the addressees of many of his poems, the intended audience of "The Canterbury Tales" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for nobility.
Text.
The question of whether "The Canterbury Tales" is finished has not yet been answered. There are 83 known manuscripts of the work from the late medieval and early Renaissance periods, more than any other vernacular literary text with the exception of "The Prick of Conscience". This is taken as evidence of the tales' popularity during the century after Chaucer's death. Fifty-five of these manuscripts are thought to have been complete at one time, while 28 are so fragmentary that it is difficult to ascertain whether they were copied individually or as part of a set. The "Tales" vary in both minor and major ways from manuscript to manuscript; many of the minor variations are due to copyists' errors, while others suggest that Chaucer added to and revised his work as it was being copied and (possibly) distributed.
Even the earliest surviving manuscripts are not Chaucer's originals, the oldest being MS Peniarth 392 D (called "Hengwrt"), compiled by a scribe shortly after Chaucer's death. The most beautiful of the manuscripts of the tales is the Ellesmere Manuscript, and many editors have followed the order of the Ellesmere over the centuries, even down to the present day. The first version of "The Canterbury Tales" to be published in print was William Caxton's 1478 edition. Since this print edition was created from a now-lost manuscript, it is counted as among the 83 manuscripts.
Order.
No authorial, arguably complete version of the "Tales" exists and no consensus has been reached regarding the order in which Chaucer intended the stories to be placed.
Textual and manuscript clues have been adduced to support the two most popular modern methods of ordering the tales. Some scholarly editions divide the "Tales" into ten "Fragments". The tales that comprise a Fragment are closely related and contain internal indications of their order of presentation, usually with one character speaking to and then stepping aside for another character. However, between Fragments, the connection is less obvious. Consequently, there are several possible orders; the one most frequently seen in modern editions follows the numbering of the Fragments (ultimately based on the Ellesmere order). Victorians frequently used the nine "Groups", which was the order used by Walter William Skeat whose edition "Chaucer: Complete Works" was used by Oxford University Press for most of the twentieth century, but this order is now seldom followed.
An alternative ordering (seen in an early manuscript containing the Canterbury Tales, the early-fifteenth century Harley MS. 7334) places Fragment VIII before VI. Fragments I and II almost always follow each other, as do VI and VII, IX and X in the oldest manuscripts. Fragments IV and V, by contrast are located in varying locations from manuscript to manuscript.
Language.
Although no manuscript exists in Chaucer's own hand, two were copied around the time of his death by Adam Pinkhurst, a scribe with whom he seems to have worked closely before, giving a high degree of confidence that Chaucer himself wrote the "Tales". Chaucer's generation of English-speakers was among the last to pronounce "e" at the end of words (so for Chaucer the word "care" was pronounced , not as in modern English). This meant that later copyists tended to be inconsistent in their copying of final -"e" and this for many years gave scholars the impression that Chaucer himself was inconsistent in using it. It has now been established, however, that -"e" was an important part of Chaucer's morphology (having a role in distinguishing, for example, singular adjectives from plural and subjunctive verbs from indicative). The pronunciation of Chaucer's writing otherwise differs most prominently from Modern English in that his language had not undergone the Great Vowel Shift; pronouncing Chaucer's vowels as they would be pronounced today in European languages like Italian, Spanish or German generally produces pronunciations more like Chaucer's own than Modern English pronunciation would. In addition, sounds now written in English but not pronounced were still pronounced by Chaucer: the word <knight> for Chaucer was , not . The pronunciation of Chaucer's poetry can now be reconstructed fairly confidently through detailed philological research; the following gives an IPA reconstruction of the opening lines of "The Merchant's Prologue"; it is likely, moreover, that when a word ending in a vowel was followed by a word beginning in a vowel, the two vowels were elided into one syllable, as seen here (with care and...):
Sources.
No other work prior to Chaucer's is known to have set a collection of tales within the framework of pilgrims on a pilgrimage. It is obvious, however, that Chaucer borrowed portions, sometimes very large portions, of his stories from earlier stories, and that his work was influenced by the general state of the literary world in which he lived. Storytelling was the main entertainment in England at the time, and storytelling contests had been around for hundreds of years. In 14th-century England the English Pui was a group with an appointed leader who would judge the songs of the group. The winner received a crown and, as with the winner of the "Canterbury Tales", a free dinner. It was common for pilgrims on a pilgrimage to have a chosen "master of ceremonies" to guide them and organise the journey. Harold Bloom suggests that the structure is mostly original, but inspired by the "pilgrim" figures of Dante and Virgil in "The Divine Comedy".
"The Decameron" by Giovanni Boccaccio contains more parallels to the "Canterbury Tales" than any other work. Like the "Tales", it features a number of narrators who tell stories along a journey they have undertaken (to flee from the Black Plague). It ends with an apology by Boccaccio, much like Chaucer's Retraction to the "Tales". A quarter of the tales in "Canterbury Tales" parallel a tale in the "Decameron", although most of them have closer parallels in other stories. Some scholars thus find it unlikely that Chaucer had a copy of the work on hand, surmising instead that he must have merely read the "Decameron" at some point. Each of the tales has its own set of sources which have been suggested by scholars, but a few sources are used frequently over several tales. These include poetry by Ovid, the Bible in one of the many vulgate versions it was available in at the time (the exact one is difficult to determine), and the works of Petrarch and Dante. Chaucer was the first author to utilise the work of these last two, both Italians. Boethius' "Consolation of Philosophy" appears in several tales, as do the works of John Gower, a known friend to Chaucer. A full list is impossible to outline in little space, but Chaucer also, lastly, seems to have borrowed from numerous religious encyclopaedias and liturgical writings, such as John Bromyard's "Summa praedicantium", a preacher's handbook, and Jerome's "Adversus Jovinianum". Many scholars say there is a good possibility Chaucer met Petrarch or Boccaccio.
Genre and structure.
"Canterbury Tales" is a collection of stories built around a frame narrative or frame tale, a common and already long established genre of its period. Chaucer's "Tales" differs from most other story "collections" in this genre chiefly in its intense variation. Most story collections focused on a theme, usually a religious one. Even in the "Decameron", storytellers are encouraged to stick to the theme decided on for the day. The idea of a pilgrimage to get such a diverse collection of people together for literary purposes was also unprecedented, though "the association of pilgrims and storytelling was a familiar one". Introducing a competition among the tales encourages the reader to compare the tales in all their variety, and allows Chaucer to showcase the breadth of his skill in different genres and literary forms.
While the structure of the "Tales" is largely linear, with one story following another, it is also much more than that. In the "General Prologue", Chaucer describes, not the tales to be told, but the people who will tell them, making it clear that structure will depend on the characters rather than a general theme or moral. This idea is reinforced when the Miller interrupts to tell his tale after the Knight has finished his. Having the Knight go first, gives one the idea that all will tell their stories by class, with the Knight going first, followed by the Monk, but the Miller's interruption makes it clear that this structure will be abandoned in favour of a free and open exchange of stories among all classes present. General themes and points of view arise as tales are told which are responded to by other characters in their own tales, sometimes after a long lapse in which the theme has not been addressed.
Lastly, Chaucer does not pay much attention to the progress of the trip, to the time passing as the pilgrims travel, or specific locations along the way to Canterbury. His writing of the story seems focused primarily on the stories being told, and not on the pilgrimage itself.
Style.
The variety of Chaucer's tales shows the breadth of his skill and his familiarity with countless rhetorical forms and linguistic styles. Medieval schools of rhetoric at the time encouraged such diversity, dividing literature (as Virgil suggests) into high, middle, and low styles as measured by the density of rhetorical forms and vocabulary. Another popular method of division came from St. Augustine, who focused more on audience response and less on subject matter (a Virgilian concern). Augustine divided literature into "majestic persuades", "temperate pleases", and "subdued teaches". Writers were encouraged to write in a way that kept in mind the speaker, subject, audience, purpose, manner, and occasion. Chaucer moves freely between all of these styles, showing favouritism to none. He not only considers the readers of his work as an audience, but the other pilgrims within the story as well, creating a multi-layered rhetorical puzzle of ambiguities. Chaucer's work thus far surpasses the ability of any single medieval theory to uncover.
With this Chaucer avoids targeting any specific audience or social class of readers, focusing instead on the characters of the story and writing their tales with a skill proportional to their social status and learning. However, even the lowest characters, such as the Miller, show surprising rhetorical ability, although their subject matter is more lowbrow. Vocabulary also plays an important part, as those of the higher classes refer to a woman as a "lady", while the lower classes use the word "wenche", with no exceptions. At times the same word will mean entirely different things between classes. The word "pitee", for example, is a noble concept to the upper classes, while in the "Merchant's Tale" it refers to sexual intercourse. Again, however, tales such as the "Nun's Priest's Tale" show surprising skill with words among the lower classes of the group, while the "Knight's Tale" is at times extremely simple.
Chaucer uses the same meter throughout almost all of his tales, with the exception of "Sir Thopas" and his prose tales. It is a decasyllable line, probably borrowed from French and Italian forms, with riding rhyme and, occasionally, a caesura in the middle of a line. His meter would later develop into the heroic meter of the 15th and 16th centuries and is an ancestor of iambic pentameter. He avoids allowing couplets to become too prominent in the poem, and four of the tales (the Man of Law's, Clerk's, Prioress', and Second Nun's) use rhyme royal.
Historical context and themes.
 "The Canterbury Tales" was written during a turbulent time in English history. The Catholic Church was in the midst of the Western Schism and, though it was still the only Christian authority in Europe, was the subject of heavy controversy. Lollardy, an early English religious movement led by John Wycliffe, is mentioned in the "Tales", as is a specific incident involving pardoners (who gathered money in exchange for absolution from sin) who nefariously claimed to be collecting for St. Mary Rouncesval hospital in England. "The Canterbury Tales" is among the first English literary works to mention paper, a relatively new invention which allowed dissemination of the written word never before seen in England. Political clashes, such as the 1381 Peasants' Revolt and clashes ending in the deposing of King Richard II, further reveal the complex turmoil surrounding Chaucer in the time of the "Tales"' writing. Many of his close friends were executed and he himself was forced to move to Kent to get away from events in London.
In 2004, Professor Linne Mooney was able to identify the scrivener who worked for Chaucer as an Adam Pinkhurst. Mooney, then a professor at the University of Maine and a visiting fellow at Corpus Christi College, Cambridge, was able to match Pinkhurst's signature, on an oath he signed, to his lettering on a copy of "The Canterbury Tales" that was transcribed from Chaucer's working copy. While some readers look to interpret the characters of "The Canterbury Tales" as historical figures, other readers choose to interpret its significance in less literal terms. After analysis of his diction and historical context, his work appears to develop a critique against society during his lifetime. Within a number of his descriptions, his comments can appear complimentary in nature, but through clever language, the statements are ultimately critical of the pilgrim's actions. It is unclear whether Chaucer would intend for the reader to link his characters with actual persons. Instead, it appears that Chaucer creates fictional characters to be general representations of people in such fields of work. With an understanding of medieval society, one can detect subtle satire at work.
Religion.
The "Tales" reflect diverse views of the Church in Chaucer's England. After the Black Death, many Europeans began to question the authority of the established Church. Some turned to lollardy, while others chose less extreme paths, starting new monastic orders or smaller movements exposing church corruption in the behaviour of the clergy, false church relics or abuse of indulgences. Several characters in the "Tales" are religious figures, and the very setting of the pilgrimage to Canterbury is religious (although the prologue comments ironically on its merely seasonal attractions), making religion a significant theme of the work.
Two characters, the Pardoner and the Summoner, whose roles apply the church's secular power, are both portrayed as deeply corrupt, greedy, and abusive. A pardoner in Chaucer's day was a person from whom one bought Church "indulgences" for forgiveness of sins, but pardoners were often thought guilty of abusing their office for their own gain. Chaucer's Pardoner openly admits the corruption of his practice while hawking his wares. The Summoner is a Church officer who brought sinners to the church court for possible excommunication and other penalties. Corrupt summoners would write false citations and frighten people into bribing them to protect their interests. Chaucer's Summoner is portrayed as guilty of the very kinds of sins he is threatening to bring others to court for, and is hinted as having a corrupt relationship with the Pardoner. In "The Friar's Tale", one of the characters is a summoner who is shown to be working on the side of the devil, not God.
Churchmen of various kinds are represented by the Monk, the Prioress, the Nun's Priest, and the Second Nun. Monastic orders, which originated from a desire to follow an ascetic lifestyle separated from the world, had by Chaucer's time become increasingly entangled in worldly matters. Monasteries frequently controlled huge tracts of land on which they made significant sums of money, while peasants worked in their employ. The Second Nun is an example of what a Nun was expected to be: her tale is about a woman whose chaste example brings people into the church. The Monk and the Prioress, on the other hand, while not as corrupt as the Summoner or Pardoner, fall far short of the ideal for their orders. Both are expensively dressed, show signs of lives of luxury and flirtatiousness and show a lack of spiritual depth. "The Prioress's Tale" is an account of Jews murdering a deeply pious and innocent Christian boy, a blood libel against Jews which became a part of English literary tradition. The story did not originate in the works of Chaucer and was well known in the 14th century.
Pilgrimage was a very prominent feature of medieval society. The ultimate pilgrimage destination was Jerusalem, but within England Canterbury was a popular destination. Pilgrims would journey to cathedrals that preserved relics of saints, believing that such relics held miraculous powers. Saint Thomas Becket, Archbishop of Canterbury, had been murdered in Canterbury Cathedral by knights of Henry II during a disagreement between Church and Crown. Miracle stories connected to his remains sprang up soon after his death, and the cathedral became a popular pilgrimage destination. The pilgrimage in the work ties all of the stories together, and may be considered a representation of Christians' striving for heaven, despite weaknesses, disagreement, and diversity of opinion.
Social class and convention.
The upper class or nobility, represented chiefly by the Knight and his Squire, was in Chaucer's time steeped in a culture of chivalry and courtliness. Nobles were expected to be powerful warriors who could be ruthless on the battlefield, yet mannerly in the King's Court and Christian in their actions. Knights were expected to form a strong social bond with the men who fought alongside them, but an even stronger bond with a woman whom they idealised to strengthen their fighting ability. Though the aim of chivalry was to noble action, often its conflicting values degenerated into violence. Church leaders often tried to place restrictions on jousts and tournaments, which at times ended in the death of the loser. "The Knight's Tale" shows how the brotherly love of two fellow knights turns into a deadly feud at the sight of a woman whom both idealise, with both knights willing to fight the other to the death to win her. Chivalry was in Chaucer's day on the decline, and it is possible that "The Knight's Tale" was intended to show its flaws, although this is disputed. Chaucer himself had fought in the Hundred Years' War under Edward III, who heavily emphasised chivalry during his reign. Two tales, "Sir Topas" and "The Tale of Melibee" are told by Chaucer himself, who is travelling with the pilgrims in his own story. Both tales seem to focus on the ill-effects of chivalry—the first making fun of chivalric rules and the second warning against violence.
The "Tales" constantly reflect the conflict between classes. For example, the division of the three estates; the characters are all divided into three distinct classes, the classes being "those who pray" (the clergy), "those who fight" (the nobility), and "those who work" (the commoners and peasantry). Most of the tales are interlinked by common themes, and some "quit" (reply to or retaliate against) other tales. Convention is followed when the Knight begins the game with a tale, as he represents the highest social class in the group. But when he is followed by the Miller, who represents a lower class, it sets the stage for the "Tales" to reflect both a respect for and a disregard for upper class rules. Helen Cooper, as well as Mikhail Bakhtin and Derek Brewer, call this opposition "the ordered and the grotesque, Lent and Carnival, officially approved culture and its riotous, and high-spirited underside." Several works of the time contained the same opposition.
Relativism versus realism.
Chaucer's characters each express different—sometimes vastly different—views of reality, creating an atmosphere of relativism. As Helen Cooper says, "Different genres give different readings of the world: the fabliau scarcely notices the operations of God, the saint's life focuses on those at the expense of physical reality, tracts and sermons insist on prudential or orthodox morality, romances privilege human emotion." The sheer number of varying persons and stories renders the "Tales" as a set unable to arrive at any definite truth or reality.
Influence on literature.
It is sometimes argued that the greatest contribution that this work made to English literature was in popularising the literary use of the vernacular, English, rather than French or Latin. English had, however, been used as a literary language for centuries before Chaucer's life, and several of Chaucer's contemporaries—John Gower, William Langland, and the Pearl Poet—also wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend rather than simply being part of it. It is interesting to note that, although Chaucer had a powerful influence in poetic and artistic terms, which can be seen in the great number of forgeries and mistaken attributions (such as The Flower and the Leaf which was translated by John Dryden), modern English spelling and orthography owes much more to the innovations made by the Court of Chancery in the decades during and after his lifetime.
Reception.
While Chaucer clearly states the addressees of many of his poems (the Book of the Duchess is believed to have been written for John of Gaunt on the occasion of his wife's death in 1368), the intended audience of "The Canterbury Tales" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for the nobility. He is referred to as a noble translator and poet by Eustache Deschamps and by his contemporary John Gower. It has been suggested that the poem was intended to be read aloud, which is probable as this was a common activity at the time. However, it also seems to have been intended for private reading as well, since Chaucer frequently refers to himself as the writer, rather than the speaker, of the work. Determining the intended audience directly from the text is even more difficult, since the audience is part of the story. This makes it difficult to tell when Chaucer is writing to the fictional pilgrim audience or the actual reader.
Chaucer's works may have been distributed in some form during his lifetime in part or in whole. Scholars speculate that manuscripts were circulated among his friends, but likely remained unknown to most people until after his death. However, the speed with which copyists strove to write complete versions of his tale in manuscript form shows that Chaucer was a famous and respected poet in his own day. The Hengwrt and Ellesmere manuscripts are examples of the care taken to distribute the work. More manuscript copies of the poem exist than for any other poem of its day except "The Prick of Conscience", causing some scholars to give it the medieval equivalent of "best-seller" status. Even the most elegant of the illustrated manuscripts, however, is not nearly as decorated and fancified as the work of authors of more respectable works such as John Lydgate's religious and historical literature.
15th century.
John Lydgate and Thomas Occleve were among the first critics of Chaucer's "Tales", praising the poet as the greatest English poet of all time and the first to show what the language was truly capable of poetically. This sentiment was universally agreed upon by later critics into the mid-15th century. Glosses included in "Canterbury Tales" manuscripts of the time praised him highly for his skill with "sentence" and rhetoric, the two pillars by which medieval critics judged poetry. The most respected of the tales was at this time the Knight's, as it was full of both.
Literary additions and supplements.
The incompleteness of the "Tales" led several medieval authors to write additions and supplements to the tales to make them more complete. Some of the oldest existing manuscripts of the tales include new or modified tales, showing that even early on, such additions were being created. These emendations included various expansions of the "Cook's Tale", which Chaucer never finished, "The Plowman's Tale", "The Tale of Gamelyn", the "Siege of Thebes", and the "Tale of Beryn".
The "Tale of Beryn", written by an anonymous author in the 15th century, is preceded by a lengthy prologue in which the pilgrims arrive at Canterbury and their activities there are described. While the rest of the pilgrims disperse throughout the town, the Pardoner seeks the affections of Kate the barmaid, but faces problems dealing with the man in her life and the innkeeper Harry Bailey. As the pilgrims turn back home, the Merchant restarts the storytelling with "Tale of Beryn". In this tale, a young man named Beryn travels from Rome to Egypt to seek his fortune only to be cheated by other businessmen there. He is then aided by a local man in getting his revenge. The tale comes from the French tale "Bérinus" and exists in a single early manuscript of the tales, although it was printed along with the tales in a 1721 edition by John Urry.
John Lydgate wrote "The Siege of Thebes" in about 1420. Like the "Tale of Beryn", it is preceded by a prologue in which the pilgrims arrive in Canterbury. Lydgate places himself among the pilgrims as one of them and describes how he was a part of Chaucer's trip and heard the stories. He characterises himself as a monk and tells a long story about the history of Thebes before the events of the "Knight's Tale". John Lydgate's tale was popular early on and exists in old manuscripts both on its own and as part of the tales. It was first printed as early as 1561 by John Stow and several editions for centuries after followed suit.
There are actually two versions of "The Plowman's Tale", both of which are influenced by the story "Piers Plowman", a work written during Chaucer's lifetime. Chaucer describes a Plowman in the "General Prologue" of his tales, but never gives him his own tale. One tale, written by Thomas Occleve, describes the miracle of the Virgin and the Sleeveless Garment. Another tale features a pelican and a griffin debating church corruption, with the pelican taking a position of protest akin to John Wycliffe's ideas.
"The Tale of Gamelyn" was included in an early manuscript version of the tales, Harley 7334, which is notorious for being one of the lower-quality early manuscripts in terms of editor error and alteration. It is now widely rejected by scholars as an authentic Chaucerian tale, although some scholars think he may have intended to rewrite the story as a tale for the Yeoman. Dates for its authorship vary from 1340 to 1370.
Literary adaptations.
Many literary works (both fiction and non-fiction alike) have used a similar frame narrative to "The Canterbury Tales" as an homage. Science fiction writer Dan Simmons wrote his Hugo Award winning novel "Hyperion" based on an extra-planetary group of pilgrims. Evolutionary biologist Richard Dawkins used "The Canterbury Tales" as a structure for his 2004 non-fiction book about evolution titled "". His animal pilgrims are on their way to find the common ancestor, each telling a tale about evolution.
Henry Dudeney's book "The Canterbury Puzzles" contains a part reputedly lost from what modern readers know as Chaucer's tales.
Historical mystery novelist P.C. Doherty wrote a series of novels based on "The Canterbury Tales", making use of the story frame and of Chaucer's characters.
Canadian author Angie Abdou translates "The Canterbury Tales" to a cross section of people, all snow sports enthusiasts but from different social backgrounds, converging on a remote backcountry ski cabin in British Columbia in the 2011 novel "The Canterbury Trail".
Adaptations and homages.
"The Two Noble Kinsmen", by William Shakespeare and John Fletcher, a retelling of "The Knight's Tale", was first performed in 1613 or 1614 and published in 1634. In 1961, Erik Chisholm completed his opera, "The Canterbury Tales". The opera is in three acts: The Wyf of Bath’s Tale, The Pardoner’s Tale and The Nun’s Priest’s Tale. Nevill Coghill's modern English version formed the basis of a musical version – first staged in 1964.
"A Canterbury Tale", a 1944 film jointly written and directed by Michael Powell and Emeric Pressburger, is loosely based on the narrative frame of Chaucer's tales. The movie opens with a group of medieval pilgrims journeying through the Kentish countryside as a narrator speaks the opening lines of the "General Prologue". The scene then makes a now-famous transition to the time of World War II. From that point on, the film follows a group of strangers, each with his or her own story and in need of some kind of redemption, are making their way to Canterbury together. The film's main story takes place in an imaginary town in Kent and ends with the main characters arriving at Canterbury Cathedral, bells pealing and Chaucer's words again resounding. "A Canterbury Tale" is recognised as one of the Powell-Pressburger team's most poetic and artful films. It was produced as wartime propaganda, using Chaucer's poetry, referring to the famous pilgrimage, and offering photography of Kent to remind the public of what made Britain worth fighting for. In one scene a local historian lectures an audience of British soldiers about the pilgrims of Chaucer's time and the vibrant history of England.
Pier Paolo Pasolini's 1972 film "The Canterbury Tales" features several of the tales, some of which keep close to the original tale and some of which are embellished. The "Cook's Tale", for instance, which is incomplete in the original version, is expanded into a full story, and the "Friar's Tale" extends the scene in which the Summoner is dragged down to hell. The film includes these two tales as well as the "Miller's Tale", the "Summoner's Tale", the "Wife of Bath's Tale", and the "Merchant's Tale".
On April 26, 1986, American radio personality Garrison Keillor opened "The News from Lake Wobegon" portion of the first live TV broadcast of his "A Prairie Home Companion" radio show with a reading of the original Middle English text of the General Prologue. He commented, "Although those words were written more than 600 years ago, they still describe spring."
English rock musician Sting paid tribute to Chaucer and the book with his 1993 concept album "Ten Summoner's Tales", which he described as ten songs (plus an epilogue number) with no theme or subject tying them together. Sting's real name is Gordon Sumner, hence the reference to the "Summoner" character in the record's title. In essence, the collection of songs was composed as "a musical Canterbury Tales".
Several more recent films, while they are not based on the tales, do have references to them. For example, in the 1995 film "Se7en", the "Parson's Tale" is an important clue to the methods of a serial killer who chooses his victims based on the seven deadly sins. The 2001 film "A Knight's Tale" took its name from "The Knight's Tale". Although it bears little resemblance to the tale, it does feature what Martha Driver and Sid Ray call an "MTV-generation" Chaucer who is a gambling addict with a way with words. Scattered references to the "Tales" include Chaucer's declaration that he will use his verse to vilify a summoner and a pardoner who have cheated him.
Television adaptations include Alan Plater's 1975 re-telling of the stories in a series of plays for BBC2: "Trinity Tales". In 2004, BBC again featured modern re-tellings of selected tales.

</doc>
<doc id="7628" url="http://en.wikipedia.org/wiki?curid=7628" title="Christine de Pizan">
Christine de Pizan

Christine de Pizan (also seen as de Pisan) (1364 – c. 1430) was an Italian French late medieval author. She served as a court writer for several dukes (Louis of Orleans, Philip the Bold of Burgundy, and John the Fearless of Burgundy) and the French royal court during the reign of Charles VI. She wrote both poetry and prose works such as biographies and books containing practical advice for women. She completed forty-one works during her 30-year career from 1399–1429. She married in 1380 at the age of 15, and was widowed 10 years later. Much of the impetus for her writing came from her need to earn a living for herself and her three children. She spent most of her childhood and all of her adult life in Paris and then the abbey at Poissy, and wrote entirely in her adopted language, Middle French.
Her early courtly poetry is marked by her knowledge of aristocratic custom and fashion of the day, particularly involving women and the practice of chivalry. Her early and later allegorical and didactic treatises reflect both autobiographical information about her life and views and also her own individualized and humanist approach to the scholastic learned tradition of mythology, legend, and history she inherited from clerical scholars and to the genres and courtly or scholastic subjects of contemporary French and Italian poets she admired. Supported and encouraged by important royal French and English patrons, she influenced 15th-century English poetry. Her success stems from a wide range of innovative writing and rhetorical techniques that critically challenged renowned writers such as Jean de Meun, author of the "Romance of the Rose", which she criticized as immoral.
In recent decades, Christine de Pizan's work has been returned to prominence by the efforts of scholars such as Charity Cannon Willard, Earl Jeffrey Richards and Simone de Beauvoir. Certain scholars have argued that she should be seen as an early feminist who efficiently used language to convey that women could play an important role within society. This characterization has been challenged by other critics, who say that it is either an anachronistic use of the word or a misinterpretation of her writing and intentions.
Life.
Christine de Pizan was born in 1365 in Venice, Italy. She was the daughter of Tommaso di Benvenuto da Pizzano (Thomas de Pizan, named for the family's origins in the town of Pizzano, south east of Bologna), a physician, court astrologer, and Councillor of the Republic of Venice. Following her birth, Thomas de Pizan accepted an appointment to the court of Charles V of France, as the king’s astrologer, alchemist, and physician. In this atmosphere, Christine was able to pursue her intellectual interests. She successfully educated herself by immersing herself in languages, in the rediscovered classics and humanism of the early Renaissance, and in Charles V’s royal archive that housed a vast number of manuscripts. But she did not assert her intellectual abilities, or establish her authority as a writer until she was widowed at the age of 25.
She married Etienne du Castel, a royal secretary to the court, at the age of 15. She had three children, a daughter (who became a nun at the Dominican Abbey in Poissy in 1397 as a companion to the king's daughter, Marie), a son Jean, and another child who died in childhood. Christine's family life was threatened in 1390 when her husband, while in Beauvais on a mission with the king, suddenly died in an epidemic. Following Castel’s death, she was left to support her mother, a niece, and her two children. When she tried to collect money from her husband’s estate, she faced complicated lawsuits regarding the recovery of salary due her husband. In order to support herself and her family, she turned to writing. By 1393, she was writing love ballads, which caught the attention of wealthy patrons within the court. These patrons were intrigued by the novelty of a female writer and had her compose texts about their romantic exploits. Her output during this period was prolific. Between 1393 and 1412, she composed over 300 ballads, and many more shorter poems.
Christine's participation in a literary debate, in 1401–1402, allowed her to move beyond the courtly circles, and ultimately to establish her status as a writer concerned with the position of women in society. During these years, she involved herself in a renowned literary controversy, the “Querelle du Roman de la Rose”. She helped to instigate this debate by beginning to question the literary merits of Jean de Meun’s the "Romance of the Rose". Written in the 13th century, the "Romance of the Rose" satirizes the conventions of courtly love while critically depicting women as nothing more than seducers. Christine specifically objected to the use of vulgar terms in Jean de Meun’s allegorical poem. She argued that these terms denigrated the proper and natural function of sexuality, and that such language was inappropriate for female characters such as Madame Raison. According to her, noble women did not use such language. Her critique primarily stems from her belief that Jean de Meun was purposely slandering women through the debated text.
The debate itself was extensive and at its end, the principal issue was no longer Jean de Meun’s literary capabilities. The principal issue had shifted to the unjust slander of women within literary texts. This dispute helped to establish Christine's reputation as a female intellectual who could assert herself effectively and defend her claims in the male-dominated literary realm. She continued to counter abusive literary treatments of women.
Works.
Christine produced a large amount of vernacular works, in both prose and verse. Her works include political treatises, mirrors for princes, epistles, and poetry.
By 1405, Christine had completed her most famous literary works, "The Book of the City of Ladies" and "The Treasure of the City of Ladies". The first of these shows the importance of women’s past contributions to society, and the second strives to teach women of all estates how to cultivate useful qualities. For example, one section of the book tells wives: "If she wants to act prudently and have the praise of both the world and her husband, she will be cheerful to him all the time"
In "The Book of the City of Ladies" Christine created a symbolic city in which women are appreciated and defended. She constructed three allegorical figures - Reason, Justice, and Rectitude - in the common pattern of literature in that era, when many books and poetry utilized stock allegorical figures to express ideas or emotions. She enters into a dialogue, a movement between question and answer, with these allegorical figures that is from a completely female perspective. Together, they create a forum to speak on issues of consequence to all women. Only female voices, examples and opinions provide evidence within this text. Christine, through Lady Reason in particular, argues that stereotypes of women can be sustained only if women are prevented from entering into the conversation. Overall, she hoped to establish truths about women that contradicted the negative stereotypes that she had identified in previous literature.
In "The Treasure of the City of Ladies", she highlights the persuasive effect of women’s speech and actions in everyday life. In this particular text, Christine argues that women must recognize and promote their ability to make peace between people. This ability will allow women to mediate between husband and subjects. She also argues that slanderous speech erodes one’s honor and threatens the sisterly bond among women. Christine then argues that "skill in discourse should be a part of every woman’s moral repertoire". She believed that a woman’s influence is realized when her speech accords value to chastity, virtue, and restraint. She argued that rhetoric is a powerful tool that women could employ to settle differences and to assert themselves. The "Treasure of the City of Ladies" provides glimpses into women's lives in 1400, from the great lady in the castle down to the merchant's wife, the servant, and the peasant. She offers advice to governesses, widows, and even prostitutes.
De Pizan was greatly interested in history, ranging from the Matter of Troy to the "founding of the royal house of France" (for her the latter was a consequence of the former). She obtained her knowledge of Troy from the "Histoire ancienne jusqu'à César", and chose an anti-Trojan position. Hector especially served as a model and a measure of masculinity for her.
In the “Querelle du Roman de la Rose,” she responded to Jean de Montreuil, who had sent her a treatise defending the sentiments expressed in the "Romance of the Rose". She begins by styling her opponent as an “expert in rhetoric” in contrast to herself, “a woman ignorant of subtle understanding and agile sentiment.” In this particular apologetic response, de Pizan belittles her own style. She is employing a rhetorical strategy by writing against the grain of her meaning, also known as antiphrasis. Her ability to employ rhetorical strategies continued when Christine began to compose literary texts following the “Querelle du Roman de la Rose.” 
Her final work was a poem eulogizing Joan of Arc, the peasant girl who said God had commanded her to secure the French throne for Charles VII. Written in 1429, "The Poem of Joan of Arc" ("Ditie de Jehanne dArc") celebrates the appearance of a woman whom Christine describes in the poem as "a simple shepherdess" while commenting: "It is a fact well worth remembering That God should now have wished (and this is the truth!) to bestow such great blessings on France, through a young virgin", adding "For there will be a King of France called Charles [VII], son of Charles [VI], who will be supreme ruler over all Kings." After completing this particular poem, it seems that Christine de Pizan, at the age of 65, decided to end her literary career.
Christine specifically sought out other women to collaborate in the creation of her work. She makes special mention of a manuscript illustrator we know only as Anastasia, whom she described as the most talented of her day.
Influence.
In her own day, Christine de Pizan was primarily a court writer who wrote commissioned works for aristocratic families, as well as addressing literary debates of the era. In modern times, she has been labeled a poetic mediator who engaged with historical texts to interpolate her royal readers and encourage ethical and judicious conduct. Some rhetorical scholars have concluded, from studying her persuasive strategies, that she forged a rhetorical identity for herself and encouraged women to embrace this identity. Some have argued that Christine de Pizan “began her literary career by singing, alone in her room, and she finished by shouting in the public square.” She left an influential footprint in the field of rhetorical discourse in an otherwise male-dominated literary field. She left forty-one surviving poetic works and a number of prose books. Simone de Beauvoir wrote in 1949 that "Épître au Dieu d'Amour" was "the first time we see a woman take up her pen in defence of her sex".

</doc>
<doc id="7630" url="http://en.wikipedia.org/wiki?curid=7630" title="Catharism">
Catharism

Catharism (; from the Greek: , "katharoi", "the pure [ones]") was a Christian dualist movement that thrived in some areas of Southern Europe, particularly northern Italy and southern France, between the 12th and 14th centuries. Cathar beliefs varied between communities because Catharism was initially taught by ascetic priests who had set few guidelines. The Cathars were a direct challenge to the Catholic Church, which denounced its practices and dismissed it outright as "the Church of Satan".
Catharism had its roots in the Paulician movement in Armenia and the Bogomils of Bulgaria, which took influences from the Paulicians. Though the term "Cathar" () has been used for centuries to identify the movement, whether the movement identified itself with this name is debatable. In Cathar texts, the terms "Good Men" ("Bons Hommes") or "Good Christians" are the common terms of self-identification. The idea of two gods or principles, one being good the other evil, was central to Cathar beliefs. The good god was the God of the New Testament and the creator of the spiritual realm, as opposed to the bad god, whom many Cathars identified as Satan, creator of the physical world of the Old Testament. All visible matter, including the human body, was created by Satan; it was therefore tainted with sin. This was the antithesis to the monotheistic Catholic Church, whose fundamental principle was that there was only one God who created all things visible and invisible. Cathars thought human spirits were the genderless spirits of angels trapped within the physical creation of Satan, cursed to be reincarnated until the Cathar faithful achieved salvation through a ritual called the consolamentum.
From the beginning of his reign, Pope Innocent III attempted to end Catharism by sending missionaries and by convincing the local authorities to act against them. However, in 1208 Innocent's papal legate Pierre de Castelnau was murdered while returning to Rome after excommunicating a local ruler in southern France, who, in his view, was too lenient with the Cathars. Pope Innocent III then abandoned the option of sending Catholic missionaries and jurists, declared Pierre de Castelnau a martyr and launched the Albigensian Crusade.
Origins.
The origins of the Cathars' beliefs are unclear but most theories agree they came from the Byzantine Empire mostly by the trade routes and spread from Bulgaria to the Netherlands. The name of Bulgarians ("Bougres") was also applied to the Albigenses, and they maintained an association with the similar Christian movement of the Bogomils ("Friends of God") of Thrace. "That there was a substantial transmission of ritual and ideas from Bogomilism to Catharism is beyond reasonable doubt." Their doctrines have numerous resemblances to those of the Bogomils and the earlier Paulicians as well as the Manicheans and the Christian Gnostics of the first few centuries AD, although, as many scholars, most notably Mark Pegg, have pointed out, it would be erroneous to extrapolate direct, historical connections based on theoretical similarities perceived by modern scholars. St John Damascene, writing in the 8th century AD, also notes of an earlier sect called the "Cathari", in his book "On Heresies", taken from the epitome provided by Epiphanius of Salamis in his "Panarion". He says of them: "They absolutely reject those who marry a second time, and reject the possibility of penance [that is, forgiveness of sins after baptism]". These are probably the same Cathari who are mentioned in Canon 8 of the First Ecumenical Council of Nicaea in the year 325, which states "...[I]f those called Cathari come over [to the faith], let them first make profession that they are willing to communicate [share full communion] with the twice-married, and grant pardon to those who have lapsed..."
It is likely that we have only a partial view of their beliefs, because the writings of the Cathars were mostly destroyed because of the doctrinal threat perceived by the Papacy; much of our existing knowledge of the Cathars is derived from their opponents. Conclusions about Cathar ideology continue to be fiercely debated with commentators regularly accusing their opponents of speculation, distortion and bias. There are a few texts from the Cathars themselves which were preserved by their opponents (the "Rituel Cathare de Lyon") which give a glimpse of the inner workings of their faith, but these still leave many questions unanswered. One large text which has survived, "The Book of Two Principles" ("Liber de duobus principiis"), elaborates the principles of dualistic theology from the point of view of some of the Albanenses Cathars.
It is now generally agreed by most scholars that identifiable historical Catharism did not emerge until at least 1143, when the first confirmed report of a group espousing similar beliefs is reported being active at Cologne by the cleric Eberwin of Steinfeld. A landmark in the "institutional history" of the Cathars was the Council, held in 1167 at Saint-Félix-Lauragais, attended by many local figures and also by the Bogomil "papa" Nicetas, the Cathar bishop of (northern) France and a leader of the Cathars of Lombardy.
The Cathars were largely a homegrown, Western European/Latin Christian phenomenon, springing up in the Rhineland cities (particularly Cologne) in the mid-12th century, northern France around the same time, and particularly southern France — the Languedoc — and the northern Italian cities in the mid-late 12th century. In the Languedoc and northern Italy, the Cathars would enjoy their greatest popularity, surviving in the Languedoc, in much reduced form, up to around 1325 and in the Italian cities until the Inquisitions of the 1260s–1300s finally rooted them out.
General beliefs.
Cathars, in general, formed an anti-sacerdotal party in opposition to the Catholic Church, protesting against what they perceived to be the moral, spiritual and political corruption of the Church.
G. K. Chesterton, the English Roman Catholic author, claimed: "..the medieval system began to be broken to pieces intellectually, long before it showed the slightest hint of falling to pieces morally. The huge early heresies, like the Albigenses, had not the faintest excuse in moral superiority."
Contemporary reports suggest otherwise, however. St Bernard of Clairvaux, for instance, although opposed to the Cathars, said of them in Sermon 65 on the Song of Songs:
When Bishop Fulk, a key leader of the anti-Cathar persecutions, excoriated the Languedoc Knights for not pursuing the heretics more diligently, he received the reply:
Sacraments.
In contrast to the Catholic Church, the Cathars had but one sacrament, the Consolamentum, or Consolation. This involved a brief spiritual ceremony to remove all sin from the believer and to induct him or her into the next higher level as a Perfect. Unlike the Roman Catholic sacrament of Penance, the Consolamentum could be taken only once.
Thus it has been alleged that many believers would eventually receive the Consolamentum as death drew near, performing the ritual of liberation at a moment when the heavy obligations of purity required of Perfecti would be temporally short. Some of those who received the sacrament of the consolamentum upon their death-beds may thereafter have shunned further food or drink in order to speed death. This has been termed the "endura". It was claimed by some of the Catholic writers that when a Cathar, after receiving the Consolamentum, began to show signs of recovery he or she would be smothered in order to ensure his or her entry into paradise. Other than at such moments of "extremis", little evidence exists to suggest this was a common Cathar practice.
The Cathars also refused the Catholic Sacrament of the eucharist saying that it could not possibly be the body of Christ. They also refused to partake in the practice of Baptism by water. The following two quotes are taken from the Catholic Inquisitor Bernard Gui’s experiences with the Cathar practices and beliefs:
Theology.
Some believe that the Catharist conception of Jesus resembled nontrinitarian modalistic monarchianism (Sabellianism) in the West and adoptionism in the East.
Bernard of Clairvaux's biographer and other sources accuse some Cathars of Arianism, and some scholars see Cathar Christology as having traces of earlier Arian roots. According to some of their contemporary enemies Cathars did not accept the Trinitarian understanding of Jesus, but considered him the human form of an angel similar to Docetic Christology. Zoé Oldenbourg (2000) compared the Cathars to "Western Buddhists" because she considered that their view of the doctrine of "resurrection" taught by Jesus was, in fact, similar to the Buddhist doctrine of reincarnation. The Cathars taught that to regain angelic status one had to renounce the material self completely. Until one was prepared to do so, he/she would be stuck in a cycle of reincarnation, condemned to live on the corrupt Earth.
The alleged sacred texts of the Cathars besides the New Testament, include The Gospel of the Secret Supper, or John's Interrogation and The Book of the Two Principles.
Social relationships.
Killing was abhorrent to the Cathars. Consequently, abstention from all animal food (sometimes exempting fish) was enjoined of the Perfecti. The Perfecti avoided eating anything considered to be a by-product of sexual reproduction. War and capital punishment were also condemned - an abnormality in Medieval Europe. In a world where few could read, their rejection of oath-taking marked them as social revolutionaries.
Cathars also rejected marriage. Their theology was based principally on the belief that the physical world, including the flesh, was irredeemably evil - as it stemmed from the evil principle or "demiurge". Therefore, reproduction was viewed by them as a moral evil to be avoided - as it continued the chain of reincarnation and suffering in the material world. It was claimed by their opponents that, given this loathing for procreation, they generally reverted to sodomy. Such was the situation that a charge of heresy leveled against a suspected Cathar was usually dismissed if the accused could show he was legally married.
Organization.
It has been alleged that the Cathar Church of the Languedoc had a relatively flat structure, distinguishing between "perfecti" (a term they did not use, instead "bonhommes") and "credentes". By about 1140 liturgy and a system of doctrine had been established. It created a number of bishoprics, first at Albi around 1165 and after the 1167 Council at Saint-Félix-Lauragais sites at Toulouse, Carcassonne, and Agen, so that four bishoprics were in existence by 1200.
In about 1225, during a lull in the Albigensian Crusade, the bishopric of Razes was added. Bishops were supported by their two assistants: a "filius maior" (typically the successor) and a "filius minor", who were further assisted by deacons. The "perfecti" were the spiritual elite, highly respected by many of the local people, leading a life of austerity and charity. In the apostolic fashion they ministered to the people and travelled in pairs.
Role of women in Catharism.
Catharism has been seen as giving women the greatest opportunities for independent action since women were found as being believers as well as Perfecti, who were able to administer the sacrament of the "consolamentum". The Cathars believed that one would be repeatedly reincarnated until one commits to the self-denial of the material world, which meant that a man could be reincarnated as a woman and vice versa, thereby rendering gender completely meaningless. The spirit was of utmost importance to the Cathars and was described as being immaterial and sexless. Because of this belief, the Cathars saw women equally capable of being spiritual leaders, which undermined the very concept of gender held by the Catholic Church and did not go unnoticed.
The women that were accused of being heretics in early medieval Christianity included those labeled Gnostics, Cathars, Beguines as well as several other groups that were sometimes "tortured and executed". The Cathars, like the Gnostics who preceded them, assigned more importance to the role of Mary Magdalene in the spread of early Christianity than the Church previously did. Her vital role as a teacher contributed to the Cathar belief that women could serve as spiritual leaders. Women were found to be included in the Perfecti in significant numbers, with numerous receiving the "consolamentum" after being widowed. Having reverence for the Gospel of John, the Cathars saw Mary as perhaps even more important than Saint Peter, the founder of the Church.
The Cathar movement proved to be extremely successful in gaining female followers because of its proto-feminist teachings along with the general feeling of exclusion from the Catholic church. Catharism attracted numerous women with the promise of a sacerdotal role that the Catholic Church did not allow. Catharism let women become a Perfect of the faith, a position of far more prestige than anything the Church offered. These female Perfects were required to adhere to a strict and ascetic lifestyle, but were still able to have their own houses. Although many women found something attractive in Catharism not all found its teachings convincing. A notable example is Hildegard of Bingen, who in 1163, gave a widely renowned sermon against the Cathars in Cologne. During this speech, Hildegard announced a state of eternal punishment and damnation to all those who accepted the Cathars beliefs.
While women Perfects rarely traveled to preach the faith, they still played a vital role in the spreading of the Catharism by establishing group homes for women. Though it was extremely uncommon, there were isolated cases of female Cathars departing from their homes to spread the faith. In the Cathar group homes, women were educated in the faith and these women would go on to bear children who would then also become believers. Through this pattern the faith grew exponentially through the efforts of women as each generation passed. Among some groups of Cathars there were even more women than there were men.
Despite women having an instrumental role in the growing of the faith, misogyny was not completely absent from the Cathar movement. Some seemingly misogynistic Cathar beliefs include that one's last incarnation had to be experienced as a man to break the cycle. This belief was inspired by later French Cathars, which taught that women must be reborn as men in order to achieve salvation. Another one is that the sexual allure of women impedes man's ability to reject the material world. Toward the end of the Cathar movement, French Catharism became more misogynistic and started the practice of excluding women Perfects. However, the influence of these type of misogynistic beliefs and practices remained rather limited on the whole of Catharism as later Italian Perfects still included women.
Suppression.
In 1147, Pope Eugene III sent a legate to the Cathar district in order to arrest the progress of the Cathars. The few isolated successes of Bernard of Clairvaux could not obscure the poor results of this mission, which clearly showed the power of the sect in the Languedoc at that period. The missions of Cardinal Peter of St. Chrysogonus to Toulouse and the Toulousain in 1178, and of Henry of Marcy, cardinal-bishop of Albano, in 1180–81, obtained merely momentary successes. Henry's armed expedition, which took the stronghold at Lavaur, did not extinguish the movement.
Decisions of Catholic Church councils—in particular, those of the Council of Tours (1163) and of the Third Council of the Lateran (1179)—had scarcely more effect upon the Cathars. When Pope Innocent III came to power in 1198, he was resolved to deal with them.
At first Innocent tried pacific conversion, and sent a number of legates into the Cathar regions. They had to contend not only with the Cathars, the nobles who protected them, and the people who respected them, but also with many of the bishops of the region, who resented the considerable authority the Pope had conferred upon his legates. In 1204, Innocent III suspended a number of bishops in Occitania; in 1205 he appointed a new and vigorous bishop of Toulouse, the former troubadour Foulques. In 1206 Diego of Osma and his canon, the future Saint Dominic, began a programme of conversion in Languedoc; as part of this, Catholic-Cathar public debates were held at Verfeil, Servian, Pamiers, Montréal and elsewhere.
Saint Dominic met and debated with the Cathars in 1203 during his mission to the Languedoc. He concluded that only preachers who displayed real sanctity, humility and asceticism could win over convinced Cathar believers. The institutional Church as a general rule did not possess these spiritual warrants. His conviction led eventually to the establishment of the Dominican Order in 1216. The order was to live up to the terms of his famous rebuke, "Zeal must be met by zeal, humility by humility, false sanctity by real sanctity, preaching falsehood by preaching truth." However, even St. Dominic managed only a few converts among the Cathari.
Albigensian Crusade.
In January 1208 the papal legate, Pierre de Castelnau - a Cistercian monk, theologian and canon lawyer - was sent to meet the ruler of the area, Raymond VI, Count of Toulouse. Known for excommunicating noblemen who protected the Cathars, Castelnau excommunicated Raymond for abetting heresy following an allegedly fierce argument during which Raymond supposedly threatened Castelnau with violence. Shortly thereafter, Castelnau was murdered as he returned to Rome, allegedly by a knight in the service of Count Raymond. His body was returned and laid to rest in the Abbey at Saint Gilles. As soon as he heard of the murder, the Pope ordered the legates to preach a crusade against the Cathars and wrote a letter to Philip Augustus, King of France, appealing for his intervention—or an intervention led by his son, Louis. This was not the first appeal but some see the murder of the legate as a turning point in papal policy. Others claim it as a fortuitous event in allowing the Pope to excite popular opinion and to renew his pleas for intervention in the south. The chronicler of the crusade which followed, Peter of Vaux de Cernay, portrays the sequence of events in such a way that, having failed in his effort to peaceably demonstrate the errors of Catharism, the Pope then called a formal crusade, appointing a series of leaders to head the assault. The French King refused to lead the crusade himself, and could not spare his son to do so either—despite his victory against John, King of England, there were still pressing issues with Flanders and the empire and the threat of an Angevin revival. Phillip did however sanction the participation of some of his more bellicose and ambitious—some might say dangerous—barons, notably Simon de Montfort and Bouchard de Marly. There followed twenty years of war against the Cathars and their allies in the Languedoc: the Albigensian Crusade.
This war pitted the nobles of the north of France against those of the south. The widespread northern enthusiasm for the Crusade was partially inspired by a papal decree permitting the confiscation of lands owned by Cathars and their supporters. This not only angered the lords of the south but also the French King, who was at least nominally the suzerain of the lords whose lands were now open to despoliation and seizure. Phillip Augustus wrote to Pope Innocent in strong terms to point this out—but the Pope did not change his policy—and many of those who went to the Midi were aware that the Pope had been equivocal over the Siege of Zadar in 1202 and the Sack of Constantinople in 1204. As the Languedoc was supposedly teeming with Cathars and Cathar sympathisers, this made the region a target for northern French noblemen looking to acquire new fiefs. The barons of the north headed south to do battle.
Their first target was the lands of the Trencavel, powerful lords of Albi, Carcassonne and the Razes—but a family with few allies in the Midi. Little was thus done to form a regional coalition and the crusading army was able to take Carcassonne, the Trencavel capital, incarcerating Raymond Roger in his own citadel where he died, allegedly of natural causes; champions of the Occitan cause from that day to this believe he was murdered. Simon de Montfort was granted the Trencavel lands by the Pope and did homage for them to the King of France, thus incurring the enmity of Peter II of Aragon who had held aloof from the conflict, even acting as a mediator at the time of the siege of Carcassonne. The remainder of the first of the two Cathar wars now essentially focused on Simon's attempt to hold on to his fabulous gains through winters where he was faced, with only a small force of confederates operating from the main winter camp at Fanjeau, with the desertion of local lords who had sworn fealty to him out of necessity—and attempts to enlarge his newfound domains in the summer when his forces were greatly augmented by reinforcements from northern France, Germany and elsewhere. Summer campaigns saw him not only retake, sometimes with brutal reprisals, what he had lost in the 'close' season, but also seek to widen his sphere of operation—and we see him in action in the Aveyron at St. Antonin and on the banks of the Rhone at Beaucaire. Simon's greatest triumph was the victory against superior numbers at the Battle of Muret—a battle which saw not only the defeat of Raymond of Toulouse and his Occitan allies—but also the death of Peter of Aragon—and the effective end of the ambitions of the house of Aragon/Barcelona in the Languedoc. This was in the medium and longer term of much greater significance to the royal house of France than it was to de Montfort—and with the battle of Bouvines was to secure the position of Philip Augustus vis a vis England and the Empire. The Battle of Muret was a massive step in the creation of the unified French kingdom and the country we know today—although Edward III, the Black Prince and Henry V would threaten later to shake these foundations.
Massacre.
The crusader army came under the command, both spiritually and militarily, of the papal legate Arnaud-Amaury, Abbot of Cîteaux. In the first significant engagement of the war, the town of Béziers was besieged on 22 July 1209. The Catholic inhabitants of the city were granted the freedom to leave unharmed, but many refused and opted to stay and fight alongside the Cathars.
The Cathars spent much of 1209 fending off the crusaders. The Béziers army attempted a sortie but was quickly defeated, then pursued by the crusaders back through the gates and into the city. Arnaud-Amaury, the Cistercian abbot-commander, is supposed to have been asked how to tell Cathars from Catholics. His reply, recalled by Caesar of Heisterbach, a fellow Cistercian, thirty years later was "Caedite eos. Novit enim Dominus qui sunt eius."—"Kill them all, the Lord will recognise His own." The doors of the church of St Mary Magdalene were broken down and the refugees dragged out and slaughtered. Reportedly, 7,000 people died there. Elsewhere in the town many more thousands were mutilated and killed. Prisoners were blinded, dragged behind horses, and used for target practice. What remained of the city was razed by fire. Arnaud-Amaury wrote to Pope Innocent III, "Today your Holiness, twenty thousand heretics were put to the sword, regardless of rank, age, or sex." The permanent population of Béziers at that time was then probably no more than 5,000, but local refugees seeking shelter within the city walls could conceivably have increased the number to 20,000.
After the success of his siege of Carcassonne, which followed the Massacre at Béziers in 1209, Simon de Montfort was designated as leader of the Crusader army. Prominent opponents of the Crusaders were Raymond Roger Trencavel, viscount of Carcassonne, and his feudal overlord Peter II, the king of Aragon, who held fiefdoms and had a number of vassals in the region. Peter died fighting against the crusade on 12 September 1213 at the Battle of Muret. Simon de Montfort was killed on 25 June 1218 after maintaining a siege of Toulouse for nine months.
Treaty and persecution.
The war ended in the Treaty of Paris (1229), by which the king of France dispossessed the house of Toulouse of the greater part of its fiefs, and that of the Trencavels (Viscounts of Béziers and Carcassonne) of the whole of their fiefs. The independence of the princes of the Languedoc was at an end. But in spite of the wholesale massacre of Cathars during the war, Catharism was not yet extinguished.
In 1215, the bishops of the Catholic Church met at the Fourth Council of the Lateran under Pope Innocent III; part of the agenda was combating the Cathar heresy.
The Inquisition was established in 1234 to uproot the remaining Cathars. Operating in the south at Toulouse, Albi, Carcassonne and other towns during the whole of the 13th century, and a great part of the 14th, it succeeded in crushing Catharism as a popular movement and driving its remaining adherents underground. Cathars who refused to recant were hanged, or burnt at the stake.
From May 1243 to March 1244, the Cathar fortress of Montségur was besieged by the troops of the seneschal of Carcassonne and the archbishop of Narbonne. On 16 March 1244, a large and symbolically important massacre took place, where over 200 Cathar Perfects were burnt in an enormous pyre at the "prat dels cremats" ("field of the burned") near the foot of the castle. Moreover, the Church decreed lesser chastisements against laymen suspected of sympathy with Cathars, at the 1235 Council of Narbonne.
A popular though as yet unsubstantiated theory holds that a small party of Cathar Perfects escaped from the fortress before the massacre at "prat dels cremats". It is widely held in the Cathar region to this day that the escapees took with them "le trésor cathar". What this treasure consisted of has been a matter of considerable speculation: claims range from sacred Gnostic texts to the Cathars' accumulated wealth, which might have included the Holy Grail (see the Section on Historical Scholarship, below).
Hunted by the Inquisition and deserted by the nobles of their districts, the Cathars became more and more scattered fugitives: meeting surreptitiously in forests and mountain wilds. Later insurrections broke out under the leadership of Roger-Bernard II, Count of Foix, Aimery III of Narbonne and Bernard Délicieux, a Franciscan friar later prosecuted for his adherence to another heretical movement, that of the Spiritual Franciscans at the beginning of the 14th century. But by this time the Inquisition had grown very powerful. Consequently, many presumed to be Cathars were summoned to appear before it. Precise indications of this are found in the registers of the Inquisitors, Bernard of Caux, Jean de St Pierre, Geoffroy d'Ablis, and others. The parfaits it was said only rarely recanted, and hundreds were burnt. Repentant lay believers were punished, but their lives were spared as long as they did not relapse. Having recanted, they were obliged to sew yellow crosses onto their outdoor clothing and to live apart from other Catholics, at least for a while.
Annihilation.
After several decades of harassment and re-proselytising, and perhaps even more importantly, the systematic destruction of their religious texts, the sect was exhausted and could find no more adepts. The leader of a Cathar revival in the Pyrenean foothills, Peire Autier was captured and executed in April 1310 in Toulouse. After 1330, the records of the Inquisition contain very few proceedings against Cathars. The last known Cathar perfectus in the Languedoc, Guillaume Bélibaste, was executed in the autumn of 1321.
From the mid-12th century onwards, Italian Catharism came under increasing pressure from the Pope and the Inquisition, "spelling the beginning of the end". Other movements, such as the Waldensians and the pantheistic Brethren of the Free Spirit, which suffered persecution in the same area, survived in remote areas and in small numbers into the 14th and 15th centuries. Some Waldensian ideas were absorbed into early Protestant sects, such as the Hussites, Lollards, and the Moravian Church (Herrnhuters of Germany).
Later history.
After the suppression of Catharism, the descendants of Cathars were at times required to live outside towns and their defences. They thus retained a certain Cathar identity, despite having returned to the Catholic religion.
Any use of the term "Cathar" to refer to people after the suppression of Catharism in the 14th century is a cultural or ancestral reference, and has no religious implication. Nevertheless, interest in the Cathars, their history, legacy and beliefs continues.
"Pays Cathare".
The term "Pays Cathare", French meaning "Cathar Country" is used to highlight the Cathar heritage and history of the region where Catharism was traditionally strongest. This area is centred around fortresses such as Montségur and Carcassonne; also the French département of the Aude uses the title "Pays Cathare" in tourist brochures. These areas have ruins from the wars against the Cathars which are still visible today.
Some criticise the promotion of the identity of "Pays Cathare" as an exaggeration for tourist purposes. Actually, most of the promoted Cathar castles were not built by Cathars but by local lords and later many of them were rebuilt and extended for strategic purposes. Good examples of these are the magnificent castles of Queribus and Peyrepertuse which are both perched on the side of precipitous drops on the last folds of the Corbieres mountains. They were for several hundred years frontier fortresses belonging to the French crown and most of what is still there dates from a post-Cathar era. The Cathars sought refuge at these sites. Many consider the County of Foix to be the actual historical centre of Catharism.
Oldest account of ordinary people told in their own words.
In an effort to find the few remaining heretics in and around the village of Montaillou, Jacques Fournier, Bishop of Pamiers, future Pope Benedict XII, had those suspected of heresy interrogated in the presence of scribes who recorded their conversations. The late 13th- to early 14th-century document, discovered in the Vatican archives in the 1960s, and edited by Jean Duvernoy is the oldest known account of the daily lives of ordinary people told in their own words. It was translated by Emmanuel Le Roy Ladurie as "Montaillou: The Promised Land of Error". In the original, the book was entitled "Montaillou, Occitan Village".
Historical scholarship.
The publication of the early scholarly book "Crusade against the Grail" by the young German Otto Rahn in the 1930s rekindled interest in the connection between the Cathars and the Holy Grail, especially in Germany. Rahn was convinced that the 13th-century work "Parzival" by Wolfram von Eschenbach was a veiled account of the Cathars. His research attracted the attention of the Nazi government and in particular of Heinrich Himmler, who made him an archaeologist in the SS.
In the 20th century, Deodat Roche (1877-1978) was a key historian, and the founder of the scholarly French-language journal "Cahiers d'Etudes Cathares". Jean Duvernoy in the 1960s and 1970s edited, translated medieval sources, studying their beliefs in an historical point of view.
Their memory and "resistance" against the crusaders is a cultural topic in the French region where they were persecuted.
Academic books in English first appeared at the beginning of the millennium: for example, Malcolm Lambert's "The Cathars" and Malcolm Barber's "The Cathars"
In art and music.
The principal legacy of the Cathar movement is in the poems and songs of the Cathar troubadors, though this artistic legacy is only a smaller part of the wider Occitan linguistic and artistic heritage. Recent artistic projects concentrating on the Cathar element in Provençal and troubador art include commercial recording projects by Thomas Binkley, electric hurdy-gurdy artist Valentin Clastrier and his CD Heresie dedicated to the church at Cathars, La Nef, and Jordi Savall.
In popular culture.
The Cathars have been depicted or re-interpreted in popular books, video games, and films such as "The Holy Blood and the Holy Grail", "The Bone Clocks", "Labyrinth", "", and Bernard Cornwell's "The Grail Quest" series. A number of semi-fictional conspiracy theories have been published that integrate the Cathars into their ideas, especially in France and Germany.
References.
Notes
Bibliography

</doc>
<doc id="7632" url="http://en.wikipedia.org/wiki?curid=7632" title="Cerebrospinal fluid">
Cerebrospinal fluid

Cerebrospinal fluid (CSF) is a clear colorless bodily fluid found in the brain and spine. It is produced in the choroid plexus of the brain. It acts as a cushion or buffer for the brain's cortex, providing a basic mechanical and immunological protection to the brain inside the skull, and it serves a vital function in cerebral autoregulation of cerebral blood flow.
The CSF occupies the subarachnoid space (the space between the arachnoid mater and the pia mater) and the ventricular system around and inside the brain and spinal cord. It constitutes the content of the ventricles, cisterns, and sulci of the brain, as well as the central canal of the spinal cord.
Structure.
Production.
The brain produces roughly 500 mL of cerebrospinal fluid per day. This fluid is constantly reabsorbed, so that only 100-160 mL is present at any one time.
Ependymal cells of the choroid plexus produce more than two thirds of CSF. The choroid plexus is a venous plexus contained within the four ventricles of the brain, hollow structures inside the brain filled with CSF. The remainder of the CSF is produced by the surfaces of the ventricles and by the lining surrounding the subarachnoid space.
Ependymal cells actively secrete sodium into the lateral ventricles. This creates osmotic pressure and draws water into the CSF space. Chloride, with a negative charge, maintains electroneutrality and moves with the positively-charged sodium. As a result, CSF contains a higher concentration of sodium and chloride than blood plasma, but less potassium, calcium and glucose and protein.
Circulation.
CSF circulates within the ventricular system of the brain. The ventricles are a series of cavities filled with CSF that reside within the brain. The majority of CSF is produced from within the two lateral ventricles. From here, the CSF passes through the Interventricular foramina (of Monro) to the third ventricle, then the cerebral aqueduct (of Sylvius) to the fourth ventricle. The fourth ventricle is an outpouching on the posterior part of the brainstem. From the fourth ventricle, the fluid passes through three foramen to enter the subarachnoid space. It passes through the Foramen of Magendie on the midline, and two Foramen of Luschka laterally. The subarachnoid space covers the brain and spinal cord.
The CSF moves in a pulsatile manner throughout the CSF system with nearly zero net flow. 
Reabsorption.
It had been thought that CSF returns to the vascular system by entering the dural venous sinuses via the arachnoid granulations (or villi). However, some have suggested that CSF flow along the cranial nerves and spinal nerve roots allow it into the lymphatic channels; this flow may play a substantial role in CSF reabsorbtion, in particular in the neonate, in which arachnoid granulations are sparsely distributed. The flow of CSF to the nasal submucosal lymphatic channels through the cribriform plate seems to be especially important.
Amount and constitution.
The CSF contains approximately 0.3% plasma proteins, or approximately 15 to 40 mg/dL, depending on sampling site, and it is produced at a rate of 500 ml/day. Since the subarachnoid space around the brain and spinal cord can contain only 135 to 150 ml, large amounts are drained primarily into the blood through arachnoid granulations in the superior sagittal sinus. Thus the CSF turns over about 3.7 times a day. This continuous flow into the venous system dilutes the concentration of larger, lipid-insoluble molecules penetrating the brain and CSF.
CSF pressure, as measured by lumbar puncture (LP), is 10-18 cmH2O (8-15 mmHg or 1.1-2 kPa) with the patient lying on the side and 20-30cmH2O (16-24 mmHg or 2.1-3.2 kPa) with the patient sitting up. In newborns, CSF pressure ranges from 8 to 10 cmH2O (4.4–7.3 mmHg or 0.78–0.98 kPa). Most variations are due to coughing or internal compression of jugular veins in the neck. When lying down, the cerebrospinal fluid as estimated by lumbar puncture is similar to the intracranial pressure.
There are quantitative differences in the distributions of a number of proteins in the CSF. In general, globular proteins and albumin are in lower concentration in ventricular CSF compared to lumbar or cisternal fluid. The "IgG index" of cerebrospinal fluid is a measure of the immunoglobulin G content, and is elevated in multiple sclerosis. It is defined as
"IgG index = (IgGCSF / IgGserum ) / (albuminCSF / albuminserum"). A cutoff value has been suggested to be 0.73, with a higher value indicating presence of multiple sclerosis.
Development.
Around the third week of development, the embryo is a three-layered disc. The embryo is covered on the dorsal surface by a layer of cells called endoderm. In the middle of the dorsal surface of the embryo is a linear structure called the notochord. As the endoderm proliferates, the notochord is dragged into the middle of the developing embryo. The notochord becomes a canal within the embryo known as the neural canal.
As the brain develops, by the fourth week of embryological development several swellings have formed within the embryo around the canal, near where the head will develop. These swellings represent different components of the central nervous system, and are three in number: the prosencephalon, mesencephalon and rhombencephalon.
The developing forebrain surrounds the neural cord. As the forebrain develops, the neural cord within it becomes a ventricle, ultimately forming the lateral ventricles. Along the inner surface of both ventricles, the ventricular wall remains thin, and a choroid plexus develops, releasing CSF. The CSF quickly fills the neural canal.
Function.
CSF serves several purposes:
Clinical significance.
When CSF pressure is elevated, cerebral blood flow may be constricted. When disorders of CSF flow occur, they may therefore affect not only CSF movement but also craniospinal compliance and the intracranial blood flow, with subsequent neuronal and glial vulnerabilities. The venous system is also important in this equation. Infants and patients shunted as small children may have particularly unexpected relationships between pressure and ventricular size, possibly due in part to venous pressure dynamics. This may have significant treatment implications, but the underlying pathophysiology needs to be further explored.
CSF connections with the lymphatic system have been demonstrated in several mammalian systems. Preliminary data suggest that these CSF-lymph connections form around the time that the CSF secretory capacity of the choroid plexus is developing (in utero). There may be some relationship between CSF disorders, including hydrocephalus and impaired CSF lymphatic transport.
Hydrocephalus.
Hydrocephalus can be caused by impaired cerebro-spinal fluid (CSF) flow, re-absorption, or excessive production of CSF. Hydrocephalus can be colloquially termed as "water on the brain" and is of medical importance. Hydrocephalus is an abnormal accumulation of cerebrospinal fluid (CSF) in the ventricles, or cavities, of the brain, which may cause increased intracranial pressure (ICP) inside the skull. It may lead to enlargement of the cranium if hydrocephalus occurs during development. It is most often accompanied by mental disability, sometimes by convulsive episodes and also tunnel vision. Hydrocephalus may become fatal if it is not corrected quickly. It is more common in infants, and in older adults.
Lumbar puncture.
CSF can be tested for the diagnosis of a variety of neurological diseases, usually obtained by a procedure called lumbar puncture.
Lumbar puncture is carried out under sterile conditions by inserting a needle into the subarachnoid space, usually between the third and fourth lumbar vertebrae. CSF is extracted through the needle, and tested. Cells in the fluid are counted, as are the levels of protein and glucose. These parameters alone may be extremely beneficial in the diagnosis of subarachnoid hemorrhage and central nervous system infections (such as meningitis). Moreover, a CSF culture examination may yield the microorganism that has caused the infection. By using more sophisticated methods, such as the detection of the oligoclonal bands, an ongoing inflammatory condition (for example, multiple sclerosis) can be recognized. A beta-2 transferrin assay is highly specific and sensitive for the detection for, e.g., CSF leakage.
Lumbar puncture can also be performed to measure the intracranial pressure, which might be increased in certain types of hydrocephalus. However a lumbar puncture should never be performed if increased intracranial pressure is suspected due to certain situations such as a tumour, because it can lead to brain herniation and ultimately death.
About one third of people experience a headache after lumbar puncture.
Baricity.
This fluid has an importance in anesthesiology. Baricity refers to the density of a substance compared to the density of human cerebrospinal fluid. Baricity is used in anesthesia to determine the manner in which a particular drug will spread in the intrathecal space.
Alzheimer's disease.
A 2010 study showed analysis of CSF for three protein biomarkers that can indicate the presence of Alzheimer's disease. The three biomarkers are CSF amyloid beta 1-42, total CSF tau protein and P-Tau181P. In the study, the biomarker test showed good sensitivity, identifying 90% of persons with Alzheimer's disease, but poor specificity, as 36% of control subjects were positive for the biomarkers. The researchers suggested the low specificity may be explained by developing but not yet symptomatic disease in controls.
History.
Various comments by ancient physicians have been read as referring to CSF. Hippocrates discussed "water" surrounding the brain when describing congenital hydrocephalus, and Galen referred to "excremental liquid" in the ventricles of the brain, which he believed was purged into the nose. But for some 16 intervening centuries of ongoing anatomical study, CSF remains unmentioned in the literature. This is perhaps because of the prevailing autopsy technique, which involved cutting off the head, thereby removing evidence of the CSF before the brain was examined. The modern rediscovery of CSF is now credited to Emanuel Swedenborg. In a manuscript written between 1741 and 1744, unpublished in his lifetime, Swedenborg referred to CSF as "spirituous lymph" secreted from the roof of the fourth ventricle down to the medulla oblongata and spinal cord. This manuscript was eventually published in translation in 1887.
Albrecht von Haller, a Swiss physician and physiologist, Avery Kinsey, made note in his 1747 book on physiology that the "water" in the brain was secreted into the ventricles and absorbed in the veins, and when secreted in excess, could lead to hydrocephalus.
Francois Magendie studied the properties of CSF by vivisection. He discovered the foramen Magendie, the opening in the roof of the fourth ventricle, but mistakenly believed that CSF was secreted by the pia mater.
Thomas Willis (noted as the discoverer of the circle of Willis) made note of the fact that the consistency of the CSF is altered in meningitis.
In 1891, W. Essex Wynter began treating tubercular meningitis by tapping the subarachnoid space, and Heinrich Quincke began to popularize lumbar puncture, which he advocated for both diagnostic and therapeutic purposes. In 19th and early 20th century literature, particularly German medical literature, "liquor cerebrospinalis" was a term used to refer to CSF.
In 1912, William Mestrezat gave the first accurate description of the chemical composition of the CSF. In 1914, Harvey W. Cushing published conclusive evidence that the CSF is secreted by the choroid plexus.

</doc>
<doc id="7633" url="http://en.wikipedia.org/wiki?curid=7633" title="Cordial">
Cordial

Cordial may refer to:

</doc>
<doc id="7635" url="http://en.wikipedia.org/wiki?curid=7635" title="Charles F. Hockett">
Charles F. Hockett

Charles Francis Hockett (January 17, 1916 – November 3, 2000) was an American linguist who developed many influential ideas in American structuralist linguistics. He represents the post-Bloomfieldian phase of structuralism often referred to as "distributionalism" or "taxonomic structuralism". His academic career spanned over half a century at Cornell and Rice universities.
Professional and academic career.
Education.
At the age of 16, Hockett enrolled at Ohio State University in Columbus, Ohio where he received a Bachelor of Arts and Master of Arts in ancient history. While enrolled at Ohio State, Hockett became interested in the work of Leonard Bloomfield, a leading figure in the field of structural linguistics. Hockett continued his education at Yale University where he studied anthropology and linguistics and received his PhD in anthropology in 1939. While studying at Yale, Hockett studied with several other influential linguists such as Edward Sapir, George P. Murdock, and Benjamin Whorf. Hockett's dissertation was based on his fieldwork in Potawatomi; his paper on Potawatomi syntax was published in "Language" in 1939. In 1948 his dissertation was published as a series in the International Journal of American Linguistics. Following fieldwork in Kickapoo and Michoacán, Mexico, Hockett did two years of postdoctoral study with Leonard Bloomfield in Chicago and Michigan.
Career.
Hockett began his teaching career in 1946 as an assistant professor of linguistics in the Division of Modern Languages at Cornell University where he was responsible for directing the Chinese language program. In 1957, Hockett became a member of Cornell's anthropology department and continued to teach anthropology and linguistics until he retired to emeritus status in 1982. In 1986, he took up an adjunct post at Rice University in Houston, Texas, where he remained active until his death in 2000.
Achievements.
Charles Hockett held membership among many academic institutions such as the National Academy of Sciences the American Academy of Arts and Sciences, and the Society of Fellows at Harvard University. He served as president of both the Linguistic Society of America and the Linguistic Association of Canada and the United States.
In addition to making many contributions to the field of structural linguistics, Hockett also considered such things as Whorfian Theory, jokes, the nature of writing systems, slips of the tongue, and animal communication and their relativeness to speech.
Outside the realm of linguistics and anthropology, Hockett practiced musical performance and composition. Hockett composed a full-length opera called "The Love of Doña Rosita" which was based on a play by Federico García Lorca and premiered at Ithaca College by the Ithaca Opera.
Hockett and his wife Shirley were vital leaders in the development of the Cayuga Chamber Orchestra in Ithaca, New York. In appreciation of the Hocketts' hard work and dedication to the Ithaca community, Ithaca College established the Charles F. Hockett Music Scholarship, the Shirley and Chas Hockett Chamber Music Concert Series, and the Hockett Family Recital Hall.
View on linguistics.
In his "Note on Structure" he argues that linguistics can be seen as a game and as a science. A linguist as player has a freedom for experimentation on all the utterances of a language, but no criterion to compare his analysis with other linguists. Late in his career, he was known for his stinging criticism of Chomskyan linguistics.
Key contributions.
Comparative method of linguistics.
One of Hockett’s most important contributions was his development of the design-feature approach to comparative linguistics where he attempted to distinguish the similarities and differences among animal communication systems and human language.
Hockett initially developed seven features which were published in the 1959 paper “Animal ‘Languages’ and Human Language.” However, after many revisions, he settled on 13 design-features which can be found in the Scientific American article “The Origin of Speech.”
Hockett argued that while every communication system has some of the 13 design features, only human, spoken language has all 13 features. In turn, this differentiates human spoken language from animal communication and other human communication systems such as written language.
Hockett's 13 design features of language.
While Hockett believed that all communication systems, animal and human alike, share many of these features, only human language contains all of the 13 design features. Additionally, traditional transmission, and duality of patterning are key to human language.
Design feature representation in other communication systems.
Foraging honeybees communicate with other members of their hive when they have discovered a relevant source of pollen, nectar, or water. In an effort to convey information about the location and distance of such resources, honeybees participate in a particular figure-eight dance known as the waggle dance.
In Hockett's "The Origin of Speech", he determined that the honeybee communication system of the waggle dance holds the following design features:
Gibbons are small apes in the family Hylobatidae. While gibbons share the same kingdom, phylum, class, and order of humans and are relatively close to man, Hockett distinguishes between the gibbon communication system and human language by noting that gibbons are devoid of the last four design features.
Gibbons possess the first nine design features, but do not possess the last four (displacement, productivity, traditional transmission, and duality of patterning).
Later additions to the features.
In a report published in 1968 with anthropologist and scientist Stuart A. Altmann, Hockett derived three more Design Features, bringing the total to 16. The additional three are:
Other additions.
Cognitive scientist and linguist at the University of Sussex Larry Trask (1944–2004) offered an alternative term and definition for number 14, Prevarication:
There has since been one more Feature added to the list, by , a director of the Undergraduate Studies program at the University of Maryland: College Park’s Anthropology school, part of the College of Behavioral and Social Sciences. His “extra” Feature is:
This follows the definition of Grammar and Syntax, as given by Merriam-Webster’s Dictionary:
Relationship between the design features and animal communication.
Additionally, Dr. Stuart defends his postulation with references to famous linguist Noam Chomsky and University of New York psychologist Gary Marcus. Chomsky theorized that humans are unique in the animal world because of their ability to utilize Design Feature 5: Total Feedback, or recursive grammar. This includes being able to correct oneself and insert explanatory or even non sequitur statements into a sentence, without breaking stride, and keeping proper grammar throughout.
While there have been studies attempting to disprove Chomsky, Marcus states that, "An intriguing possibility is that the capacity to recognize recursion might be found only in species that can acquire new patterns of vocalization, for example, songbirds, humans and perhaps some cetaceans." This is in response to a by psychologist Timothy Gentner of the University of California at San Diego. Gentner’s study found that starling songbirds use recursive grammar to identify “odd” statements within a given “song.” However, the study does not necessarily debunk Chomsky’s observation because it has not yet been proven that songbirds have the semantic ability to generalize from patterns.
 that symbolic thought is necessary for grammar-based speech, and thus Homo Erectus and all preceding “humans” would have been unable to comprehend modern speech. Rather, their utterances would have been halting and even quite confusing to us, 
today.
Hockett's "design features" of language and other animal communication systems.
The ]]: Phonetics Laboratory Faculty of Linguistics, Philology and Phonetics published the following chart, detailing how Hockett's (and Altmann's) Design Features fit into other forms of communication, in animals:

</doc>
<doc id="7638" url="http://en.wikipedia.org/wiki?curid=7638" title="Consilience">
Consilience

In science and history, consilience (also convergence of evidence or concordance of evidence) refers to the principle that evidence from independent, unrelated sources can "converge" to strong conclusions. That is, when multiple sources of evidence are in agreement, the conclusion can be very strong even when none of the individual sources of evidence are very strong on their own. Most established scientific knowledge is supported by a convergence of evidence: if not, the evidence is comparatively weak, and there will not likely be a strong scientific consensus.
The principle is based on the unity of knowledge; measuring the same result by several different methods should lead to the same answer. For example, it should not matter whether one measures the distance between the Great Pyramids of Giza by laser rangefinding, by satellite imaging, or with a meter stick - in all three cases, the answer should be approximately the same. For the same reason, different dating methods in geochronology should concur, a result in chemistry should not contradict a result in geology, etc.
The word "consilience" was originally coined as the phrase "consilience of inductions" by William Whewell ("consilience" refers to a "jumping together" of knowledge).
Description.
Consilience requires the use of independent methods of measurement, meaning that the methods have few shared characteristics. That is, the mechanism by which the measurement is made is different; each method is dependent on an unrelated natural phenomenon. For example, the accuracy of laser rangefinding measurements is based on the scientific understanding of lasers, while satellite pictures and meter sticks rely on different phenomena. Because the methods are independent, when one of several methods is in error, it is very unlikely to be in error in the "same way" as any of the other methods, and a difference between the measurements will be observed. If the scientific understanding of the properties of lasers were inaccurate, then the laser measurement would be inaccurate but the others would not. 
As a result, when several different methods agree, this is strong evidence that "none" of the methods are in error and the conclusion is correct. This is because of a greatly reduced likelihood of errors: for a consensus estimate from multiple measurements to be wrong, the errors would have to be similar for all samples and all methods of measurement, which is extremely unlikely. Random errors will tend to cancel out as more measurements are made, due to regression to the mean; systematic errors will be detected by differences between the measurements (and will also tend to cancel out since the direction of the error will still be random). This is how scientific theories reach high confidence – over time, they build up a large degree of evidence which converges on the same conclusion.
When results from different strong methods do appear to conflict, this is treated as a serious problem to be reconciled. For example, in the 19th century, the Sun appeared to be no more than 20 million years old, but the Earth appeared to be no less than 300 million years (resolved by the discovery of nuclear fusion and radioactivity, and the theory of quantum mechanics); or current attempts to resolve theoretical differences between quantum mechanics and general relativity.
Significance.
Because of consilience, the strength of evidence for any particular conclusion is related to how many independent methods are supporting the conclusion, as well as how different these methods are. Those techniques with the fewest (or no) shared characteristics provide the strongest consilience and result in the strongest conclusions. This also means that confidence is usually strongest when considering evidence from different fields, because the techniques are usually very different. 
For example, the theory of evolution is supported by a convergence of evidence from genetics, molecular biology, paleontology, geology, biogeography, comparative anatomy, comparative physiology, and many other fields. In fact, the evidence within each of these fields is itself a convergence providing evidence for the theory. (As a result, to disprove evolution, most or all of these independent lines of evidence would have to be found to be in error.) The strength of the evidence, considered together as a whole, results in the strong scientific consensus that the theory is correct. In a similar way, evidence about the history of the universe is drawn from astronomy, astrophysics, planetary geology, and physics.
Finding similar conclusions from multiple independent methods is also evidence for the reliability of the methods themselves, because consilience eliminates the possibility of all potential errors that do not affect all the methods equally. This is also used for the validation of new techniques through comparison with the consilient ones. If only partial consilience is observed, this allows for the detection of errors in methodology; any weaknesses in one technique can be compensated for by the strengths of the others. Alternatively, if using more than one or two techniques for every experiment is infeasible, some of the benefits of consilience may still be obtained if it is well-established that these techniques usually give the same result.
Consilience is important across all of science, including the social sciences, and is often used as an argument for scientific realism by philosophers of science. Each branch of science studies a subset of reality that depends on factors studied in other branches. Atomic physics underlies the workings of chemistry, which studies emergent properties that in turn are the basis of biology. Psychology is not separate from the study of properties emergent from the interaction of neurons and synapses. Sociology, economics, and anthropology are each, in turn, studies of properties emergent from the interaction of countless individual humans. The concept that all the different areas of research are studying one real, existing universe is an apparent explanation of why scientific knowledge determined in one field of inquiry has often helped in understanding other fields.
Deviations from consilience.
Consilience does not forbid deviations: in fact, since not all experiments are perfect, some deviations from established knowledge are expected. However, when the convergence is strong enough, then new evidence inconsistent with the previous conclusion is not usually enough to outweigh that convergence. Without an equally strong convergence on the new result, the weight of evidence will still favor the established result. This means that the new evidence is most likely to be wrong.
Science denialism (for example, AIDS denialism) is often based on a misunderstanding of this property of consilience. A denier may promote small gaps not yet accounted for by the consilient evidence, or small amounts of evidence contradicting a conclusion without accounting for the pre-existing strength resulting from consilience. More generally, to insist that all evidence converge precisely with no deviations would be naïve falsificationism, equivalent to considering a single contrary result to falsify a theory when another explanation, such as equipment malfunction or misinterpretation of results, is much more likely.
In history.
Historical evidence also converges in an analogous way. For example: if five ancient historians, none of whom knew each other, all claim that Julius Caesar seized power in Rome in 49 BCE, this is strong evidence in favor of that event occurring even if each individual historian is only partially reliable. By contrast, if the same historian had made the same claim five times in five different places (and no other types of evidence were available), the claim is much weaker because it originates from a single source. The evidence from the ancient historians could also converge with evidence from other fields, such as archaeology: for example, evidence that many senators fled Rome at the time, that the battles of Caesar’s civil war occurred, and so forth.
Consilience has also been discussed in reference to Holocaust denial. 
That is, individually the evidence may underdetermine the conclusion, but together they overdetermine it. A similar way to state this is that to ask for one "particular" piece of evidence in favor of a conclusion is a flawed question.
Outside the sciences.
In addition to the sciences, consilience can be important to the arts, ethics,and religion. Both artists and scientists have identified the importance of biology in the process of artistic innovation.
History of the concept.
Consilience has its roots in the ancient Greek concept of an intrinsic orderliness that governs our cosmos, inherently comprehensible by logical process, a vision at odds with mystical views in many cultures that surrounded the Hellenes. The rational view was recovered during the high Middle Ages, separated from theology during the Renaissance and found its apogee in the Age of Enlightenment.
Whewell’s definition was that:
More recent descriptions include:
Edward O. Wilson.
Although the concept of consilience in Whewell's sense was widely discussed by philosophers of science, the term was unfamiliar to the broader public until the end of the 20th century, when it was revived in "Consilience: The Unity of Knowledge," a 1998 book by the humanist biologist Edward Osborne Wilson, as an attempt to bridge the culture gap between the sciences and the humanities that was the subject of C. P. Snow's "The Two Cultures and the Scientific Revolution" (1959).
Wilson held that with the rise of the modern sciences, the sense of unity gradually was lost in the increasing fragmentation and specialization of knowledge in the last two centuries. He asserted that the sciences, humanities, and arts have a common goal: to give a purpose to understanding the details, to lend to all inquirers "a conviction, far deeper than a mere working proposition, that the world is orderly and can be explained by a small number of natural laws." Wilson's concept is a much broader notion of consilience than that of Whewell, who was merely pointing out that generalizations invented to account for one set of phenomena often account for others as well.
A parallel view lies in the term universology, which literally means "the science of the universe." Universology was first advocated for the study of the interconnecting principles and truths of all domains of knowledge by Stephen Pearl Andrews, a 19th-century utopian futurist and anarchist.

</doc>
<doc id="7642" url="http://en.wikipedia.org/wiki?curid=7642" title="Clarence Brown">
Clarence Brown

Clarence Brown (May 10, 1890 – August 17, 1987) was an American film director.
Early life.
Born in Clinton, Massachusetts, to a cotton manufacturer, Brown moved to the South when he was 11. He attended Knoxville High School and the University of Tennessee, both in Knoxville, Tennessee, graduating from the university at the age of 19 with two degrees in engineering. An early fascination in automobiles led Brown to a job with the Stevens-Duryea Company, then to his own Brown Motor Car Company in Alabama. He later abandoned the car dealership after developing an interest in motion pictures around 1913. He was hired by the Peerless Studio at Fort Lee, New Jersey, and became an assistant to the French-born director Maurice Tourneur.
Career.
After serving in World War I, Brown was given his first co-directing credit (with Tourneur) for "The Great Redeemer" (1920). Later that year, he directed a major portion of "The Last of the Mohicans" after Tourneur was injured in a fall.
Brown moved to Universal in 1924, and then to MGM, where he stayed until the mid-1950s. At MGM he was one of the main directors of their female stars–he directed Joan Crawford six times and Greta Garbo seven.
He not only made the difficult transition from silent cinema to sound cinema, but thrived there, proving himself to be an "actor's director": listening to his actors, respecting their instincts, and often incorporating their suggestions into scenes. In doing so, Brown created believable, under-played, naturalistic dialogue scenes stripped of melodrama, pulsing with the honest rhythms of real-life conversation. He was nominated five times (see below) for the Academy Award as a director, and once as a producer, but never received an Oscar. However, he did win Best Foreign Film for "Anna Karenina" starring Garbo at the 1935 Venice International Film Festival.
Brown's films gained a total of 38 Academy Award nominations and earned nine Oscars. Brown himself received six Academy Award nominations and in 1949 won the British Academy Award for the film version of William Faulkner's "Intruder in the Dust.
In 1957, Brown was awarded , given by George Eastman House for distinguished contribution to the art of film.
Brown retired a wealthy man due to his real estate investments, but refused to watch new movies, as he feared they might cause him to restart his career. In the 1970s, Brown became a much-sought guest lecturer on the film-festival circuit, thanks in part to his connection with Garbo.
The Clarence Brown Theater, on the campus of the University of Tennessee, is named in his honor. He is tied with Robert Altman and Alfred Hitchcock for the most Academy Award nominations for best director without a single win.
Filmography.
NOTE: In 1929/1930, Brown received one Academy Award nomination for two films. According to the Academy of Motion Picture Arts and Sciences, "As allowed by the award rules for this year, a single nomination could honor work in one or more films."

</doc>
<doc id="7643" url="http://en.wikipedia.org/wiki?curid=7643" title="Conciliation">
Conciliation

Conciliation is an alternative dispute resolution (ADR) process whereby the parties to a dispute use a conciliator, who meets with the parties separately in an attempt to resolve their differences. They do this by lowering tensions, improving communications, interpreting issues, providing technical assistance, exploring potential solutions and bringing about a negotiated settlement.
Conciliation differs from arbitration in that the conciliation process, in and of itself, has no legal standing, and the conciliator usually has no authority to seek evidence or call witnesses, usually writes no decision, and makes no award.
Conciliation differs from mediation in that the main goal is to conciliate, most of the time by seeking concessions. In mediation, the mediator tries to guide the discussion in a way that optimizes parties' needs, takes feelings into account and reframes representations.
In conciliation the parties seldom, if ever, actually face each other across the table in the presence of the conciliator.
The conciliation prize is a reward gained by the conciliator on a successfully resolved dispute. This term has since entered wider common usage.
Effectiveness.
Recent studies in the processes of negotiation have indicated the effectiveness of a technique that deserves mention here. A conciliator assists each of the parties to independently develop a list of all of their objectives (the outcomes which they desire to obtain from the conciliation). The conciliator then has each of the parties separately prioritize their own list from most to least important. He/She then goes back and forth between the parties and encourages them to "give" on the objectives one at a time, starting with the least important and working toward the most important for each party in turn. The parties rarely place the same priorities on all objectives, and usually have some objectives that are not listed by the other party. Thus the conciliator can quickly build a string of successes and help the parties create an atmosphere of trust which the conciliator can continue to develop.
Most successful conciliators are highly skilled negotiators. Some conciliators operate under the auspices of any one of several non-governmental entities, and for governmental agencies such as the Federal Mediation and Conciliation Service in the United States
Historical conciliation.
Historical conciliation is an applied conflict resolution approach that utilizes historical narratives to positively transform relations between societies in conflicts. Historical conciliation can utilize many different methodologies, including mediation, sustained dialogue, apologies, acknowledgement, support of public commemoration activities, and public diplomacy.
Historical conciliation is not an excavation of objective facts. The point of facilitating historical questions is not to discover all the facts in regard to who was right or wrong. Rather, the objective is to discover the complexity, ambiguity, and emotions surrounding both dominant and non-dominant cultural and individual narratives of history. It is also not a rewriting of history. The goal is not to create a combined narrative that everyone agrees upon. Instead, the aim is to create room for critical thinking and more inclusive understanding of the past and conceptions of “the other.”
Conflicts that are addressed through historical conciliation have their roots in conflicting identities of the people involved. Whether the identity at stake is their ethnicity, religion or culture, it requires a comprehensive approach that takes people’s needs, hopes, fears, and concerns into account.
Japan.
Japanese law makes extensive use of in civil disputes. The most common forms are civil conciliation and domestic conciliation, both of which are managed under the auspices of the court system by one judge and two non-judge "conciliators."
Civil conciliation is a form of dispute resolution for small lawsuits, and provides a simpler and cheaper alternative to litigation. Depending on the nature of the case, non-judge experts (doctors, appraisers, actuaries, and so on) may be called by the court as conciliators to help decide the case.
Domestic conciliation is most commonly used to handle contentious divorces, but may apply to other domestic disputes such as the annulment of a marriage or acknowledgment of paternity. Parties in such cases are required to undergo conciliation proceedings and may only bring their case to court once conciliation has failed.

</doc>
<doc id="7645" url="http://en.wikipedia.org/wiki?curid=7645" title="Cyclone (programming language)">
Cyclone (programming language)

The Cyclone programming language is intended to be a safe dialect of the C language. Cyclone is designed to avoid buffer overflows and other vulnerabilities that are endemic in C programs, without losing the power and convenience of C as a tool for system programming.
Cyclone development was started as a joint project of AT&T Labs Research and Greg Morrisett's group at Cornell in 2001. Version 1.0 was released on May 8, 2006.
Language features.
Cyclone attempts to avoid some of the common pitfalls of C, while still maintaining its look and performance. To this end, Cyclone places the following limits on programs:
To maintain the tool set that C programmers are used to, Cyclone provides the following extensions:
For a better high-level introduction to Cyclone, the reasoning behind Cyclone and the source of these lists, see .
Cyclone looks, in general, much like C, but it should be viewed as a C-like language.
Pointer/reference types.
Cyclone implements three kinds of reference (following C terminology these are called pointers):
The purpose of introducing these new pointer types is to avoid common problems when using pointers. Take for instance a function, called codice_17 that takes a pointer to an int:
Although the person who wrote the function codice_17 could have inserted codice_1 checks, let us assume that for performance reasons they did not. Calling codice_20 will result in undefined behavior (typically, although not necessarily, a SIGSEGV being sent to the application). To avoid such problems, Cyclone introduces the codice_14 pointer type, which can never be codice_1. Thus, the "safe" version of codice_17 would be:
This tells the Cyclone compiler that the argument to codice_17 should never be codice_1, avoiding the aforementioned undefined behavior. The simple change of codice_13 to codice_14 saves the programmer from having to write codice_1 checks and the operating system from having to trap codice_1 pointer dereferences. This extra limit, however, can be a rather large stumbling block for most C programmers, who are used to being able to manipulate their pointers directly with arithmetic. Although this is desirable, it can lead to buffer overflows and other "off-by-one"-style mistakes. To avoid this, the codice_16 pointer type is delimited by a known bound, the size of the array. Although this adds overhead due to the extra information stored about the pointer, it improves safety and security. Take for instance a simple (and naïve) codice_31 function, written in C:
This function assumes that the string being passed in is terminated by NULL (codice_32). However, what would happen if codice_33 were passed to this string? This is perfectly legal in C, yet would cause codice_31 to iterate through memory not necessarily associated with the string codice_35. There are functions, such as codice_36 which can be used to avoid such problems, but these functions are not standard with every implementation of ANSI C. The Cyclone version of codice_31 is not so different from the C version:
Here, codice_31 bounds itself by the length of the array passed to it, thus not going over the actual length. Each of the kinds of pointer type can be safely cast to each of the others, and arrays and strings are automatically cast to codice_16 by the compiler. (Casting from codice_16 to codice_13 invokes a bounds check, and casting from codice_16 to codice_14 invokes both a codice_1 check and a bounds check. Casting from codice_13 or codice_16 results in no checks whatsoever; the resulting codice_16 pointer has a size of 1.)
Dangling pointers and region analysis.
Consider the following code, in C:
This returns an object that is allocated on the stack of the function codice_48, which is not available after the function returns. While gcc and other compilers will warn about such code, the following will typically compile without warnings:
Cyclone does regional analysis of each segment of code, preventing dangling pointers, such as the one returned from this version of codice_48. All of the local variables in a given scope are considered to be part of the same region, separate from the heap or any other local region. Thus, when analyzing codice_48, the compiler would see that codice_51 is a pointer into the local stack, and would report an error.
Examples.
The best example to start with is the classic Hello world program:
External links.
Presentations:

</doc>
<doc id="7646" url="http://en.wikipedia.org/wiki?curid=7646" title="Cognitivism">
Cognitivism

Cognitivism may refer to:

</doc>
<doc id="7647" url="http://en.wikipedia.org/wiki?curid=7647" title="Counter">
Counter

In digital logic and computing, a counter is a device which stores (and sometimes displays) the number of times a particular event or process has occurred, often in relationship to a clock signal.
Electronic counters.
In electronics, counters can be implemented quite easily using register-type circuits such as the flip-flop, and a wide variety of classifications exist:
Each is useful for different applications. Usually, counter circuits are digital in nature, and count in natural binary. Many types of counter circuits are available as digital building blocks, for example a number of chips in the 4000 series implement different counters.
Occasionally there are advantages to using a counting sequence other than the natural binary sequence—such as the binary coded decimal counter, a linear feedback shift register counter, or a Gray-code counter.
Counters are useful for digital clocks and timers, and in oven timers, VCR clocks, etc.
Asynchronous (ripple) counter.
An asynchronous (ripple) counter is a single d-type flip-flop, with its J (data) input fed from its own inverted output. This circuit can store one bit, and hence can count from zero to one before it overflows (starts over from 0). This counter will increment once for every clock cycle and takes two clock cycles to overflow, so every cycle it will alternate between a transition from 0 to 1 and a transition from 1 to 0. Notice that this creates a new clock with a 50% duty cycle at exactly half the frequency of the input clock. If this output is then used as the clock signal for a similarly arranged D flip-flop (remembering to invert the output to the input), one will get another 1 bit counter that counts half as fast. Putting them together yields a two-bit counter:
You can continue to add additional flip-flops, always inverting the output to its own input, and using the output from the previous flip-flop as the clock signal. The result is called a ripple counter, which can count to where "n" is the number of bits (flip-flop stages) in the counter. Ripple counters suffer from unstable outputs as the overflows "ripple" from stage to stage, but they do find frequent application as dividers for clock signals, where the instantaneous count is unimportant, but the division ratio overall is (to clarify this, a 1-bit counter is exactly equivalent to a divide by two circuit; the output frequency is exactly half that of the input when fed with a regular train of clock pulses).
The use of flip-flop outputs as clocks leads to timing skew between the count data bits, making this ripple technique incompatible with normal synchronous circuit design styles.
Synchronous counter.
In synchronous counters, the clock inputs of all the flip-flops are connected together and are triggered by the input pulses. Thus, all the flip-flops change state simultaneously (in parallel). The circuit below is a 4-bit synchronous counter. The J and K inputs of FF0 are connected to HIGH. FF1 has its J and K inputs connected to the output of FF0, and the J and K inputs of FF2 are connected to the output of an AND gate that is fed by the outputs of FF0 and FF1.
A simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the image to the right) is for each bit to toggle when all of the less significant bits are at a logic high state. For example, bit 1 toggles when bit 0 is logic high; bit 2 toggles when both bit 1 and bit 0 are logic high; bit 3 toggles when bit 2, bit 1 and bit 0 are all high; and so on.
Synchronous counters can also be implemented with hardware finite state machines, which are more complex but allow for smoother, more stable transitions.
Hardware-based counters are of this type.A simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the image to the right) is for each bit to toggle when all of the less significant bits are at a logic high state
Decade counter.
A decade counter is one that counts in decimal digits, rather than binary. A decade counter may have each (that is, it may count in binary-coded decimal, as the 7490 integrated circuit did) or other binary encodings. "A decade counter is a binary counter that is designed to count to 1010b (decimal 10). An ordinary four-stage counter can be easily modified to a decade counter by adding a NAND gate as in the schematic to the right. Notice that FF2 and FF4 provide the inputs to the NAND gate. The NAND gate outputs are connected to the CLR input of each of the FFs." 
A decade counter is one that counts in decimal digits, rather than binary. It counts from 0 to 9 and then resets to zero. The counter output can be set to zero by pulsing the reset line low. The count then increments on each clock pulse until it reaches 1001 (decimal 9). When it increments to 1010 (decimal 10) both inputs of the NAND gate go high. The result is that the NAND output goes low, and resets the counter to zero. D going low can be a CARRY OUT signal, indicating that there has been a count of ten.
Ring counter.
A ring counter is a circular shift register which is initiated such that only one of its flip-flops is the state one while others are in their zero states.
A ring counter is a Shift Register (a cascade connection of flip-flops) with the output of the last one connected to the input of the first, that is, in a ring. Typically, a pattern consisting of a single bit is circulated so the state repeats every n clock cycles if n flip-flops are used.
Johnson counter.
A Johnson counter (or switchtail ring counter, twisted-ring counter, walking-ring counter, or Moebius counter) is a modified ring counter, where the output from the last stage is inverted and fed back as input to the first stage. The register cycles through a sequence of bit-patterns, whose length is equal to twice the length of the shift register, continuing indefinitely. These counters find specialist applications, including those similar to the decade counter, digital-to-analog conversion, etc. They can be implemented easily using D- or JK-type flip-flops.
Computer science counters.
In computability theory, a counter is considered a type of memory. A counter stores a single natural number (initially zero) and can be arbitrarily long. A counter is usually considered in conjunction with a finite-state machine (FSM), which can perform the following operations on the counter:
The following machines are listed in order of power, with each one being strictly more powerful than the one below it:
For the first and last, it doesn't matter whether the FSM is a deterministic finite automaton or a nondeterministic finite automaton. They have power. The first two and the last one are levels of the Chomsky hierarchy.
The first machine, an FSM plus two counters, is equivalent in power to a Turing machine. See the article on counter machines for a proof.
Web counter.
A web counter or hit counter is a computer software program that indicates the number of visitors, or hits, a particular webpage has received. Once set up, these counters will be incremented by one every time the web page is accessed in a web browser.
The number is usually displayed as an inline digital image or in plain text or on a physical counter such as a mechanical counter. Images may be presented in a variety of fonts, or styles; the classic example is the wheels of an odometer.
"Web counter" was popular in the 1980s and 1990s, later replaced by more detailed and complete web traffic measures.
Computer based counters.
Many automation systems use PC and laptops to monitor different parameters of machines and production data. Counters may count parameters such as the number of pieces produced, the production batch number, and measurements of the amounts of material used.
Mechanical counters.
Long before electronics became common, mechanical devices were used to count events. These are known as tally counters. They typically consist of a series of disks mounted on an axle, with the digits 0 through 9 marked on their edge. The right most disk moves one increment with each event. Each disk except the left-most has a protrusion that, after the completion of one revolution, moves the next disk to the left one increment. Such counters were originally used to control manufacturing processes, but were later used as odometers for bicycles and cars and in tape recorders and fuel dispensers. One of the largest manufacturers was the Veeder-Root company, and their name was often used for this type of counter.
Mechanical counter were used to accumulate totals in tabulating machines that pioneered the data processing industry.

</doc>
<doc id="7649" url="http://en.wikipedia.org/wiki?curid=7649" title="Cervical mucus method">
Cervical mucus method

Cervical mucus method may refer to a specific method of fertility awareness or natural family planning:

</doc>
<doc id="7651" url="http://en.wikipedia.org/wiki?curid=7651" title="Coleridge">
Coleridge

Coleridge may refer to:

</doc>
<doc id="7655" url="http://en.wikipedia.org/wiki?curid=7655" title="Clay Mathematics Institute">
Clay Mathematics Institute

The Clay Mathematics Institute (CMI) is a private, non-profit foundation, based in Providence, Rhode Island. CMI's scientific activities are managed from the President's office in Oxford, United Kingdom. The Institute is "dedicated to increasing and disseminating mathematical knowledge." It gives out various awards and sponsorships to promising mathematicians. The institute was founded in 1998 through the sponsorship of Boston businessman Landon T. Clay. Harvard mathematician Arthur Jaffe was the first president of CMI. 
While the institute is best known for its Millennium Prize Problems, it carries out a wide range of activities, including a postdoctoral program (five Clay Research Fellows are supported each year) and a bi-annual summer school, the proceedings of which are published jointly with the American Mathematical Society.
Governance.
The Institute is run according to a standard structure comprising a scientific advisory committee that decides on grant-awarding and research proposals, and a board of directors that oversees and approves the committee's decisions. , the board is made up of members of the Clay family, whereas the advisory committee is composed of leading authorities in mathematics, namely Sir Andrew Wiles, Yum-Tong Siu, Richard Melrose, Andrei Okounkov, and Simon Donaldson. Nicholas Woodhouse is the current president of CMI.
Millennium Prize Problems.
The institute is best known for establishing the Millennium Prize Problems on May 24, 2000. These seven problems are considered by CMI to be "important classic questions that have resisted solution over the years". For each problem, the first person to solve it will be awarded $1,000,000 by the CMI. In announcing the prize, CMI drew a parallel to Hilbert's problems, which were proposed in 1900, and had a substantial impact on 20th century mathematics. Of the initial twenty-three Hilbert problems, most of which have been solved, only the Riemann hypothesis (formulated in 1859) is included in the seven Millennium Prize Problems.
For each problem, the Institute had a professional mathematician write up an official statement of the problem, which will be the main standard by which a given solution will be measured against. The seven problems are:
Some of the mathematicians who were involved in the selection and presentation of the seven problems were Atiyah, Bombieri, Connes, Deligne, Fefferman, Milnor, Mumford, Wiles, and Witten.
Other awards.
The Clay Research Award.
In recognition of major breakthroughs in mathematical research, the institute has an annual prize - the Clay Research Award. Its recipients to date are Ian Agol, Manindra Agrawal, Yves Benoist, Manjul Bhargava, Danny Calegari, Alain Connes, Nils Dencker, Alex Eskin, David Gabai, Ben Green, Christopher Hacon, Richard Hamilton, Michael Harris, Jeremy Kahn, Laurent Lafforgue, Gérard Laumon, Vladimir Markovic, James McKernan, Ngô Bảo Châu, Jonathan Pila, Jean-François Quint, Oded Schramm, Stanislav Smirnov, Terence Tao, Clifford Taubes, Richard Taylor, Claire Voisin, Jean-Loup Waldspurger, Andrew Wiles, and Edward Witten.
Other activities.
Besides the Millennium Prize Problems, the Clay Mathematics Institute also supports mathematics via the awarding of
research fellowships (which range from two to five years, and are aimed at younger mathematicians), as well as shorter-term
scholarships for programs, individual research, and book writing. The Institute also has a yearly Clay Research Award, recognizing major breakthroughs in mathematical research. Finally, the Institute also organizes a number of summer schools, conferences, workshops, public lectures, and outreach activities aimed primarily at junior mathematicians (from the high school to postdoctoral level).
CMI publications are available in PDF form at most six months after they appear in print.

</doc>
<doc id="7659" url="http://en.wikipedia.org/wiki?curid=7659" title="Cerebral arteriovenous malformation">
Cerebral arteriovenous malformation

A cerebral arteriovenous malformation (AVM) is an abnormal connection between the arteries and veins in the brain.
Signs and symptoms.
The most frequently observed problems related to an AVM are headaches and seizures while at least 15% of the population at detection have no symptoms at all. Other common symptoms are a pulsing noise in the head, progressive weakness and numbness and vision changes as well as debilitating, excruciating pain.
In serious cases, the blood vessels rupture and there is bleeding within the brain (intracranial hemorrhage). Nevertheless in more than half of patients with AVM, hemorrhage is the first symptom. Symptoms due to bleeding include loss of consciousness, sudden and severe headache, nausea, vomiting, incontinence, and blurred vision, amongst others. Impairments caused by local brain tissue damage on the bleed site are also possible, including seizure, one-sided weakness (hemiparesis), a loss of touch sensation on one side of the body and deficits in language processing (aphasia). Minor bleeding can occur with no noticeable symptoms. Following the bleed's cessation, most AVM victims return to normal, after the blood vessel has had time to repair itself.
AVMs in certain critical locations may stop the circulation of the cerebrospinal fluid, causing accumulation of the fluid within the skull and giving rise to a clinical condition called hydrocephalus. A stiff neck can occur as the result of increased pressure within the skull and irritation of the meninges.
Diagnosis.
An AVM diagnosis is established by neuroimaging studies after a complete neurological and physical examination. Three main techniques are used to visualize the brain and search for AVM: computed tomography (CT), magnetic resonance imaging (MRI), and cerebral angiography. A CT scan of the head is usually performed first when the subject is symptomatic. It can suggest the approximate site of the bleed. MRI is more sensitive than CT in the diagnosis of AVMs and provides better information about the exact location of the malformation. More detailed pictures of the tangle of blood vessels that compose an AVM can be obtained by using radioactive agents injected into the blood stream. If a CT is used in conjunction of dye this is called a computerized tomography angiogram while if MRI is used it is called magnetic resonance angiogram. The best images of an AVM are obtained through cerebral angiography. This procedure involves using a catheter, threaded through an artery up to the head, to deliver a contrast agent into the AVM. As the contrast agent flows through the AVM structure, a sequence of X-ray images are obtained.
Grading.
A common method of grading cerebral AVMs is the Spetzler-Martin grade. This system was designed to assess the patient's risk of neurological deficit after open surgical resection, based on characteristics of the AVM itself. Based on this system, AVMs may be classified as grades 1 - 5.
"Eloquent cortex" is a name used by neurologists for areas of cortex that, if removed will result in loss of sensory processing or linguistic ability, minor paralysis, or paralysis.
The risk of post-surgical neurological deficit (difficulty with language, motor weakness, vision loss) increases with increasing Spetzler-Martin grade.
Pathophysiology.
AVMs are an abnormal connection between the arteries and veins in the human brain. Arteriovenous malformations are most commonly of prenatal origin. The cause of AVMs remains unknown. In a normal brain oxygen enriched blood from the heart travels in sequence through smaller blood vessels going from arteries, to arterioles and then capillaries. Oxygen is removed in the latter vessel to be used by the brain. After the oxygen is removed blood reaches venules and later veins which will take it back to the heart and lungs. On the other hand when there is an AVM blood goes directly from arteries to veins through the abnormal vessels disrupting the normal circulation of blood.
Prognosis.
The main risk is intracranial hemorrhage. This risk is difficult to quantify since many patients with asymptomatic AVMs will never come to medical attention. Small AVMs tend to bleed more often than do larger ones, the opposite of cerebral aneurysms. If a rupture or bleeding incident occurs, the blood may penetrate either into the brain tissue (cerebral hemorrhage) or into the subarachnoid space, which is located between the sheaths (meninges) surrounding the brain (subarachnoid hemorrhage). Bleeding may also extend into the ventricular system (intraventricular hemorrhage). Cerebral hemorrhage appears to be most common.
One long-term study (mean follow up greater than 20 years) of over 150 symptomatic AVMs (either presenting with bleeding or seizures) found the risk of cerebral hemorrhage to be approximately 4% per year, slightly higher than the 2-3% seen in other studies. A simple, rough approximation of a patient's lifetime bleeding risk is 105 - (patient age in years). This equation assumes a 3% yearly bleeding risk. For example, a healthy 30 year old patient would have approximately a 75% lifetime risk of at least one bleeding event.
Treatment.
Treatment depends on the location and size of the AVM and whether there is bleeding or not.
The treatment in the case of sudden bleeding is focused on restoration of vital function. Anticonvulsant medications such as phenytoin are often used to control seizure; medications or procedures may be employed to relieve intracranial pressure. Eventually, curative treatment may be required to prevent recurrent hemorrhage. However, any type of intervention may also carry a risk of creating a neurological deficit.
Preventive treatment of as yet unruptured brain AVMs has been controversial, as several studies suggested favorable long-term outcome for unruptured AVM patients not undergoing intervention. The NIH-funded longitudinal ARUBA study ("A Randomized trial of Unruptured Brain AVMs) compares the risk of stroke and death in patients with preventive AVM eradication versus those followed without intervention. Interim results suggest that fewer strokes occur as long as patients with unruptured AVM do not undergo intervention. Because of the higher than expected event rate in the interventional arm of the ARUBA study, NIH/NINDS has stopped patient enrollment in April 2013, while continuing to follow all participants to determine whether the difference in stroke and death in the two arms changes over time.
Surgical elimination of the blood vessels involved is the preferred curative treatment for many types of AVM. Surgery is performed by a neurosurgeon who temporarily removes part of the skull (craniotomy), separates the AVM from surrounding brain tissue, and resects the abnormal vessels. While surgery can result in an immediate, complete removal of the AVM, risks exist depending on the size and the location of the malformation. The preferred treatment of Spetzler-Martin grade 1 and 2 AVMs in young, healthy patients is surgical resection due to the relatively small risk of neurological damage compared to the high lifetime risk of hemorrhage. Grade 3 AVMs may or may not be amenable to surgery. Grade 4 and 5 AVMs are not usually surgically treated.
Radiosurgery has been widely used on small AVMs with considerable success. The Gamma Knife is an apparatus used to precisely apply a controlled radiation dosage to the volume of the brain occupied by the AVM. While this treatment does not require an incision and craniotomy (with their own inherent risks), three or more years may pass before the complete effects are known, during which time patients are at risk of bleeding. Complete obliteration of the AVM may or may not occur after several years, and repeat treatment may be needed. Radiosurgery is itself not without risk. In one large study, nine percent of patients had transient neurological symptoms, including headache, after radiosurgery for AVM. However, most symptoms resolved, and the long-term rate of neurological symptoms was 3.8%.
Embolization is the occlusion of blood vessels most commonly with a glue-like substance introduced by a radiographically guided catheter. Such glue blocks the vessel and reduces blood flow into the AVM. Embolization is frequently used as an adjunct to either surgery or radiation treatment. Before other treatments it reduces the size of the AVM while during surgery it reduces the risk of bleeding. However, embolization alone may completely obliterate some AVMs. In high flow intranidal fistula we can use balloon to reduce the flow so that embolization can be done safely.
Epidemiology.
The annual new detection rate incidence of AVMs is approximately 1 per 100,000 a year. The point prevalence in adults is approximately 18 per 100,000. AVMs are more common in males than females, although in females pregnancy may start or worsen symptoms due the increase in blood flow and volume it usually brings.
Research directions.
No randomized, controlled clinical trial has established a survival benefit for treating patients (either with open surgery or radiosurgery) with AVMs that have not yet bled.

</doc>
<doc id="7660" url="http://en.wikipedia.org/wiki?curid=7660" title="Comparative method (linguistics)">
Comparative method (linguistics)

In linguistics, the comparative method is a technique for studying the development of languages by performing a feature-by-feature comparison of two or more languages with common descent from a shared ancestor, as opposed to the method of internal reconstruction, which analyses the internal development of a single language over time. Ordinarily both methods are used together to reconstruct prehistoric phases of languages, to fill in gaps in the historical record of a language, to discover the development of phonological, morphological, and other linguistic systems, and to confirm or refute hypothesized relationships between languages.
The comparative method was developed over the 19th century. Key contributions were made by the Danish scholars Rasmus Rask and Karl Verner and the German scholar Jacob Grimm. The first linguist to offer reconstructed forms from a proto-language was August Schleicher, in his "Compendium der vergleichenden Grammatik der indogermanischen Sprachen", originally published in 1861. Here is Schleicher’s explanation of why he offered reconstructed forms:
In the present work an attempt is made to set forth the inferred Indo-European original language side by side with its really existent derived languages. Besides the advantages offered by such a plan, in setting immediately before the eyes of the student the final results of the investigation in a more concrete form, and thereby rendering easier his insight into the nature of particular Indo-European languages, there is, I think, another of no less importance gained by it, namely that it shows the baselessness of the assumption that the non-Indian Indo-European languages were derived from Old-Indian (Sanskrit).
Demonstrating genetic relationship.
The comparative method aims to prove that two or more historically attested languages are descended from a single proto-language by comparing lists of cognate terms. From them, regular sound correspondences between the languages are established, and a sequence of regular sound changes can then be postulated, which allows the proto-language to be reconstructed. Relation is deemed certain only if at least a partial reconstruction of the common ancestor is feasible, and if regular sound correspondences can be established with chance similarities ruled out.
Terminology.
"Descent" is defined as transmission across the generations: children learn a language from the parents' generation and after being influenced by their peers transmit it to the next generation, and so on. For example, a continuous chain of speakers across the centuries links Vulgar Latin to all of its modern descendants.
Two languages are "genetically related" if they descended from the same ancestor language. For example, Spanish and French both come from Latin and therefore belong to the same family, the Romance languages.
However, it is possible for languages to have different degrees of relatedness. English, for example, is related to both German and Russian, but is more closely related to the former than it is to the latter. Although all three languages share a common ancestor, Proto-Indo-European, English and German also share a more recent common ancestor, Proto-Germanic, while Russian does not. Therefore, English and German are considered to belong to a different subgroup, the Germanic languages.
"Shared retentions" from the parent language are not sufficient evidence of a sub-group. For example, as a result of heavy borrowing from Arabic into Persian, Modern Persian in fact takes more of its vocabulary from Arabic than from its direct ancestor, Proto-Indo-Iranian. The division of related languages into sub-groups is more certainly accomplished by finding "shared linguistic innovations" from the parent language.
Origin and development of the method.
Languages have been compared since antiquity. For example, in the 1st century BC the Romans were aware of the similarities between Greek and Latin, which they explained mythologically, as the result of Rome being a Greek colony speaking a debased dialect. In the 9th or 10th century, Yehuda Ibn Quraysh compared the phonology and morphology of Hebrew, Aramaic, and Arabic, but attributed this resemblance to the Biblical story of Babel, with Abraham, Isaac and Joseph retaining Adam's language, with other languages at various removes becoming more altered from the original Hebrew.
In publications of 1647 and 1654, Marcus van Boxhorn first described a rigid methodology for historical linguistic comparisons and proposed the existence of an Indo-European proto-language (which he called "Scythian") unrelated to Hebrew, but ancestral to Germanic, Greek, Romance, Persian, Sanskrit, Slavic, Celtic and Baltic languages. The Scythian theory was further developed by Andreas Jäger (1686) and William Wotton (1713), who made first forays to reconstruct this primitive common language. In 1710 and 1723, Lambert ten Kate first formulated the regularity of sound laws, introducing among others, the term root vowel.
Another early systematic attempt to prove the relationship between two languages on the basis of similarity of grammar and lexicon was made by the Hungarian János Sajnovics in 1770, when he attempted to demonstrate the relationship between Sami and Hungarian (work that was later extended to the whole Finno-Ugric language family in 1799 by his countryman Samuel Gyarmathi), But the origin of modern historical linguistics is often traced back to Sir William Jones, an English philologist living in India, who in 1786 made his famous 
“The Sanscrit language, whatever be its antiquity, is of a wonderful structure; more perfect than the Greek, more copious than the Latin, and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and the forms of grammar, than could possibly have been produced by accident; so strong indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists. There is a similar reason, though not quite so forcible, for supposing that both the Gothick and the Celtick, though blended with a very different idiom, had the same origin with the Sanscrit; and the old Persian might be added to the same family.”
The comparative method developed out of attempts to reconstruct the proto-language mentioned by Jones, which he did not name, but subsequent linguists named Proto-Indo-European (PIE). The first professional comparison between the Indo-European languages known then was made by the German linguist Franz Bopp in 1816. Though he did not attempt a reconstruction, he demonstrated that Greek, Latin and Sanskrit shared a common structure and a common lexicon. Friedrich Schlegel in 1808 first stated the importance of using the eldest possible form of a language when trying to prove its relationships; in 1818, Rasmus Christian Rask developed the principle of regular sound changes to explain his observations of similarities between individual words in the Germanic languages and their cognates in Greek and Jacob Grimm - better known for his "Fairy Tales" - in "Deutsche Grammatik" (published 1819-37 in four volumes) made use of the comparative method in attempting to show the development of the Germanic languages from a common origin, the first systematic study of diachronic language change.
Both Rask and Grimm were unable to explain apparent exceptions to the sound laws that they had discovered. Although Hermann Grassmann explained one of these anomalies with the publication of Grassmann's law in 1862, it was Karl Verner who in 1875 made a methodological breakthrough when he identified a pattern now known as Verner's law, the first sound law based on comparative evidence showing that a phonological change in one phoneme could depend on other factors within the same word, such as the neighbouring phonemes and the position of the accent, now called "conditioning environments".
Similar discoveries made by the "Junggrammatiker" (usually translated as Neogrammarians) at the University of Leipzig in the late 1800s led them to conclude that all sound changes were ultimately regular, resulting in the famous statement by Karl Brugmann and Hermann Osthoff in 1878 that "sound laws have no exceptions". This idea is fundamental to the modern comparative method, since the method necessarily assumes regular correspondences between sounds in related languages, and consequently regular sound changes from the proto-language. This "Neogrammarian Hypothesis" led to application of the comparative method to reconstruct Proto-Indo-European, with Indo-European being at that time by far the most well-studied language family. Linguists working with other families soon followed suit, and the comparative method quickly became the established method for uncovering linguistic relationships.
Application.
There is no fixed set of steps to be followed in the application of the comparative method, but Lyle Campbell suggests some basic steps and so does Terry Crowley, who are both authors of introductory texts in historical linguistics. The abbreviated summary below is based on their concepts of how to proceed.
Step 1, assemble potential cognate lists.
This step involves making lists of words that are likely cognates among the languages being compared. If there is a regularly recurring match between the phonetic structure of basic words with similar meanings a genetic kinship can probably be established. For example, looking at the Polynesian family linguists might come up with a list similar to the following (a list actually used by them would be much longer):
Borrowings or false cognates could skew or obscure the correct data. For example, English "taboo" () is like the six Polynesian forms due to borrowing from Tongan into English, and not because of a genetic similarity. This problem can usually be overcome by using basic vocabulary such as kinship terms, numbers, body parts, pronouns, and other basic terms. Nonetheless, even basic vocabulary can be sometimes borrowed. Finnish, for example, borrowed the word for "mother", "äiti", from Gothic "aiþei". While English borrowed the pronouns "they", "them", and "their(s)" from Norse, Thomason and Everett argue that Pirahã, a Muran language of South America for which a number of controversial claims are made, borrowed all its pronouns from Nhengatu.
Step 2, establish correspondence sets.
The next step is to determine the regular sound correspondences exhibited by the potential cognates lists. Mere phonetic similarity, as between English "day" and Latin "dies" (both with the same meaning), has no probative value. English initial "d-" does "not" regularly match and whatever sporadic matches can be observed are due either to chance (as in the above example) or to borrowing (for example, Latin "diabolus" and English "devil", both ultimately of Greek origin). English and Latin "do" exhibit a regular correspondence of "t-" : "d-" (where the notation "A : B" means "A corresponds to B"); for example,
If there are many regular correspondence sets of this kind (the more the better), then a common origin becomes a virtual certainty, particularly if some of the correspondences are non-trivial or unusual.
Step 3, discover which sets are in complementary distribution.
During the late 18th to late 19th century, two major developments improved the method's effectiveness.
First, it was found that many sound changes are conditioned by a specific "context". For example, in both Greek and Sanskrit, an aspirated stop evolved into an unaspirated one, but only if a second aspirate occurred later in the same word; this is Grassmann's law, first described for Sanskrit by Sanskrit grammarian Pāṇini and promulgated by Hermann Grassmann in 1863.
Second, it was found that sometimes sound changes occurred in contexts that were later lost. For instance, in Sanskrit velars ("k"-like sounds) were replaced by palatals ("ch"-like sounds) whenever the following vowel was "*i" or "*e". Subsequent to this change, all instances of "*e" were replaced by "a". The situation would have been unreconstructable, had not the original distribution of "e" and "a" been recoverable from the evidence of other Indo-European languages. For instance, Latin suffix "que", "and", preserves the original "*e" vowel that caused the consonant shift in Sanskrit:
Verner's Law, discovered by Karl Verner in about 1875, is a similar case: the voicing of consonants in Germanic languages underwent a change that was determined by the position of the old Indo-European accent. Following the change, the accent shifted to initial position. Verner solved the puzzle by comparing the Germanic voicing pattern with Greek and Sanskrit accent patterns.
This stage of the comparative method, therefore, involves examining the correspondence sets discovered in step 2 and seeing which of them apply only in certain contexts. If two (or more) sets apply in complementary distribution, they can be assumed to reflect a single original phoneme: "some sound changes, particularly conditioned sound changes, can result in a proto-sound being associated with more than one correspondence set".
For example, the following potential cognate list can be established for Romance languages, which descend from Latin:
They evidence two correspondence sets, "k : k" and "k : :
Since French "" only occurs before "a" where the other languages also have "a", while French "k" occurs elsewhere, the difference is due to different environments and the sets are complementary. They can therefore be assumed to reflect a single proto-phoneme (in this case "*k", spelled <c> in Latin). The original words are corpus, crudus, catena and captiare, all with an initial k-sound. If more evidence along these lines were given, one might conclude to an alteration of the original k because of a different environment.
A more complex case involves consonant clusters in Proto-Algonquian. The Algonquianist Leonard Bloomfield used the reflexes of the clusters in four of the daughter languages to reconstruct the following correspondence sets:
Although all five correspondence sets overlap with one another in various places, they are not in complementary distribution, and so Bloomfield recognized that a different cluster must be reconstructed for each set; his reconstructions were, respectively, "*hk", "*xk", "*čk" (=), "*šk" (=), and "çk" (where "‘x’" and "‘ç’" are arbitrary symbols, not attempts to guess the phonetic value of the proto-phonemes).
Step 4, reconstruct proto-phonemes.
Typology assists in deciding what reconstruction best fits the data. For example, the voicing of voiceless stops between vowels is common, but not the devoicing of voiced stops there. If a correspondence "-t-" : "-d-" between vowels is found in two languages, the proto-phoneme is more likely to be "*-t-", with a development to the voiced form in the second language. The opposite reconstruction would create a rare type.
However, unusual sound changes do occur. The Proto-Indo-European word for "two", for example, is reconstructed as "*dwō", which is reflected in Classical Armenian as "erku". Several other cognates demonstrate a regular change "*dw-" → "erk-" in Armenian. Similarly, in Bearlake, a dialect of the Athabaskan language of Slavey, there has been a sound change of Proto-Athabaskan "*ts" → Bearlake '. It is very unlikely that "*dw-" changed directly into "erk-" and "*ts" into ', but instead they must have gone through several intermediate steps to arrive at the later forms. It is not phonetic similarity which matters when utilizing the comparative method, but regular sound correspondences.
By the Principle of Economy, the reconstruction of a proto-phoneme should require as few sound changes as possible to arrive at the modern reflexes in the daughter languages. For example, Algonquian languages exhibit the following correspondence set:
The simplest reconstruction for this set would be either "*m" or "*b". Both "*m" → "b" and "*b" → "m" are likely. Because "m" occurs in five of the languages, and "b" in only one, if "*b" is reconstructed, then it is necessary to assume five separate changes of "*b" → "m", whereas if "*m" is reconstructed, it is only necessary to assume a single change of "*m" → "b". "*m" would be most economical. (This argument assumes that the languages other than Arapaho are at least partly independent of each other. If they all formed a common subgroup, the development "*b" → "m" would only have to be assumed having occurred once.)
Step 5, examine the reconstructed system typologically.
In the final step, the linguist checks to see how the proto-phonemes fit the known typological constraints. For example, in a hypothetical system,
there is only one voiced stop, "*b", and although there is an alveolar and a velar nasal, "*n" and "*ŋ", there is no corresponding labial nasal. However, languages generally (though not always) tend to maintain symmetry in their phonemic inventories. In this case, the linguist might attempt to investigate the possibilities that what was earlier reconstructed as "*b" is in fact "*m", or that the "*n" and "*ŋ" are in fact "*d" and "*g".
Even a symmetrical system can be typologically suspicious. For example, the traditional Proto-Indo-European stop inventory is:
An earlier voiceless aspirated row was removed on grounds of insufficient evidence. Since the mid-20th century, a number of linguists have argued that this phonology is implausible; that it is extremely unlikely for a language to have a voiced aspirated (breathy voice) series without a corresponding voiceless aspirated series. A potential solution was provided by Thomas Gamkrelidze and Vyacheslav V. Ivanov, who argued that the series traditionally reconstructed as plain voiced should in fact be reconstructed as glottalized — either implosive or ejective . The plain voiceless and voiced aspirated series would thus be replaced by just voiceless and voiced, with aspiration being a non-distinctive quality of both. This example of the application of linguistic typology to linguistic reconstruction has become known as the Glottalic Theory. It has a large number of proponents but is not generally accepted. As an alternative, the voiceless aspirated row was restored.
The reconstruction of proto-sounds logically precedes the reconstruction of grammatical morphemes (word-forming affixes and inflectional endings), patterns of declension and conjugation, and so on. The full reconstruction of an unrecorded protolanguage is an open-ended task.
Limitations.
Problems with the history of historical linguistics.
The limitations of the comparative method were recognized by the very linguists who developed it, but it is still seen as a valuable tool. In the case of Indo-European, the method seemed to at least partially validate the centuries-old search for an Ursprache, the original language. These others were presumed ordered in a family tree, becoming the Tree model of the neogrammarians.
The archaeologists followed suit, attempting to find archaeological evidence of a culture or cultures that could be presumed to have spoken a proto-language, such as Vere Gordon Childe's "The Aryans: a study of Indo-European origins", 1926. Childe was a philologist turned archaeologist. These views culminated in the "Siedlungsarchaologie", or "settlement-archaeology", of Gustaf Kossinna, becoming known as "Kossinna's Law." He asserted that cultures represent ethnic groups, including their languages. It was rejected as a law in the post-World-War-II era. The fall of Kossinna's Law removed the temporal and spatial framework previously applied to many proto-languages. Fox concludes:
The Comparative Method "as such" is not, in fact, historical; it provides evidence of linguistic relationships to which we may give a historical interpretation. ...[Our increased knowledge about the historical processes involved] has probably made historical linguists less prone to equate the idealizations required by the method with historical reality. ...Provided we keep [the interpretation of the results and the method itself] apart, the Comparative Method can continue to be used in the reconstruction of earlier stages of languages.
Proto-languages can be verified in many historical instances, such as Latin. Although no longer a law, settlement-archaeology is known to be essentially valid for some cultures that straddle history and prehistory, such as the Celtic Iron Age (mainly Celtic) and Mycenaean civilization (mainly Greek). None of these models can be or have been completely rejected, and yet none alone are sufficient.
Problems with the neogrammarian hypothesis.
The foundation of the comparative method, and of comparative linguistics in general, is the Neogrammarians' fundamental assumption that "sound laws have no exceptions." When it was initially proposed, critics of the Neogrammarians proposed an alternate position, summarized by the maxim "each word has its own history". Several types of change do in fact alter words in non-regular ways. Unless identified, they may hide or distort laws and cause false perceptions of relationship.
Borrowing.
All languages borrow words from other languages in various contexts. They are likely to have followed the laws of the languages from which they were borrowed rather than the laws of the borrowing language.
Areal diffusion.
Borrowing on a larger scale occurs in areal diffusion, when features are adopted by contiguous languages over a geographical area. The borrowing may be phonological, morphological or lexical. A false proto-language over the area may be reconstructed for them or may be taken to be a third language serving as a source of diffused features.
Several areal features and other influences may converge to form a sprachbund, a wider region sharing features that appear to be related but are diffusional. For instance, the Mainland Southeast Asia linguistic area suggested several false classifications of such languages as Chinese, Thai and Vietnamese before it was recognized.
Random mutations.
Sporadic changes, such as irregular inflections, compounding, and abbreviation, do not follow any laws. For example, the Spanish words "palabra" ('word'), "peligro" ('danger') and "milagro" ('miracle') should have been "parabla", "periglo", "miraglo" by regular sound changes from the Latin "parabŏla", "perĩcǔlum" and "mĩrãcǔlum", but the "r" and "l" changed places by sporadic metathesis.
Analogy.
Analogy is the sporadic change of a feature to be like another feature in the same or a different language. It may affect a single word or be generalized to an entire class of features, such as a verb paradigm. For example, the Russian word for "nine", by regular sound changes from Proto-Slavic, should have been , but is in fact . It is believed that the initial ' changed to ' under influence of the word for "ten" in Russian, .
Gradual application.
Students of contemporary language changes, such as William Labov, note that even a systematic sound change is at first applied in an unsystematic fashion, with the percentage of its occurrence in a person's speech dependent on various social factors. The sound change gradually spreads, a process known as lexical diffusion. While not invalidating the Neogrammarians' axiom that "sound laws have no exceptions", their gradual application shows that they do not always apply to all lexical items at the same time. Hock notes, "While it probably is true in the long run every word has its own history, it is not justified to conclude as some linguists have, that therefore the Neogrammarian position on the nature of linguistic change is falsified."
Problems with the Tree Model.
The comparative method is used to construct a Tree model (German "Stammbaum") of language evolution, in which daughter languages are seen as branching from the proto-language, gradually growing more distant from it through accumulated phonological, morpho-syntactic, and lexical changes.
The presumption of a well-defined node.
The tree model features nodes that are presumed to be distinct proto-languages existing independently in distinct regions during distinct historical times. The reconstruction of unattested proto-languages lends itself to that illusion: they cannot be verified and the linguist is free to select whatever definite times and places for them seem best. Right from the outset of Indo-European studies, however, Thomas Young said:It is not, however, very easy to say what the definition should be that should constitute a separate language, but it seems most natural to call those languages distinct, of which the one cannot be understood by common persons in the habit of speaking the other … Still, however, it may remain doubtfull whether the Danes and the Swedes could not, in general, understand each other tolerably well … nor is it possible to say if the twenty ways of pronouncing the sounds, belonging to the Chinese characters, ought or ought not to be considered as so many languages or dialects… But, … the languages so nearly allied must stand next to each other in a systematic order…
The assumption of uniformity in a proto-language, implicit in the comparative method, is problematic. Even in small language communities there are always dialect differences, whether based on area, gender, class, or other factors. The Pirahã language of Brazil is spoken by only several hundred people, but it has at least two different dialects, one spoken by men and one by women. Campbell points out:
It is not so much that the comparative method 'assumes' no variation; rather, it is just that there is nothing built into the comparative method which would allow it to address variation directly...This assumption of uniformity is a reasonable idealization; it does no more damage to the understanding of the language than, say, modern reference grammars do which concentrate on a language's general structure, typically leaving out consideration of regional or social variation.
Different dialects, as they evolve into separate languages, remain in contact with one another and influence each other. Even after they are considered distinct, languages near to one another continue to influence each other, often sharing grammatical, phonological, and lexical innovations. A change in one language of a family may spread to neighboring languages; and multiple waves of change are communicated like waves across language and dialect boundaries, each with its own randomly delimited range. If a language is divided into an inventory of features, each with its own time and range (isoglosses), they do not all coincide. History and prehistory may not offer a time and place for a distinct coincidence, as may be the case for proto-Italic, in which case the proto-language is only a concept. However, Hock observes:
The discovery in the late nineteenth century that isoglosses can cut across well-established linguistic boundaries at first created considerable attention and controversy. And it became fashionable to oppose a wave theory to a tree theory... Today, however, it is quite evident that the phenomena referred to by these two terms are complementary aspects of linguistic change...
Subjectivity of the reconstruction.
The reconstruction of unknown proto-languages is inherently subjective. In the Proto-Algonquian example above, the choice of "*m" as the parent phoneme is only "likely", not "certain". It is conceivable that a Proto-Algonquian language with "*b" in those positions split into two branches, one which preserved "*b" and one which changed it to "*m" instead; and while the first branch only developed into Arapaho, the second spread out wider and developed into all the other Algonquian tribes. It is also possible that the nearest common ancestor of the Algonquian languages used some other sound instead, such as "*p", which eventually mutated to "*b" in one branch and to "*m" in the other. While examples of strikingly complicated and even circular developments are indeed known to have occurred (such as PIE "*t" > Pre-Proto-Germanic "*þ" > PG "*ð" > Proto-West-Germanic "*d" > Old High German "t" in "fater" > Modern German "Vater"), in the absence of any evidence or other reason to postulate a more complicated development, the preference of a simpler explanation is justified by the principle of parsimony, also known as Occam's razor. Since reconstruction involves many of these choices, some linguists prefer to view the reconstructed features as abstract representations of sound correspondences, rather than as objects with a historical time and place.
The existence of proto-languages and the validity of the comparative method is verifiable in cases where the reconstruction can be matched to a known language, which may only be known as a shadow in the loanwords of another language. For example Finnic languages such as Finnish have borrowed many words from an early stage of Germanic, and the shape of the loans matches the forms that have been reconstructed for Proto-Germanic. Finnish "kuningas" 'king' and "kaunis" 'beautiful' match the Germanic reconstructions *"kuningaz" and *"skauniz" (>German "König" 'king', "schön" 'beautiful').
Additional models.
The Wave model was developed in the 1870s as an alternative to the Tree model, in order to represent the historical patterns of language diversification. Both the tree-based and the wave-based representations are compatible with the Comparative Method.
By contrast, some approaches are incompatible with the Comparative method, including glottochronology and mass lexical comparison. Most historical linguists consider these to be flawed and unreliable.

</doc>
<doc id="7661" url="http://en.wikipedia.org/wiki?curid=7661" title="Council of Constance">
Council of Constance

The Council of Constance is the 15th century ecumenical council recognized by the Roman Catholic Church, held from 1414 to 1418. The council ended the Three-Popes Controversy, by deposing or accepting the resignation of the remaining Papal claimants and electing Pope Martin V.
The Council also condemned and executed Jan Hus and ruled on issues of national sovereignty, the rights of pagans, and just war in response to a conflict between the Kingdom of Poland and the Order of the Teutonic Knights. The Council is important for its relationship to ecclesial Conciliarism and Papal supremacy.
Origin and background.
The council's main purpose was to end the Papal schism which had resulted from the confusion following the Avignon Papacy. Pope Gregory XI's return to Rome in 1377, followed by his death and the controversial election of his successor, Pope Urban VI, resulted in the defection of a number of cardinals and the election of a rival pope based at Avignon in 1378. After thirty years of schism, the Council of Pisa had sought to resolve the situation by deposing the two claimant popes and elected a new pope, Alexander V. The council claimed that in such a situation, a council of bishops had greater authority than just one bishop, even if he were the bishop of Rome. Though Alexander and his successor, John XXIII, gained widespread support, especially at the cost of the Avignon pope, the schism remained, now involving not two but three claimants: Gregory XII at Rome, Benedict XIII at Avignon and John XXIII.
Therefore, many voices, including Sigismund, King of Germany and Hungary (and later Holy Roman Emperor) pressed for another council to resolve the issue. That council was called by John XXIII and was held from 16 November 1414 to 22 April 1418 in Constance, Germany. According to Joseph McCabe, the council was attended by roughly 29 cardinals, 100 "learned doctors of law and divinity," 134 abbots, and 183 bishops and archbishops. An innovation at the Council was that instead of voting as individuals, the bishops voted in national blocks, explicitly confirming the national pressures that had fueled the schism since 1378.
Decrees and doctrinal status.
The famous decree "Haec Sancta Synodus," which gave primacy to the authority of the Council and thus became a source for ecclesial conciliarism, was promulgated in the fifth session, 6 April 1415:
Marks the high-water mark of the Conciliar movement of reform. This decree, however, is not considered valid by the Magisterium of the Catholic Church, since it was never approved by Pope Gregory XII or his successors, and was passed by the Council in a session before his confirmation. The Church declared the first sessions of the Council of Constance an invalid and illicit assembly of Bishops, gathered under the authority of John XXIII.
The acts of the Council were not made public until 1442, at the behest of the Council of Basel; they were printed in 1500. The creation of a book on how to die was ordered by the council, and thus written in 1415 called "Ars moriendi".
Ending the Western Schism.
With the support of King Sigismund, enthroned before the high altar of the cathedral of Constance, the Council of Constance recommended that all three popes abdicate, and that another be chosen. In part because of the constant presence of the King, other rulers demanded that they have a say in who would be pope.
Gregory XII then sent representatives to Constance, whom he granted full powers to summon, open and preside over an Ecumenical Council; he also empowered them to present his resignation to the Papacy. This would pave the way for the end of the Western Schism.
The legates were received by King Sigismund and by the assembled Bishops, and the King yielded the presidency of the proceedings to the papal legates, Cardinal Dominici of Ragusa and Prince Charles of Malatesta. On 4 July 1415 the Bull of Gregory XII which appointed Malatesta and Cardinal Dominici of Ragusa as his proxies at the council was formally read before the assembled Bishops. The cardinal then read a decree of Gregory XII which convoked the council and authorized its succeeding acts. Thereupon, the Bishops voted to accept the summons. Prince Malatesta immediately informed the Council that he was empowered by a commission from Pope Gregory XII to resign the Papal Throne on the Pontiff's behalf. He asked the Council whether they would prefer to receive the abdication at that point or at a later date. The Bishops voted to receive the Papal abdication immediately. Thereupon the commission by Gregory XII authorizing his proxy to resign the Papacy on his behalf was read and Malatesta, acting in the name of Gregory XII, pronounced the resignation of the papacy by Gregory XII and handed a written copy of the resignation to the assembly.
Former Pope Gregory XII was then created titular Cardinal Bishop of Porto and Santa Ruffina by the Council, with rank immediately below the Pope (which made him the highest-ranking person in the Church, since, due to his abdication, the See of Peter was vacant). Gregory XII's cardinals were accepted as true cardinals by the Council, but the members of the council delayed electing a new pope for fear that a new pope would restrict further discussion of pressing issues in the Church.
By the time the anti-popes were all deposed and the new Pope, Martin V, was elected, two years had passed since Gregory XII's abdication, and Gregory was already dead. The council took great care to protect the legitimacy of the succession, ratified all his acts and a new pontiff was chosen. The new pope, Martin V, elected November 1417, soon asserted the absolute authority of the papal office.
Condemnation of Jan Hus.
A second goal of the council was to continue the reforms begun at the Council of Pisa. These reforms were largely directed against John Wycliffe, mentioned in the opening session, and condemned in the eighth, 4 May 1415 and Jan Hus, and their followers. Jan Hus, summoned to Constance under a letter of safe conduct, was found guilty of heresy by the council and turned over to the secular court. " This holy synod of Constance, seeing that God's church has nothing more that it can do, relinquishes Jan Hus to the judgment of the secular authority and decrees that he is to be relinquished to the secular court." (Council of Constance-Session 15—6 July 1415). The secular court sentenced him to the stake.<br>
Jerome of Prague, a supporter of Jan Hus, came to Constance, to offer assistance. But he was similarly arrested, judged, found guilty of heresy and turned over to the same secular court, with the same outcome as Hus. Poggio Bracciolini attended the Council and related the unfairness of the process against Jerome.
Polish–Lithuanian–Teutonic conflict.
In 1411, the First Peace of Thorn ended the Polish–Lithuanian–Teutonic War, in which the Teutonic Knights fought the Kingdom of Poland and Grand Duchy of Lithuania. However, the peace was not stable and further conflicts arose regarding demarcation of the Samogitian borders. The tensions erupted into the brief Hunger War in summer 1414. It was concluded that the disputes would be mediated by the Council of Constance. The propaganda war soon grew from a border quarrel to a fundamental dispute of the Teutonic mission – did the Knights have the right to wage the crusade? was it a Just War?
The Polish position was defended by Paulus Vladimiri, rector of the Jagiellonian University, who challenged legality of the Teutonic crusade. He argued that a forced conversion was incompatible with free will, which was an essential component of a genuine conversion. Therefore the Knights could only wage a defensive war if pagans violated natural rights of the Christians. Vladimiri further stipulated that infidels had rights which had to be respected, and neither the Pope nor the Holy Roman Emperor had the authority to violate them. Poles and Lithuanians also brought a group of Samogitian representatives to testify of atrocities committed by the Knights.
John of Falkenberg proved to be the fiercest opponent of the Poles. In his "Liber de doctrina", Falkenberg argued that "the Emperor has the right to slay even peaceful infidels simply because they are pagans (...). The Poles deserve death for defending infidels, and should be exterminated even more than the infidels; they should be deprived of their sovereignty and reduced to slavery." In "Satira", he attacked Polish King Jogaila, calling him a "mad dog" unworthy to be king. Falkenberg was condemned and imprisoned for such libel, but was not officially accused of heresy. Other opponents included Grand Master's proctor Peter Wormditt, Dominic of San Gimignano, John Urbach, Ardecino de Porta of Novara, and Bishop of Ciudad Rodrigo Andrew Escobar. They argued that the Knights were perfectly justified in their crusade as it was a sacred duty of Christians to spread the true faith. Cardinal Pierre d'Ailly published an independent opinion that attempted to somewhat balance both Polish and Teutonic positions.
The Council did not make any political decisions. It established Diocese of Samogitia, with seat in Medininkai and subordinated to Lithuanian dioceses, and appointed Matthias of Trakai as the first bishop. Pope Martin V appointed Polish King Jogaila and Lithuanian Grand Duke Vytautas as vicars general in Pskov and Veliky Novgorod in recognition of their Catholicism. After another round of futile negotiations, Gollub War broke out in 1422. It ended with the Treaty of Melno. Polish-Teutonic wars continued for another hundred years.

</doc>
<doc id="7662" url="http://en.wikipedia.org/wiki?curid=7662" title="Churches Uniting in Christ">
Churches Uniting in Christ

Churches Uniting in Christ (CUIC) is an ecumenical organization that brings together ten mainline American denominations (including both predominantly white and predominantly black churches), and was inaugurated on January 20, 2002 in Memphis, Tennessee on the balcony of the Lorraine Motel. It is the successor organization to the Consultation on Church Union.
History.
Origins.
CUIC is the successor organization to the Consultation on Church Union (COCU), which had been founded in 1962. The original task of COCU was to negotiate a consensus between its nine (originally four) member communions (it also included three "advisory participant" churches). However, it never succeeded in this goal, despite making progress on several ecumenical fronts. At COCU's 18th plenary meeting in St. Louis, Missouri (January 1999), CUIC was proposed as a new relationship among the nine member communions. Each member communion voted to join CUIC over the next few years.
Inauguration.
Heads of communion from each member of COCU (as well as the ELCA, a partner in mission and dialogue) inaugurated the group on the day before Martin Luther King, Jr. Day in 2002 at the motel where he was killed. This particular location highlighted the group's focus on racism as a major dividing factor between and among churches.
Task Forces.
The Coordinating Council of CUIC created three task forces: Ministry, Racial Justice, and Local and Regional Ecumenism. Each task force represented an important part of early CUIC work. Local ecumenical liturgies were encouraged, and excitement initially built around "pilot programs" in Denver, Los Angeles, and Memphis. The Racial Justice task force created gatherings and discussions on racial justice. The Ministry task force received much of the attention from church structures, however. The group had been given a mandate to complete work on reconciliation by 2007, and in 2003 began working on a document entitled "Mutual Recognition and Mutual Reconciliation of Ministries."
Mutual Recognition and Mutual Reconciliation of Ministries (MRMRM).
One of the most difficult issues concerning recognition and reconciliation of ministries was that of the historic episcopate. This was one of the issues that defeated proposals for union by COCU as well. The group approached this problem through dialogue, soliciting information from each member communion on the particularities of their theology and ecclesiology in order to come to a mutually acceptable conclusion.
CUIC released the seventh and final draft of the MRMRM document in June 2005. Much work was done in 2006 on this document, which focused on "Episkope," the oversight of ministry. The work culminated in a consultation on episkope in St. Louis in October 2006 involving the heads of communion of the members of CUIC. At this consultation, the MRMRM document was met with resistance, and concern was raised in particular that CUIC was focusing too narrowly on reconciliation of ministries and "not taking seriously our commitment to working on those issues of systemic racism that remain at the heart of our continuing and separated life as churches here in the United States."
Moravian Church (Northern Province).
The nine churches which inaugurated CUIC in 2002 were joined by the Moravian Church, Northern Province. The Moravians had been partners in mission and dialogue since 2002, but joined as a member communion after the October 2006 consultation on Episkope.
Suspension of Activities.
In 2007, the African Methodist Episcopal Zion Church and the African Methodist Episcopal Church withdrew from CUIC. Neither body sent representatives to the CUIC plenary on January 11–14, 2008, though the AME Council of Bishops never voted to suspend membership officially. They felt the other churches were not doing enough to counter the history of racial injustice between black and white churches. In response to this, the remaining churches in CUIC decided in 2008 to suspend their work while they seek reconciliation with these churches. This work began with a group of representatives who revisited the 1999 document "Call to Christian Commitment and Action to Combat Racism," which is available on the current CUIC website. This also meant eliminating the position of Director as well as the suspension of the work of the CUIC task forces. As of 2012, CUIC no longer has physical offices, opting instead for a virtual office and storing the archives of both CUIC and COCU at Princeton Seminary's Henry Luce III Library.
Reconciliation Efforts.
The African Methodist Episcopal Church resumed its participation by the February 2010 plenary meeting, where CUIC moved to refocus on its eight marks of commitment and a shared concern for racial justice as a major dividing factor facing ecumenism. Although the African Methodist Episcopal Zion Church has not rejoined the group, efforts have continued to bring this communion back into membership. The Rev. Staccato Powell, an AMEZ pastor, preached at the 2011 CUIC plenary in Ft. Lauderdale, Florida as a part of these reconciliation efforts. Combating racism has again become a priority of CUIC. Concerns over the historic episcopate have been sidelined since 2008, though they may re-emerge. The group's focus on mutual reconciliation of ministries has been revisited in the light of racism and the impact that racism may have on exchanging ministers between denominations. Therefore, the coordinating council of CUIC created a consultation on race and ministry while also choosing to partner with the Samuel Dewitt Proctor Conference, a social justice organization involved in African American faith communities.
Purpose.
The purpose of CUIC has always been unity (as reflected in their current slogan, "reconciling the baptized, seeking unity with justice"). This reflects one of the core scripture passages in the ecumenical movement, Jesus' prayer in John 17:21, "That they all may be one". CUIC has approached this goal of unity in various ways throughout its history.
Racism.
Racism has been a primary focus of CUIC since 2002 (and, indeed, a primary focus of COCU alongside other forms of exclusion and prejudice, such as sexism and ableism). According to Dan Krutz, former president of CUIC, "Overcoming racism has been a focal point of CUIC since its beginning... Racism may be the biggest sin that divides churches." Even before the absence of the AME and AMEZ churches at the January 2011 plenary, some in CUIC had noticed the lack of commitment to racial reconciliation. Since 2008, however, racism has become an even more pressing concern. This has led CUIC to address issues of racism in the public sphere, including the killing of Trayvon Martin and the recovery from the 2010 Haiti Earthquake.
Marks of Commitment.
According to their website, one of the reasons for transitioning from COCU to CUIC is so that member churches "stop 'consulting' and start living their unity in Christ more fully." This means that each member communion in CUIC agrees to abide by the eight Marks of Commitment, which are summarized as follows:

</doc>
<doc id="7663" url="http://en.wikipedia.org/wiki?curid=7663" title="Canadian Unitarian Council">
Canadian Unitarian Council

The Canadian Unitarian Council (CUC) is the national body for Unitarian Universalists in Canada.
The CUC is a member of the International Council of Unitarians and Universalists.
Principles and sources.
Source information is "The Principles and Sources of Our Religious Faith" from the Canadian Unitarian Council 
Principles.
Member congregations of the Canadian Unitarian Council, commit to support and promote:
Sources.
They view their tradition as living in the sense that, like language, it naturally adapts over time. They identify, but do not limit themselves to, the following foundations and resources which are both a part of their historical tradition and resources as they grow:
Individuals are not expected to, themselves, ascribe to all these traditions. Instead, as a congregation, they support every individual's search for truth and faith. At the same time each individual is expected to, more than tolerate, but to accept and celebrate the diversity of beliefs within their community. The same is true between congregations within the CUC which support the diverse nature and growth of other congregations.
The current are based on the UUA's . The CUC had a whose mandate was to consider revising them.
Organization.
The CUC is made up of 46 member congregations and emerging groups which are divided into four regions: "BC" (British Columbia), "Western" (Alberta to Thunder Bay), "Central" (between Thunder Bay and Kingston), and "Eastern" (Kingston, Ottawa and everything east of that). However, for youth ministry in Canada, the "Central" and "Eastern" regions are combined to form a youth region known as "QuOM" (Quebec, Ontario and the Maritimes), giving the youth only three regions for their activities.
Member congregations and emerging groups are served by volunteer Service Consultants, Congregational Networkers, and a series of other committees. There are two directors of regional services, one for the Western two regions, and one for the Eastern two regions. The Director of Lifespan Learning oversees development of religious exploration programming.
Policies and business of the CUC are determined at the Annual Conference and Meeting (ACM), consisting of the Annual Conference, in which workshops are held, and the Annual General Meeting, in which business matters and plenary meetings are performed. The keynote address of the ACM is the Confluence Lecture, which is comparable to the UUA's Ware Lecture in prestige. In early days this event simply consisted of the Annual General Meeting component as the Annual Conference component was not added to much later. Past ACMs have been held in the following locations:
^Not an ACM, but an "Annual General Meeting" and "Symposium", and unlike ACMs it was organized by the CUC and the Unitarian Universalist Ministers of Canad instead of a local congregation.<br>
Founding.
This section quoted from by Rev. Dr. Charles W. Eddis:
The formation of the CUC was a long-held dream. Proposals to form a Canadian organization were made by G.C. Holland, minister of the Ottawa church, in 1898, Samuel A. Eliot, President of the American Unitarian Association in 1908, Charles Huntingdon Pennoyer, minister of the Halifax Universalist Church in 1909, and Horace Westwood, a Unitarian minister in Winnipeg in 1913. In 1946 The Commission on the Work of the Churches of the British Unitarians recommended that “the Assembly should interest itself in the formation of a Canadian Unitarian Association which many Unitarians there believe to be necessary.”
The first native seeds were planted with the publication of The Canadian Unitarian in Ottawa from 1940 to 1946, a small newsletter distributed with the newsletters of Canadian churches. After the Second World War, the growth of the Unitarians in Canada began to show the strength which would make some Canadian organization feasible, if not imperative. Unitarians, most notably Toronto ministers, generated considerable media attention from the centre of Canada’s English-language media. The Unitarian Service Committee of Canada, founded in 1945, was receiving considerable attention both in city newspapers and on television, so much so that the word “Unitarian” became a household world, though its meaning was not that widely known. In 1946 there were six Icelandic Unitarian churches with 272 members, and five English-speaking churches with 1,049 members. The Universalists had five churches with 459 members. In 1961 there were three Universalist churches with 68 members, and three Icelandic and eleven English-speaking Unitarian churches with 3,476 members, and in addition 22 Unitarian fellowships with 773 members. The Universalists almost disappeared in Canada, outside of a small rural church in southwest Ontario, and were probably saved in the other two surviving locations by influx of Canadian Unitarians. By contrast, Unitarian membership more than tripled in the same fifteen years. In 1953 there were six Unitarian ministers serving congregations in Canada. Ten years later there were five ministers in the Toronto area alone.
In early April, 1961, a meeting with delegates from ten congregations was held in Montreal. The plan was approved 8 to 1, with the understanding that “The Council will function within the framework of the continental Unitarian Universalist Association.”
Relationship to the Unitarian Universalist Association.
Up until July 2002, almost all member congregations of the CUC were also members of the Unitarian Universalist Association (UUA). In the past, most services to CUC member congregations were provided by the UUA. However, with an agreement in 2001 between the UUA and CUC, from July 2002 onwards most services have been provided by the CUC to its own member congregations.
The Canadian Unitarian Universalist youth of the day disapproved of this change in relationship. It is quite evident in the words of this statement, which was adopted by the attendees of the 2001 youth conference held at the Unitarian Church of Montreal: "We the youth of Canada are deeply concerned about the direction the CUC seems to be taking. As stewards of our faith, adults have a responsibility to take into consideration the concerns of youth. We are opposed to making this massive jump in our evolutionary progress."
The UUA continues to provide services relating to ministerial settlement as well as a very small amount of the youth (14–20) and young adult (18–35) programming and services.
Unitarians and Universalists.
While the name of the organization is the Canadian Unitarian Council, the CUC includes congregations with Unitarian, Universalist, Unitarian Universalist and Universalist Unitarian in their names. Changing the name of the CUC has occasionally been debated, but there have been no successful motions. To recognize the diversity, the abbreviation is often written as U*U (and playfully read as "You star, you"). Note, not all CUC members like this playful reading and so when these people write the abbreviation they leave out the star(*), just writing UU instead.

</doc>
<doc id="7668" url="http://en.wikipedia.org/wiki?curid=7668" title="Charles Mingus">
Charles Mingus

Charles Mingus Jr. (April 22, 1922 – January 5, 1979) was a highly influential American jazz double bassist, composer and bandleader. Mingus's compositions retained the hot and soulful feel of hard bop and drew heavily from black gospel music while sometimes drawing on elements of Third Stream, free jazz, and classical music. Yet Mingus avoided categorization, forging his own brand of music that fused tradition with unique and unexplored realms of jazz. He once cited Duke Ellington and church as his main influences.
Mingus focused on collective improvisation, similar to the old New Orleans jazz parades, paying particular attention to how each band member interacted with the group as a whole. In creating his bands, he looked not only at the skills of the available musicians, but also their personalities. Many musicians passed through his bands and later went on to impressive careers. He recruited talented and sometimes little-known artists, whom he utilized to assemble unconventional instrumental configurations. As a performer, Mingus was a pioneer in double bass technique, widely recognized as one of the instrument's most proficient players.
Nearly as well known as his ambitious music was Mingus's often fearsome temperament, which earned him the nickname "The Angry Man of Jazz". His refusal to compromise his musical integrity led to many onstage eruptions, exhortations to musicians, and dismissals. Because of his brilliant writing for midsize ensembles, and his catering to and emphasizing the strengths of the musicians in his groups, Mingus is often considered the heir of Duke Ellington, for whom he expressed great admiration. Indeed, Dizzy Gillespie had once claimed Mingus reminded him "of a young Duke", citing their shared "organizational genius".
Mingus' compositions continue to be played by contemporary musicians ranging from the repertory bands Mingus Big Band, Mingus Dynasty, and Mingus Orchestra, to the high school students who play the charts and compete in the Charles Mingus High School Competition.
Gunther Schuller has suggested that Mingus should be ranked among the most important American composers, jazz or otherwise. In 1988, a grant from the National Endowment for the Arts made possible the cataloging of Mingus compositions, which were then donated to the Music Division of the New York Public Library for public use. In 1993, The Library of Congress acquired Mingus's collected papers—including scores, sound recordings, correspondence and photos—in what they described as "the most important acquisition of a manuscript collection relating to jazz in the Library's history".
Biography.
Early life and career.
Charles Mingus was born in Nogales, Arizona. He was raised largely in the Watts area of Los Angeles. His mother's heritage was Chinese and English, while historical records indicate that his father was the illegitimate offspring of a black farmhand and his Swedish employer's white granddaughter. In Mingus's autobiography "Beneath the Underdog" he recounts a story told to him by his father, Charles Mingus Sr., according to which his white grandmother was actually a first cousin of Abraham Lincoln. Charles Mingus Sr. claims to have been raised by his mother and her husband as a white person until he was fourteen, when his mother revealed to her family that the child's true father was a black slave, after which he had to run away from his family and live on his own. The autobiography doesn't confirm whether Charles Mingus Sr. or Mingus himself believed this story was true, or whether it was merely an embellished version of the Mingus family's lineage. Mingus was a nephew of Fess Williams.
His mother allowed only church-related music in their home, but Mingus developed an early love for other music, especially Duke Ellington. He studied trombone, and later cello, although he was unable to follow the cello professionally because, at the time, it was nearly impossible for a black musician to make a career of classical music, and the cello was not yet accepted as a jazz instrument. Despite this, Mingus was still attached to the cello; as he studied bass with Red Callender in the late 1930s, Callender even commented that the cello was still Mingus's main instrument. In "Beneath the Underdog", Mingus states that he did not actually start learning bass until Buddy Collette accepted him into his swing band under the stipulation that he be the band's bass player.
Due to a poor education, the young Mingus could not read musical notation quickly enough to join the local youth orchestra. This had a serious impact on his early musical experiences, leaving him feeling ostracized from the classical music world. These early experiences, in addition to his lifelong confrontations with racism, were reflected in his music, which often focused on themes of racism, discrimination and justice. Much of the cello technique he learned was applicable to double bass when he took up the instrument in high school. He studied for five years with Herman Reinshagen, principal bassist of the New York Philharmonic, and compositional techniques with Lloyd Reese. Throughout much of his career, he played a bass made in 1927 by the German maker Ernst Heinrich Roth.
Beginning in his teen years, Mingus was writing quite advanced pieces; many are similar to Third Stream because they incorporate elements of classical music. A number of them were recorded in 1960 with conductor Gunther Schuller, and released as "Pre-Bird", referring to Charlie "Bird" Parker; Mingus was one of many musicians whose perspectives on music were altered by Parker into "pre- and post-Bird" eras.
Mingus gained a reputation as a bass prodigy. His first major professional job was playing with former Ellington clarinetist Barney Bigard. He toured with Louis Armstrong in 1943, and by early 1945 was recording in Los Angeles in a band led by Russell Jacquet, which also included Teddy Edwards, Maurice Simon, Bill Davis, and Chico Hamilton, and in May that year, in Hollywood, again with Teddy Edwards, in a band led by Howard McGhee. He then played with Lionel Hampton's band in the late 1940s; Hampton performed and recorded several of Mingus's pieces. A popular trio of Mingus, Red Norvo and Tal Farlow in 1950 and 1951 received considerable acclaim, but Mingus's race caused problems with club owners and he left the group. Mingus was briefly a member of Ellington's band in 1953, as a substitute for bassist Wendell Marshall. Mingus's notorious temper led to him being one of the few musicians personally fired by Ellington (Bubber Miley and drummer Bobby Durham are among the others), after an on-stage fight between Mingus and Juan Tizol.
Also in the early 1950s, before attaining commercial recognition as a bandleader, Mingus played gigs with Charlie Parker, whose compositions and improvisations greatly inspired and influenced him. Mingus considered Parker the greatest genius and innovator in jazz history, but he had a love-hate relationship with Parker's legacy. Mingus blamed the Parker mythology for a derivative crop of pretenders to Parker's throne. He was also conflicted and sometimes disgusted by Parker's self-destructive habits and the romanticized lure of drug addiction they offered to other jazz musicians. In response to the many sax players who imitated Parker, Mingus titled a song, "If Charlie Parker were a Gunslinger, There'd be a Whole Lot of Dead Copycats" (released on "Mingus Dynasty" as "Gunslinging Bird").
Based in New York.
In 1952 Mingus co-founded Debut Records with Max Roach so he could conduct his recording career as he saw fit. The name originated from his desire to document unrecorded young musicians. Despite this, the best-known recording the company issued was of the most prominent figures in bebop. On May 15, 1953, Mingus joined Dizzy Gillespie, Parker, Bud Powell, and Roach for a concert at Massey Hall in Toronto, which is the last recorded documentation of the two lead instrumentalists playing together. After the event, Mingus chose to overdub his barely audible bass part back in New York; the original version was issued later. The two 10" albums of the Massey Hall concert (one featured the trio of Powell, Mingus and Roach) were among Debut Records' earliest releases. Mingus may have objected to the way the major record companies treated musicians, but Gillespie once commented that he did not receive any royalties "for years and years" for his Massey Hall appearance. The records though, are often regarded as among the finest live jazz recordings.
One story, has it that Mingus was involved in a notorious incident while playing a 1955 club date billed as a "reunion" with Parker, Powell, and Roach. Powell, who suffered from alcoholism and mental illness (possibly exacerbated by a severe police beating and electroshock treatments), had to be helped from the stage, unable to play or speak coherently. As Powell's incapacitation became apparent, Parker stood in one spot at a microphone, chanting "Bud Powell...Bud Powell..." as if beseeching Powell's return. Allegedly, Parker continued this incantation for several minutes after Powell's departure, to his own amusement and Mingus's exasperation. Mingus took another microphone and announced to the crowd, "Ladies and Gentleman, please don't associate me with any of this. This is not jazz. These are sick people." This was Parker's last public performance; about a week later he died after years of substance abuse.
Mingus often worked with a mid-sized ensemble (around 8–10 members) of rotating musicians known as the Jazz Workshop. Mingus broke new ground, constantly demanding that his musicians be able to explore and develop their perceptions on the spot. Those who joined the Workshop (or Sweatshops as they were colorfully dubbed by the musicians) included Pepper Adams, Jaki Byard, Booker Ervin, John Handy, Jimmy Knepper, Charles McPherson and Horace Parlan. Mingus shaped these musicians into a cohesive improvisational machine that in many ways anticipated free jazz. Some musicians dubbed the workshop a "university" for jazz.
"Pithecanthropus Erectus" among other creations.
The decade that followed is generally regarded as Mingus's most productive and fertile period. Impressive new compositions and albums appeared at an astonishing rate: some "thirty" records in ten years, for a number of record labels (Atlantic Records, Candid, Columbia Records, Impulse! Records and others), a pace perhaps unmatched by any other musicians except Ellington. 
Mingus had already recorded around ten albums as a bandleader, but 1956 was a breakthrough year for him, with the release of "Pithecanthropus Erectus", arguably his first major work as both a bandleader and composer. Like Ellington, Mingus wrote songs with specific musicians in mind, and his band for "Erectus" included adventurous musicians: piano player Mal Waldron, alto saxophonist Jackie McLean and the Sonny Rollins-influenced tenor of J. R. Monterose. The title song is a ten-minute tone poem, depicting the rise of man from his hominid roots ("Pithecanthropus erectus") to an eventual downfall. A section of the piece was free improvisation, free of structure or theme.
Another album from this period, "The Clown" (1957 also on Atlantic Records), the title track of which features narration by humorist Jean Shepherd, was the first to feature drummer Dannie Richmond, who remained his preferred drummer until Mingus's death in 1979. The two men formed one of the most impressive and versatile rhythm sections in jazz. Both were accomplished performers seeking to stretch the boundaries of their music while staying true to its roots. When joined by pianist Jaki Byard, they were dubbed "The Almighty Three".
"Mingus Ah Um" and other works.
In 1959 Mingus and his jazz workshop musicians recorded one of his best-known albums, "Mingus Ah Um". Even in a year of standout masterpieces, including Dave Brubeck's "Time Out", Miles Davis's "Kind of Blue", and Ornette Coleman's prophetic "The Shape of Jazz to Come", this was a major achievement, featuring such classic Mingus compositions as "Goodbye Pork Pie Hat" (an elegy to Lester Young) and the vocal-less version of "Fables of Faubus" (a protest against segregationist Arkansas governor Orval E. Faubus that features double-time sections). Also during 1959, Mingus recorded the album "Blues & Roots", which was released the following year. As Mingus explained in his liner notes: "I was born swinging and clapped my hands in church as a little boy, but I've grown up and I like to do things other than just swing. But blues can do more than just swing."
Mingus witnessed Ornette Coleman's legendary—and controversial—1960 appearances at New York City's Five Spot jazz club. He initially expressed rather mixed feelings for Coleman's innovative music: "...if the free-form guys could play the same tune twice, then I would say they were playing something...Most of the time they use their fingers on the saxophone and they don't even know what's going to come out. They're experimenting." That same year, however, Mingus formed a quartet with Richmond, trumpeter Ted Curson and multi-instrumentalist Eric Dolphy. This ensemble featured the same instruments as Coleman's quartet, and is often regarded as Mingus rising to the challenging new standard established by Coleman. "Charles Mingus Presents Charles Mingus" was the quartet's only album. This album also features the version of "Fables of Faubus" with lyrics, aptly titled "Original Faubus Fables".
Only one misstep occurred in this era: 1962's "Town Hall Concert". An ambitious program, it was plagued with troubles from its inception. Mingus's vision, now known as "Epitaph", was finally realized by conductor Gunther Schuller in a concert in 1989, 10 years after Mingus's death.
"The Black Saint and the Sinner Lady" and other Impulse! albums.
In 1963, Mingus released "The Black Saint and the Sinner Lady", a sprawling, multi-section masterpiece, described as "one of the greatest achievements in orchestration by any composer in jazz history." The album was also unique in that Mingus asked his psychotherapist to provide notes for the record.
Mingus also released "Mingus Plays Piano", an unaccompanied album featuring some fully improvised pieces, in 1963.
In addition, 1963 saw the release of "Mingus Mingus Mingus Mingus Mingus", an album praised by critic Nat Hentoff.
In 1964 Mingus put together one of his best-known groups, a sextet including Dannie Richmond, Jaki Byard, Eric Dolphy, trumpeter Johnny Coles, and tenor saxophonist Clifford Jordan. The group was recorded frequently during its short existence; Coles fell ill and left during a European tour. Dolphy stayed in Europe after the tour ended, and died suddenly in Berlin on June 28, 1964. 1964 was also the year that Mingus met his future wife, Sue Graham Ungaro. The couple were married in 1966 by Allen Ginsberg. Facing financial hardship, Mingus was evicted from his New York home in 1966.
"Changes".
Mingus's pace slowed somewhat in the late 1960s and early 1970s. In 1974 he formed a quintet with Richmond, pianist Don Pullen, trumpeter Jack Walrath and saxophonist George Adams. They recorded two well-received albums, "Changes One" and "Changes Two". Mingus also played with Charles McPherson in many of his groups during this time. "Cumbia and Jazz Fusion" in 1976 sought to blend Colombian music (the "Cumbia" of the title) with more traditional jazz forms. In 1971, Mingus taught for a semester at the University at Buffalo, The State University of New York as the Slee Professor of Music.
Later career and death.
By the mid-1970s, Mingus was suffering from amyotrophic lateral sclerosis (ALS). His once formidable bass technique suffered, until he could no longer play the instrument. He continued composing, however, and supervised a number of recordings before his death. At the time of his death, Mingus was working on an album named after him with Joni Mitchell, which included lyrics added by Mitchell to Mingus compositions, including "Goodbye Pork Pie Hat". The album featured the talents of Wayne Shorter, Herbie Hancock, and another influential bassist and composer, Jaco Pastorius.
Mingus died, aged 56, in Cuernavaca, Mexico, where he had traveled for treatment and convalescence. His ashes were scattered in the Ganges River.
Personality and temper.
As respected as Mingus was for his musical talents, he was sometimes feared for his occasional violent onstage temper, which was at times directed at members of his band, and other times aimed at the audience. He was physically large, prone to obesity (especially in his later years), and was by all accounts often intimidating and frightening when expressing anger or displeasure. Mingus was prone to clinical depression. He tended to have brief periods of extreme creative activity, intermixed with fairly long periods of greatly decreased output.
When confronted with a nightclub audience talking and clinking ice in their glasses while he performed, Mingus stopped his band and loudly chastised the audience, stating "Isaac Stern doesn't have to put up with this shit." Mingus reportedly destroyed a $20,000 bass in response to audience heckling at New York's Five Spot.
Guitarist and singer Jackie Paris was a first-hand witness to Mingus's irascibility. Paris recalls his time in the Jazz Workshop: "He chased everybody off the stand except [drummer] Paul Motian and me... The three of us just wailed on the blues for about an hour and a half before he called the other cats back."
On October 12, 1962, Mingus punched Jimmy Knepper in the mouth while the two men were working together at Mingus's apartment on a score for his upcoming concert at New York Town Hall and Knepper refused to take on more work. The blow from Mingus broke off a crowned tooth and its underlying stub. According to Knepper, this ruined his embouchure and resulted in the permanent loss of the top octave of his range on the trombone – a significant handicap for any professional trombonist. This attack temporarily ended their working relationship and Knepper was unable to perform at the concert. Charged with assault, Mingus appeared in court in January 1963 and was given a suspended sentence. Knepper did again work with Mingus in 1977 and played extensively with the Mingus Dynasty, formed after Mingus's death in 1979.
In 1966, Mingus was evicted from his apartment at 5 Great Jones Street in New York City for nonpayment of rent, captured in the 1968 documentary film "", directed by Thomas Reichman. The film also features Mingus performing in clubs and in the apartment, firing a shotgun, composing at the piano, playing with and taking care of his young daughter Caroline, and discussing love, art, politics, and the music school he had hoped to create.
Legacy.
The Mingus Big Band.
The music of Charles Mingus is currently being performed and reinterpreted by the Mingus Big Band, which, starting October 2008, plays every Monday at Jazz Standard in New York City, and often tours the rest of the U.S. and Europe. Elvis Costello has written lyrics for a few Mingus pieces. He had once sung lyrics for one piece, "Invisible Lady", being backed by the Mingus Big Band on the album, "Tonight at Noon: Three of Four Shades of Love".
In addition to the Mingus Big Band, there is the Mingus Orchestra and the Mingus Dynasty, each of which are managed by Jazz Workshop, Inc., and run by Mingus's widow Sue Graham Mingus.
"Epitaph".
"Epitaph" is considered one of Charles Mingus's masterpieces. The composition is 4,235 measures long, requires two hours to perform, and is one of the longest jazz pieces ever written. "Epitaph" was only completely discovered during the cataloging process after his death, by musicologist Andrew Homzy. With the help of a grant from the Ford Foundation, the score and instrumental parts were copied, and the piece itself was premiered by a 30-piece orchestra, conducted by Gunther Schuller. This concert was produced by Mingus's widow, Sue Graham Mingus, at Alice Tully Hall on June 3, 1989, ten years after his death. It was performed again at several concerts in 2007. The performance at Walt Disney Concert Hall is available on . The complete score was published in 2008 by Hal Leonard.
Autobiography.
Mingus wrote the sprawling, exaggerated, quasi-autobiography, "Beneath the Underdog: His World as Composed by Mingus", throughout the 1960s, and it was published in 1971. Its "stream of consciousness" style covered several aspects of Mingus's life that had previously been off-record. In addition to his musical and intellectual proliferation, Mingus goes into great detail about his perhaps overstated sexual exploits. He claims to have had over 31 affairs over the course of his life (including 26 prostitutes in one sitting). This does not include any of his five wives (he claims to have been married to two of them simultaneously). In addition, he asserts that he held a brief career as a pimp. This has never been confirmed.
Mingus's autobiography also serves as an insight into his psyche, as well as his attitudes about race and society. Autobiographic accounts of abuse at the hands of his father from an early age, being bullied as a child, his removal from a white musician's union, and grappling with disapproval while married to white women and other examples of the hardship and prejudice.
Cover versions.
Considering the number of compositions that Charles Mingus wrote, his works have not been recorded as often as comparable jazz composers. The only Mingus tribute albums recorded during his lifetime were baritone saxophonist Pepper Adams's album, "Pepper Adams Plays Charlie Mingus", in 1963, and Joni Mitchell's album Mingus, in 1979. Of all his works, his elegant elegy for Lester Young, "Goodbye Pork Pie Hat" (from "Mingus Ah Um") has probably had the most recordings. Besides recordings from the expected jazz artists, the song has also been recorded by musicians as disparate as Jeff Beck, Andy Summers, Eugene Chadbourne, and Bert Jansch and John Renbourn with and without Pentangle. Joni Mitchell sang a version with lyrics that she wrote for the song.
Elvis Costello has recorded "Hora Decubitus" (from "Mingus Mingus Mingus Mingus Mingus") on "My Flame Burns Blue" (2006). "Better Git It in Your Soul" was covered by Davey Graham on his album "Folk, Blues, and Beyond." Trumpeter Ron Miles performs a version of "Pithecanthropus Erectus" on his EP "Witness." New York Ska Jazz Ensemble has done a cover of Mingus's "Haitian Fight Song", as have Pentangle and others. Hal Willner's 1992 tribute album "Weird Nightmare: Meditations on Mingus" (Columbia Records) contains idiosyncratic renditions of Mingus's works involving numerous popular musicians including Chuck D, Keith Richards, Henry Rollins and Dr. John. The Italian band Quintorigo recorded an entire album devoted to Mingus's music, titled "Play Mingus".
Gunther Schuller's edition of Mingus's "Epitaph" which premiered at Lincoln Center in 1989 was subsequently released on Columbia/Sony Records.
One of the ultimate tributes to Mingus came on September 29, 1969 at a festival honoring him. Duke Ellington performed "The Clown" at the festival. Duke himself did Jean Shepherd's narration. As of this date, this recording has not been issued.

</doc>
<doc id="7669" url="http://en.wikipedia.org/wiki?curid=7669" title="Centimetre">
Centimetre

A centimetre (international spelling as used by the International Bureau of Weights and Measures; symbol cm) or centimeter (American spelling) is a unit of length in the metric system, equal to one hundredth of a metre, "centi" being the SI prefix for a factor of . The centimetre was the base unit of length in the now deprecated centimetre-gram-second (CGS) system of units.
Though for many physical quantities, SI prefixes for factors of 103—like "milli-" and "kilo-"—are often preferred by technicians, the centimetre remains a practical unit of length for many everyday measurements. A centimetre is approximately the width of the fingernail of an average adult person.
Equivalence to other units of length.
One cubic centimetre is equal to 1 millilitre, under the current SI system of units.
Uses of centimetre.
In addition to its use in the measurement of length, the centimetre is used:
Unicode symbols.
For the purposes of compatibility with Chinese, Japanese and Korean (CJK) characters, Unicode has symbols for:
They are mostly used only with East Asian fixed-width CJK fonts, because they are equal in size to one Chinese character.

</doc>
<doc id="7670" url="http://en.wikipedia.org/wiki?curid=7670" title="Central Coast">
Central Coast

The place designation Central Coast may refer to the following:

</doc>
<doc id="7671" url="http://en.wikipedia.org/wiki?curid=7671" title="Committee on Data for Science and Technology">
Committee on Data for Science and Technology

The Committee on Data for Science and Technology (CODATA) was established in 1966 as an interdisciplinary committee of the International Council for Science. It seeks to improve the compilation, critical evaluation, storage, and retrieval of data of importance to science and technology.
The CODATA Task Group on Fundamental Constants was established in 1969. Its purpose is to periodically provide the international scientific and technological communities with an internationally accepted set of values of the fundamental physical constants and closely related conversion factors for use worldwide. The first such CODATA set was published in 1973, later in 1986, 1998, 2002 and the fifth in 2006. The latest version is Version 6.0 called "2010CODATA" published on 2 June 2011.
The CODATA recommended values of fundamental physical constants are published at the NIST Reference on Constants, Units, and Uncertainty.
CODATA sponsors the every two years.

</doc>
<doc id="7672" url="http://en.wikipedia.org/wiki?curid=7672" title="Chuck Jones">
Chuck Jones

Charles Martin "Chuck" Jones (September 21, 1912 – February 22, 2002) was an American animator, cartoon artist, screenwriter, producer, and director of animated films, most memorably of "Looney Tunes" and "Merrie Melodies" shorts for the Warner Bros. Cartoons studio. He directed many classic animated cartoon shorts starring Bugs Bunny, Daffy Duck, the Road Runner and Wile E. Coyote, Pepé Le Pew, Porky Pig and a slew of other Warner characters.
After his career at Warner Bros. ended in 1962, Jones started Sib Tower 12 Productions, and began producing cartoons for Metro-Goldwyn-Mayer, including a new series of "Tom and Jerry" shorts and the television adaptation of Dr. Seuss' "How the Grinch Stole Christmas!". He later started his own studio, Chuck Jones Productions, which created several one-shot specials, and periodically worked on "Looney Tunes" related works.
Jones was nominated for an Academy Award eight times and won three times, receiving awards for the cartoons "For Scent-imental Reasons", "So Much for So Little", and "The Dot and the Line". He received an Honorary Academy Award in 1996 for his work in the animation industry. Film historian Leonard Maltin has praised Jones' work at Warner Bros., MGM and Chuck Jones Productions. He also said that the "feud" that there may have been between Jones and colleague Bob Clampett was mainly because they were so different from each other. In Jerry Beck's "The 50 Greatest Cartoons", ten of the entries were directed by Jones, with four out of the five top cartoons being Jones shorts.
Early life.
Jones was born in Spokane, Washington on September 21, 1912. He later moved with his parents and three siblings to the Los Angeles, California area.
In his autobiography, "Chuck Amuck", Jones credits his artistic bent to circumstances surrounding his father, who was an unsuccessful businessman in California in the 1920s. His father, Jones recounts, would start every new business venture by purchasing new stationery and new pencils with the company name on them. When the business failed, his father would quietly turn the huge stacks of useless stationery and pencils over to his children, requiring them to use up all the material as fast as possible. Armed with an endless supply of high-quality paper and pencils, the children drew constantly. Later, in one art school class, the professor gravely informed the students that they each had 100,000 bad drawings in them that they must first get past before they could possibly draw anything worthwhile. Jones recounted years later that this pronouncement came as a great relief to him, as he was well past the 200,000 mark, having used up all that stationery. Jones and several of his siblings went on to artistic careers.
During his artistic education, he worked part-time as a janitor. After graduating from Chouinard Art Institute, Jones got a phone call from a friend called Fred Kopietz, which had been hired by the Ub Iwerks studio and offered him a job. He worked his way up in the animation industry, starting as a cell washer; "then I moved up to become a painter in black and white, some color. Then I went on to take animator's drawings and traced them on to the celluloid. Then I became what they call an in-betweener, which is the guy that does the drawing between the drawings the animator makes". While at Iwerks, he met a cel painter named Dorothy Webster, who would later become his first wife.
Warner Bros..
"Related article: Chuck Jones filmography"
Chuck Jones joined Leon Schlesinger Productions, the independent studio that produced "Looney Tunes" and "Merrie Melodies" for Warner Bros., in 1933 as an assistant animator. In 1935, he was promoted to animator, and assigned to work with new Schlesinger director Tex Avery. There was no room for the new Avery unit in Schlesinger's small studio, so Avery, Jones, and fellow animators Bob Clampett, Virgil Ross, and Sid Sutherland were moved into a small adjacent building they dubbed "Termite Terrace". When Clampett was promoted to director in 1937, Jones was assigned to his unit; the Clampett unit was briefly assigned to work with Jones' old employer, Ub Iwerks, when Iwerks subcontracted four cartoons to Schlesinger in 1937. Jones became a director (or "supervisor", the original title for an animation director in the studio) himself in 1938 when Frank Tashlin left the studio. Jones' first cartoon was "The Night Watchman", which featured a cute kitten who would later evolve into Sniffles the mouse.
He was actively involved in efforts to unionize the staff of Leon Schlesinger Studios. He was responsible for recruiting animators, layout men, and background people. Almost all animators joined, in reaction to salary cuts imposed by Leon Schlesinger. The Metro-Goldwyn-Mayer cartoon studio had already signed a union contract, encouraging their counterparts under Schlesinger. In a meeting with his staff, Schlesinger talked for a few minutes, then turned over the meeting to his attorney. His insulting manner had a unifying effect on the staff. Jones gave a pep talk at the union headquarters. As negotiations broke down, the staff decided to go on strike. Schlesinger locked them out of the studio for a few days, before agreeing to sign the contract. A Labor Management Committee was formed and Jones served as a moderator. Because of his role as a supervisor in the studio, he could not himself join the union.
Many of Jones' cartoons of the 1930s and early 1940s were lavishly animated, but audiences and fellow Schlesinger staff members found them lacking in genuine humor. Jones' early cartoons were an attempt to follow in the footsteps of Walt Disney's shorts (especially with such cartoons as "Tom Thumb in Trouble" and the Sniffles cartoons). Jones credits "The Dover Boys" in 1942 as the film where he "learned how to be funny." "The Dover Boys" is also one of the first uses of stylized animation in American film, breaking away from the more realistic animation styles influenced by the Disney Studio. This was also the period where Jones created many of his lesser-known characters, including Charlie Dog, Hubie and Bertie, and The Three Bears.
During World War II, Jones worked closely with Theodor Geisel, better known as Dr. Seuss, to create the "Private Snafu" series of Army educational cartoons. Private Snafu comically educated soldiers on topics like spies and laziness in a more risque way than general audiences would have been used to at the time. Jones later collaborated with Seuss on animated adaptations of Seuss' books, including "How the Grinch Stole Christmas!" in 1966.
Jones directed such shorts as "The Weakly Reporter", a 1944 short that related to shortages and rationing on the home front. During the same year, he directed "Hell-Bent for Election", a campaign film for Franklin D. Roosevelt. He also directed the less-widely known Angel Puss, which is no longer available in any authorized release and is among the group of controversial cartoons known to animation buffs as the Censored Eleven.
Jones hit his stride in the late 1940s and continued to make his best-regarded works through the 1950s. Jones-created characters from this period includes Claude Cat, Marc Antony and Pussyfoot, Charlie Dog, Michigan J. Frog, and his three most popular creations, Marvin the Martian, Pepe LePew, the Road Runner, and Wile E. Coyote. Jones and writer Michael Maltese collaborated on the Road Runner cartoons, "Duck Amuck", "One Froggy Evening", and "What's Opera, Doc?". Other staff at Unit A that Jones collaborated with include layout artist, background designer, co-director Maurice Noble; animator and co-director Abe Levitow; and animators Ken Harris and Ben Washam.
In 1950, Jones and Maltese began working on "Rabbit Fire", a short that has changed Daffy Duck's personality. They decided to make him a totally different character; instead of the wacky, comic relief character he had been, they turned Daffy into a vain, egomaniacal prima donna wanting to steal the spotlight from Bugs Bunny. Of his versions of Bugs and Daffy, Chuck Jones has said, "Bugs is who we want to be. Daffy is who we are."
Jones remained at Warner Bros. throughout the 1950s, except for a brief period in 1953 when Warner closed the animation studio. During this interim, Jones found employment at Walt Disney Pictures, where he teamed with Ward Kimball for a four month period of uncredited work on "Sleeping Beauty" (1959). Upon the reopening of the Warner animation department, Jones was rehired and reunited with most of his unit.
In the early 1960s, Jones and his wife Dorothy wrote the screenplay for the animated feature "Gay Purr-ee". The finished film would feature the voices of Judy Garland, Robert Goulet and Red Buttons as cats in Paris, France. The feature was produced by UPA, and directed by his former Warner collaborator, Abe Levitow. Jones moonlighted to work on the film, since he had an exclusive contract with Warner Bros. UPA completed the film and made it available for distribution in 1962; it was picked up by Warner Bros. When Warner discovered that Jones had violated his exclusive contract with them, they terminated him. Jones' former animation unit was laid off after completing the final cartoon in their pipeline, "The Iceman Ducketh", and the rest of the Warner Bros. Cartoons studio was closed in early 1963. Jones claimed in his autobiography that this happened because Warner finally learned "they" weren't making Mickey Mouse cartoons.
Post-Warner Bros..
With business partner Les Goldman, Jones started an independent animation studio, Sib Tower 12 Productions, and brought on most of his unit from Warner Bros., including Maurice Noble and Michael Maltese. In 1963, Metro-Goldwyn-Mayer contracted with Sib Tower 12 to have Jones and his staff produce new "Tom and Jerry" cartoons as well as a television adaptation of all Tom and Jerry theatricals produced to that date. This included major editing, including writing out the African-American maid, Mammy Two-Shoes, and replacing her with one of Irish descent voiced by June Foray. In 1964, Sib Tower 12 was absorbed by MGM and was renamed MGM Animation/Visual Arts. Jones' animated short film "The Dot and the Line: A Romance in Lower Mathematics" won the 1965 Academy Award for Best Animated Short. Jones also directed the classic animated short "The Bear That Wasn't".
As the "Tom and Jerry" series wound down (it would be discontinued in 1967), Jones produced more for television. In 1966, he produced and directed the TV special "How the Grinch Stole Christmas!", featuring the voice and facial models based on the readings by Boris Karloff. Jones continued to work on other TV specials such as "Horton Hears a Who!" (1970), but his main focus during this time was producing the feature film "The Phantom Tollbooth", which did lukewarm business when MGM released it in 1970. Jones co-directed 1969's "The Pogo Special Birthday Special", based on the Walt Kelly comic strip, and voiced the characters of Porky Pine and Bun Rab. It was at this point that he decided to start 'ST Incorporated'.
MGM closed the animation division in 1970, and Jones once again started his own studio, Chuck Jones Productions. He produced a Saturday morning children's TV series for the American Broadcasting Company called "The Curiosity Shop" in 1971. In 1973, he produced an animated version of the George Selden book "The Cricket in Times Square", and would go on to produce two sequels. His most notable work during this period was three animated TV adaptations of short stories from Rudyard Kipling's "The Jungle Book": "Mowgli's Brothers", "The White Seal" and "Rikki-Tikki-Tavi". During this period, Jones began to experiment with more realistically designed characters, most of which having larger eyes, leaner bodies, and altered proportions, such as those of the Looney Tunes characters. He also focused less on slapstick and became more involved with his writing process as well. Jones resumed working with Warner Bros. in 1976 with the animated TV adaptation of "The Carnival of the Animals" with Bugs Bunny and Daffy Duck. Jones also produced the 1979 film "The Bugs Bunny/Road Runner Movie" which was a compilation of Jones' best theatrical shorts; Jones produced new Road Runner shorts for "The Electric Company" series and "Bugs Bunny's Looney Christmas Tales" (1979), and even newer shorts were made for "Bugs Bunny's Bustin' Out All Over" (1980).
From 1977–1978, Jones wrote and drew the newspaper comic strip "Crawford" (also known as "Crawford & Morgan") for the Chicago Tribune-NY News Syndicate. In 2011 IDW Publishing collected Jones' strip as part of their Library of American Comic Strips.
In 1978, Jones' wife Dorothy died; three years later, he married Marian Dern, the writer of the comic strip "Rick O'Shay".
Jones-Avery letter.
On December 11, 1975, shortly after the release of Bugs Bunny Superstar, which prominently featured Bob Clampett, Jones wrote a letter to Tex Avery, accusing Clampett of taking credit for ideas that were not his and for characters created by other directors (notably Jones's Sniffles and Friz Freleng's Yosemite Sam). Their correspondence was never published in the media; nonetheless, it was sent to Michael Barrier, who had conducted the interview with Clampett and was distributed by Jones to multiple people concerned with animation over the years.
Animator Milt Gray described this as a smear campaign by Chuck Jones. Michael Barrier claims that Clampett had given himself too much credit, but also that Jones had had ill feelings towards him ever since their days at Termite Terrace, due to the fact that Bob Clampett was made an animation director before Chuck Jones. Nonetheless most of his claims in his interview, mainly those that he was the originator of Bugs Bunny have been proven overly exaggerated and false (his claim of participation on the making of "A Wild Hare", on which he isn't credited).
Robert McKimson claimed in an interview that many animators but mostly Clampett contributed to the crazy personality of Bugs, while others like Chuck Jones concentrated more on the more calmed-down gags. As far as plagiarism is concerned, McKimson cited that the animators would always be looking at each other's sheets to see if they could borrow some punchlines and cracks.
Later years.
Through the 1980s and 1990s, Jones was painting cartoon and parody art, sold through animation galleries by his daughter's company, Linda Jones Enterprises. Jones was the creative consultant and character designer for two Raggedy Ann animated specials and the first "Alvin and the Chipmunks" Christmas special "A Chipmunk Christmas". He made a cameo appearance in the 1984 film "Gremlins" and directed the Bugs Bunny/Daffy Duck animated sequences that bookend "" (1990). In 1988, Jones contributed to the creation of London's Museum of the Moving Image by spending several days working high on scaffolding creating a chase sequence directly onto the high walls of the museum. Jones directed animated sequences for various features such as a lengthy sequence in the 1992 film "Stay Tuned" and a shorter one seen at the start of the 1993 film "Mrs. Doubtfire".
In his later years, Jones became the most vocal alumnus of the Termite Terrace studio, frequently giving lectures, seminars, and working to educate newcomers in the animation field. Jones was not a fan of much contemporary animation, terming most of it, especially television cartoons such as those of Hanna-Barbera, "illustrated radio". In the 1990s, Jones directed a few Looney Tunes-based and non-related cartoons, a notable one being 1994's "Chariots of Fur", his final Road Runner cartoon. Jones' final Looney Tunes cartoon was "From Hare to Eternity" in 1996, which starred Bugs Bunny and Yosemite Sam, with Greg Burson voicing Bugs. The cartoon was dedicated to Friz Freleng, who had passed on in 1995. Jones' final animation project was a series of 13 shorts starring a timber wolf character he had designed in the 1960s named Thomas Timber Wolf. The series was released online by Warner Bros. in 2000. From 2001 until 2004, Cartoon Network aired "The Chuck Jones Show" which features shorts directed by him. The show won the Annie Award for Outstanding Achievement in an Animated Special Project.
Death.
Jones died of heart failure on February 22, 2002. He was cremated and his ashes were scattered at sea. After his death, the Looney Tunes cartoon "Daffy Duck for President", based on the book that Jones had written and using Jones' style for the characters, originally scheduled to be released in 2000, was released in 2004 as part of of the "" DVD set.
Accolades.
Jones was a historical authority as well as a major contributor to the development of animation throughout the 20th century. He received an honorary degree from Oglethorpe University in 1993. For his contribution to the motion picture industry, Jones has a star on the Hollywood Walk of Fame at 7011 Hollywood Blvd.
Jones, whose work had been nominated eight times over his career for an Oscar (winning the award three times: "For Scent-imental Reasons", "So Much for So Little", and "The Dot and the Line"), received an Honorary Academy Award in 1996 by the Board of Governors of the Academy of Motion Picture Arts and Sciences, for "the creation of classic cartoons and cartoon characters whose animated lives have brought joy to our real ones for more than half a century." At that year's awards show, Robin Williams, a self-confessed "Jones-aholic," presented the Honorary award to Jones, calling him "The Orson Welles of cartoons.", and the audience gave Jones a standing ovation as he walked onto the stage. For himself, a flattered Jones wryly remarked in his acceptance speech, "Well, what can I say in the face of such humiliating evidence? I stand guilty before the world of directing over three hundred cartoons in the last fifty or sixty years. Hopefully this means you've forgiven me."
Jones' life and legacy were celebrated January 12, 2012, with the official grand opening of "The Chuck Jones Experience" at Circus Circus Las Vegas. Many of Jones' family welcomed celebrities, animation aficionados and visitors to the new attraction when they opened the attraction in an appropriate and unconventional way. Among those in attendance were Jones' widow, Marian Jones; daughter Linda Clough; and grandchildren Craig, Todd and Valerie Kausen.

</doc>
<doc id="7673" url="http://en.wikipedia.org/wiki?curid=7673" title="Costume">
Costume

Costume is the distinctive style of dress of a particular people, class, or period. A costume can be a particular style of clothing worn to portray the wearer as a character or type of character other than their regular persona at a social event such as a masquerade, a fancy dress party or in a theatre performance.
Major categories.
Theatrical costume.
One of the more prominent places people see costumes is in theatre, film and on television. In combination with other aspects, theatrical costumes can help actors portray characters' age, gender role, profession, social class, personality, ethnicity, and even information about the historical period/era, geographic location and time of day, as well as the season or weather of the theatrical performance. Often, stylized theatrical costumes can exaggerate some aspect of a character; for example Harlequin and Pantaloon in the Commedia dell'arte.
National costume.
National costume or regional costume expresses local (or exiled) identity and emphasises a culture's unique attributes. It is often a source of national pride. Examples of such are a Scotsman in a kilt or a Japanese person in a kimono.
Holidays and festivals.
The wearing of costumes has become an important part of such holidays and festivals as Mardi Gras and Halloween (see Halloween costume for more information), and (to a lesser extent) people may also wear costumes in conjunction with other holidays, such as Christmas and Easter. Mardi Gras costumes usually take the form of jesters and other fantasy characters, while Halloween costumes traditionally take the form of supernatural creatures such as ghosts, vampires, pop culture icons and angels. In modern times Halloween costumes sometimes also include fashion show type costumes. Christmas and Easter costumes typically portray mythical characters such as Santa Claus (by donning a santa suit and beard) or the Easter Bunny by putting on an animal costume. Costumes may serve to portray various other characters during secular holidays, such as an Uncle Sam costume worn on the Independence day for example.
In Judaism, a common practice is to dress up on Purim. The Jews celebrate the change of their destiny. They were delivered from being the victims of an evil decree against them and were instead allowed by the King to destroy their enemies. A quote from the Book of Esther, which says: "On the contrary" (hebrew: ונהפוך הוא ) is the reason that wearing a costume has become so popular among the Jews on this holiday.
Children.
Costumes also serve as an avenue for children to explore and roleplay. Children can dress up in various forms; for example characters from history or fiction like pirates, princesses or cowboys, common jobs like nurses or police officers, or animals such as those seen in zoos or farms.
Mascots.
Another very popular situation where costumes are employed are for sporting events, where people dressed as their team's representative mascot help the club or team rally round their team's cause. Animal costumes which are visually very similar to mascot costumes are also popular among the members of the furry fandom where they are referred to as fursuits that match ones animal persona, or fursona.
Cosplay.
Cosplay, a word of Japanese origin that's short for "costume play", is a performance art in which participants wear costumes and accessories to represent a specific character or idea that is usually always identified with a unique name (as opposed to a generic word). These costume wearers often interact to create a subculture centered on role play, so most time they can be seen in a play group, gathering or convention. A significant amount of these kinds costumes may be originally designed or homemade and unique, depending on the character, idea, or object the costumer wearer is attempting to imitate or represent. The costumes themselves are often artistically judged to how well they represent the subject or object that the costume wearer is attempting to contrive.

</doc>
<doc id="7674" url="http://en.wikipedia.org/wiki?curid=7674" title="Cable car (railway)">
Cable car (railway)

A cable car in the context of mass transit is a system using rail cars that are hauled by a continuously moving cable running at a constant speed. Individual cars stop and start by releasing and gripping this cable as required. Cable cars are distinct from funiculars, where the cars are permanently attached to the cable, and cable railways, which are similar to funiculars, but where the rail vehicles are attached and detached manually.
History.
The first cable-operated railway, employing a moving rope that could be picked up or released by a grip on the cars was
the Fawdon railway (or wagonway) in 1826, a Colliery railway line. The London and Blackwall Railway, which opened for passengers in east London, England, in 1840 used such a system. The rope available at the time proved too susceptible to wear and the system was abandoned in favour of steam locomotives after eight years. In America, the first cable car installation in operation probably was the West Side and Yonkers Patent Railway in New York City, which ran from 1 July 1868 to 1870. The cable technology used in this elevated railway involved collar-equipped cables and claw-equipped cars, and proved cumbersome. The line was closed and rebuilt, and reopened with steam locomotives.
Other cable cars to use grips were those of the Clay Street Hill Railroad, which later became part of the San Francisco cable car system. The building of this line was promoted by Andrew Smith Hallidie with design work by William Eppelsheimer, and it was first tested in 1873. The success of these grips ensured that this line became the model for other cable car transit systems, and this model is often known as the "Hallidie Cable Car".
In 1881 the Dunedin cable tramway system opened in Dunedin, New Zealand and became the first such system outside San Francisco. For Dunedin, George Smith Duncan further developed the Hallidie model, introducing the pull curve and the slot brake; the former was a way to pull cars through a curve, since Dunedin's curves were too sharp to allow coasting, while the latter forced a wedge down into the cable slot to stop the car. Both of these innovations were generally adopted by other cities, including San Francisco.
In Australia the Melbourne cable tramway system operated from 1885 to 1940. It was one of the most extensive in the world with 1200 trams and trailers operating over 15 routes with 103 km (64 miles) of track. Sydney also had a few cable tram routes.
Cable cars rapidly spread to other cities, although the major attraction for most was the ability to displace horsecar (or mule-drawn) systems rather than the ability to climb hills. Many people at the time viewed horse-drawn transit as unnecessarily cruel, and the fact that a typical horse could work only four or five hours per day necessitated the maintenance of large stables of draft animals that had to be fed, housed, groomed, medicated and rested. Thus, for a period, economics worked in favour of cable cars even in relatively flat cities.
For example, the Chicago City Railway, also designed by Eppelsheimer, opened in Chicago in 1882 and went on to become the largest and most profitable cable car system. As with many cities, the problem in flat Chicago was not one of grades but of transportation capacity. This caused a different approach to the combination of grip car and trailer. Rather than using a grip car and single trailer, as many cities did, or combining the grip and trailer into a single car, like San Francisco's "California Cars", Chicago used grip cars to pull trains of up to three trailers.
In 1883 the New York and Brooklyn Bridge Railway was opened, which had a most curious feature: though it was a cable car system, it used steam locomotives to get the cars into and out of the terminals. After 1896 the system was changed to one on which a motor car was added to each train to maneuver at the terminals, while en route, the trains were still propelled by the cable.
On 25 September 1883 a test of a cable car system was held by Liverpool United Tramways and Omnibus Company in Kirkdale, Liverpool. This would have been the first cable car system in Europe, but the company decided against implementing it. Instead the distinction went to the 1884 route from Archway to Highgate, north London, which used a continuous cable and grip system on the 1 in 11 (9%) climb of Highgate Hill. The installation was not reliable and was replaced by electric traction in 1909. Other cable car systems were implemented in Europe, though, among which was the Glasgow District Subway, the first underground cable car system, in 1896. (London's first deep-level tube railway, the City & South London Railway, had earlier also been built for cable haulage but had been converted to electric traction before opening in 1890.) A few more cable car systems were built in the United Kingdom, Portugal and France, but European cities, having many more curves in their streets, were less suitable for cable cars than American cities.
Though some new cable car systems were still being built, by 1890 the cheaper to construct and simpler to operate electrically-powered trolley or tram started to become the norm, and eventually started to replace existing cable car systems. For a while hybrid cable/electric systems operated, for example in Chicago where electric cars had to be pulled by grip cars through the loop area, due to the lack of trolley wires there. Eventually, San Francisco became the only street-running manually operated system to survive—Dunedin, the second city with such cars, was also the second-last city to operate them, closing down in 1957.
Recent Revival.
In the last decades of the 20th century cable traction in general has seen a limited revival as automatic people movers, used in resort areas, airports (for example, Toronto Airport), huge hospital centers and some urban settings. While many of these systems involve cars permanently attached to the cable, the Minimetro system from Poma/Leitner Group and the Cable Liner system from DCC Doppelmayr Cable Car both have variants that allow the cars to be automatically decoupled from the cable under computer control, and can thus be considered a modern interpretation of the cable car.
Operation.
The cable is itself powered by a stationary motor or engine situated in a cable house or power house. The speed at which it moves is relatively constant depending on the number of units gripping the cable at any given time.
The cable car begins moving when a clamping device attached to the car, called a "grip", applies pressure to ("grips") the moving cable. Conversely the car is stopped by releasing pressure on the cable (with or without completely detaching) and applying the brakes. This gripping and ungripping action may be manual, as was the case in all early cable car systems, or automatic, as is the case in some recent cable operated people mover type systems. Gripping must be an even and gradual process in order to avoid bringing the car to cable speed too quickly and unacceptably jarring the passengers.
In the case of manual systems, the grip resembles a very large pair of pliers, and considerable strength and skill are required to operate the car. As many early cable car operators discovered the hard way, if the grip is not applied properly, it can damage the cable, or even worse, become entangled in the cable. In the latter case, the cable car may not be able to stop and can wreak havoc along its route until the cable house realizes the mishap and halts the cable.
One apparent advantage of the cable car is its relative energy efficiency, because of the economy of centrally located power stations, and the ability of descending cars to transfer energy to ascending cars. However, this advantage is totally negated by the relatively large energy consumption required to simply move the cable over and under the numerous guide rollers and around the many sheaves. Approximately 95% of the tractive effort in the San Francisco system is expended in simply moving the four cables at 9.5 miles per hour. Electric cars with regenerative braking do offer the advantages, without the problem of moving a cable. In the case of steep grades, however, cable traction has the major advantage of not depending on adhesion between wheels and rails. There is also the obvious advantage that keeping the car gripped to the cable will also limit the downhill speed to that of the cable.
Because of the constant and relatively low speed, a cable car's potential to cause harm in an accident can be underestimated. Even with a cable car traveling at only 9 miles per hour, the mass of the cable car and the combined strength and speed of the cable can do quite a lot of damage in a collision.
Relation to Funiculars.
A cable car is superficially similar to a funicular, but differs from such a system in that its cars are not permanently attached to the cable and can stop independently, whereas a funicular has cars that are permanently attached to the propulsion cable, which is itself stopped and started. A cable car cannot climb as steep a grade as a funicular, but many more cars can be operated with a single cable, making it more flexible, and allowing a higher capacity. During the rush hour on San Francisco's Market Street Railway, a car would leave the terminal every 15 seconds.
A few funicular railways operate in street traffic, and because of this operation are often incorrectly described as cable cars. Examples of such operation, and the consequent confusion, are:
Even more confusingly, a hybrid cable car/funicular line once existed in the form of the original Wellington Cable Car, in the New Zealand city of Wellington. This line had both a continuous loop haulage cable that the cars gripped using a cable car gripper, and a balance cable permanently attached to both cars over an undriven pulley at the top of the line. The descending car gripped the haulage cable and was pulled downhill, in turn pulling the ascending car (which remained ungripped) uphill by the balance cable. This line was rebuilt in 1979 and is now a standard funicular, although it retains its old cable car name.
List of cable car systems.
Cities currently operating cable cars.
Traditional cable car systems.
The best known existing cable car system is the San Francisco cable car system in the city of San Francisco, California. San Francisco's cable cars constitute the oldest and largest such system in permanent operation, and it is the only one to still operate in the traditional manner with manually operated cars running in street traffic.
Modern cable car systems.
Several cities operate a modern version of the cable car system. These systems are fully automated and run on their own reserved right of way. They are commonly referred to as people movers, although that term is also applied to systems with other forms of propulsion, including funicular style cable propulsion.
These cities include:
Cities previously operating cable cars.
United States.
8th St. Tunnel in use (1887-1956)
External links.
Information
Patents

</doc>
<doc id="7676" url="http://en.wikipedia.org/wiki?curid=7676" title="Creaky voice">
Creaky voice

In linguistics, creaky voice (sometimes called laryngealisation, pulse phonation, vocal fry, or glottal fry), is a special kind of phonation in which the arytenoid cartilages in the larynx are drawn together; as a result, the vocal folds are compressed rather tightly, becoming relatively slack and compact. They vibrate irregularly at 20–50 pulses per second, about two octaves below the frequency of normal voicing, and the airflow through the glottis is very slow. However, although creaky voice may occur with very low pitch, as at the end of a long intonation unit, it can occur with any pitch.
A slight degree of laryngealisation, occurring in some Korean consonants for example, is called "stiff voice". The Danish prosodic feature "stød" is an example of a form of laryngealisation that has a phonemic function. Creaky voice has been reported to be prevalent in American English as spoken in the Pacific Northwest of the United States. However, it has also been reported that use of creaky voice has been spreading among women across the United States.
Some languages, such as Jalapa Mazatec, use creaky voice as a linguistically significant marker; that is, the presence or absence of creaky voice can change the meaning of a word.
In the International Phonetic Alphabet, creaky voice of a phone is represented by a diacritical tilde , for example .

</doc>
<doc id="7677" url="http://en.wikipedia.org/wiki?curid=7677" title="Computer monitor">
Computer monitor

A monitor or a display is an electronic visual display for computers. The monitor comprises the display device, circuitry and an enclosure. The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) thin panel, while older monitors used a cathode ray tube (CRT) about as deep as the screen size.
Originally, computer monitors were used for data processing while television receivers were used for entertainment. From the 1980s onwards, computers (and their monitors) have been used for both data processing and entertainment, while televisions have implemented some computer functionality. The common aspect ratio of televisions, and then computer monitors, has also changed from 4:3 to 16:9 (and 16:10).
History.
Early electronic computers were fitted with a panel of light bulbs where the state of each particular bulb would indicate the on/off state of a particular register bit inside the computer. This allowed the engineers operating the computer to monitor the internal state of the machine, so this panel of lights came to be known as the 'monitor'. As early monitors were only capable of displaying a very limited amount of information, and were very transient, they were rarely considered for programme output. Instead, a line printer was the primary output device, while the monitor was limited to keeping track of the programme's operation.
In time this array of light bulbs was replaced by a cathode ray tube which could display the equivalent of several dozen light bulbs with greater reliability.
As technology developed it was realized that the output of a CRT display was more flexible than a panel of light bulbs and eventually, by giving control of what was displayed to the programme itself, the monitor itself became a powerful output device in its own right.
Technologies.
Multiple technologies have been used for computer monitors. Until the 21st century most used cathode ray tubes but they have largely been superseded by LCD monitors.
Cathode ray tube.
The first computer monitors used cathode ray tubes (CRTs). Prior to the advent of home computers in the late 1970s, it was common for a video display terminal (VDT) using a CRT to be physically integrated with a keyboard and other components of the system in a single large chassis. The display was monochrome and far less sharp and detailed than on a modern flat-panel monitor, necessitating the use of relatively large text and severely limiting the amount of information that could be displayed at one time. High-resolution CRT displays were developed for specialized military, industrial and scientific applications but they were far too costly for general use. 
Some of the earliest home computers (such as the TRS-80 and Commodore PET) were limited to monochrome CRT displays, but color display capability was already a standard feature of the pioneering Apple II, introduced in 1977, and the specialty of the more graphically sophisticated Atari 800, introduced in 1979. Either computer could be connected to the antenna terminals of an ordinary color TV set or used with a purpose-made CRT color monitor for optimum resolution and color quality. Lagging several years behind, in 1981 IBM introduced the Color Graphics Adapter, which could display four colors with a resolution of 320 x 200 pixels, or it could produce 640 x 200 pixels with two colors. In 1984 IBM introduced the Enhanced Graphics Adapter which was capable of producing 16 colors and had a resolution of 640 x 350.
By the end of the 1980s color CRT monitors that could clearly display 1024 x 768 pixels were widely available and increasingly affordable. During the following decade maximum display resolutions gradually increased and prices continued to fall. CRT technology remained dominant in the PC monitor market into the new millennium partly because it was cheaper to produce and offered viewing angles close to 180 degrees. CRTs still offer some image quality advantages over LCD displays but improvements to the latter have made them much less obvious. The dynamic range of early LCD panels was very poor, and although text and other motionless graphics were sharper than on a CRT, an LCD characteristic known as pixel lag caused moving graphics to appear noticeably smeared and blurry.
Liquid crystal display.
There are multiple technologies that have been used to implement liquid crystal displays (LCD). Throughout the 1990s, the primary use of LCD technology as computer monitors was in laptops where the lower power consumption, lighter weight, and smaller physical size of LCDs justified the higher price versus a CRT. Commonly, the same laptop would be offered with an assortment of display options at increasing price points: (active or passive) monochrome, passive color, or active matrix color (TFT). As volume and manufacturing capability have improved, the monochrome and passive color technologies were dropped from most product lines.
TFT-LCD is a variant of LCD which is now the dominant technology used for computer monitors.
The first standalone LCD displays appeared in the mid-1990s selling for high prices. As prices declined over a period of years they became more popular, and by 1997 were competing with CRT monitors. Among the first desktop LCD computer monitors was the Eizo L66 in the mid-1990s, the Apple Studio Display in 1998, and the Apple Cinema Display in 1999. In 2003, TFT-LCDs outsold CRTs for the first time, becoming the primary technology used for computer monitors. The main advantages of LCDs over CRT displays are that LCDs consume less power, take up much less space, and are considerably lighter. The now common active matrix TFT-LCD technology also has less flickering than CRTs, which reduces eye strain. On the other hand, CRT monitors have superior contrast, have superior response time, are able to use multiple screen resolutions natively, and there is no discernible flicker if the refresh rate is set to a sufficiently high value. LCD monitors have now very high temporal accuracy and can be used for vision research.
Organic light-emitting diode.
Organic light-emitting diode (OLED) monitors provide higher contrast and better viewing angles than LCDs but they require more power when displaying documents with white or bright backgrounds. In 2011, a OLED monitor cost $7500, but the prices are expected to drop.
Measurements of performance.
The performance of a monitor is measured by the following parameters:
Size.
On two-dimensional display devices such as computer monitors the display size or viewable image size is the actual amount of screen space that is available to display a picture, video or working space, without obstruction from the case or other aspects of the unit's design. The main measurements for display devices are: width, height, total area and the diagonal.
The size of a display is usually by monitor manufacturers given by the diagonal, i.e. the distance between two opposite screen corners. This method of measurement is inherited from the method used for the first generation of CRT television, when picture tubes with circular faces were in common use. Being circular, only their diameter was needed to describe their size. Since these circular tubes were used to display rectangular images, the diagonal measurement of the rectangle was equivalent to the diameter of the tube's face. This method continued even when cathode ray tubes were manufactured as rounded rectangles; it had the advantage of being a single number specifying the size, and was not confusing when the aspect ratio was universally 4:3.
The estimation of the monitor size by the distance between opposite corners does not take into account the display aspect ratio, so that for example a 16:9 widescreen display has less area, than a 4:3 screen. The 4:3 screen has dimensions of and area , while the widescreen is , .
Aspect ratio.
Until about 2003, most computer monitors had a aspect ratio and some had . Between 2003 and 2006, monitors with and mostly (8:5) aspect ratios became commonly available, first in laptops and later also in standalone monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen computer game play and movie viewing, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time. In 2008 16:10 became the most common sold aspect ratio for LCD monitors and the same year 16:10 was the mainstream standard for laptops and notebook computers.
In 2010 the computer industry started to move over from to because 16:9 was chosen to be the standard high-definition television display size, and because they were cheaper to manufacture. Eventually, monitors with non-HD resolutions such as 1920x1200 were no longer produced.
In 2011 non-widescreen displays with 4:3 aspect ratios were only being manufactured in small quantities. According to Samsung this was because the "Demand for the old 'Square monitors' has decreased rapidly over the last couple of years," and "I predict that by the end of 2011, production on all 4:3 or similar panels will be halted due to a lack of demand."
Resolution.
The resolution for computer monitors has increased over time. From 320x200 during the early 1980s, to 800x600 during the late 1990s. Since 2009, the most commonly sold resolution for computer monitors is 1920x1080. Top-end consumer products are limited to 2560x1600 at . Apple introduced 2880x1800 with Retina MacBook Pro at on June 12, 2012.
Additional features.
Power saving.
Most modern monitors will switch to a power-saving mode if no video-input signal is received. This allows modern operating systems to turn off a monitor after a specified period of inactivity. This also extends the monitor's service life.
Some monitors will also switch themselves off after a time period on standby.
Most modern laptops provide a method of screen dimming after periods of inactivity or when the battery is in use. This extends battery life and reduces wear.
Integrated accessories.
Many monitors have other accessories (or connections for them) integrated. This places standard ports within easy reach and eliminates the need for another separate hub, camera, microphone, or set of speakers. These monitors have advanced microprocessors which contain codec information, Windows Interface drivers and other small software which help in proper functioning of these functions.
Glossy screen.
Some displays, especially newer LCD monitors, replace the traditional anti-glare matte finish with a glossy one. This increases color saturation and sharpness but reflections from lights and windows are very visible. Anti-reflective coatings are sometimes applied to help reduce reflections, although this only mitigates the effect.
Curved designs.
In about 2009, NEC/Alienware together with Ostendo Technologies (based in Carlsbad, CA) were offering a curved (concave) monitor that allows better viewing angles near the edges, covering 75% of peripheral vision. This monitor had 2880x900 resolution, LED backlight and was marketed as suitable both for gaming and office work, while for $6499 it was rather expensive. As of 2013, the monitor is no longer available. Ostendo Technologies is no longer pursuing curved monitor technology.
Directional screen.
Narrow viewing angle screens are used in some security conscious applications.
3D.
Newer monitors are able to display a different image for each eye, often with the help of special glasses, giving the perception of depth.
A directional screen which generates 3D images without headgear.
Touch screen.
These monitors use touching of the screen as an input method. Items can be selected or moved with a finger, and finger gestures may be used to convey commands. The screen will need frequent cleaning due to image degradation from fingerprints.boom
Tablet screens.
A combination of a monitor with a graphics tablet. Such devices are typically unresponsive to touch without the use of one or more special tools' pressure. Newer models however are now able to detect touch from any pressure and often have the ability to detect tilt and rotation as well.
Touch and tablet screens are used on LCD displays as a substitute for the light pen, which can only work on CRTs.
Mounting.
Computer monitors are provided with a variety of methods for mounting them depending on the application and environment.
Desktop.
A desktop monitor is typically provided with a stand from the manufacturer which lifts the monitor up to a more ergonomic viewing height. The stand may be attached to the monitor using a proprietary method or may use, or be adaptable to, a Video Electronics Standards Association, VESA, standard mount. Using a VESA standard mount allows the monitor to be used with an after-market stand once the original stand is removed. Stands may be fixed or offer a variety of features such as height adjustment, horizontal swivel, and landscape or portrait screen orientation.
VESA mount.
The Flat Display Mounting Interface (FDMI), also known as VESA Mounting Interface Standard (MIS) or colloquially as a VESA mount, is a family of standards defined by the Video Electronics Standards Association for mounting flat panel monitors, TVs, and other displays to stands or wall mounts. It is implemented on most modern flat-panel monitors and TVs.
For Computer Monitors, the VESA Mount typically consists of four threaded holes on the rear of the display that will mate with an adapter bracket.
Rack mount.
Rack mount computer monitors are available in two styles and are intended to be mounted into a 19-inch rack:
A fixed rack mount monitor is mounted directly to the rack with the LCD visible at all times. The height of the unit is measured in rack units (RU) and 8U or 9U are most common to fit 17-inch or 19-inch LCD displays. The front sides of the unit are provided with flanges to mount to the rack, providing appropriately spaced holes or slots for the rack mounting screws. A 19-inch diagonal LCD is the largest size that will fit within the rails of a 19-inch rack. Larger LCDs may be accommodated but are 'mount-on-rack' and extend forward of the rack. There are smaller display units, typically used in broadcast environments, which fit multiple smaller LCD displays side by side into one rack mount.
A stowable rack mount monitor is 1U, 2U or 3U high and is mounted on rack slides allowing the display to be folded down and the unit slid into the rack for storage. The display is visible only when the display is pulled out of the rack and deployed. These units may include only a display or may be equipped with a keyboard creating a KVM (Keyboard Video Monitor). Most common are systems with a single LCD display but there are systems providing two or three displays in a single rack mount system.
Panel mount.
A panel mount computer monitor is intended for mounting into a flat surface with the front of the display unit protruding just slightly. They may also be mounted to the rear of the panel. A flange is provided around the LCD display, sides, top and bottom, to allow mounting. This contrasts with a rack mount display where the flanges are only on the sides. The flanges will be provided with holes for thru-bolts or may have studs welded to the rear surface to secure the unit in the hole in the panel. Often a gasket is provided to provide a water-tight seal to the panel and the front of the LCD will be sealed to the back of the front panel to prevent water and dirt contamination.
Open frame.
An open frame monitor provides the LCD monitor and enough supporting structure to hold associated electronics and to minimally support the LCD. Provision will be made for attaching the unit to some external structure for support and protection. Open frame LCD displays are intended to be built in to some other piece of equipment. An arcade video game would be a good example with the display mounted inside the cabinet. There is usually an open frame display inside all end-use displays with the end-use display simply providing an attractive protective enclosure. Some rack mount LCD display manufacturers will purchase desk-top displays, take them apart, and discard the outer plastic parts, keeping the inner open-frame LCD display for inclusion into their product.
Security vulnerabilities.
According to an NSA document leaked to Der Spiegel, the NSA sometimes swaps the monitor cables on targeted computers with a bugged monitor cable in order to allow the NSA to remotely see what's displayed on the targeted computer monitor.
Van Eck phreaking is the process of remotely displaying the contents of a CRT or LCD display by detecting its electromagnetic emissions. It is named after Dutch computer researcher Wim van Eck, who in 1985 published the first paper on it, including proof of concept. Phreaking is the process of exploiting telephone networks, used here because of its connection to eavesdropping.

</doc>
<doc id="7681" url="http://en.wikipedia.org/wiki?curid=7681" title="ClearType">
ClearType

ClearType is Microsoft's implementation of subpixel rendering technology in rendering text in a font system. ClearType attempts to improve the appearance of text on certain types of computer display screens by sacrificing color fidelity for additional intensity variation. This trade-off is asserted to work well on LCD flat panel monitors.
ClearType was first announced at the November 1998 COMDEX exhibition. The technology was first introduced in software in January 2000 as an always-on feature of Microsoft Reader, which was released to the public in August 2000.
ClearType was significantly changed with the introduction of DirectWrite in Windows 7.
Background.
Computer displays where the positions of individual pixels are permanently fixed—such as most modern flat panel displays—can show saw-tooth edges when displaying small, high-contrast graphic elements, such as text. ClearType uses spatial anti-aliasing at the subpixel level to reduce visible artifacts on such displays when text is rendered, making the text appear "smoother" and less jagged. ClearType also uses very heavy font hinting to force the font to fit into the pixel grid. This increases edge contrast and readability of small fonts at the expense of font rendering fidelity and has been criticized by graphic designers for making different fonts look similar .
Like most other types of subpixel rendering, ClearType involves a compromise, sacrificing one aspect of image quality (color or "chrominance" detail) for another (light and dark or "luminance" detail). The compromise can improve text appearance when luminance detail is more important than chrominance.
Only user and system applications render application of ClearType. ClearType does not alter other graphic display elements (including text already in bitmaps). For example, ClearType enhancement renders text on the screen in Microsoft Word, but text placed in a bitmapped image in a program such as Adobe Photoshop is not. In theory, the method (called "RGB Decimation" internally) can enhance the anti-aliasing of any digital image.
ClearType is not used when printing text. Most printers already use such small pixels that aliasing is rarely a problem, and they don't have the addressable fixed subpixels ClearType requires. Nor does ClearType affect text stored in files. ClearType only applies any processing to the text while it is being rendered onto the screen.
ClearType was invented in the Microsoft e-Books team by Bert Keely and Greg Hitchcock. It was then analyzed by researchers in the company, and signal processing expert John Platt designed an improved version of the algorithm. Dick Brass, a Vice President at Microsoft from 1997 to 2004, complained that the company was slow in moving ClearType to market in the portable computing field.
How ClearType works.
Normally, the software in a computer treats the computer’s display screen as a rectangular array of square, indivisible pixels, each of which has an intensity and color that are determined by the blending of three primary colors: red, green, and blue. However, actual display hardware usually implements each pixel as a group of three adjacent, independent "subpixels," each of which displays a different primary color. Thus, on a real computer display, each pixel is actually composed of separate red, green, and blue subpixels. For example, if a flat-panel display is examined under a magnifying glass, the pixels may appear as follows:
In the illustration above, there are nine pixels but 27 subpixels.
If the computer controlling the display knows the exact position and color of all the subpixels on the screen, it can take advantage of this to improve the apparent resolution in certain situations. If each pixel on the display actually contains three rectangular subpixels of red, green, and blue, in that fixed order, then things on the screen that are smaller than one full pixel in size can be rendered by lighting only one or two of the subpixels. For example, if a diagonal line with a width smaller than a full pixel must be rendered, then this can be done by lighting only the subpixels that the line actually touches. If the line passes through the leftmost portion of the pixel, only the red subpixel is lit; if it passes through the rightmost portion of the pixel, only the blue subpixel is lit. This effectively triples the horizontal resolution of the image at normal viewing distances; the drawback is that the line thus drawn will show color fringes (at some points it might look green, at other points it might look red or blue).
ClearType uses this method to improve the smoothness of text. When the elements of a type character are smaller than a full pixel, ClearType lights only the appropriate subpixels of each full pixel in order to more closely follow the outlines of that character. Text rendered with ClearType looks “smoother” than text rendered without it, provided that the pixel layout of the display screen exactly matches what ClearType expects.
The following picture shows a 4× enlargement of the word "Wikipedia" rendered using ClearType.
The word was originally rendered using a Times New Roman 12 pt font.
In this magnified view, it becomes clear that, while the overall smoothness of the text seems to improve, there is also color fringing of the text.
An extreme close-up of a color display shows (a) text rendered without ClearType and (b) text rendered with ClearType. Note the changes in subpixel intensity that are used to increase effective resolution when ClearType is enabled—without ClearType, all sub-pixels of a given pixel have the same intensity.
In the above lines of text, when the orange circle is shown, all the text in the frame is rendered using ClearType (RGB subpixel rendering); when the orange circle is absent all the text is rendered using normal (full pixel greyscale) anti-aliasing.
ClearType, human vision and cognition.
ClearType and similar technologies work on the theory that variations in intensity are more noticeable than variations in color.
Expert opinion.
In a MSDN article, Microsoft acknowledges that "[t]ext that is rendered with ClearType can also appear significantly different when viewed by individuals with varying levels of color sensitivity. Some individuals can detect slight differences in color better than others." This opinion is shared by font designer Thomas Phinney (Vice President of FontLab and formerly with Adobe Systems): "There is also considerable variation between individuals in their sensitivity to color fringing. Some people just notice it and are bothered by it a lot more than others."
Hinting expert Beat Stamm, who worked on ClearType at Microsoft, agrees that ClearType may look blurry at 96 dpi, which was a typical resolution for LCDs in 2008, but adds that higher resolution displays improve on this aspect: "WPF [Windows Presentation Foundation] uses method C [ClearType with fractional pixel positioning], but few display devices have a sufficiently high resolution to make the potential blur a moot point for everybody. . . . Some people are ok with the blur in Method C, some aren’t. Anecdotal evidence suggests that some people are fine with Method C when reading continuous text at 96 dpi (e.g. Times Reader, etc.) but not in UI scenarios. Many people are fine with the colors of ClearType, even at 96 dpi, but a few aren’t… To my eyes and at 96 dpi, Method C doesn’t read as well as Method A. It reads “blurrily” to me. Conversely, at 144 dpi, I don’t see a problem with Method C. It looks and reads just fine to me." One illustration of the potential problem is the following image:
In the above block of text, the same portion of text is shown in the upper half without and in the lower half with ClearType rendering (as opposed to Standard and ClearType in the previous image). This and the previous example with the orange circle demonstrate the blurring introduced. For many observers this blurring is beneficial; others do not find ClearType beneficial.
Empirical studies.
A 2001 study conducted by researchers from Clemson University and The University of Pennsylvania on "18 users who spent 60 minutes reading fiction from each of three different displays" found that "When reading from an LCD display, users preferred text rendered with ClearType™. ClearType also yielded higher readability judgments and lower ratings of mental fatigue." A 2002 study on 24 users conducted by the same researchers from Clemson University also found that "Participants were significantly more accurate at identifying words with ClearType™ than without ClearType™."
According to a 2006 study at the University of Texas at Austin by Dillon et al., ClearType "may not be universally beneficial". The study notes that maximum benefit may be seen when the information worker is spending large proportions of their time reading text (which is not necessarily the case for the majority of computer users today). Additionally, over one third of the study participants experienced some disadvantage when using ClearType. Whether ClearType, or other rendering, should be used is very subjective and it must be the choice of the individual, with the report recommending "to allow users to disable [ClearType] if they find it produces effects other than improved performance".<reF>Dillon, A., Kleinman, L., Choi, G. O., & Bias, R. (2006). . CHI ’06: Proceedings of the SIGCHI conference on Human Factors in computing systems, 503-511.</ref>
Another 2007 empirical study found that "while ClearType rendering does not improve text legibility, reading speed or comfort compared to perceptually-tuned grayscale rendering, subjects prefer text with moderate ClearType rendering to text with grayscale or higher-level ClearType contrast."
A 2007 survey of the literature by Microsoft researcher Kevin Larson presented a different picture: "Peer-reviewed studies have consistently found that using ClearType boosts reading performance compared with other text-rendering systems. In a 2004 study, for instance, Lee Gugerty, a psychology professor at Clemson University, in South Carolina, measured a 17 percent improvement in word recognition accuracy with ClearType. Gugerty’s group also showed, in a sentence comprehension study, that ClearType boosted reading speed by 5 ­percent and comprehension by 2 ­percent. Those results were unusual because, typically, any gain in reading speed decreases comprehension. Similarly, in a study published last year, psychologist Andrew Dillon at the University of Texas at Austin found that when subjects were asked to scan a spreadsheet and pick out certain information, they did those tasks 7 percent faster with ClearType."
Display requirements.
ClearType and allied technologies require display hardware with fixed pixels and subpixels. More precisely, the positions of the pixels and subpixels on the screen must be exactly known to the computer to which it is connected. This is the case for flat-panel displays, on which the positions of the pixels are permanently fixed by the design of the screen itself. Almost all flat panels have a perfectly rectangular array of square pixels, each of which contains three rectangular subpixels in the three primary colors, with the normal ordering being red, green, and blue, arranged in vertical bands. ClearType assumes this arrangement of pixels when rendering text.
ClearType does not work properly with flat-panel displays that are operated at resolutions other than their “native” resolutions, since only the native resolution corresponds exactly to the actual positions of pixels on the screen of the display.
If a display does not have the type of fixed pixels that ClearType expects, text rendered with ClearType enabled actually looks worse than type rendered without it. Some flat panels have unusual pixel arrangements, with the colors in a different order, or with the subpixels positioned differently (in three horizontal bands, or in other ways). ClearType needs to be manually tuned for use with such displays (see below).
Displays that have no fixed pixel positions, such as shadow mask CRT displays, may be harder to read if ClearType is enabled. However, on CRT displays with a similar pixel arrangement as flat-panel displays, such as aperture grille CRT displays, it can result in a slightly improved readability.
Sensitivity to display orientation.
Because ClearType utilizes the physical layout of the red, green and blue pigments of the LCD screen, it is sensitive to the orientation of the display.
ClearType in Windows XP currently supports the RGB and BGR sub pixel structures. Rotated displays, in which the subpixels are arranged vertically rather than horizontally, are "not" currently supported. Using ClearType on these display configurations will actually reduce the display quality. The best option for users of Windows XP having rotated LCD displays (Tablet PCs or swivel-stand LCD displays) is using regular anti-aliasing, or switching off font-smoothing altogether.
The software developer documentation for Windows CE states that ClearType for rotated screens is supported on that platform.
Vertical sub pixel structures are not supported in Windows XP.
Implementations.
ClearType is also an integrated component of the Windows Presentation Foundation text-rendering engine.
ClearType in GDI.
ClearType can be globally enabled or disabled for GDI applications. A control panel applet is available to let the users tune the GDI ClearType settings. The GDI implementation of ClearType does not support sub-pixel positioning.
ClearType tuning.
Some versions of Microsoft Windows, as supplied, allow ClearType to be turned on or off, with no adjustment; other versions allow tuning of the ClearType parameters. A Microsoft ClearType tuner utility is available for free download for Windows versions lacking this facility. If ClearType is disabled in the operating system, applications with their own ClearType controls can still support it. Microsoft Reader (for e-books) has its own ClearType tuner.
ClearType in WPF.
All text in Windows Presentation Foundation is anti-aliased and rendered using ClearType. There are separate ClearType registry settings for GDI and WPF applications, but by default the WPF entries are absent, and the GDI values are used in their absence. WPF registry entries can be tuned using the instructions from the MSDN WPF Text Blog.
ClearType in WPF supports sub-pixel positioning, natural advance widths, Y-direction anti-aliasing and hardware acceleration. WPF supports aggressive caching of pre-rendered ClearType text in video memory. The extent to which this is supported is dependent on the video card. DirectX 10 cards will be able to cache the font glyphs in video memory, then perform the composition (assembling of character glyphs in the correct order, with the correct spacing), alpha blending (application of anti-aliasing), and RGB blending (ClearType's sub-pixel color calculations), entirely in hardware. This means that only the original glyphs need to be stored in video memory once per font (Microsoft estimates that this would require 2 MB of video memory per font), and other operations such as the display of anti-aliased text on top of other graphics — including video — can also be done with no computation effort on the part of the CPU. DirectX 9 cards will only be able to cache the alpha-blended glyphs in memory, thus requiring the CPU to handle glyph composition and alpha-blending before passing this to the video card. Caching these partially rendered glyphs requires significantly more memory (Microsoft estimates 5 MB per process). Cards that don't support DirectX 9 have no hardware-accelerated text rendering capabilities.
ClearType in DirectWrite.
The font rendering engine in DirectWrite supports an improved version of ClearType, as demonstrated at PDC 2008. The improved version is sometimes called "Natural ClearType".<reF>http://blogs.msdn.com/b/e7/archive/2009/02/13/advances-in-typography-and-text-rendering-in-windows-7.aspx</ref> The improvements have been confirmed by independent sources, such as Firefox developers; they were particularly noticeable for OpenType fonts in Compact Font Format (CFF).<reF>https://blog.mozilla.org/nattokirai/2009/10/22/better-postscript-cff-font-rendering-with-directwrite/</ref>
Despite these improvements, Word 2013 stopped using ClearType. The reasons invoked are, in the words of Murray Sargent: "There is a problem with ClearType: it depends critically on the color of the background pixels. This isn’t a problem if you know a priori that those pixels are white, which is usually the case for text. But the general case involves calculating what the colors should be for an arbitrary background and that takes time. Meanwhile, Word 2013 enjoys cool animations and smooth zooming. Nothing jumps any more. Even the caret (the blinking vertical line at the text insertion point) glides from one position to the next as you type. Jerking movement just isn’t considered cool any more. Well animations and zooms have to be faster than human response times in order to appear smooth. And that rules out ClearType in animated scenarios at least with present generation hardware. And in future scenarios, screens will have sufficiently high resolution that gray-scale anti-aliasing should suffice."<Ref>http://blogs.msdn.com/b/murrays/archive/2014/05/31/crisp-text-display.aspx</ref>
Patents.
ClearType is a registered trademark and Microsoft claims protection under the following U.S. patents:
Other uses of the ClearType brand.
The ClearType name was also referred to the screens of Microsoft Surface tablets. ClearType HD Display includes a 1366x768 screen, while ClearType Full HD Display includes a 1920x1080 screen.

</doc>
<doc id="7682" url="http://en.wikipedia.org/wiki?curid=7682" title="Centriole">
Centriole

In cell biology a centriole is a cylindrical cell structure composed mainly of a protein called tubulin that is found in most eukaryotic cells. An associated pair of centrioles, surrounded by an amorphous mass of dense material, called the pericentriolar material, or PCM, makes up a compound structure called a centrosome.
Centrioles are present in the cells of most eukaryotes, for example those of animals. However, they are absent from conifers (pinophyta), flowering plants (angiosperms) and most fungi, and are only present in the male gametes of charophytes, bryophytes, seedless vascular plants, cycads, and gingko. 
Most centrioles are made up of nine sets of microtubule triplets, arranged in a cylinder.
Deviations from this structure include crabs and "Drosophila melanogaster" embryos, with nine doublets, and "Caenorhabditis elegans" sperm cells and early embryos, with nine singlets.
Edouard van Beneden and Theodor Boveri made the first observation and identification of centrioles in 1883 and 1888 respectively, while the pattern of centriole duplication was first worked out independently by Etienne de Harven and Joseph G. Gall circa 1950 
Cell division.
Centrioles are involved in the organization of the mitotic spindle and in the completion of cytokinesis. Centrioles were previously thought to be required for the formation of a mitotic spindle in animal cells. However, more recent experiments have demonstrated that cells whose centrioles have been removed via laser ablation can still progress through the G1 stage of interphase before centrioles can be synthesized later in a de novo fashion. Additionally, mutant flies lacking centrioles develop normally, although the adult flies' cells lack flagella and cilia and as a result, they die shortly after birth.
Cellular organization.
Centrioles are a very important part of centrosomes, which are involved in organizing microtubules in the cytoplasm. The position of the centriole determines the position of the nucleus and plays a crucial role in the spatial arrangement of the cell. 
Ciliogenesis.
In organisms with flagella and cilia, the position of these organelles is determined by the mother centriole, which becomes the basal body. An inability of cells to use centrioles to make functional cilia and flagella has been linked to a number of genetic and developmental diseases. In particular, the inability of centrioles to properly migrate prior to ciliary assembly has recently been linked to Meckel-Gruber syndrome.
Animal development.
Proper orientation of cilia via centriole positioning toward the posterior of embryonic node cells is critical for establishing left–right asymmetry during mammalian development.
Centriole duplication.
Before DNA replication, cells contain two centrioles. The older of the two centrioles is termed the "mother centriole", the other the "daughter". During the cell division cycle, a new centriole grows from the side of each mother centriole. After duplication, the two centriole pairs will remain attached to each other orthogonally until mitosis. At that point the mother and daughter centrioles separate dependently on an enzyme called separase.
The two centrioles in the centrosome are tied to one another. The mother centriole has radiating appendages at the distal end of its long axis and is attached to its daughter at the proximal end. Each daughter cell formed after cell division will inherit one of these pairs. Centrioles start duplicating when DNA replicates.
Origin.
The last common ancestor of all eukaryotes was a ciliated cell with centrioles. Some lineages of eukaryotes, such as land plants, do not have centrioles except in their motile male gametes. Centrioles are completely absent from all cells of conifers and flowering plants, which do not have ciliate or flagellate gametes.
It is unclear if the last common ancestor had one or two cilia. Important genes required for centriole growth, like centrins, are only found in eukaryotes and not in bacteria or archaeans.

</doc>
<doc id="7683" url="http://en.wikipedia.org/wiki?curid=7683" title="Creation science">
Creation science

Creation science or scientific creationism is a branch of creationism that attempts to provide scientific support for the Genesis creation narrative in the Book of Genesis and disprove or reinterpret the scientific facts, theories and scientific paradigms about the history of the Earth, cosmology and biological evolution.
The overwhelming consensus of the scientific community is that creation science is a religious, not a scientific view, and that creation science does not qualify as science because it lacks empirical support, supplies no tentative hypotheses, and resolves to describe natural history in terms of scientifically untestable supernatural causes. Creation science has been characterized as a pseudo-scientific attempt to map the Bible into scientific facts. According to a popular introductory philosophy of science text, "virtually all professional biologists regard creation science as a sham."
It began in the 1960s as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and nullify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. The main ideas in creation science are: the belief in "creation "ex nihilo"" (Latin: out of nothing); the conviction that the Earth was created within the last 6,000–10,000 years; the belief that mankind and other life on Earth were created as distinct fixed "baraminological" "kinds"; and the idea that fossils found in geological strata were deposited during a cataclysmic flood which completely covered the entire Earth. As a result, creation science also challenges the commonly accepted geologic and astrophysical theories for the age and origins of the Earth and Universe, which creationists acknowledge are irreconcilable to the account in the Book of Genesis. Creation science proponents often refer to the theory of evolution as "Darwinism" or as "Darwinian evolution."
The creation science texts and curricula that first emerged in the 1960s focused upon concepts derived from a literal interpretation of the Bible and were overtly religious in nature, most notably linking Noah's flood in the Biblical Genesis account to the geological and fossil record in a system termed "flood geology." These works attracted little notice beyond the schools and congregations of conservative fundamental and Evangelical Christians until the 1970s when its followers challenged the teaching of evolution in the public schools and other venues in the United States, bringing it to the attention of the public-at-large and the scientific community. Many school boards and lawmakers were persuaded to include the teaching of creation science alongside evolution in the science curriculum. Creation science texts and curricula used in churches and Christian schools were revised to eliminate their Biblical and theological references, and less explicitly sectarian versions of creation science education were introduced in public schools in Louisiana, Arkansas, and other regions in the United States.
The 1982 ruling in "McLean v. Arkansas" found that creation science fails to meet the essential characteristics of science and that its chief intent is to advance a particular religious view. The teaching of creation science in public schools in the United States effectively ended in 1987 following the United States Supreme Court decision in "Edwards v. Aguillard". The court affirmed that a statute requiring the teaching of creation science alongside evolution when evolution is taught in Louisiana public schools was unconstitutional because its sole true purpose was to advance a particular religious belief. In response to this ruling, drafts of the creation science school textbook "Of Pandas and People" were edited to change references to creation to intelligent design before its publication in 1989. The intelligent design movement promoted this version, then teaching intelligent design in public school science classes was found to be unconstitutional in the 2005 "Kitzmiller v. Dover Area School District" federal court case.
Beliefs and activities.
Religious basis.
Creation science is based largely upon chapters 1–11 of the Book of Genesis. These describe how God calls the world into existence through the power of speech ("And God said, Let there be light," etc.) in six days, calls all the animals and plants into existence, and molds the first man from clay and the first woman from a rib taken from the man's side; a world-wide flood destroys all life except for Noah and his family and representatives of the animals, and Noah becomes the ancestor of the 70 "nations" of the world; the nations live together and speak one language until the incident of the Tower of Babel, when God disperses them and gives them their different languages. Creation science rarely goes beyond biblical stories in its study, and attempts to explain history and science within the span of Biblical chronology, which places the initial act of creation some six thousand years ago.
Modern religious affiliations.
Most creation science proponents hold fundamentalist or Evangelical Christian beliefs in Biblical literalism or Biblical inerrancy, as opposed to the higher criticism supported by Liberal Christianity in the Fundamentalist–Modernist Controversy. However, there are also examples of Islamic and Jewish scientific creationism that conform to the accounts of creation as recorded in their religious doctrines.
The Seventh-day Adventist Church has a history of support for creation science. This dates back to George McCready Price, an active Seventh-day Adventist who developed views of flood geology, which formed the basis of creation science. This work was continued by the Geoscience Research Institute, an official institute of the Seventh-day Adventist Church, located on its Loma Linda University campus in California.
Creation science is generally rejected by the Church of England as well as the Roman Catholic Church. The Pontifical Gregorian University has officially discussed intelligent design as a "cultural phenomenon" without scientific elements. The Church of England's official website cites Charles Darwin's local work assisting people in his religious parish.
Views on science.
Creation science rejects evolution's theory of the common descent of all living things on the Earth. Instead, it asserts that the field of evolutionary biology is itself pseudoscientific or even a religion. Creationists argue instead for a system called baraminology, which considers the living world to be descended from uniquely created kinds or "baramins."
Creation science incorporates the concept of catastrophism to reconcile current landforms and fossil distributions with Biblical interpretations, proposing the remains resulted from successive cataclysmic events, such as a world-wide flood and subsequent ice age. It rejects one of the fundamental principles of modern geology (and of modern science generally), uniformitarianism, which applies the same physical and geological laws observed on the Earth today to interpret the Earth's geological history.
Sometimes creationists attack other scientific concepts, like the Big Bang cosmological model or methods of scientific dating which measure radioactive decay. Young Earth creationists also reject current estimates of the age of the universe and the age of the Earth, arguing for creationist cosmologies with timescales much shorter than those determined by modern physical cosmology and geological science, typically less than 10,000 years.
The scientific community has overwhelmingly rejected the ideas put forth in creation science as lying outside the boundaries of a legitimate science. The foundational premises underlying scientific creationism disqualify it as a science because the answers to all inquiry therein are preordained to conform to Bible doctrine, and because that inquiry is constructed upon theories which are not empirically testable in nature. Scientists also deem creation science's attacks against biological evolution to be without scientific merit. Those views of the scientific community were accepted in two significant court decisions in the 1980s which found the field of creation science to be a religious mode of inquiry, not a scientific one.
History.
The teaching of evolution was gradually introduced into more and more public high school textbooks in the United States after 1900, but in the aftermath of the First World War the growth of fundamentalist Christianity gave rise to a creationist opposition to such teaching. Legislation prohibiting the teaching of evolution was passed in certain regions, most notably Tennessee's Butler Act of 1925. The Soviet Union's successful launch of "Sputnik 1" in 1957 sparked national concern that the science education in public schools was outdated. In 1958, the United States passed National Defense Education Act which introduced new education guidelines for science instruction. With federal grant funding, the Biological Sciences Curriculum Study (BSCS) drafted new standards for the public schools' science textbooks which included the teaching of evolution. Almost half the nation's high schools were using textbooks based on the guidelines of the BSCS soon after they were published in 1963. The Tennessee legislature did not repeal the Butler Act until 1967.
Creation science (dubbed "scientific creationism" at the time) emerged as an organized movement during the 1960s. It was strongly influenced by the earlier work of armchair geologist George McCready Price who wrote works such as "The New Geology" (1923) to advance what he termed "new catastrophism" and dispute the current geological time frames and explanations of geologic history. Price's work was cited at the Scopes Trial of 1925, yet although he frequently solicited feedback from geologists and other scientists, they consistently disparaged his work. Price's "new catastrophism" also went largely unnoticed by other creationists until its revival with the 1961 publication of "" by John C. Whitcomb and Henry M. Morris, a work which quickly became an important text on the issue to fundamentalist Christians and expanded the field of creation science beyond critiques of geology into biology and cosmology as well. Soon after its publication, a movement was underway to have the subject taught in United States' public schools.
Court determinations.
The various state laws prohibiting teaching of evolution were overturned in 1968 when the United States Supreme Court ruled in "Epperson v. Arkansas" such laws violated the Establishment Clause of the First Amendment to the United States Constitution. This ruling inspired a new creationist movement to promote laws requiring that schools give balanced treatment to creation science when evolution is taught. The 1981 Arkansas Act 590 was one such law that carefully detailed the principles of creation science that were to receive equal time in public schools alongside evolutionary principles. The act defined creation science as follows:
"'Creation-science' means the scientific evidences for creation and inferences from those evidences. Creation-science includes the scientific evidences and related inferences that indicate:
This legislation was examined in "McLean v. Arkansas", and the ruling handed down on January 5, 1982, concluded that creation-science as defined in the act "is simply not science". The judgement defined the following as essential characteristics of science:
The court ruled that creation science failed to meet these essential characteristics and identified specific reasons. After examining the key concepts from creation science, the court found:
The court further noted that no recognized scientific journal had published any article espousing the creation science theory as described in the Arkansas law, and stated that the testimony presented by defense attributing the absence to censorship was not credible.
In its ruling, the court wrote that for any theory to qualify as scientific, the theory must be tentative, and open to revision or abandonment as new facts come to light. It wrote that any methodology which begins with an immutable conclusion which cannot be revised or rejected, regardless of the evidence, is not a scientific theory. The court found that creation science does not culminate in conclusions formed from scientific inquiry, but instead begins with the conclusion, one taken from a literal wording of the Book of Genesis, and seeks only scientific evidence to support it.
The law in Arkansas adopted the same two-model approach as that put forward by the Institute for Creation Research, one allowing only two possible explanations for the origins of life and existence of man, plants and animals: it was either the work of a creator or it was not. Scientific evidence that failed to support the theory of evolution was posed as necessarily scientific evidence in support of creationism, but in its judgment the court ruled this approach to be no more than a "contrived dualism which has not scientific factual basis or legitimate educational purpose."
The judge concluded that "Act 590 is a religious crusade, coupled with a desire to conceal this fact," and that it violated the First Amendment's Establishment Clause.
The decision was not appealed to a higher court, but had a powerful influence on subsequent rulings. Louisiana's 1982 Balanced Treatment for Creation-Science and Evolution-Science Act, authored by State Senator Bill P. Keith, judged in the 1987 United States Supreme Court case "Edwards v. Aguillard", and was handed a similar ruling. It found the law to require the balanced teaching of creation science with evolution had a particular religious purpose and was therefore unconstitutional.
Intelligent design splits off.
In 1984, "The Mystery of Life's Origin" was first published. It was co-authored by chemist and creationist Charles B. Thaxton with Walter L. Bradley and Roger L. Olsen, the foreword written by Dean H. Kenyon, and sponsored by the Christian based Foundation for Thought and Ethics (FTE). The work presented scientific arguments against current theories of abiogenesis and offered an hypothesis of special creation instead. While the focus of creation science had until that time centered primarily on the criticism of the fossil evidence for evolution and validation of the creation myth of the Bible, this new work posed the question whether science reveals that even the simplest living systems were far too complex to have developed by natural, unguided processes.
Kenyon later co-wrote with creationist Percival Davis a book intended as a "scientific brief for creationism" to use as a supplement to public high school biology textbooks. Thaxton was enlisted as the book's editor, and the book received publishing support from the FTE. Prior to its release, the 1987 Supreme Court ruling in "Edwards v. Aguillard" barred the teaching of creation science and creationism in public school classrooms. The book, originally titled "Biology and Creation" but renamed "Of Pandas and People", was released in 1989 and became the first published work to promote the anti-evolutionist design argument under the name intelligent design. The contents of the book later became a focus of evidence in the federal court case, "Kitzmiller v. Dover Area School District", when a group of parents filed suit to halt the teaching of intelligent design in Dover, Pennsylvania, public schools. School board officials there had attempted to include "Of Pandas and People" in their biology classrooms and testimony given during the trial revealed the book was originally written as a creationist text but following the adverse decision in the Supreme Court it underwent simple cosmetic editing to remove the explicit allusions to "creation" or "creator," and replace them instead with references to "design" or "designer."
By the mid-1990s, intelligent design had become a separate movement. The creation science movement is distinguished from the intelligent design movement, or neo-creationism, because most advocates of creation science accept scripture as a literal and inerrant historical account, and their primary goal is to corroborate the scriptural account through the use of science. In contrast, as a matter of principle, neo-creationism eschews references to scripture altogether in its polemics and stated goals (see Wedge strategy). By so doing, intelligent design proponents have attempted to succeed where creation science has failed in securing a place in public school science curricula. Carefully avoiding any reference to the identity of the intelligent designer as God in their public arguments, intelligent design proponents sought to reintroduce the creationist ideas into science classrooms while sidestepping the First Amendment's prohibition against religious infringement. However, the intelligent design curriculum was struck down as a violation of the Establishment Clause in "Kitzmiller v. Dover Area School District", the judge in the case ruling "that ID is nothing less than the progeny of creationism."
Today, creation science as an organized movement is primarily centered within the United States. Creation science organizations are also known in other countries, most notably Creation Ministries International which was founded (under the name Creation Science Foundation) in Australia.
Proponents are usually aligned with a Christian denomination, primarily with those characterized as evangelical, conservative, or fundamentalist. While creationist movements also exist in Islam and Judaism, these movements do not use the phrase "creation science" to describe their beliefs.
Issues.
Creation science has its roots in the work of young Earth creationist George McCready Price disputing modern science's account of natural history, focusing particularly on geology and its concept of uniformitarianism, and his efforts instead to furnish an alternative empirical explanation of observable phenomena which was compatible with strict Biblical literalism. Price's work was later discovered by civil engineer Henry M. Morris, who is now considered to be the father of creation science. Morris and later creationists expanded the scope with attacks against the broad spectrum scientific findings that point to the antiquity of the Universe and common ancestry among species, including growing body of evidence from the fossil record, absolute dating techniques, and cosmogony.
The proponents of creation science often say that they are concerned with religious and moral questions as well as natural observations and predictive hypotheses. Many state that their opposition to scientific evolution is primarily based on religion.
The overwhelming majority of scientists are in agreement that the claims of science are necessarily limited to those that develop from natural observations and experiments which can be replicated and substantiated by other scientists, and that claims made by creation science do not meet those criteria. Duane Gish, a prominent creation science proponent, has similarly claimed, "We do not know how the creator created, what processes He used, "for He used processes which are not now operating anywhere in the natural universe." This is why we refer to creation as special creation. We cannot discover by scientific investigation anything about the creative processes used by the Creator." But he also makes the same claim against science's evolutionary theory, maintaining that on the subject of origins, scientific evolution is a religious theory which cannot be validated by science.
Metaphysical assumptions.
Creation science makes the "a priori" metaphysical assumption that there exists a creator of the life whose origin is being examined. Christian creation science holds that the description of creation is given in the Bible, that the Bible is inerrant in this description (and elsewhere), and therefore empirical scientific evidence must correspond with that description. Creationists also view the preclusion of all supernatural explanations within the sciences as a doctrinaire commitment to exclude the supreme being and miracles. They claim this to be the motivating factor in science's acceptance of Darwinism, a term used in creation science to refer to evolutionary biology which is also often used as a disparagement. Critics argue that creation science is religious rather than scientific because it stems from faith in a religious text rather than by the application of the scientific method. The United States National Academy of Sciences (NAS) has stated unequivocally, "Evolution pervades all biological phenomena. To ignore that it occurred or to classify it as a form of dogma is to deprive the student of the most fundamental organizational concept in the biological sciences. No other biological concept has been more extensively tested and more thoroughly corroborated than the evolutionary history of organisms." Anthropologist Eugenie Scott has noted further, "Religious opposition to evolution propels antievolutionism. Although antievolutionists pay lip service to supposed scientific problems with evolution, what motivates them to battle its teaching is apprehension over the implications of evolution for religion."
Creation science advocates argue that scientific theories of the origins of the Universe, Earth, and life are rooted in "a priori" presumptions of methodological naturalism and uniformitarianism, each of which is disputed. In some areas of science such as chemistry, meteorology or medicine, creation science proponents do not challenge the application of naturalistic or uniformitarian assumptions. Traditionally, creation science advocates have singled out those scientific theories judged to be in conflict with held religious beliefs, and it is against those theories that they concentrate their efforts.
Religious criticism.
Some mainstream Christian churches criticize creation science on theological grounds, asserting either that religious faith alone should be a sufficient basis for belief in the truth of creation, or that efforts to prove the Genesis account of creation on scientific grounds are inherently futile because reason is subordinate to faith and cannot thus be used to prove it.
Many Christian theologies, including Liberal Christianity, consider the Genesis creation myth to be a poetic and allegorical work rather than a literal history, and many Christian churches—including the Roman Catholic, Anglican and the more liberal denominations of the Lutheran, Methodist, Congregationalist and Presbyterian faiths—have either rejected creation science outright or are ambivalent to it. Belief in non-literal interpretations of Genesis is often cited as going back to Saint Augustine.
Theistic evolution and evolutionary creationism are theologies that reconcile belief in a creator with biological evolution. Each holds the view that there is a creator but that this creator has employed the natural force of evolution to unfold a divine plan. Religious representatives from faiths compatible with theistic evolution and evolutionary creationism have challenged the growing perception that belief in a creator is inconsistent with the acceptance of evolutionary theory. Spokespersons from the Catholic Church have specifically criticized biblical creationism for relying upon literal interpretations of biblical scripture as the basis for determining scientific fact.
Scientific criticism.
The National Academy of Sciences states that "the claims of creation science lack empirical support and cannot be meaningfully tested" and that "creation science is in fact not science and should not be presented as such in science classes." According to Joyce Arthur writing for "Skeptic" magazine, the "creation 'science' movement gains much of its strength through the use of distortion and scientifically unethical tactics" and "seriously misrepresents the theory of evolution."
Scientists have considered the hypotheses proposed by creation science and have rejected them because of a lack of evidence. Furthermore, the claims of creation science do not refer to natural causes and cannot be subject to meaningful tests, so they do not qualify as scientific hypotheses. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms. Most mainline Christian denominations have concluded that the concept of evolution is not at odds with their descriptions of creation and human origins.
A summary of the objections to creation science by scientists follows:
By invoking claims of "abrupt appearance" of species as a miraculous act, creation science is unsuited for the tools and methods demanded by science, and it cannot be considered scientific in the way that the term "science" is currently defined. Scientists and science writers commonly characterize creation science as a pseudoscience.
Historical, philosophical, and sociological criticism.
Historically, the debate of whether creationism is compatible with science can be traced back to 1874, the year science historian John William Draper published his "History of the Conflict between Religion and Science". In it Draper portrayed the entire history of scientific development as a war against religion. This presentation of history was propagated further by followers such as Andrew Dickson White in his two-volume "A History of the Warfare of Science with Theology in Christendom" (1896). Their conclusions have been disputed.
In the United States, the principal focus of creation science advocates is on the government-supported public school systems, which are prohibited by the Establishment Clause from promoting specific religions. Historical communities have argued that Biblical translations contain many translation errors and errata, and therefore that the use of biblical literalism in creation science is self-contradictory.
Areas of study.
Subjects within creation science correspond to the scientific disciplines of biology, earth sciences and astronomy.
Creationist biology.
Creation biology centers on an idea derived from Genesis that states that life was created by God, in a finite number of "created kinds," rather than through biological evolution from a common ancestor. Creationists consider that any observable speciation descends from these distinctly created kinds through inbreeding, deleterious mutations and other genetic mechanisms. Whereas evolutionary biologists and creationists share similar views of microevolution, creationists disagree that the process of macroevolution can explain common ancestry among organisms far beyond the level of common species. Creationists contend that there is no empirical evidence for new plant or animal species, and deny fossil evidence has ever been found documenting the process.
Popular arguments against evolution have changed since the publishing of Henry M. Morris' first book on the subject, "Scientific Creationism" (1974), but some consistent themes remain: that missing links or gaps in the fossil record are proof against evolution; that the increased complexity of organisms over time through evolution is not possible due to the law of increasing entropy; that it is impossible that the mechanism of natural selection could account for common ancestry; and that evolutionary theory is untestable. The origin of the human species is particularly hotly contested; the fossil remains of purported hominid ancestors are not considered by advocates of creation biology to be evidence for a speciation event involving "Homo sapiens". Creationists also assert that early hominids, are either apes, or humans.
Richard Dawkins has explained evolution as "a theory of gradual, incremental change over millions of years, which starts with something very simple and works up along slow, gradual gradients to greater complexity," and described the existing fossil record as entirely consistent with that process. Biologists emphasize that transitional gaps between those fossils recovered are to be expected, that the existence of any such gaps cannot be invoked to disprove evolution, and that instead the fossil evidence that could be used to disprove the theory would be those fossils which are found and which are entirely inconsistent with what can be predicted or anticipated by the evolutionary model. One example given by Dawkins was, "If there were a single hippo or rabbit in the Precambrian, that would completely blow evolution out of the water. None have ever been found."
Earth sciences and geophysics.
Flood geology.
Flood geology is a concept based on the belief that most of Earth's geological record was formed by the Great Flood described in the story of Noah's Ark. Fossils and fossil fuels are believed to have formed from animal and plant matter which was buried rapidly during this flood, while submarine canyons are explained as having formed during a rapid runoff from the continents at the end of the flood. Sedimentary strata are also claimed to have been predominantly laid down during or after Noah's flood and orogeny. Flood geology is a variant of catastrophism and is contrasted with geological science in that it rejects standard geological principles such as uniformitarianism and radiometric dating. For example, the Creation Research Society argues that "uniformitarianism is wishful thinking."
Geologists conclude that no evidence for such a flood is observed in the preserved rock layers and moreover that such a flood is physically impossible, given the current layout of land masses. For instance, since Mount Everest currently is approximately 8.8 kilometres in elevation and the Earth's surface area is 510,065,600 km2, the volume of water required to cover Mount Everest to a depth of 15 cubits (6.8 m), as indicated by Genesis 7:20, would be 4.6 billion cubic kilometres. Measurements of the amount of precipitable water vapor in the atmosphere have yielded results indicating that condensing all water vapor in a column of atmosphere would produce liquid water with a depth ranging between zero and approximately 70mm, depending on the date and the location of the column. Nevertheless, there continue to be many adherents to flood geology, and in recent years new theories have been introduced such as catastrophic plate tectonics and catastrophic orogeny.
Radiometric dating.
Creationists point to experiments they have performed, which they claim demonstrate that 1.5 billion years of nuclear decay took place over a short period of time, from which they infer that "billion-fold speed-ups of nuclear decay" have occurred, a massive violation of the principle that radioisotope decay rates are constant, a core principle underlying nuclear physics generally, and radiometric dating in particular.
The scientific community points to numerous flaws in the creationists' experiments, to the fact that their results have not been accepted for publication by any peer-reviewed scientific journal, and to the fact that the creationist scientists conducting them were untrained in experimental geochronology.
The constancy of the decay rates of isotopes is well supported in science. Evidence for this constancy includes the correspondences of date estimates taken from different radioactive isotopes as well as correspondences with non-radiometric dating techniques such as dendrochronology, ice core dating, and historical records. Although scientists have noted slight increases in the decay rate for isotopes subject to extreme pressures, those differences were too small to significantly impact date estimates. The constancy of the decay rates is also governed by first principles in quantum mechanics, wherein any deviation in the rate would require a change in the fundamental constants. According to these principles, a change in the fundamental constants could not influence different elements uniformly, and a comparison between each of the elements' resulting unique chronological timescales would then give inconsistent time estimates.
In refutation of young Earth claims of inconstant decay rates affecting the reliability of radiometric dating, Roger C. Wiens, a physicist specializing in isotope dating states:
Radiohaloes.
In the 1970s, young Earth creationist Robert V. Gentry proposed that radiohaloes in certain granites represented evidence for the Earth being created instantaneously rather than gradually. This idea has been criticized by physicists and geologists on many grounds including that the rocks Gentry studied were not primordial and that the radionuclides in question need not have been in the rocks initially.
Thomas A. Baillieul, a geologist and retired senior environmental scientist with the United States Department of Energy, disputed Gentry's claims in an article entitled, "'Polonium Haloes' Refuted: A Review of 'Radioactive Halos in a Radio-Chronological and Cosmological Perspective' by Robert V. Gentry." Baillieul noted that Gentry was a physicist with no background in geology and given the absence of this background, Gentry had misrepresented the geological context from which the specimens were collected. Additionally, he noted that Gentry relied on research from the beginning of the 20th century, long before radioisotopes were thoroughly understood; that his assumption that a polonium isotope caused the rings was speculative; and that Gentry falsely argued that the half-life of radioactive elements varies with time. Gentry claimed that Baillieul could not publish his criticisms in a reputable scientific journal, although some of Baillieul's criticisms rested on work previously published in reputable scientific journals.
Astronomy and cosmology.
Creationist cosmologies.
Several attempts have been made by creationists to construct a cosmology consistent with a young Universe rather than the standard cosmological age of the universe, based on the belief that Genesis describes the creation of the Universe as well as the Earth. The primary challenge for young-universe cosmologies is that the accepted distances in the Universe require millions or billions of years for light to travel to Earth (the "starlight problem"). An older creationist idea, proposed by creationist astronomer Barry Setterfield, is that the speed of light has decayed in the history of the Universe. More recently, creationist physicist Russell Humphreys has proposed a hypothesis called "white hole cosmology" which suggests that the Universe expanded out of a white hole less than 10,000 years ago; the apparent age of the universe results from relativistic effects. Humphreys' theory is advocated by creationist organisations such as Answers in Genesis; however because the predictions of Humphreys' cosmology conflict with current well-established observations, it is not accepted by the scientific community.
Planetology.
Various claims are made by creationists concerning alleged evidence that the age of the Solar System is of the order of thousands of years, in contrast to the scientifically accepted age of 4.6 billion years. It is commonly argued that the number of comets in the Solar System is much higher than would be expected given its supposed age. Creationist astronomers express scepticism about the existence of the Kuiper belt and Oort cloud. Creationists also argue that the recession of the Moon from the Earth is incompatible with either the Moon or the Earth being billions of years old. These claims have been refuted by planetologists.
In response to increasing evidence suggesting that Mars once possessed a wetter climate, some creationists have proposed that the global flood affected not only the Earth but also Mars and other planets. People who support this claim include creationist astronomer Wayne Spencer and Russell Humphreys.
An ongoing problem for creationists is the presence of impact craters on nearly all Solar System objects, which is consistent with scientific explanations of solar system origins but creates insuperable problems for young Earth claims. Creationists Harold Slusher and Richard Mandock, along with Glenn Morton (who later repudiated this claim) asserted that impact craters on the Moon are subject to rock flow, and so cannot be more than a few thousand years old. While some creationist astronomers assert that different phases of meteoritic bombardment of the Solar System occurred during creation week and during the subsequent Great Flood, others regard this as unsupported by the evidence and call for further research.
External links.
Notable creationist museums in the United States:

</doc>
<doc id="7685" url="http://en.wikipedia.org/wiki?curid=7685" title="List of cartographers">
List of cartographers

Cartography is the study of map making and cartographers are map makers.

</doc>
<doc id="7689" url="http://en.wikipedia.org/wiki?curid=7689" title="Cirth">
Cirth

The Cirth (; "Runes") are the letters of a semi-artificial script which was invented by J. R. R. Tolkien for the constructed languages he devised and used in his works. The initial C in Cirth is pronounced as a K, never as an S. "Cirth" is plural and is written with a capital "C" when referring to the writing system—the runes themselves can be called "cirth". A single rune is a "certh" (). The words "cirth" and "certh" are Sindarin; the corresponding Quenya words are "certar" () and "certa" (). The Sindarin "Certhas" and "Angerthas" mean "runic alphabet" and "long rune-rows" respectively. 
The runic alphabet used by the Dwarves of Middle-earth was adapted by J.R.R. Tolkien from real-life runes. In "The Hobbit", the Anglo-Saxon futhorc was used in the publication with few changes; in "The Lord of the Rings" a new system of runes, the Cirth, was devised.
Writing system.
Since the Cirth are an alphabet, one rune generally stands for one sound (phoneme), and sounds that would be written with a digraph in English (such as "sh" and "th") are written with one rune. Words are separated by a dot rather than a space, and double consonants are grouped together into one rune, the same as if it were a single consonant. Presumably this alphabet was meant to be used in conjunction with a Dwarvish language, but mostly it is used for transliterations.
Many letters have shapes also found in the historical futhorc runes (used in "The Hobbit"), but their sound values are dissimilar. Rather, the system of assignment of sound values is much more systematic in the Cirth than in the historical runes (e.g., voiced variants of a voiceless sound are expressed by an additional stroke). A similar system has been proposed for a few historical runes (e.g. "p" ᛈ and "w" ᚹ as variants of "b" ᛒ), but is in any case much more obscure. There are a few identities between cirth and runic letters, "i" with runic ᛁ, "k" with Younger Futhark ᚴ and "ch" with the Futhorc ᚳ; "p" is furthermore reminiscent of Latin P (runic ᚹ "w").
Cirth are written according to a certain mode specifically adapted for a language, and the values of individual cirth may vary greatly according to the mode used. Three modes for Cirth are described in detail in Appendix E of "The Lord of the Rings", and others are known to exist or have been developed by enthusiasts.
The Cirth are not yet part of the Unicode Standard. However the ConScript Unicode Registry has defined the to U+E0FF range of the Unicode "Private Use Area" for Cirth.
The inscriptions on the top of the title pages for "The Lord of the Rings" are written in Cirth.
Internal history.
In the fictional history of Middle-earth, the original Certhas Daeron was created by the elf Daeron, the minstrel of king Thingol of Doriath and was later expanded into what was known as the Angerthas Daeron. Although the Cirth were later largely replaced by the Tengwar (which were enhanced and brought by Fëanor), they were adopted by Dwarves to write down their Khuzdul language (Angerthas Moria and Angerthas Erebor) because their straight lines were better suited to carving than the curved strokes of the Tengwar. An example of Cirth writings is the inscription on Balin's tomb in Moria. Cirth was also adapted, in its older and simpler form, by various kinds of Men and even Orcs. For example, it was used by the Men of Dale and the Rohirrim and the Orcs of Moria.
Concept and creation.
The division between the older Cirth of Daeron and their adaptation by Dwarves and Men has been interpreted as a parallel drawn by Tolkien to the development of the Futhorc to the Younger Futhark. The original Elvish Cirth "as supposed products of a superior culture" are focused on logical arrangement and a close connection between form and value whereas the adaptations by mortal races introduced irregularities. Similar to the Germanic tribes who had no written literature and used only simple runes before their conversion to Christianity, the Sindar Elves of Beleriand with their Cirth were introduced to the more elaborate Tengwar of Fëanor when the Noldor Elves returned to Middle-earth from the lands of the divine Valar.
Letters.
The Angerthas Daeron consists of 60 letters: 
This chart showing the runes shared by the "Angerthas Daeron" and "Angerthas Moria" is presented in Appendix E of "The Return of the King". Some of the "cirth" had different values for the Elvish and Dwarvish languages, and some were used in only one system or the other. Where two forms appear in the same cell they are simply variants or alternates, with no difference in sound. 
° Elvish use only<br>
† The apostrophe marks "the clear or glottal beginning of a [Dwarvish] word with an initial vowel".

</doc>
<doc id="7697" url="http://en.wikipedia.org/wiki?curid=7697" title="Lockheed C-130 Hercules">
Lockheed C-130 Hercules

The Lockheed C-130 Hercules is a four-engine turboprop military transport aircraft designed and built originally by Lockheed, now Lockheed Martin. Capable of using unprepared runways for takeoffs and landings, the C-130 was originally designed as a troop, medical evacuation, and cargo transport aircraft. The versatile airframe has found uses in a variety of other roles, including as a gunship (AC-130), for airborne assault, search and rescue, scientific research support, weather reconnaissance, aerial refueling, maritime patrol, and aerial firefighting. It is now the main tactical airlifter for many military forces worldwide. Over 40 models and variants of the Hercules serve with more than 60 nations.
The C-130 entered service with U.S. in the 1950s, followed by Australia and others. During its years of service, the Hercules family has participated in numerous military, civilian and humanitarian aid operations. The family has the longest continuous production run of any military aircraft in history. In 2007, the C-130 became the fifth aircraft—after the English Electric Canberra, Boeing B-52 Stratofortress, Tupolev Tu-95, and Boeing KC-135 Stratotanker, all designs with various forms of aviation gas turbine powerplants—to mark 50 years of continuous use with its original primary customer, in this case, the United States Air Force. The C-130 is one of the only military aircraft to remain in continuous production for over 50 years with its original customer, as the updated C-130J Super Hercules.
Design and development.
Background and requirements.
The Korean War, which began in June 1950, showed that World War II-era piston-engine transports—Fairchild C-119 Flying Boxcars, Douglas C-47 Skytrains and Curtiss C-46 Commandos—were inadequate for modern warfare. Thus, on 2 February 1951, the United States Air Force issued a General Operating Requirement (GOR) for a new transport to Boeing, Douglas, Fairchild, Lockheed, Martin, Chase Aircraft, North American, Northrop, and Airlifts Inc. The new transport would have a capacity of 92 passengers, 72 combat troops or 64 paratroopers in a cargo compartment that was approximately long, high, and wide. Unlike transports derived from passenger airliners, it was to be designed from the ground-up as a combat transport with loading from a hinged loading ramp at the rear of the fuselage. This innovation for military cargo aircraft was first pioneered, known as the "Trapoklappe", on the World War II German Junkers Ju 252 and Junkers Ju 352 "Hercules" transport prototypes.
The Hercules resembled a larger four-engine brother to the C-123 Provider with a similar wing and cargo ramp layout that evolved from the Chase XCG-20 Avitruc, which in turn, was first designed and flown as a cargo glider in 1947. The Boeing C-97 Stratofreighter also had a rear ramp, which made it possible to drive vehicles onto the plane (also possible with forward ramp on a C-124). The ramp on the Hercules was also used to airdrop cargo, which included low-altitude extraction for Sheridan tanks and even dropping large improvised "daisy cutter" bombs.
A key feature was the introduction of the Allison T56 turboprop powerplant, first developed specifically for the C-130. At the time, the turboprop was a new application of turbine engines that used exhaust gases to turn a propeller, which offered greater range at propeller-driven speeds compared to pure turbojets, which were faster but thirstier. As was the case on helicopters of that era, such as the UH-1 Huey, turboshafts produced much more power for their weight than piston engines. Lockheed would subsequently use the same engines and technology in the Lockheed L-188 Electra. That aircraft failed financially in its civilian configuration but was successfully adapted into the Lockheed P-3 Orion maritime patrol and submarine attack aircraft where the efficiency and endurance of turboprops excelled.
The new Lockheed cargo plane design possessed a range of , takeoff capability from short and unprepared strips, and the ability to fly with one engine shut down. Fairchild, North American, Martin, and Northrop declined to participate. The remaining five companies tendered a total of ten designs: Lockheed two, Boeing one, Chase three, Douglas three, and Airlifts Inc. one. The contest was a close affair between the lighter of the two Lockheed (preliminary project designation L-206) proposals and a four-turboprop Douglas design.
The Lockheed design team was led by Willis Hawkins, starting with a 130 page proposal for the "Lockheed L-206". Hall Hibbard, Lockheed vice president and chief engineer, saw the proposal and directed it to Kelly Johnson, who did not care for the low-speed, unarmed aircraft, and remarked, "If you sign that letter, you will destroy the Lockheed Company." Both Hibbard and Johnson signed the proposal and the company won the contract for the now-designated Model 82 on 2 July 1951.
The first flight of the "YC-130" prototype was made on 23 August 1954 from the Lockheed plant in Burbank, California. The aircraft, serial number "53-3397", was the second prototype, but the first of the two to fly. The YC-130 was piloted by Stanley Beltz and Roy Wimmer on its 61-minute flight to Edwards Air Force Base; Jack Real and Dick Stanton served as flight engineers. Kelly Johnson flew chase in a Lockheed P2V Neptune.
Into production.
After the two prototypes were completed, production began in Marietta, Georgia, where over 2,300 C-130s have been built through 2009.
The initial production model, the "C-130A", was powered by Allison T56-A-9 turboprops with three-blade propellers and originally equipped with the blunt nose of the prototypes. Deliveries began in December 1956, continuing until the introduction of the "C-130B" model in 1959. Some A models were re-designated "C-130D" after being equipped with skis. The newer C-130B had ailerons with increased boost—3,000 psi (21 MPa) versus 2,050 psi (14 MPa)—as well as uprated engines and four-bladed propellers that were standard until the J-model's introduction.
Improved versions.
As the C-130A became operational with Tactical Air Command (TAC), the C-130's lack of range became apparent and additional fuel capacity was added in the form of external pylon-mounted tanks at the end of the wings.
The C-130B model was developed to complement the A models that had previously been delivered, and incorporated new features, particularly increased fuel capacity in the form of auxiliary tanks built into the center wing section and an AC electrical system. Four-bladed Hamilton Standard propellers replaced the Aeroproducts three-bladed propellers that distinguished the earlier A-models. An electronic reconnaissance variant of the C-130B was designated C-130B-II. A total of 13 aircraft were converted. The C-130B-II was distinguished by its false external wing fuel tanks, which were disguised signals intelligence (SIGINT) receiver antennas. These pods were slightly larger than the standard wing tanks found on other C-130Bs. Most aircraft featured a swept blade antenna on the upper fuselage, as well as extra wire antennas between the vertical fin and upper fuselage not found on other C-130s. Radio call numbers on the tail of these aircraft were regularly changed so as to confuse observers and disguise their true mission.
The extended-range "C-130E" model entered service in 1962 after it was developed as an interim long-range transport for the Military Air Transport Service. Essentially a B-model, the new designation was the result of the installation of 1,360 US gal (5,150 L) "Sargent Fletcher" external fuel tanks under each wing's midsection and more powerful Allison T56-A-7A turboprops. The hydraulic boost pressure to the ailerons was reduced back to 2050 psi as a consequence of the external tanks' weight in the middle of the wingspan. The E model also featured structural improvements, avionics upgrades and a higher gross weight. Australia took delivery of 12 C130E Hercules during 1966–67 to supplement the 12 C-130A models already in service with the RAAF. Sweden and Spain fly the TP-84T version of the C-130E fitted for aerial refueling capability.
Refueling versions.
The "KC-130" tankers, originally "C-130F"s procured for the US Marine Corps (USMC) in 1958 (under the designation "GV-1") are equipped with a removable 3,600 US gal (13,626 l) stainless steel fuel tank carried inside the cargo compartment. The two wing-mounted hose and drogue aerial refueling pods each transfer up to 300 US gal per minute (19 l per second) to two aircraft simultaneously, allowing for rapid cycle times of multiple-receiver aircraft formations, (a typical tanker formation of four aircraft in less than 30 minutes). The US Navy's "C-130G" has increased structural strength allowing higher gross weight operation.
More improvements.
The "C-130H" model has updated Allison T56-A-15 turboprops, a redesigned outer wing, updated avionics and other minor improvements. Later "H" models had a new, fatigue-life-improved, center wing that was retrofitted to many earlier H-models. The H model remains in widespread use with the United States Air Force (USAF) and many foreign air forces. Initial deliveries began in 1964 (to the RNZAF), remaining in production until 1996. An improved C-130H was introduced in 1974, with Australia purchasing 12 of type in 1978 to replace the original 12 C-130A models, which had first entered RAAF Service in 1958.
The United States Coast Guard employs the HC-130H for long-range search and rescue, drug interdiction, illegal migrant patrols, homeland security, and logistics.
C-130H models produced from 1992 to 1996 were designated as C-130H3 by the USAF. The "3" denoting the third variation in design for the H series. Improvements included ring laser gyros for the INUs, GPS receivers, a partial glass cockpit (ADI and HSI instruments), a more capable APN-241 color radar, night vision device compatible instrument lighting, and an integrated radar and missile warning system. The electrical system upgrade included Generator Control Units (GCU) and Bus Switching units (BSU)to provide stable power to the more sensitive upgraded components.
The equivalent model for export to the UK is the "C-130K", known by the Royal Air Force (RAF) as the "Hercules C.1". The "C-130H-30" ("Hercules C.3" in RAF service) is a stretched version of the original Hercules, achieved by inserting a 100 in (2.54 m) plug aft of the cockpit and an 80 in (2.03 m) plug at the rear of the fuselage. A single C-130K was purchased by the Met Office for use by its Meteorological Research Flight, where it was classified as the "Hercules W.2". This aircraft was heavily modified (with its most prominent feature being the long red and white striped atmospheric probe on the nose and the move of the weather radar into a pod above the forward fuselage). This aircraft, named "Snoopy", was withdrawn in 2001 and was then modified by Marshall of Cambridge Aerospace as flight-testbed for the A400M turbine engine, the TP400. The C-130K is used by the RAF Falcons for parachute drops. Three C-130K (Hercules C Mk.1P) were upgraded and sold to the Austrian Air Force in 2002.
Later models.
The "MC-130E Combat Talon" was developed for the USAF during the Vietnam War to support special operations missions throughout Southeast Asia, and spawned both an "MC-130H Combat Talon II" as well as a family of other special missions aircraft. 37 of the earliest models currently operating with the Air Force Special Operations Command (AFSOC) are scheduled to be replaced by new-production MC-130J versions. The EC-130 Commando Solo is another special missions variant within AFSOC, albeit operated solely by an AFSOC-gained wing in the Pennsylvania Air National Guard, and is a psychological operations/information operations (PSYOP/IO) platform equipped as an aerial radio station and television stations able to transmit messaging over commercial frequencies. Other versions of the EC-130, most notably the EC-130H Compass Call, are also special variants, but are assigned to the Air Combat Command (ACC). The AC-130 gunship was first developed during the Vietnam War to provide close air support and other ground-attack duties.
The "HC-130" is a family of long-range search and rescue variants used by the USAF and the U.S. Coast Guard. Equipped for deep deployment of Pararescuemen (PJs), survival equipment, and (in the case of USAF versions) aerial refueling of combat rescue helicopters, HC-130s are usually the on-scene command aircraft for combat SAR missions (USAF only) and non-combat SAR (USAF and USCG). Early USAF versions were also equipped with the Fulton surface-to-air recovery system, designed to pull a person off the ground using a wire strung from a helium balloon. The John Wayne movie "The Green Berets" features its use. The Fulton system was later removed when aerial refueling of helicopters proved safer and more versatile. The movie "The Perfect Storm" depicts a real life SAR mission involving aerial refueling of a New York Air National Guard HH-60G by a New York Air National Guard HC-130P.
The "C-130R" and "C-130T" are U.S. Navy and USMC models, both equipped with underwing external fuel tanks. The USN C-130T is similar, but has additional avionics improvements. In both models, aircraft are equipped with Allison T56-A-16 engines. The USMC versions are designated "KC-130R" or "KC-130T" when equipped with underwing refueling pods and pylons and are fully night vision system compatible.
The RC-130 is a reconnaissance version. A single example is used by the Islamic Republic of Iran Air Force, the aircraft having originally been sold to the former Imperial Iranian Air Force.
The "Lockheed L-100 (L-382)" is a civilian variant, equivalent to a C-130E model without military equipment. The L-100 also has two stretched versions.
Next generation.
In the 1970s, Lockheed proposed a C-130 variant with turbofan engines rather than turboprops, but the U.S. Air Force preferred the takeoff performance of the existing aircraft. In the 1980s, the C-130 was intended to be replaced by the Advanced Medium STOL Transport project. The project was canceled and the C-130 has remained in production.
Building on lessons learned, Lockheed Martin modified a commercial variant of the C-130 into a High Technology Test Bed (HTTB). This test aircraft set numerous short takeoff and landing performance records and significantly expanded the database for future derivatives of the C-130. Modifications made to the HTTB included extended chord ailerons, a long chord rudder, fast-acting double-slotted trailing edge flaps, a high-camber wing leading edge extension, a larger dorsal fin and dorsal fins, the addition of three spoiler panels to each wing upper surface, a long-stroke main and nose landing gear system, and changes to the flight controls and a change from direct mechanical linkages assisted by hydraulic boost, to fully powered controls, in which the mechanical linkages from the flight station controls operated only the hydraulic control valves of the appropriate boost unit. The HTTB first flew on 19 June 1984, with civil registration of N130X. After demonstrating many new technologies, some of which were applied to the C-130J, the HTTB was lost in a fatal accident on 3 February 1993, at Dobbins Air Reserve Base, in Marietta, Georgia. The crash was attributed to disengagement of the rudder fly-by-wire flight control system, resulting in a total loss of rudder control capability while conducting ground minimum control speed tests (Vmcg). The disengagement was a result of the inadequate design of the rudder's integrated actuator package by its manufacturer; the operator's insufficient system safety review failed to consider the consequences of the inadequate design to all operating regimes. A factor which contributed to the accident was the flight crew's lack of engineering flight test training.
In the 1990s, the improved C-130J Super Hercules was developed by Lockheed (later Lockheed Martin). This model is the newest version and the only model in production. Externally similar to the classic Hercules in general appearance, the J model has new turboprop engines, six-bladed propellers, digital avionics, and other new systems.
Upgrades and changes.
In 2000, Boeing was awarded a contract to develop an Avionics Modernization Program kit for the C-130. The program was beset with delays and cost overruns until project restructuring in 2007. On 2 September 2009, Bloomberg news reported that the planned Avionics Modernization Program (AMP) upgrade to the older C-130s would be dropped to provide more funds for the F-35, CV-22 and airborne tanker replacement programs. However, in June 2010, Department of Defense approved funding for the initial production of the AMP upgrade kits. Under the terms of this agreement, the USAF has cleared Boeing to begin low-rate initial production (LRIP) for the C-130 AMP. A total of 198 aircraft are expected to feature the AMP upgrade. The current cost per aircraft is although Boeing expects that this price will drop to US$7 million for the 69th aircraft.
An engine enhancement program saving fuel and providing lower temperatures in the T56 engine has been approved, and the US Air Force expects to save $2 billion and extend the fleet life.
Replacement.
In October 2010, the Air Force released a capabilities request for information (CRFI) for the development of a new airlifter to replace the C-130. The new aircraft is to carry a 190 percent greater payload and assume the mission of mounted vertical maneuver (MVM). The greater payload and mission would enable it to carry medium-weight armored vehicles and drop them off at locations without long runways. Various options are being considered, including new or upgraded fixed-wing designs, rotorcraft, tiltrotors, or even an airship. Development could start in 2014, and become operational by 2024. The C-130 fleet of around 450 planes would be replaced by only 250 aircraft. The Air Force had attempted to replace the C-130 in the 1970s through the Advanced Medium STOL Transport project, which resulted in the C-17 Globemaster III that instead replaced the C-141 Starlifter. The Air Force Research Laboratory funded Lockheed and Boeing demonstrators for the Speed Agile concept, which had the goal of making a STOL aircraft that can take off and land at speeds as low as on airfields less than 2,000 ft long and cruise at Mach 0.8-plus. Boeing's design used upper-surface blowing from embedded engines on the inboard wing and blown flaps for circulation control on the outboard wing. Lockheed's design also used blown flaps outboard, but inboard used patented reversing ejector nozzles. Boeing's design completed over 2,000 hours of windtunnel tests in late 2009. It was a 5 percent-scale model of a narrowbody design with a payload. When the AFRL increased the payload requirement to , they tested a 5% scale model of a widebody design with a take-off gross weight and an "A400M-size" wide cargo box. It would be powered by four IAE V2533 turbofans. In August 2011, the AFRL released pictures of the Lockheed Speed Agile concept demonstrator. A 23% scale model went through wind tunnel tests to demonstrate its hybrid powered lift, which combines a low drag airframe with simple mechanical assembly to reduce weight and better aerodynamics. The model had four engines, including two Williams FJ44 turbofans. On 26 March 2013, Boeing was granted a patent for its swept-wing powered lift aircraft.
As of January 2014, Air Mobility Command, Air Force Materiel Command and the Air Force Research Lab are in the early stages of defining requirements for the C-X next generation airlifter program to replace both the C-130 and C-17. An aircraft would be produced from the early 2030s to the 2040s. If requirements are decided for operating in contested airspace, Air Force procurement of C-130s would end by the end of the decade to not have them serviceable by the 2030s and operated when they can't perform in that environment. Development of the airlifter depends heavily on the Army's "tactical and operational maneuver" plans. Two different cargo planes could still be created to separately perform tactical and strategic missions, but which course to pursue is to be decided before C-17s need to be retired.
Operational history.
Military.
The first production aircraft, C-130As were first delivered beginning in 1956 to the 463d Troop Carrier Wing at Ardmore AFB, Oklahoma and the 314th Troop Carrier Wing at Sewart AFB, Tennessee. Six additional squadrons were assigned to the 322d Air Division in Europe and the 315th Air Division in the Far East. Additional aircraft were modified for electronics intelligence work and assigned to Rhein-Main Air Base, Germany while modified RC-130As were assigned to the Military Air Transport Service (MATS) photo-mapping division.
In 1958, a U.S. reconnaissance C-130A-II of the 7406th Support Squadron was shot down over Armenia by MiG-17s.
Australia became the first non-American force to operate the C-130A Hercules with 12 examples being delivered from late 1958. These aircraft were fitted with AeroProducts three-blade, 15-foot diameter propellers. The Royal Canadian Air Force became another early user with the delivery of four B-models (Canadian designation C-130 Mk I) in October / November 1960.
In 1963, a Hercules achieved and still holds the record for the largest and heaviest aircraft to land on an aircraft carrier. During October and November that year, a USMC KC-130F (BuNo "149798"), loaned to the U.S. Naval Air Test Center, made 29 touch-and-go landings, 21 unarrested full-stop landings and 21 unassisted take-offs on at a number of different weights. The pilot, LT (later RADM) James H. Flatley III, USN, was awarded the Distinguished Flying Cross for his role in this test series. The tests were highly successful, but the idea was considered too risky for routine "Carrier Onboard Delivery" (COD) operations. Instead, the Grumman C-2 Greyhound was developed as a dedicated COD aircraft. The Hercules used in the test, most recently in service with Marine Aerial Refueler Squadron 352 (VMGR-352) until 2005, is now part of the collection of the National Museum of Naval Aviation at NAS Pensacola, Florida.
In 1964, C-130 crews from the 6315th Operations Group at Naha Air Base, Okinawa commenced forward air control (FAC; "Flare") missions over the Ho Chi Minh Trail in Laos supporting USAF strike aircraft. In April 1965 the mission was expanded to North Vietnam where C-130 crews led formations of B-57 bombers on night reconnaissance/strike missions against communist supply routes leading to South Vietnam. In early 1966 Project Blind Bat/Lamplighter was established at Ubon RTAFB, Thailand. After the move to Ubon the mission became a four-engine FAC mission with the C-130 crew searching for targets then calling in strike aircraft. Another little-known C-130 mission flown by Naha-based crews was Operation "Commando Scarf", which involved the delivery of chemicals onto sections of the Ho Chi Minh Trail in Laos that were designed to produce mud and landslides in hopes of making the truck routes impassable.
In November 1964, on the other side of the globe, C-130Es from the 464th Troop Carrier Wing but loaned to 322d Air Division in France, flew one of the most dramatic missions in history in the former Belgian Congo. After communist Simba rebels took white residents of the city of Stanleyville hostage, the U.S. and Belgium developed a joint rescue mission that used the C-130s to airlift and then drop and air-land a force of Belgian paratroopers to rescue the hostages. Two missions were flown, one over Stanleyville and another over Paulis during Thanksgiving weeks. The headline-making mission resulted in the first award of the prestigious MacKay Trophy to C-130 crews.
In the Indo-Pakistani War of 1965, the Pakistan Air Force modified/improvised several aircraft for use as heavy bombers, and attacks were made on Indian bridges and troop concentrations with some successes. No aircraft were lost in the operations, though one was slightly damaged.
In October 1968, a C-130Bs from the 463rd Tactical Airlift Wing dropped a pair of M-121 10,000 pound bombs that had been developed for the massive B-36 bomber but had never been used. The U.S. Army and U.S. Air Force resurrected the huge weapons as a means of clearing landing zones for helicopters and in early 1969 the 463rd commenced Commando Vault missions. Although the stated purpose of COMMANDO VAULT was to clear LZs, they were also used on enemy base camps and other targets.
During the late 1960s, the U.S. was eager to get information on Chinese nuclear capabilities. After the failure of the Black Cat Squadron to plant operating sensor pods near the Lop Nur Nuclear Weapons Test Base using a Lockheed U-2, the CIA developed a plan, named "Heavy Tea", to deploy two battery-powered sensor pallets near the base. To deploy the pallets, a Black Bat Squadron crew was trained in the U.S. to fly the C-130 Hercules. The crew of 12, led by Col Sun Pei Zhen, took off from Takhli Royal Thai Air Force Base in an unmarked U.S. Air Force C-130E on 17 May 1969. Flying for six and a half hours at low altitude in the dark, they arrived over the target and the sensor pallets were dropped by parachute near Anxi in Gansu province. After another six and a half hours of low altitude flight, they arrived back at Takhli. The sensors worked and uploaded data to a U.S. intelligence satellite for six months, before their batteries wore out. The Chinese conducted two nuclear tests, on 22 September 1969 and 29 September 1969, during the operating life of the sensor pallets. Another mission to the area was planned as Operation "Golden Whip", but was called off in 1970. It is most likely that the aircraft used on this mission was either C-130E serial number 64-0506 or 64-0507 (cn 382-3990 and 382-3991). These two aircraft were delivered to Air America in 1964. After being returned to the U.S. Air Force sometime between 1966 and 1970, they were assigned the serial numbers of C-130s that had been destroyed in accidents. 64-0506 is now flying as 62-1843, a C-130E that crashed in Vietnam on 20 December 1965 and 64-0507 is now flying as 63-7785, a C-130E that had crashed in Vietnam on 17 June 1966.
The A-model continued in service through the Vietnam War, where the aircraft assigned to the four squadrons at Naha AB, Okinawa and one at Tachikawa Air Base, Japan performed yeoman's service, including operating highly classified special operations missions such as the BLIND BAT FAC/Flare mission and FACT SHEET leaflet mission over Laos and North Vietnam. The A-model was also provided to the South Vietnamese Air Force as part of the Vietnamization program at the end of the war, and equipped three squadrons based at Tan Son Nhut AFB. The last operator in the world is the Honduran Air Force, which is still flying one of five A model Hercules (FAH "558", c/n 3042) as of October 2009. As the Vietnam War wound down, the 463rd Troop Carrier/Tactical Airlift Wing B-models and A-models of the 374th Tactical Airlift Wing were transferred back to the United States where most were assigned to Air Force Reserve and Air National Guard units.
Another prominent role for the B model was with the United States Marine Corps, where Hercules initially designated as GV-1s replaced C-119s. After Air Force C-130Ds proved the type's usefulness in Antarctica, the U.S. Navy purchased a number of B-models equipped with skis that were designated as LC-130s. C-130B-II electronic reconnaissance aircraft were operated under the SUN VALLEY program name primarily from Yokota Air Base, Japan. All reverted to standard C-130B cargo aircraft after their replacement in the reconnaissance role by other aircraft.
The C-130 was also used in the 1976 Entebbe raid in which Israeli commando forces carried a surprise assault to rescue 103 passengers of an airliner hijacked by Palestinian and German terrorists at Entebbe Airport, Uganda. The rescue force — 200 soldiers, jeeps, and a black Mercedes-Benz (intended to resemble Ugandan Dictator Idi Amin's vehicle of state) — was flown over almost entirely at an altitude of less than from Israel to Entebbe by four Israeli Air Force (IAF) Hercules aircraft without mid-air refueling (on the way back, the planes refueled in Nairobi, Kenya).
During the Falklands War () of 1982, Argentine Air Force C-130s undertook highly dangerous, daily re-supply night flights as blockade runners to the Argentine garrison on the Falkland Islands. They also performed daylight maritime survey flights. One was lost during the war. Argentina also operated two KC-130 tankers during the war, and these refueled both the Douglas A-4 Skyhawks and Navy Dassault-Breguet Super Étendards; some C-130s were modified to operate as bombers with bomb-racks under their wings. The British also used RAF C-130s to support their logistical operations.
During the Gulf War of 1991 (Operation "Desert Storm"), the C-130 Hercules was used operationally by the U.S. Air Force, U.S. Navy and U.S. Marine Corps, along with the air forces of Australia, New Zealand, Saudi Arabia, South Korea and the UK. The MC-130 Combat Talon variant also made the first attacks using the largest conventional bombs in the world, the BLU-82 "Daisy Cutter" and GBU-43/B Massive Ordnance Air Blast bomb, also known as the MOAB. Daisy Cutters were used to clear landing zones and to eliminate mine fields. The weight and size of the weapons make it impossible or impractical to load them on conventional bombers. The GBU-43/B MOAB is a successor to the BLU-82 and can perform the same function, as well as perform strike functions against hardened targets in a low air threat environment.
Since 1992, two successive C-130 aircraft named "Fat Albert" have served as the support aircraft for the U.S. Navy Blue Angels flight demonstration team. "Fat Albert I" was a TC-130G ("15189"), while "Fat Albert II" is a C-130T ("164763"). Although "Fat Albert" supports a Navy squadron, it is operated by the U.S. Marine Corps (USMC) and its crew consists solely of USMC personnel. At some air shows featuring the team, "Fat Albert" takes part, performing flyovers. Until 2009, it also demonstrated its rocket-assisted takeoff (RATO) capabilities; these ended due to dwindling supplies of rockets.
The AC-130 also holds the record for the longest sustained flight by a C-130. From 22 to 24 October 1997, two AC-130U gunships flew 36 hours nonstop from Hurlburt Field Florida to Taegu (Daegu), South Korea while being refueled seven times by KC-135 tanker aircraft. This record flight shattered the previous record longest flight by over 10 hours while the two gunships took on of fuel. The gunship has been used in every major U.S. combat operation since Vietnam, except for Operation "El Dorado Canyon", the 1986 attack on Libya.
During the invasion of Afghanistan in 2001 and the ongoing support of the International Security Assistance Force (Operation "Enduring Freedom"), the C-130 Hercules has been used operationally by Australia, Belgium, Canada, Denmark, France, Italy, the Netherlands, New Zealand, Norway, Portugal, South Korea, Spain, the UK and the United States.
During the 2003 invasion of Iraq (Operation "Iraqi Freedom"), the C-130 Hercules was used operationally by Australia, the UK and the United States. After the initial invasion, C-130 operators as part of the Multinational force in Iraq used their C-130s to support their forces in Iraq.
Since 2004, the Pakistan Air Force has employed C-130s in the War in North-West Pakistan. Some variants had forward looking infrared (FLIR Systems Star Safire III EO/IR) sensor balls, to enable close tracking of Islamist militants.
Civilian.
The U.S. Forest Service developed the Modular Airborne FireFighting System for the C-130 in the 1970s, which allows regular aircraft to be temporarily converted to an airtanker for fighting wildfires. In the late 1980s, 22 retired USAF C-130As were removed from storage at Davis-Monthan Air Force Base and transferred to the U.S. Forest Service who then sold them to six private companies to be converted into air tankers (see U.S. Forest Service airtanker scandal). After one of these aircraft crashed due to wing separation in flight as a result of fatigue stress cracking, the entire fleet of C-130A air tankers was permanently grounded in 2004 (see 2002 airtanker crashes). C-130s have been used to spread chemical dispersants onto the massive oil slick in the Gulf Coast in 2010.
Variants.
Significant military variants of the C-130 include:
Accidents.
The C-130 Hercules has had a low accident rate in general. The Royal Air Force recorded an accident rate of about one aircraft loss per 250,000 flying hours over the last 40 years, placing it behind Vickers VC10s and Lockheed TriStars with no flying losses. USAF C-130A/B/E-models had an overall attrition rate of 5% as of 1989 as compared to 1-2% for commercial airliners in the U.S., according to the NTSB, 10% for B-52 bombers, and 20% for fighters (F-4, F-111), trainers (T-37, T-38), and helicopters (H-3).
A total of 70 aircraft were lost by the U.S. Air Force and the U.S. Marine Corps during combat operations in the Vietnam War in Southeast Asia. By the nature of the Hercules' worldwide service, the pattern of losses provides an interesting barometer of the global hot spots over the past 50 years.
On 17 August 1988, then President of Pakistan, General Zia-ul-Haq was killed along with the then U.S. Ambassador to Pakistan, Arnold Lewis Raphel, when a Pakistan Air Force C-130 carrying them crashed soon after takeoff from Bahawalpur, Pakistan.
References.
Notes
Citations
Bibliography

</doc>
<doc id="7699" url="http://en.wikipedia.org/wiki?curid=7699" title="Commodore 1570">
Commodore 1570

The Commodore 1570 is a 5¼" floppy disk drive for the Commodore 128 home/personal computer. It is a single-sided, 170KB version of the double-sided Commodore 1571, released as a stopgap measure when Commodore International was unable to provide large enough quantities of 1571s due to a shortage of double-sided drive mechanisms (supplied from an outside manufacturer). Like the 1571, it can read and write both GCR and MFM disk formats.
The 1570 utilizes a 1571 logic board in a cream-colored Commodore 1541 case with a drive mechanism similar to the 1541's except that it was equipped with track-zero detection. Like the 1571, its built-in DOS provided a data burst mode for transferring data to the C128 computer at a faster speed than a 1541 can. Its ROM also contains some DOS bug fixes that didn't appear in the 1571 until much later. The 1570 can read and write all single-sided CP/M-format disks that the 1571 can access.
Although the 1570 is compatible with the Commodore 64, the C64 isn't capable of taking advantage of the drive's higher-speed operation, and when used with the C64 it's little more than a pricier 1541. Also, many early buyers of the C128 chose to temporarily make do with a 1541 drive, perhaps owned as part of a previous C64 setup, until the 1571 became more widely available.

</doc>
<doc id="7700" url="http://en.wikipedia.org/wiki?curid=7700" title="Commodore 1571">
Commodore 1571

The Commodore 1571 is Commodore's high-end 5¼" floppy disk drive. With its double-sided drive mechanism, it has the ability to utilize double-sided, double-density (DS/DD) floppy disks natively. This is in contrast to its predecessors, the 1541 and 1570, which can fully utilize such disks only if the user manually flipped them over to access the second side. (However, the two methods are not interchangeable; disks which had their back side created in a 1541 by flipping them over would have to be flipped in the 1571 too, and the back side of disks written in a 1571 using the native support for two-sided operation could not be read in a 1541).
Release & features.
The 1571 was released to match the Commodore 128, both design-wise and feature-wise. It was announced in the summer of 1985, at the same time as the C128, and became available in quantity later that year. The later C128"D" had a 1571-compatible drive integrated in the system unit. A double-sided disk on the 1571 would have a capacity of 340 kB (70 tracks, 1,360 disk blocks of 256 bytes each); as 8 kB are reserved for system use (directory and block availability information) and, under of each block serve as pointers to the next logical block, = 337,312 B or about were available for user data. (However, with a program organizing disk storage on its own, all space could be used, e.g. for data disks.)
The 1571 features a "burst mode" when used in conjunction with the C128 (although not when used with the Commodore 64 or VIC-20). This mode replaced the slow bit-banging serial routines of the 1541 with a true serial shift register implemented in hardware, thus dramatically increasing the drive speed. Although this originally had been planned when Commodore first switched from the parallel IEEE-488 interface to a custom serial interface, hardware bugs in the VIC-20's 6522 VIA shift register prevented it from working properly.
For compatibility with copy-protected software, the 1571 could closely emulate the 1541. This mode was the default when the drive was used in conjunction with a C64; while always being able to read and write the 1541's of single-sided, in this mode it also would format disks single-sided and transfer data at 1541 speed. An undocumented command allowed the drive to format and use the second side of a disk, but only in single-sided mode.
The 1571 was noticeably quieter than its predecessor and tended to run cooler as well, even though, like the 1541, it had an internal power supply (later Commodore drives, like the 1541-II and the 3½" 1581, came with external power supplies). The 1541-II/1581 power supply makes mention of a 1571-II, hinting that Commodore may have intended to release a version of the 1571 with an external power supply. However, no 1571-IIs are known to exist. The embedded OS in the 1571 was an improvement over the 
Early 1571s had a bug in the ROM-based disk operating system that caused relative files to corrupt if they occupied both sides of the disk. A version 2 ROM was released, but though it cured the initial bug, it introduced some minor quirks of its own - particularly with the 1541 emulation. Curiously, it was also identified as V3.0.
As with the 1541, Commodore initially could not meet demand for the 1571, and that lack of availability and the drive's relatively high price (about US$300) presented an opportunity for cloners. Two 1571 clones appeared, one from Oceanic and one from Blue Chip, but legal action from Commodore quickly drove them from the market.
Commodore announced a dual-drive version of the 1571, to be called the 1572, but quickly canceled it, reportedly due to technical difficulties with the 1572 DOS. It would have had four times as much RAM as the 1571, and twice as much ROM. The 1572 would have allowed for fast disk backups of non-copy-protected media, much like the old 4040, 8050, and 8250 dual drives.
The 1571 built into the European plastic-case C128 D computer is electronically identical to the stand-alone version, but 1571 version integrated into the later metal-case C128 D (often called C128 DCR, for D Cost-Reduced) differs a lot from the stand-alone 1571. It includes a newer DOS, version 3.1, replaces the MOS Technology CIA interface chip, of which only a few features were used by the 1571 DOS, with a very much simplified chip called 5710, and has some compatibility issues with the stand-alone drive. Because this internal 1571 does not have an unused 8-bit input/output port on any chip, unlike most other Commodore drives, it is not possible to install a parallel cable in this drive, such as that used by SpeedDOS, Dolphin DOS and some other fast third-party Commodore DOS replacements.
Disk format.
Unlike the 1541, which was limited to GCR formatting, the 1571 could do both GCR and MFM disk formats. A C128 in CP/M mode equipped with a 1571 was capable of reading and writing floppy disks formatted for many CP/M computers; specifically, the following formats:
Other MFM formats were possible if their characteristics were added to the CP/M C128-specific source code (available from Commodore) and the CP/M operating system were re-assembled. However, booting CP/M was only supported from disks in the standard Commodore GCR format; the MFM formats could only be used once the system was running.
Depending on format, CP/M disks would format to with a mechanical maximum capacity of a format (as with generally). 
With additional software, it was possible to read and write to MS-DOS-formatted floppies as well. Numerous commercial and public-domain programs for this purpose became available, the best-known being SOGWAP's "Big Blue Reader". Although the C128 could not run any DOS-based software, this capability allowed data files to be exchanged with PC users. Reading or disks was possible as well with special software, but the standard format, which used FM rather than MFM encoding, could not be handled by the 1571 hardware without modifying the drive circuitry as the control line that determines if FM or MFM encoding is used by the disc controller chip was permanently wired to ground (MFM mode) rather than being under software control.
In the 1541 format, while 40 tracks are possible for a drive like the 154x/157x, only are used. Commodore chose not to use the upper five tracks by default (or at least to use more than 35) due to the bad quality of some of the drive mechanisms, which did not always work reliably on those tracks. By reducing the number of tracks used (and thus the capacity), Commodore could further reduce cost - in contrast to the double-density drives used e.g. in IBM PCs of the day which saved 180 kB on one side (by using a 40-track format).
For compatibility and ease of implementation, the 1571's double-sided format of one logical disk side with was created by putting together the lower 35 physical tracks on each of the physical sides of the disk rather than using two times even though there were no more quality problems with the mechanisms of the 1571 drives.

</doc>
<doc id="7701" url="http://en.wikipedia.org/wiki?curid=7701" title="Cocaine">
Cocaine

Cocaine (INN) (benzoylmethylecgonine, an ecgonine derivative) is a tropane alkaloid that is obtained from the leaves of the coca plant. The name comes from "coca" and the alkaloid suffix "-ine", forming "cocaine". It is a stimulant, an appetite suppressant, and a nonspecific voltage gated sodium channel blocker, which in turn causes it to produce anaesthesia at low doses. Biologically, cocaine acts as a serotonin–norepinephrine–dopamine reuptake inhibitor, also known as a triple reuptake inhibitor (TRI). It is addictive due to its effect on the mesolimbic reward pathway. At high doses, it is markedly more dangerous than other CNS stimulants, including the entire amphetamine drug class, due to its effect on sodium channels, since blockade of Nav1.5 can cause sudden cardiac death.
Unlike most molecules, cocaine has pockets with both high hydrophilic and lipophilic efficiency, violating the rule of hydrophilic-lipophilic balance. This causes it to cross the blood–brain barrier far better than other psychoactive chemicals and may even induce blood-brain barrier breakdown.
It is controlled internationally by the Single Convention on Narcotic Drugs (Schedule I, preparation in Schedule III).
Medical effects.
Cocaine is a powerful nervous system stimulant. Its effects can last from fifteen to thirty minutes, to an hour. That is all depending on the amount of the intake dosage and the route of administration. Cocaine can be in the form of fine white powder, bitter to the taste. When inhaled or injected, it causes a numbing effect. “Crack” cocaine is a smokeable form of cocaine made into small “rocks” by processing cocaine with sodium bicarbonate (baking soda) and water.
Cocaine increases alertness, feelings of well-being and euphoria, energy and motor activity, feelings of competence and sexuality. Anxiety, paranoia and restlessness can also occur, especially during the comedown. With excessive dosage, tremors, convulsions and increased body temperature are observed. Severe cardiac adverse events, particularly sudden cardiac death, become a serious risk at high doses due to cocaine's blocking effect on cardiac sodium channels.
Acute.
With excessive or prolonged use, the drug can cause itching, tachycardia, hallucinations, and paranoid delusions. Overdoses cause hyperthermia and a marked elevation of blood pressure, which can be life-threatening, arrhythmias, and death.
Chronic.
Chronic cocaine intake causes strong imbalances of transmitter levels in order to compensate extremes. Thus, receptors disappear from the cell surface or reappear on it, resulting more or less in an "off" or "working mode" respectively, or they change their susceptibility for binding partners (ligands)mechanisms called down-/upregulation. However, studies suggest cocaine abusers do not show normal age-related loss of striatal dopamine transporter (DAT) sites, suggesting cocaine has neuroprotective properties for dopamine neurons. Possible side effects include insatiable hunger, aches, insomnia/oversleeping, lethargy, and persistent runny nose. Depression with suicidal ideation may develop in very heavy users. Finally, a loss of vesicular monoamine transporters, neurofilament proteins, and other morphological changes appear to indicate a long term damage of dopamine neurons. All these effects contribute a rise in tolerance thus requiring a larger dosage to achieve the same effect.
The lack of normal amounts of serotonin and dopamine in the brain is the cause of the dysphoria and depression felt after the initial high. Physical withdrawal is not dangerous. Physiological changes caused by cocaine withdrawal include vivid and unpleasant dreams, insomnia or hypersomnia, increased appetite and psychomotor retardation or agitation.
Physical side effects from chronic smoking of cocaine include hemoptysis, bronchospasm, pruritus, fever, diffuse alveolar infiltrates without effusions, pulmonary and systemic eosinophilia, chest pain, lung trauma, sore throat, asthma, hoarse voice, dyspnea (shortness of breath), and an aching, flu-like syndrome. Cocaine constricts blood vessels, dilates pupils, and increases body temperature, heart rate, and blood pressure. It can also cause headaches and gastrointestinal complications such as abdominal pain and nausea. A common but untrue belief is that the smoking of cocaine chemically breaks down tooth enamel and causes tooth decay. However, cocaine does often cause involuntary tooth grinding, known as bruxism, which can deteriorate tooth enamel and lead to gingivitis. Additionally, stimulants like cocaine, methamphetamine, and even caffeine cause dehydration and dry mouth. Since saliva is an important mechanism in maintaining one's oral pH level, chronic stimulant abusers who do not hydrate sufficiently may experience demineralization of their teeth due to the pH of the tooth surface dropping too low (below 5.5).
Chronic intranasal usage can degrade the cartilage separating the nostrils (the septum nasi), leading eventually to its complete disappearance. Due to the absorption of the cocaine from cocaine hydrochloride, the remaining hydrochloride forms a dilute hydrochloric acid.
Cocaine may also greatly increase this risk of developing rare autoimmune or connective tissue diseases such as lupus, Goodpasture's disease, vasculitis, glomerulonephritis, Stevens–Johnson syndrome and other diseases. It can also cause a wide array of kidney diseases and renal failure.
Cocaine misuse doubles both the risks of hemorrhagic and ischemic strokes, as well as increases the risk of other infarctions, such as myocardial infarction.
Addiction.
Cocaine dependence (or addiction) is psychological dependency on the regular use of cocaine. 
Biosynthesis.
The first synthesis and elucidation of the cocaine molecule was by Richard Willstätter in 1898. Willstätter's synthesis derived cocaine from tropinone. Since then, Robert Robinson and Edward Leete have made significant contributions to the mechanism of the synthesis. (-NO3)
The additional carbon atoms required for the synthesis of cocaine are derived from acetyl-CoA, by addition of two acetyl-CoA units to the "N"-methyl-Δ1-pyrrolinium cation. The first addition is a Mannich-like reaction with the enolate anion from acetyl-CoA acting as a nucleophile towards the pyrrolinium cation. The second addition occurs through a Claisen condensation. This produces a racemic mixture of the 2-substituted pyrrolidine, with the retention of the thioester from the Claisen condensation. In formation of tropinone from racemic ethyl [2,3-13C2]4(Nmethyl-2-pyrrolidinyl)-3-oxobutanoate there is no preference for either stereoisomer. In the biosynthesis of cocaine, however, only the (S)-enantiomer can cyclize to form the tropane ring system of cocaine. The stereoselectivity of this reaction was further investigated through study of prochiral methylene hydrogen discrimination. This is due to the extra chiral center at C-2. This process occurs through an oxidation, which regenerates the pyrrolinium cation and formation of an enolate anion, and an intramolecular Mannich reaction. The tropane ring system undergoes hydrolysis, SAM-dependent methylation, and reduction via NADPH for the formation of methylecgonine. The benzoyl moiety required for the formation of the cocaine diester is synthesized from phenylalanine via cinnamic acid. Benzoyl-CoA then combines the two units to form cocaine.
"N"-methyl-pyrrolinium cation.
The biosynthesis begins with L-Glutamine, which is derived to L-ornithine in plants. The major contribution of L-ornithine and L-arginine as a precursor to the tropane ring was confirmed by Edward Leete. Ornithine then undergoes a Pyridoxal phosphate-dependent decarboxylation to form putrescine. In animals, however, the urea cycle derives putrescine from ornithine. L-ornithine is converted to L-arginine, which is then decarboxylated via PLP to form agmatine. Hydrolysis of the imine derives "N"-carbamoylputrescine followed with hydrolysis of the urea to form putrescine. The separate pathways of converting ornithine to putrescine in plants and animals have converged. A SAM-dependent "N"-methylation of putrescine gives the "N"-methylputrescine product, which then undergoes oxidative deamination by the action of diamine oxidase to yield the aminoaldehyde. Schiff base formation confirms the biosynthesis of the "N"-methyl-Δ1-pyrrolinium cation.
Robert Robinson's acetonedicarboxylate.
The biosynthesis of the tropane alkaloid, however, is still uncertain. Hemscheidt proposes that Robinson's acetonedicarboxylate emerges as a potential intermediate for this reaction. Condensation of "N"-methylpyrrolinium and acetonedicarboxylate would generate the oxobutyrate. Decarboxylation leads to tropane alkaloid formation.
Reduction of tropinone.
The reduction of tropinone is mediated by NADPH-dependent reductase enzymes, which have been characterized in multiple plant species. These plant species all contain two types of the reductase enzymes, tropinone reductase I and tropinone reductase II. TRI produces tropine and TRII produces pseudotropine. Due to differing kinetic and pH/activity characteristics of the enzymes and by the 25-fold higher activity of TRI over TRII, the majority of the tropinone reduction is from TRI to form tropine.
Pharmacology.
Appearance.
Cocaine in its purest form is a white, pearly product. Cocaine appearing in powder form is a salt, typically cocaine hydrochloride (CAS 53-21-4). Street market cocaine is frequently adulterated or “cut” with various powdery fillers to increase its weight; the substances most commonly used in this process are baking soda; sugars, such as lactose, dextrose, inositol, and mannitol; and local anesthetics, such as lidocaine or benzocaine, which mimic or add to cocaine's numbing effect on mucous membranes. Cocaine may also be "cut" with other stimulants such as methamphetamine. Adulterated cocaine is often a white, off-white or pinkish powder.
The color of “crack” cocaine depends upon several factors including the origin of the cocaine used, the method of preparation – with ammonia or baking soda – and the presence of impurities, but will generally range from white to a yellowish cream to a light brown. Its texture will also depend on the adulterants, origin and processing of the powdered cocaine, and the method of converting the base. It ranges from a crumbly texture, sometimes extremely oily, to a hard, almost crystalline nature.
Forms.
Salts.
Cocaine is a weakly alkaline compound (an "alkaloid"), and can therefore combine with acidic compounds to form various salts. The hydrochloride (HCl) salt of cocaine is by far the most commonly encountered, although the sulfate (-SO4) and the nitrate (-NO3) are occasionally seen. Different salts dissolve to a greater or lesser extent in various solvents – the hydrochloride salt is polar in character and is quite soluble in water.
Basic.
As the name implies, “freebase” is the base form of cocaine, as opposed to the salt form. It is practically insoluble in water whereas hydrochloride salt is water soluble.
Smoking freebase cocaine has the additional effect of releasing methylecgonidine into the user's system due to the pyrolysis of the substance (a side effect which insufflating or injecting powder cocaine does not create). Some research suggests that smoking freebase cocaine can be even more cardiotoxic than other routes of administration because of methylecgonidine's effects on lung tissue and liver tissue.
Pure cocaine is prepared by neutralizing its compounding salt with an alkaline solution which will precipitate to non-polar basic cocaine. It is further refined through aqueous-solvent Liquid-liquid extraction.
Crack cocaine.
Crack is a lower purity form of free-base cocaine that is usually produced by neutralization of cocaine hydrochloride with a solution of baking soda (sodium bicarbonate, NaHCO3) and water, producing a very hard/brittle, off-white-to-brown colored, amorphous material that contains sodium carbonate, entrapped water, and other by-products as the main impurities.
The "freebase" and "crack" forms of cocaine are usually administered by vaporization of the powdered substance into smoke, which is then inhaled. The origin of the name "crack" comes from the "crackling" sound (and hence the onomatopoeic moniker “crack”) that is produced when the cocaine and its impurities (i.e. water, sodium bicarbonate) are heated past the point of vaporization. Pure cocaine base/crack can be smoked because it vaporizes smoothly, with little or no decomposition at , which is below the boiling point of water. The smoke produced from cocaine base is usually described as having a very distinctive, pleasant taste.
In contrast, cocaine hydrochloride does not vaporize until heated to a much higher temperature (about 197 °C), and considerable decomposition/burning occurs at these high temperatures. This effectively destroys some of the cocaine, and yields a sharp, acrid, and foul-tasting smoke.
Smoking or vaporizing cocaine and inhaling it into the lungs produces an almost immediate "high" that can be very powerful (and addicting) quite rapidly – this initial crescendo of stimulation is known as a "rush". While the stimulating effects may last for hours, the euphoric sensation is very brief, prompting the user to smoke more immediately.
Coca leaf infusions.
Coca herbal infusion (also referred to as coca tea) is used in coca-leaf producing countries much as any herbal medicinal infusion would elsewhere in the world. The free and legal commercialization of dried coca leaves under the form of filtration bags to be used as "coca tea" has been actively promoted by the governments of Peru and Bolivia for many years as a drink having medicinal powers. Visitors to the city of Cuzco in Peru, and La Paz in Bolivia are greeted with the offering of coca leaf infusions (prepared in tea pots with whole coca leaves) purportedly to help the newly arrived traveler overcome the malaise of high altitude sickness. The effects of drinking coca tea are a mild stimulation and mood lift. It does not produce any significant numbing of the mouth nor does it give a rush like snorting cocaine. In order to prevent the demonization of this product, its promoters publicize the unproven concept that much of the effect of the ingestion of coca leaf infusion would come from the secondary alkaloids, as being not only quantitatively different from pure cocaine but also qualitatively different.
It has been promoted as an adjuvant for the treatment of cocaine dependence. In one controversial study, coca leaf infusion was used—in addition to counseling—to treat 23 addicted coca-paste smokers in Lima, Peru. Relapses fell from an average of four times per month before treatment with coca tea to one during the treatment. The duration of abstinence increased from an average of 32 days prior to treatment to 217 days during treatment. These results suggest that the administration of coca leaf infusion plus counseling would be an effective method for preventing relapse during treatment for cocaine addiction. Importantly, these results also suggest strongly that the primary pharmacologically active metabolite in coca leaf infusions is actually cocaine and not the secondary alkaloids.
The cocaine metabolite benzoylecgonine can be detected in the urine of people a few hours after drinking one cup of coca leaf infusion.
Routes of administration.
Oral.
Many users rub the powder along the gum line, or onto a cigarette filter which is then smoked, which numbs the gums and teeth – hence the colloquial names of "numbies", "gummers" or "cocoa puffs" for this type of administration. This is mostly done with the small amounts of cocaine remaining on a surface after insufflation (snorting). Another oral method is to wrap up some cocaine in rolling paper and swallow (parachute) it. This is sometimes called a "snow bomb."
Coca leaf.
Coca leaves are typically mixed with an alkaline substance (such as lime) and chewed into a wad that is retained in the mouth between gum and cheek (much in the same as chewing tobacco is chewed) and sucked of its juices. The juices are absorbed slowly by the mucous membrane of the inner cheek and by the gastrointestinal tract when swallowed. Alternatively, coca leaves can be infused in liquid and consumed like tea. Ingesting coca leaves generally is an inefficient means of administering cocaine. Advocates of the consumption of the coca leaf state that coca leaf consumption should not be criminalized as it is not actual cocaine, and consequently it is not properly the illicit drug. Because cocaine is hydrolyzed and rendered inactive in the acidic stomach, it is not readily absorbed when ingested alone. Only when mixed with a highly alkaline substance (such as lime) can it be absorbed into the bloodstream through the stomach. The efficiency of absorption of orally administered cocaine is limited by two additional factors. First, the drug is partly catabolized by the liver. Second, capillaries in the mouth and esophagus constrict after contact with the drug, reducing the surface area over which the drug can be absorbed. Nevertheless, cocaine metabolites can be detected in the urine of subjects that have sipped even one cup of coca leaf infusion. Therefore, this is an actual additional form of administration of cocaine, albeit an inefficient one.
Orally administered cocaine takes approximately 30 minutes to enter the bloodstream. Typically, only a third of an oral dose is absorbed, although absorption has been shown to reach 60% in controlled settings. Given the slow rate of absorption, maximum physiological and psychotropic effects are attained approximately 60 minutes after cocaine is administered by ingestion. While the onset of these effects is slow, the effects are sustained for approximately 60 minutes after their peak is attained.
Contrary to popular belief, both ingestion and insufflation result in approximately the same proportion of the drug being absorbed: 30 to 60%. Compared to ingestion, the faster absorption of insufflated cocaine results in quicker attainment of maximum drug effects. Snorting cocaine produces maximum physiological effects within 40 minutes and maximum psychotropic effects within 20 minutes, however, a more realistic activation period is closer to 5 to 10 minutes, which is similar to ingestion of cocaine. Physiological and psychotropic effects from nasally insufflated cocaine are sustained for approximately 40–60 minutes after the peak effects are attained.
Coca tea, an infusion of coca leaves, is also a traditional method of consumption. The tea has often been recommended for travelers in the Andes to prevent altitude sickness. However, its actual effectiveness has never been systematically studied. This method of consumption has been practised for many centuries by the indigenous tribes of South America. One specific purpose of ancient coca leaf consumption was to increase energy and reduce fatigue in messengers who made multi-day quests to other settlements.
In 1986 an article in the "Journal of the American Medical Association" revealed that U.S. health food stores were selling dried coca leaves to be prepared as an infusion as “Health Inca Tea.” While the packaging claimed it had been "decocainized," no such process had actually taken place. The article stated that drinking two cups of the tea per day gave a mild stimulation, increased heart rate, and mood elevation, and the tea was essentially harmless. Despite this, the DEA seized several shipments in Hawaii, Chicago, Georgia, and several locations on the East Coast of the United States, and the product was removed from the shelves.
Insufflation.
Nasal insufflation (known colloquially as "snorting," "sniffing," or "blowing") is the most common method of ingestion of recreational powdered cocaine in the Western world. The drug coats and is absorbed through the mucous membranes lining the sinuses. When insufflating cocaine, absorption through the nasal membranes is approximately 30–60%, with higher doses leading to increased absorption efficiency. Any material not directly absorbed through the mucous membranes is collected in mucus and swallowed (this "drip" is considered pleasant by some and unpleasant by others). In a study of cocaine users, the average time taken to reach peak subjective effects was 14.6 minutes. Any damage to the inside of the nose is because cocaine highly constricts blood vesselsand therefore blood and oxygen/nutrient flowto that area. Nosebleeds after cocaine insufflation are due to irritation and damage of mucus membranes by foreign particles and adulterants and not the cocaine itself; as a vasoconstrictor, cocaine acts to reduce bleeding.
Prior to insufflation, cocaine powder must be divided into very fine particles. Cocaine of high purity breaks into fine dust very easily, except when it is moist (not well stored) and forms "chunks," which reduces the efficiency of nasal absorption.
Rolled up banknotes, hollowed-out pens, cut straws, pointed ends of keys, specialized spoons, long fingernails, and (clean) tampon applicators are often used to insufflate cocaine. Such devices are often called "tooters" by users. The cocaine typically is poured onto a flat, hard surface (such as a mirror, CD case or book) and divided into "bumps", "lines" or "rails", and then insufflated. The amount of cocaine in a line varies widely from person to person and occasion to occasion (the purity of the cocaine is also a factor), but one line is generally considered to be a single dose and is typically 35 mg (a "bump") to 100 mg (a "rail"). As tolerance builds rapidly in the short-term (hours), many lines are often snorted to produce greater effects.
A study by Bonkovsky and Mehta reported that, just like shared needles, the sharing of straws used to "snort" cocaine can spread blood diseases such as Hepatitis C.
Injection.
Drug injection provides the highest blood levels of drug in the shortest amount of time. Subjective effects not commonly shared with other methods of administration include a ringing in the ears moments after injection (usually when in excess of 120 milligrams) lasting 2 to 5 minutes including tinnitus and audio distortion. This is colloquially referred to as a "bell ringer". In a study of cocaine users, the average time taken to reach peak subjective effects was 3.1 minutes. The euphoria passes quickly. Aside from the toxic effects of cocaine, there is also danger of circulatory emboli from the insoluble substances that may be used to cut the drug. As with all injected illicit substances, there is a risk of the user contracting blood-borne infections if sterile injecting equipment is not available or used. Additionally, because cocaine is a vasoconstrictor, and usage often entails multiple injections within several hours or less, subsequent injections are progressively more difficult to administer, which in turn may lead to more injection attempts and more sequelae from improperly performed injection.
An injected mixture of cocaine and heroin, known as “speedball” is a particularly dangerous combination, as the converse effects of the drugs actually complement each other, but may also mask the symptoms of an overdose. It has been responsible for numerous deaths, including celebrities such as John Belushi, Chris Farley, Mitch Hedberg, River Phoenix and Layne Staley.
Experimentally, cocaine injections can be delivered to animals such as fruit flies to study the mechanisms of cocaine addiction.
Inhalation.
Inhalation or smoking is one of the several means cocaine is administered. Cocaine is smoked by inhaling the vapor by sublimating solid cocaine by heating. In a 2000 Brookhaven National Laboratory medical department study, based on self reports of 32 abusers who participated in the study,"peak high" was found at mean of 1.4min +/- 0.5 minutes.
Smoking freebase or crack cocaine is most often accomplished using a pipe made from a small glass tube, often taken from "Love roses," small glass tubes with a paper rose that are promoted as romantic gifts. These are sometimes called "stems", "horns", "blasters" and "straight shooters". A small piece of clean heavy copper or occasionally stainless steel scouring padoften called a "brillo" (actual Brillo pads contain soap, and are not used), or "chore", named for "Chore Boy" brand copper scouring pads,serves as a reduction base and flow modulator in which the "rock" can be melted and boiled to vapor. Crack smokers also sometimes smoke through a soda can with small holes in the bottom.
Crack is smoked by placing it at the end of the pipe; a flame held close to it produces vapor, which is then inhaled by the smoker. The effects, felt almost immediately after smoking, are very intense and do not last long usually 5 to 15 minutes.
When smoked, cocaine is sometimes combined with other drugs, such as cannabis, often rolled into a joint or blunt. Powdered cocaine is also sometimes smoked, though heat destroys much of the chemical; smokers often sprinkle it on cannabis.
The language referring to paraphernalia and practices of smoking cocaine vary, as do the packaging methods in the street level sale.
Suppository.
Little research has been focused on the suppository (anal or vaginal insertion) method of administration, also known as "plugging". This method of administration is commonly administered using an oral syringe. Cocaine can be dissolved in water and withdrawn into an oral syringe which may then be lubricated and inserted into the anus or vagina before the plunger is pushed. Anecdotal evidence of its effects are infrequently discussed, possibly due to social taboos in many cultures. The rectum and the vaginal canal is where the majority of the drug would likely be taken up, through the membranes lining its walls.
Mechanism of action.
The pharmacodynamics of cocaine involve the complex relationships of neurotransmitters (inhibiting monoamine uptake in rats with ratios of about: serotonin:dopamine = 2:3, serotonin:norepinephrine = 2:5) The most extensively studied effect of cocaine on the central nervous system is the blockade of the dopamine transporter protein. Dopamine transmitter released during neural signaling is normally recycled via the transporter; i.e., the transporter binds the transmitter and pumps it out of the synaptic cleft back into the presynaptic neuron, where it is taken up into storage vesicles. Cocaine binds tightly at the dopamine transporter forming a complex that blocks the transporter's function. The dopamine transporter can no longer perform its reuptake function, and thus dopamine accumulates in the synaptic cleft. This results in an enhanced and prolonged postsynaptic effect of dopaminergic signaling at dopamine receptors on the receiving neuron. Prolonged exposure to cocaine, as occurs with habitual use, leads to homeostatic dysregulation of normal (i.e. without cocaine) dopaminergic signaling via down-regulation of dopamine receptors and enhanced signal transduction. The decreased dopaminergic signaling after chronic cocaine use may contribute to depressive mood disorders and sensitize this important brain reward circuit to the reinforcing effects of cocaine (for example, enhanced dopaminergic signalling only when cocaine is self-administered). This sensitization contributes to the intractable nature of addiction and relapse.
Dopamine-rich brain regions such as the ventral tegmental area, nucleus accumbens, and prefrontal cortex are frequent targets of cocaine addiction research. Of particular interest is the pathway consisting of dopaminergic neurons originating in the ventral tegmental area that terminate in the nucleus accumbens. This projection may function as a "reward center", in that it seems to show activation in response to drugs of abuse like cocaine in addition to natural rewards like food or sex. While the precise role of dopamine in the subjective experience of reward is highly controversial among neuroscientists, the release of dopamine in the nucleus accumbens is widely considered to be at least partially responsible for cocaine's rewarding effects. This hypothesis is largely based on laboratory data involving rats that are trained to self-administer cocaine. If dopamine antagonists are infused directly into the nucleus accumbens, well-trained rats self-administering cocaine will undergo extinction (i.e. initially increase responding only to stop completely) thereby indicating that cocaine is no longer reinforcing (i.e. rewarding) the drug-seeking behavior. 
Cocaine's effects on serotonin (5-hydroxytryptamine, 5-HT) show across multiple serotonin receptors, and is shown to inhibit the re-uptake of 5-HT3 specifically as an important contributor to the effects of cocaine. The overabundance of 5-HT3 receptors in cocaine conditioned rats display this trait, however the exact effect of 5-HT3 in this process is unclear. The 5-HT2 receptor (particularly the subtypes 5-HT2AR, 5-HT2BR and 5-HT2CR) show influence in the evocation of hyperactivity displayed in cocaine use.
In addition to the mechanism shown on the above chart, cocaine has been demonstrated to bind as to directly stabilize the DAT transporter on the open outward-facing conformation. Further, cocaine binds in such a way as to inhibit a hydrogen bond innate to DAT. Cocaine's binding properties are such that it attaches so this hydrogen bond will not form and is blocked from formation due to the tightly locked orientation of the cocaine molecule. Research studies have suggested that the affinity for the transporter is not what is involved in habituation of the substance so much as the conformation and binding properties to where & how on the transporter the molecule binds.
Sigma receptors are affected by cocaine, as cocaine functions as a sigma ligand agonist. Further specific receptors it has been demonstrated to function on are NMDA and the D1 dopamine receptor.
Cocaine also blocks sodium channels, thereby interfering with the propagation of action potentials; thus, like lignocaine and novocaine, it acts as a local anesthetic. It also functions on the binding sites to the dopamine and serotonin sodium dependent transport area as targets as separate mechanisms from its reuptake of those transporters; unique to its local anesthetic value which makes it in a class of functionality different from both its own derived phenyltropanes analogues which have that removed. In addition to this cocaine has some target binding to the site of the Kappa-opioid receptor as well. Cocaine also causes vasoconstriction, thus reducing bleeding during minor surgical procedures. The locomotor enhancing properties of cocaine may be attributable to its enhancement of dopaminergic transmission from the substantia nigra. Recent research points to an important role of circadian mechanisms and clock genes in behavioral actions of cocaine.
Because nicotine increases the levels of dopamine in the brain, many cocaine users find that consumption of tobacco products during cocaine use enhances the euphoria. This, however, may have undesirable consequences, such as uncontrollable chain smoking during cocaine use (even users who do not normally smoke cigarettes have been known to chain smoke when using cocaine), in addition to the detrimental health effects and the additional strain on the cardiovascular system caused by tobacco.
Cocaine can often cause reduced food intake, many chronic users lose their appetite and can experience severe malnutrition and significant weight loss. Cocaine effects, further, are shown to be potentiated for the user when used in conjunction with new surroundings and stimuli, and otherwise novel environs.
Metabolism and excretion.
Cocaine is extensively metabolized, primarily in the liver, with only about 1% excreted unchanged in the urine. The metabolism is dominated by hydrolytic ester cleavage, so the eliminated metabolites consist mostly of benzoylecgonine (BE), the major metabolite, and other significant metabolites in lesser amounts such as ecgonine methyl ester (EME) and ecgonine. Further minor metabolites of cocaine include norcocaine, p-hydroxycocaine, m-hydroxycocaine, p-hydroxybenzoylecgonine (pOHBE), and m-hydroxybenzoylecgonine.
Depending on liver and kidney function, cocaine metabolites are detectable in urine. Benzoylecgonine can be detected in urine within four hours after cocaine intake and remains detectable in concentrations greater than 150 ng/mL typically for up to eight days after cocaine is used. Detection of accumulation of cocaine metabolites in hair is possible in regular users until the sections of hair grown during use are cut or fall out.
If consumed with alcohol, cocaine combines with alcohol in the liver to form cocaethylene. Studies have suggested cocaethylene is both more euphorigenic, and has a higher cardiovascular toxicity than cocaine by itself.
A study in mice has suggested that capsaicin found in pepper spray may interact with cocaine with potentially fatal consequences. The method through which they would interact however, is not known.
Detection in biological fluids.
Cocaine and its major metabolites may be quantitated in blood, plasma or urine to monitor for abuse, confirm a diagnosis of poisoning or assist in the forensic investigation of a traffic or other criminal violation or a sudden death. Most commercial cocaine immunoassay screening tests cross-react appreciably with the major cocaine metabolites, but chromatographic techniques can easily distinguish and separately measure each of these substances. When interpreting the results of a test, it is important to consider the cocaine usage history of the individual, since a chronic user can develop tolerance to doses that would incapacitate a cocaine-naive individual, and the chronic user often has high baseline values of the metabolites in his system. Cautious interpretation of testing results may allow a distinction between passive or active usage, and between smoking versus other routes of administration. In 2011, researchers at John Jay College of Criminal Justice reported that dietary zinc supplements can mask the presence of cocaine and other drugs in urine. Similar claims have been made in web forums on that topic.
Local anesthetic.
Cocaine was historically useful as a topical anesthetic in eye and nasal surgery, although it is now predominantly used for nasal and lacrimal duct surgery. The major disadvantages of this use are cocaine's intense vasoconstrictor activity and potential for cardiovascular toxicity. Cocaine has since been largely replaced in Western medicine by synthetic local anesthetics such as benzocaine, proparacaine, lignocaine/xylocaine/lidocaine, and tetracaine though it remains available for use if specified. If vasoconstriction is desired for a procedure (as it reduces bleeding), the anesthetic is combined with a vasoconstrictor such as phenylephrine or epinephrine. In Australia it is currently prescribed for use as a local anesthetic for conditions such as mouth and lung ulcers. Some ENT specialists occasionally use cocaine within the practice when performing procedures such as nasal cauterization. In this scenario dissolved cocaine is soaked into a ball of cotton wool, which is placed in the nostril for the 10–15 minutes immediately before the procedure, thus performing the dual role of both numbing the area to be cauterized, and vasoconstriction. Even when used this way, some of the used cocaine may be absorbed through oral or nasal mucosa and give systemic effects.
In 2005, researchers from Kyoto University Hospital proposed the use of cocaine in conjunction with phenylephrine administered in the form of an eye drop as a diagnostic test for Parkinson's disease.
Usage.
According to a 2007 United Nations report, Spain is the country with the highest rate of cocaine usage (3.0% of adults in the previous year). Other countries where the usage rate meets or exceeds 1.5% are the United States (2.8%), England and Wales (2.4%), Canada (2.3%), Italy (2.1%), Bolivia (1.9%), Chile (1.8%), and Scotland (1.5%).
Europe.
Cocaine is the second most popular illegal recreational drug in Europe (behind marijuana). Since the mid-1990s, overall cocaine usage in Europe has been on the rise, but usage rates and attitudes tend to vary between countries. Countries with the highest usage rates are: The United Kingdom, Spain, Italy, and Republic of Ireland.
Approximately 12 million Europeans (3.6%) have used cocaine at least once, 4 million (1.2%) in the last year, and 2 million in the last month (0.5%).
Young adults.
About 3.5 million or 87.5% of those who have used the drug in the last year are young adults (15–34 years old). Usage is particularly prevalent among this demographic: 4% to 7% of males have used cocaine in the last year in Spain, Denmark, Republic of Ireland, Italy, and the United Kingdom. The ratio of male to female users is approximately 3.8:1, but this statistic varies from 1:1 to 13:1 depending on country.
United States.
Cocaine is the second most popular illegal recreational drug in the United States (behind marijuana) and the U.S. is the world's largest consumer of cocaine. Cocaine is commonly used in middle to upper class communities and is known as a "rich man's drug". It is also popular amongst college students, as a party drug. A study throughout the entire United States has reported that around 48 percent of people who graduated high school in 1979 have used Cocaine recreationally during some point in their lifetime, compared to approximately 20 percent of students who graduated between the years of 1980 and 1995.
Its users span over different ages, races, and professions. In the 1970s and 1980s, the drug became particularly popular in the disco culture as cocaine usage was very common and popular in many discos such as Studio 54.
History.
Discovery.
For over a thousand years South American indigenous peoples have chewed the leaves of "Erythroxylon coca", a plant that contains vital nutrients as well as numerous alkaloids, including cocaine. The coca leaf was, and still is, chewed almost universally by some indigenous communities. The remains of coca leaves have been found with ancient Peruvian mummies, and pottery from the time period depicts humans with bulged cheeks, indicating the presence of something on which they are chewing. There is also evidence that these cultures used a mixture of coca leaves and saliva as an anesthetic for the performance of trepanation.
When the Spanish arrived in South America, most at first ignored aboriginal claims that the leaf gave them strength and energy, and declared the practice of chewing it the work of the Devil. But after discovering that these claims were true, they legalized and taxed the leaf, taking 10% off the value of each crop. In 1569, Nicolás Monardes described the indigenous peoples' practice of chewing a mixture of tobacco and coca leaves to induce "great contentment":
In 1609, Padre Blas Valera wrote:
Isolation and naming.
Although the stimulant and hunger-suppressant properties of coca had been known for many centuries, the isolation of the cocaine alkaloid was not achieved until 1855. Various European scientists had attempted to isolate cocaine, but none had been successful for two reasons: the knowledge of chemistry required was insufficient at the time. Additionally contemporary conditions of sea-shipping from South America could degrade the cocaine in the plant samples available to Europeans. 
The cocaine alkaloid was first isolated by the German chemist Friedrich Gaedcke in 1855. Gaedcke named the alkaloid "erythroxyline", and published a description in the journal "Archiv der Pharmazie."
In 1856, Friedrich Wöhler asked Dr. Carl Scherzer, a scientist aboard the "Novara" (an Austrian frigate sent by Emperor Franz Joseph to circle the globe), to bring him a large amount of coca leaves from South America. In 1859, the ship finished its travels and Wöhler received a trunk full of coca. Wöhler passed on the leaves to Albert Niemann, a Ph.D. student at the University of Göttingen in Germany, who then developed an improved purification process.
Niemann described every step he took to isolate cocaine in his dissertation titled "Über eine neue organische Base in den Cocablättern" ("On a New Organic Base in the Coca Leaves"), which was published in 1860—it earned him his Ph.D. and is now in the British Library. He wrote of the alkaloid's "colourless transparent prisms" and said that, "Its solutions have an alkaline reaction, a bitter taste, promote the flow of saliva and leave a peculiar numbness, followed by a sense of cold when applied to the tongue." Niemann named the alkaloid "cocaine" from "coca" (from Quechua "cuca") + suffix "ine". Because of its use as a local anesthetic, a suffix "-caine" was later extracted and used to form names of synthetic local anesthetics.
The first synthesis and elucidation of the structure of the cocaine molecule was by Richard Willstätter in 1898. The synthesis started from tropinone, a related natural product and took five steps.
Medicalization.
With the discovery of this new alkaloid, Western medicine was quick to exploit the possible uses of this plant.
In 1879, Vassili von Anrep, of the University of Würzburg, devised an experiment to demonstrate the analgesic properties of the newly discovered alkaloid. He prepared two separate jars, one containing a cocaine-salt solution, with the other containing merely salt water. He then submerged a frog's legs into the two jars, one leg in the treatment and one in the control solution, and proceeded to stimulate the legs in several different ways. The leg that had been immersed in the cocaine solution reacted very differently from the leg that had been immersed in salt water.
Karl Koller (a close associate of Sigmund Freud, who would write about cocaine later) experimented with cocaine for ophthalmic usage. In an infamous experiment in 1884, he experimented upon himself by applying a cocaine solution to his own eye and then pricking it with pins. His findings were presented to the Heidelberg Ophthalmological Society. Also in 1884, Jellinek demonstrated the effects of cocaine as a respiratory system anesthetic. In 1885, William Halsted demonstrated nerve-block anesthesia, and James Leonard Corning demonstrated peridural anesthesia. 1898 saw Heinrich Quincke use cocaine for spinal anesthesia.
Today, cocaine has very limited medical use. "See the section Cocaine as a local anesthetic"
Popularization.
In 1859, an Italian doctor, Paolo Mantegazza, returned from Peru, where he had witnessed first-hand the use of coca by the local indigenous peoples. He proceeded to experiment on himself and upon his return to Milan he wrote a paper in which he described the effects. In this paper he declared coca and cocaine (at the time they were assumed to be the same) as being useful medicinally, in the treatment of "a furred tongue in the morning, flatulence, and whitening of the teeth."
A chemist named Angelo Mariani who read Mantegazza's paper became immediately intrigued with coca and its economic potential. In 1863, Mariani started marketing a wine called Vin Mariani, which had been treated with coca leaves, to become cocawine. The ethanol in wine acted as a solvent and extracted the cocaine from the coca leaves, altering the drink's effect. It contained 6 mg cocaine per ounce of wine, but Vin Mariani which was to be exported contained 7.2 mg per ounce, to compete with the higher cocaine content of similar drinks in the United States. A "pinch of coca leaves" was included in John Styth Pemberton's original 1886 recipe for Coca-Cola, though the company began using decocainized leaves in 1906 when the Pure Food and Drug Act was passed. The actual amount of cocaine that Coca-Cola contained during the first 20 years of its production is practically impossible to determine.
In 1879 cocaine began to be used to treat morphine addiction. Cocaine was introduced into clinical use as a local anesthetic in Germany in 1884, about the same time as Sigmund Freud published his work "Über Coca", in which he wrote that cocaine causes:
In 1885 the U.S. manufacturer Parke-Davis sold cocaine in various forms, including cigarettes, powder, and even a cocaine mixture that could be injected directly into the user's veins with the included needle. The company promised that its cocaine products would "supply the place of food, make the coward brave, the silent eloquent and render the sufferer insensitive to pain."
By the late Victorian era cocaine use had appeared as a vice in literature. For example, it was injected by Arthur Conan Doyle's fictional Sherlock Holmes, generally to offset the boredom he felt when he was not working on a case.
In early 20th-century Memphis, Tennessee, cocaine was sold in neighborhood drugstores on Beale Street, costing five or ten cents for a small boxful. Stevedores along the Mississippi River used the drug as a stimulant, and white employers encouraged its use by black laborers.
In 1909, Ernest Shackleton took "Forced March" brand cocaine tablets to Antarctica, as did Captain Scott a year later on his ill-fated journey to the South Pole.
During the mid-1940s, amidst WWII, cocaine was considered for inclusion as an ingredient of a future generation of 'pep pills' for the German military code named D-IX.
Prohibition.
Prohibition of cocaine in the United States.
Calls for prohibition began long before the Harrison Act was passed by Congress in 1914 – a law requiring cocaine and narcotics to be dispensed only with a doctor's order. Before this, various factors and groups acted on primarily a state level influencing a move towards prohibition and away from a "laissez-faire" attitude.
Cocaine consumption had grown in 1903 to about five times that of 1890, predominately by non-medical users outside the middle-aged, white, professional class. Cocaine became associated with laborers, youths, blacks and the urban underworld.
Popularization of cocaine is first evident with laborers who used it as a stimulant to increase productivity, often supplied by employers. African American workers were believed by employers to be better at physical work and it was thought that it provided added strength to their constitution which, according to the Medical News, made blacks “impervious to the extremes of heat and cold.” Instead, cocaine use quickly acquired a reputation as dangerous and in 1897, the first state bill of control for cocaine sales came from a mining county in Colorado.
Laborers from other races used cocaine, such as in northern cities, where cocaine was often cheaper than alcohol. In the Northeast in particular, cocaine became popular amongst workers in factories, textile mills and on rail roads. In some instances, cocaine use supplemented or replaced caffeine as the drug-of-choice to keep workers awake and working overtime.
Fears of coerced cocaine use, and in particular that young girls would become addicted and thereby enter prostitution, were widespread. Tales of the corruption of the youth by cocaine were common but there is little evidence to support their veracity.
Mainstream media reported cocaine epidemics as early as 1894 in Dallas, Texas. Reports of the cocaine epidemic would foreshadow a familiar theme in later so-called epidemics, namely that cocaine presented a social threat more dangerous than simple health effects and had insidious results when used by blacks and members of the lower class. Similar anxiety-ridden reports appeared throughout cities in the South leading some to declare that “the cocaine habit has assumed the proportions of an epidemic among the colored people.” In 1900, state legislatures in Alabama, Georgia and Tennessee considered anti-cocaine bills for the first time.
Hyperbolic reports of the effect of cocaine on African Americans went hand-in-hand with this hysteria. In 1901, the Atlanta Constitution reported that “Use of the drug [cocaine] among negroes is growing to an alarming extent.” The "New York Times" reported that under the influence of cocaine, “sexual desires are increased and perverted … peaceful negroes become quarrelsome, and timid negroes develop a degree of 'Dutch courage' that is sometimes almost incredible.” A medical doctor even wrote “cocaine is often the direct incentive to the crime of rape by the negroes.” To complete the characterization, a judge in Mississippi declared that supplying a “negro” with cocaine was more dangerous than injecting a dog with rabies.
These attitudes not only influenced drug law and policy but also led to increased violence against African Americans. In 1906, a major race riot led by whites erupted; it was sparked by reports of crimes committed by black ‘cocaine fiends.’ Indeed, white-led, race riots spawning from reports of blacks under the influence of cocaine were not uncommon. Police in the South widely adopted the use of a heavier caliber handguns so as to better stop a cocaine-crazed black person – believed to be empowered with super-human strength. Another dangerous myth perpetuated amongst police was that cocaine imbued African Americans with tremendous accuracy with firearms and therefore police were better advised to shoot first in questionable circumstances.
Ultimately public opinion rested against the cocaine user. Criminality was commonly believed to be a natural result of cocaine use. Much of the influence for these kind of perceptions came from the widespread publicity given to notorious cases. While the historical reality of cocaine’s effect on violence and crime is difficult to disentangle from inflamed perceptions, it does appear that public opinion was swayed by the image of the violent, cocaine-crazed fiend and pushed over the edge by a few violent episodes. It was an image of the cocaine-user that carried acute racial overtones.
Before any substantive federal regulation of cocaine, state and local municipalities evoked their own means to regulate cocaine. Because of the initial lack of targeted legislation, on both federal and state level, the most typical strategy by law enforcement was the application of nuisance laws pertaining to vagrancy and disturbing the peace. Subsequent legislative actions aimed at controlling the distribution of cocaine rather than its manufacture. Reformers took this approach in part because of legal precedents which made it easier to control distributors such as pharmacies; state and local boards of health or boards of pharmacy often took the place of regulatory bodies for controlling the distribution of cocaine.
Some states took the position of outright banning of all forms of cocaine sale; Georgia was the first to do this in 1902. A New Orleans ordinance banned cocaine sales as well but left an ill-defined exception for therapeutic uses. A more common requirement was to restrict the sale of cocaine or impose labeling requirements. A 1907 California law limiting sale of cocaine to only those with a physician’s prescription resulted in the arrest of over 50 store owners and clerks in the first year. A 1913 New York state law limited druggists’ cocaine stocks to under 5 ounces. Labeling requirements initially operated on a state level with some states even going so far as to require that cocaine and cocaine-containing products be labeled as poison.
Eventually the federal government stepped in and instituted a national labeling requirement for cocaine and cocaine-containing products through the Food and Drug Act of 1906. The next important federal regulation was the Harrison Narcotics Tax Act of 1914. While this act is often seen as the start of prohibition, the act itself was not actually a prohibition on cocaine, but instead set up a regulatory and licensing regime. The Harrison Act did not recognize addiction as a treatable condition and therefore the therapeutic use of cocaine, heroin or morphine to such individuals was outlawed leading the Journal of American Medicine to remark, “[the addict] is denied the medical care he urgently needs, open, above-board sources from which he formerly obtained his drug supply are closed to him, and he is driven to the underworld where he can get his drug, but of course, surreptitiously and in violation of the law.” The Harrison Act left manufacturers of cocaine untouched so long as they met certain purity and labeling standards. Despite that cocaine was typically illegal to sell and legal outlets were more rare, the quantities of legal cocaine produced declined very little. Legal cocaine quantities did not decrease until the Jones-Miller Act of 1922 put serious restrictions on cocaine manufactures.
Modern usage.
In many countries, cocaine is a popular recreational drug. In the United States, the development of "crack" cocaine introduced the substance to a generally poorer inner-city market. Use of the powder form has stayed relatively constant, experiencing a new height of use during the late 1990s and early 2000s in the U.S., and has become much more popular in the last few years in the UK. 
Cocaine use is prevalent across all socioeconomic strata, including age, demographics, economic, social, political, religious, and livelihood. 
The estimated U.S. cocaine market exceeded US$70 billion in street value for the year 2005, exceeding revenues by corporations such as Starbucks. There is a tremendous demand for cocaine in the U.S. market, particularly among those who are making incomes affording luxury spending, such as single adults and professionals with discretionary income. Cocaine’s status as a club drug shows its immense popularity among the "party crowd".
In 1995 the World Health Organization (WHO) and the United Nations Interregional Crime and Justice Research Institute (UNICRI) announced in a press release the publication of the results of the largest global study on cocaine use ever undertaken. However, a decision by an American representative in the World Health Assembly banned the publication of the study, because it seemed to make a case for the positive uses of cocaine. An excerpt of the report strongly conflicted
with accepted paradigms, for example "that occasional cocaine use does not typically lead to
severe or even minor physical or social problems." In the sixth meeting of the B committee the US representative threatened that "If WHO activities relating to drugs failed to reinforce proven drug control approaches, funds for the relevant programs should be curtailed". This led to the decision to discontinue publication. A part of the study has been recuperated. Available are profiles of cocaine use in 20 countries.
It was reported in October 2010 that the use of cocaine in Australia has doubled since monitoring began in 2003.
A problem with illegal cocaine use, especially in the higher volumes used to combat fatigue (rather than increase euphoria) by long-term users, is the risk of ill effects or damage caused by the compounds used in adulteration. Cutting or "stepping on" the drug is commonplace, using compounds which simulate ingestion effects, such as Novocain (procaine) producing temporary anesthaesia as many users believe a strong numbing effect is the result of strong and/or pure cocaine, ephedrine or similar stimulants that are to produce an increased heart rate. The normal adulterants for profit are inactive sugars, usually mannitol, creatine or glucose, so introducing active adulterants gives the illusion of purity and to 'stretch' or make it so a dealer can sell more product than without the adulterants. The adulterant of sugars therefore allows the dealer to sell the product for a higher price because of the illusion of purity and allows to sell more of the product at that higher price, enabling dealers to significantly increase revenue with little additional cost for the adulterants. A study by the European Monitoring Centre for Drugs and Drug Addiction in 2007 showed that the purity levels for street purchased cocaine was often under 5% and on average under 50% pure.
Society and culture.
Legal status.
The production, distribution and sale of cocaine products is restricted (and illegal in most contexts) in most countries as regulated by the Single Convention on Narcotic Drugs, and the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. In the United States the manufacture, importation, possession, and distribution of cocaine is additionally regulated by the 1970 Controlled Substances Act.
Some countries, such as Peru and Bolivia permit the cultivation of coca leaf for traditional consumption by the local indigenous population, but nevertheless prohibit the production, sale and consumption of cocaine. In addition, some parts of Europe and Australia allow processed cocaine for medicinal uses only.
Interdiction.
In 2004, according to the United Nations, 589 tonnes of cocaine were seized globally by law enforcement authorities. Colombia seized 188 t, the United States 166 t, Europe 79 t, Peru 14 t, Bolivia 9 t, and the rest of the world 133 t.
Illicit trade.
Because of the drugs potential for addiction and overdose, cocaine is generally treated as a 'hard drug', with severe penalties for possession and trafficking. Demand remains high, and consequently black market cocaine is quite expensive. Unprocessed cocaine, such as coca leaves, are occasionally purchased and sold, but this is exceedingly rare as it is much easier and more profitable to conceal and smuggle it in powdered form. The scale of the market is immense: 770 tonnes times $100 per gram retail = up to $77 billion.
Production.
Until 2012, Colombia was the world's leading producer of cocaine. Three-quarters of the world's annual yield of cocaine has been produced in Colombia, both from cocaine base imported from Peru (primarily the Huallaga Valley) and Bolivia, and from locally grown coca. There was a 28% increase from the amount of potentially harvestable coca plants which were grown in Colombia in 1998. This, combined with crop reductions in Bolivia and Peru, made Colombia the nation with the largest area of coca under cultivation after the mid-1990s. Coca grown for traditional purposes by indigenous communities, a use which is still present and is permitted by Colombian laws, only makes up a small fragment of total coca production, most of which is used for the illegal drug trade.
An interview with a coca farmer published in 2003 described a mode of production by acid-base extraction that has changed little since 1905. Roughly of leaves were harvested per hectare, six times per year. The leaves were dried for half a day, then chopped into small pieces with a strimmer and sprinkled with a small amount of powdered cement (replacing sodium carbonate from former times). Several hundred pounds of this mixture was soaked in of gasoline for a day, then the gasoline was removed and the leaves were pressed for remaining liquid, after which they could be discarded. Then battery acid (weak sulfuric acid) was used, one bucket per of leaves, to create a phase separation in which the cocaine free base in the gasoline was acidified and extracted into a few buckets of "murky-looking smelly liquid". Once powdered caustic soda was added to this, the cocaine precipitated and could be removed by filtration through a cloth. The resulting material, when dried, was termed "pasta" and sold by the farmer. The 3750 pound yearly harvest of leaves from a hectare produced of "pasta", approximately 40–60% cocaine. Repeated recrystallization from solvents, producing "pasta lavada" and eventually crystalline cocaine, were performed at specialized laboratories after the sale.
Attempts to eradicate coca fields through the use of defoliants have devastated part of the farming economy in some coca growing regions of Colombia, and strains appear to have been developed that are more resistant or immune to their use. Whether these strains are natural mutations or the product of human tampering is unclear. These strains have also shown to be more potent than those previously grown, increasing profits for the drug cartels responsible for the exporting of cocaine. Although production fell temporarily, coca crops rebounded in numerous smaller fields in Colombia, rather than the larger plantations.
The cultivation of coca has become an attractive, and in some cases even necessary, economic decision on the part of many growers due to the combination of several factors, including the persistence of worldwide demand, the lack of other employment alternatives, the lower profitability of alternative crops in official crop substitution programs, the eradication-related damages to non-drug farms, and the spread of new strains of the coca plant.
The latest estimate provided by the U.S. authorities on the annual production of cocaine in Colombia refers to 290 metric tons.
As of the end of 2011, the seizure operations of Colombian cocaine carried out in different countries have totaled 351.8 metric tons of cocaine, i.e. 121.3% of Colombia’s annual production according to the U.S. Department of State’s estimates.
Synthesis.
Synthetic cocaine would be highly desirable to the illegal drug industry, as it would eliminate the high visibility and low reliability of offshore sources and international smuggling, replacing them with clandestine domestic laboratories, as are common for illicit methamphetamine. However, natural cocaine remains the lowest cost and highest quality supply of cocaine. Actual full synthesis of cocaine is rarely done. Formation of inactive enantiomers (cocaine has 4 chiral centres – "1R", "2R", "3S", and "5S" – hence a total potential of 16 possible enantiomers and diastereoisomers) plus synthetic by-products limits the yield and purity.
Names like "synthetic cocaine" and "new cocaine" have been misapplied to phencyclidine (PCP) and various designer drugs.
Trafficking and distribution.
Organized criminal gangs operating on a large scale dominate the cocaine trade. Most cocaine is grown and processed in South America, particularly in Colombia, Bolivia, Peru, and smuggled into the United States and Europe, the United States being the world's largest consumer of cocaine, where it is sold at huge markups; usually in the US at $80–$120 for 1 gram, and $250–300 for 3.5 grams (1/8 of an ounce, or an "eight ball").
Caribbean and Mexican routes.
Cocaine shipments from South America transported through Mexico or Central America are generally moved over land or by air to staging sites in northern Mexico. The cocaine is then broken down into smaller loads for smuggling across the U.S.–Mexico border. The primary cocaine importation points in the United States are in Arizona, southern California, southern Florida, and Texas. Typically, land vehicles are driven across the U.S.-Mexico border. Sixty five percent of cocaine enters the United States through Mexico, and the vast majority of the rest enters through Florida.
Cocaine traffickers from Colombia, and recently Mexico, have also established a labyrinth of smuggling routes throughout the Caribbean, the Bahama Island chain, and South Florida. They often hire traffickers from Mexico or the Dominican Republic to transport the drug. The traffickers use a variety of smuggling techniques to transfer their drug to U.S. markets. These include airdrops of in the Bahama Islands or off the coast of Puerto Rico, mid-ocean boat-to-boat transfers of , and the commercial shipment of tonnes of cocaine through the port of Miami.
Chilean route.
Another route of cocaine traffic goes through Chile, this route is primarily used for cocaine produced in Bolivia since the nearest seaports lie in northern Chile. The arid Bolivia-Chile border is easily crossed by 4x4 vehicles that then head to the seaports of Iquique and Antofagasta. While the price of cocaine is higher in Chile than in Peru and Bolivia, the final destination is usually Europe, especially Spain where drug dealing networks exist among South American immigrants.
Techniques.
Cocaine is also carried in small, concealed, kilogram quantities across the border by couriers known as “mules” (or “mulas”), who cross a border either legally, for example, through a port or airport, or illegally elsewhere. The drugs may be strapped to the waist or legs or hidden in bags, or hidden in the body. If the mule gets through without being caught, the gangs will reap most of the profits. If he or she is caught however, gangs will sever all links and the mule will usually stand trial for trafficking alone.
Bulk cargo ships are also used to smuggle cocaine to staging sites in the western Caribbean–Gulf of Mexico area. These vessels are typically 150–250-foot (50–80 m) coastal freighters that carry an average cocaine load of approximately 2.5 tonnes. Commercial fishing vessels are also used for smuggling operations. In areas with a high volume of recreational traffic, smugglers use the same types of vessels, such as go-fast boats, as those used by the local populations.
Sophisticated drug subs are the latest tool drug runners are using to bring cocaine north from Colombia, it was reported on 20 March 2008. Although the vessels were once viewed as a quirky sideshow in the drug war, they are becoming faster, more seaworthy, and capable of carrying bigger loads of drugs than earlier models, according to those charged with catching them.
Sales to consumers.
Cocaine is readily available in all major countries' metropolitan areas. According to the "Summer 1998 Pulse Check," published by the U.S. Office of National Drug Control Policy, cocaine use had stabilized across the country, with a few increases reported in San Diego, Bridgeport, Miami, and Boston. In the West, cocaine usage was lower, which was thought to be due to a switch to methamphetamine among some users; methamphetamine is cheaper, three and a half times more powerful, and lasts 12 to 24 times longer with each dose. Nevertheless, the number of cocaine users remain high, with a large concentration among urban youth.
In addition to the amounts previously mentioned, cocaine can be sold in "bill sizes": for example, $10 might purchase a "dime bag," a very small amount (0.1–0.15 g) of cocaine. Twenty dollars might purchase 0.15–0.3 g. However, in lower Texas, it is sold cheaper due to it being easier to receive: a dime for $10 is 0.4g, a 20 is 0.8–1.0 gram and an 8-ball (3.5g) is sold for $60 to $80, depending on the quality and dealer. These amounts and prices are very popular among young people because they are inexpensive and easily concealed on one's body. Quality and price can vary dramatically depending on supply and demand, and on geographic region.
The European Monitoring Centre for Drugs and Drug Addiction reports that the typical retail price of cocaine varied between €50 and €75 per gram in most European countries, although Cyprus, Romania, Sweden and Turkey reported much higher values.
Consumption.
World annual cocaine consumption, as of 2000, stands at around 600 tonnes, with the United States consuming around 300 t, 50% of the total, Europe about 150 t, 25% of the total, and the rest of the world the remaining 150 t or 25%.
The 2010 UN World Drug Report concluded that "it appears that the North American cocaine market has declined in value from US$47 billion in 1998 to US$38 billion in 2008. Between 2006 and 2008, the value of the market remained basically stable."

</doc>
<doc id="7706" url="http://en.wikipedia.org/wiki?curid=7706" title="Cartesian coordinate system">
Cartesian coordinate system

A Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a "coordinate axis" or just "axis" of the system, and the point where they meet is its "origin", usually at ordered pair . The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.
One can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, "n" Cartesian coordinates (an element of real "n"-space) specify the point in an "n"-dimensional Euclidean space for any dimension "n". These coordinates are equal, up to sign, to distances from the point to "n" mutually perpendicular hyperplanes.
The invention of Cartesian coordinates in the 17th century by René Descartes (Latinized name: "Cartesius") revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2 in a plane may be described as the set of all points whose coordinates "x" and "y" satisfy the equation .
Cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory, and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering, and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design, and other geometry-related data processing.
History.
The adjective "Cartesian" refers to the French mathematician and philosopher René Descartes (who used the name "Cartesius" in Latin).
The idea of this system was developed in 1637 in writings by Descartes and independently by Pierre de Fermat, although Fermat also worked in three dimensions and did not publish the discovery. Both authors used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' "La Géométrie" was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.
The development of the Cartesian coordinate system would play an intrinsic role in the development of the calculus by Isaac Newton and Gottfried Wilhelm Leibniz.
Nicole Oresme, a French cleric and friend of the Dauphin (later to become King Charles V) of the 14th Century, used constructions similar to Cartesian coordinates well before the time of Descartes and Fermat.
Many other coordinate systems have been developed since Descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space.
Description.
One dimension.
Choosing a Cartesian coordinate system for a one-dimensional space—that is, for a straight line—involves choosing a point "O" of the line (the origin), a unit of length, and an orientation for the line. An orientation chooses which of the two half-lines determined by "O" is the positive, and which is negative; we then say that the line "is oriented" (or "points") from the negative half towards the positive half. Then each point "P" of the line can be specified by its distance from "O", taken with a + or − sign depending on which half-line contains "P".
A line with a chosen Cartesian system is called a number line. Every real number has a unique location on the line. Conversely, every point on the line can be interpreted as a number in an ordered continuum such as the real numbers.
Two dimensions.
The modern Cartesian coordinate system in two dimensions (also called a rectangular coordinate system) is defined by an ordered pair of perpendicular lines (axes), a single unit of length for both axes, and an orientation for each axis. (Early systems allowed "oblique" axes, that is, axes that did not meet at right angles.) The lines are commonly referred to as the "x"- and "y"-axes where the "x"-axis is taken to be horizontal and the "y"-axis is taken to be vertical. The point where the axes meet is taken as the origin for both, thus turning each axis into a number line. For a given point "P", a line is drawn through "P" perpendicular to the "x"-axis to meet it at "X" and second line is drawn through "P" perpendicular to the "y"-axis to meet it at "Y". The coordinates of "P" are then "X" and "Y" interpreted as numbers "x" and "y" on the corresponding number lines. The coordinates are written as an ordered pair .
The point where the axes meet is the common origin of the two number lines and is simply called the "origin". It is often labeled "O" and if so then the axes are called "Ox" and "Oy". A plane with "x"- and "y"-axes defined is often referred to as the Cartesian plane or "xy" plane. The value of "x" is called the "x"-coordinate or abscissa and the value of "y" is called the "y"-coordinate or ordinate.
The choices of letters come from the original convention, which is to use the latter part of the alphabet to indicate unknown values. The first part of the alphabet was used to designate known values.
In the Cartesian plane, reference is sometimes made to a unit circle or a unit hyperbola.
Three dimensions.
Choosing a Cartesian coordinate system for a three-dimensional space means choosing an ordered triplet of lines (axes) that are pair-wise perpendicular, have a single unit of length for all three axes and have an orientation for each axis. As in the two-dimensional case, each axis becomes a number line. The coordinates of a point "P" are obtained by drawing a line through "P" perpendicular to each coordinate axis, and reading the points where these lines meet the axes as three numbers of these number lines.
Alternatively, the coordinates of a point "P" can also be taken as the (signed) distances from "P" to the three planes defined by the three axes. If the axes are named "x", "y", and "z", then the "x"-coordinate is the distance from the plane defined by the "y" and "z" axes. The distance is to be taken with the + or − sign, depending on which of the two half-spaces separated by that plane contains "P". The "y" and "z" coordinates can be obtained in the same way from the "x"–"z" and "x"–"y" planes respectively.
Higher dimensions.
A Euclidean plane with a chosen Cartesian system is called a Cartesian plane. Since Cartesian coordinates are unique and non-ambiguous, the points of a Cartesian plane can be identified with pairs of real numbers; that is with the Cartesian product formula_1, where formula_2 is the set of all reals. In the same way, the points any Euclidean space of dimension "n" be identified with the tuples (lists) of "n" real numbers, that is, with the Cartesian product formula_3.
Generalizations.
The concept of Cartesian coordinates generalizes to allow axes that are not perpendicular to each other, and/or different units along each axis. In that case, each coordinate is obtained by projecting the point onto one axis along a direction that is parallel to the other axis (or, in general, to the hyperplane defined by all the other axes). In such an oblique coordinate system the computations of distances and angles must be modified from that in standard Cartesian systems, and many standard formulas (such as the Pythagorean formula for the distance) do not hold.
Notations and conventions.
The Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in or . The origin is often labelled with the capital letter "O". In analytic geometry, unknown or generic coordinates are often denoted by the letters "x" and "y" on the plane, and "x", "y", and "z" in three-dimensional space. This custom comes from a convention of algebra, which use letters near the end of the alphabet for unknown values (such as were the coordinates of points in many geometric problems), and letters near the beginning for given quantities.
These conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted "t" and "p". Each axis is usually named after the coordinate which is measured along it; so one says the "x-axis", the "y-axis", the "t-axis", etc.
Another common convention for coordinate naming is to use subscripts, as in "x"1, "x"2, ... "x""n" for the "n" coordinates in an "n"-dimensional space; especially when "n" is greater than 3, or not specified. Some authors prefer the numbering "x"0, "x"1, ... "x""n"−1. These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.
In mathematical illustrations of two-dimensional Cartesian systems, the first coordinate (traditionally called the abscissa) is measured along a horizontal axis, oriented from left to right. The second coordinate (the ordinate) is then measured along a vertical axis, usually oriented from bottom to top.
However, computer graphics and image processing often use a coordinate system with the "y" axis oriented downwards on the computer display. This convention developed in the 1960s (or earlier) from the way that images were originally stored in display buffers.
For three-dimensional systems, a convention is to portray the "x"–"y" plane horizontally, with the "z" axis added to represent height (positive up). Furthermore, there is a convention to orient the "x"-axis toward the viewer, biased either to the right or left. If a diagram (3D projection or 2D perspective drawing) shows the "x" and "y" axis horizontally and vertically, respectively, then the "z" axis should be shown pointing "out of the page" towards the viewer or camera. In such a 2D diagram of a 3D coordinate system, the "z" axis would appear as a line or ray pointing down and to the left or down and to the right, depending on the presumed viewer or camera perspective. In any diagram or display, the orientation of the three axes, as a whole, is arbitrary. However, the orientation of the axes relative to each other should always comply with the right-hand rule, unless specifically stated otherwise. All laws of physics and math assume this right-handedness, which ensures consistency.
For 3D diagrams, the names "abscissa" and "ordinate" are rarely used for "x" and "y", respectively. When they are, the "z"-coordinate is sometimes called the applicate. The words "abscissa", "ordinate" and "applicate" are sometimes used to refer to coordinate axes rather than the coordinate values.
Quadrants and octants.
The axes of a two-dimensional Cartesian system divide the plane into four infinite regions, called quadrants, each bounded by two half-axes. These are often numbered from 1st to 4th and denoted by Roman numerals: I (where the signs of the two coordinates are +,+), II (−,+), III (−,−), and IV (+,−). When the axes are drawn according to the mathematical custom, the numbering goes counter-clockwise starting from the upper right ("north-east") quadrant.
Similarly, a three-dimensional Cartesian system defines a division of space into eight regions or octants, according to the signs of the coordinates of the points. The convention used for naming a specific octant is to list its signs, e.g. (+ + +) or (− + −). The generalization of the quadrant and octant to arbitrary number of dimensions is the orthant, and a similar naming system applies.
Cartesian formulas for the plane.
Distance between two points.
The Euclidean distance between two points of the plane with Cartesian coordinates formula_4 and formula_5 is
This is the Cartesian version of Pythagoras' theorem. In three-dimensional space, the distance between points formula_7 and formula_8 is
which can be obtained by two consecutive applications of Pythagoras' theorem.
Euclidean transformations.
The Euclidean transformations or Euclidean motions are the (bijective) mappings of points of the Euclidean plane to themselves which preserve distances between points. There are four types of these mappings (also called isometries): translations, rotations, reflections and glide reflections.
Translation.
Translating a set of points of the plane, preserving the distances and directions between them, is equivalent to adding a fixed pair of numbers to the Cartesian coordinates of every point in the set. That is, if the original coordinates of a point are , after the translation they will be
Rotation.
To rotate a figure counterclockwise around the origin by some angle formula_11 is equivalent to replacing every point with coordinates ("x","y") by the point with coordinates ("x'","y'"), where
Thus:
formula_14
Reflection.
If are the Cartesian coordinates of a point, then are the coordinates of its reflection across the second coordinate axis (the Y-axis), as if that line were a mirror. Likewise, are the coordinates of its reflection across the first coordinate axis (the X-axis). In more generality, reflection across a line through the origin making an angle formula_11 with the x-axis, is equivalent to replacing every point with coordinates by the point with coordinates , where
Thus:
formula_18
Glide reflection.
A glide reflection is the composition of a reflection across a line followed by a translation in the direction of that line. It can be seen that the order of these operations does not matter (the translation can come first, followed by the reflection).
General matrix form of the transformations.
These Euclidean transformations of the plane can all be described in a uniform way by using matrices. The result formula_19 of applying a Euclidean transformation to a point formula_20 is given by the formula
where "A" is a 2×2 orthogonal matrix and is an arbitrary ordered pair of numbers; that is,
where
To be "orthogonal", the matrix "A" must have orthogonal rows with same Euclidean length of one, that is,
and
This is equivalent to saying that "A" times its transpose must be the identity matrix. If these conditions do not hold, the formula describes a more general affine transformation of the plane provided that the determinant of "A" is not zero.
The formula defines a translation if and only if "A" is the identity matrix. The transformation is a rotation around some point if and only if "A" is a rotation matrix, meaning that
A reflection or glide reflection is obtained when,
Assuming that translation is not used transformations can be combined by simply multiplying the associated transformation matrices.
Affine transformation.
Another way to represent coordinate transformations in Cartesian coordinates is through affine transformations. In affine transformations an extra dimension is added and all points are given a value of 1 for this extra dimension. The advantage of doing this is that point translations can be specified in the final column of matrix "A". In this way, all of the euclidean transformations become transactable as matrix point multiplications. The affine transformation is given by:
Using affine transformations multiple different euclidean transformations including translation can be combined by simply multiplying the corresponding matrices.
Scaling.
An example of an affine transformation which is not a Euclidean motion is given by scaling. To make a figure larger or smaller is equivalent to multiplying the Cartesian coordinates of every point by the same positive number "m". If are the coordinates of a point on the original figure, the corresponding point on the scaled figure has coordinates
If "m" is greater than 1, the figure becomes larger; if "m" is between 0 and 1, it becomes smaller.
Shearing.
A shearing transformation will push the top of a square sideways to form a parallelogram. Horizontal shearing is defined by:
Shearing can also be applied vertically:
Orientation and handedness.
In two dimensions.
Fixing or choosing the "x"-axis determines the "y"-axis up to direction. Namely, the "y"-axis is necessarily the perpendicular to the "x"-axis through the point marked 0 on the "x"-axis. But there is a choice of which of the two half lines on the perpendicular to designate as positive and which as negative. Each of these two choices determines a different orientation (also called "handedness") of the Cartesian plane.
The usual way of orienting the axes, with the positive "x"-axis pointing right and the positive "y"-axis pointing up (and the "x"-axis being the "first" and the "y"-axis the "second" axis) is considered the "positive" or "standard" orientation, also called the "right-handed" orientation.
A commonly used mnemonic for defining the positive orientation is the "right hand rule". Placing a somewhat closed right hand on the plane with the thumb pointing up, the fingers point from the "x"-axis to the "y"-axis, in a positively oriented coordinate system.
The other way of orienting the axes is following the "left hand rule", placing the left hand on the plane with the thumb pointing up.
When pointing the thumb away from the origin along an axis towards positive, the curvature of the fingers indicates a positive rotation along that axis.
Regardless of the rule used to orient the axes, rotating the coordinate system will preserve the orientation. Switching any two axes will reverse the orientation, but switching both will leave the orientation unchanged.
In three dimensions.
Once the "x"- and "y"-axes are specified, they determine the line along which the "z"-axis should lie, but there are two possible directions on this line. The two possible coordinate systems which result are called 'right-handed' and 'left-handed'. The standard orientation, where the "xy"-plane is horizontal and the "z"-axis points up (and the "x"- and the "y"-axis form a positively oriented two-dimensional coordinate system in the "xy"-plane if observed from "above" the "xy"-plane) is called right-handed or positive.
The name derives from the right-hand rule. If the index finger of the right hand is pointed forward, the middle finger bent inward at a right angle to it, and the thumb placed at a right angle to both, the three fingers indicate the relative directions of the "x"-, "y"-, and "z"-axes in a "right-handed" system. The thumb indicates the "x"-axis, the index finger the "y"-axis and the middle finger the "z"-axis. Conversely, if the same is done with the left hand, a left-handed system results.
Figure 7 depicts a left and a right-handed coordinate system. Because a three-dimensional object is represented on the two-dimensional screen, distortion and ambiguity result. The axis pointing downward (and to the right) is also meant to point "towards" the observer, whereas the "middle" axis is meant to point "away" from the observer. The red circle is "parallel" to the horizontal "xy"-plane and indicates rotation from the "x"-axis to the "y"-axis (in both cases). Hence the red arrow passes "in front of" the "z"-axis.
Figure 8 is another attempt at depicting a right-handed coordinate system. Again, there is an ambiguity caused by projecting the three-dimensional coordinate system into the plane. Many observers see Figure 8 as "flipping in and out" between a convex cube and a concave "corner". This corresponds to the two possible orientations of the coordinate system. Seeing the figure as convex gives a left-handed coordinate system. Thus the "correct" way to view Figure 8 is to imagine the "x"-axis as pointing "towards" the observer and thus seeing a concave corner.
Representing a vector in the standard basis.
A point in space in a Cartesian coordinate system may also be represented by a position vector, which can be thought of as an arrow pointing from the origin of the coordinate system to the point. If the coordinates represent spatial positions (displacements), it is common to represent the vector from the origin to the point of interest as formula_33. In two dimensions, the vector from the origin to the point with Cartesian coordinates (x, y) can be written as:
where formula_35, and formula_36 are unit vectors in the direction of the "x"-axis and "y"-axis respectively, generally referred to as the "standard basis" (in some application areas these may also be referred to as versors). Similarly, in three dimensions, the vector from the origin to the point with Cartesian coordinates formula_37 can be written as:
where formula_39 is the unit vector in the direction of the z-axis.
There is no "natural" interpretation of multiplying vectors to obtain another vector that works in all dimensions, however there is a way to use complex numbers to provide such a multiplication. In a two dimensional cartesian plane, identify the point with coordinates with the complex number . Here, i is the imaginary unit and is identified with the point with coordinates , so it is not the unit vector in the direction of the "x"-axis. Since the complex numbers can be multiplied giving another complex number, this identification provides a means to "multiply" vectors. In a three dimensional cartesian space a similar identification can be made with a subset of the quaternions.
Applications.
Cartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application. 1) Units of distance must be decided defining the spatial size represented by the numbers used as coordinates. 2) An origin must be assigned to a specific spatial location or landmark, and 3) the orientation of the axes must be defined using available directional cues for (n-1) of the n axes.
Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (i.e. geospatial 3D). What units make sense? Kilometers are a good choice, since the original definition of the kilometer was geospatial...10,000 km equalling the surface distance from Equator to North Pole. Where to place the origin? Based on symmetry, the gravitational center of the Earth suggests a natural landmark (which can be sensed via satellite orbits). Finally, how to orient X, Y and Z axis directions? The axis of Earth's spin provides a natural direction strongly associated with "up vs. down", so positive Z can adopt the direction from geocenter to North Pole. A location on the Equator is needed to define the X-axis, and the Prime Meridian stands out as a reference direction, so the X-axis takes the direction from geocenter out to [ 0 degrees longitude, 0 degrees latitude ]. Note that with 3 dimensions, and two perpendicular axes directions pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right hand rule, the Y-axis must point out from the geocenter to [ 90 degrees longitude, 0 degrees latitude ]. So what are the geocentric coordinates of the Empire State Building in New York City? Using [ longitude = −73.985656, latitude = 40.748433 ], Earth radius = 40,000/2π, and transforming from spherical --> Cartesian coordinates, you can estimate the geocentric coordinates of the Empire State Building, [ "x", "y", "z" ] = [ 1330.53 km, –4635.75 km, 4155.46 km ]. GPS navigation relies on such geocentric coordinates.
In engineering projects, agreement on the definition of coordinates is a crucial foundation. One cannot assume that coordinates come predefined for a novel application, so knowledge of how to erect a coordinate system where there is none is essential to applying René Descartes' ingenious thinking.
While spatial apps employ identical units along all axes, in business and scientific apps, each axis may have different units of measurement associated with it (such as kilograms, seconds, pounds, etc.). Although four- and higher-dimensional spaces are difficult to visualize, the algebra of Cartesian coordinates can be extended relatively easily to four or more variables, so that certain calculations involving many variables can be done. (This sort of algebraic extension is what is used to define the geometry of higher-dimensional spaces.) Conversely, it is often helpful to use the geometry of Cartesian coordinates in two or three dimensions to visualize algebraic relationships between two or three of many non-spatial variables.
The graph of a function or relation is the set of all points satisfying that function or relation. For a function of one variable, "f", the set of all points , where is the graph of the function "f". For a function "g" of two variables, the set of all points , where is the graph of the function "g". A sketch of the graph of such a function or relation would consist of all the salient parts of the function or relation which would include its relative extrema, its concavity and points of inflection, any points of discontinuity and its end behavior. All of these terms are more fully defined in calculus. Such graphs are useful in calculus to understand the nature and behavior of a function or relation.

</doc>
<doc id="7708" url="http://en.wikipedia.org/wiki?curid=7708" title="Commandant of the Marine Corps">
Commandant of the Marine Corps

The Commandant of the Marine Corps (CMC) is normally the highest-ranking officer in the United States Marine Corps and is a member of the Joint Chiefs of Staff. The CMC reports directly to the United States Secretary of the Navy and is responsible for ensuring the organization, policy, plans, and programs for the Marine Corps as well as advising the President, the Secretary of Defense, the National Security Council, the Homeland Security Council, and the Secretary of the Navy on matters involving the Marine Corps. Under the authority of the Secretary of the Navy, the CMC designates Marine personnel and resources to the commanders of Unified Combatant Commands. The commandant performs all other functions prescribed in Section 5043 in Title 10 of the United States Code or delegates those duties and responsibilities to other officers in his administration in his name. As with the other joint chiefs, the Commandant is an administrative position and has no operational command authority over United States Marine Corps forces. 
The Commandant is nominated by the President for a four-year term of office and must be confirmed by the Senate. By statute, the Commandant is appointed as a four-star general while serving in office. "The Commandant is directly responsible to the Secretary of the Navy for the total performance of the Marine Corps. This includes the administration, discipline, internal organization, training, requirements, efficiency, and readiness of the service. The Commandant is also responsible for the operation of the Marine Corps material support system." Since 1801, the official residence of the Commandant has been located in the Marine Barracks in Washington, D.C. and his main offices are in Arlington, Virginia.
Responsibilities.
The responsibilities of the Commandant are outlined in Title 10, Section 5043 the United States Code and is "Subject to the authority, direction, and control of the Secretary of the Navy". As stated in the U.S. Code, the Commandant shall preside over the Headquarters, Marine Corps, transmit the plans and recommendations of the Headquarters, Marine Corps, to the Secretary and advise the Secretary with regard to such plans and recommendations, after approval of the plans or recommendations of the Headquarters, Marine Corps, by the Secretary, act as the agent of the Secretary in carrying them into effect, exercise supervision, consistent with the authority assigned to commanders of unified or specified combatant commands under chapter 6 of this title, over such of the members and organizations of the Marine Corps and the Navy as the Secretary determines, perform the duties prescribed for him by section 171 of this title and other provisions of law and perform such other military duties, not otherwise assigned by law, as are assigned to him by the President, the Secretary of Defense, or the Secretary of the Navy.
List of commandants.
Thirty-five men have served as the Commandant of the Marine Corps, including the current Commandant James F. Amos. The first Commandant was Samuel Nicholas, who took office as a captain, though there was no office titled "Commandant" at the time, and the Second Continental Congress had authorized that the senior-most Marine could take a rank up to Colonel. The longest-serving was Archibald Henderson, sometimes referred to as the "Grand old man of the Marine Corps" due to his thirty-nine-year tenure. In the 236-year history of the United States Marine Corps, only one Commandant has ever been fired from the job: Anthony Gale, as a result of a court-martial in 1820.

</doc>
<doc id="7710" url="http://en.wikipedia.org/wiki?curid=7710" title="California Department of Transportation">
California Department of Transportation

The California Department of Transportation (Caltrans) is an executive department within the U.S. state of California. 
Caltrans manages the state highway system (which includes the California Freeway and Expressway System) and is actively involved with public transportation systems throughout the state. 
The department is part of the state cabinet-level California State Transportation Agency (CalSTA). 
Like the majority of state government agencies, Caltrans is headquartered in Sacramento.
History.
The earliest predecessor of Caltrans was the Bureau of Highways, which was created by the California Legislature and signed into law by Governor James Budd in 1895. This agency consisted of three commissioners who were charged with analyzing the state road system and making recommendations. At the time, there was no state highway system, since roads were purely a local responsibility. California's roads consisted of crude dirt roads maintained by county governments, as well as some paved roads within city boundaries, and this "ad hoc" system was no longer adequate for the needs of the state's rapidly growing population. After the commissioners submitted their report to the governor on November 25, 1896, the legislature replaced the Bureau with the Department of Highways.
Due to the state's weak fiscal condition and corrupt politics, little progress was made until 1907, when the legislature replaced the Department of Highways with the Department of Engineering, within which there was a Division of Highways. Voters approved an $18 million bond issue for the construction of a state highway system in 1910, and the first Highway Commission was convened in 1911. On August 7, 1912, the department broke ground on its first construction project, the section of El Camino Real between South San Francisco and Burlingame (now part of California State Route 82). The year 1912 also saw the founding of the Transportation Laboratory and the creation of seven administrative divisions (the predecessors of the 12 district offices that exist today).
In 1913, the legislature started requiring vehicle registration and allocated the resulting funds to support regular highway maintenance, which began the next year.
In 1921, the legislature turned the Department of Engineering into the Department of Public Works.
The history of Caltrans and its predecessor agencies during the 20th century was marked by many firsts. It was one of the first agencies in the United States to paint centerlines on highways statewide (thanks to June McCarroll); the first to build a freeway west of the Mississippi River (the Pasadena Freeway); the first to build a four-level stack interchange; the first to develop and deploy non-reflective raised pavement markers, better known as Botts' dots; and one of the first to implement dedicated freeway-to-freeway connector ramps for high-occupancy vehicle lanes.
In late 1972, the legislature approved a reorganization (suggested in a study initiated by then-Governor Ronald Reagan) in which the Department of Public Works was merged with the Department of Aeronautics to become the modern Department of Transportation.
Administration.
For administrative purposes, Caltrans divides the State of California into 12 districts, supervised by district offices. Most districts cover multiple counties; District 12 (Orange County) is the only district with one county. The largest districts by population are District 4 (San Francisco Bay Area) and District 7 (Los Angeles and Ventura counties). Like most state agencies, Caltrans maintains its headquarters in Sacramento, which is covered by District 3.
Current projects.
Important projects include Interstate 105, the reconstruction of the SR 91/SR 60/I-215 interchange, the expansion of I-215 through San Bernardino's downtown to the city's University District, and the San Francisco – Oakland Bay Bridge (East Span).

</doc>
<doc id="7712" url="http://en.wikipedia.org/wiki?curid=7712" title="Continuation War">
Continuation War

The Continuation War (; ; 25 June 1941 – 19 September 1944) refers to the hostilities between Finland and the Soviet Union during World War II, from 1941 to 1944.
At the time of the war, the Finns adopted this name to make clear this war's relationship to the preceding Winter War. The Soviet Union saw the war as part of its struggle against Nazi Germany and its allies, on the Eastern Front of World War II. The war was known in the Soviet Union as the Great Patriotic War. Germany regarded its operations in the region as part of its overall war efforts on the Eastern Front, and it provided Finland with critical material support and military cooperation.
Acts of war between the Soviet Union and Finland started on 22 June 1941, the day Germany launched its invasion of the Soviet Union, with covert Finnish operations. Open warfare began with a Soviet air offensive on 25 June. Subsequent Finnish operations undid its post-Winter War concessions to the Soviet Union on the Karelian Isthmus and Ladoga Karelia, and captured East Karelia by September 1941. On the Karelian Isthmus, the Finns halted their offensive 30 km from Leningrad, at the pre-World War II border between the Soviet Union and Finland. Finnish forces did not participate in the siege of Leningrad directly, holding their pre-World War II territory on the Karelian Isthmus for two and a half years instead. In 1944, Soviet air forces conducted air raids on Helsinki and other major Finnish cities. Eventually, in summer 1944, the Soviet strategic offensive drove the Finns from most of the territories they had gained during the war, but the Finnish Army later brought the offensive to a standstill in July 1944. A ceasefire ended hostilities on 5 September and was followed by the Moscow Armistice on 19 September. The 1947 Paris peace treaty concluded the war formally. Finland ceded Petsamo Province to the Soviets, leased Porkkala peninsula to them, and paid reparations, while ultimately retaining its independence.
Background.
Winter War.
On 23 August 1939, the Soviet Union and Germany signed the Molotov–Ribbentrop Pact whereby the parties divided the independent countries of Finland, Estonia, Latvia, Lithuania, Poland, and Romania into spheres of interest, with Finland falling to the Soviet sphere of interest. Shortly afterward, Germany invaded Poland and as a result the United Kingdom and France declared war against Germany. The Soviet Union invaded eastern Poland on 17 September. Next, Moscow demanded that the Baltic states allow the establishment of Soviet military bases and the stationing of troops on their soil. The Baltic governments accepted these ultimatums, signing corresponding agreements in September and October 1939.
In October 1939, the Soviet Union attempted to negotiate with Finland for the transfer of Finnish territories on the Karelian Isthmus and the islands of the Gulf of Finland to the Soviet Union and for the establishment of a Soviet military base near the Finnish capital Helsinki. The Finnish government refused, and the Red Army attacked Finland on 30 November 1939. Condemnation of the Soviets by the League of Nations and by countries all over the world had no effect on Soviet policy. International help for Finland was planned, but very little actual help materialized, except from Sweden. The Moscow Peace Treaty, which was signed on 12 March 1940, ended the Winter War. By the terms of the treaty, Finland lost one eleventh of its national territory and about 13% of its economic capacity. However, Finland had avoided having the Soviet Union annex the whole country.
Interim peace.
Finland's foreign policy had been based on multilateral guarantees for support from the League of Nations and Nordic countries and was considered a failure. Finnish public opinion favored the reconquest of Finnish Karelia. Finland's government declared the country's defense to be its first priority, and military expenditures rose to nearly half of government spending. Finland purchased and received donations of war materiel during and immediately after the Winter War. On Finland's southern frontier the Soviet Union had acquired a military base in Hanko near the capital Helsinki, which employed over 30,000 Soviet military personnel.
Finland also had to resettle some 420,000 evacuees from the lost territories. To ensure the supply of food, it was necessary to clear new land for the evacuees to cultivate. This was facilitated by the Rapid Settlement Act. The Finnish leadership wanted to preserve the spirit of unanimity that was commonly felt throughout the country during the Winter War. The divisive White Guard tradition of the Civil War 16 May victory day celebration was therefore discontinued. Relations between Finland and the Soviet Union remained strained despite the signing of the one-sided peace treaty, and there were disputes regarding the implementation of the conditions of the treaty. Finland sought security against further territorial depredations by the Soviet Union and proposed mutual defence agreements with Norway and Sweden, but these initiatives were quashed by Moscow.
German and Soviet expansion in Europe.
After the Winter War, Germany was not popular in Finland as it was considered an ally of the Soviet Union. However, the Finnish government began to restore diplomatic relations with Germany. Finland continued its Western-oriented policy and negotiated a war trade agreement with the United Kingdom, but the agreement was renounced after the German invasion of Denmark and Norway on 9 April, when Britain cut all trade and traffic communications with Scandinavia. With the fall of France, a policy of Western orientation was no longer considered an option in Finnish foreign policy. On 15 and 16 June, the Soviet Union occupied the Baltic states without resistance. Soviet Puppet regimes were installed; and within two months Estonia, Latvia, and Lithuania were incorporated as Soviet Republics within the Soviet Union. By mid-1940, the two remaining northern democracies, Finland and Sweden, were encircled by the totalitarian states of Germany and the Soviet Union.
On 23 June, a short time after the Soviet occupations of the Baltic states began, the Soviet foreign minister Vyacheslav Molotov contacted the Finns and demanded a mining licence for the Soviet Union at the nickel mines in Petsamo or alternately the establishing of a joint Soviet-Finnish company to operate there. The licence to mine the deposit had earlier been granted to a British-Canadian company, and the proposition was rejected. The next month, the Soviets demanded that Finland destroy the fortifications built in the Åland islands and give the Soviets the right to use Finnish railways to transport Soviet troops to the newly acquired Soviet base at Hanko. The Finns very reluctantly agreed to these demands. On 24 July, Molotov accused the Finnish government of persecuting the so-called Society for Peace and Friendship between Finland and USSR, a pro-communist group; and soon afterwards, he publicly supported this group. The society organized demonstrations, some of which turned into riots.
Finnish relations with Germany and the USSR.
On 31 July 1940, the German leader Adolf Hitler gave the order to start planning an assault on the Soviet Union. This meant that Germany had to reassess its positions regarding both Finland and Romania. Until then, Germany had rejected Finnish appeals to purchase arms, but in August the Germans allowed the secret sale of weapons to Finland. German and Finnish military authorities made an agreement on 12 September, and an official exchange of diplomatic notes was sent on 22 September. At the same time, German troops were allowed to transit through Sweden and Finland. In practice, this meant Germany had redrawn the border of German and Soviet spheres of influence.
Due to the changed situation, Molotov made a visit to Berlin on 12–13 November. He wanted Germany to withdraw its troops from Finland and stop enabling Finnish anti-Soviet sentiments. He also reminded the Germans of the 1939 Soviet–German non-aggression pact. Hitler asked how the Soviet Union planned to settle the "Finnish question". Molotov answered that it would happen in the same manner as in Bessarabia and the Baltic states. Hitler rejected this. The following December, the Soviet Union, Germany, and the United Kingdom all voiced opinions concerning suitable Finnish presidential candidates. Risto Ryti was the only candidate none of these three powers objected to. He was elected on 19 December.
In January 1941, the Soviet Union demanded to take control of the Petsamo mining area. Finland rejected this, as it by then had a rebuilt defense force and was encouraged by Germany to reject the Soviet demand. On 18 December 1940, Hitler had officially approved Operation Barbarossa, the German invasion of the Soviet Union. He expected both Finland and Romania to join the German campaign. Two days earlier, Finnish Major General Paavo Talvela had met German Colonel General Franz Halder and, a couple days later, Reichsmarschall Hermann Göring, in Berlin. This was the first time the Germans advised the Finns, in carefully couched diplomatic terms, that they were preparing for a war with the Soviet Union. Outlines of the actual plan were revealed in January 1941 and regular contacts between Finnish and German military leaders started from February.
In late spring 1941, the Soviet Union made a number of goodwill gestures in order to prevent Finland from completely falling under German influence. Soviet ambassador Ivan Zotov was replaced with the more flexible Pavel Orlov. Furthermore, the Soviet government announced that it no longer opposed a rapprochement between Finland and Sweden. However, these conciliatory measures did not have any effect on Finnish policy.
Path to war.
Finnish-German agreement.
On 20 May 1941 the Germans invited some Finnish officers to Germany to discuss the coordination of Operation Barbarossa. The participants met on 25–28 May in Salzburg and Berlin, and continued their meeting in Helsinki from 3 to 6 June. They agreed upon the arrival of German troops, Finnish mobilization, and general division of operations. They also agreed that the Finnish Army would start mobilization on 15 June, but the Germans did not reveal the final date for the assault. The Finnish decisions were made by a small group of political and military leaders, and the rest of the government was largely kept in the dark. The government was not informed until 9 June that the country would start mobilization of reservists due to tensions between Germany and the Soviet Union.
Deployments and pre-assaults.
The Germans took responsibility for the stretch of the front in northern Finland consisting of Finnish Lapland. The Finnish army was now much stronger than it had been during the Winter War, now boasting 475,000 men. The artillery, too, was relatively strong. However, there was only one tank battalion and a lack of motorized transportation.
At the beginning of the war, the Soviet Union had eighteen divisions in the region, against fifteen Finnish and four German divisions. The Finns enjoyed air supremacy. Furthermore, the Soviet Union needed its best units and most up-to-date materiel on its western front.
The German troops assaulted the Soviet Union on 22 June elsewhere but not from Finland. However, German minelayers hiding in the Archipelago Sea laid two large minefields across the Gulf of Finland in the late hours of 21 June. Later the same night, German bombers flew along the Gulf of Finland to Leningrad and mined the harbour and the river Neva. On the return trip, these bombers landed for refueling on an airfield in Utti. In the early hours of 22 June, Finnish forces launched Operation Kilpapurjehdus, which aimed to man the demilitarized Åland Islands. An international treaty on the status of the islands called for Finland to defend them in case of the threat of an attack. However, the operation was coordinated with the Nazi invasion, and the Soviet consulate there was arrested. According to Finnish historian Mauno Jokipii, Finland knew that it had violated international norms.
On 21 June, Finnish units began to concentrate at the Finnish-Soviet border, where they were arranged into defensive formations. Finland mobilised 16 infantry divisions, one cavalry brigade, and two jäger brigades, which were all standard infantry brigades, except for an armoured battalion in the 1st Jäger Brigade. Separate battalions were mostly formed from border guard units and were used mainly for reconnaissance. Soviet military plans estimated that Finland would be able to mobilise only ten infantry divisions, as it had done in the Winter War, but they failed to take into account the materiel Finland had purchased between the wars and its training of all available men. Two German mountain divisions were stationed at Petsamo and two infantry divisions at Salla. On the morning of 22 June, the German Mountain Corps Norway began its advance from northern Norway to Petsamo. Finland did not allow direct German attacks from its soil into the Soviet Union. On the same day, another German infantry division was moved from Oslo to face Ladoga Karelia.
On the Soviet side, the Karelian Isthmus was covered by the 23rd Army. Ladoga Karelia was defended by the 7th Army. In the Murmansk–Salla region, there was the 14th Army with the 42nd Corps. The Red Army also had around 40 battalions of separate regiments and fortification units present. Leningrad was garrisoned by three infantry divisions and one mechanized corps. As the initial devastating German strike against the Soviet Air Force had not affected air units located near Finland, the Soviets deployed 700 planes as well as some aircraft from the Navy against 300 Finnish planes.
Soviet air attack.
On the morning of 25 June, the Soviet Union launched an air offensive of 460 fighters and bombers targeting 19 airfields in Finland; however, inaccurate intelligence and poor bombing accuracy caused several raids to hit Finnish cities or municipalities. There was considerable destruction in the cities. Twenty-three Soviet bombers were lost, while the Finns didn't lose any aircraft. 
The Soviet Union stated that the air attack was directed against German targets, especially airfields, in Finland. At the same time, Soviet artillery stationed at the Hanko base began to shell Finnish targets, and a minor Soviet infantry attack was launched over the Finnish side of the border in Parikkala.
The bombings offered the Finnish government a ground for claiming that the country had become the target of a new assault, and the Finnish parliament approved the "defensive war" as a "fait accompli". According to historian David Kirby, the message was intended more for public opinion in Finland than abroad, where it was seen that the country was in the German camp.
1941: Finnish offensive.
In July the Finnish military began its planned offensive. According to Finnish historian Olli Vehviläinen, in 1941 most Finns thought that the scope of the new offensive was only to regain what had been wrongly taken in the Winter War.
The Soviet Union struggled to contain the German invasion, and soon the Soviet High Command had to call all available units stationed along the Finnish border to the rapidly deteriorating front line. According to Finnish historian Ohto Manninen, because of this, the initial air offensive against Finland could not be followed by a supporting land offensive as allegedly planned. Moreover, 237th Infantry Division and, excluding the 198th Motorized Division, the Soviet 10th Mechanized Corps were withdrawn from Ladoga Karelia, thus stripping most of the reserves from the remaining defending Soviet units.
Reconquest of Ladoga Karelia.
The Finnish plans for the offensive in Ladoga Karelia were completed on 28 June. The offensive was launched on 10 July, and already by 16 July the Finns reached the shore of Lake Ladoga and cut the defending Soviet army in two, hindering the Soviets' defense of the area. Finnish headquarters halted the offensive in Ladoga Karelia on 25 July after reconquering the area of Ladoga Karelia lost to the Soviet Union in 1940 and after advancing as far as Vitele. The Finnish offensive then moved on to other sections of the front.
Reconquest of the Karelian Isthmus.
The Finnish II Corps (II AK) started its offensive in the region of the Karelian Isthmus on 31 July. Finnish troops reached the shores of Lake Ladoga on 9 August, surrounding most of three defending Soviet divisions on the northwestern coast of the lake, from where the Soviet divisions were evacuated across the lake. On 22 August the Finnish IV AK Corps started its offensive from the 1940 border between the Gulf of Finland and the II AK, and advanced towards Viborg. By 23 August, the Finnish II Corps had reached the Vuoksi waterway from the east and continued to surround the Soviet forces defending Viborg. The Soviet withdrawal order came too late, and the Soviet divisions lost much of their equipment, although a sizable portion of their manpower was later evacuated via the Koivisto islands. The badly mauled defending Soviet army was unable to halt the Finnish offensive, and by 2 September the Finns had reached the 1939 border along its whole length. On 31 August, Finnish headquarters ordered the 2nd and 4th Army Corps, which had advanced the furthest, to halt their offensive after reaching a line just past the former border that ran from the mouth of the River Sestra via Retukylä, Aleksandrovka, and the eastern edge of the village of S. Beloostrov () to Ohta and form for defense.
According to Soviet sources, the Finns advanced and took the settlement of Novyi Beloostrov with its train station on 4 September, but a Soviet counter-attack threw them out the next day. The war diary of the Finnish 12th Division facing this settlement notes that it was quiet at the time while the neighboring 18th Division had orders on the morning of 4 September 1941 to form a line of defense north of N. Beloostrov, and the Finnish 6th Regiment responsible for the Finnish 18th Division's front line facing N. Beloostrov formed for defense along the small stream (Serebryanyy ruchey) north of N. Belootrov on 4 September 1941. According to Finnish sources, Soviet forces advanced north from N. Beloostrov and attacked the Finnish positions along the small stream on the morning of 5 September 1941, but the Finns managed to repel them. Staryi Beloostrov (Valkeasaari) was taken by the Finns on September 4 and the Soviet counterattacks failed to retake the settlement. Finnish forces captured N. Beloostrov again on 10 or 11 September 1941. According to the war diary of the Finnish 12th Division, this was done to strengthen their lines. The Soviet war correspondent Luknitsky noted that this created a dangerous bulge in the Soviet defensive line. According to Russian historian Nazarenko, the Finns were not able to advance further due to stronger Soviet defensive positions. Fighting for the settlement continued until 20 September, when the Soviets managed to force the Finns out. After that the front stabilized.
Conquest of East Karelia.
The Finnish offensive in East Karelia started in early July in the northern section of the front. In early September, the attack in the northern section reached Rukajärvi (Ругозеро, Rugozero) village and Finnish headquarters halted the offensive there. On August 27, Finnish headquarters ordered the offensive in the south to reach the Svir River. Finnish troops cut the Kirov railroad on 7 September, crossed the Svir on 15 September, and then halted the offensive. Advance troops reached the shores of Lake Onega on 24 September. The town of Petrozavodsk was captured on 1 October after the Soviets withdrew to avoid encirclement. On 6 November, the Finnish headquarters ordered their forces to capture Karhumäki and then move to defense. The Finnish forces captured the area of Karhumäki and Povenets, and halted the offensive in early December.
Related to the Finnish advance to the Svir, the German Army Group North advanced from the south towards the Svir River and managed to capture Tikhvin before Soviet counterattacks forced the Germans to withdraw to the Volkhov River. Soviet forces also made several attempts to force the Finns out from their bridgehead south of the Svir during October and December 1941; however, the Soviet efforts to reduce the bridgehead were blocked by the Finns. Soviet forces also attacked the German 163rd Division, which was operating under Finnish command, across the Svir in October 1941; but the Soviet forces that had crossed the river were pushed back soon after.
Operation Silver Fox in the North.
The German objective in northern Finland was to take Murmansk and seize control of the Murman Railway. Murmansk was the only year-round ice-free port in the north, and it was a threat to the nickel mine at Petsamo. Operation Silver Fox was run by the German AOK Norwegen and had two Finnish divisions under its command. The German soldiers were from central Europe and they had difficulties moving over a roadless terrain of swamp and forest. Troops managed to advance some distance with heavy casualties, but the terrain offered good defensive positions for the Soviet resistance.
The order for German–Finnish troops to move to defensive operations was given on 17 November, when attempts to reach the Murmansk Railway had failed.
Naval campaign.
Although the Soviet Red Banner Baltic fleet started the war in a strong position, German naval mine warfare and aerial supremacy, and the rapid advance by the German land forces forced the Soviet Navy to evacuate its bases to Kronstadt and Leningrad. The Soviets' evacuations from Tallinn and Hanko proved to be very costly operations for them. As the Soviet Navy withdrew to the eastern end of the Gulf of Finland, it left nearly the whole Baltic Sea as well as many of the islands to the Germans and Finns. Although Soviet submarines caused some threat to German traffic on the Baltic, the withdrawal of the Soviet Navy made the Baltic Sea a "German lake" until the second half of 1944. Although the Soviet Navy left in a hurry, the naval mines it had managed to lay before and during the evacuations caused casualties both to the Germans and the Finns, including the loss of one of the two Finnish coastal defence ships, the .
Political development.
Germany's main forces advanced rapidly deep into Soviet territory during the first weeks of the Operation Barbarossa campaign. The Finns believed the Germans would defeat the Soviet Union quickly. President Ryti envisioned Greater Finland, where the country and other Finnic people would live inside a "natural defence borderline" by incorporating the Kola Peninsula, East Karelia, and perhaps even northern Ingria. In public the proposed frontier was introduced by the slogan "A short border – a long peace". Some members of the Finnish parliament, such as the Social Democrats and the Swedish People's Party, opposed the idea, arguing that maintaining the 1939 frontier would be enough. On July 10, Finnish Commander-in-Chief C. G. E. Mannerheim gave an order of the day, the Sword scabbard declaration, in which he pledged to liberate Karelia. The Finnish government assured the Americans that it was unaware of the order.
Finland had prepared for a short war, but in late autumn it was clear that there would be no decisive outcome in the short term. Finnish troops suffered losses during their advance; and, overall, German victory became uncertain as German troops were halted near Moscow. The Finnish economy suffered from a lack of labour, food shortages, and increased prices. The Finnish government had to demobilize part of the army so that industrial and agricultural production would not collapse. In October, Finland informed Germany that it would need of grains to manage until next year's harvest. The German authorities would have rejected the request, but Hitler himself agreed. Annual grain deliveries amounted to almost one-half of the Finnish domestic crop. In November, Finland decided to join the Anti-Comintern Pact. The advance in East Karelia was halted on 6 December. The Finns had suffered 75,000 casualties, of whom 25,000 were Finnish deaths during the advance.
Finland and Western allies.
Finland worked to maintain good relations with the Western powers. The Finnish government stressed that Finland was fighting as a co-belligerent with Germany against the Soviet Union only to protect itself. Furthermore, Finland stressed that it was still the same democratic country as it had been in the Winter War. However, on 12 July 1941, the United Kingdom had signed an agreement of joint action with the Soviet Union. Furthermore, under German pressure, Finland had to close the British legation in Helsinki. As a result, diplomatic relations between Finland and the United Kingdom were broken on 1 August. On 28 November, Britain presented Finland an ultimatum, in which it demanded that Finland cease military operations by 3 December. Unofficially, Finland informed the Western powers that troops would halt their advance in the next few days. The reply did not satisfy the United Kingdom, which declared war on Finland on 6 December 1941. The Commonwealth member states of Canada, Australia, India, and New Zealand followed.
1942–43: Trench warfare.
Military operations.
Although military operations during 1942 and 1943 were limited, the front did see some action. In early 1942, Soviet Karelian Front forces attempted to retake Medvezhyegorsk, which had been lost to the Finns in late 1941. As spring came, the Soviet forces also went on the offensive on the Svir front as well as in Kiestinki region. All Soviet offensives started promisingly, but due either to the Soviets overextending their lines or stubborn defensive resistance, the Soviet offensives were stopped and repulsed. After Finnish and German counterattacks in Kiestinki, the eventual front lines had moved very little. In September 1942, the Soviets tried again at Kriv near Medvezhyegorsk, but despite five days of fighting, the Soviets managed to push the Finnish lines back only on a roughly -long stretch of the front.
Unconventional warfare was fought in both the Finnish and Soviet wilderness. Finnish LRRPs organized both by Finnish HQ—4th Separate Battalion (Er.P 4)—and by local units patrolled beyond Soviet lines. In summer 1942, the Soviet Union had formed the 1st Partisan Brigade. The unit was only 'partisan' in name, as it was essentially more than 600 men and women on long range patrol. The 1st Partisan Brigade was able to infiltrate beyond Finnish patrol lines but was found out and largely destroyed.
On the naval front, the Soviet Baltic Fleet still operated from the besieged city of Leningrad. In early 1942, Soviet forces recaptured the island of Gogland but lost both Gogland and Bolshoy Tyuters to the Finns later in spring 1942. During the winter of 1941/1942, the Soviet Baltic Fleet made the decision to use the large Soviet submarine fleet to carry the fight to the enemy. Though initial submarine operations in the summer of 1942 were successful, the German Kriegsmarine and Finnish Navy soon stepped up their anti-submarine efforts, making the Soviet submarine operations later in 1942 very costly. The underwater offensive carried out by the Soviets convinced the Germans to lay anti-submarine nets as well as supporting minefields between Porkkala and Naissaar which proved to be an insurmountable obstacle for the Soviet submarines.
Diplomatic manoeuvers.
Operation Barbarossa was planned as a "blitzkrieg" intended to last a few weeks. British and U.S. observers believed that the invasion would be concluded before August. In the autumn of 1941, this turned out to be wrong, and leading Finnish military officers started to doubt Germany's capability to finish the war quickly. German troops in northern Finland faced circumstances they were not properly prepared for, and failed to reach their targets, most importantly Murmansk. As the lines stabilized, Finland sent out peace feelers to the Soviet Union several times. Germany was alarmed by this, and reacted by drawing down shipments of desperately needed materials each time. The idea that Finland had to continue the war while putting its own forces in the least possible danger gained increasing support, perhaps in the hope that the "Wehrmacht" and the Red Army would wear each other down enough for negotiations to begin, or to at least get them out of the way of Finland's independent decisions. Nationalist elements, including the IKL, may also have continued to hope for an eventual victory by Germany.
Finland's participation in the war brought major benefits to Germany. The Soviet fleet was blockaded in the Gulf of Finland, so that the Baltic was freed for the training of German submarine crews as well as for German shipping, especially for the transport of vital iron ore from northern Sweden and nickel and rare metals (needed in steel processing) from the Petsamo area. The Finnish front secured the northern flank of the German Army Group North in the Baltic states. The sixteen Finnish divisions tied down numerous Soviet troops, put pressure on Leningrad (although Mannerheim refused to attack it directly), and threatened the Murmansk railway. Additionally, Sweden was further isolated and was increasingly pressured to comply with German and Finnish wishes, though with limited success.
Despite Finland's contributions to the German cause, the Western Allies had ambivalent feelings, torn between residual goodwill for Finland and the need to accommodate their vital ally, the Soviet Union. As a result, Britain declared war against Finland, but the United States did not. With few exceptions, there was no combat between these countries and Finland, but Finnish sailors were interned overseas. In the United States, Finland was denounced for naval attacks made on American Lend-Lease shipments, but received approval for continuing to make payments on its World War I debt throughout the inter-war period.
Because Finland joined the Anti-Comintern Pact and signed other agreements with Germany, Italy, and Japan, the Allies characterized Finland as one of the Axis Powers, although the term used in Finland is "co-belligerence with Germany", emphasizing the lack of a formal military alliance.
International volunteers and support.
As in the Winter War, Swedish volunteers were recruited. Until December 1941, they were tasked with guarding the Soviet naval base at Hanko. When it was evacuated by sea in December 1941, the Swedish unit was officially disbanded. During the Continuation War, the volunteers signed up for three to six months of service. In all, over 1,600 Swedish volunteers fought for Finland, although only about 60 remained by the summer of 1944. About a third of the volunteers had previously participated in the Winter War. Another significant group—about a quarter of the men—were Swedish officers on leave.
From 1942 to 1944 there was also a "Schutzstaffel" (SS) battalion of volunteers on the northern Finnish front recruited from Norway, then under German occupation, and similarly, some Danes. About 3,400 Estonian volunteers took part. On other occasions, the Finns received a total of about 2,100 Soviet prisoners of war in return for those Soviet POWs they turned over to the Germans. These POWs were mainly Estonians and Karelians who were willing to join the Finnish army. These, as well as some volunteers from occupied Eastern Karelia, formed the "Kinship Battalion" (Finnish language: "Heimopataljoona"). At the end of the war, the USSR requested members of the Kinship Battalion to be handed over. Some managed to escape before or during transport, but most of them were either sent to the labor camps or executed.
Finnish occupation policy.
On 19 July 1941, the Finns set up the military administration in occupied East Karelia. The goal of the administration was to prepare the region for eventual incorporation into Finland. In the early stage, the Finns aimed at ethnic cleansing where the Russian population would be moved out of the area once the war was over. They would be replaced with Finnic peoples such as Karelians, Finns, Estonians, Ingrians, and Vepsians. The Russian population was deemed "non-national". Most of the East Karelian population had been evacuated before the Finnish forces arrived. About 85,000 people—mostly the elderly, women, and children—were left behind, and less than half of them were Karelians. A significant number of civilians—almost 30% of the remaining Russians—were interned in concentration camps.
The winter of 1941–42 was an ordeal for the Finnish urban population, due to poor harvests and a shortage of agricultural laborers. However, for the interned it was disastrous; more than 3,500 people died, mostly from starvation. This figure amounted to 13.8% of the inmates, while the corresponding figure for the free population of the occupied territories was 2.6% and for Finland proper 1.4%. Conditions gradually improved, ethnic discrimination in wage levels and food rations were terminated the following year, and new schools were established for the Russian-speaking population. By the end of the occupation, mortality rates dropped to the same levels as in Finland proper.
Soviet partisans.
Soviet partisans conducted a number of operations in Finland and in Eastern Karelia from 1941 to 1944. The major one failed when the 1st Partisan Brigade was destroyed in the beginning of August 1942 at Lake Seesjärvi. Partisans distributed propaganda newspapers, "Pravda" in Finnish and "Lenin's Banner" in Russian. One of the leaders of the partisan movement in Finland and Karelia was Yuri Andropov.
Finnish sources state that partisan activity in East Karelia focused mainly on Finnish military supply and communication targets, but almost two thirds of the attacks on the Finnish side of the border targeted civilians, killing 200 and injuring 50, including children and the elderly.
Jews in Finland.
Finland had a small (approx. 2,300) Jewish population. They had full civil rights and fought with other Finns in the ranks of the Finnish Army. The Germans had mentioned the Finnish Jews at the Wannsee Conference in January 1942, wishing to transport them to Majdanek in General Government. SS leader Heinrich Himmler mentioned the Finnish Jews during his visit in Finland in the summer of 1942. Finnish Prime Minister Jukka Rangell replied that Finland had no "Jewish question". However, there were differences for Jewish refugees in Finland. In November 1942, the Finns handed eight Jewish refugees over to the Gestapo. This raised protests among the Finnish Social Democrat ministers, and after this event no more refugees were handed over. Over 500 Jewish refugees were granted asylum.
The field synagogue in Eastern Karelia was one of the very few functioning synagogues on the Axis side during the war. There were even several cases of Jewish officers of Finland's army being awarded the German Iron Cross, which they declined. German soldiers were treated by Jewish medical officers who sometimes saved the soldiers' lives.
Finland and the Western allies.
The Continuation War represents the only case of a genuinely democratic state participating in World War II on the side of the Axis powers, albeit without being a signatory of the Tripartite Pact. The United Kingdom declared war on Finland on 6 December 1941 (Finnish Independence Day), with Canada and New Zealand declaring war on Finland on 7 December and Australia and South Africa declaring war the next day. U.S. Secretary of State Cordell Hull congratulated the Finnish envoy on 3 October 1941 for the liberation of Karelia but warned Finland not to enter Soviet territory; furthermore, the United States did not declare war on Finland when it went to war with the Axis countries and, together with the UK, approached Soviet Premier Joseph Stalin at the Tehran Conference about acknowledging Finnish independence. However, the U.S. government seized Finnish merchant ships in American ports, and in the summer of 1944 shut down Finnish diplomatic and commercial offices in the United States as a result of President Ryti's treaty with Germany. The U.S. government later warned Finland about the consequences of continued adherence to the Axis.
The best-known British action on Finnish soil was an aircraft carrier strike on German and Finnish ships in the Finnish harbour of Petsamo on 31 July 1941. This attack achieved little except the loss of three British aircraft, but it was intended as a demonstration of British support for its Soviet ally. Later in 1941, Hurricanes of No. 151 Wing RAF, based at Murmansk, provided local air cover for Soviet troops and fighter escorts for Soviet bombers. The British contribution to the war was occasional but significant.
Finnish radio intelligence is said to have participated effectively in German actions against British convoys to Murmansk. Throughout the war, German aircraft operating from airfields in northern Finland attacked British air and naval units based in Murmansk and Archangelsk.
1944: Soviet offensive.
Overtures for peace.
Finland began to actively seek a way out of the war after the disastrous German defeat at the Battle of Stalingrad in February 1943. Edwin Linkomies formed a new cabinet with peace as the top priority. Negotiations were conducted intermittently in 1943–44 between Finland and its representative, Juho Kusti Paasikivi, on one side, and the Western Allies and the Soviet Union on the other, but no agreement was reached. Stalin decided to force Finland to surrender; a bombing campaign on Helsinki followed. The air campaign in February 1944 included three major air attacks involving a total of over 6,000 sorties. Finnish anti-aircraft defences managed to repel the raids as only five percent of the dropped bombs hit their planned targets. Helsinki's air defense included the strategic placing of searchlights and fires as decoys outside the city to lure the Soviet bombers to drop their payloads in what were actually unpopulated areas. Major air attacks also hit Oulu and Kotka, but because of radio intelligence and effective anti-aircraft defences, the number of casualties was small.
Meanwhile, the lengthy and ferocious German defence in Narva aided by the Estonians eliminated Soviet-occupied Estonia as a favorable base for Soviet amphibious invasions and air attacks against Helsinki and other Finnish cities. The tactical success of the army detachment "Narwa" from mid-February to April diminished the hopes of the Stavka to assault Finland and force it into capitulation from Estonia. Finland terminated the negotiations in mid-April 1944, because they considered the Soviet terms to be impossible to fulfill.
Soviet strategic offensive.
On 9 June 1944, the Soviet Union opened a major offensive against Finnish positions on the Karelian Isthmus and in the area of Lake Ladoga (it was timed to accompany D-Day). On the -wide breakthrough segment the Red Army had concentrated 3,000 guns and mortars. In some places, the concentration of artillery pieces exceeded 200 guns for every kilometer of the front (one every ). On that day, Soviet artillery fired over 80,000 rounds along the front on the Karelian Isthmus. On the second day of the offensive, the Soviet forces broke through the Finnish front lines. The Soviets penetrated the second line of defence by the sixth day. The Soviet pressure on the Karelian Isthmus forced the Finns to reinforce the area. This allowed the second Soviet offensive in Eastern Karelia to meet less resistance and to capture Petrozavodsk by 28 June 1944. According to Erickson (1991), James Gebhardt (1989), and Glantz (1998), the main objective of the Soviet offensives was to force Finland from the war.
German help for Finland.
Finland especially lacked modern antitank weaponry which could stop Soviet heavy tanks, and German Foreign Minister Joachim von Ribbentrop offered these in exchange for a guarantee that Finland would not seek a separate peace again. On 26 June, President Risto Ryti gave this guarantee as a personal undertaking, which he intended to last only for the remainder of his presidency. In addition to delivering thousands of hand-held "Panzerfaust" and "Panzerschreck" antitank weapons, Hitler sent the 122nd Infantry Division, the half-strength 303rd Assault Gun Brigade, and "Luftwaffe" "Detachment Kuhlmey" to provide temporary support in the most threatened defense sectors.
With new supplies from Germany, the Finnish army halted the Soviet advance in early July 1944. At this point, the Finnish forces had retreated about one hundred kilometres, which brought them to approximately the same line of defence they had held at the end of the Winter War. This line was known as the VKT-line (short for "Viipuri–Kuparsaari–Taipale"; it ran from Viborg to the River Vuoksi to Lake Ladoga at Taipale), where the Finnish Army stopped the Soviet offensive in the Battle of Tali-Ihantala in spite of Soviet numerical and materiel superiority. The front stabilized once again.
Finland's exit from the war.
A few battles were fought in the latter stages of the war. The last of them was the Battle of Ilomantsi, a Finnish victory, from 26 July to 13 August 1944. The struggle to contain the Soviet offensive was exhausting Finnish resources. The German support under the Ryti-Ribbentrop Agreement had prevented a disaster, but it was believed the country would not be able to hold another major attack. The Soviet advances against German Army Groups Center and North further complicated matters for Finland.
With the front being stable so far, it was a good time for Finland to seek a way out of the war. At the beginning of August President Ryti resigned to allow Finland to sue for peace again, which the new government did in late August. The Soviet peace terms were harsh, but the $600,000,000 reparations demanded in the spring were reduced to $300,000,000, most likely due to pressure from the United States and Britain. However, after the ceasefire the Soviets insisted that the payments should be based on 1938 prices, which doubled the amount. This sum constituted half of Finland's annual gross domestic product in 1939.
Prisoners of war.
Soviet prisoners of war in Finland.
The number of Soviet prisoners of war was estimated to be around 64,000 persons. Of these, 56,000 were captured in 1941. About 2,600 to 2,800 Soviet prisoners of war were handed over to the Germans in exchange for roughly 2,200 Finnic prisoners of war. Food was especially scarce in 1942 in Finland due to a bad harvest, and this was the primary reason for the number of deaths. Out of 64,188 Soviet POWs, 18,318 died in Finnish prisoner of war camps.
Finnish prisoners of war in the Soviet Union.
There are two views of the number of Finnish prisoners of war. The Soviet and Russian view is that of 2,377 Finnish prisoners of war who reached the prison camps 1,954 were returned after the Moscow Armistice. The Finnish view is that of the original approximately 3,500 Finnish prisoners of war, only about 2,000 were returned (more than 40% perished). The difference can be at least partially explained by the Soviet practice of counting only the prisoners who survived to reach a prison camp.
Armistice and aftermath.
Mannerheim had repeatedly reminded the Germans that in case their troops in Estonia retreated, Finland would be forced to make peace even on extremely unfavourable terms. The territory of Estonia would have provided the Soviet army a favourable base for amphibious invasions and air attacks against Finland's capital, Helsinki, and other strategic targets in Finland, and would have severed Finnish access to the sea. The initial German reaction to Finland's announcement of ambitions for a separate peace was limited to only verbal opposition. However, the Germans then arrested hundreds of sailors on Finnish merchant ships in Germany, Denmark, and Norway.
Previously, in return for critically needed food and defense materiel from the Germans, President Ryti had personally committed, in writing, that no separate peace with the Soviets would be attempted. Accordingly, it became clear that he must resign, paving the way for a separate peace. Finland's military leader Mannerheim was appointed president in an extraordinary procedure by the Finnish parliament. In agreeing to take office, he accepted responsibility for ending the war.
On 4 September 1944, the cease-fire ended military actions on the Finnish side. The Soviet Union ended hostilities exactly 24 hours after the Finns. An armistice between the Soviet Union and Finland was signed in Moscow on 19 September. Finland had to make many concessions: the Soviet Union regained the borders of 1940, with the addition of the Petsamo area (now Pechengsky District, Russia); the Porkkala peninsula (adjacent to Helsinki) was leased to the USSR as a naval base for fifty years; and transit rights were granted. Finland's army was to be demobilized with haste, but Finland was first required to expel all German troops from its territory within 14 days. As the Germans did not leave Finland by the given deadline, the Finns fought their former co-belligerents in the Lapland War. Finland was also required to clear the minefields in Karelia (including East Karelia) and in the Gulf of Finland. Retreating German forces had also mined northern Finland heavily. The demining was a long operation, especially in the sea areas, lasting until 1952. One-hundred Finnish army personnel were killed and over 200 wounded during this process, most of them in Lapland.
As sizable numbers of civilians who had been relocated into Finland from Karelia in 1939-40 had moved back into Karelia during the war, they had to be evacuated again; of the 260,000 civilians who had moved back into the Karelia, only 19 chose to remain and become Soviet citizens.
Nevertheless, in contrast to the rest of the Eastern front countries, where the war was fought to the end, a Soviet occupation of Finland did not occur and the country retained sovereignty. Neither did the Communists rise to power as they had in the Eastern Bloc countries. A policy called the "Paasikivi–Kekkonen line" formed the basis of Finnish foreign policy towards the Soviet Union until the Soviet Union's dissolution in 1991.
Analysis.
Finnish reasons for entering the war and nature of Finnish-German relations.
Finland re-entered World War II mainly because of the Soviet invasion of Finland during the Winter War, which had taken place after Finnish intentions of relying on the League of Nations and Nordic neutrality to avoid conflicts had failed from lack of outside support. During the Continuation War, Finland primarily aimed to reverse its territorial losses under the March 1940 Moscow Peace Treaty and, depending on the success of the German invasion of the Soviet Union, to possibly expand, especially into East Karelia (Karelo-Finnish Soviet Socialist Republic). Some right-wing groups also supported a Greater Finland ideology. Henrik Lunde notes that, unlike many of Germany's allies, Finland survived World War II without losing its independence, although the price for war was high in war casualties, reparation payments, territorial loss, a bruised international reputation according to Olli Vehviläinen, and according to some, subsequent Soviet influence on Finland's foreign policy during the Cold War. According to Tuulikki Vuonokari, the Finnish–German alliance was different from most of the other Axis relationships, an example of which was the participation of Finnish Jews in the fight against the Soviet Union. The Finnish government did not take any anti-Jewish measures, despite repeated requests from Nazi Germany. One remarkable aspect of the Finnish-German relationship was that Finland never signed the Tripartite Pact, which was signed by all "de jure" Axis countries. The Finns, and Mannerheim in particular, clearly stated they would fight against the Soviets only to the extent necessary to redress the balance of the 1940 treaty (which would ultimately have dire consequences for Germany, when Finland refused to advance beyond its 1939 borders to complete the German encirclement of Leningrad.) However, for Hitler the matter was irrelevant; and he saw Finland as an ally.
Finland adopted the concept of a "parallel war" whereby it sought to pursue its own objectives in concert with, but separate from, Nazi Germany, as "co-belligerents".
Tides of war.
Major events across Europe and the tides of war in general had a significant impact on the course of World War II in Finland:
Soviet "buffer zones" across Europe.
Soviet sources maintain that Soviet policies up to the Continuation War were best explained as defensive measures by offensive means: The Soviet division of occupied Poland with Germany, the Soviet occupations of Lithuania, Latvia and Estonia, and the Soviet invasion of Finland in the Winter War are described as elements in the Soviets' construction of a security zone or buffer region between the perceived threat from the capitalist powers of Western Europe and the Communist Soviet Union. These Soviet sources see the post-war establishment of Soviet satellite states in the Warsaw Pact countries and the Finnish-Soviet Agreement of Friendship, Cooperation, and Mutual Assistance as the conclusion of this Soviet defense plan. Western historians such as Norman Davies and John Lukacs dispute this view and describe the pre-war Soviet policy as an attempt to stay out of the war and regaining land lost after the fall of the Russian Empire.
Assessment of Soviet designs for Finland and their outcome.
Several Western historians, while noting the Soviets' assertion of their alleged need for a Soviet security buffer, contend the Soviet designs on Finland were no different from the other Baltic countries. American Dan Reiter (1990) notes, “[Finland recognized] that the Soviet Union was unlikely to be satisfied with territorial concessions as a means to increase its security. [T]he Soviets viewed the control of small buffer states as critical to their security...This was the motivation", he asserts, "behind the de facto 1940 Soviet annexation of the Baltic States, and Moscow saw the control of Finland also as ultimately being necessary." Reiter and British historian Victor Rothwell quote Soviet Foreign Minister Molotov as telling his Lithuanian counterpart at the time Lithuania was effectively absorbed into the USSR, “[S]mall states will disappear...Baltic states, including Finland, will be included within the honourable family of Soviet peoples. However, contends Reiter, "[T]he fear of rising costs of fighting pushed Stalin to accept a limited war outcome with Finland, rather than pursue absolute victory", although a contemporary "Soviet document... called for the brutal military occupation of Finland at war’s end." “The Finnish victory [at Ilomantsi] ended the Soviet offensive in Finland and persuaded the Soviets to give up their demand for Finland's unconditional surrender". Peter Provis (1999) concludes his essay on point, “By following [self-censorship and limited appeasement] policies and fulfilling the Soviet Union's demands [for great reparations]...Finland avoided the same fate as other nations that were 'liberated' by the Red Army...Finland had once again defended her independence in a global conflict that engulfed and destroyed many other nations...The Finns had once again demonstrated their determination to avoid defeat by the Soviet Union and maintained their independence".
Russian historian Nikolai Baryshnikov disputes the view that the Soviet Union wanted to deprive Finland of its independence, and that Finnish "defensive victories" prevented this. He argues that there is no documentary evidence for such claims and that the Soviet government was always open for negotiations. Baryshnikov cites the former head of the Office of Information of the Finnish General Staff, Kalle Lehmus, and other Finnish sources to show that the Finnish leaders already knew of the limited Soviet plans for Finland in the first half of July 1944, after intelligence indicated that some Soviet divisions were to be transferred to reserve in Leningrad.

</doc>
<doc id="7713" url="http://en.wikipedia.org/wiki?curid=7713" title="Chinese remainder theorem">
Chinese remainder theorem

The Chinese remainder theorem is a result about congruences in number theory and its generalizations in abstract algebra. It was first published in the 3rd to 5th centuries by Chinese mathematician Sun Tzu.
In its basic form, the Chinese remainder theorem will determine a number "n" that when divided by some given divisors leaves given remainders. For example, what is the lowest number "n" that when divided by 3 leaves a remainder of 2, when divided by 5 leaves a remainder of 3, and when divided by 7 leaves a remainder of 2?
Theorem statement.
The original form of the theorem, contained in the 5th-century book "Sunzi's Mathematical Classic" () by the Chinese mathematician Sun Tzu and later generalized with a complete solution called "Dayanshu" () in Qin Jiushao's 1247 "Mathematical Treatise in Nine Sections" (, "Shushu Jiuzhang"), is a statement about simultaneous congruences.
Suppose are positive integers that are pairwise coprime. Then, for any given sequence of integers , there exists an integer solving the following system of simultaneous congruences.
Furthermore, all solutions of this system are congruent modulo the product, . Hence
Sometimes, the simultaneous congruences can be solved even if the are not pairwise coprime. A solution exists if and only if:
All solutions are then congruent modulo the least common multiple of the .
Sun Tzu's work contains neither a proof nor a full algorithm. What amounts to an algorithm for solving this problem was described by Aryabhata (6th century; see ). Special cases of the Chinese remainder theorem were also known to Brahmagupta (7th century), and appear in Fibonacci's Liber Abaci (1202).
A modern restatement of the theorem in algebraic language is that for a positive integer with prime factorization
we have the isomorphism between a ring and the direct product of its prime power parts:
The theorem can also be restated in the language of combinatorics as the fact that the infinite arithmetic progressions of integers form a Helly family .
Existence and uniqueness.
The existence and uniqueness of the solution can easily be seen through a non-constructive argument:
There are different -tuples of remainders. Let us call this set . On the other hand and each element of corresponds to an element of . Can two numbers correspond to the same member of ? That is, can they have the same set of remainders when divided by ? If they did then would be divisible by each . Since the are relatively prime, would be divisible by their product: . This can't be, so this function is one-to-one. Since , it must be onto as well. Thus we have established the existence of a bijection.
Existence can be seen by an explicit construction of . Let denote the multiplicative inverse of given by the Extended Euclidean algorithm. It is defined exactly when and are coprime; the following construction explains why this condition is needed.
Case of two equations ().
Consider the system:
Since , Bézout's identity implies:
This is true because we are using the inverses provided by the Extended Euclidean algorithm; for any other inverses, this would not necessarily be true, but still be valid .
Multiplying both sides by , we get
If we take the congruence modulo for the right-hand-side expression, it is readily seen that
But we know that , thus this suggests that the coefficient of the first term on the right-hand-side expression can be replaced by . Similarly, we can show that the coefficient of the second term can be substituted by . We can now define the value
and it is seen to satisfy both congruences, for example:
General case.
The same type of construction works in the general case of congruence equations. Let be the product of every modulus then define
and this is seen to satisfy the system of congruences by a similar calculation as before.
Finding the solution with basic algebra and modular arithmetic.
For example, consider the problem of finding an integer such that
Brute-force Approach.
A brute-force approach converts these congruences into sets and writes the elements out to the product of (the solutions modulo 60 for each congruence):
To find an x that satisfies all three congruences, intersect the three sets to get:
Which can be expressed as
Algebraic Approach.
Another way to find a solution is with basic algebra, modular arithmetic, and stepwise substitution.
We start by translating these congruences into equations for some , and :
Start by substituting the from the first equation into the second congruence:
meaning that for some integer . Substitute into the first equation:
Substitute this into the third congruence:
meaning that for some integer . Finally,
So, we have solutions 
Notice that 60 = lcm(3,4,5). If the moduli are pairwise coprime (as they are in this example), the solutions will be congruent modulo their product.
A constructive algorithm to find the solution.
The following algorithm only applies if the are pairwise coprime. (For simultaneous congruences when the moduli are not pairwise coprime, the method of successive substitution can often yield solutions.)
Suppose, as above, that a solution is required for the system of congruences:
Define: . For each , the integers and are coprime. Using the extended Euclidean algorithm we can find such that . Substitute for to arrive at: So the remainder of divided by is . On the other hand, guarantees that divides for . To summarize:
Because of this, and the multiplication rules allowed in congruences, one solution to the system of simultaneous congruences is:
For example, consider the problem of finding an integer such that
Using the extended Euclidean algorithm, for modulo 3 and 20 [4 × 5], we find ; i.e., . For modulo 4 and 15 [3 × 5], we get , i.e. . Finally, for modulo 5 and 12 [3 × 4], we get , i.e. . A solution is therefore . All other solutions are congruent to 191 modulo 60, [3 × 4 × 5], which means they are all congruent to 11 modulo 60.
Note: There are multiple implementations of the extended Euclidean algorithm which will yield different sets of , and . These sets however will produce the same solution; i.e., .
Statement for principal ideal domains.
This statement is a straightforward generalization of the above theorem about integer congruences: is a principal ideal domain, the surjectivity of the map "f" shows that every system of congruences of the form
can be solved for , and the injectivity of the map "f" shows that all the solutions are congruent modulo .
Proof. This map is well-defined and an isomorphism of rings; the inverse isomorphism can be constructed as follows. For each , the elements and are coprime, and therefore there exist elements and in with
Set . Then it is clear that
Thus the inverse of "f" is the map
Statement for general rings.
The general form of the Chinese remainder theorem, which implies all the statements given above, can be formulated for commutative rings and ideals.
Here is a version of the theorem where "R" is not required to be commutative:
Applications.
Sequence Numbering.
The Chinese remainder theorem can be used to construct an elegant Gödel numbering for sequences, which is needed to prove Gödel's incompleteness theorems.
Fast Fourier Transform.
The Good-Thomas fast Fourier transform algorithm exploits a re-indexing of the data based on the Chinese remainder theorem. The Prime-factor FFT algorithm contains an implementation.
RSA Algorithm.
In the RSA algorithm calculations are made modulo , where is a product of two large prime numbers and . 1,024-, 2,048- or 4,096-bit integers are commonly used, making calculations in very time-consuming. By the Chinese remainder theorem, however, these calculations can be done in the isomorphic ring instead. Since and are normally of about the same size, that is about , calculations in the latter representation are much faster. Note that RSA algorithm implementations using this isomorphism are more susceptible to fault injection attacks.
Encryption.
The Chinese remainder theorem can also be used in secret sharing, which consists of distributing a set of shares among a group of people who, all together (but no one alone), can recover a certain secret from the given set of shares. Each of the shares is represented in a congruence, and the solution of the system of congruences using the Chinese remainder theorem is the secret to be recovered. Secret Sharing using the Chinese Remainder Theorem uses, along with the Chinese remainder theorem, special sequences of integers that guarantee the impossibility of recovering the secret from a set of shares with less than a certain cardinality.
Hermite Interpolation.
Solution. Introducing the polynomials
the problem may be equivalently reformulated as a system of simultaneous congruences:
By the Chinese remainder theorem in the principal ideal domain , there is a unique polynomial such that:
A direct construction, in analogy with the above proof for the integer number case, can be performed as follows. Define the polynomials
The partial fraction decomposition of gives polynomials with degrees such that
so that
Then a solution of the simultaneous congruence system is given by the polynomial
and the minimal degree solution is this one reduced modulo , that is the unique with degree less than .
Dedekind's Theorem.
Proof. First assume that is a field, otherwise, replace the integral domain by its quotient field, and nothing will change. We can linearly extend the monoid homomorphisms to -algebra homomorphisms , where is the monoid ring of over . Then, by linearity, the condition
yields
Next, for the two -linear maps and are not proportional to each other. Otherwise and would also be proportional, and thus equal since as monoid homomorphisms they satisfy: , which contradicts the assumption that they are distinct.
Therefore the kernels and are distinct. Since is a field, is a maximal ideal of for every . Because they are distinct and maximal the ideals and are coprime whenever . The Chinese Remainder Theorem (for general rings) yields an isomorphism:
where
Consequently, the map
is surjective. Under the isomorphisms the map corresponds to:
Now,
yields
for every vector in the image of the map . Since is surjective, this means that
for every vector
Consequently, . QED.
Non-commutative case: a caveat.
Sometimes in the commutative case, the conclusion of the Chinese Remainder Theorem is stated as . This version does not hold in the non-commutative case, since , as can be seen from the following case:
Proof. Observe that is formed by all polynomials with an in every term and that every polynomial in vanishes under the substitution . Then clearly . Define a "term in ", as an element of the multiplicative monoid of generated by and , and its degree as the usual degree of the term after the substitution . On the other hand, suppose . Observe that a term in of maximum degree depends on otherwise under the substitution can not vanish. The same happens then for an element . Note that the last , from left to right, in a term of maximum degree in an element of is preceded by more than one . (We are counting here all the preceding s. E.g., in the last is preceded by three s.) This proves that since the last in the term of maximum degree in () is preceded by only one . Hence .
However, it is true in general that implies . To see this, note that , while the opposite inclusion is obvious. Also, we have in general that, provided are pairwise coprime two-sided ideals in , the natural map
is an isomorphism. Note that can be replaced by a sum over all orderings of of their product (or just a sum over enough orderings, using inductively that for coprime ideals ).

</doc>
<doc id="7716" url="http://en.wikipedia.org/wiki?curid=7716" title="Cyril M. Kornbluth">
Cyril M. Kornbluth

Cyril M. Kornbluth (July 2, 1923 – March 21, 1958) was an American science fiction author and a notable member of the Futurians. He used a variety of pen-names, including Cecil Corwin, S. D. Gottesman, Edward J. Bellin, Kenneth Falconer, Walter C. Davies, Simon Eisner, Jordan Park, Arthur Cooke, Paul Dennis Lavond and Scott Mariner. The "M" in Kornbluth's name may have been in tribute to his wife, Mary Byers; Kornbluth's colleague and collaborator Frederik Pohl confirmed Kornbluth's lack of any actual middle name in at least one interview.
Biography.
Kornbluth was born and grew up in Inwood, in New York City, He was of Polish Jewish descent. the son of a "second-generation [American] Jew" who ran his own tailor shop. According to his widow, Kornbluth was a "precocious child", learning to read by the age of three and writing his own stories by the time he was seven. He graduated high school at thirteen, received a CCNY scholarship at fourteen, and was "thrown out for leading a student strike" before graduating.
As a teenager, he became a member of the Futurians, an influential group of science fiction fans and writers. While a member of the Futurians, he met and became friends with Isaac Asimov, Frederik Pohl, Donald A. Wollheim, Robert A. W. Lowndes, and his future wife Mary Byers. He also participated in the Fantasy Amateur Press Association.
Kornbluth served in the US Army during World War II (European 'Theatre'). He received a Bronze Star for his service in the Battle of the Bulge, where he served as a member of a heavy machine gun crew. Upon his discharge, he returned to finish his education, which had been interrupted by the war, at the University of Chicago. While living in Chicago he also worked at Trans-Radio Press, a news wire service. In 1951 he started writing full-time, returning to the East Coast where he collaborated on novels with his old Futurian friends Frederik Pohl and Judith Merril.
Work.
Kornbluth began writing at 15. His first solo story, "The Rocket of 1955," was published in Richard Wilson's fanzine "Escape" (Vol 1 No 2, August 1939); his first collaboration, "Stepsons of Mars," written with Richard Wilson and published under the name "Ivar Towers," appeared in the April 1940 "Astonishing". His other short fiction includes "The Little Black Bag", "The Marching Morons", "The Altar at Midnight", "MS. Found in a Chinese Fortune Cookie", "Gomez" and "The Advent on Channel 12".
"The Little Black Bag" was first adapted for television live on the television show "Tales of Tomorrow" on May 30, 1952. It was later adapted for television by the BBC in 1969 for its "Out of the Unknown" series. In 1970, the same story was adapted by Rod Serling for an episode of his "Night Gallery" series. This dramatization starred Burgess Meredith as the alcoholic Dr. William Fall, who had long lost his doctor's license and become a homeless alcoholic. He finds a bag containing advanced medical technology from the future (2098), which, after an unsuccessful attempt to pawn it, he uses benevolently.
"The Marching Morons" is a look at a far future in which the world's population consists of five billion idiots and a few million geniuses – the precarious minority of the "elite" working desperately to keep things running behind the scenes. In his introduction to "The Best of C.M. Kornbluth", Pohl states that "The Marching Morons" is a direct sequel to "The Little Black Bag": it is easy to miss this, as "Bag" is set in the contemporary present while "Morons" takes place several centuries from now, and there is no character who appears in both stories. The titular black bag in the first story is actually an artifact from the time period of "The Marching Morons": a medical kit filled with self-driven instruments enabling a far-future moron to "play doctor". A future Earth similar to "The Marching Morons" – a civilisation of morons protected by a small minority of hidden geniuses – is used again in the final stages of Kornbluth & Pohl's "Search the Sky".
"MS. Found in a Chinese Fortune Cookie" (1957) is supposedly written by Kornbluth using notes by "Cecil Corwin", who has been declared insane and incarcerated, and who smuggles out in fortune cookies the ultimate secret of life. This fate is said to be Kornbluth's response to the unauthorized publication of "Mask of Demeter" (as by "Corwin" and "Martin Pearson" (Donald A. Wollheim)) in Wollheim's anthology "Prize Science Fiction" in 1953.
Biographer Mark Rich describes the 1958 story "Two Dooms" as one of several stories which are "concern[ed] with the ethics of theoretical science" and which "explore moral quandaries of the atomic age":
Many of Kornbluth's novels were written as collaborations: either with Judith Merril (using the pseudonym Cyril Judd), or with Frederik Pohl. By far the most successful and important of these were the novels "Gladiator-At-Law" and "The Space Merchants". "The Space Merchants" contributed significantly to the maturing and to the wider academic respectability of the science fiction genre, not only in America but also in Europe. Kornbluth also wrote several novels under his own name, the most successful being "The Syndic" and "Not This August".
Death.
Kornbluth died at age 34 in Levittown, New York. Scheduled to meet with Bob Mills in New York City to interview for the position of editor of "The Magazine of Fantasy & Science Fiction", Kornbluth had to shovel out his driveway, which left him running behind. Racing to make his train, he suffered a heart attack on the platform of the train station.
A number of short stories remained unfinished at Kornbluth's death; these were eventually completed and published by Pohl. One of these stories, "The Meeting" ("The Magazine of Fantasy & Science Fiction", November 1972), was the co-winner of the 1973 Hugo Award for Best Short Story; it tied with R. A. Lafferty's "Eurema's Dam." Almost all of Kornbluth's solo SF stories have been collected as "His Share of Glory: The Complete Short Science Fiction of C. M. Kornbluth" (NESFA Press, 1997).
Personality and habits.
Frederik Pohl, in his autobiography "The Way the Future Was", Damon Knight, in his memoir "The Futurians", and Isaac Asimov, in his memoirs "In Memory Yet Green" and "I. Asimov: A Memoir", all give vivid descriptions of Kornbluth as a man of odd personal habits and vivid eccentricities. Among the traits which they describe:
Bibliography.
Collections.
Anthony Boucher praised the collection, saying "Kornbluth's sharp observation is everywhere present, and in most of the stories his bitter insight." P. Schuyler Miller reviewed the collection favorably for "Astounding Science Fiction".
Spider Robinson praised this collection, saying "I haven't enjoyed a book so much in years." Mark Rich wrote, "Critics judging Kornbluth by this anthology, edited by Pohl, have seen a growing bitterness in his later stories. This reflects editorial choice more than reality, because Kornbluth also wrote delightful humor in his last years, in stories not collected here. These tales demonstrate Kornbluth's effective use of everyday individuals from a variety of ethnic backgrounds as well as his well-tuned ear for dialect."
Trivia.
Kornbluth's name is mentioned in Lemony Snicket's "Series of Unfortunate Events" as a member of V.F.D., a secret organization dedicated to the promotion of literacy, classical learning, and crime prevention.

</doc>
<doc id="7720" url="http://en.wikipedia.org/wiki?curid=7720" title="Coprophagia">
Coprophagia

Coprophagia or coprophagy is the consumption of feces. The word is derived from the Greek κόπρος "copros", "feces" and φαγεῖν "phagein", "to eat". Many animal species eat feces as a normal behavior; other species may not normally consume feces but do so under unusual conditions. Coprophagy refers to many kinds of feces eating including eating feces of other species (heterospecifics), of other individuals (allocoprophagy), or its own (autocoprophagy), those once deposited or taken directly from the anus.
In animals.
Invertebrates.
Coprophagous insects consume and redigest the feces of large animals. These feces contain substantial amounts of semi-digested food (herbivores' digestive systems are especially inefficient). A notable feces-eating insect is the dung-beetle and the most common is the fly.
Termites eat one another's feces as a means of obtaining their hindgut protists. Termites and protists have a symbiotic relationship (e.g. with the protozoan "Mixotricha paradoxa") that allows the termites to digest the cellulose in their diet via the protists. It has also been proposed that hormones are passed to offspring in this way.
Vertebrates.
Pigs sometimes eat the feces of herbivores that leave a significant amount of semi-digested matter, including their own. In some cultures, it was common for poor families to collect horse feces to feed their pigs. However, allowing domestic pigs to consume feces contributes to the risk of parasite infection. The pig toilet is an ancient method of feeding pigs on garbage and human feces, and is used in China.
Cattle in the United States are often fed chicken litter due to the high amount of protein and low cost of the feed compared to other sources of protein. It has been reported that this process is made safe in regards to bacteria loading by heating the chicken litter to prior to consumption. There are, however, concerns that the practice of feeding chicken litter to cattle could lead to bovine spongiform encephalopathy (mad-cow disease) because of the crushed bone meal in chicken feed. The U.S. Food and Drug Administration regulates this practice by attempting to prevent the introduction of any part of a cow's brain or spinal cord into livestock feed. Other countries, like Canada, have banned chicken litter for use as a livestock feed.
Capybara, rabbits, hamsters and some other related species are hindgut fermenters which digest cellulose by microbial fermentation. In addition, they extract further nutrition from grass by ingesting their feces meaning their food passes through the gut a second time. Soft fecal pellets of partially digested food are excreted and generally consumed immediately. Consuming these cecotropes (or "night feces" produced in the cecum), is important for obtaining vitamin B12, which intestinal bacteria produce from Cobalt salts. Vitamin B12 must then be re-introduced to the digestive system since only the stomach (and not the intestines) is capable of absorbing vitamin B12. They also produce normal droppings, which are not eaten.
The young of elephants, pandas, koalas, and hippos eat the feces of their mothers or other animals in the herd to obtain the bacteria required to properly digest vegetation found on their ecosystems. When they are born, their intestines do not contain these bacteria (they are sterile). Without them, they would be unable to obtain any nutritional value from plants.
Gorillas eat their own feces and the feces of other gorillas. Similar behavior has also been observed among chimpanzees. Such behavior may serve to improve absorption of vitamins or of nutritive elements made available from the re-ingestion of seeds.
Hamsters, guinea pigs and chinchillas eat their own droppings, which are thought to be a source of vitamins B and K, produced by bacteria in the gut. Apes have been observed eating horse feces for the salt content. Monkeys have been observed eating elephant feces. Coprophagia has also been observed in the naked mole rat.
Mother cats are known to eat the feces of their newborn kittens during the earliest phase after birth, presumably to eliminate cues to potential predators and to keep the den clean.
Some domesticated and wild mammals are known to consume feces. In the wild they either bury or eat waste to protect their trail from predators.
In plants.
Some carnivorous plants, such as pitcher plants of the genus "Nepenthes", obtain nourishment from the feces of commensal animals.
In humans.
In medicine.
Fecal bacteriotherapy is when feces from a close relative or spouse are given to patients suffering from intractable diarrhea caused by "Clostridium difficile". The objective is to repopulate the intestines with the normal gut flora (intestinal bacteria) which kill the clostridium. The healthy stool is administered by nasogastric tube, enema, or in a capsule.
Consuming other people's feces carries the risk of contracting diseases and bacteria spread such as "E. coli", Hepatitis A, Hepatitis E, pneumonia, polio, and influenza. Coprophagia also carries a risk of contracting intestinal parasites.
Lewin reported that "... consumption of fresh, warm camel feces has been recommended by Bedouins as a remedy for bacterial dysentery; its efficacy (probably attributable to the antibiotic subtilisin from "Bacillus subtilis") was confirmed by German soldiers in Africa during World War II".
The introduction of foreign bacteria into the human gastrointestinal tract via infusion of fecal enemas is an established medical practice in cases of ulcerative colitis, especially where the patient's own intestinal flora has been significantly depleted by prior use of antibiotics.
Coprophagia has been observed in a small number of patients with schizophrenia, depression, and pica.
Centuries ago, physicians used to taste their patients' feces, to better judge their state and condition.
In sex.
Some human coprophiles engage in coprophagia as a sexual fetish. Until 1995, the only documented cases of coprophagia in humans were those with schizophrenia or other mental illness, but it has now been shown to occur among relatively mentally healthy individuals. Psychiatrists using the classification system of the DSM-IV would consider this a symptom of the paraphilia called coprophilia - "if the behavior, sexual urges, or fantasies cause clinically significant distress or impairment in social, occupational, or other important areas of functioning". Coprophagia is also depicted in pornography, usually under the term "scat" (from "scatology").

</doc>
<doc id="7721" url="http://en.wikipedia.org/wiki?curid=7721" title="C. L. Moore">
C. L. Moore

Catherine Lucille Moore (January 24, 1911 – April 4, 1987) was an American science fiction and fantasy writer, most often as C. L. Moore. She was one of the first women to write in either genre, and paved the way for many other female speculative fiction writers. She and her first husband Henry Kuttner were prolific co-authors under their own names and three pseudonyms.
Biography.
Moore was born on January 24, 1911 in Indianapolis, Indiana. She was chronically ill as a child and spent much of her time reading literature of the fantastic. She left college during the Great Depression in the United States to work as a secretary at the Fletcher Trust Company in Indianapolis.
Her first stories appeared in pulp magazines during the mid-1930s, including two significant series in "Weird Tales", then edited by Farnsworth Wright. One features the rogue and adventurer Northwest Smith wandering through the Solar System; the other features Jirel of Joiry, one of the first female protagonists in sword-and-sorcery fiction. Both are sometimes named for their lead characters.
The most famous Northwest Smith story is "Shambleau," which was also Moore's first professional sale. It appeared in the November 1933 issue and netted her a hundred dollars.
The most famous Jirel story is also the first one, "Black God's Kiss," which was the October 1934 cover story with subtitle "the weirdest story ever told" (see figure). Her early stories were notable for their emphasis on the senses and emotions, which was unusual at the time.
Moore's work also appeared in "Astounding Science Fiction" magazine throughout the 1940s. Several stories written for that magazine were later collected in her first published book, "Judgment Night" (Gnome Press, 1952). One of the most remarkable was the 1944 novella "No Woman Born," which went on to be included in more than ten different science fiction anthologies including "The Best of C. L. Moore".
Included in that collection were "Judgment Night" (first published in August and September 1943), the lush rendering of a future galactic empire with a sober meditation on the nature of power and its inevitable loss; “The Code” (July 1945), an homage to the classic Faust with modern theories and Lovecraftian dread; "Promised Land" (February 1950) and "Heir Apparent" (July 1950) both documenting the grim twisting that mankind must undergo in order to spread into the solar system; and "Paradise Street" (September 1950), a futuristic take on the Old West conflict between lone hunter and wilderness-taming settlers.
Moore met Henry Kuttner, also a science fiction writer, in 1936 when he wrote her a fan letter under the impression that "C. L. Moore" was a man. They married in 1940 and thereafter wrote almost all of their stories in collaboration—under their own names and using the joint pseudonyms C. H. Liddell, Lawrence O'Donnell, and Lewis Padgett—most commonly the latter, a combination of their mothers' maiden names.
In this very prolific partnership they managed to combine Moore's style with Kuttner's more cerebral storytelling. Their works include a classic, "Mimsy Were the Borogoves," the basis for the film "The Last Mimzy", and "Vintage Season". They also collaborated on a story that combined Moore’s signature characters, Northwest Smith and Jirel of Joiry: "Quest of the Starstone" (1937).
After Kuttner's death in 1958, Moore continued teaching his writing course at the University of Southern California but wrote almost no fiction. She did write for a few television shows under her married name, but upon marrying Thomas Reggie (who was not a writer) in 1963, she ceased writing entirely.
In 1981 Moore received two annual awards for her career in fantasy literature, the World Fantasy Award for Life Achievement, chosen by a panel of judges at the World Fantasy Convention, and the Gandalf Grand Master Award, chosen by vote of participants in the World Science Fiction Convention. (Thus she became the eighth and final Grand Master of Fantasy, sponsored by the Swordsmen and Sorcerers' Guild of America, in partial analogy to the Grand Master of Science Fiction sponsored by the Science Fiction Writers of America.)
C. L. Moore was an active member of the Tom and Terri Pinckard Science Fiction literary salon, and was a frequent contributor to literary discussions with the regular membership including Larry Niven, Norman Spinrad, A. E. van Vogt, Jerry Pournelle, Robert Bloch, George Clayton Johnson, and others, as well as many visiting writers and speakers. She developed Alzheimer's disease but that was not obvious for several years. She had ceased to attend the meetings when she was nominated to be the first woman Grand Master of the Science Fiction Writers of America; the nomination was withdrawn at the request of her husband, Thomas Reggie, who said that the award and ceremony would be at best confusing and likely upsetting to her, given the progress of her disease. That caused dismay among the former SFWA presidents, for she was a great favorite to receive the award. (Former presidents and current officers select a living writer as Grand Master of SF, no more than one annually.)
C. L. Moore died on April 4, 1987 at her home in Hollywood, California after a long battle with Alzheimer's.
The Science Fiction and Fantasy Hall of Fame inducted Moore in 1998, its third class of two deceased and two living writers.

</doc>
<doc id="7722" url="http://en.wikipedia.org/wiki?curid=7722" title="Compactron">
Compactron

Compactrons are a type of thermionic valve, or vacuum tube, which contain arrangements of diodes, triodes, or pentodes in multiple combination arrays, as well as high or low-voltage and power types.
History.
The Compactron is a 12-pin vacuum tube family introduced in 1961 by General Electric in Owensboro, Kentucky to compete with transistorized electronics during the solid state transition. Television sets were a primary application.
Use was prevalent in televisions because transistors were slow to achieve the high power and frequency capabilities needed particularly in color television sets. The first portable color television, the General Electric Porta-Color, was designed using 13 tubes, 10 of which were Compactrons. 
Compactron's integrated valve design helped lower power consumption and heat generation (they were to tubes what integrated circuits were to transistors). Compactrons were also used in a few high end Hi-Fi stereos. They were also used by the Ampeg guitar amplifier company in some of their guitar amps. No modern tube based Hi-Fi systems are known to use this tube type, as simpler and more readily available tubes have again filled this niche.
Notable features.
A distinguishing feature of most Compactrons is the placement of the evacuation tip on the bottom end, rather than the top end as was customary with "miniature" tubes, and a characteristic 3/4" diameter circle pin pattern.
Examples.
Examples of Compactrons type types include:
Due to their specific applications in television circuits, many different Compactron types were produced. Almost all were assigned using standard US tube numbers.
Technological obsolescence.
Integrated circuits (of the analogue and digital type) gradually took over all of the functions that the Compactron was designed for. "Hybrid" television sets produced in the early to mid-1970s made use of a combination of tubes (typically Compactrons), transistors, and integrated circuits in the same set. By the mid-1980s this type of tube was functionally obsolete. Compactrons simply don't exist in any TV sets designed after 1986. Other specialist uses of the tube declined in parallel with the television set manufacture. Manufacture of Compactrons ceased in the early 1990s. New old stock replacements for almost all Compactron types produced are easily found for sale on the Internet.
Of note, in the 1960s, the 6BK11 Triple Triode Compactron tube was used by the Ampeg amplifier company in some of their guitar amps. Ampeg is the only guitar amp manufacturer that used this tube in their amps. The 6BK11 is in increasingly short supply today as an NOS replacement for stock tubes. NOS tubes are very important in guitar amps because they keep the amp performing as it was designed, which means it will produce the desired tone. There is a whole "cottage industry" that exists in producing vacuum tubes that are replacements for stock guitar amp tubes. This is because older, class A, open-ended, hand-wired, tube-driven guitar amps sound and perform much better than solid-state amps. Because they sound so much better, older amps command a premium resale value on the open market, and so in turn, any parts in the amp that may wear out over time or with use, (including the vacuum tubes), also command similar values. The 6BK11 is a good example of this.

</doc>
<doc id="7723" url="http://en.wikipedia.org/wiki?curid=7723" title="Carmichael number">
Carmichael number

In number theory, a Carmichael number is a composite number formula_1 which satisfies the modular arithmetic congruence relation:
for all integers formula_3 for which formula_4 and formula_1 are relatively prime. They are named for Robert Carmichael. The Carmichael numbers are the subset of the Knödel numbers, "K"1.
Overview.
Fermat's little theorem states that that if "p" is a prime number, then for any integer "b", the number "b" "p" − "b" is an integer multiple of "p". Carmichael numbers are composite numbers which have the same property of modular arithmetic congruence. In fact, Carmichael numbers are also called Fermat pseudoprimes or absolute Fermat pseudoprimes. Carmichael numbers are important because they pass the Fermat primality test but are not actually prime. Since Carmichael numbers exist, this primality test cannot be relied upon to prove the primality of a number, although it can still be used to prove a number is composite. This makes tests based on Fermat's Little Theorem risky compared to other more stringent tests such as the Solovay-Strassen primality test or a strong pseudoprime test. Still, as numbers become larger, Carmichael numbers become very rare. For example, there are 20,138,200 Carmichael numbers between 1 and 1021 (approximately one in 50 trillion (5*1013) numbers).
Korselt's criterion.
An alternative and equivalent definition of Carmichael numbers is given by Korselt's criterion.
It follows from this theorem that all Carmichael numbers are odd, since any even composite number that is square-free (and hence has only one prime factor of two) will have at least one odd prime factor, and thus formula_11 results in an even dividing an odd, a contradiction. (The oddness of Carmichael numbers also follows from the fact that formula_12 is a Fermat witness for any even composite number.)
From the criterion it also follows that Carmichael numbers are cyclic.
Discovery.
Korselt was the first who observed the basic properties of Carmichael numbers, but he could not find any examples. In 1910, Carmichael found the first and smallest such number, 561, which explains the name "Carmichael number".
That 561 is a Carmichael number can be seen with Korselt's criterion. Indeed, formula_13 is square-free and formula_14, formula_15 and formula_16.
The next six Carmichael numbers are :
These first seven Carmichael numbers, from 561 to 8911, were all found by the Czech mathematician Václav Šimerka in 1885 (thus preceding not just Carmichael but also Korselt, although Šimerka did not find anything like Korselt's criterion). His work, however, remained unnoticed.
J. Chernick proved a theorem in 1939 which can be used to construct a subset of Carmichael numbers. The number formula_23 is a Carmichael number if its three factors are all prime. Whether this formula produces an infinite quantity of Carmichael numbers is an open question (though it is implied by Dickson's conjecture).
Paul Erdős heuristically argued there should be infinitely many Carmichael numbers. In 1994 it was shown by W. R. (Red) Alford, Andrew Granville and Carl Pomerance that there really do exist infinitely many Carmichael numbers. Specifically, they showed that for sufficiently large formula_1, there are at least formula_25 Carmichael numbers between 1 and formula_1.
Löh and Niebuhr in 1992 found some very large Carmichael numbers, including one with 1,101,518 factors and over 16 million digits.
Properties.
Factorizations.
Carmichael numbers have at least three positive prime factors. The first Carmichael numbers with formula_27 prime factors are :
The first Carmichael numbers with 4 prime factors are :
The second Carmichael number (1105) can be expressed as the sum of two squares in more ways than any smaller number. The third Carmichael number (1729) is the Hardy-Ramanujan Number: the smallest number that can be expressed as the sum of two cubes in two different ways.
Distribution.
Let formula_28 denote the number of Carmichael numbers less than or equal to formula_29. The distribution of Carmichael numbers by powers of 10:
In 1953, Knödel proved the upper bound:
for some constant formula_31.
In 1956, Erdős improved the bound to
for some constant formula_33. He further gave a heuristic argument suggesting that this upper bound should be close to the true growth rate of formula_28. The table below gives approximate minimal values for the constant "k" in the Erdős bound for formula_35 as "n" grows:
In the other direction, Alford, Granville and Pomerance proved in 1994 that for sufficiently large "X",
In 2005, this bound was further improved by Harman to
and then has subsequently improved the exponent to just over formula_38.
Regarding the asymptotic distribution of Carmichael numbers, there have been several conjectures. In 1956, Erdős conjectured that there were formula_39 Carmichael numbers for "X" sufficiently large. In 1981, Pomerance sharpened Erdős' heuristic arguments to conjecture that there are
Carmichael numbers up to "X". However, inside current computational ranges (such as the counts of Carmichael numbers performed by Pinch up to 1021), these conjectures are not yet borne out by the data.
Generalizations.
The notion of Carmichael number generalizes to a Carmichael ideal in any number field "K". For any nonzero prime ideal formula_41 in formula_42, we have formula_43 for all formula_44 in formula_42, where formula_46 is the norm of the ideal formula_41. (This generalizes Fermat's little theorem, that formula_48 for all integers "m" when "p" is prime.) Call a nonzero ideal formula_49 in formula_42 Carmichael if it is not a prime ideal and formula_51 for all formula_52, where formula_53 is the norm of the ideal formula_49. When "K" is formula_55, the ideal formula_49 is principal, and if we let "a" be its positive generator then the ideal formula_57 is Carmichael exactly when "a" is a Carmichael number in the usual sense. 
When "K" is larger than the rationals it is easy to write down Carmichael ideals in formula_42: for any prime number "p" that splits completely in "K", the principal ideal formula_59 is a Carmichael ideal. Since infinitely many prime numbers split completely in any number field, there are infinitely many Carmichael ideals in formula_42. For example, if "p" is any prime number that is 1 mod 4, the ideal ("p") in the Gaussian integers Z["i"] is a Carmichael ideal.
Both prime and Carmichael numbers satisfy the following equality:
Higher-order Carmichael numbers.
Carmichael numbers can be generalized using concepts of abstract algebra.
The above definition states that a composite integer "n" is Carmichael
precisely when the "n"th-power-raising function "p""n" from the ring Z"n" of integers modulo "n" to itself is the identity function. The identity is the only Z"n"-algebra endomorphism on Z"n" so we can restate the definition as asking that "p""n" be an algebra endomorphism of Z"n".
As above, "p""n" satisfies the same property whenever "n" is prime.
The "n"th-power-raising function "p""n" is also defined on any Z"n"-algebra A. A theorem states that "n" is prime if and only if all such functions "p""n" are algebra endomorphisms.
In-between these two conditions lies the definition of Carmichael number of order m for any positive integer "m" as any composite number "n" such that "p""n" is an endomorphism on every Z"n"-algebra that can be generated as Z"n"-module by "m" elements. Carmichael numbers of order 1 are just the ordinary Carmichael numbers.
Properties.
Korselt's criterion can be generalized to higher-order Carmichael numbers, as shown by Howe.
A heuristic argument, given in the same paper, appears to suggest that there are infinitely many Carmichael numbers of order "m", for any "m". However, not a single Carmichael number of order 3 or above is known.

</doc>
