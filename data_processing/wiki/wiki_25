<doc id="4157" url="http://en.wikipedia.org/wiki?curid=4157" title="Brown University">
Brown University

Brown University is a private Ivy League research university in Providence, Rhode Island.
Founded in 1764 as "The College in the English Colony of Rhode Island and Providence Plantations," Brown is the seventh oldest institution of higher education in the United States and one of the nine Colonial Colleges established before the American Revolution. The University comprises The College, the Graduate School, Alpert Medical School, the School of Engineering, the School of Public Health, and the School of Professional Studies (which includes the IE Brown Executive MBA program). Brown's international programs are organized through the Watson Institute for International Studies. The Brown/RISD Dual Degree Program, offered in conjunction with the Rhode Island School of Design, is a five-year course that awards degrees from both institutions.
The undergraduate acceptance rate (8.6 percent for the class of 2018) is among the country's most selective. At its foundation in 1764, Brown was the first college in America to assert in its charter that "Sectarian differences of opinions, shall not make any Part of the Public and Classical Instruction." Its engineering program, established in 1847, was the first in what is now known as the Ivy League. In 1971 Brown's coordinate women's institution, Pembroke College, which had been founded in 1891, was fully merged into the university. Brown's present-day defining characteristic is the New Curriculum—sometimes referred to in education theory as the Brown Curriculum—which was adopted by faculty vote in 1969 after a period of student lobbying. The New Curriculum eliminated mandatory "general education" distribution requirements, made students "the architects of their own syllabus," and allowed them to take any course for a grade of satisfactory or unrecorded no-credit.
Brown's main campus is located in the College Hill Historic District in the city of Providence, the second largest city in New England. The University's neighborhood is a federally listed architectural district with a dense concentration of ancient buildings. On the western edge of the campus, Benefit Street contains "one of the finest cohesive collections of restored seventeenth- and eighteenth-century architecture in the United States". The University buildings surrounding its oldest greens display the full palette of college architecture over a century and a half—Georgian, Federal, Greek Revival, Ruskinian Gothic, Venetian Gothic, Richardsonian Romanesque, and Beaux-Arts.
The school colors are seal brown, cardinal red, and white. The school mascot, chosen in 1904, is the brown bear or Kodiak bear and is named "Bruno." Two large statues of Bruno—on the College Green and at the sports complex—are frequent subjects of tourist photography. The latter statue, a colossal fourteen feet tall, is the work of famed British animal sculptor Nick Bibby and was installed in November 2013. People associated with the University are known as Brunonians.
History.
The Foundation and the Charter.
The history of Brown University may be said to begin in 1761 when three residents of Newport, Rhode Island, drafted a petition to the General Assembly of the colony:
"Your Petitioners propose to open a literary institution or School for instructing young Gentlemen in the Languages, Mathematics, Geography & History, & such other branches of Knowledge as shall be desired. That for this End ... it will be necessary to procure a place for the erecting, & to erect a public Building or Buildings for the boarding of the youth & the Residence of the Professors."
The three petitioners were Ezra Stiles, pastor of Newport's Second Congregation Church and future president of Yale; William Ellery, Jr., future signer of the Declaration of Independence; and Josias Lyndon, future governor of the colony. Stiles and Ellery would two years later be co-authors of the Charter of the College. The editor of Stiles's papers observes that, "This draft of a petition connects itself with other evidence of Dr. Stiles's project for a Collegiate Institution in Rhode Island, before the charter of what became Brown University."
In 1762 there is further documentary evidence that Stiles was making plans for a college. On January 20, Chauncey Whittelsey, pastor of the First Church of New Haven, answered a letter from Stiles:
"The week before last I sent you the Copy of Yale College Charter. ... Should you make any Progress in the Affair of a Colledge, I should be glad to hear of it; I heartily wish you Success therein."
The Philadelphia Association of Baptist Churches also had an eye on Rhode Island, home of the mother church of their denomination, the First Baptist Church in America, founded in Providence in 1638 by Roger Williams. The Baptists were as yet unrepresented among colonial colleges—the Congregationalists had Harvard and Yale, the Episcopalians had the College of William and Mary and King's College (later Columbia), and the Presbyterians had the College of New Jersey (later Princeton). Writing in 1784, Isaac Backus, historian of the New England Baptists and an inaugural Trustee of Brown, described the October 1762 resolution taken at Philadelphia:
"The Philadelphia Association obtained such an acquaintance with our affairs, as to bring them to an apprehension that it was practicable and expedient to erect a college in the Colony of Rhode-Island, under the chief direction of the Baptists; ... Mr. James Manning, who took his first degree in New-Jersey college in September, 1762, was esteemed a suitable leader in this important work."
Manning arrived at Newport in July 1763 and was introduced to Stiles, who agreed to write the Charter for the College. Stiles's first draft was read to the General Assembly in August 1763 and rejected by Baptist members who worried that the College Board of Fellows would underrepresent the Baptists. A revised Charter, written by Stiles and Ellery, was adopted by the Assembly on March 3, 1764.
In September 1764 the inaugural meeting of the College Corporation was held at Newport. Governor Stephen Hopkins was chosen chancellor, former and future governor Samuel Ward was vice chancellor, John Tillinghast treasurer, and Thomas Eyres secretary. The Charter stipulated that the Board of Trustees comprise 22 Baptists, five Quakers, five Episcopalians, and four Congregationalists. Of the 12 Fellows, eight should be Baptists—including the College president—"and the rest indifferently of any or all Denominations."
The Charter was not, as is sometimes supposed, the grant of King George III, but rather an Act of the colonial General Assembly. In two particulars the Charter may be said to be a uniquely progressive document. First, where other colleges had curricular strictures against opposing doctrines, Brown's Charter asserted that "Sectarian differences of opinions, shall not make any Part of the Public and Classical Instruction." Second, according to University historian Walter Bronson, "the instrument governing Brown University recognized more broadly and fundamentally than any other the principle of denominational cooperation." The oft-repeated statement that Brown's Charter alone prohibited a religious test for College membership is inaccurate; other college charters were also liberal in that particular.
James Manning was sworn in as the College's first president in 1765 and served until 1791. In 1770 the College moved from Warren, Rhode Island, to the crest of College Hill overlooking Providence. Solomon Drowne, a freshman in the class of 1773, wrote in his diary on March 26, 1770:
"This day the Committee for settling the spot for the College, met at the New-Brick School House, when it was determined it should be set on ye Hill opposite Mr. John Jenkes; up the Presbyterian Lane."
Presbyterian Lane is the present College Street. The eight-acre site, in two parcels, had been purchased by the Corporation for £219, mainly from Moses Brown and John Brown, the parcels having "formed a part of the original home lots of their ancestor, Chad Brown, and of George Rickard, who bought them from the Indians." University Hall—known until 1823 as "The College Edifice"—was modelled on Nassau Hall at the College of New Jersey. Its construction was managed by the firm of Nicholas Brown and Company, which spent £2844 in the first year building the College Edifice and the adjacent President's House.
The Brown family
The Brown family—Nicholas Brown, his son Nicholas Brown, Junior, class of 1786, John Brown, Joseph Brown, and Moses Brown—were instrumental in moving the College to Providence and securing its endowment. Joseph became a professor of natural philosophy at the College, John served as its treasurer from 1775 to 1796, and Nicholas, Junior, succeeded his uncle as treasurer from 1796 to 1825.
On September 8, 1803, the Corporation voted, "That the donation of $5000 Dollars, if made to this College within one Year from the late Commencement, shall entitle the donor to name the College." In a letter dated September 6, 1804, that appeal was answered by College treasurer Nicholas Brown, Junior, and the Corporation honored its promise: "In gratitude to Mr. Brown, the Corporation at the same meeting voted, 'That this College be called and known in all future time by the Name of Brown University'." Over the years, the benefactions of Nicholas Brown, Junior, would total nearly $160,000, an enormous sum for that period, and included the buildings Hope College and Manning Hall, built 1821-22 and 1834-35.
It is sometimes erroneously supposed that Brown University was "named after" John Brown, whose commercial activity included the transportation of African slaves. In fact, Brown University was named for Nicholas Brown, Junior—philanthropist, progressive, founder of the Providence Athenaeum, co-founder of Butler Hospital, and, crucially, an abolitionist. Under the guidance of his uncle Moses Brown, one of the leading abolitionists of his day, Nicholas Brown, Junior, became a financier of the movement. (The opposing attitudes to the slave trade within the Brown family—from John Brown's unapologetic participation, to Moses and Nicholas Brown's activist opposition—are described in Ricardo Howell, "Slavery, the Brown Family of Providence and Brown University" (2001, published online at Brown.edu), and in Charles Rappleye, "Sons of Providence: The Brown Brothers, the Slave Trade, and the American Revolution" (New York, 2006)).
The American Revolution
In the fall of 1776, with British vessels patrolling Narragansett Bay, the College library was moved out of Providence for safekeeping. On December 7, 1776, six thousand British and Hessian troops under the command of Sir Peter Parker sailed into Newport harbor. In a letter written after the war, College president Manning said:
"The royal Army landed on Rhode Island & took possession of the same: This brought their Camp in plain View from the College with the naked Eye; upon which the Country flew to Arms & marched for Providence, there, unprovided with Barracks they marched into the College & dispossessed the Students, about 40 in Number."
"In the claim for damages presented by the Corporation to the Unted States government," says the University historian, "it is stated that the American troops used it for barracks and hospital from December 10, 1776, to April 20, 1780, and that the French troops used it for a hospital from June 26, 1780, to May 27, 1782." The French troops were those of the Comte de Rochambeau.
The New Curriculum.
In 1850, Brown President Francis Wayland wrote: "The various courses should be so arranged that, insofar as practicable, every student might study what he chose, all that he chose, and nothing but what he chose." Adopted in 1969, the New Curriculum is a milestone in the University's history and is seen as the realization of Wayland's vision.
The curriculum was the result of a paper written by Ira Magaziner and Elliot Maxwell titled "Draft of a Working Paper for Education at Brown University." The paper came out of a year-long Group Independent Study Project (GISP) involving 80 students and 15 professors. The GISP was inspired by student-initiated experimental schools, especially San Francisco State College, and sought ways to "put students at the center of their education" and "teach students how to think rather than just teaching facts."
The paper made concrete proposals for the new curriculum, including interdisciplinary freshman-year courses that would introduce "modes of thought," with instruction from faculty brought together from different disciplines. The aim was to transform the traditional survey course—often experienced passively by first-year students—into a more engaging process, an investigation of the intellectual and philosophical connections between disciplines. A grading option of Satisfactory/No Credit would be introduced to encourage students to try courses outside their grade-point comfort zone. In practice, this grading innovation of the New Curriculum—sometimes misunderstood and mischaracterized—has been its most successful component, responsible, in the decades since its adoption, for uncounted career-changing decisions—studio art swapped for neuroscience, biology swapped for anthropology, mathematics swapped for playwriting (and Pulitzer Prizes).
In the spring of 1969, following student rallies in support of reform, University president Ray Heffner appointed the Special Committee on Curricular Philosophy, tasked with developing specific reforms. The resulting report, called the Maeder Report after its committee chair, was presented to the faculty, which voted the New Curriculum into existence on May 7, 1969. Its key features included:
The Modes of Thought course, a key component in the original conception of the New Curriculum, was early on discontinued, but all of the other elements are still in place. In 2006 the reintroduction of plus/minus grading was broached by persons concerned about grade inflation. After a canvassing of alumni, faculty, and students, including the original authors of the Magaziner-Maxwell Report, the idea was rejected by the College Curriculum Council.
Campus.
Brown is the largest institutional landowner in Providence, with properties on College Hill and in the Jewelry District. The College Hill campus was built contemporarily with the eighteenth- and nineteenth-century precincts that surround it, so that University buildings blend with the architectural fabric of the city. The only indicator of "campus" is a brick and wrought-iron fence on Prospect, George, and Waterman streets, enclosing the College Green and Front Green. The character of Brown's urban campus is, then, European organic rather than American landscaped.
Main campus.
The main campus, comprising 235 buildings and , is on College Hill in Providence's East Side. It is reached from downtown principally by three extremely steep streets—College, Waterman, and Angell—which run through the Benefit Street historic district and the campus of the Rhode Island School of Design. College Street, culminating with Van Wickle Gates at the top of the hill, is especially beautiful, and is the setting for the Convocation and Commencement processions.
Van Wickle Gates
The Van Wickle Gates, dedicated on June 18, 1901, have a pair of smaller side gates that are open year round, and a large central gate that is opened two days a year for Convocation and Commencement. At Convocation the gate opens inward to admit the procession of new students. At Commencement the gate opens outward for the procession of graduates. A Brown superstition is that students who walk through the central gate a second time prematurely will not graduate, although walking backwards is said to cancel the hex. Members of the Brown University Band famously flout the superstition by walking through the gate three times too many, as they annually play their role in the Commencement parade.
The core green spaces of the main campus are the Front (or "Quiet") Green, the College (or "Main") Green, and the Ruth J. Simmons Quadrangle (until 2012 called Lincoln Field). The old buildings on these three greens are the most photographed.
Adjacent to this older campus are, to the south, academic buildings and residential quadrangles, including Wriston, Keeney, and Gregorian quadrangles; to the east, Sciences Park occupying two city blocks; to the north, connected to Simmons Quadrangle by The Walk, academic and residential precincts, including the life sciences complex and the Pembroke Campus; and to the west, on the slope of College Hill, academic buildings, including List Art Center and the Hay and Rockefeller libraries. Also on the slope of College Hill, contiguous with Brown, is the campus of the Rhode Island School of Design.
John Hay Library
The John Hay Library is the second oldest library on campus. It was opened in 1910 and named for John Hay (class of 1858, private secretary to Abraham Lincoln and Secretary of State under two Presidents) at the request of his friend Andrew Carnegie, who contributed half of the $300,000 cost of the building. It is now the repository of the University's archives, rare books and manuscripts, and special collections. Noteworthy among the latter are the Anne S. K. Brown Military Collection (described as "the foremost American collection of material devoted to the history and iconography of soldiers and soldiering"), the Harris Collection of American Poetry and Plays (described as "the largest and most comprehensive collection of its kind in any research library"), the Lownes Collection of the History of Science (described as "one of the three most important private collections of books of science in America"), and (for popularity of requests) the papers of H.P. Lovecraft. The Hay Library is home to one of the broadest collections of incunabula (15th-century printed books) in the Americas, as well as such rarities as the manuscript of Orwell's "Nineteen Eighty-Four" and a Shakespeare First Folio. There are also three books bound in human skin.
John Carter Brown Library
The John Carter Brown Library, founded in 1846, is administered separately from the University, but has been located on the Main Green of the campus since 1904. It is generally regarded as the world's leading collection of primary historical sources pertaining to the Americas before 1825. It houses a very large percentage of the titles published before that date about the discovery, settlement, history, and natural history of the New World. The "JCB", as it is known, published the 29-volume "Bibliotheca Americana", a principal bibliography in the field. Typical of its noteworthy holdings is the best preserved of the eleven surviving copies of the Bay Psalm Book the earliest extant book printed in British North America and the most expensive printed book in the world. There is also a very fine Shakespeare First Folio, added to the collection by John Carter Brown's widow (a Shakespeare enthusiast) on the grounds that it includes "The Tempest", a play set in the New World. The JCB holdings comprise more than 50,000 early titles and about 16,000 modern books, as well as prints, manuscripts, maps, and other items in the library's specialty.
Haffenreffer Museum.
The exhibition galleries of the Haffenreffer Museum of Anthropology, Brown's teaching museum, are located in Manning Hall on the campus's main green. Its one million artifacts, available for research and educational purposes, are located at its Collections Research Center in Bristol, RI. The museum's goal is to inspire creative and critical thinking about culture by fostering interdisciplinary understanding of the material world. It provides opportunities for faculty and students to work with collections and the public, teaching through objects and programs in classrooms and exhibitions. The museum sponsors lectures and events in all areas of anthropology, and also runs an extensive program of outreach to local schools.
The "Walk" connects Pembroke Campus to the main campus. It is a succession of green spaces extending from Ruth Simmons Quadrangle (Lincoln Field) in the south to the Pembroke College monument on Meeting Street in the north. It is bordered by departmental buildings and the Granoff Center for the Creative Arts. A focal point of The Walk will be the Maya Lin-designed water-circulating topographical sculpture of Narragansett Bay, to be installed in 2014 next to the Institute for the Study of Environment and Society.
Pembroke campus.
The Women's College in Brown University, known as Pembroke College, was founded in October 1891. When it merged with Brown in 1971, the Pembroke Campus was absorbed into the Brown campus. The Pembroke campus is centered on a quadrangle that fronts on Meeting Street, where a garden and monument—with scale-model of the quadrangle in bronze—compose the formal entry to the campus. The Pembroke campus is one of the most pleasing spaces at Brown, with noteworthy examples of Victorian and Georgian architecture. The west side of the quadrangle comprises Pembroke Hall (1897), Smith-Buonanno Hall (1907, formerly Pembroke Gymnasium), and Metcalf Hall (1919); the east side comprises Alumnae Hall (1927) and Miller Hall (1910); the quadrangle culminates on the north with Andrews Hall (1947) and its terrace and garden. Pembroke Hall, originally a classroom building and library, now houses the Cogut Center for the Humanities.
East Campus, centered on Hope and Charlesfield streets, was originally the site of Bryant University. In 1969, as Bryant was preparing to move to Smithfield, Rhode Island, Brown bought their Providence campus for $5 million. This expanded the Brown campus by and 26 buildings, included several historic houses, notably the Isaac Gifford Ladd house, built 1850 (now Brown's Orwig Music Library), and the Robert Taft House, built 1895 (now King House). The area was named East Campus in 1971.
Thayer Street runs through Brown's main campus, north to south, and is College Hill's reduced-scale counterpart to Harvard Square or Berkeley's Telegraph Avenue. Restaurants, cafes, bistros, tavernas, pubs, bookstores, second-hand shops, and the like abound. Tourists, people-watchers, buskers, and students from Providence's six colleges make the scene. Half a mile south of campus is Thayer Street's hipper cousin, Wickenden Street. More picturesque and with older architecture, it features galleries, pubs, specialty shops, artist-supply stores, and a regionally famous coffee shop that doubles as a film set (for Woody Allen and others).
Brown Stadium, built in 1925 and home to the football team, is located approximately a mile to the northeast of the main campus. Marston Boathouse, the home of the crew teams, lies on the Blackstone/Seekonk River, to the southeast of campus. Brown's Warren Alpert Medical School is situated in the historic Jewelry District of Providence, near the medical campus of Brown's teaching hospitals, Rhode Island Hospital, Women and Infants Hospital, and Hasbro Children's Hospital. Other University research facilities in the Jewelry District include the Laboratories for Molecular Medicine.
Brown's School of Public Health occupies a landmark modernist building overlooking Memorial Park on the Providence Riverwalk. Brown also owns the Mount Hope Grant in Bristol, Rhode Island, an important Native American and King Philip's War site. Brown's Haffenreffer Museum of Anthropology Collection Research Center, particularly strong in Native American items, is located in the Mount Hope Grant.
Academics.
Presidents.
Brown's current president Christina Hull Paxson took office in 2012. She had previously been dean of the Woodrow Wilson School at Princeton University and a past-chair of Princeton's economics department. In 2014 and 2015 Paxson will preside during the year-long celebration of the 250th anniversary of Brown's founding. Her immediate predecessor as president was Ruth J. Simmons, the first African American president of an Ivy League institution. Simmons will remain at Brown as a professor of Comparative Literature and Africana Studies.
The College.
Founded in 1764, the College is the oldest school of Brown University. About 6,400 undergraduate students are currently enrolled in the College, and 79 concentrations (majors) are offered. Completed concentrations of undergraduates by area are social sciences 42 percent, humanities 26 percent, life sciences 17 percent, and physical sciences 14 percent. The concentrations with the greatest number of students are Biology, History, and International Relations. Brown is one of the few schools in the United States with an undergraduate concentration (major) in Egyptology. Undergraduates can also design an independent concentration if the existing programs do not align with their curricular focus.
35 percent of undergraduates pursue graduate or professional study immediately, 60 percent within 5 years, and 80 percent within 10 years. For the Class of 1998, 75 percent of all graduates have since enrolled in a graduate or professional degree program. The degrees acquired were doctoral 22 percent, master's 35 percent, medicine 28 percent, and law 14 percent.
The highest fields of employment for graduates of the College are business 36 percent, education 19 percent, health/medical 6 percent, arts 6 percent, government 6 percent, and communications/media 5 percent.
The language of the College Charter has been interpreted as discouraging the establishment professional schools. Brown and Princeton are the only Ivy League colleges with neither business school nor law school. Brown recently developed an Executive MBA program in conjunction with one of the leading Business Schools in Europe; IE Business School in Madrid. In this partnership, Brown provides traditional coursework while IE provides most of the business-related subjects.
Brown/RISD Dual Degree Program.
Brown's near neighbor on College Hill is the Rhode Island School of Design (RISD), America's top-ranked art college. Brown and RISD students can cross-register at the two institutions, with Brown students permitted to take as many as four courses at RISD that count towards a Brown degree. The two institutions partner to provide various student-life services and the two student bodies compose a synergy in the College Hill cultural scene.
The Brown/RISD Dual Degree Program, among the most selective in the country, accepts 13 to 15 students each year from more than 700 applicants. It combines the complementary strengths of the two institutions, integrating studio art at RISD with the entire spectrum of Brown's departmental offerings. Students are admitted to the Dual Degree Program for a course lasting five years and culminating in both the Bachelor of Arts (A.B.) degree from Brown and the Bachelor of Fine Arts (B.F.A.) degree from RISD. Prospective students must apply to the two schools separately and be accepted by separate admissions committees. Their application must then be approved by a third Brown/RISD joint committee.
Admitted students spend the first year in residence at RISD completing its "foundation course," and the second year in residence at Brown. Another year at each school ensues, with the fifth year spent according to the student's electives. Program participants are noted for their creative and original approach to cross-disciplinary opportunities, combining, for example, industrial design with engineering, or anatomical illustration with human biology, or philosophy with sculpture, or architecture with urban studies. An annual "BRDD Exhibition" is a well-publicized and heavily attended event, drawing interest and attendees from the wider world of industry, design, the media, and the fine arts.
Theatre and playwriting
Brown's theatre and playwriting programs are among of the best-regarded in the country. Since 2003 seven different Brown graduates have either won (four times) or been nominated for (five times) the Pulitzer Prize—including winners Lynn Nottage '86, Ayad Akhtar '93, Nilo Cruz '94, and Quiara Alegría Hudes '04; and nominees Sarah Ruhl '97 (twice), Gina Gionfriddo '97 (twice), and Stephen Karam '02. In "American Theater" magazine's 2009 ranking of the most-produced American plays, Brown graduates occupied four of the top five places—Peter Nachtrieb '97, Rachel Sheinkin '89, Sarah Ruhl '97, and Stephen Karam '02.
The undergraduate concentration (major) encompasses programs in theatre history, performance theory, playwriting, dramaturgy, acting, directing, dance, speech, and technical production. Applications for doctoral and masters degree programs are made through the University Graduate School. Masters degrees in acting and directing are pursued in conjunction with the Rep MFA program, which partners with one of the country's great regional theatres, Trinity Repertory Company, home of the last longstanding resident acting company in the country. Trinity Rep's present artistic director Curt Columbus succeeded Oskar Eustis in 2006, when Eustis was chosen to lead New York's Public Theater.
The many performance spaces available to Brown students include the Chace and Dowling theaters at Trinity Rep; the McCormack Family, Lee Strasberg, Rites and Reason, Ashamu Dance, Stuart, and Leeds theatres in University departments; the Upstairs Space and Downstairs Space belonging to the wholly student-run Production Workshop; and Alumnae Hall, used by Brown University Gilbert & Sullivan and by Brown Opera Productions. Production design courses utilize the John Street Studio of Eugene Lee, three-time Tony Award-winner.
Writing programs.
Writing at Brown—fiction, non-fiction, poetry, playwriting, screenwriting, electronic writing, mixed media, and the undergraduate writing proficiency requirement—is catered for by various centers and degree programs, and a faculty that has long included nationally and internationally known authors. The undergraduate concentration (major) in literary arts offers courses in fiction, poetry, screenwriting, literary hypermedia, and translation. Graduate programs include the fiction and poetry MFA writing programs in the literary arts department, and the MFA playwriting program in the theatre arts and performance studies department. The non-fiction writing program is offered in the English department. Screenwriting and cinema narrativity courses are offered in the departments of literary arts and modern culture and media. The undergraduate writing proficiency requirement is supported by the Writing Center.
Author prizewinners.
Alumni authors take their degrees across the spectrum of degree concentrations, but a gauge of the strength of writing at Brown is the number of major national writing prizes won. To note only winners since the year 2000: Pulitzer Prize for Fiction-winners Jeffrey Eugenides '82 (2003) and Marilynne Robinson '66 (2005); British Orange Prize-winners Marilynne Robinson '66 (2009) and Madeline Miller '00 (2012); Pulitzer Prize for Drama-winners Nilo Cruz '94 (2003), Lynn Nottage '86 (2009), Quiara Alegría Hudes '04 (2012), and Ayad Akhtar '93 (2013); Pulitzer Prize for Journalism-winners James Risen '77 (twice, 2002, 2006), Mark Maremont '80 (twice, 2003, 2007), Gareth Cook '91 (2005), Peter Kovacs '77 (2006), Stephanie Grace '86 (2006), Mary Swerczek '98 (2006), Jane B. Spencer '99 (2006), Usha Lee McFarling '89 (2007), James Bandler '89 (2007), Amy Goldstein '75 (2009), and David Rohde '90 (twice, 1996, 2009).
Computer science.
Teaching of computer science began at Brown in 1956 when an IBM machine was installed and computing courses were offered through the departments of Economics and Applied Mathematics. In January 1958 an IBM650 was added, the only one of its type between Hartford and Boston. In 1960 Brown's first computer building, designed by Philip Johnson, was opened on George Street and an IBM7070 computer installed the next year. It was given full Departmental status in 1979. In 2009, IBM and Brown announced the installation of a supercomputer (by teraflops standards), the most powerful in the southeastern New England region.
The Hypertext Editing Systems, HES and FRESS, were invented in the 1960s at Brown by Andries van Dam, Ted Nelson, and Bob Wallace, with Nelson coining the word "hypertext". Van Dam's students were instrumental in the origin of the XML, XSLT, and related Web standards. Brown alumni who have distinguished themselves in the computer sciences and industry are listed in the Notable people section, below. They include a principal architect of the Macintosh Operating System, a principal architect of the Intel 80386 microprocessor line, the Microsoft Windows 95 project chief, a CEO of Apple, the current head of the MIT Computer Science and Artificial Intelligence Laboratory, the inaugural chair of the Computing Community Consortium, and design chiefs at Pixar and Industrial Light & Magic, protegees of graphics guru Andries van Dam. The character "Andy" in the animated film "Toy Story" is taken to be an homage to Van Dam from his students employed at Pixar. Van Dam denies this, but a copy of his book ("Computer Graphics: Principles and Practice") appears on Andy's bookshelf in the film. Brown computer science graduate and "Heroes" actor Masi Oka '97, was an animator at Industrial Light & Magic.
The department today is home to The CAVE. This project is a virtual reality room used for everything from three-dimensional drawing classes to tours of the circulatory system for medical students. In 2000 students from Brown's Technology House converted the south face of the Sciences Library into a Tetris game, the first high-rise-building Tetris ever attempted. Code named La Bastille, the game used a personal computer running Linux, a radio-frequency video game controller, eleven circuit boards, a 12-story data network, and over 10,000 Christmas lights.
The Joukowsky Institute for Archaeology.
The Joukowsky Institute for Archaeology and the Ancient World pursues fieldwork and excavations, regional surveys, and academic study of the archaeology and art of the ancient Mediterranean, Egypt, and Western Asia from the Levant to the Caucasus. The Institute has a very active fieldwork profile, with faculty-led excavations and regional surveys presently in Petra, Jordan, in West-Central Turkey, at Abydos in Egypt, and in Sudan, Italy, Mexico, Guatemala, Montserrat in the West Indies, and Providence, Rhode Island.
The Institute's faculty includes cross-appointments from the departments of Egyptology, Assyriology, Classics, Anthropology, and History of Art and Architecture. Faculty research and publication areas include Greek and Roman art and architecture, landscape archaeology, urban and religious architecture of the Levant, Roman provincial studies, the Aegean Bronze Age, and the archaeology of the Caucasus. The Institute offers visiting teaching appointments and postdoctoral fellowships which have, in recent years, included Near Eastern Archaeology and Art, Classical Archaeology and Art, Islamic Archaeology and Art, and Archaeology and Media Studies.
Egyptology and Assyriology
Facing the Joukowsky Institute, across the Front Green, is the Department of Egyptology and Assyriology, formed in 2006 by the merger of Brown's renowned departments of Egyptology and History of Mathematics. It is one of only a handful of such departments in the United States. The curricular focus is on three principal areas: Egyptology (the study of the ancient languages, history, and culture of Egypt), Assyriology (the study of the ancient lands of present-day Iraq, Syria, and Turkey), and the history of the ancient exact sciences (astronomy, astrology, and mathematics). Many courses in the department are open to all Brown undergraduates without prerequisite, and include archaeology, languages, history, and Egyptian and Mesopotamian religions, literature, and science. Students concentrating (majoring) in the department choose a track of either Egyptology or Assyriology. Graduate level study comprises three tracks to the doctoral degree: Egyptology, Assyriology, or the History of the Exact Sciences in Antiquity.
The Watson Institute for International Studies.
The Watson Institute for International Studies is a center for the study of global issues and one of the leading institutes of its type in the country. It occupies an architecturally distinctive building designed by Uruguayan architect Rafael Viñoly. The Institute was initially endowed by Thomas Watson, Jr., Brown class of 1937, former Ambassador to the Soviet Union, and longtime president of IBM. Institute faculty presently include, or formerly included, Italian prime minister and European Commission president Romano Prodi, Brazilian president Fernando Henrique Cardoso, Chilean president Ricardo Lagos Escobar, Mexican novelist and statesman Carlos Fuentes, Brazilian statesman and United Nations commission head Paulo Sérgio Pinheiro, Indian foreign minister and ambassador to the United States Nirupama Rao, American diplomat and Dayton Peace Accords author Richard Holbrooke (Brown '62), and Sergei Khrushchev, editor of the papers of his father Nikita Khrushchev, leader of the Soviet Union.
The Institute's curricular interest is organized into the principal themes of development, security, and governance—with further focuses on globalization, economic uncertainty, security threats, environmental degradation, and poverty. Three Brown undergraduate concentrations (majors) are hosted by the Watson Institute—Development Studies, International Relations, and Public Policy. The Institute is also home to various centers, including the Brazil Initiative, Brown-India Initiative, China Initiative, Middle East Studies center, and the Center for Latin American and Caribbean Studies (CLACS). In recent years, the most internationally-cited product of the Watson Institute has been its Costs of War Project, first released in 2011 and continuously updated. The Project comprises a team of economists, anthropologists, political scientists, legal experts, and physicians, and seeks to calculate the economic costs, human casualties, and impact on civil liberties of the wars in Iraq, Afghanistan, and Pakistan since 2001.
The School of Engineering.
Established in 1847, Brown's engineering program is the oldest in the Ivy League and the third oldest civilian engineering program in the country, preceded only by Rensselaer Polytechnic Institute (1824) and Union College (1845). In 1916 the departments of electrical, mechanical, and civil engineering were merged into a Division of Engineering, and in 2010 the division was elevated to a School of Engineering.
Engineering at Brown is especially interdisciplinary. The School is organized without the traditional departments or boundaries found at most schools, and follows a model of connectivity between disciplines—including biology, medicine, physics, chemistry, computer science, the humanities and the social sciences. The School practices an innovative clustering of faculties in which engineers team with non-engineers to bring a convergence of ideas.
The Pembroke Center
The Pembroke Center for Teaching and Research on Women was established at Brown in 1981 by Joan Wallach Scott as a research center on gender. It was named for Pembroke College, the former women's coordinate college at Brown, and is affiliated with Brown's Sarah Doyle Women's Center. It supports the undergraduate concentration in Gender and Sexuality Studies, post-doctoral research fellowships, the annual Pembroke Seminar, and other academic programs. The Center also manages various collections, archives, and resources, including the Elizabeth Weed Feminist Theory Papers and the Christine Dunlap Farnham Archive.
The Graduate School
Established in 1887, the Graduate School has around 2,000 students studying over 50 disciplines. 20 different master's degrees are offered as well as Ph.D. degrees in over 40 subjects ranging from applied mathematics to public policy. Overall, admission to the Graduate School is most competitive with an acceptance rate of about 18 percent.
Alpert Medical School.
The University's medical program started in 1811, but the school was suspended by President Wayland in 1827 after the program's faculty declined to live on campus (a new requirement under Wayland). In 1975, the first M.D. degrees from the new Program in Medicine were awarded to a graduating class of 58 students. In 1991, the school was officially renamed the Brown University School of Medicine, then renamed once more to Brown Medical School in October 2000. In January 2007, Warren Alpert donated $100 million to Brown Medical School, in recognition of which its name was changed to the Warren Alpert Medical School of Brown University.
In 2014 "U.S. News & World Report" ranked Brown's medical school the 5th most selective in the country, with an acceptance rate of 2.9 percent.
"U.S. News" ranks it 29th for research and 28th in primary care.
The medical school is known especially for its eight-year Program in Liberal Medical Education (PLME), inaugurated in 1984 and one of the most selective programs in the nation. Each year, approximately 60 high school students matriculate into the PLME out of an applicant pool of about 1,600. Since 1976, the Early Identification Program (EIP) has encouraged Rhode Island residents to pursue careers in medicine by recruiting sophomores from Providence College, Rhode Island College, the University of Rhode Island, and Tougaloo College. In 2004, the school once again began to accept applications from premedical students at other colleges and universities via AMCAS like most other medical schools. The medical school also offers combined degree programs leading to the M.D./Ph.D., M.D./M.P.H. and M.D./M.P.P. degrees.
The Marine Biological Laboratory.
The Marine Biological Laboratory (MBL) is an independent research institution established in 1882 at Woods Hole, Massachusetts. The laboratory is linked to 54 current or past Nobel Laureates who have been research or teaching faculty. Since 2005 the MBL and Brown have collaborated in a Ph.D. program in biological and environmental sciences that combines faculty at both institutions, including the faculties of the Ecosystems Center, the Bay Paul Center, the Program in Cellular Dynamics, and the Marine Resources Center.
Admission and financial aid.
Brown is among the most selective universities in the country. For the undergraduate class of 2018, Brown received 30,432 applicants, of whom 2,619 were accepted, for an acceptance rate of 8.6 percent. Additionally, for the academic year 2013-14 there were 1,769 applicants from other colleges who wished to transfer to Brown, of whom 6 percent were accepted. According to the university, of applicants who were either salutatorian or valedictorian, 15 and 18 percent respectively were accepted. In the enrolled class of 2017, 94 percent were in the top tenth of their high school class. In 2013 the Graduate School accepted 17 percent of 9,215 applicants. In 2014, "U.S. News" ranked Brown's Warren Alpert Medical School the 5th most selective in the country, with an acceptance rate of 2.9 percent.
Brown admission policy is stipulated need-blind for all domestic applicants. Brown's financial aid basics website, in August 2014, stated: "For families (including both parents) with a total income below $60,000, and assets less than $100,000, no parent contribution is calculated towards the Expected Family Contribution (EFC). For families with total income below $100,000, the loan component of the financial aid award is replaced with additional scholarship." In 2013-14, the program awarded need-based scholarships worth $95 million. The average need-based award for the class of 2017 was $43,427.
Sustainability
Brown has committed to "minimize its energy use, reduce negative environmental impacts and promote environmental stewardship." The Energy and Environmental Advisory Committee has developed a set of ambitious goals for the university to reduce its carbon emissions and eventually achieve carbon neutrality. The "Brown is Green" website collects information about Brown's progress toward greenhouse gas emissions reductions and related campus initiatives, such as student groups, courses, and research. Brown's grade of A-minus was the top one issued in the 2009 report of the Sustainable Endowments Institute (no A-grade was issued).
Brown has a number of active environmental leadership groups on campus. These groups have begun a number of campus-wide environmental initiatives—including promoting the reduction of supply and demand of bottled water and investigating a composting program.
Athletics.
Brown is a member of the Ivy League athletic conference, which is categorized as a Division I (top level) conference of the National Collegiate Athletic Association (NCAA). The Brown Bears are the third largest university sports program in the United States, sponsoring 38 varsity intercollegiate teams (Harvard sponsors 42 and Princeton 39). Brown's athletic program is one of the "U.S. News & World Report" top 20—the "College Sports Honor Roll"—based on breadth of program and athletes' graduation rates. Brown's newest varsity team is women's rugby, promoted from club-sport status in 2014.
Brown women's rowing has won 7 national titles in the last 14 years. Brown men's rowing perennially finishes in the top 5 in the nation, most recently winning silver, bronze, and silver in the national championship races of 2012, 2013, and 2014. The men's and women's crews have also won championship trophies at the Henley Royal Regatta and the Henley Women's Regatta. Brown's men's soccer is consistently ranked in the top 20, and has won 18 Ivy League titles overall; recent soccer graduates play professionally in Major League Soccer and overseas. Brown football, under its most successful coach historically, Phil Estes, won Ivy League championships in 1999, 2005, and 2008. (Brown football's reemergence is credited to its 1976 Ivy League championship team, "The Magnificent Andersons," so named for its coach, John Anderson.) High-profile alumni of the football program include Houston Texans head coach Bill O'Brien; Penn State football coach Joe Paterno, Heisman Trophy namesake John W. Heisman, and Pollard Award namesake Fritz Pollard. The Men's Lacrosse team also has a long and storied history. Brown women's gymnastics won the Ivy League tournament in 2013 and 2014. Brown varsity equestrian has won the Ivy League championship several times. Brown also supports competitive intercollegiate club sports, including sailing and ultimate frisbee. The men's ultimate team, Brownian Motion, has twice won the national championship, in 2000 and 2005.
The first intercollegiate ice hockey game in America was played between Brown and Harvard on January 19, 1898. The first university rowing regatta larger than a dual-meet was held between Brown, Harvard, and Yale at Lake Quinsigamond in Massachusetts on July 26, 1859
Student life.
Residential and Greek societies
About 12 percent of Brown students are in fraternities and sororities. There are 11 residential Greek houses: six fraternities (Alpha Epsilon Pi, Delta Phi, Delta Tau, Phi Kappa Psi, Sigma Chi, and Theta Delta Chi; three sororities (Alpha Chi Omega, Kappa Alpha Theta, and Kappa Delta), one co-ed house (Zeta Delta Xi), and one co-ed literary society (Alpha Delta Phi). Phi Sigma Kappa fraternity was present on campus from 1906 to 1939, but was unable to reactivate after WWII due to wartime losses. All recognized Greek-letter organizations are located on campus in Wriston Quadrangle in University-owned housing. They are overseen by the Greek Council.
An alternative to Greek-letter organizations are the program houses organized by themes. As with Greek houses, the residents of program houses select their new members, usually at the start of the Spring semester. Examples of program houses are St. Anthony Hall (located in King House), Buxton International House, the Machado French/Hispanic House, Technology House, Harambee (African culture) House, Social Action House and Interfaith House.
Currently, there are three student cooperative houses at Brown. Two of them, Watermyn and Finlandia on Waterman Street, are owned by the Brown Association for Cooperative Housing (BACH), an non-profit corporation owned by its members. The third co-op, West House, is located in a Brown-owned house on Brown Street. The three organizations run a vegetarian co-op for the larger community.
Societies and clubs
The earliest societies at Brown were devoted to oration and debate. The Pronouncing Society is mentioned in the diary of Solomon Drowne, class of 1773, who was voted its president in 1771. It seems to have disappeared during the Revolutionary War. We next hear of the Misokosmian Society, founded in 1794 and renamed the Philermenian Society in 1798. This was effectively a secret society with membership limited to 45. It met fortnightly to hear speeches and debate and thrived until the Civil War; in 1821 its library held 1594 volumes. In 1799 a chapter of the Philandrian Society, also secret, was established at the College. In 1806 the United Brothers was formed as an egalitarian alternative to the Philermenian Society. "These two great rivals," says the University historian, "divided the student body between them for many years, surviving into the days of President Sears. A tincture of political controversy sharpened their rivalry, the older society inclining to the aristocratic Federals, the younger to the Republicans, the democrats of that day. ... The students continuing to increase in number, they outran the constitutional limits of both societies, and a third, the Franklin Society, was established in 1824; it never had the vitality of the other two, however, and died after ten years." Other nineteenth century clubs and societies, too numerous to treat here, are described in Bronson's history of the University.
The Cammarian Club—founded in 1893 and taking its name from the Latin for lobster, its members' favorite dinner food—was at first a semi-secret society which "tapped" 15 seniors each year. In 1915, self-perpetuating membership gave way to popular election by the student body, and thenceforward the Club served as the "de facto" undergraduate student government. In 1971, unaccountably, it voted the name Cammarian Club out of existence, thereby amputating its tradition and longevity. The successor and present-day organization is the generically-named Undergraduate Council of Students.
Societas Domi Pacificae, known colloquially as "Pacifica House," is a present-day, self-described secret society, which nonetheless publishes a website and an email address. It claims a continuous line of descent from the Franklin Society of 1824, citing a supposed intermediary "Franklin Society" traceable in the nineteenth century. But the intermediary turns out to be, on closer inspection, the well-known Providence Franklin Society, a civic organization unconnected to Brown whose origins and activity are well-documented. It was founded in 1821 by merchants William Grinnell and Joseph Balch, Jr., and chartered by the General Assembly in January 1823. The "Pacifica House" account of this (conflated) Franklin Society cites published mentions of it in 1859, 1876, and 1883. But the first of these (Rhees 1859, see footnote "infra") is merely a sketch of the 1824 Brown organization; the second (Stockwell 1876) is a reference-book article on the Providence Franklin Society itself; and the third is the Providence Franklin Society's own publication, which the "Pacifica House" reference mis-ascribes to "Franklin Society," dropping the word "Providence."
Student organizations
There are over 300 registered student organizations on campus with diverse interests. The Student Activities Fair, during the orientation program, provides first-year students the opportunity to become acquainted with the wide range of organizations. A sample of organizations includes:
Rankings.
Brown ranked 7th in the country (between Princeton and Columbia) in a study of high school seniors' revealed preferences for matriculation conducted by economists at Harvard, Wharton, and Boston University, and published in 2005 by the National Bureau of Economic Research.
The 2008 Center for College Affordability and Productivity (CCAP) ranked Brown 5th in the country among national universities."
Brown ranked 5th in the country (behind Yale, Princeton, Harvard, and Stanford) in Newsweek/The Daily Beast's "America's Brainiac Schools"—based on the number of prestigious scholarships won (adjusted for student body size), including the Rhodes Scholarship, the Truman Scholarship, the Marshall Scholarship, the Gates Scholarship (since 2001), and the Fulbright scholarship (since 1993). Also factored in are standardized test scores, admissions rates, and students in the top 10 percent of their high school class.
In 2014, "U.S. News" ranked Brown's Warren Alpert Medical School the 5th most selective in the country, with an acceptance rate of 2.9 percent.
In the 2012 evaluation of MFA writing programs by "Poets & Writers Magazine", Brown was ranked 4th in the country, 3rd for selectivity, and 1st in the Ivy League.
The "Forbes magazine" annual ranking of "America's Top Colleges 2014"—which differs from the "U.S. News" by putting research universities and liberal arts colleges in a single sequence—ranked Brown 13th overall and 8th among research universities.
"Forbes" also ranked Brown 7th in the country (between Caltech and Princeton) among "America's Most Entrepreneurial Universities." The "Forbes" ranking was based on an "entrepreneurial ratio" of alumni and students who were business owners compared with alumni and student body population.
"U.S. News & World Report" ranked Brown 16th in a three way tie with University of Notre Dame and Vanderbilt University among national universities in its 2015 edition. The 2013 edition had ranked Brown 4th for undergraduate teaching, tied with Yale.
As it had in 2007 and 2010, the 2011 Princeton Review email poll of college students ranked Brown 1st in the country for "Happiest Students." Brown is 3rd in the country (tied with Stanford) in the number of students awarded Fulbright grants, according to the October 2010 ranking compiled by the "Chronicle of Higher Education".
Notable people.
Brown undergraduate alumni are currently governors of 4 out of the 50 states: Bobby Jindal '91 of Louisiana, Maggie Hassan '80 of New Hampshire, Jack Markell '82 of Delaware, and Lincoln Chafee '75 of Rhode Island.
Father of American public school education Horace Mann (1819), Chair of the Federal Reserve Janet Yellen '67, Chief Justice of the Supreme Court and U.S. Secretary of State Charles Evans Hughes (1881), philanthropist John D. Rockefeller Jr. (1897), U.S. Secretary of State John Hay (1852), U.S. Secretary of State and Attorney General Richard Olney (1856), "Lafayette of the Greek Revolution" and its historian Samuel Gridley Howe (1821).
Prominent alums in business and finance include World Bank President Jim Yong Kim '82, Bank of America CEO Brian Moynihan '81, CNN founder and America's Cup yachtsman Ted Turner '60, IBM chairman and CEO Thomas Watson, Jr. '37, McKinsey & Co. co-founder Marvin Bower '25, NASDAQ's first CEO Gordon Macklin '50, Chase Manhattan Bank CEO Willard C. Butcher '48, Citibank chairman William R. Rhodes '57, Tiffany & Co CEO Walter Hoving '20, Trans World Airlines chairman Charles C. Tillinghast, Jr. '32, Apple Inc. CEO John Sculley '61, Starwood Capital Group founder and CEO Barry Sternlicht '82, Providence Equity Partners founder and CEO Jonathan M. Nelson '77, Facebook CFO David Ebersman '91, financier and "car czar" Steven Rattner '74, and magazine editor John F. Kennedy, Jr. '83.
Important figures in the history of education include civil libertarian and Amherst College president Alexander Meiklejohn, first president of the University of South Carolina Jonathan Maxcy (1787), Bates College founder Oren B. Cheney (1836), longest-serving University of Michigan president (1871–1909) James Burrill Angell (1849), University of California president (1899–1919) Benjamin Ide Wheeler (1875), and Morehouse College's first African-American president John Hope (1894)
Alumni in the computer sciences and industry include Apple Macintosh and Mac OS designer Andy Hertzfeld '75, architect of Intel 386, 486, and Pentium microprocessors John H. Crawford '75, first Microsoft Windows project chief Brad Silverberg, Apple Computer CEO (1983–1993) John Sculley '61, MIT computer science chair John Guttag '71, University of Washington computer science chair Ed Lazowska '72, inventor of the first silicon transistor Gordon Kidd Teal '31.
Alumni in the arts and media include actress Laura Linney '86, actor John Krasinski '01, "Modern Family" actress Julie Bowen '91, "Harry Potter" films actress Emma Watson '14, editor in chief of "The Onion" Cole Bolton '04, MSNBC program hosts Chris Hayes '01 and Alex Wagner '99, NPR program host Ira Glass '82, "About a Boy" actor David Walton '01, singer-composer Mary Chapin Carpenter '81, "" actor Hill Harper, musicians Damian Kulash '98 and Dhani Harrison '02, film directors Todd Haynes '85, Doug Liman '88, and Davis Guggenheim '86, and director-actor Tim Blake Nelson '86, "New Yorker" humorist and Marx Brothers screenwriter S.J. Perelman '25, novelists Nathanael West '24, Jeffrey Eugenides '83, Rick Moody '83, Edwidge Danticat (MFA '93), Marilynne Robinson '66, and Amity Gaige '95, playwrights Sarah Ruhl '97, Lynn Nottage '86, Richard Foreman '59, Alfred Uhry '58, and Nilo Cruz (MFA '94), actress Jo Beth Williams '70, composer Duncan Sheik '92, singer Lisa Loeb '90, Rockefeller Center and Tribune Tower architect Raymond Hood (1902), composer and synthesizer pioneer Wendy Carlos '62, Pulitzer Prize-winning journalist James Risen '77, composer Rusty Magee, political pundit Mara Liasson, 20th Century Fox Film Group president Tom Rothman '76, Black Entertainment Television chairman and CEO Debra L. Lee '76, HBO Sports president Ross Greenburg '77, MTV Films and Nick Movies president Scott Aversano '91, CNN US News Operations president Jonathan Klein '80, Bravo TV president Lauren Zalaznick '84.
Other notable alumni include NASA head during first seven Apollo missions Thomas O. Paine '42, chief scientist NASA Mars and lunar programs James B. Garvin '78, Governor of Wyoming Territory and Governor of Nebraska John Milton Thayer (1841), Governor of Rhode Island Augustus Bourn (1855), diplomat Richard Holbrooke '62, sportscaster Chris Berman '77, Houston Texans head coach Bill O'Brien '92, former Penn State football coach Joe Paterno '50, Heisman Trophy namesake John W. Heisman '91, Pollard Award namesake, Pro Football Hall of Fame member and first black All-American and NFL head coach Fritz Pollard '19, National Security Council counter-terrorism director RP Eddy '94, presidential advisor Ira Magaziner '69, founder of The Gratitude Network and The Intersection event Randy Haykin '85. Also royals and nobles such as Prince Rahim Aga Khan, Prince Faisal bin Al Hussein, Princess Leila Pahlavi of Iran '92, Prince Nikolaos of Greece and Denmark, Prince Nikita Romanov, Princess Theodora of Greece and Denmark, Prince Jaime of Bourbon-Parma, Duke of San Jaime and Count of Bardi, Prince Ra'ad bin Zeid, Lady Gabriella Windsor, Prince Alexander von Fürstenberg and Countess Cosima von Bülow Pavoncelli.
Nobel Laureates Craig Mello '82 and Jerry White '87, Cooley-Tukey FFT algorithm co-originator John Wilder Tukey '36, Gurney Professor of History and Political Science at Harvard Adam Ulam '44, physicist Lewis E. Little '62, Lasker Award winning biologist and founder of microbial pathogenesis Stanley Falkow (PhD '59), MIT neuroscience department chair Mark F. Bear (B.A. '78, Ph.D '84), Penn psychologist, Lasker Award winner and cognitive therapy originator Aaron Beck '50, John Bates Clark Medal winning MIT economist Jerry A. Hausman '68, University of Chicago School of Law dean Daniel Fischel, Chicago Booth economist Randall Kroszner '84, Stanford Law School dean Larry Kramer (legal scholar), '80, and Arthur L. Horwich.
Notable past or current faculty have included Nobel Laureates Lars Onsager, George Stigler, Vernon L. Smith, George Snell and Leon Cooper, Fields Medal winning mathematician David Mumford, mathematician Ulf Grenander, Pulitzer Prize–winning historian Gordon S. Wood, Sakurai Prize winning physicist Gerald Guralnik for co-elucidation of the Higgs mechanism, award-winning physicist John M. Kosterlitz of the Kosterlitz-Thouless transition, computer scientist and inventor of hypertext Andries van Dam, computer scientist Robert Sedgewick, prominent engineers Daniel C. Drucker, L. Ben Freund, and Mayo D. Hersey, BrainGate inventor John Donoghue (Ph.D 79'), neuroscientist Mark F. Bear (B.A '78, Ph.D '82), biologist and prominent advocate of biological evolution Kenneth R. Miller, first president of the American Sociological Association Lester Frank Ward, economists Hyman Minsky, Peter MacAvoy, who was a former member of the US Council of Economic Advisers, William Poole (economist), Ross Levine, Oded Galor and Peter Howitt (economist), former Prime Minister of Italy and former EU chief Romano Prodi, former President of Brazil Fernando Cardoso, former President of Chile Ricardo Lagos, writers Carlos Fuentes, Chinua Achebe, Robert Coover, Robert Creeley and Keith Waldrop, former Presidents of the American Philosophical Association Jaegwon Kim and Ernest Sosa, philosophers Curt Ducasse, Roderick Chisholm, and Martha Nussbaum, linguist Hans Kurath, political scientist James Morone and Senior Fellow Sergei Khrushchev.
References.
Notes

</doc>
<doc id="4158" url="http://en.wikipedia.org/wiki?curid=4158" title="Bill Atkinson">
Bill Atkinson

Bill Atkinson (born 1951) is an American computer engineer and photographer. Atkinson worked at Apple Computer from 1978 to 1990. He received his undergraduate degree from the University of California, San Diego, where Apple Macintosh developer Jef Raskin was one of his professors. Atkinson continued his studies as a graduate student at the University of Washington.
Atkinson was part of the Apple Macintosh development team, and was the creator of the ground-breaking MacPaint application, among others. He also designed and implemented QuickDraw, the fundamental toolbox that the Lisa and Macintosh used for graphics. QuickDraw's performance was essential for the success of the Macintosh's graphical user interface. He also was one of the main designers of the Lisa and Macintosh user interfaces. Atkinson also conceived, designed and implemented HyperCard, the first popular hypermedia system.
Around 1990, General Magic's founding, with Bill Atkinson as one of the three co-founders, met the following press in "Byte" magazine:
The obstacles to General Magic's success may appear daunting, but General Magic is not your typical start-up company. Its partners include some of the biggest players in the worlds of computing, communications, and consumer electronics, and it's loaded with top-notch engineers who have been given a clean slate to reinvent traditional approaches to ubiquitous worldwide communications.
In 2007 Atkinson began working as an outside developer with Numenta, a startup working on computer intelligence. On his work there Atkinson said, "what Numenta is doing is more fundamentally important to society than the personal computer and the rise of the Internet."
Some of Atkinson's noteworthy contributions to the field of computing include:
Atkinson now works as a nature photographer.

</doc>
<doc id="4160" url="http://en.wikipedia.org/wiki?curid=4160" title="Battle of Lostwithiel">
Battle of Lostwithiel

The Battles of Lostwithiel or Lostwithiel Campaign, took place near Lostwithiel and Fowey during the First English Civil War in 1644. It was victory for the Royalists commanded by King Charles over the Parliamentarians commanded by Earl of Essex.
Prelude.
After defeating the Army of Sir William Waller at the Battle of Cropredy Bridge, King Charles marched west in pursuit of the Parliamentarian army of the Earl of Essex, who was invading the Royalist stronghold of Cornwall.
Essex had been misled into believing that he could expect substantial support from the people of Cornwall. When he had reached Bodmin on 28 July, he found that there was no chance of supplies or recruits, and he also learned that the Royalist army was at Launceston, close to his rear. He withdrew to Lostwithiel, covering the port of Fowey. Essex had previously arranged to rendezvous at Fowey with the Parliamentarian fleet under the Earl of Warwick, but no ships appeared. Warwick was unable to leave Portsmouth because of westerly winds.
King Charles's army had been reinforced as it marched, and outnumbered that of Essex by nearly two to one. The first clashes took place on 2 August, but little action took place for several days, as the King waited for all his forces to arrive and Essex waited for the fleet.
Battle.
On 13 August, the Royalists began to attack in earnest, occupying several outposts on the east bank of the River Fowey, making it even more difficult for help to reach Essex. A Parliamentarian attempt to send a relieving force under Lieutenant General Middleton was defeated at Bridgwater in Somerset.
On 21 August, the Royalists attacked Essex's positions north of Lostwithiel, capturing the ruins of Restormel Castle. Royalist cavalry threatened to cut the Parliamentarians off from Fowey. Essex realised that there was no hope of relief and ordered his cavalry to break out of the encirclement. Under Sir William Balfour, they broke through the Royalist lines on the night of 31 August, eventually reaching Plymouth 30 miles to the east.
The increasingly demoralised Parliamentarian infantry fell back towards Fowey in pouring rain. They were forced to abandon several guns which became bogged down in the muddy roads. On 1 September, the pursuing Royalists captured Castle Dore, another ruined fortification which the Parliamentarians were using to anchor their lines. Essex left Sir Philip Skippon, his Sergeant Major General of Foot, in command while he himself escaped to Plymouth in a fishing boat.
On 2 September, Skippon, having been told that his infantry were unable to break out as the cavalry had done, and having been offered generous terms by the King, surrendered 6,000 infantry and all his army's guns and train.
Aftermath.
The disarmed soldiers marched eastward to Portsmouth in continuing bad weather, being continually robbed and threatened by local people. About 1,000 died of exposure and hunger, and 1,000 more deserted or fell sick.
Charles meanwhile wheeled about and marched toward London.
This setback for Parliament in Cornwall, and the last major victory for the Royalists, was reversed by Sir Thomas Fairfax leading the New Model Army at or near Tresillian Bridge, close to Truro on 12 March 1645.

</doc>
<doc id="4162" url="http://en.wikipedia.org/wiki?curid=4162" title="Beeb">
Beeb

The nickname Beeb may refer to:

</doc>
<doc id="4163" url="http://en.wikipedia.org/wiki?curid=4163" title="Bertrand Russell">
Bertrand Russell

Bertrand Arthur William Russell, 3rd Earl Russell, OM, FRS (; 18 May 1872 – 2 February 1970) was a British philosopher, logician, mathematician, historian, social critic and political activist. At various points in his life he considered himself a liberal, a socialist, and a pacifist, but he also admitted that he had never been any of these in any profound sense. He was born in Monmouthshire, into one of the most prominent aristocratic families in Britain.
Russell led the British "revolt against idealism" in the early 20th century. He is considered one of the founders of analytic philosophy along with his predecessor Gottlob Frege, colleague G. E. Moore, and his protégé Ludwig Wittgenstein. He is widely held to be one of the 20th century's premier logicians. With A. N. Whitehead he wrote "Principia Mathematica", an attempt to create a logical basis for mathematics. His philosophical essay "On Denoting" has been considered a "paradigm of philosophy". His work has had a considerable influence on logic, mathematics, set theory, linguistics, artificial intelligence, cognitive science, computer science (see type theory and type system), and philosophy, especially philosophy of language, epistemology, and metaphysics.
Russell was a prominent anti-war activist; he championed anti-imperialism and went to prison for his pacifism during World War I. Later, he campaigned against Adolf Hitler, then criticised Stalinist totalitarianism, attacked the involvement of the United States in the Vietnam War, and was an outspoken proponent of nuclear disarmament. In 1950 Russell was awarded the Nobel Prize in Literature "in recognition of his varied and significant writings in which he champions humanitarian ideals and freedom of thought".
Biography.
Early life and background.
Bertrand Russell was born on 18 May 1872 at Ravenscroft, Trellech, Monmouthshire, into an influential and liberal family of the British aristocracy. His parents, Viscount and Viscountess Amberley, were radical for their times. Lord Amberley consented to his wife's affair with their children's tutor, the biologist Douglas Spalding. Both were early advocates of birth control at a time when this was considered scandalous. Lord Amberley was an atheist and his atheism was evident when he asked the philosopher John Stuart Mill to act as Russell's secular godfather. Mill died the year after Russell's birth, but his writings had a great effect on Russell's life.
His paternal grandfather, the Earl Russell, had been asked twice by Queen Victoria to form a government, serving her as Prime Minister in the 1840s and 1860s.
The Russells had been prominent in England for several centuries before this, coming to power and the peerage with the rise of the Tudor dynasty (see: Duke of Bedford). They established themselves as one of Britain's leading Whig families, and participated in every great political event from the Dissolution of the Monasteries in 1536–40 to the Glorious Revolution in 1688–89 and the Great Reform Act in 1832.
Lady Amberley was the daughter of Lord and Lady Stanley of Alderley. Russell often feared the ridicule of his maternal grandmother, one of the campaigners for education of women.
Childhood and adolescence.
Russell had two siblings: brother Frank (nearly seven years older than Bertrand), and sister Rachel (four years older). In June 1874 Russell's mother died of diphtheria, followed shortly by Rachel's death. In January 1876, his father died of bronchitis following a long period of depression. Frank and Bertrand were placed in the care of their staunchly Victorian paternal grandparents, who lived at Pembroke Lodge in Richmond Park. His grandfather, former Prime Minister Earl Russell, died in 1878, and was remembered by Russell as a kindly old man in a wheelchair. His grandmother, the Countess Russell (née Lady Frances Elliot), was the dominant family figure for the rest of Russell's childhood and youth.
The countess was from a Scottish Presbyterian family, and successfully petitioned the Court of Chancery to set aside a provision in Amberley's will requiring the children to be raised as agnostics. Despite her religious conservatism, she held progressive views in other areas (accepting Darwinism and supporting Irish Home Rule), and her influence on Bertrand Russell's outlook on social justice and standing up for principle remained with him throughout his life. (One could challenge the view that Bertrand stood up for his principles, based on his own well-known quotation: "I would never die for my beliefs, I could be wrong".) Her favourite Bible verse, 'Thou shalt not follow a multitude to do evil' (Exodus 23:2), became his motto. The atmosphere at Pembroke Lodge was one of frequent prayer, emotional repression, and formality; Frank reacted to this with open rebellion, but the young Bertrand learned to hide his feelings.
Russell's adolescence was very lonely, and he often contemplated suicide. He remarked in his autobiography that his keenest interests were in religion and mathematics, and that only his wish to know more mathematics kept him from suicide. He was educated at home by a series of tutors. At age eleven, his brother Frank introduced him to the work of Euclid, which transformed Russell's life.
During these formative years he also discovered the works of Percy Bysshe Shelley. In his autobiography, he writes: "I spent all my spare time reading him, and learning him by heart, knowing no one to whom I could speak of what I thought or felt, I used to reflect how wonderful it would have been to know Shelley, and to wonder whether I should meet any live human being with whom I should feel so much sympathy". Russell claimed that beginning at age 15, he spent considerable time thinking about the validity of Christian religious dogma, which he found very unconvincing. At this age, he came to the conclusion that there is no free will and, two years later, that there is no life after death. Finally, at the age of 18, after reading Mill's "Autobiography", he abandoned the "First Cause" argument and became an atheist.
University and first marriage.
Russell won a scholarship to read for the Mathematical Tripos at Trinity College, Cambridge, and commenced his studies there in 1890, taking as coach Robert Rumsey Webb. He became acquainted with the younger George Edward Moore and came under the influence of Alfred North Whitehead, who recommended him to the Cambridge Apostles. He quickly distinguished himself in mathematics and philosophy, graduating as a high Wrangler in 1893 and becoming a Fellow in the latter in 1895.
Russell first met the American Quaker Alys Pearsall Smith when he was 17 years old. He became a friend of the Pearsall Smith family—they knew him primarily as 'Lord John's grandson' and enjoyed showing him off—and travelled with them to the continent; it was in their company that Russell visited the Paris Exhibition of 1889 and was able to climb the Eiffel Tower soon after it was completed.
He soon fell in love with the puritanical, high-minded Alys, who was a graduate of Bryn Mawr College near Philadelphia, and, contrary to his grandmother's wishes, married her on 13 December 1894. Their marriage began to fall apart in 1901 when it occurred to Russell, while he was cycling, that he no longer loved her. She asked him if he loved her and he replied that he didn't. Russell also disliked Alys's mother, finding her controlling and cruel. It was to be a hollow shell of a marriage and they finally divorced in 1921, after a lengthy period of separation.
During this period, Russell had passionate (and often simultaneous) affairs with a number of women, including Lady Ottoline Morrell and the actress Lady Constance Malleson.
Early career.
Russell began his published work in 1896 with "German Social Democracy", a study in politics that was an early indication of a lifelong interest in political and social theory. In 1896 he taught German social democracy at the London School of Economics, where he also lectured on the science of power in the autumn of 1937. He was a member of the Coefficients dining club of social reformers set up in 1902 by the Fabian campaigners Sidney and Beatrice Webb.
He now started an intensive study of the foundations of mathematics at Trinity.
In 1898 he wrote "An Essay on the Foundations of Geometry" which discussed the Cayley-Klein metrics used for non-Euclidean geometry.
He attended the International Congress of Philosophy in Paris in 1900 where he met Giuseppe Peano and Alessandro Padoa. The Italians had responded to Georg Cantor, making a science of set theory; they gave Russell their literature including the Formulario mathematico. Russell was impressed by the precision of Peano's arguments at the Congress, read the literature upon returning to England, and came upon Russell's paradox. In 1903 he published "The Principles of Mathematics", a work on foundations of mathematics. It advanced a thesis of logicism, that mathematics and logic are one and the same.
At the age of 29, in February 1901, Russell underwent what he called a "sort of mystic illumination", after witnessing Whitehead's wife's acute suffering in an angina attack. "I found myself filled with semi-mystical feelings about beauty... and with a desire almost as profound as that of the Buddha to find some philosophy which should make human life endurable", Russell would later recall. "At the end of those five minutes, I had become a completely different person."
In 1905 he wrote the essay "On Denoting", which was published in the philosophical journal "Mind". Russell became a fellow of the Royal Society in 1908. The three-volume "Principia Mathematica", written with Whitehead, was published between 1910 and 1913. This, along with the earlier "The Principles of Mathematics", soon made Russell world-famous in his field.
In 1910 he became a lecturer in the University of Cambridge, where he was approached by the Austrian engineering student Ludwig Wittgenstein, who became his PhD student. Russell viewed Wittgenstein as a genius and a successor who would continue his work on logic. He spent hours dealing with Wittgenstein's various phobias and his frequent bouts of despair. This was often a drain on Russell's energy, but Russell continued to be fascinated by him and encouraged his academic development, including the publication of Wittgenstein's "Tractatus Logico-Philosophicus" in 1922. Russell delivered his lectures on Logical Atomism, his version of these ideas, in 1918, before the end of the First World War. Wittgenstein was, at that time, serving in the Austrian Army and subsequently spent nine months in an Italian prisoner of war camp at the end of the conflict.
First World War.
During the First World War, Russell was one of the very few people to engage in active pacifist activities, and in 1916, he was dismissed from Trinity College following his conviction under the Defence of the Realm Act.
Russell played a significant part in the "Leeds Convention" in June 1917 — a historic event which saw well over a thousand "anti-war socialists" gather; many being delegates from the Independent Labour Party and the Socialist Party, united in their pacifist beliefs and advocating a peace settlement. The international press reported that Russell appeared alongside a number of Labour MPs, including both the future Prime Minister, Ramsey McDonald, and the future Chancellor of the Exchequer, Philip Snowden and that former Liberal MP, and anti-conscription campaigner, Professor Arnold Lupton, was also a guest. After the event, Russell told Lady Ottoline that, "to my surprise, when I got up to speak, I was given the greatest ovation that was possible to give anybody".
The Trinity incident resulted in Russell being charged a fine of £100, which he refused to pay, hoping that he would be sent to prison. However, his books were sold at auction to raise the money. The books were bought by friends; he later treasured his copy of the King James Bible that was stamped "Confiscated by Cambridge Police".
A later conviction for publicly lecturing against inviting the US to enter the war on Britain's side resulted in six months' imprisonment in Brixton prison (see "Bertrand Russell's views on society") in 1918.
While in prison, Russell read enormously, and wrote the book "Introduction to Mathematical Philosophy".
He was reinstated in 1919, resigned in 1920, was Tarner Lecturer 1926, and became a Fellow again in 1944 and remained as such until 1949.
In 1924, Bertrand again gained press attention when attending a "banquet" in the House of Commons with well-known campaigners, including Arnold Lupton, who had been both a Member of Parliament and had also endured imprisonment for "passive resistance to military or naval service".
Between the wars.
In August 1920 Russell travelled to Russia as part of an official delegation sent by the British government to investigate the effects of the Russian Revolution. He met Vladimir Lenin and had an hour-long conversation with him. In his autobiography, he mentions that he found Lenin rather disappointing, sensing an "impish cruelty" in him and comparing him to "an opinionated professor". He cruised down the Volga on a steamship. His experiences destroyed his previous tentative support for the revolution. He wrote a book "The Practice and Theory of Bolshevism" about his experiences on this trip, taken with a group of 24 others from Britain, all of whom came home thinking well of the régime, despite Russell's attempts to change their minds. For example, he told them that he heard shots fired in the middle of the night and was sure these were clandestine executions, but the others maintained that it was only cars backfiring.
Russell's lover Dora Black, a British author, feminist and socialist campaigner, visited Russia independently at the same time; in contrast to his reaction, she was enthusiastic about the revolution.
The next autumn Russell went, accompanied by Dora, to Peking (as it was then known in the West) to lecture on philosophy for one year. He went with optimism and hope, seeing China as then being on a new path. Other scholars present in China at the time included John Dewey and Rabindranath Tagore, the Indian Nobel-laureate poet. Before leaving China, Russell became gravely ill with pneumonia, and incorrect reports of his death were published in the Japanese press. When the couple visited Japan on their return journey, Dora took on the role of spurning the local press by handing out notices reading "Mr. Bertrand Russell, having died according to the Japanese press, is unable to give interviews to Japanese journalists". Apparently they found this harsh and reacted resentfully.
Dora was six months pregnant when the couple returned to England on 26 August 1921. Russell arranged a hasty divorce from Alys, marrying Dora six days after the divorce was finalised, on 27 September 1921. Their children were John Conrad Russell, 4th Earl Russell, born on 16 November 1921, and Katharine Jane Russell (now Lady Katharine Tait), born on 29 December 1923. Russell supported his family during this time by writing popular books explaining matters of physics, ethics, and education to the layman. Some have suggested that at this point he had an affair with Vivienne Haigh-Wood, the English governess and writer, and first wife (the Eliots did not formally separate until 1933) of T. S. Eliot.
Together with Dora, he founded the experimental Beacon Hill School in 1927. The school was run from a succession of different locations, including its original premises at the Russells' residence, Telegraph House, near Harting, West Sussex. On 8 July 1930 Dora gave birth to her third child Harriet Ruth. After he left the school in 1932, Dora continued it until 1943.
Upon the death of his elder brother Frank, in 1931, Russell became the 3rd Earl Russell. He once said that his title was primarily useful for securing hotel rooms.
Russell's marriage to Dora grew increasingly tenuous, and it reached a breaking point over her having two children with an American journalist, Griffin Barry. They separated in 1932 and finally divorced. On 18 January 1936, Russell married his third wife, an Oxford undergraduate named Patricia ("Peter") Spence, who had been his children's governess since 1930. Russell and Peter had one son, Conrad Sebastian Robert Russell, 5th Earl Russell, who became a prominent historian and one of the leading figures in the Liberal Democratic party.
During the 1930s, Russell became a close friend and collaborator of V. K. Krishna Menon, then secretary of the India League, the foremost lobby for Indian independence in Great Britain.
Second World War.
Russell opposed rearmament against Nazi Germany, but in 1940 changed his view that avoiding a full-scale world war was more important than defeating Hitler. He concluded that Adolf Hitler taking over all of Europe would be a permanent threat to democracy. In 1943, he adopted a stance toward large-scale warfare, "Relative Political Pacifism": War was always a great evil, but in some particularly extreme circumstances, it may be the lesser of two evils.
Before World War II, Russell taught at the University of Chicago, later moving on to Los Angeles to lecture at the UCLA Department of Philosophy. He was appointed professor at the City College of New York (CCNY) in 1940, but after a public outcry the appointment was annulled by a court judgment that pronounced him "morally unfit" to teach at the college due to his opinions—notably those relating to sexual morality, detailed in "Marriage and Morals" (1929). The protest was started by the mother of a student who would not have been eligible for his graduate-level course in mathematical logic; many intellectuals, led by John Dewey, protested at his treatment. Albert Einstein's oft-quoted aphorism that "great spirits have always encountered violent opposition from mediocre minds" originated in his open letter supporting Russell's appointment dated 19 March 1940, to Morris Raphael Cohen, a professor emeritus at CCNY. Dewey and Horace M. Kallen edited a collection of articles on the CCNY affair in "The Bertrand Russell Case". He soon joined the Barnes Foundation, lecturing to a varied audience on the history of philosophy; these lectures formed the basis of "A History of Western Philosophy". His relationship with the eccentric Albert C. Barnes soon soured, and he returned to Britain in 1944 to rejoin the faculty of Trinity College.
Later life.
During the 1940s and 1950s, Russell participated in many broadcasts over the BBC, particularly "The Brains Trust" and the Third Programme, on various topical and philosophical subjects. By this time Russell was world-famous outside academic circles, frequently the subject or author of magazine and newspaper articles, and was called upon to offer opinions on a wide variety of subjects, even mundane ones. En route to one of his lectures in Trondheim, Russell was one of 24 survivors (among a total of 43 passengers) in an aeroplane crash in Hommelvik in October 1948. He said he owed his life to smoking since the people who drowned were in the non-smoking part of the plane. "A History of Western Philosophy" (1945) became a best-seller and provided Russell with a steady income for the remainder of his life.
In 1943, Russell expressed support for Zionism: "I have come gradually to see that, in a dangerous and largely hostile world, it is essential to Jews to have some country which is theirs, some region where they are not suspected aliens, some state which embodies what is distinctive in their culture".
In a speech in 1948, Russell said that if the USSR's aggression continued, it would be morally worse to go to war after the USSR possessed an atomic bomb than before it possessed one, because if the USSR had no bomb the West's victory would come more swiftly and with fewer casualties than if there were atom bombs on both sides. At that time, only the United States possessed an atomic bomb, and the USSR was pursuing an extremely aggressive policy towards the countries in Eastern Europe which it was absorbing into its sphere of influence. Many understood Russell's comments to mean that Russell approved of a first strike in a war with the USSR, including Nigel Lawson, who was present when Russell spoke. Others, including Griffin, who obtained a transcript of the speech, have argued that he was merely explaining the usefulness of America's atomic arsenal in deterring the USSR from continuing its domination of Eastern Europe.
However, just after the atomic bombs exploded over Hiroshima and Nagasaki, Russell wrote letters, and pubished articles in newspapers from 1945-1948, stating clearly that it was morally justified and better to go to war against the USSR using atomic bombs while the USA possessed them and before the USSR did. After the USSR exploded the atomic bomb, Russel changed his position 180 degrees and advocated now the total abolishment of atomic weapons.
In 1948, Russell was invited by the BBC to deliver the inaugural Reith Lectures—what was to become an annual series of lectures, still broadcast by the BBC. His series of six broadcasts, titled "Authority and the Individual", explored themes such as the role of individual initiative in the development of a community and the role of state control in a progressive society. Russell continued to write about philosophy. He wrote a foreword to "Words and Things" by Ernest Gellner, which was highly critical of the later thought of Ludwig Wittgenstein and of ordinary language philosophy. Gilbert Ryle refused to have the book reviewed in the philosophical journal "Mind", which caused Russell to respond via "The Times". The result was a month-long correspondence in "The Times" between the supporters and detractors of ordinary language philosophy, which was only ended when the paper published an editorial critical of both sides but agreeing with the opponents of ordinary language philosophy.
In the King's Birthday Honours of 9 June 1949, Russell was awarded the Order of Merit, and the following year he was awarded the Nobel Prize in Literature. When he was given the Order of Merit, George VI was affable but slightly embarrassed at decorating a former jailbird, saying, "You have sometimes behaved in a manner that would not do if generally adopted". Russell merely smiled, but afterwards claimed that the reply "That's right, just like your brother" immediately came to mind. In 1952 Russell was divorced by Spence, with whom he had been very unhappy. Conrad, Russell's son by Spence, did not see his father between the time of the divorce and 1968 (at which time his decision to meet his father caused a permanent breach with his mother).
Russell married his fourth wife, Edith Finch, soon after the divorce, on 15 December 1952. They had known each other since 1925, and Edith had taught English at Bryn Mawr College near Philadelphia, sharing a house for 20 years with Russell's old friend Lucy Donnelly. Edith remained with him until his death, and, by all accounts, their marriage was a happy, close, and loving one. Russell's eldest son John suffered from serious mental illness, which was the source of ongoing disputes between Russell and his former wife Dora. John's wife Susan was also mentally ill, and eventually Russell and Edith became the legal guardians of their three daughters, two of whom were later diagnosed with schizophrenia.
In September 1961, at the age of 89, Russell was jailed for seven days in Brixton Prison after taking part in an anti-nuclear demonstration in London, for "breach of peace". The magistrate offered to exempt him from jail if he pledged himself to "good behaviour", to which Russell replied: "No, I won't."
In 1962 Russell played a public role in the Cuban Missile Crisis: in an exchange of telegrams with Soviet leader Nikita Khrushchev, Khrushchev assured him that the Soviet government would not be reckless. Russell sent this telegram to President Kennedy: 
YOUR ACTION DESPERATE. THREAT TO HUMAN SURVIVAL. NO CONCEIVABLE JUSTIFICATION. CIVILIZED MAN CONDEMNS IT. WE WILL NOT HAVE MASS MURDER. ULTIMATUM MEANS WAR... END THIS MADNESS.
According to historian Peter Knight, after JFK's assassination, Russell, "prompted by the emerging work of the lawyer Mark Lane in the US ... rallied support from other noteworthy and left-leaning compatriots to form a "Who Killed Kennedy Committee" in June 1964, members of which included Michael Foot MP, Caroline Benn, the publisher Victor Gollancz, the writers John Arden and J. B. Priestley, and the Oxford history professor Hugh Trevor-Roper. Russell published a highly critical article weeks before the Warren Commission Report was published, setting forth "16 Questions on the Assassination" and equating the Oswald case with the Dreyfus affair of late 19th-century France, in which the state wrongly convicted an innocent man. Russell also criticised the American press for failing to heed any voices critical of the official version.
Political causes.
Russell spent the 1950s and 1960s engaged in political causes primarily related to nuclear disarmament and opposing the Vietnam War. The 1955 Russell–Einstein Manifesto was a document calling for nuclear disarmament and was signed by eleven of the most prominent nuclear physicists and intellectuals of the time. In 1966–67, Russell worked with Jean-Paul Sartre and many other intellectual figures to form the Russell Vietnam War Crimes Tribunal to investigate the conduct of the United States in Vietnam. He wrote a great many letters to world leaders during this period.
In 1956, immediately before and during the Suez Crisis, Russell expressed his opposition to what he viewed as European imperialism in the Middle East. He viewed the crisis as another reminder of what he saw as a pressing need for a more effective mechanism for international governance, and to restrict national sovereignty to places such as the Suez Canal area "where general interest is involved". At the same time the Suez Crisis was taking place, the world was also captivated by the Hungarian Revolution and the subsequent crushing of the revolt by intervening Soviet forces. Russell attracted criticism for speaking out fervently against the Suez war while ignoring Soviet repression in Hungary, to which he responded that he did not criticise the Soviets "because there was no need. Most of the so-called Western World was fulminating". Although he later feigned a lack of concern, at the time he was disgusted by the brutal Soviet response, and on 16 November 1956, he expressed approval for a declaration of support for Hungarian scholars which Michael Polanyi had cabled to the Soviet embassy in London twelve days previously, shortly after Soviet troops had already entered Budapest.
In November 1957 Russell wrote an article addressing US President Dwight D. Eisenhower and Soviet Premier Nikita Khrushchev, urging a summit to consider "the conditions of co-existence". Khrushchev responded that peace could indeed be served by such a meeting. In January 1958 Russell elaborated his views in "The Observer", proposing a cessation of all nuclear-weapons production, with Britain taking the first step by unilaterally suspending its own nuclear-weapons program if necessary, and with Germany "freed from all alien armed forces and pledged to neutrality in any conflict between East and West". US Secretary of State John Foster Dulles replied for Eisenhower. The exchange of letters was published as "The Vital Letters of Russell, Khrushchev, and Dulles".
Russell was asked by "The New Republic", a liberal American magazine, to elaborate his views on world peace. He suggested that all nuclear-weapons testing and constant flights by planes armed with nuclear weapons be halted immediately, and negotiations be opened for the destruction of all Hydrogen bombs, with the number of conventional nuclear devices limited to ensure a balance of power. He proposed that Germany be reunified and accept the Oder-Neisse line as its border, and that a neutral zone be established in Central Europe, consisting at the minimum of Germany, Poland, Hungary, and Czechoslovakia, with each of these countries being free of foreign troops and influence, and prohibited from forming alliances with countries outside the zone. In the Middle East, Russell suggested that the West avoid opposing Arab nationalism, and proposed a United Nations peacekeeping force to guard Israel's frontiers to ensure that Israel was protected from aggression and prevented from committing it. He also suggested Western recognition of the People's Republic of China, and that it be admitted to the UN with a permanent seat on the UN Security Council.
He was in contact with Lionel Rogosin while the latter was filming his anti-war film "Good Times, Wonderful Times" in the 1960s. He became a hero to many of the youthful members of the New Left. In early 1963, in particular, Russell became increasingly vocal in his disapproval of the Vietnam War, and felt that the US government's policies there were near-genocidal. In 1963 he became the inaugural recipient of the Jerusalem Prize, an award for writers concerned with the freedom of the individual in society. In 1964 he was one of eleven world figures who issued an appeal to Israel and the Arab countries to accept an arms embargo and international supervision of nuclear plants and rocket weaponry. In October 1965 he tore up his Labour Party card because he suspected Harold Wilson's Labour government was going to send troops to support the United States in Vietnam.
Final years and death.
Russell published his three-volume autobiography in 1967, 1968, and 1969. Russell made a cameo appearance playing himself in the anti-war Hindi film "Aman" which was released in India in 1967. This was Russell's only appearance in a feature film.
On 23 November 1969 he wrote to "The Times" newspaper saying that the preparation for show trials in Czechoslovakia was "highly alarming". The same month, he appealed to Secretary General U Thant of the United Nations to support an international war crimes commission to investigate alleged torture and genocide by the United States in South Vietnam during the Vietnam War. The following month, he protested to Alexei Kosygin over the expulsion of Aleksandr Solzhenitsyn from the Writers Union.
On 31 January 1970 Russell issued a statement condemning Israel's aggression in the Middle East, and in particular, Israeli bombing raids being carried out deep in Egyptian territory as part of the War of Attrition. He called for an Israeli withdrawal to the pre-Six-Day War borders. This was Russell's final political statement or act. It was read out at the International Conference of Parliamentarians in Cairo on 3 February 1970, the day after his death.
Russell died of influenza on 2 February 1970 at his home, Plas Penrhyn, in Penrhyndeudraeth, Merionethshire, Wales. His body was cremated in Colwyn Bay on 5 February 1970. In accordance with his will, there was no religious ceremony; his ashes were scattered over the Welsh mountains later that year.
In 1980 a memorial to Russell was commissioned by a committee including the philosopher A. J. Ayer. It consists of a bust of Russell in Red Lion Square in London sculpted by Marcelle Quinton.
Titles and honours from birth.
Russell held throughout his life the following styles and honours:
Views.
Views on philosophy.
Russell is generally credited with being one of the founders of analytic philosophy. He was deeply impressed by Gottfried Leibniz (1646–1716), and wrote on every major area of philosophy except aesthetics. He was particularly prolific in the field of metaphysics, the logic and the philosophy of mathematics, the philosophy of language, ethics and epistemology. When Brand Blanshard asked Russell why he didn't write on aesthetics, Russell replied that he didn't know anything about it, "but that is not a very good excuse, for my friends tell me it has not deterred me from writing on other subjects".
Views on religion.
Russell described himself as an agnostic, "speaking to a purely philosophical audience", but as an atheist "speaking popularly", on the basis that he could not disprove the Christian God similar to the way that he could not disprove the Olympic Gods either. For most of his adult life Russell maintained that religion is little more than superstition and, despite any positive effects that religion might have, it is largely harmful to people. He believed that religion and the religious outlook serve to impede knowledge and foster fear and dependency, and are responsible for much of our world's wars, oppression, and misery. He was a member of the Advisory Council of the British Humanist Association and President of Cardiff Humanists until his death.
Views on society.
Political and social activism occupied much of Russell's time for most of his life. Russell remained politically active almost to the end of his life, writing to and exhorting world leaders and lending his name to various causes.
Russell argued for a "scientific society", where war would be abolished, population growth limited, and prosperity shared. He suggested the establishment of a "single supreme world government" able to enforce peace, claiming that "the only thing that will redeem mankind is co-operation".
Russell was an active supporter of the Homosexual Law Reform Society, being one of the signatories of A.E. Dyson's 1958 letter to "The Times" calling for a change in the law regarding male homosexual practices, which were partly legalised in 1967, when Russell was still alive.
In "Reflections on My Eightieth Birthday" ("Postscript" in his "Autobiography"), Russell wrote: "I have lived in the pursuit of a vision, both personal and social. Personal: to care for what is noble, for what is beautiful, for what is gentle; to allow moments of insight to give wisdom at more mundane times. Social: to see in imagination the society that is to be created, where individuals grow freely, and where hate and greed and envy die because there is nothing to nourish them. These things I believe, and the world, for all its horrors, has left me unshaken".
Selected bibliography.
A selected bibliography of Russell's books in English, sorted by year of first publication:
Russell also wrote many pamphlets, introductions, articles, and letters to the editor. One pamphlet titled, "I Appeal unto Caesar: The Case of the Conscientious Objectors", ghostwritten for Margaret Hobhouse, the mother of imprisoned peace activist Stephen Henry Hobhouse allegedly helped secure the release of hundreds of conscientious objectors from prison.
His works can be found in anthologies and collections, perhaps most notably "The Collected Papers of Bertrand Russell", which McMaster University began publishing in 1983. This collection of his shorter and previously unpublished works is now up to 16 volumes, and many more are forthcoming. An additional three volumes catalogue just his bibliography. The Russell Archives at McMaster University possess over 30,000 of his letters.
External links.
Other.
 

</doc>
<doc id="4165" url="http://en.wikipedia.org/wiki?curid=4165" title="Boeing 767">
Boeing 767

The Boeing 767 is a mid- to large-size, long-range, wide-body twin-engine jet airliner built by Boeing Commercial Airplanes. It was the manufacturer's first wide-body twinjet and its first airliner with a two-crew glass cockpit. The aircraft has two turbofan engines, a conventional tail, and, for reduced aerodynamic drag, a supercritical wing design. Designed as a smaller wide-body airliner than earlier aircraft such as the 747, the 767 has seating capacity for 181 to 409 persons and a design range of , depending on variant. Development of the 767 occurred in tandem with a narrow-body twinjet, the 757, resulting in shared design features which allow pilots to obtain a common type rating to operate both aircraft.
The 767 is produced in three fuselage lengths. The original 767-200 entered service in 1982, followed by the 767-300 in 1986 and the 767-400ER, an extended-range (ER) variant, in 2000. The extended-range 767-200ER and 767-300ER models entered service in 1984 and 1988, respectively, while a production freighter version, the 767-300F, debuted in 1995. Conversion programs have modified passenger 767-200 and 767-300 series aircraft for cargo use, while military derivatives include the E-767 surveillance aircraft, the KC-767 and KC-46 aerial tankers, and VIP transports. Engines featured on the 767 include the General Electric CF6, Pratt & Whitney JT9D and PW4000, and Rolls-Royce RB211 turbofans.
United Airlines first placed the 767 in commercial service in 1982. The aircraft was initially flown on domestic and transcontinental routes, during which it demonstrated the reliability of its twinjet design. In 1985, the 767 became the first twin-engined airliner to receive regulatory approval for extended overseas flights. The aircraft was then used to expand non-stop service on medium- to long-haul intercontinental routes. In 1986, Boeing initiated studies for a higher-capacity 767, ultimately leading to the development of the 777, a larger wide-body twinjet. In the 1990s, the 767 became the most frequently used airliner for transatlantic flights between North America and Europe.
The 767 is the first twinjet wide-body type to reach 1,000 aircraft delivered. As of December 2013, Boeing has received 1,110 orders for the 767 from 71 customers; 1,061 have been delivered. A total of 838 of these aircraft were in service in July 2012; the most popular variant is the 767-300ER, with 582 delivered; Delta Air Lines is the largest operator, with 94 aircraft. Competitors have included the Airbus A300, A310, and A330-200, while a successor, the 787 Dreamliner, entered service in October 2011.
Development.
Background.
In 1970, Boeing's 747 became the first wide-body jetliner to enter service. The 747 was the first passenger jet that was wide enough to feature a twin-aisle cabin. Two years later, the manufacturer began a development study, code-named 7X7, for a new wide-body aircraft intended to replace the 707 and other early generation narrow-body jets. The aircraft would also provide twin-aisle seating, but in a smaller fuselage than the existing 747, McDonnell Douglas DC-10, and Lockheed L-1011 TriStar wide-bodies. To defray the high cost of development, Boeing signed risk-sharing agreements with Italian corporation Aeritalia and the Civil Transport Development Corporation (CTDC), a consortium of Japanese aerospace companies. This marked the manufacturer's first major international joint venture, and both Aeritalia and the CTDC received supply contracts in return for their early participation. The initial 7X7 was conceived as a short take-off and landing airliner intended for short-distance flights, but customers were unenthusiastic about the concept, leading to its redefinition as a mid-size, transcontinental-range airliner. At this stage the proposed aircraft featured two or three engines, with possible configurations including over-wing engines and a T-tail.
By 1976, a twinjet layout, similar to the one which had debuted on the Airbus A300, became the baseline configuration. The decision to use two engines reflected increased industry confidence in the reliability and economics of new-generation jet powerplants. While airline requirements for new wide-body aircraft remained ambiguous, the 7X7 was generally focused on mid-size, high-density markets. As such, it was intended to transport large numbers of passengers between major cities. Advancements in civil aerospace technology, including high-bypass-ratio turbofan engines, new flight deck systems, aerodynamic improvements, and lighter construction materials were to be applied to the 7X7. Many of these features were also included in a parallel development effort for a new mid-size narrow-body airliner, code-named 7N7, which would become the 757. Work on both proposals proceeded through the airline industry upturn in the late 1970s.
In January 1978, Boeing announced a major extension of its Everett factory—which was then dedicated to the manufacture of the 747—to accommodate its new wide-body family. In February 1978, the new jetliner received the 767 model designation, and three variants were planned: a 767-100 with 190 seats, a 767-200 with 210 seats, and a trijet 767MR/LR version with 200 seats intended for intercontinental routes. The 767MR/LR was subsequently renamed 777 for differentiation purposes. The 767 was officially launched on July 14, 1978, when United Airlines ordered 30 of the 767-200 variant, followed by 50 more 767-200 orders from American Airlines and Delta Air Lines later that year. The 767-100 was ultimately not offered for sale, as its capacity was too close to the 757's seating, while the 777 trijet was eventually dropped in favor of standardizing around the twinjet configuration.
Design effort.
In the late 1970s, operating cost replaced capacity as the primary factor in airliner purchases. As a result, the 767's design process emphasized fuel efficiency from the outset. Boeing targeted a 20 to 30 percent cost saving over earlier aircraft, mainly through new engine and wing technology. As development progressed, engineers used computer-aided design for over one-third of the 767's design drawings, and performed 26,000 hours of wind tunnel tests. Design work occurred concurrently with the 757 twinjet, leading Boeing to treat both as almost one program to reduce risk and cost. Both aircraft would ultimately receive shared design features, including avionics, flight management systems, instruments, and handling characteristics. Combined development costs were estimated at $3.5 to $4 billion.
Early 767 customers were given the choice of Pratt & Whitney JT9D or General Electric CF6 turbofans, marking the first time that Boeing had offered more than one engine option at the launch of a new airliner. Both jet engine models had a maximum output of of thrust. The engines were mounted approximately one-third the length of the wing from the fuselage, similar to previous wide-body trijets. The larger wings were designed using an aft-loaded shape which reduced aerodynamic drag and distributed lift more evenly across their surface span than any of the manufacturer's previous aircraft. The wings provided higher-altitude cruise performance, added fuel capacity, and expansion room for future stretched variants. The initial 767-200 was designed for sufficient range to fly across North America or across the northern Atlantic, and would be capable of operating routes up to .
The 767's fuselage width was set midway between that of the 707 and the 747 at . While it was narrower than previous wide-body designs, seven abreast seating with two aisles could be fitted, and the reduced width produced less aerodynamic drag. However, the fuselage was not wide enough to accommodate two standard LD3 wide-body unit load devices side-by-side. As a result, a smaller container, the LD2, was created specifically for the 767. The adoption of a conventional tail design also allowed the rear fuselage to be tapered over a shorter section, providing for parallel aisles along the full length of the passenger cabin, and eliminating irregular seat rows toward the rear of the aircraft.
The 767 was the first Boeing wide-body to be designed with a two-crew digital glass cockpit. Cathode ray tube (CRT) color displays and new electronics replaced the role of the flight engineer by enabling the pilot and co-pilot to monitor aircraft systems directly. Despite the promise of reduced crew costs, United Airlines initially demanded a conventional three-person cockpit, citing concerns about the risks associated with introducing a new aircraft. The carrier maintained this position until July 1981, when a U.S. presidential task force determined that a crew of two was safe for operating wide-body jets. A three-crew cockpit remained as an option and was fitted to the first production models. Ansett Australia ordered 767s with three-crew cockpits due to union demands; it was the only airline to operate 767s so configured. The 767's two-crew cockpit was also applied to the 757, allowing pilots to operate both aircraft after a short conversion course, and adding incentive for airlines to purchase both types.
Production and testing.
To produce the 767, Boeing formed a network of subcontractors which included domestic suppliers and international contributions from Italy's Aeritalia and Japan's CTDC. The wings and cabin floor were produced in-house, while Aeritalia provided control surfaces, Boeing Vertol made the leading edge for the wings, and Boeing Wichita produced the forward fuselage. The CTDC provided multiple assemblies through its constituent companies, namely Fuji Heavy Industries (wing fairings and gear doors), Kawasaki Heavy Industries (center fuselage), and Mitsubishi Heavy Industries (rear fuselage, doors, and tail). Components were integrated during final assembly at the Everett factory. For expedited production of wing spars, the main structural member of aircraft wings, the Everett factory received robotic machinery to automate the process of drilling holes and inserting fasteners. This method of wing construction expanded on techniques developed for the 747. Final assembly of the first aircraft began on July 6, 1979.
The prototype aircraft, registered N767BA and equipped with JT9D turbofans, rolled out on August 4, 1981. By this time, the 767 program had accumulated 173 firm orders from 17 customers, including Air Canada, All Nippon Airways, Britannia Airways, Transbrasil, and Trans World Airlines (TWA). On September 26, 1981, the prototype took its maiden flight under the command of company test pilots Tommy Edmonds, Lew Wallick, and John Brit. The maiden flight was largely uneventful, save for the inability to retract the landing gear owing to a hydraulic fluid leak. The prototype was used for subsequent flight tests.
The 10-month 767 flight test program utilized the first six aircraft built. The first four aircraft were equipped with JT9D engines, while the fifth and sixth were fitted with CF6 engines. The test fleet was largely used to evaluate avionics, flight systems, handling, and performance, while the sixth aircraft was used for route-proving flights. During testing, pilots described the 767 as generally easy to fly, with its maneuverability unencumbered by the bulkiness associated with larger wide-body jets. Following the successful completion of 1,600 hours of flight tests, the JT9D-powered 767-200 received certification from the U.S. Federal Aviation Administration (FAA) and the U.K. Civil Aviation Authority (CAA) in July 1982. The first delivery occurred on August 19, 1982, to United Airlines. The CF6-powered 767-200 received certification in September 1982, followed by the first delivery to Delta Air Lines on October 25, 1982.
Service entry and operations.
The 767 entered service with United Airlines on September 8, 1982. The aircraft's first commercial flight used a JT9D-powered 767-200 on the Chicago-to-Denver route. The CF6-powered 767-200 commenced service three months later with Delta Air Lines. Upon delivery, early 767s were mainly deployed on domestic routes, including U.S. transcontinental services. American Airlines and TWA began flying the 767-200 in late 1982, while Air Canada, China Airlines, and El Al began operating the aircraft in 1983. The aircraft's introduction was relatively smooth, with few operational glitches and greater dispatch reliability than prior jetliners. In its first year, the 767 logged a 96.1 percent rate of takeoff without delay due to technical issues, which exceeded the industry average for new aircraft. Operators reported generally favorable ratings for the twinjet's sound levels, interior comfort, and economic performance. Resolved issues were minor and included the recalibration of a leading edge sensor to prevent false readings, the replacement of an evacuation slide latch, and the repair of a tailplane pivot to match production specifications.
Seeking to capitalize on its new wide-body's potential for growth, Boeing offered an extended-range model, the 767-200ER, in its first year of service. Ethiopian Airlines placed the first order for the type in December 1982. Featuring increased gross weight specifications and greater fuel capacity, the extended-range model could carry heavier payloads at distances up to , and was targeted at overseas customers. The 767-200ER entered service with El Al on March 27, 1984. The type was mainly ordered by international airlines operating medium-traffic, long-distance flights.
In the mid-1980s, the 767 spearheaded the growth of twinjet flights across the northern Atlantic under extended-range twin-engine operational performance standards (ETOPS) regulations, the FAA's safety rules governing transoceanic flights by aircraft with two engines. Before the 767, over-water flight paths of twinjets could be no more than 90 minutes away from diversion airports. In May 1985, the FAA granted its first approval for 120-minute ETOPS flights to 767 operators, on an individual airline basis starting with TWA, provided that the operator met flight safety criteria. This allowed the aircraft to fly overseas routes at up to two hours' distance from land. The larger safety margins were permitted because of the improved reliability demonstrated by the twinjet and its turbofan engines. The FAA lengthened the ETOPS time to 180 minutes for CF6-powered 767s in 1989, making the type the first to be certified under the longer duration, and all available engines received approval by 1993. Regulatory approval spurred the expansion of transoceanic 767 flights and boosted the aircraft's sales.
Stretched derivatives.
Forecasting airline interest in larger-capacity models, Boeing announced the stretched 767-300 in 1983 and the extended-range 767-300ER in 1984. Both models offered a 20 percent passenger capacity increase, while the extended-range version was capable of operating flights up to . Japan Airlines placed the first order for the 767-300 in September 1983. Following its first flight on January 30, 1986, the type entered service with Japan Airlines on October 20, 1986. The 767-300ER completed its first flight on December 9, 1986, but it was not until March 1987 that the first firm order, from American Airlines, was placed. The type entered service with American Airlines on March 3, 1988. The 767-300 and 767-300ER gained popularity after entering service, and came to account for approximately two-thirds of all 767s sold.
After the debut of the first stretched 767s, Boeing sought to address airline requests for even more capacity by proposing larger models, including a partial double-deck version informally named the "Hunchback of Mukilteo" (from a town near Boeing's Everett factory) with a 757 body section mounted over the aft main fuselage. In 1986, the manufacturer announced the 767-X, a revised model with extended wings and a wider cabin, but received little interest. By 1988, the 767-X had evolved into an all-new twinjet, which revived the 777 designation. Until the 777's 1995 debut, the 767-300 and 767-300ER remained Boeing's second-largest wide-bodies behind the 747.
Buoyed by a recovering global economy and ETOPS approval, 767 sales accelerated in the mid-to-late 1980s, with 1989 being the most prolific year with 132 firm orders. By the early 1990s, the wide-body twinjet had become its manufacturer's annual best-selling aircraft, despite a slight decrease due to economic recession. During this period, the 767 became the most common airliner for transatlantic flights between North America and Europe. By the end of the decade, 767s crossed the Atlantic more frequently than all other aircraft types combined. The 767 also propelled the growth of point-to-point flights which bypassed major airline hubs in favor of direct routes. Taking advantage of the aircraft's lower operating costs and smaller capacity, operators added non-stop flights to secondary population centers, thereby eliminating the need for connecting flights. The increase in the number of cities receiving non-stop services caused a paradigm shift in the airline industry as point-to-point travel gained prominence at the expense of the traditional hub-and-spoke model.
In February 1990, the first 767 equipped with Rolls-Royce RB211 turbofans, a 767-300, was delivered to British Airways. Six months later, the carrier temporarily grounded its entire 767 fleet after discovering cracks in the engine pylons of several aircraft. The cracks were related to the extra weight of the RB211 engines, which are heavier than other 767 engines. During the grounding, interim repairs were conducted to alleviate stress on engine pylon components, and a parts redesign in 1991 prevented further cracks. Boeing also performed a structural reassessment, resulting in production changes and modifications to the engine pylons of all 767s in service.
In January 1993, following an order from UPS Airlines, Boeing launched a freighter variant, the 767-300F, which entered service with UPS on October 16, 1995. The 767-300F featured a main deck cargo hold, upgraded landing gear, and strengthened wing structure. In November 1993, the Japanese government launched the first 767 military derivative when it placed orders for the , an Airborne Early Warning and Control (AWACS) variant based on the 767-200ER. The first two , featuring extensive modifications to accommodate surveillance radar and other monitoring equipment, were delivered in 1998 to the Japan Self-Defense Forces.
In November 1995, after abandoning development of a smaller version of the 777, Boeing announced that it was revisiting studies for a larger 767. The proposed 767-400X, a second stretch of the aircraft, offered an over 12 percent capacity increase versus the 767-300, and featured an upgraded flight deck, enhanced interior, and wider wingspan. The variant was specifically aimed at Delta Air Lines' pending replacement of its aging Lockheed L-1011 TriStars, and faced competition from the A330-200, a shortened derivative of the Airbus A330. In March 1997, Delta Air Lines launched the 767-400ER when it ordered the type to replace its L-1011 fleet. In October 1997, Continental Airlines also ordered the 767-400ER to replace its McDonnell Douglas DC-10 fleet. The type completed its first flight on October 9, 1999, and entered service with Continental Airlines on September 14, 2000.
Further developments.
In the early 2000s, cumulative 767 deliveries approached 900, but new sales declined during an airline industry downturn. In 2001, Boeing dropped plans for a longer-range model, the 767-400ERX, in favor of the proposed Sonic Cruiser, a new jetliner which aimed to fly 15 percent faster while having comparable fuel costs as the 767. The following year, the manufacturer announced the KC-767 Tanker Transport, a second military derivative of the 767-200ER. Launched with an order in October 2002 from the Italian Air Force, the KC-767 was intended for the dual role of refueling other aircraft and carrying cargo. The Japanese government became the second customer for the type in March 2003. In May 2003, the United States Air Force (USAF) announced its intent to lease KC-767s to replace its aging KC-135 tankers. The plan was suspended in March 2004 amid a conflict of interest scandal, resulting in multiple U.S. government investigations and the departure of several Boeing officials, including Philip Condit, the company's chief executive officer, and chief financial officer Michael Sears. The first KC-767s were delivered in 2008 to the Japan Self-Defense Forces.
In late 2002, after airlines expressed reservations about its emphasis on speed over cost reduction, Boeing halted development of the Sonic Cruiser. The following year, the manufacturer announced the 7E7, a mid-size 767 successor made from composite materials which promised to be 20 percent more fuel efficient. The new jetliner was the first stage of a replacement aircraft initiative called the Boeing Yellowstone Project. Customers embraced the 7E7, later renamed 787 Dreamliner, and within two years it had become the fastest-selling airliner in the company's history. In 2005, Boeing opted to continue 767 production despite record Dreamliner sales, citing a need to provide customers waiting for the 787 with a more readily available option. Subsequently, the 767-300ER was offered to customers affected by 787 delays, including All Nippon Airways and Japan Airlines. Some aging 767s, exceeding 20 years in age, were also kept in service past planned retirement dates due to the delays. To extend the operational lives of older aircraft, airlines increased heavy maintenance procedures, including D-check teardowns and inspections for corrosion, a recurring issue on aging 767s. The first 787s would ultimately enter service with All Nippon Airways in October 2011, three-and-a-half years behind schedule.
In 2007, the 767 received a production boost when UPS and DHL Aviation placed a combined 33 orders for the 767-300F. Renewed freighter interest led Boeing to consider enhanced versions of the 767-200 and 767-300F with increased gross weights, 767-400ER wing extensions, and 777 avionics. However, net orders for the 767 declined from 24 in 2008 to just three in 2010. During the same period, operators upgraded aircraft already in service; in 2008, the first 767-300ER retrofitted with blended winglets from Aviation Partners Incorporated debuted with American Airlines. The manufacturer-sanctioned winglets, at in height, improved fuel efficiency by an estimated 6.5 percent. Other carriers including All Nippon Airways and Delta Air Lines also ordered winglet kits.
On February 2, 2011, the 1,000th 767 rolled out, destined for All Nippon Airways. The aircraft was the 91st 767-300ER ordered by the Japanese carrier, and with its completion the 767 became the second wide-body airliner to reach the thousand-unit milestone after the 747. The 1,000th aircraft also marked the last model produced on the original 767 assembly line. Beginning with the 1,001st aircraft, production moved to another area in the Everett factory which occupied nearly half the space as before. The new assembly line made room for 787 production and aimed to boost manufacturing efficiency by over 20 percent.
At the inauguration of its new assembly line, the 767's order backlog numbered approximately 50, only enough for production to last until 2013. Despite the reduced backlog, Boeing officials expressed optimism that additional orders were forthcoming. On February 24, 2011, the USAF announced its selection of the KC-767 Advanced Tanker, an upgraded variant of the KC-767, for its KC-X fleet renewal program. The selection followed two rounds of tanker competition between Boeing and Airbus parent EADS, and came eight years after the USAF's original 2003 announcement of its plan to lease KC-767s. The tanker order encompassed 179 aircraft and was expected to sustain 767 production past 2013. In December 2011, FedEx Express announced a 767-300F order for 27 aircraft to replace its DC-10 freighters, citing the USAF tanker order and Boeing's decision to continue production as contributing factors. FedEx Express announced an agreement to buy an additional 19 of the −300F variant in June 2012. Fifteen of these have been added to Boeing's orders and deliveries table as of June 30, 2012.
Design.
Overview.
The 767 is a low-wing cantilever monoplane with a conventional tail unit featuring a single fin and rudder. The wings are swept at 31.5 degrees and optimized for a cruising speed of Mach 0.8 (). Each wing features a supercritical cross-section and is equipped with six-panel leading edge slats, single- and double-slotted flaps, inboard and outboard ailerons, and six spoilers. The airframe further incorporates carbon-fiber reinforced plastic composite wing surfaces, Kevlar fairings and access panels, plus improved aluminum alloys, which together reduce overall weight by versus preceding aircraft.
To distribute the aircraft's weight on the ground, the 767 has a retractable tricycle landing gear with four wheels on each main gear and two for the nose gear. The original wing and gear design accommodated the stretched 767-300 without major changes. The 767-400ER features a larger, more widely spaced main gear with 777 wheels, tires, and brakes. To prevent damage if the tail section contacts the runway surface during takeoff, 767-300 and 767-400ER models are fitted with a retractable tailskid. The 767 has exit doors near the front and rear of the aircraft on the left side.
In addition to shared avionics and computer technology, the 767 uses the same auxiliary power unit, electric power systems, and hydraulic parts as the 757. A raised cockpit floor and the same forward cockpit windows result in similar pilot viewing angles. Related design and functionality allows 767 pilots to obtain a common type rating to operate the 757 and share the same seniority roster with pilots of either aircraft.
Flight systems.
The original 767 flight deck uses six Rockwell Collins CRT screens to display electronic flight instrument system (EFIS) and engine indication and crew alerting system (EICAS) information, allowing pilots to handle monitoring tasks previously performed by the flight engineer. The CRTs replace conventional electromechanical instruments found on earlier aircraft. An enhanced flight management system, improved over versions used on early 747s, automates navigation and other functions, while an automatic landing system facilitates CAT IIIb instrument landings in low visibility situations. The 767 became the first aircraft to receive CAT IIIb certification from the FAA for landings with minimum visibility in 1984. On the 767-400ER, the cockpit layout is simplified further with six Rockwell Collins liquid crystal display (LCD) screens, and adapted for similarities with the 777 and the Next Generation 737. To retain operational commonality, the LCD screens can be programmed to display information in the same manner as earlier 767s. In 2012, Boeing and Rockwell Collins launched a further 787-based cockpit upgrade for the 767, featuring three landscape-format LCD screens that can display two windows each.
The 767 is equipped with three redundant hydraulic systems for operation of control surfaces, landing gear, and other equipment. Each engine powers a separate hydraulic system, and the third system uses electric pumps. A ram air turbine is fitted to provide power for basic controls in the event of an emergency. An early form of fly-by-wire is employed for spoiler operation, utilizing electric signaling instead of traditional control cables. The fly-by-wire system reduces weight and provides for the independent operation of individual spoilers.
Interior.
The 767 features a twin-aisle cabin with a typical configuration of six abreast in business class and seven across in economy. The standard seven abreast, 2–3–2 economy class layout places approximately 87 percent of all seats at a window or aisle. As a result, the aircraft can be largely occupied before center seats need to be filled, and each passenger is no more than one seat from the aisle. It is possible to configure the aircraft with extra seats for up to an eight abreast configuration, but this results in a cramped cabin and is therefore uncommon.
The 767 interior introduced larger overhead bins and more lavatories per passenger than previous aircraft. The bins are wider to accommodate garment bags without folding, and strengthened for heavier carry-on items. A single, large galley is installed near the aft doors, allowing for more efficient meal service and simpler resupply while at airports. Passenger and service doors are an overhead plug type, which retract upwards, and commonly-used doors can be equipped with an electric-assist system.
In 2000, a 777-style interior, known as the Boeing Signature Interior, debuted on the 767-400ER. Subsequently adopted for all new-build 767s, the Signature Interior features even larger overhead bins, indirect lighting, and sculpted, curved panels. The 767-400ER also received larger windows derived from the 777. Older 767s can be retrofitted with the Signature Interior. Some operators have adopted a simpler modification known as the Enhanced Interior, featuring curved ceiling panels and indirect lighting with minimal modification of cabin architecture, as well as aftermarket modifications such as the NuLook 767 package by Heath Tecna.
Variants.
The 767 has been produced in three fuselage lengths. These debuted in progressively larger form as the 767-200, 767-300, and 767-400ER, respectively. Longer-range variants include the 767-200ER and 767-300ER, while cargo models include the 767-300F, a production freighter, and conversions of passenger 767-200 and 767-300 models.
When referring to different variants, Boeing and airlines often collapse the model number (767) and the variant designator (e.g. –200 or –300) into a truncated form (e.g. "762" or "763"). Subsequent to the capacity number, designations may or may not append the range identifier. The International Civil Aviation Organization (ICAO) aircraft type designator system uses a similar numbering scheme, but adds a preceding manufacturer letter; all variants based on the 767-200 and 767-300 are classified under the codes "B762" and "B763", respectively, while the 767-400ER receives the designation of "B764."
767-200.
The 767-200 was the original model and entered service with United Airlines in 1982. The type has been used primarily by mainline U.S. carriers for domestic routes between major hub centers such as Los Angeles to Washington. The 767-200 was the first aircraft to be used on transatlantic ETOPS flights, beginning with TWA on February 1, 1985 under 90-minute diversion rules. Deliveries for the variant totaled 128 aircraft. There were 55 passenger and freighter conversions of the model in commercial service as of July 2013. The type's competitors included the Airbus A300 and A310.
The 767-200 ceased production in the late 1980s due to being superseded by the extended-range 767-200ER. Some early 767-200s were subsequently upgraded to extended-range specification. In 1998, Boeing began offering 767-200 conversions to 767-200SF (Special Freighter) specification for cargo use, and Israel Aerospace Industries has been licensed to perform cargo conversions since 2005. The conversion process entails the installation of a side cargo door, strengthened main deck floor, and added freight monitoring and safety equipment. The 767-200SF is positioned as a replacement for Douglas DC-8 freighters.
767-200ER.
The 767-200ER was the first extended-range model and entered service with El Al in 1984. The type's increased range is due to an additional center fuel tank and a higher maximum takeoff weight (MTOW) of up to . The type was originally offered with the same engines as the 767-200, while more powerful Pratt & Whitney PW4000 and General Electric CF6 engines later became available. The 767-200ER was the first 767 to complete a non-stop transatlantic journey, and broke the flying distance record for a twinjet airliner on April 17, 1988 with an Air Mauritius flight from Halifax, Nova Scotia to Port Louis, Mauritius, covering a distance of . The 767-200ER has been acquired by international operators seeking smaller wide-body aircraft for long-haul routes such as New York to Beijing. Deliveries of the type totaled 121 with no unfilled orders. As of July 2013, 47 examples of passenger and freighter conversion versions were in airline service. The type's competitors included the Airbus A300-600R and the A310-300.
767-300.
The 767-300, the first stretched version of the aircraft, entered service with Japan Airlines in 1986. The type features a fuselage extension over the 767-200, achieved by additional sections inserted before and after the wings, for an overall length of . Reflecting the growth potential built into the original 767 design, the wings, engines, and most systems were largely unchanged on the 767-300. An optional mid-cabin exit door is positioned ahead of the wings on the left, while more powerful Pratt & Whitney PW4000 and Rolls-Royce RB211 engines later became available. The 767-300's increased capacity has been used on high-density routes within Asia and Europe. Deliveries for the type totaled 104 aircraft with no unfilled orders remaining. As of July 2013, 59 of the variant were in airline service. The type's main competitor was the Airbus A300.
767-300ER.
The 767-300ER, the extended-range version of the 767-300, entered service with American Airlines in 1988. The type's increased range is made possible by greater fuel tankage and a higher MTOW of . Design improvements allowed the available MTOW to increase to by 1993. Power is provided by Pratt & Whitney PW4000, General Electric CF6, or Rolls-Royce RB211 engines. Typical routes for the type include Los Angeles to Frankfurt. The combination of increased capacity and range offered by the 767-300ER has been particularly attractive to both new and existing 767 operators, allowing it to become the most successful version of the aircraft. Airlines have placed more orders for the type than all other variants combined. As of October 2013, 767-300ER deliveries stand at 582 with 1 unfilled order. Airlines had 533 examples in service as of July 2013. The type's main competitor is the Airbus A330-200.
767-300F.
The 767-300F, the production freighter version of the 767-300ER, entered service with UPS Airlines in 1995. The 767-300F can hold up to 24 standard pallets on its main deck and up to 30 LD2 unit load devices on the lower deck, with a total cargo volume of . The freighter has a main deck cargo door and crew exit, while the lower deck features two port-side cargo doors and one starboard cargo door. A general market version with onboard freight-handling systems, refrigeration capability, and crew facilities was delivered to Asiana Airlines on August 23, 1996. As of October 2013, 767-300F deliveries stand at 86 with 44 unfilled orders. Airlines operated 90 examples of the freighter variant and freighter conversions in July 2013.
In June 2008, All Nippon Airways took delivery of the first 767-300BCF (Boeing Converted Freighter), a modified passenger-to-freighter model. The conversion work was performed in Singapore by ST Aerospace Services, the first supplier to offer a 767-300BCF program, and involved the addition of a main deck cargo door, strengthened main deck floor, and additional freight monitoring and safety equipment. Since then, Boeing, Israel Aerospace Industries, and Wagner Aeronautical have also offered passenger-to-freighter conversion programs for 767-300 series aircraft.
767-400ER.
The 767-400ER, the first Boeing wide-body jet resulting from two fuselage stretches, entered service with Continental Airlines in 2000. The type features a stretch over the 767-300, for a total length of . The wingspan is also increased by through the addition of raked wingtips. Other differences include an updated cockpit, redesigned landing gear, and 777-style Signature Interior. Power is provided by uprated Pratt & Whitney PW4000 or General Electric CF6 engines.
The FAA granted approval for the 767-400ER to operate 180-minute ETOPS flights before it entered service. Because its fuel capacity was not increased over preceding models, the 767-400ER has a range of , less than previous extended-range 767s. Typical routings for the type include London to Tokyo. A longer-range version, the 767-400ERX, was offered for sale in 2000 but cancelled a year later, leaving the 767-400ER as the sole version of the largest 767. There is only the -400ER variant, and no -400 variant. The variant's only two airline customers, Continental Airlines (now merged with United Airlines) and Delta Air Lines, received 37 aircraft, with no unfilled orders. All 37 of the 767-400ER were in service as of July 2013. One additional example was produced as a military testbed, and later sold as a VIP transport. The type's closest competitor is the Airbus A330-200.
Military and government.
Versions of the 767 serve in a number of military and government applications, with responsibilities ranging from airborne surveillance and refueling to cargo and VIP transport. Several military 767s have been derived from the 767-200ER, the longest-range version of the aircraft.
Undeveloped variants.
767-400ERX.
Boeing offered the 767-400ERX, a longer-range version of the largest 767 model, for sale in 2000. Introduced along with the 747X, the type was to be powered by the 747X's engines, namely the Engine Alliance GP7000 and the Rolls-Royce Trent 600. An increased range of was specified. Kenya Airways provisionally ordered three 767-400ERXs to supplement its 767 fleet, but after Boeing cancelled the type's development in 2001, switched the order to the 777-200ER.
E-10 MC2A.
The Northrop Grumman E-10 MC2A was to be a 767-400ER-based replacement for the USAF's 707-based E-3 Sentry AWACS, E-8 Joint STARS, and RC-135 SIGINT aircraft. The E-10 MC2A would have included an all-new AWACS system, with a powerful Active Electronically Scanned Array (AESA) that was also capable of jamming enemy aircraft or missiles. One 767-400ER aircraft was produced as a testbed for systems integration, but the program was terminated in January 2009 and the prototype sold to Bahrain as a VIP transport.
Operators.
The customers that have ordered the most 767s are Delta Air Lines, All Nippon Airways, and United Airlines. Delta Air Lines is the largest customer, having received 117 aircraft. The Atlanta-based carrier is also the only customer to have ordered all passenger versions of the 767. Its 100th example, a 767-400ER, was delivered in October 2000. United Airlines was the only carrier operating all versions of the 767 ER series (762ER, 763ER, and 764ER) as of November 2012. The largest cargo customer is UPS Airlines, having received 59 aircraft as of October 2013.
A total of 821 aircraft (all 767 variants) were in airline service in July 2013, with airline operators Delta Air Lines (94), American Airlines (70), UPS Airlines (56), United Airlines (51), All Nippon Airways (50), Japan Airlines (48), LAN Airlines (43), ABX Air (34), Air Canada (30), and others with fewer aircraft of the type.
Incidents and accidents.
As of February 2014, the Boeing 767 has been in 38 aviation occurrences, including 14 hull-loss accidents. Six fatal crashes, including three hijackings, have resulted in a total of 851 occupant fatalities. The airliner's first fatal crash, Lauda Air Flight 004, occurred near Bangkok on May 26, 1991, following the in-flight deployment of the left engine thrust reverser on a 767-300ER; none of the 223 aboard survived; as a result of this accident all 767 thrust reversers were deactivated until a redesign was implemented. Investigators determined that an electronically-controlled valve, common to late-model Boeing aircraft, was to blame. A new locking device was installed on all affected jetliners, including 767s. On October 31, 1999, EgyptAir Flight 990, a 767-300ER, crashed off Nantucket Island, Massachusetts, in international waters killing all 217 people on board. The U.S. National Transportation Safety Board (NTSB) determined the probable cause to be due to a deliberate action by the first officer; Egypt disputed this conclusion. On April 15, 2002, Air China Flight 129, a 767-200ER, crashed into a hill amid inclement weather while trying to land at Gimhae International Airport in Busan, South Korea. The crash resulted in the death of 129 of the 166 people on board, and the cause was attributed to pilot error.
An early 767 incident was survived by all on board. On July 23, 1983, Air Canada Flight 143, a 767-200, ran out of fuel in-flight and had to glide with both engines out for almost to an emergency landing at Gimli, Manitoba. The pilots used the aircraft's ram air turbine to power the hydraulic systems for aerodynamic control. There were no fatalities and only minor injuries. This aircraft was nicknamed "Gimli Glider" for the airport at which it landed. The aircraft, registered C-GAUN, continued flying for Air Canada until its retirement in January 2008.
The 767 has been involved in six hijackings, three resulting in loss of life, for a combined total of 282 occupant fatalities. On November 23, 1996, Ethiopian Airlines Flight 961, a 767-200ER, was hijacked and crash-landed in the Indian Ocean near the Comoros Islands after running out of fuel, killing 125 out of the 175 persons onboard; survivors have been rare among instances of land-based aircraft ditching on water. Two 767s were involved in the September 11 attacks on the World Trade Center in 2001, resulting in the collapse of its two main towers. American Airlines Flight 11, a 767-200ER, crashed into the north tower, killing all 92 people on board, and United Airlines Flight 175, a 767-200, crashed into the south tower, with the death of all 65 on board. In addition, more than 2,600 people perished in the towers or on the ground. A foiled 2001 shoe bomb plot involving an American Airlines 767-300ER resulted in passengers being required to remove their shoes for scanning at U.S. security checkpoints.
On November 1, 2011, LOT Polish Airlines Flight 16, a 767-300ER, safely landed at Warsaw Frederic Chopin Airport in Warsaw, Poland after a mechanical failure of the landing gear forced an emergency landing with the landing gear up. There were no injuries, but the aircraft involved was damaged and subsequently written off. At the time of the incident, aviation analysts speculated that it may have been the first instance of a complete landing gear failure in the 767's service history. Subsequent investigation however determined that while a damaged hose had disabled the aircraft's primary landing gear extension system, an otherwise functional backup system was inoperative due to an accidentally deactivated circuit breaker.
In January 2014, the U.S. Federal Aviation Administration issued a directive that ordered inspections of the elevators on more than 400 767s beginning in March 2014; the focus is on fasteners and other parts that can fail and cause the elevators to jam. The issue was first identified in 2000 and has been the subject of several Boeing service bulletins. The inspections and repairs are required to be completed within six years.
Retirement and display.
As new 767s roll off the assembly line, older models have been retired and scrapped. One complete aircraft is known to have been retained for exhibition, specifically N102DA, the first 767-200 to operate for Delta Air Lines and the twelfth example built. The exhibition aircraft, named "The Spirit of Delta" by the employees who helped purchase it in 1982, underwent restoration at the Delta Air Lines Air Transport Heritage Museum in Atlanta, Georgia. The restoration was completed in 2010. Featuring the original delivered interior as well as historical displays, the aircraft is viewable by visitors (self-guided) daily, during the museum's operating hours. Hangar renovations, begun in June 2013, are now complete, and the museum is accessible on a daily basis. 
In 2010, four retired American Airlines 767-200s were dismantled for parts in Roswell, New Mexico, and their nose sections removed intact for collector or film use. Of these four aircraft, the cockpit of N301AA, the eighth 767 built and the first of its type to be delivered to American Airlines, was transported to Victorville, California, to be restored for museum display. As of 2013, the cockpit section of N301AA is housed at the interim museum location of the American Museum of Aviation, a nonprofit organization in Las Vegas, Nevada, along with a display of American Airlines photographs. The organization is also attempting to restore the forward upper fuselage and cockpit section of N601UA, a former United Airlines aircraft and the second 767 built.
Specifications.
Sources: Boeing 767 general specifications, Boeing 767 variant specifications and other sources

</doc>
<doc id="4166" url="http://en.wikipedia.org/wiki?curid=4166" title="Bill Walsh (American football coach)">
Bill Walsh (American football coach)

William Ernest "Bill" Walsh (November 30, 1931 – July 30, 2007) was the head coach of the San Francisco 49ers and the Stanford Cardinal football team, during which time he popularized the West Coast offense. After retiring from the 49ers, Walsh worked as sports broadcaster for several years and then returned as head coach at Stanford for three seasons.
Walsh went 102–63–1 with the 49ers, winning ten of his 14 postseason games along with six division titles, three NFC Championship titles, and three Super Bowls. He was named the NFL's Coach of the Year in 1981 and 1984. In 1993, he was elected to the Pro Football Hall of Fame.
Early career.
Born in Los Angeles, Walsh played running back in the San Francisco Bay Area for Hayward High School in Hayward.
Walsh played quarterback at the College of San Mateo for two seasons. Both John Madden and Walsh played and coached at the College of San Mateo early in their careers. After playing at the College of San Mateo, Walsh transferred to San José State University, where he played tight end and defensive end. He also participated in intercollegiate boxing. Walsh graduated from San Jose State with a bachelor's degree in physical education in 1955. He served under Bob Bronzan as a graduate assistant coach on the Spartans football coaching staff and graduated with a master's degree in physical education from San Jose State in 1959. His master's thesis was entitled "Flank Formation Football -- Stress:: Defense". Thesis 796.W228f
Following graduation, Walsh coached at Washington High School in Fremont, leading the football and swim teams.
Walsh was coaching in Fremont when he interviewed for an assistant coaching position with Marv Levy, who had just been hired as the head coach at the University of California, Berkeley.
"I was very impressed, individually, by his knowledge, by his intelligence, by his personality and hired him," Levy said.
After Cal, Walsh did a stint at Stanford as an assistant coach, before beginning his pro coaching career.
Professional football career.
Walsh began his pro coaching career in 1966 as an assistant with the AFL's Oakland Raiders. As a Raider assistant, Walsh was trained in the vertical passing offense favored by Al Davis, putting Walsh in Davis's mentor Sid Gillman's coaching tree.
In 1968, Walsh moved to the AFL expansion Cincinnati Bengals, joining the staff of legendary coach Paul Brown. It was there that Walsh developed the philosophy now known as the "West Coast Offense", as a matter of necessity. Cincinnati's new quarterback, Virgil Carter, was known for his great mobility and accuracy but lacked a strong arm necessary to throw deep passes. Thus, Walsh modified the vertical passing scheme he had learned during his time with the Raiders, designing a horizontal passing system that relied on quick, short throws - often spreading the ball across the entire width of the field. The new offense was much better suited to Carter's physical abilities; he led the league in pass completion percentage in 1971.
Walsh spent eight seasons as an assistant with the Bengals. Ken Anderson eventually replaced Carter as starting quarterback, and together with star wide receiver Isaac Curtis, produced a consistent, effective offensive attack. Initially, Walsh started out as the wide receivers coach from 1968 to 1970 before also coaching the quarterbacks from 1971 to 1975.
When Brown retired as head coach following the 1975 season and appointed Bill "Tiger" Johnson as his successor, Walsh resigned and served as an assistant coach for Tommy Prothro with the San Diego Chargers in 1976. In a 2006 interview, Walsh claimed that during his tenure with the Bengals, Brown "worked against my candidacy" to be a head coach anywhere in the league. "All the way through I had opportunities, and I never knew about them," Walsh said. "And then when I left him, he called whoever he thought was necessary to keep me out of the NFL."
In 1977, Walsh was hired as the head coach at Stanford where he stayed for two seasons. His two Stanford teams were successful, posting a 9–3 record in 1977 with a win in the Sun Bowl, and 8–4 in 1978 with a win in the Bluebonnet Bowl. His notable players at Stanford included quarterbacks Guy Benjamin and Steve Dils, wide receivers James Lofton and Ken Margerum, linebacker Gordy Ceresino, in addition to running back Darrin Nelson. Walsh was the Pac-8 Conference Coach of the Year in 1977.
In 1979, Walsh was hired as head coach of the San Francisco 49ers. The long-suffering 49ers went 2–14 in 1978, the season before Walsh's arrival and repeated the same dismal record in his first season. Walsh got the entire organization to buy into his philosophy and vowed to turn around a miserable situation. Despite their second consecutive 2-14 record, the 49ers were playing more competitive football. 
In 1979, Walsh drafted quarterback Joe Montana from Notre Dame in the third round. After a 59-14 blowout loss to Dallas in week 6 of the 1980 season, Walsh promoted Montana to starting QB. On a Monday Night Football game, December 7, 1980, vs. the New Orleans Saints, Montana brought the 49ers back from a 35-7 halftime deficit to win 38-35 in overtime. The 49ers improved in 1980 to 6–10, but more importantly, Walsh had the 49ers making great strides and they were getting better every week. San Francisco won its first championship in 1981, just two years after winning two games.
Under Walsh the 49ers won Super Bowl championships in 1981, 1984 and 1988. Walsh served as 49ers head coach for ten years, and during his tenure he and his coaching staff perfected the style of play known popularly as the West Coast offense. Walsh was nicknamed "The Genius" for both his innovative play calling and design. Walsh would regularly script the first 10-15 offensive plays before the start of each game. In the ten years during which Walsh was the 49ers' head coach, San Francisco scored 3,714 points (24.4 per game), the most of any team in the league during that span.
In addition to drafting Joe Montana, Walsh drafted Ronnie Lott, Charles Haley, and Jerry Rice. He also traded a 2nd and 4th round pick in the 1987 draft for Steve Young. His success with the 49ers was rewarded with his election to the Professional Football Hall of Fame in 1993.
1981 championship.
The 1981 season saw Walsh lead the 49ers to a Super Bowl championship; the team rose from the cellar to the top of the NFL in just two seasons. Four important wins during the 1981 season were two wins each over the Los Angeles Rams and the Dallas Cowboys. The Rams were only one year removed from a Super Bowl appearance, and had dominated the series with the 49ers since 1967 winning 23, losing 3 and tying 1. The 49ers' two wins over the Rams in 1981 marked the shift of dominance in favor of the 49ers that lasted until 1998 with 30 wins (including 17 consecutively) against only 6 defeats.
In 1981, the 49ers blew out the Cowboys in week 6 of the regular season. On "Monday Night Football" that week, the 49ers' win was not included in the famous halftime highlights. Walsh felt that this was because the Cowboys were scheduled to play the Rams the next week in a rare Sunday night game and that showing the highlights of the 49ers' win would potentially hurt the game's ratings. However, Walsh used this as a motivating factor for his team, who felt they were disrespected.
The 49ers faced the Cowboys again that same season in the NFC title game. The game was very close, and in the fourth quarter Walsh called a series of running plays as the 49ers marched down the field against the Cowboys prevent defense, which had been expecting the 49ers to mainly pass. The 49ers came from behind to win the game on Dwight Clark's memorable TD reception (The Catch), propelling Walsh to his first Super Bowl. Walsh and the 49ers defeated Cincinnati in the Super Bowl, which was played in Pontiac, Michigan. Walsh would later write that the 49ers' two wins over the Rams showed a shift of power in their division, while the wins over the Cowboys showed a shift of power in the conference.
Prominent assistant coaches.
Many of Bill Walsh's assistant coaches went on to be head coaches themselves, including George Seifert, Mike Holmgren, Ray Rhodes, and Dennis Green. After Walsh's retirement from the 49ers, Seifert succeeded him as 49ers head coach, and guided San Francisco to victories in Super Bowl XXIV and Super Bowl XXIX. Holmgren won a Super Bowl with the Green Bay Packers, and made 3 Super Bowl appearances as a head coach: 2 with the Packers, and another with the Seattle Seahawks. These coaches in turn have their own disciples who have used Walsh's West Coast system, such as former Washington Redskins head coach Mike Shanahan and former Houston Texans head coach Gary Kubiak. Mike Shanahan was an offensive coordinator under George Seifert and went on to win Super Bowl XXXII and Super Bowl XXXIII during his time as head coach of the Denver Broncos. Kubiak was first a quarterback coach with the 49ers, then offensive coordinator for Shanahan with the Denver Broncos. Dennis Green trained Tony Dungy, who won a Super Bowl with the Indianapolis Colts, and Brian Billick with his brother-in law and linebackers coach Mike Smith. Billick won a Super Bowl as head coach of the Baltimore Ravens. Mike Holmgren trained many of his assistants to become head coaches, including Jon Gruden and Andy Reid. Gruden won a Super Bowl with the Tampa Bay Buccaneers. Reid served as head coach of the Philadelphia Eagles from 1999-2012, and guided the Eagles to multiple winning seasons and numerous playoff appearances. In addition to this, Marc Trestman, current head coach of the Chicago Bears, served as Offensive Coordinator under Seifert in the 90's. Gruden himself would train Mike Tomlin, who led the Pittsburgh Steelers to their sixth Super Bowl championship, and Jim Harbaugh, whose 49ers would face his brother, John Harbaugh, whom Reid himself trained, and the Baltimore Ravens at Super Bowl XLVII, which marked the Ravens' second World Championship.
Bill Walsh was viewed as a strong advocate for African-American head coaches in the NFL and NCAA. Thus, the impact of Walsh also changed the NFL into an equal opportunity for African-American coaches. Along with Ray Rhodes and Dennis Green, Tyrone Willingham became the head coach at Stanford, then later Notre Dame and Washington. One of Mike Shanahan's assistants, Karl Dorrell went on to be the head coach at UCLA. Walsh directly helped propel Dennis Green into the NFL head coaching ranks by offering to take on the head coaching job at Stanford.
Bill Walsh coaching tree.
Many former and current NFL head coaches trace their lineage back to Bill Walsh on his coaching tree, shown below. Walsh, in turn, belonged to the coaching tree of American Football League great and Hall of Fame coach Sid Gillman of the AFL's Los Angeles/San Diego Chargers.
Later career.
After leaving the coaching ranks immediately following his team's victory in Super Bowl XXIII, Walsh went to work as a broadcaster for NBC (teaming with Dick Enberg to form the lead broadcasting team while replacing Merlin Olsen in the booth). Walsh returned to Stanford once again as head coach in 1992 (Bob Trumpy subsequently replaced him on the NBC telecasts, and would, in turn be replaced by Paul Maguire who would later be joined by Phil Simms), leading the Cardinal to a 10-3 record and a Pacific-10 Conference co-championship. Stanford finished the season with an upset victory over Penn State in the Blockbuster Bowl on January 1, 1993 and a # 9 ranking in the final AP Poll. In 1994, after consecutive losing seasons, Walsh left Stanford and retired from coaching.
Walsh would also return to the 49ers, serving as Vice President and General Manager from 1999 to 2001 and was a special consultant to the team for three years afterwards. In 2004, Walsh was appointed as special assistant to the athletic director at Stanford. In 2005, after then-athletic director Ted Leland stepped down to take a position at the University of the Pacific, Walsh was named interim athletic director. He also acted as a consultant for his alma mater San Jose State University in their search for an Athletic Director and Head Football Coach in 2005.
Bill Walsh was also the author of three books, a motivational speaker, and taught classes at the Stanford Graduate School of Business.
Walsh was a Board Member for the Lott IMPACT Trophy, which is named after Pro Football Hall of Fame defensive back Ronnie Lott, and is awarded annually to college football's Defensive IMPACT Player of the Year. Walsh served as a keynote speaker at the award's banquet.
Leukemia.
Death.
Bill Walsh died of leukemia at 10:45 am on July 30, 2007, at his home in Woodside, California. Following Walsh's death, the playing field at Candlestick Park was renamed "Bill Walsh Field". Additionally, the regular San Jose State versus Stanford football game was renamed the "Bill Walsh Legacy Game".
Family.
Walsh is survived by his wife Geri, his son Craig and his daughter Elizabeth. Walsh also lost a son, Steve, in 2002. Craig Walsh flipped the coin at Super Bowl XLIII in Glendale, Arizona, accompanied by his sister, their mother and several ex-49ers.
External links.
 

</doc>
<doc id="4168" url="http://en.wikipedia.org/wiki?curid=4168" title="Utility knife">
Utility knife

A utility knife is a knife used for general or utility purposes. The utility knife was originally a fixed blade knife with a cutting edge suitable for general work such as cutting hides and cordage, scraping hides, butchering animals, cleaning fish, and other tasks. Craft knives are tools mostly used for crafts.
Today, the term "utility knife" also includes small folding or retractable-blade knives suited for use in the modern workplace or in the construction industry.
History.
The fixed-blade utility knife was developed some 500,000 years ago, when humans began to make knives made of stone. These knives were general-purpose tools, designed for cutting and shaping wooden implements, scraping hides, preparing food, and for other utilitarian purposes.
By the 19th century the fixed-blade utility knife had evolved into a steel-bladed outdoors field knife capable of butchering game, cutting wood, and preparing campfires and meals. With the invention of the backspring, pocket-size utility knives were introduced with folding blades and other folding tools designed to increase the utility of the overall design. The folding pocketknife and utility tool is typified by the "Camper" or "Boy Scout" pocketknife, the U.S. folding utility knife, the Swiss Army Knife, and by multi-tools fitted with knife blades. The development of stronger locking blade mechanisms for folding knives—as with the Spanish navaja, the Opinel, and the Buck 110 Folding Hunter—significantly increased the utility of such knives when employed for heavy-duty tasks such as preparing game or cutting through dense or tough materials.
Contemporary utility knives.
The fixed or folding blade utility knife is popular for both indoor and outdoor use. One of the most popular types of workplace utility knife is the retractable or folding utility knife (also known as a "Stanley knife", "box cutter", "X-Acto knife", or by various other names). These types of utility knives are designed as multi-purpose cutting tools for use in a variety of trades and crafts. Designed to be lightweight and easy to carry and use, utility knives are commonly used in factories, warehouses, construction projects, and other situations where a tool is routinely needed to mark cut lines, trim plastic or wood materials, or to cut tape, cord, strapping, cardboard, or other packaging material.
Names.
In British, Australian and New Zealand English, along with Dutch and Austrian German, a utility knife frequently used in the construction industry is known as a "Stanley knife". This name is a genericised trademark named after Stanley Works, a manufacturer of such knives. In Israel and Switzerland, these knives are known as "Japanese knives". In Brazil they are known as "estiletes" or "cortadores Olfa" (the latter, being another genericised trademark). In Portugal and Canada they are also known as "X-Acto" (yet another genericised trademark). In India, the Philippines, France, Italy, and Egypt, they are simply called "cutter". In general Spanish, they are known as "cortaplumas" (penknife, when it comes to folding blades); in Spain, Mexico, and Costa Rica, they are colloquially known as "cutters"; in Argentina and Uruguay the segmented fixed-blade knives are known as "Trinchetas". 
Other names for the tool are "box cutter" or "boxcutter", "razor blade knife", "razor knife", "carpet knife", "pen knife", "stationery knife", "sheetrock knife", or "drywall knife". Some of these names refer to a different kind of knife depending on the region. For example, in the mid-Atlantic US, the X-Acto name is likelier to evoke only a specific subset of these knives (the ), which may explain why the "utility knife" name, with its specificity, is more common there for . Also, in this region, "box cutter" usually evokes only a specific subset of these knives ( whose body consists only of a flat sleeve stamped from sheet steel), and "pen knife" usually evokes only a folding pocket knife.
Design.
Utility knives may use fixed, folding, or retractable or replaceable blades, and come in a wide variety of lengths and styles suited to the particular set of tasks they are designed to perform. Thus, an outdoors utility knife suited for camping or hunting might use a broad fixed blade, while a utility knife designed for the construction industry might feature a replaceable utility or razor blade for cutting packaging, cutting shingles, marking cut lines, or scraping paint.
Fixed blade utility knife.
Large fixed-blade utility knives are most often employed in an outdoors context, such as fishing, camping, or hunting. Outdoor utility knives typically feature sturdy blades from in length, with edge geometry designed to resist chipping and breakage. 
The term "utility knife" may also refer to small fixed-blade knives used for crafts, model-making and other artisanal projects. These small knives feature light-duty blades best suited for cutting thin, lightweight materials. The small, thin blade and specialized handle permit cuts requiring a high degree of precision and control.
Workplace utility knives.
The largest construction or workplace utility knife typically feature retractable and replaceable blades, made of either die-cast metal or molded plastic. Some use standard razor blades, others specialized double-ended utility blades. The user can adjust how far the blade extends from the handle, so that, for example, the knife can be used to cut the tape sealing a package without damaging the contents of the package. When the blade becomes dull, it can be quickly reversed or switched for a new one. Spare or used blades are stored in the hollow handle of some models, and can be accessed by removing a screw and opening the handle. Other models feature a quick-change mechanism that allows replacing the blade without tools, as well as a flip-out blade storage tray. The blades for this type of utility knife come in both double- and single-ended versions, and are interchangeable with many, but not all, of the later copies. Specialized blades also exist for cutting string, linoleum, and other materials.
Another style is a snap-off utility knife that contains a long, segmented blade that slides out from it. As the endmost edge becomes dull, it can be broken off the remaining blade, exposing the next section, which is sharp and ready for use. The snapping is best accomplished with a blade snapper that is often built-in, or a pair of pliers, and the break occurs at the score lines, where the metal is thinnest. When all the individual segments are used, the knife may be thrown away, or, more often, refilled with a replacement blade. This design was introduced by Japanese manufacturer Olfa Corporation in 1956 as the world's first snap-off blade and was inspired from analyzing the sharp cutting edge produced when glass is broken and how pieces of a chocolate bar break into segments.
Another utility knife often used for cutting open boxes consists of a simple sleeve around a rectangular handle into which single-edge utility blades can be inserted. The sleeve slides up and down on the handle, holding the blade in place during use and covering the blade when not in use. The blade holder may either retract or fold into the handle, much like a folding-blade pocketknife. The blade holder is designed to expose just enough edge to cut through one layer of corrugated fiberboard, to minimize chances of damaging contents of cardboard boxes.
Use as weapon.
Most utility knives are not well suited to use as offensive weapons, with the exception of some outdoor-type utility knives employing longer blades. However, even small razor-blade type utility knives may sometimes find use as slashing weapons. It has been suggested by United States government officials that "box-cutter knives" were used in the September 11, 2001 terrorist attacks against the United States, though the exact design of the knives actually used is unclear. 
Small work-type utility knives have also been used to commit robbery and other crimes. In June 2004, a Japanese student was slashed to death with a segmented-type utility knife.
In the United Kingdom, the law was changed to raise the age limit for purchasing knives, including utility knives, from 16 to 18.

</doc>
<doc id="4169" url="http://en.wikipedia.org/wiki?curid=4169" title="Bronze">
Bronze

Bronze is an alloy consisting primarily of copper.
The addition of other metals (usually tin, sometimes arsenic), produces an alloy much harder than plain copper. The historical period where the archeological record contains many bronze artifacts is known as the Bronze Age.
Because historical pieces were often made of brasses (copper and zinc) and bronzes with different compositions, modern museum and scholarly descriptions of older objects increasingly use the more inclusive term "copper alloy" instead.
The word "bronze" (1730–40) is borrowed from French "bronze" (1511), itself borrowed from Italian "bronzo" "bell metal, brass" (13th century) (transcribed in Medieval Latin as "bronzium"), from either:
History.
The discovery of bronze enabled people to create metal objects which were harder and more durable than previously possible. Bronze tools, weapons, armor, and building materials such as decorative tiles were harder and more durable than their stone and copper ("Chalcolithic") predecessors. Initially, bronze was made out of copper and arsenic, forming arsenic bronze, or from naturally or artificially mixed ores of copper and arsenic. It was only later that tin was used, becoming the major non-copper ingredient of bronze in the late 3rd millennium BC. Tin bronze was superior to arsenic bronze in that the alloying process could be more easily controlled, and the resulting alloy was stronger and easier to cast. Also, unlike arsenic, tin is not toxic.
The earliest tin-alloy bronze dates to 4500 BCE in a Vinča culture site in Pločnik (Serbia). Other early examples date to the late 4th millennium BC in Susa (Iran) and some ancient sites in China, Luristan (Iran) and Mesopotamia (Iraq).
Ores of copper and the far rarer tin are not often found together (exceptions include one ancient site in Thailand and one in Iran), so serious bronze work has always involved trade. Tin sources and trade in ancient times had a major influence on the development of cultures. In Europe, a major source of tin was England's deposits of ore in Cornwall, which were traded as far as Phoenicia in the Eastern Mediterranean.
Though bronze is generally harder than wrought iron, with Vickers hardness of 60–258 vs. 30–80, the Bronze Age gave way to the Iron Age because iron was easier to find and easier to process into a usable grade of metal (it can be made into higher grades, but doing so takes significantly more effort and skill). Pure iron is soft, and the process of beating and folding sponge iron to make wrought iron removes carbon and other impurities from the metal which need to be re-introduced to improve hardness. Careful control of the alloying and tempering eventually allowed for wrought iron with properties comparable to modern steel.
Bronze was still used during the Iron Age. For many purposes, the weaker wrought iron was found to be sufficiently strong. Archaeologists suspect that a serious disruption of the tin trade precipitated the transition. The population migrations around 1200–1100 BC reduced the shipping of tin around the Mediterranean (and from Great Britain), limiting supplies and raising prices.
 As the art of working in iron improved, iron became cheaper, and as cultures advanced from wrought iron (typically forged by hand – "wrought" – by blacksmiths) to machine forged iron (typically made with trip hammers powered by water), the blacksmiths learned how to make steel, which is stronger than bronze and holds a sharper edge longer.
Composition.
There are many different bronze alloys, but typically modern bronze is 88% copper and 12% tin. Alpha bronze consists of the alpha solid solution of tin in copper. Alpha bronze alloys of 4–5% tin are used to make coins, springs, turbines and blades. Historical "bronzes" are highly variable in composition, as most metalworkers probably used whatever scrap was on hand; the metal of the 12th-century English Gloucester Candlestick is bronze containing a mixture of copper, zinc, tin, lead, nickel, iron, antimony, arsenic with an unusually large amount of silver – between 22.5% in the base and 5.76% in the pan below the candle. The proportions of this mixture may suggest that the candlestick was made from a hoard of old coins. The Benin Bronzes are really brass, and the Romanesque Baptismal font at St Bartholomew's Church, Liège is described as both bronze and brass.
In the Bronze Age, two forms of bronze were commonly used: "classic bronze", about 10% tin, was used in casting; and "mild bronze", about 6% tin, was hammered from ingots to make sheets. Bladed weapons were mostly cast from classic bronze, while helmets and armor were hammered from mild bronze.
Commercial bronze (90% copper and 10% zinc) and architectural bronze (57% copper, 3% lead, 40% zinc) are more properly regarded as brass alloys because they contain zinc as the main alloying ingredient. They are commonly used in architectural applications.
Bismuth bronze is a bronze alloy with a composition of 52% copper, 30% nickel, 12% zinc, 5% lead, and 1% bismuth. It is able to hold a good polish and so is sometimes used in light reflectors and mirrors.
Plastic bronze is bronze containing a significant quantity of lead which makes for improved plasticity possibly used by the ancient Greeks in their ship construction.
Other bronze alloys include aluminium bronze, phosphor bronze, manganese bronze, bell metal, arsenical bronze, speculum metal and cymbal alloys.
Properties.
Typically bronze only oxidizes superficially; once a copper oxide (eventually becoming copper carbonate) layer is formed, the underlying metal is protected from further corrosion. However, if copper chlorides are formed, a corrosion-mode called "bronze disease" will eventually completely destroy it. Copper-based alloys have lower melting points than steel or iron, and are more readily produced from their constituent metals. They are generally about 10 percent heavier than steel, although alloys using aluminium or silicon may be slightly less dense. Bronzes are softer and weaker than steel—bronze springs, for example, are less stiff (and so store less energy) for the same bulk. Bronze resists corrosion (especially seawater corrosion) and metal fatigue more than steel and is a better conductor of heat and electricity than most steels. The cost of copper-base alloys is generally higher than that of steels but lower than that of nickel-base alloys.
Copper and its alloys have a huge variety of uses that reflect their versatile physical, mechanical, and chemical properties. Some common examples are the high electrical conductivity of pure copper, the low-friction properties of bearing bronze (bronze which has a high lead content— 6-8%), the resonant qualities of bell bronze (20% tin, 80% copper), and the resistance to corrosion by sea water of several bronze alloys.
The melting point of bronze varies depending on the ratio of the alloy components and is about . Bronze may be nonmagnetic, but certain alloys containing iron or nickel may have magnetic properties.
Uses.
Bronze was especially suitable for use in boat and ship fittings prior to the wide employment of stainless steel owing to its combination of toughness and resistance to salt water corrosion. Bronze is still commonly used in ship propellers and submerged bearings.
In the 20th century, silicon was introduced as the primary alloying element, creating an alloy with wide application in industry and the major form used in contemporary statuary. Sculptors may prefer silicon bronze because of the ready availability of silicon bronze brazing rod, which allows color-matched repair of defects in castings. Aluminium is also used for the structural metal aluminium bronze.
It is also widely used for cast bronze sculpture. Many common bronze alloys have the unusual and very desirable property of expanding slightly just before they set, thus filling in the finest details of a mold. Bronze parts are tough and typically used for bearings, clips, electrical connectors and springs.
Bronze also has very low metal-on-metal friction, which made it invaluable for the building of cannon where iron cannonballs would otherwise stick in the barrel. It is still widely used today for springs, bearings, bushings, automobile transmission pilot bearings, and similar fittings, and is particularly common in the bearings of small electric motors. Phosphor bronze is particularly suited to precision-grade bearings and springs. It is also used in guitar and piano strings.
Unlike steel, bronze struck against a hard surface will not generate sparks, so it (along with beryllium copper) is used to make hammers, mallets, wrenches and other durable tools to be used in explosive atmospheres or in the presence of flammable vapors.
Bronze is used to make bronze wool for woodworking applications where steel wool would discolor oak.
Bronze statues.
In India, bronze sculptures from the Kushana (Chausa hoard) and Gupta periods (Brahma from Mirpur-Khas, Akota Hoard, Sultanganj Buddha) and later periods (Hansi Hoard) have been found.
Indian Hindu artisans from the period of the Chola empire in Tamil Nadu used bronze to create intricate statues via the lost wax casting method with ornate detailing depicting the deities of Hinduism mostly, but also the lifestyle of the period. The art form survives to this day, with many silpis, craftsmen, working in the areas of Swamimalai and Chennai.
The Assyrian king Sennacherib (704–681 BC) claims to have been the first to cast monumental bronze statues (of up to 30 tonnes) using two-part moulds instead of the lost-wax method.
In antiquity other cultures also produced works of high art using bronze. For example: in Africa, the bronze heads of the Kingdom of Benin; in Europe, Grecian bronzes typically of figures from Greek mythology; in east Asia, Chinese bronzes of the Shang and Zhou dynasty—more often ceremonial vessels but including some figurine examples. Bronze sculptures, although known for their longevity, still undergo microbial degradation; such as from certain species of yeasts.
Bronze continues into modern times as one of the materials of choice for monumental statuary.
Musical instruments.
Bronze is the preferred metal for top-quality bells, particularly bell metal, which is about 23% tin.
Nearly all professional cymbals are made from bronze, which gives a desirable balance of durability and timbre. Several types of bronze are used, commonly B20 bronze, which is roughly 20% tin, 80% copper, with traces of silver, or the tougher B8 bronze which is made from 8% tin and 92% copper. As the tin content in a bell or cymbal rises, the timbre drops.
Bronze is also used for the windings of steel and nylon strings of various stringed instruments such as the double bass, piano, harpsichord, and the guitar. Bronze strings are commonly reserved on pianoforte for the lower pitch tones, as they possess a superior sustain quality to that of high-tensile steel.
Bronzes of various metallurgical properties are widely used in struck idiophones around the world, notably bells, singing bowls, gongs, cymbals and other idiophones from Asia. Examples include Tibetan singing bowls, temple bells of many sizes and shapes, gongs, Javanese gamelan and other bronze musical instruments. The earliest bronze archeological finds in Indonesia date from 1–2 BCE, including flat plates probably suspended and struck by a wooden or bone mallet. Ancient bronze drums from Thailand and Vietnam date back 2,000 years. Bronze bells from Thailand and Cambodia date back to 3,600 BCE.
Some companies are now making saxophones from phosphor bronze (3.5 to 10% tin and up to 1% phosphorus content). Bell bronze is used to make the tone rings of many professional model banjos. The tone ring is a heavy (usually 3 lbs.) folded or arched metal ring attached to a thick wood rim, over which a skin, or most often, a plastic membrane (or head) is stretched - it is the bell bronze that gives the banjo a crisp powerful lower register and clear, bell-like treble register-especially in bluegrass music.
Medals.
Bronze has been used in the manufacture of various types of medals for centuries, and are known in contemporary times for being awarded for third place in sporting competitions and other events. The later usage was in part attributed to the choices of gold, silver and bronze to represent the first three Ages of Man in Greek mythology: the Golden Age, when men lived among the gods; the Silver age, where youth lasted a hundred years; and the Bronze Age, the era of heroes, and was first adopted at the 1904 Summer Olympics. At the 1896 event, silver was awarded to winners and bronze to runners-up, while at 1900 other prizes were given, not medals.
Industrial.
Various kinds of bronze are used in many different industrial applications.
Phosphor bronze is used for ships' propellers, musical instruments, and electrical contacts. Bearings are often made of bronze for its friction properties. It can be filled with oil to make the proprietary Oilite and similar material for bearings. Aluminium bronze is very hard and is used for bearings and machine tool ways.

</doc>
<doc id="4170" url="http://en.wikipedia.org/wiki?curid=4170" title="Benelux">
Benelux

Benelux (sometimes also written as "Bénélux" in French) is a union of states comprising three neighbouring countries in midwestern Europe: Belgium, the Netherlands and Luxembourg. The union's name is formed from joining the first two or three letters of each country's nameBelgium Netherlands Luxembourgand was first used to name the customs agreement that initiated the union (signed in 1944). It is now used in a more general way to refer to the geographic, economic and cultural grouping of the three countries.
In 1951, these countries joined West Germany, France, and Italy to form the European Coal and Steel Community, the predecessor of the European Economic Community (EEC) and today's European Union (EU).
The main institutions of the Union are the Committee of Ministers, the Parliament, the Council of the Union and the Secretariat-General, while the Benelux Organization for Intellectual Property and the Benelux Court of Justice cover the same territory but are not part of the Economic Union.
The Benelux Secretary-General is located in Brussels. It is the central administrative pillar of the Benelux Economic Union. It handles the secretariat of the Committee of Ministers, the Council of Economic Union and the various committees and working parties. Moreover, it ensures the registry of the Benelux Court of Justice.
Politics.
A Benelux Parliament (originally referred to as an "Interparliamentary Consultative Council") was created in 1955. This parliamentary assembly is composed of 21 members of the Dutch parliament, 21 members of the Belgian national and regional parliaments, and 7 members of the Luxembourgish parliament.
In 1944, the three countries signed the London Customs Convention, the treaty that established the Benelux Customs Union. Ratified in 1947, the treaty was in force from 1948 until being supplanted by the Benelux Economic Union. The treaty establishing the Benelux Economic Union ("Benelux Economische Unie/Union Économique Benelux") was signed on 3 February 1958 in The Hague and came into force on 1 November 1960 to promote the free movement of workers, capital, services, and goods in the region. Under the Treaty the Union implies the co-operation of economic, financial and social policies.
Law.
The Benelux Economic Union involves an intergovernmental cooperation. Decisions are taken unanimously.
The unification of the law of the three Benelux countries is mainly achieved by regulations of its Committee of Ministers, that only bind the three states, but are not directly applicable in their internal legal orders. They only become legally valid after having been incorporated into national law.
The Treaty establishing the Benelux Economic Union has provided the Committee of Ministers with the following legal instruments: decisions, conventions, recommendations and directives.
The Committee of Ministers can promulgate decisions in the fields for which it has competence - those fields are explicitly set down in the Union Treaty or the additional conventions. When the Committee of Ministers adopts a decision, it immediately becomes binding on the three governments. For a decision to be also applicable to the citizen, it must be transposed into national law.
The Union Treaty is not exhaustive. For this reason, Article 19 of the Treaty provides that the Committee of Ministers may conclude additional conventions. These therefore constitute extensions of the Union Treaty. They are submitted to the national parliaments for approval in keeping with the ratification procedure applied in each of the Member States. Thus, there is a large number of Benelux conventions in a wide range of subject matters.
Approval of a recommendation by the Committee of Ministers is not legally binding, but rather a moral stance by the three governments. Recommendations are not devoid of any binding effect in that their approval implies an undertaking in view of their execution.
The Committee of Ministers can issue directives to the Council of Economic Union, the Committees, the General Secretariat and the joint services.
In 1965, the treaty establishing a Benelux Court of Justice was signed. It entered into force in 1975. The Court, composed of judges from the highest courts of the three States, has to guarantee the uniform interpretation of common legal rules. This international judicial institution is located in Brussels.
The Benelux is particularly active in the field of intellectual property. The three countries established a "Benelux Trademarks Office" and a "Benelux Designs Office", both situated in The Hague. In 2005, they concluded a treaty establishing a "Benelux Organization for Intellectual Property" which replaced both offices upon its entry into force on 1 September 2006. This Organization is the official body for the registration of trademarks and designs in the Benelux. In addition, it offers the possibility to formally record the existence of ideas, concepts, designs, prototypes and the like.
Demographics and geography.
The Benelux region has a total population of about 28,365,937 and occupies an area of approximately . Thus, the Benelux has a population density of 380/km² (983/sq mi).
Sports.
In 2000, Belgium and the Netherlands jointly hosted the UEFA European Championship. In June 2007, representatives of the three countries announced they would bid, as a single political entity, for the 2018 FIFA World Cup.
Renewal of the agreement.
The Treaty between the Benelux countries establishing the Benelux Economic Union was limited to a period of 50 years. During the following years, and even more so after the creation of the European Union, the Benelux cooperation focused on developing other fields of activity within a constantly changing international context.
At the end of the 50 years, the governments of the three Benelux countries decided to renew the agreement, taking into account the new aspects of the Benelux-cooperation – such as security – and the new federal government structure of Belgium. The original establishing treaty, set to expire in 2010, was replaced by a new legal framework (called the Treaty revising the Treaty establishing the Benelux Economic Union), which was signed on 17 June 2008.
The new treaty has no set time limit and the name of the "Benelux Economic Union" changed to "Benelux Union" to reflect the broad scope on the union. The main objectives of the treaty are the continuation and enlargement of the cooperation between the three member states within a larger European context. The renewed treaty explicitly foresees the possibility that the Benelux countries will cooperate with other European member States or with regional cooperation structures. The new Benelux cooperation focuses on three main topics: internal market and economic union, sustainability, justice and internal affairs. The number of structures in the renewed Treaty has been reduced and thus simplified. Five Benelux institutions remain: the Benelux Committee of Ministers, the Benelux Council, the Benelux Parliament, the Benelux Court of Justice, the Benelux Secretariat General. Beside these five institutions, the Benelux Organization for Intellectual Property is also present in this Treaty.

</doc>
<doc id="4171" url="http://en.wikipedia.org/wiki?curid=4171" title="Boston Herald">
Boston Herald

The Boston Herald is a daily newspaper whose primary market is Boston, Massachusetts, United States, and its surrounding area. It was started in 1846 and is one of the oldest daily newspapers in the United States. It has been awarded eight Pulitzer Prizes in its history, including four for editorial writing and three for photography before it was converted to tabloid format in 1981. The Herald was named one of the "10 Newspapers That 'Do It Right'" in 2012 by "Editor & Publisher".
History.
The "Herald"'s history can be traced back through two lineages, the "Daily Advertiser" and the old "Boston Herald", and two media moguls, William Randolph Hearst and Rupert Murdoch.
The Original "Boston Herald".
The original "Boston Herald" was founded in 1846 by a group of Boston printers jointly under the name of John A. French & Company. The paper was published as a single two-sided sheet, selling for one cent. Its first editor, William O. Eaton, just 22 years old, said "The "Herald" will be independent in politics and religion; liberal, industrious, enterprising, critically concerned with literacy and dramatic matters, and diligent in its mission to report and analyze the news, local and global."
In 1847 the "Boston Herald" absorbed the Boston "American Eagle" and the Boston "Daily Times".
"The Boston Herald and Boston Journal".
In October 1917, John H. Higgins, the publisher and treasurer of the Boston Herald bought out its next door neighbor "The Boston Journal" and created "The Boston Herald and Boston Journal"
"The American Traveler".
Even earlier than the "Herald", the weekly "American Traveler" was founded in 1825 as a bulletin for stagecoach listings.
The "Boston Evening Traveller".
The "Boston Evening Traveler" was founded in 1845. The " Boston Evening Traveler" was the successor to the weekly "American Traveler" and the semi-weekly "Boston Traveler". In 1912, the "Herald" acquired the "Traveler", continuing to publish both under their own names. For many years, the newspaper was controlled by many of the investors in United Shoe Machinery Co. After a newspaper strike in 1967, Herald-Traveler Corp. suspended the afternoon "Traveler" and absorbed the evening edition into the Herald to create the "Boston Herald Traveler."
"The Boston Daily Advertiser".
The "Boston Daily Advertiser" was established in 1813 in Boston by Nathan Hale. The paper grew to prominence throughout the 19th century, taking over other Boston area papers. In 1832 The Advertiser took over control of "The Boston Patriot", and then in 1840 it took over and absorbed "The Boston Gazette". The paper was purchased by William Randolph Hearst in 1917. In 1920 the "Advertiser" was merged with "The Boston Record", initially the combined newspaper was called the "Boston Advertiser" however when the combined newspaper became an illustrated tabloid in 1921 it was renamed "The Boston American". Hearst Corp. continued using the name "Advertiser" for its Sunday paper until the early 1970s.
"The Boston Record".
On September 3, 1884 "The Boston Evening Record" was started by the "Boston Advertiser" as a campaign newspaper. The "Record" was so popular that it was made a permanent publication.
"The Boston American".
In 1904, William Randolph Hearst began publishing his own newspaper in Boston called "The American". Hearst ultimately ended up purchasing the "Daily Advertiser" in 1917. By 1938, the "Daily Advertiser" had changed to the "Daily Record", and "The American" had become the "Sunday Advertiser". A third paper owned by Hearst, called the "Afternoon Record", which had been renamed the "Evening American", merged in 1961 with the "Daily Record" to form the "Record American". The "Sunday Advertiser" and "Record American" would ultimately be merged in 1972 into "The Boston Herald Traveler" a line of newspapers that stretched back to the old "Boston Herald".
"The Boston Herald Traveler".
In 1946, Herald-Traveler Corporation acquired Boston radio station WHDH. Two years later, WHDH-FM was licensed, and on November 26, 1957, WHDH-TV made its début as an ABC affiliate on channel 5. In 1961, WHDH-TV's affiliation switched to CBS. Herald-Traveler Corp. operated for years under temporary authority from the Federal Communications Commission stemming from controversy over luncheon meetings the newspaper's chief executive had with an FCC commissioner during the original licensing process (Some Boston broadcast historians accuse the "Boston Globe" of being covertly behind the proceeding. The "Herald Traveler" was Republican in sympathies, and the "Globe" then had a firm policy of not endorsing political candidates.) The FCC ordered comparative hearings, and in 1969 a competing applicant, Boston Broadcasters, Inc. was granted a construction permit to replace WHDH-TV on channel 5. Herald-Traveler Corp. fought the decision in court—by this time, revenues from channel 5 were all but keeping the newspaper afloat—but its final appeal ran out in 1972, and on March 19 WHDH-TV was forced to surrender channel 5 to the new WCVB-TV.
"The Boston Herald Traveler and Record American".
Without a television station to subsidize the newspaper, the "Herald Traveler" was no longer able to remain in business, and the newspaper was sold to Hearst Corporation, which published the rival all-day newspaper, the "Record American". The two papers were merged to become an all-day paper called the "Boston Herald Traveler and Record American" in the morning and "Record-American and Boston Herald Traveler" in the afternoon. The first editions published under the new combined name were those of June 19, 1972. The afternoon edition was soon dropped and the unwieldy name shortened to "Boston Herald American", with the Sunday edition called the "Sunday Herald Advertiser". The "Herald American" was printed in broadsheet format, and failed to target a particular readership; where the "Record American" had been a typical city tabloid, the "Herald Traveler" was a Republican paper.
Murdoch purchases "The Herald American".
The "Herald American" converted to tabloid format in September 1981, but Hearst faced steep declines in circulation and advertising. The company announced it would close the "Herald American"—making Boston a one-newspaper town—on December 3, 1982. When the deadline came, Australian media baron Rupert Murdoch was negotiating to buy the paper and save it. He closed on the deal after 30 hours of talks with Hearst and newspaper unions—and five hours after Hearst had sent out notices to newsroom employees telling them they were terminated. The newspaper announced its own survival the next day with a full-page headline: "You Bet We're Alive!"
The "Boston Herald" once again.
Murdoch changed the paper's name back to the "Boston Herald". The "Herald" continued to grow, expanding its coverage and increasing its circulation until 2001, when nearly all newspapers fell victim to declining circulations and revenue.
Independent ownership.
In February 1994, Murdoch's News Corporation was forced to sell the paper, in order that its subsidiary Fox Television Stations could legally consummate its purchase of Fox affiliate WFXT (Channel 25) because Massachusetts Senator Ted Kennedy included language in an appropriations barring one company from owning a newspaper and television station in the same market. Patrick J. Purcell, who was the publisher of the "Boston Herald" and a former News Corporation executive, purchased the "Herald" and established it as an independent newspaper. Several years later, Purcell would give the "Herald" a suburban presence it never had by purchasing the money-losing Community Newspaper Company from Fidelity Investments. Although the companies merged under the banner of Herald Media, Inc., the suburban papers maintained their distinct editorial and marketing identity.
After years of operating profits at Community Newspaper and losses at the "Herald", Purcell in 2006 sold the suburban chain to newspaper conglomerate Liberty Group Publishing of Illinois, which soon after changed its name to GateHouse Media. The deal, which also saw GateHouse acquiring "The Patriot Ledger" and "The Enterprise" in south suburban Quincy and Brockton, netted $225 million for Purcell, who vowed to use the funds to clear the "Herald"'s debt and reinvest in the Paper.
Awards.
The "Herald"'s four Pulitzer Prizes for Editorial Writing, in 1924, 1927, 1949 and 1954, are among the most awarded to a single newspaper in the category. In 1957 Harry Trask was a young staff photographer at the "Traveler" when he was awarded a Pulitzer Prize for his photo sequence of the sinking of in July 1956. "Herald" photographer Stanley Forman received two Pulitzer Prizes consecutively in 1976 and 1977, the first for "Fire Escape Collapse", a dramatic shot of a young child falling in mid-air from her mother's arms on the upper stories of a burning apartment building to the waiting arms of firefighters below. The 1977 Pulitzer was awarded for "The Soiling of Old Glory", as Ted Landsmark, an African American civil rights lawyer, was charged at by a protester with an American flag during the Boston busing crisis. 
In 2006 the "Herald" won two SABEW awards from the Society of American Business Editors and Writers: one for its breaking news coverage of the takeover of the Boston-based Gillette Company by Procter & Gamble, and another for "overall excellence."
"Boston Herald" in Education Program.
The Boston Herald Newspapers in Education (NIE) program provides teachers with classroom newspapers and educational materials designed to help students of all ages and abilities excel. This is made possible through donations from Herald readers and other sponsors. The Boston Herald is available in two formats: the print edition and the online Smart Edition. The website can be found at http://bostonheraldnie.com/
Prices.
The "Boston Herald" prices are: $1.50 daily, $2.00 Sunday.

</doc>
<doc id="4173" url="http://en.wikipedia.org/wiki?curid=4173" title="Babe Ruth">
Babe Ruth

George Herman "Babe" Ruth, Jr. (February 6, 1895 – August 16, 1948), was an American baseball outfielder and pitcher who played 22 seasons in Major League Baseball (MLB) from 1914 to 1935. Nicknamed "The Bambino" and "The Sultan of Swat", he began his career as a stellar left-handed pitcher for the Boston Red Sox, but achieved his greatest fame as a slugging outfielder for the New York Yankees. Ruth established many MLB batting (and some pitching) records, including career home runs (714), slugging percentage (.690), runs batted in (RBIs) (2,213), bases on balls (2,062), and on-base plus slugging (OPS) (1.164), some of which have been broken. He was one of the first five inductees into the National Baseball Hall of Fame in 1936.
At age seven, Ruth was sent to St. Mary's Industrial School for Boys, (at Wilkens Avenue near Caton Avenue, on the outskirts of the city in suburban southwestern Baltimore County), a reformatory where he learned life lessons and baseball skills from Brother Matthias Boutlier, of the Christian Brothers, the school's disciplinarian and a capable baseball player. In 1914, Ruth was signed to play minor-league baseball for the Baltimore Orioles. Soon sold to the Red Sox, by 1916 he had built a reputation as an outstanding pitcher who sometimes hit long home runs, a feat unusual for any player in the pre-1920 dead-ball era. Although Ruth twice won 23 games in a season as a pitcher and was a member of three World Series championship teams with Boston, he wanted to play every day and was allowed to convert to an outfielder. He responded by breaking the MLB single-season home run record in 1919.
After that season, Red Sox owner Harry Frazee controversially sold Ruth to the Yankees. In his 15 years with New York, Ruth helped the Yankees win seven league championships and four World Series championships. His big swing led to escalating home run totals that not only drew fans to the ballpark and boosted the sport's popularity but also helped usher in the live-ball era of baseball, in which it evolved from a low-scoring game of strategy to a sport where the home run was a major factor. As part of the Yankees' vaunted "Murderer's Row" lineup of 1927, Ruth hit 60 home runs, extending his MLB single-season record. He retired in 1935 after a short stint with the Boston Braves. During his career, Ruth led the league in home runs during a season twelve times.
Ruth's legendary power and charismatic personality made him a larger-than-life figure in the "Roaring Twenties". During his career, he was the target of intense press and public attention for his baseball exploits and off-field penchants for drinking and womanizing. His often reckless lifestyle was tempered by his willingness to do good by visiting children at hospitals and orphanages. He was denied a job in baseball for most of his retirement, most likely due to poor behavior during parts of his playing career. In his final years, Ruth made many public appearances, especially in support of American efforts in World War II. In 1946, he became ill with cancer, and died two years later. Ruth is regarded as one of the greatest sports heroes in American culture, and is considered by many to be the greatest baseball player of all time.
Early years.
George Herman Ruth, Jr., was born at 216 Emory Street in Pigtown, a rough neighborhood of Baltimore, Maryland. Ruth's parents, George Herman Ruth, Sr., and Katherine Schamberger, were both German-American. George Ruth, Sr., had a series of jobs, including lightning rod salesman and streetcar operator,
before becoming a counterman in a family-owned combination grocery and saloon on Frederick Street. George Jr. was born in the house of his maternal grandfather, Pius Schamberger, a German immigrant and trade unionist. Only one of young George's seven siblings, his younger sister Mamie, survived infancy.
Many aspects of Ruth's childhood are undetermined, including even the date of his parents' marriage. The family moved to 339 South Goodyear Street, not far from the rail yards, when young George was a toddler; by the time he was six, his father had a saloon with an upstairs apartment at 426 West Camden Street. Details about why he was sent, at the age of seven, to St. Mary's Industrial School for Boys, a reformatory and orphanage, are similarly scanty. Babe Ruth, as an adult, suggested that not only was he running the streets and rarely attending school, he was drinking beer when his father was not looking. Stories exist that after a violent incident at the saloon, the city authorities decided the environment was unsuitable for a small child. At St. Mary's, which he entered on June 13, 1902, he was recorded as "incorrigible"; he spent much of the next twelve years there.
Although St. Mary's inmates received an education, a substantial amount of time was devoted to work, particularly once the boys turned 12. Ruth became a shirtmaker, and was also proficient as a carpenter. He would adjust his own shirt collars, rather than having a tailor do it, even during his well-paid baseball career. The boys, aged 5 to 21, did most work around the facility, from cooking to shoemaking, and renovated St. Mary's in 1912. The food was simple, and the Xaverian Brothers who ran the school insisted on strict discipline; corporal punishment was common. Ruth's nickname there was "Niggerlips", as he had large facial features and was darker than most boys at the all-white reformatory.
Ruth was sometimes allowed to rejoin his family, or was placed at St. James's Home, a supervised residence with work in the community, but he was always returned to St. Mary's. He rarely was visited by his family; his mother died when he was 12 and by some accounts, he was permitted to leave St. Mary's only to attend the funeral. How Ruth came to play baseball there is uncertain: according to one account, his placement at St. Mary's was due in part to repeatedly breaking Baltimore's windows with long hits while playing street ball; by another, he was told to join a team on his first day at St. Mary's by the school's athletic director, Brother Herman, becoming a catcher even though left-handers rarely play that position. During his time there he also played third base and shortstop, again unusual for a left-hander, and was forced to wear mitts and gloves made for right-handers. He was encouraged in his pursuits by the school's Prefect of Discipline, Brother Matthias Boutlier, a native of Nova Scotia. A large man, Brother Matthias was greatly respected by the boys both for his strength and for his fairness. For the rest of his life, Ruth would praise Brother Matthias, and his running and hitting styles closely resembled his teacher's. Ruth stated, "I think I was born as a hitter the first day I ever saw him hit a baseball." The older man became a mentor and role model to George; biographer Robert W. Creamer commented on the closeness between the two:
The school's influence remained with Ruth in other ways: a lifelong Catholic, he would sometimes attend Mass after carousing all night, and he became a well-known member of the Knights of Columbus. He would visit orphanages, schools, and hospitals throughout his life, often avoiding publicity. He was generous to St. Mary's as he became famous and rich, donating money and his presence at fundraisers, and spending $5,000 to buy Brother Matthias a Cadillac in 1926—subsequently replacing it when it was destroyed in an accident. Nevertheless, his biographer Leigh Montville suggests that many of the off-the-field excesses of Ruth's career were driven by the deprivations of his time at St. Mary's.
Most of the boys at St. Mary's played baseball, with organized leagues at different levels of proficiency. Ruth later estimated that he played 200 games a year as he steadily climbed the ladder of success. Although he played all positions at one time or another (including infield positions generally reserved for right-handers), he gained stardom as a pitcher. According to Brother Matthias, Ruth was standing to one side laughing at the bumbling pitching efforts of fellow students, and Matthias told him to go in and see if he could do better. After becoming the best pitcher at St. Mary's, in 1913, when Ruth was 18, he was allowed to leave the premises to play weekend games on teams drawn from the community. He was mentioned in several newspaper articles, for both his pitching prowess and ability to hit long home runs.
Baltimore Orioles.
In early 1914, Ruth was signed to a professional baseball contract by Jack Dunn, owner and manager of the minor-league Baltimore Orioles, an International League team. The circumstances of Ruth's signing cannot be stated with certainty, with historical fact obscured by stories that cannot all be true. By some accounts, Dunn was urged to attend a game between an all-star team from St. Mary's and one from another Xaverian facility, Mount St. Mary's College. Some versions have Ruth running away before the eagerly awaited game, to return in time to be punished, and then pitching St. Mary's to victory as Dunn watched. Others have Washington Senators pitcher Joe Engel, a Mount St. Mary's graduate, pitching in an alumni game after watching a preliminary contest between the college's freshmen and a team from St. Mary's, including Ruth. Engel watched Ruth play, then told Dunn about him at a chance meeting in Washington. Ruth, in his autobiography, stated that he worked out for Dunn for a half hour, and was signed. According to biographer Kal Wagenheim, there were legal difficulties to be straightened out as Ruth was supposed to remain at the school until he turned 21. Ruth was to receive a salary of $250 per month.
The train journey to spring training in Fayetteville, North Carolina, in early March was likely Ruth's first outside the Baltimore area. The rookie ballplayer was the subject of various pranks by the veterans, who were probably also the source of his famous nickname. There are various accounts of how Ruth came to be called Babe, but most center on his being referred to as "Dunnie's babe" or a variant. "Babe" was at that time a common nickname in baseball, with perhaps the most famous to that point being Pittsburgh Pirates pitcher and 1909 World Series hero Babe Adams, who appeared younger than he was.
Babe Ruth's first appearance as a professional ballplayer was in an intersquad game on March 7, 1914. Ruth played shortstop, and pitched the last two innings of a 15–9 victory. In his second at bat, Ruth hit a long home run to right, which was reported locally to be longer than a legendary shot hit in Fayetteville by Jim Thorpe. His first appearance against a team in organized baseball was an exhibition against the major-league Philadelphia Phillies: Ruth pitched the middle three innings, giving up two runs in the fourth, but then settling down and pitching a scoreless fifth and sixth. The following afternoon, Ruth was put in during the sixth inning against the Phillies and did not allow a run the rest of the way. The Orioles scored seven runs in the bottom of the eighth to overcome a 6–0 deficit, making Ruth the winning pitcher.
Once the regular season began, Ruth was a star pitcher who was also dangerous at the plate. The team performed well, yet received almost no attention from the Baltimore press. A third major league, the Federal League, had begun play, and the local franchise, the Baltimore Terrapins, restored that city to the major leagues for the first time since 1902. Few fans visited Oriole Park, where Ruth and his teammates labored in relative obscurity. Ruth may have been offered a bonus and a larger salary to jump to the Terrapins; when rumors to that effect swept Baltimore, giving Ruth the most publicity he had experienced to date, a Terrapins official denied it, stating it was their policy not to sign players under contract to Dunn.
The competition from the Terrapins caused Dunn to sustain large losses. Although by late June the Orioles were in first place, having won over two-thirds of their games, the paid attendance dropped as low as 150. Dunn explored a possible move by the Orioles to Richmond, Virginia, as well as the sale of a minority interest in the club. These possibilities fell through, leaving Dunn with little choice other than to sell his best players to major league teams to raise money. He offered Ruth to the reigning World Series champions, Connie Mack's Philadelphia Athletics, but Mack had his own financial problems. The Cincinnati Reds and New York Giants expressed interest in Ruth, but Dunn sold his contract, along with those of pitchers Ernie Shore and Ben Egan, to the Boston Red Sox of the American League (AL) on July 4. The sale price was announced as $25,000 but other reports lower the amount to half that, or possibly $8,500 plus the cancellation of a $3,000 loan. Ruth remained with the Orioles for several days while the Red Sox completed a road trip, and reported to the team in Boston on July 11.
Major League career.
Boston Red Sox (1914–19).
Developing star.
Ruth arrived in Boston on July 11, 1914, along with Egan and Shore. Ruth later told of meeting the woman he would first marry, Helen Woodford, that morning—she was then a 16-year-old waitress at Landers Coffee Shop, and Ruth related that she served him when he had breakfast there. Other stories, though, suggest the meeting happened on another day, and perhaps under other circumstances. Regardless of when he began to woo his first wife, he won his first game for the Red Sox that afternoon, 4–3, over the Cleveland Naps. He pitched to catcher Bill Carrigan, who was also the Red Sox manager. Shore was given a start by Carrigan the next day; he won that and his second start and thereafter was pitched regularly. Ruth lost his second start, and was thereafter little used. As a batter, in his major-league debut, Ruth went 0-for-2 against left-hander Willie Mitchell, striking out in his first at bat, before being removed for a pinch hitter in the seventh inning. Ruth was not much noticed by the fans, as Bostonians watched the Red Sox's crosstown rivals, the Braves, begin a legendary comeback that would take them from last place on the Fourth of July to the 1914 World Series championship.
Egan was traded to Cleveland after two weeks on the Boston roster. During his time as a Red Sox, he kept an eye on the inexperienced Ruth, much as Dunn had in Baltimore. When he was traded, no one took his place as supervisor. Ruth's new teammates considered him brash, and would have preferred him, as a rookie, to remain quiet and inconspicuous. When Ruth insisted on taking batting practice despite his being both a rookie who did not play regularly, and a pitcher, he arrived to find his bats sawn in half. His teammates nicknamed him "the Big Baboon", a name the swarthy Ruth, who had disliked the nickname "Niggerlips" at St. Mary's, detested. Ruth had received a raise on promotion to the major leagues, and quickly acquired tastes for fine food, liquor, and women, among other temptations.
Manager Carrigan allowed Ruth to pitch two exhibition games in mid-August. Although Ruth won both against minor-league competition, he was not restored to the pitching rotation. It is uncertain why Carrigan did not give Ruth additional opportunities to pitch. There are legends—filmed for the screen in "The Babe Ruth Story" (1948)—that the young pitcher had a habit of signaling his intent to throw a curveball by sticking out his tongue slightly, and that he was easy to hit until this changed. Creamer pointed out that it is common for inexperienced pitchers to display such habits, and the need to break Ruth of his would not constitute a reason to not use him at all. The biographer suggested that Carrigan was unwilling to use Ruth due to poor behavior by the rookie.
On July 30, 1914, Boston owner Joseph Lannin had purchased the minor-league Providence Grays, members of the International League. The Providence team had been owned by several people associated with the Detroit Tigers, including star hitter Ty Cobb, and as part of the transaction, a Providence pitcher was sent to the Tigers. To soothe Providence fans upset at losing a star, Lannin announced that the Red Sox would soon send a replacement to the Grays. This was intended to be Ruth, but his departure for Providence was delayed when Cincinnati Reds owner Garry Herrmann claimed him off waivers. After Lannin wrote to Herrmann explaining that the Red Sox wanted Ruth in Providence so he could develop as a player, and would not release him to a major league club, Herrmann allowed Ruth to be sent to the minors. Carrigan later stated that Ruth was not sent down to Providence to make him a better player, but to help the Grays win the International League pennant (league championship).
Ruth joined the Grays on August 18, 1914. What was left of the Baltimore Orioles after Dunn's deals had managed to hold on to first place until August 15, after which they continued to fade, leaving the pennant race between Providence and Rochester. Ruth was deeply impressed by Providence manager "Wild Bill" Donovan, previously a star pitcher with a 25–4 win–loss record for Detroit in 1907; in later years, he credited Donovan with teaching him much about pitching. Ruth was called upon often to pitch, in one stretch starting (and winning) four games in eight days. On September 5 in Toronto, Ruth pitched a one-hit 9–0 victory, and hit his first professional home run, his only one as a minor leaguer, off Ellis Johnson. Recalled to Boston after Providence finished the season in first place, he pitched and won a game for the Red Sox against the New York Yankees on October 2, getting his first major league hit, a double. Ruth finished the season with a record of 2–1 as a major leaguer and 23–8 in the International League (for Baltimore and Providence). Once the season concluded, Ruth married Helen in Ellicott City, Maryland. Creamer speculated that they did not marry in Baltimore, where the newlyweds boarded with George Ruth, Sr., to avoid possible interference from those at St. Mary's—both bride and groom were not yet of age and Ruth remained on parole from that institution until his 21st birthday.
Ruth reported to his first major league spring training in Hot Springs, Arkansas, in March 1915. Despite a relatively successful first season, he was not slated to start regularly for the Red Sox, who had two stellar left-handed pitchers already: the established stars Dutch Leonard, who had broken the record for the lowest earned run average (ERA) in a single season; and Ray Collins, a 20-game winner in both 1913 and 1914. Ruth was ineffective in his first start, taking the loss in the third game of the season. Injuries and ineffective pitching by other Boston pitchers gave Ruth another chance, and after some good relief appearances, Carrigan allowed Ruth another start, and he won a rain-shortened seven inning game. Ten days later, the manager had him start against the New York Yankees at the Polo Grounds. Ruth took a 3–2 lead into the ninth, but lost the game 4–3 in 13 innings. Ruth, hitting ninth as was customary for pitchers, hit a massive home run into the upper deck in right field off of Jack Warhop. At the time, home runs were rare in baseball, and Ruth's majestic shot awed the crowd. The winning pitcher, Warhop, would in August 1915 conclude a major league career of eight seasons, undistinguished but for being the first major league pitcher to give up a home run to Babe Ruth.
Carrigan was sufficiently impressed by Ruth's pitching to give him a spot in the starting rotation. Ruth finished the 1915 season 18–8 as a pitcher; as a hitter, he batted .315 and had four home runs. The Red Sox won the AL pennant, but with the pitching staff healthy, Ruth was not called upon to pitch in the 1915 World Series against the Philadelphia Phillies. Boston won in five games; Ruth was used as a pinch hitter in Game Five, but grounded out against Phillies ace Grover Cleveland Alexander. Despite his success as a pitcher, Ruth was acquiring a reputation for long home runs; at Sportsman's Park against the St. Louis Browns, a Ruth hit soared over Grand Avenue, breaking the window of a Chevrolet dealership.
In 1916, there was attention focused on Ruth for his pitching, as he engaged in repeated pitching duels with the ace of the Washington Senators, Walter Johnson. The two met five times during the season, with Ruth winning four and Johnson one (Ruth had a no decision in Johnson's victory). Two of Ruth's victories were by the score of 1–0, one in a 13-inning game. Of the 1–0 shutout decided without extra innings, AL President Ban Johnson stated, "That was one of the best ball games I have ever seen." For the season, Ruth went 23–12, with a 1.75 ERA and nine shutouts, both of which led the league. Ruth's nine shutouts in 1916 set a league record for left-handers that would remain unmatched until Ron Guidry tied it in 1978. The Red Sox won the pennant and World Series again, this time defeating the Brooklyn Superbas (as the Dodgers were then known) in five games. Ruth started and won Game 2, 2–1, in 14 innings. Until another game of that length was played in 2005, this was the longest World Series game, and Ruth's pitching performance is still the longest postseason complete game victory.
Carrigan retired as player and manager after 1916, returning to his native Maine to be a businessman. Ruth, who played under four managers who are in the National Baseball Hall of Fame, always maintained that Carrigan, who is not enshrined there, was the best skipper he ever played for. There were other changes in the Red Sox organization that offseason, as Lannin sold the team to a three-man group headed by New York theatrical promoter Harry Frazee. Jack Barry was hired by Frazee as manager.
Emergence as a hitter.
Ruth went 24–13 with a 2.01 ERA and six shutouts in 1917, but the Sox finished in second place in the league, nine games behind the Chicago White Sox in the standings. On June 23 at Washington, Ruth made a memorable pitching start. When the home plate umpire called the first four pitches as balls, Ruth threw a punch at him, and was ejected from the game and later suspended for ten days. Ernie Shore was called in to relieve Ruth, and was allowed eight warm-up pitches. The runner who had reached base on the walk was caught stealing, and Shore retired all 26 batters he faced to win the game. Shore's feat was listed as a perfect game for many years; in 1991, Major League Baseball's (MLB) Committee on Statistical Accuracy caused it to be listed as a combined no-hitter. In 1917, Ruth was used little as a batter, other than his plate appearances while pitching, and hit .325 with two home runs.
The entry of the United States into World War I occurred at the start of the season, and overshadowed the sport. Conscription was introduced in September 1917, and most baseball players in the big leagues were of draft age. This included Barry, who was a player-manager, and who joined the Naval Reserve in an attempt to avoid the draft, only to be called up after the 1917 season. Frazee hired International League President Ed Barrow as Red Sox manager. Barrow had spent the previous 30 years in a variety of baseball jobs, though he never played the game professionally. With the major leagues shorthanded due to the war, Barrow had many holes in the Red Sox lineup to fill. Ruth also noticed these vacancies in the lineup, and, dissatisfied in the role of a pitcher who appeared every four or five days, wanted to play every day at another position. Barrow tried Ruth at first base and in the outfield during the exhibition season, but as the team moved towards Boston and the season opener, restricted him to pitching. At the time, Ruth was possibly the best left-handed pitcher in baseball; allowing him to play another position was an experiment that could have backfired.
Inexperienced as a manager, Barrow had player Harry Hooper advise him on baseball game strategy. Hooper urged his manager to allow Ruth to play another position when he was not pitching, arguing to Barrow, who had invested in the club, that the crowds were larger on days when Ruth played, attracted by his hitting. Barrow gave in early in May; Ruth promptly hit home runs in four consecutive games (one an exhibition), the last off of Walter Johnson. For the first time in his career (disregarding pinch-hitting appearances), Ruth was allowed a place in the batting order higher than ninth.
Although Barrow predicted that Ruth would beg to return to pitching the first time he experienced a batting slump, that did not occur. Barrow used Ruth primarily as an outfielder in the war-shortened 1918 season. Ruth hit .300, with 11 home runs, enough to secure him a share of the major league home run title with Tillie Walker of the Philadelphia Athletics. He was still occasionally used as a pitcher, and had a 13–7 record with a 2.22 ERA.
The Red Sox won their third pennant in four years, and faced the Chicago Cubs in the 1918 World Series, beginning on September 5, the earliest in history. The season was shortened as the government had ruled that baseball players eligible for the military would have to be inducted or work in critical war industries, such as armaments plants. Ruth pitched Game One for the Red Sox, a 1–0 shutout. Before Game Four, Ruth injured his left hand in a fight; he pitched anyway. He gave up seven hits and six walks, but was helped by outstanding fielding behind him and by his own batting efforts, as a fourth-inning triple by Ruth gave his team a 2–0 lead. The Cubs tied the game in the eighth inning, but the Red Sox scored to take a 3–2 again in the bottom of that inning. After Ruth gave up a hit and a walk to start the ninth inning, he was relieved on the mound by Joe Bush. To keep Ruth and his bat in the game, he was sent to play left field. Bush retired the side to give Ruth his second win of the Series, and the third and last World Series pitching victory of his career, against no defeats, in three pitching appearances. Ruth's effort gave his team a three-games-to-one lead, and two days later the Red Sox won their third Series in four years, four games to two. Before allowing the Cubs to score in Game Four, Ruth pitched consecutive scoreless innings, a record for the World Series that stood until broken, after Ruth's death, by Whitey Ford in 1961. Ruth was prouder of that record than he was of any of his batting feats.
With the World Series over, Ruth made himself exempt from conscription by accepting a nominal position with a Pennsylvania steel mill. Many industrial establishments took pride in their baseball teams and sought to engage major leaguers. The end of the war in November set Ruth free to play baseball without such contrivances.
During the 1919 season, Ruth pitched in only 17 of his 130 games, compiling an 8–5 record as Barrow used him as a pitcher mostly in the early part of the season, when the Red Sox manager still had hopes of a second consecutive pennant. By late June, the Red Sox were clearly out of the race, and Barrow had no objection to Ruth concentrating on his hitting, if only because it drew people to the ballpark. Ruth had hit a home run against the Yankees on Opening Day, and another during a month-long batting slump that soon followed. Relieved of his pitching duties, Ruth began an unprecedented spell of slugging home runs, which gave him widespread public and press attention. Even his failures were seen as majestic—one sportswriter noted, "When Ruth misses a swipe at the ball, the stands quiver".
Two home runs by Ruth on July 5, and one in each of two consecutive games a week later, raised his season total to 11, tying his career best from 1918. The first record to fall was the AL single-season mark of 16, set by Ralph "Socks" Seybold in 1902. Ruth matched that on July 29, then pulled ahead toward the major league record of 24, set by Buck Freeman in 1899. Ruth reached this on September 8, by which time, writers had discovered that Ned Williamson of the 1884 Chicago White Stockings had hit 27—though in a ballpark where the distance to right field was only . On September 20, "Babe Ruth Day" at Fenway Park, Ruth won the game with a home run in the bottom of the ninth inning, tying Williamson. He broke the record four days later against the Yankees at the Polo Grounds, and hit one more against the Senators to finish with 29. The home run at Washington made Ruth the first major league player to hit a home run at all eight ballparks in his league. In spite of Ruth's hitting heroics, the Red Sox finished sixth, games behind the league champion White Sox.
Sale to New York.
As an out-of-towner from New York City, Frazee had been regarded with suspicion by Boston's sportswriters and baseball fans when he bought the team. He won them over with success on the field and a willingness to build the Red Sox by purchasing or trading for players. He offered the Senators $60,000 for Walter Johnson, but Washington owner Clark Griffith was unwilling. Even so, Frazee was successful in bringing other players to Boston, especially as replacements for players in the military. This willingness to spend for players helped the Red Sox secure the 1918 title. The 1919 season saw record-breaking attendance, and Ruth's home runs for Boston made him a national sensation. Nevertheless, on December 26, 1919, Frazee sold Ruth's contract to the New York Yankees.
Not all of the circumstances concerning the sale are known, but brewer and former congressman Jacob Ruppert, the New York team's principal owner, reportedly asked Yankee manager Miller Huggins what the team needed to be successful. "Get Ruth from Boston", Huggins supposedly replied, noting that Frazee was perennially in need of money to finance his theatrical productions. In any event, there was precedent for the Ruth transaction: when Boston pitcher Carl Mays left the Red Sox in a 1919 dispute, Frazee had settled the matter by selling Mays to the Yankees, though over the opposition of AL President Johnson.
According to one of Ruth's biographers, Jim Reisler, "why Frazee needed cash in 1919—and large infusions of it quickly—is still, more than 80 years later, a bit of a mystery". The often-told story is that Frazee needed money to finance the musical "No, No, Nanette", which was a Broadway hit and brought Frazee financial security. That play did not open until 1925, however, by which time Frazee had sold the Red Sox. Still, the story may be true in essence: "No, No, Nanette" was based on a Frazee-produced play, "My Lady Friends", which opened in 1919.
There were other financial pressures on Frazee, despite his team's success. Ruth, fully aware of baseball's popularity and his role in it, wanted to renegotiate his contract, signed before the 1919 season for $10,000 per year through 1921. He demanded that his salary be doubled, or he would sit out the season and cash in on his popularity through other ventures. Ruth's salary demands were causing other players to ask for more money. Additionally, Frazee still owed Lannin as much as $125,000 from the purchase of the club.
Although Ruppert and his co-owner, Colonel Tillinghast Huston, were both wealthy, and had aggressively purchased and traded for players in 1918 and 1919 to build a winning team, Ruppert faced losses in his brewing interests as Prohibition was implemented, and if their team left the Polo Grounds, where the Yankees were the tenants of the New York Giants, building a stadium in New York would be expensive. Nevertheless, when Frazee, who moved in the same social circles as Huston, hinted to the colonel that Ruth was available for the right price, the Yankees owners quickly pursued the purchase.
Frazee sold the rights to Babe Ruth for $100,000, the largest sum ever paid for a baseball player. The deal also involved a $350,000 loan from Ruppert to Frazee, secured by a mortgage on Fenway Park. Once it was agreed, Frazee informed Barrow, who, stunned, told the owner that he was getting the worse end of the bargain. Cynics have suggested that Barrow may have played a larger role in the Ruth sale, as less than a year after, he became the Yankee general manager, and in the following years made a number of purchases of Red Sox players from Frazee. The $100,000 price included $25,000 in cash, and notes for the same amount due November 1 in 1920, 1921, and 1922; Ruppert and Huston assisted Frazee in selling the notes to banks for immediate cash.
The transaction was contingent on Ruth signing a new contract, which was quickly accomplished—Ruth agreed to fulfill the remaining two years on his contract, but was given a $20,000 bonus, payable over two seasons. The deal was announced on January 6, 1920. Reaction in Boston was mixed: some fans were embittered at the loss of Ruth; others conceded that the slugger had become difficult to deal with. "The New York Times" suggested presciently, "The short right field wall at the Polo Grounds should prove an easy target for Ruth next season and, playing seventy-seven games at home, it would not be surprising if Ruth surpassed his home run record of twenty-nine circuit clouts next Summer." According to Reisler, "The Yankees had pulled off the sports steal of the century."
According to Marty Appel in his history of the Yankees, the transaction, "changed the fortunes of two high-profile franchises for decades". The Red Sox, winners of five of the first sixteen World Series, those played between 1903 and 1919, would not win another pennant until 1946, or another World Series until 2004, a drought attributed in baseball superstition to Frazee's sale of Ruth and sometimes dubbed the "Curse of the Bambino". The Yankees, on the other hand, had not won the AL championship prior to their acquisition of Ruth. They won seven AL pennants and four World Series with Ruth, and lead baseball with 40 pennants and 27 World Series titles in their history.
New York Yankees (1920–34).
Initial success (1920–23).
As a Yankee, Ruth's transition from a pitcher to a power-hitting outfielder became complete. In his fifteen-year Yankee career, consisting of over 2,000 games, Ruth broke many batting records, while making only five widely scattered appearances on the mound, winning all of them.
At the end of April 1920, the Yankees were 4–7, with the Red Sox leading the league with a 10–2 mark. Ruth had done little, having injured himself swinging the bat. Both situations began to change on May 1, when Ruth hit a ball completely out of the Polo Grounds, a feat believed only to have been previously accomplished by Joe Jackson. The Yankees won, 6–0, taking three out of four from the Red Sox. Ruth hit his second home run on May 2, and by the end of the month had set a major league record for home runs in a month with 11, and promptly broke it with 13 in June. Fans responded with record attendance: on May 16, Ruth and the Yankees drew 38,600 to the Polo Grounds, a record for the ballpark, and 15,000 fans were turned away. Large crowds jammed stadiums to see Ruth play when the Yankees were on the road.
The home runs kept coming; Ruth tied his own record of 29 on July 15, and broke it with home runs in both games of a doubleheader four days later. By the end of July, he had 37, but his pace slackened somewhat after that. Nevertheless, on September 4, he both tied and broke the organized baseball record for home runs in a season, snapping Perry Werden's 1895 mark of 44 in the minor Western League. The Yankees played well as a team, battling for the league lead early in the summer, but slumped in August in the AL pennant battle with Chicago and Cleveland. The championship was won by Cleveland, surging ahead after the Black Sox Scandal broke on September 28 and led to the suspension of many of the team's top players, including Joe Jackson. The Yankees finished third, but drew 1.2 million fans to the Polo Grounds, the first time a team had drawn a seven figure attendance. The rest of the league sold 600,000 more tickets, many fans there to see Ruth, who led the league with 54 home runs, 158 runs, and 137 runs batted in (RBIs).
Ruth was aided in his exploits, in 1920 and afterwards, by the fact that the A.J. Reach Company, maker of baseballs used in the major leagues, was using a more efficient machine to wind the yarn found within the baseball. When these went into play in 1920, the start of the live-ball era, the number of home runs increased by 184 over the previous year across the major leagues. Baseball statistician Bill James points out that while Ruth was likely aided by the change in the baseball, there were other factors at work, including the gradual abolition of the spitball (accelerated after the death of Ray Chapman, struck by a pitched ball thrown by Mays in August 1920) and the more frequent use of new baseballs (also a response to Chapman's death). Nevertheless, James theorizes that Ruth's 1920 explosion might have happened in 1919, had a full season of 154 games been played rather than 140, had Ruth refrained from pitching 133 innings that season, and if he were playing with any other home field but Fenway Park, where he hit only 9 of 29 home runs.
Yankees business manager Harry Sparrow had died early in the 1920 season; to replace him, Ruppert and Huston hired Barrow. Ruppert and Barrow quickly made a deal with Frazee for New York to acquire some of the players who would be mainstays of the early Yankee pennant-winning teams, including catcher Wally Schang and pitcher Waite Hoyt. The 21-year old Hoyt became close to Ruth:
Ruth hit home runs early and often in the 1921 season, during which he broke Roger Connor's mark for home runs in a career, 138. Each of the almost 600 home runs Ruth hit in his career after that extended his own record. After a slow start, the Yankees were soon locked in a tight pennant race with Cleveland, winners of the 1920 World Series. On September 15, Ruth hit his 55th home run, shattering his year-old single season record. In late September, the Yankees visited Cleveland and won three out of four games, giving them the upper hand in the race, and clinched their first pennant a few days later. Ruth finished the regular season with 59 home runs, batting .378 and with a slugging percentage of .846.
The Yankees had high expectations when they met the New York Giants in the 1921 World Series, and the Yankees won the first two games with Ruth in the lineup. However, Ruth badly scraped his elbow during Game 2, sliding into third base (he had walked and stolen both second and third bases). After the game, he was told by the team physician not to play the rest of the series. Despite this advice, he did play in the next three games, and pinch-hit in Game Eight of the best-of-nine series, but the Yankees lost, five games to three. Ruth hit .316, drove in five runs and hit his first World Series home run.
After the Series, Ruth and teammates Bob Meusel and Bill Piercy participated in a barnstorming tour in the Northeast. A rule then in force prohibited World Series participants from playing in exhibition games during the offseason, the purpose being to prevent Series participants from replicating the Series and undermining its value. Baseball Commissioner Kenesaw Mountain Landis suspended the trio until May 20, 1922, and fined them their 1921 World Series checks. In August 1922, the rule was changed to allow limited barnstorming for World Series participants, with Landis's permission required.
On March 6, 1922, Ruth signed a new contract, for three years at $52,000 a year. The largest sum ever paid a ballplayer to that point, it represented 40% of the team's player payroll. Despite his suspension, Ruth was named the Yankees' new on-field captain prior to the 1922 season. During the suspension, he worked out with the team in the morning, and played exhibition games with the Yankees on their off days. He and Meusel returned on May 20, to a sellout crowd at the Polo Grounds, but Ruth batted 0-for-4, and was booed. On May 25, he was thrown out of the game for throwing dust in umpire George Hildebrand's face, then climbed into the stands to confront a heckler. Ban Johnson ordered him fined, suspended, and stripped of his captaincy. In his shortened season, Ruth appeared in 110 games, batted .315, with 35 home runs, and drove in 99 runs, but compared to his previous two dominating years, the 1922 season was a disappointment. Despite Ruth's off-year, Yankees managed to win the pennant to face the New York Giants for the second straight year in the World Series. In the Series, Giants manager John McGraw instructed his pitchers to throw him nothing but curveballs, and Ruth never adjusted. Ruth had just two hits in seventeen at bats, and the Yankees lost to the Giants for the second straight year, by 4–0 (with one tie game). Sportswriter Joe Vila called him, "an exploded phenomenon".
After the season, Ruth was a guest at an Elks Club banquet, set up by Ruth's agent with Yankee team support. There, each speaker, concluding with future New York mayor Jimmy Walker, censured him for his poor behavior. An emotional Ruth promised reform, and, to the surprise of many, followed through. When he reported to spring training, he was in his best shape as a Yankee, weighing only .
The Yankees's status as tenants of the Giants at the Polo Grounds had become increasingly uneasy, and in 1922 Giants owner Charles Stoneham stated that the Yankees's lease, expiring after that season, would not be renewed. Ruppert and Huston had long contemplated a new stadium, and had taken an option on property at 161st Street and River Avenue in the Bronx. Yankee Stadium was completed in time for the home opener on April 18, 1923, at which the Babe hit the first home run in what was quickly dubbed "the House that Ruth Built". The ballpark was designed with Ruth in mind: although the venue's left-field fence was further from home plate than at the Polo Grounds, Yankee Stadium's right-field fence was closer, making home runs easier to hit for left-handed batters. To spare Ruth's eyes, right field–his defensive position–was not pointed into the afternoon sun, as was traditional; left fielder Meusel was soon suffering headaches from squinting toward home plate.
The Yankees were never challenged, leading the league for most of the 1923 season and winning the AL pennant by 17 games. Ruth finished the season with a career-high .393 batting average and major-league leading 41 home runs (tied with Cy Williams). Another career high for Ruth in 1923 was his 45 doubles, and he reached base 379 times, then a major league record. For the third straight year, the Yankees faced the Giants in the World Series, which Ruth dominated. He batted .368, walked eight times, scored eight runs, hit three home runs and slugged 1.000 during the series, as the Yankees won their first World Series championship, four games to two.
Batting title and "bellyache" (1924–25).
In 1924, the Yankees were favored to become the first team to win four consecutive pennants. Plagued by injuries, they found themselves in a battle with the Senators. Although the Yankees won 18 of 22 at one point in September, the Senators beat out the Yankees by two games. Ruth hit .378, winning his only AL batting title, with a league-leading 46 home runs.
Ruth had kept up his efforts to stay in shape in 1923 and 1924, but by early 1925 weighed nearly . His annual visit to Hot Springs, Arkansas, early in the year, where he exercised and took saunas, did him no good as he spent much of the time carousing. He became ill while there, and suffered relapses during spring training. Ruth collapsed in Asheville, North Carolina, as the team journeyed north. He was put on a train for New York, where he was briefly hospitalized. A rumor circulated that he had died, prompting British newspapers to print a premature obituary. In New York, Ruth collapsed again and was found unconscious in his hotel bathroom. He was taken to a hospital where he suffered multiple convulsions. After sportswriter W. O. McGeehan wrote that Ruth's illness was due to binging on hot dogs and soda pop before a game, it became known as "the bellyache heard 'round the world". However, the exact cause of his ailment has never been confirmed and remains a mystery. Glenn Stout, in his history of the Yankees, notes that the Ruth legend is "still one of the most sheltered in sports"; he suggests that alcohol was at the root of Ruth's illness, pointing to the fact that Ruth remained six weeks at St. Vincent's Hospital but was allowed to leave, under supervision, for workouts with the team for part of that time. He concludes that the hospitalization was behavior-related. Playing just 98 games, Ruth had his worst season as a Yankee; he finished with a .290 average and 25 home runs. The Yankees finished next to last in the AL with a 69–85 record, their last season with a losing record until 1965.
Murderer's Row (1926–28).
Ruth spent part of the offseason of 1925–26 working out at Artie McGovern's gym, getting back into shape. Barrow and Huggins had rebuilt the team, surrounding the veteran core with good young players like Tony Lazzeri and Lou Gehrig. Nevertheless, New York was not expected to win the pennant.
Babe Ruth returned to his normal production during 1926, batting .372 with 47 home runs and 146 RBIs. The Yankees built a ten-game lead by mid-June, and coasted to win the pennant by three games. The St. Louis Cardinals had won the National League with the lowest winning percentage for a pennant winner to that point (.578) and the Yankees were expected to win the World Series easily. Although the Yankees won the opener in New York, St. Louis took Games Two and Three. In Game Four, Ruth hit three home runs, the first time this had been done in a World Series game, to lead the Yankees to victory; in the fifth game Ruth caught a ball as he crashed into the fence, described by baseball writers as a defensive gem. New York took that game, but Grover Cleveland Alexander won Game Six for St. Louis to tie the Series at three games each, then got very drunk. He was nevertheless inserted into Game Seven in the seventh inning and shut down the Yankees to win the game, 3–2, and win the Series. Ruth had hit his fourth home run of the Series earlier in the game, and was the only Yankee to reach base off Alexander, walking in the ninth inning before being caught stealing to end the game. Although Ruth's attempt to steal second is often deemed a baserunning blunder, Creamer pointed out that the Yankees's chances of tying the game would have been greatly improved with a runner in scoring position.
The 1926 Series was also known for Ruth's promise to Johnny Sylvester, a hospitalized 11-year-old, that he would hit a home run on his behalf. Sylvester had been injured in a fall from a horse, and a friend of Sylvester's father gave the boy two autographed baseballs signed by Yankees and Cardinals, and relayed a promise from Ruth, who did not know the boy, to hit a home run for him. After the Series, Ruth visited the boy in the hospital. When the matter became public, the press greatly inflated it, and by some accounts, Ruth saved a dying boy's life by visiting him, emotionally promising to hit a home run, and doing so.
The 1927 New York Yankees team is considered one of the greatest squads that ever took the field. Known as Murderer's Row because of the power of its lineup, the team won a then-AL-record 110 games, and took the AL pennant by 19 games, clinching first place on Labor Day. With little suspense as to the pennant race, the nation's attention turned to Ruth's pursuit of his own single-season home run record of 59. He was not alone in this chase: Gehrig proved to be a slugger capable of challenging Ruth for his home run crown, tying Ruth with 24 home runs late in June. Through July and August, they were never separated by more than two home runs. Gehrig took the lead, 45–44, in the first game of a doubleheader at Fenway Park early in September; Ruth responded with two of his own to take the lead, as it proved permanently—Gehrig finished with 47. Even so, as of September 6, Ruth was still several games off his 1921 pace, and going into the final series against the Senators, had only 57. He hit two in the first game of the series, including one off of Paul Hopkins, facing his first major league batter, to tie the record. The following day, September 30, he broke it with his 60th homer, in the eighth inning off Tom Zachary to break a 2–2 tie. "Sixty! Let's see some son of a bitch try to top that one", Ruth exulted after the game. In addition to his career-high 60 home runs, Ruth batted .356, drove in 164 runs and slugged .772. In the 1927 World Series, the Yankees swept the Pittsburgh Pirates in four games; the National Leaguers were disheartened after watching the Yankees take batting practice before Game One, with ball after ball leaving Forbes Field. According to Appel, "The 1927 New York Yankees. Even today, the words inspire awe ... all baseball success is measured against the '27 team."
Before the 1928 season, Ruth signed a new contract for an unprecedented $80,000 per year. The season started off well for the Yankees, who led the league in the early going. But the Yankees were plagued by injuries, erratic pitching and inconsistent play. The Philadelphia Athletics, rebuilding after some lean years, erased the Yankees' big lead and even took over first place briefly in early September. The Yankees, however, regained first place when they beat the Athletics three out of four games in a pivotal series at Yankee Stadium later that month, and clinched the pennant in the final weekend of the season. Ruth's play in 1928 mirrored his team's performance. He got off to a hot start and on August 1, he had 42 home runs. This put him ahead of his 60 home run pace from the previous season. He then slumped for the latter part of the season, and he hit just twelve home runs in the last two months. Ruth's batting average also fell to .323, well below his career average. Nevertheless, he ended the season with 54 home runs. The Yankees swept the favored Cardinals in four games in the World Series, with Ruth batting .625 and hitting three home runs in Game Four, including one off Alexander.
"Called shot" and final Yankee years (1929–34).
Before the 1929 season, Ruppert, who had bought out Huston in 1923, announced that the Yankees would wear uniform numbers to allow fans at cavernous Yankee Stadium to tell one player from another. The Cardinals and Indians had each experimented with uniform numbers; the Yankees were the first to use them on both home and away uniforms. As Ruth batted third, he was given number 3. Ruth himself affected the Yankee uniform in one particular: to make him look slimmer, Ruppert added the iconic pinstripes.
Although the Yankees started well, the Athletics soon proved they were the better team in 1929, splitting two series with the Yankees in the first month of the season, then taking advantage of a Yankee losing streak in mid-May to gain first place. Although Ruth performed well, the Yankees were not able to catch the Athletics—Connie Mack had built another great team. Tragedy struck the Yankees late in the year as manager Huggins died of erysipelas, a bacterial skin infection, on September 25, only ten days after he had last led the team. Despite past differences, Ruth praised Huggins and described him as a "great guy". The Yankees finished second, 18 games behind the Athletics. Ruth hit .345 during the season, with 46 home runs and 154 RBIs.
The Yankees hired Bob Shawkey as manager, their fourth choice. Ruth politicked for the job of player-manager, but was not seriously considered by Ruppert and Barrow; Stout deems this the first hint Ruth would have no future with the Yankees once he was done as a player. Shawkey, a former Yankees player and teammate of Ruth, was unable to command the slugger's respect. The Athletics won their second consecutive pennant and World Series, as the Yankees finished in third place, sixteen games back. During that season Ruth was asked by a reporter what he thought of his yearly salary of $80,000 being more than President Hoover's $75,000. His response was, "I know, but I had a better year than Hoover." In 1930, Ruth hit .359 with 49 home runs (his best in his years after 1928) and 153 RBIs, and pitched his first game in nine years, a complete game victory. At the end of the season, Shawkey was fired and replaced with Cubs manager Joe McCarthy, though Ruth again unsuccessfully sought the job.
McCarthy was a disciplinarian, but chose not to interfere with Ruth, and the slugger for his part did not seek conflict with the manager. The team improved in 1931, but was no match for the Athletics, who won 107 games, games in front of the Yankees. Ruth, for his part, hit .373, with 46 home runs and 163 RBIs. He had 31 doubles, his most since 1924. In the 1932 season, the Yankees went 107–47 and won the pennant. Ruth's effectiveness had decreased somewhat, but he still hit .341 with 41 home runs and 137 RBIs. Nevertheless, he twice was sidelined due to injury during the season.
The Yankees faced the Cubs, McCarthy's former team, in the 1932 World Series. There was bad blood between the two teams as the Yankees resented the Cubs only awarding half a World Series share to Mark Koenig, a former Yankee. The games at Yankee Stadium had not been sellouts; both were won by the home team, with Ruth collecting two singles, but scoring four runs as he was walked four times by the Cubs pitchers. In Chicago, Ruth was resentful at the hostile crowds that met the Yankees's train and jeered them at the hotel. The crowd for Game Three included New York Governor Franklin D. Roosevelt, the Democratic candidate for president, who sat with Chicago Mayor Anton Cermak. Many in the crowd threw lemons at Ruth, a sign of derision, and others (as well as the Cubs themselves) shouted abuse at Ruth and other Yankees. They were briefly silenced when Ruth hit a three-run home run off Charlie Root in the first inning, but soon revived, and the Cubs tied the score at 4–4 in the fourth inning. When Ruth came to the plate in the top of the fifth, the Chicago crowd and players, led by pitcher Guy Bush, were screaming insults at Ruth. With the count at two balls and one strike, Ruth gestured, possibly in the direction of center field, and after the next pitch (a strike), may have pointed there with one hand. Ruth hit the fifth pitch over the center field fence; estimates were that it traveled nearly . Whether or not Ruth intended to indicate where he planned to (and did) hit the ball, the incident has gone down in legend as Babe Ruth's called shot. The Yankees won Game Three, and the following day clinched the Series with another victory. During that game, Bush hit Ruth on the arm with a pitch, causing words to be exchanged and provoking a game-winning Yankee rally.
Ruth remained productive in 1933, as he batted .301, with 34 home runs, 103 RBIs, and a league-leading 114 walks, as the Yankees finished second, seven games behind the Senators. He was selected to play right field by Athletics manager Connie Mack in the first Major League Baseball All-Star Game, held on July 6, 1933, at Comiskey Park in Chicago. He hit the first home run in the All-Star Game's history, a two-run blast against Bill Hallahan during the third inning, which helped the AL win the game 4–2. During the final game of the 1933 season, as a publicity stunt organized by his team, Ruth was called upon and pitched a complete game victory against the Red Sox, his final appearance as a pitcher. Despite unremarkable pitching numbers, Ruth had a 5–0 record in five games for the Yankees, raising his career totals to 94–46.
In 1934, Ruth played in his last full season. He accepted a pay cut from Ruppert to $35,000, but was still the highest-paid player in the major leagues. He could still handle a bat, recording a .288 batting average with 22 home runs, statistics Reisler described as "merely mortal". Ruth was selected to the AL All-Star team for the second consecutive year. During the game, New York Giants pitcher Carl Hubbell struck out Ruth and four other future Hall-of-Famers consecutively. The Yankees finished second again, seven games behind the Tigers.
Boston Braves (1935).
Although Ruth knew he was nearly finished as a player, he desired to remain in baseball as a manager. He was often spoken of as a possible candidate as managerial jobs opened up, but in 1932, when he was mentioned as a contender for the Red Sox position, stated that he was not yet ready to leave the field. There were rumors that Ruth was a likely candidate each time when the Cleveland Indians, Cincinnati Reds, and Detroit Tigers were looking for a manager, but nothing came of them.
Just before the 1934 season, Ruppert offered to make Ruth manager of the Yankees' top minor-league team, the Newark Bears, but he was talked out of it by his wife, Claire (Helen had died in 1929), and his business manager. Early in the 1934 season, Ruth began openly campaigning to become manager of the Yankees. However, the Yankee job was never a serious possibility. Ruppert always supported McCarthy, who would remain in his position for another 12 seasons. Ruth and McCarthy had never gotten along, and Ruth's managerial ambitions further chilled their relations. By the end of the season, Ruth hinted that he would retire unless Ruppert named him manager of the Yankees. For his part, Ruppert wanted his slugger to leave the team without drama and hard feelings when the time came.
During the 1934–35 offseason, Ruth circled the world with his wife, including a barnstorming tour of the Far East. At his final stop before returning home, in the United Kingdom, Ruth was introduced to cricket by Australian player Alan Fairfax, and after having little luck in a cricketer's stance, stood as a baseball batter and launched some massive shots around the field, destroying the bat in the process. Although Fairfax regretted that he could not have the time to make Ruth a cricket player, Ruth had lost any interest in such a career upon learning that the best batsmen made only about $40 per week.
Also during the offseason, Ruppert had been sounding out the other clubs in hopes of finding one that would be willing to take Ruth as a manager and/or a player. However, the only teams that seriously considered hiring Ruth were the Tigers and Athletics. Connie Mack gave some thought to stepping down as manager in favor of Ruth, but later dropped the idea, saying that Ruth's wife would be running the team in a month if Ruth ever took over.
While the barnstorming tour was under way, Ruppert began negotiating with Boston Braves owner Judge Emil Fuchs, who wanted Ruth as a gate attraction. Although the Braves had enjoyed modest recent success, finishing fourth in the National League in both 1933 and 1934, the team performed poorly at the box office. Unable to afford the rent at Braves Field, Fuchs had considered holding dog races there when the Braves were not at home, only to be turned down by Landis. After a series of phone calls, letters, and meetings, the Yankees traded Ruth to the Braves on February 26, 1935. Ruppert had stated that he would not release Ruth to go to another team as a full-time player. For this reason, it was announced that Ruth would become a team vice president and would be consulted on all club transactions, in addition to playing. He was also made assistant manager to Braves skipper Bill McKechnie. In a long letter to Ruth a few days before the press conference, Fuchs promised Ruth a share in the Braves' profits, with the possibility of becoming co-owner of the team. Fuchs also raised the possibility of Ruth succeeding McKechnie as manager, perhaps as early as 1936. Ruppert called the deal "the greatest opportunity Ruth ever had".
There was considerable attention as Ruth reported for spring training. He did not hit his first home run of the spring until after the team had left Florida, and was beginning the road north in Savannah. He hit two in an exhibition against the Bears. Amid much press attention, Ruth played his first home game in Boston in over 16 years. Before an opening-day crowd of over 25,000, including five of New England's six state governors, Ruth accounted for all of the Braves' runs in a 4–2 defeat of the New York Giants, hitting a two-run home run, singling to drive in a third run and later in the inning scoring the fourth. Although age and weight had slowed him, he made a running catch in left field which sportswriters deemed the defensive highlight of the game.
Although Ruth had two hits in the second game of the season, it soon settled down to a routine of Ruth performing poorly when he played at all, and the Braves losing most games. As the spring progressed, Ruth's deterioration became even more pronounced. He remained productive at the plate early on, but could do little else. His condition had deteriorated to the point that he could barely trot around the bases. His fielding had become so poor that three Braves pitchers threatened not to take the mound if he was in the lineup. Before long, Ruth stopped hitting as well. He grew increasingly annoyed that McKechnie ignored most of his advice. (McKechnie later said that Ruth's presence made enforcing discipline nearly impossible.)
Ruth soon realized that Fuchs had deceived him, and had no intention of giving him off-the-field responsibility or the manager's job. He later stated that his duties as vice president consisted of making public appearances and autographing tickets. Ruth also found out that far from giving him a share of the profits, Fuchs wanted him to invest some of his money in the team in a last-ditch effort to improve its balance sheet. As it turned out, both Fuchs and Ruppert had known all along that Ruth's non-playing positions were meaningless.
By the end of the first month of the season, Ruth believed he was finished even as a part-time player. As early as May 12, he asked Fuchs to let him retire. Ultimately, Fuchs persuaded Ruth to remain at least until after the Memorial Day doubleheader in Philadelphia. In the interim was a western road trip, at which the rival teams had scheduled days to honor him. In Chicago and St. Louis, Ruth performed poorly, and his batting average sank to .155, with only three home runs. In the first two games in Pittsburgh, Ruth had only one hit, though a long fly caught by Paul Waner probably would have been a home run in any other ballpark besides Forbes Field.
Ruth played in the third game of the Pittsburgh series on May 25, 1935, and added one more tale to his playing legend. Ruth went 4-for-4, including three home runs, though the Braves lost the game 11–7. The last two were off Ruth's old Cubs nemesis, Guy Bush. The final home run, both of the game and of Ruth's career, sailed over the upper deck in right field and out of the ballpark, the first time anyone had hit a fair ball completely out of Forbes Field. Ruth was urged to make this his last game, but he had given his word to Fuchs and played in Cincinnati and Philadelphia. The first game of the doubleheader in Philadelphia—the Braves lost both—was his final major league appearance. On June 2, after an argument with Fuchs, Ruth retired. He finished 1935 with a .181 average—easily his worst as a full-time position player—and the final six of his 714 home runs. The Braves, 10–27 when Ruth left, finished 38–115, at .248 the worst winning percentage in modern National League history. Fuchs gave up control of the Braves before the end of the season, insolvent like his team; the National League took over the franchise at the end of the year.
Retirement.
1935–46.
Although Fuchs had given Ruth his unconditional release, no major league team expressed an interest in hiring him in any capacity. Ruth still hoped to be hired as a manager if he could not play anymore, but only one managerial position, Cleveland, became available between Ruth's retirement and the end of the 1937 season. Asked if he had considered Ruth for the job, Indians owner Alva Bradley replied in the negative. Creamer believed Ruth was unfairly treated in never being given an opportunity to manage a major league club. The author felt there was not a relationship between personal conduct and managerial success, noting that McGraw, Billy Martin, and Bobby Valentine were winners despite character flaws. Team owners and general managers saw Ruth's lifestyle as a reason for denying him a managerial job; Barrow said of him, "How can he manage other men when he can't even manage himself?"
Ruth played much golf and in a few exhibition baseball games, demonstrating a continuing ability to draw large crowds. This was a major factor in his hiring as first base coach by the Dodgers, in 1938. Brooklyn general manager Larry MacPhail made it clear when Ruth was hired that he would not be considered for the manager's job if, as expected, Burleigh Grimes retired at the end of the season. Although much was said about what Ruth could teach the younger players, in practice, his duties were to appear on the field in uniform and encourage base runners—he was not called upon to relay signs. He got along well with everyone except team captain Leo Durocher, who was hired as Grimes' replacement at season's end. Ruth returned to retirement, never again to work in baseball.
On July 4, 1939, Ruth spoke on Lou Gehrig Appreciation Day at Yankee Stadium as members of the 1927 Yankees and a sellout crowd turned out to honor the first baseman, forced into premature retirement by a disease which would kill him in two years. The next week, Ruth went to Cooperstown, New York, for the formal opening of the Baseball Hall of Fame—he had been one of the first five players elected three years previously. As radio broadcasts of baseball became popular, he sought a job in that field, arguing that his celebrity and knowledge of baseball would assure large audiences, but he was made no offers. During World War II, he made many personal appearances to advance the war effort, including his last appearance as a player at Yankee Stadium, in a 1943 exhibition for the Army–Navy Relief Fund. He hit a long fly ball off Walter Johnson; the blast left the field, curving foul, but Ruth circled the bases anyway. In 1946, he made a final effort at a job in baseball, contacting new Yankees boss MacPhail, but was sent a rejection letter.
Cancer and death (1946–48).
As early as the war years, doctors had cautioned Ruth to take better care of his health, and he grudgingly followed their advice, limiting his drinking and not going on a proposed trip to support the troops in the South Pacific. In 1946, Ruth began experiencing severe pain over his left eye, and had difficulty swallowing. In November 1946, he entered French Hospital in New York for tests, which revealed Ruth had an inoperable malignant tumor at the base of his skull and in his neck. His name and fame gave him access to experimental treatments, and he was one of the first cancer patients to receive both drugs and radiation treatment simultaneously. He was discharged from the hospital in February, having lost , and went to Florida to recuperate. He returned to New York and Yankee Stadium after the season started. The new commissioner, Happy Chandler (Judge Landis had died in 1944), proclaimed April 27, 1947, Babe Ruth Day around the major leagues, with the most significant observance to be in the Bronx. A number of teammates and others spoke in honor of Ruth, who briefly addressed the crowd of almost 60,000.
Around this time, developments in chemotherapy offered some hope, and Ruth, who had not been told he had cancer out of his family's fear he might do himself harm, was put on teropterin, a folic acid derivative—he may have been the first human subject. He showed dramatic improvement during the summer of 1947, so much so that his case was presented by his doctors at a scientific meeting, without using his name. He was able to travel around the country, doing promotional work for the Ford Motor Company on American Legion Baseball. He appeared again at another day in his honor at Yankee Stadium in September, but was not well enough to pitch in an old-timers game as he had hoped.
The improvement was only a temporary remission, and by late 1947, Ruth was unable to help with the writing of his autobiography, "The Babe Ruth Story", which was almost entirely ghostwritten. In and out of the hospital in New York, he left for Florida in February 1948, doing what activities he could, and returned to New York after six weeks to appear at a book-signing party. He also went to California to witness the filming of the book.
On June 5, 1948, a "gaunt and hollowed out" Ruth visited Yale University to donate a manuscript of "The Babe Ruth Story" to its library. On June 13, Ruth visited Yankee Stadium for the final time in his life, appearing at the 25th anniversary celebrations of "The House that Ruth Built". By this time he had lost much weight and had difficulty walking. Introduced along with his surviving teammates from 1923, Ruth used a bat as a cane. The photo of Ruth taken from behind, standing near home plate and facing "Ruthville" (right field) became one of baseball's most famous and widely circulated photographs, and won the Pulitzer Prize.
Ruth made one final trip on behalf of American Legion Baseball, then entered Memorial Sloan–Kettering Cancer Center, where he would die. He was never told he had cancer, but before his death, had surmised it. He was able to leave the hospital for a few short trips, including a final visit to Baltimore. On July 26, 1948, Ruth left the hospital to attend the premiere of the film "The Babe Ruth Story". Shortly thereafter, Ruth returned to the hospital for the final time. He was barely able to speak. Ruth's condition gradually became worse; only a few visitors were allowed to see him, one of whom was National League president and future Commissioner of Baseball Ford Frick. "Ruth was so thin it was unbelievable. He had been such a big man and his arms were just skinny little bones, and his face was so haggard", Frick said years later.
Thousands of New Yorkers, including many children, stood vigil outside the hospital in Ruth's final days. On August 16, 1948, at 8:01 p.m., Babe Ruth died in his sleep at the age of 53. Instead of a wake at a funeral home, his casket was taken to Yankee Stadium, where it remained for two days; 77,000 people filed past to pay him tribute. His funeral Mass took place at St. Patrick's Cathedral; a crowd estimated at 75,000 waited outside. Ruth rests with his second wife, Claire, on a hillside in Section 25 at the Gate of Heaven Cemetery in Hawthorne, New York.
Personal life.
Ruth met Helen Woodford, by some accounts, in a coffee shop in Boston where she was a waitress, and they were married on October 17, 1914. They adopted a daughter, Dorothy, in 1921. Ruth and Helen separated around 1925 reportedly due to his repeated infidelities. Their last public appearance together came during the 1926 World Series. Helen died in a fire in Watertown, Massachusetts, in 1929, in a house owned by Edward Kinder, a dentist with whom she had been living as "Mrs. Kinder". In her book, "My Dad, the Babe", Dorothy claimed that she was Ruth's biological child by a girlfriend named Juanita Jennings. She died in 1989.
On April 17, 1929, Ruth married actress and model Claire Merritt Hodgson (1897–1976) and adopted her daughter, Julia. By one account, Julia and Dorothy were, through no fault of their own, the reason for the seven-year rift in Ruth's relationship with teammate Lou Gehrig. Sometime in 1932 Gehrig's mother, during a conversation which she assumed was private, remarked, "It's a shame [Claire] doesn't dress Dorothy as nicely as she dresses her own daughter." When the comment inevitably got back to Ruth, he angrily told Gehrig to tell his mother to mind her own business. Gehrig in turn took offense at what he perceived as Ruth's disrespectful treatment of his mother. The two men reportedly never spoke off the field until they reconciled at Yankee Stadium on Lou Gehrig Appreciation Day in 1939.
Although he was married through most of his baseball career, Ruth stated when Colonel Huston asked him to tone down his lifestyle, "I'll promise to go easier on drinking and to get to bed earlier, but not for you, fifty thousand dollars, or two-hundred and fifty thousand dollars will I give up women. They're too much fun."
Memorial and museum.
On April 19, 1949, the Yankees unveiled a granite monument in Ruth's honor in center field of Yankee Stadium. The monument was located in the field of play next to a flagpole and similar tributes to Huggins and Gehrig until the stadium was remodeled from 1974–1975, which resulted in the outfield fences moving inward and enclosing the monuments from the playing field. This area was known thereafter as Monument Park. Yankee Stadium, "the House that Ruth Built", was replaced after the 2008 season with a new Yankee Stadium across the street from the old one; Monument Park was subsequently moved to the new venue behind the center field fence. Ruth's uniform number 3 has been retired by the Yankees, and he is one of five Yankees players or managers to have a granite monument within the stadium.
The Babe Ruth Birthplace Museum is located at 216 Emory Street, a Baltimore row house where Ruth was born, and three blocks west of Oriole Park at Camden Yards, where the AL's Baltimore Orioles play. The property was restored and opened to the public in 1973 by the non-profit Babe Ruth Birthplace Foundation, Inc. Ruth's widow, Claire, his two daughters, Dorothy and Julia, and his sister, Mamie, helped select and install exhibits for the museum.
Contemporary impact.
Ruth was the first baseball star to be the subject of overwhelming interest by the public. Baseball had seen star players before, such as Cobb and "Shoeless Joe" Jackson, but both men had uneasy relations with fans, in Cobb's case sometimes marked by violence. Ruth's biographers agree that he benefited from the timing of his ascension to "Home Run King", with an America hit hard by both the war and the 1918 flu pandemic longing for something to help put these traumas behind it. He also resonated in a country which felt, in the aftermath of the war, that it took second place to no one. Montville argues that as a larger-than-life figure capable of unprecedented athletic feats in the nation's largest city, Ruth became an icon of the significant social changes which marked the early 1920s. Glenn Stout notes in his history of the Yankees, "Ruth was New York incarnate—uncouth and raw, flamboyant and flashy, oversized, out of scale, and absolutely unstoppable".
Ruth became such a symbol of the United States during his lifetime that during World War II, Japanese soldiers yelled in English "To hell with Babe Ruth" to anger American soldiers. (Ruth replied that he hoped that "every Jap that mention[ed] my name gets shot"). Creamer recorded that "Babe Ruth transcended sport, moved far beyond the artificial limits of baselines and outfield fences and sports pages". Wagenheim stated, "He appealed to a deeply rooted American yearning for the definitive climax: clean, quick, unarguable." According to Glenn Stout, "Ruth's home runs were exalted, uplifting experience that meant more to fans than any runs they were responsible for. A Babe Ruth home run was an event unto itself, one that meant anything was possible."
Ruth's penchant for hitting home runs altered how baseball is played. Prior to 1920, home runs were unusual, and managers tried to win games by building a run by getting a runner on base, and bring him around to score through such means as the stolen base, the bunt, and the hit and run. Advocates of what was dubbed "inside baseball", such as Giants manager McGraw, disliked the home run, considering it a blot on the purity of the game. According to sportswriter W. A. Phelon after the 1920 season, Ruth's breakout performance that season and the response in excitement and attendance, "settled, for all time to come, that the American public is nuttier over the Home Run than the Clever Fielding or the Hitless Pitching. Viva el Home Run and two times viva Babe Ruth, exponent of the home run, and overshadowing star." Bill James notes, "When the owners discovered that the fans "liked" to see home runs, and when the foundations of the games were simultaneously imperiled by disgrace [in the Black Sox Scandal], then there was no turning back." While a few, such as McGraw and Cobb, decried the passing of the old-style play, teams quickly began to seek and develop sluggers.
According to contemporary sportswriter Grantland Rice, only two sports figures of the 1920s approached Ruth in popularity—boxer Jack Dempsey and racehorse Man o' War. One of the factors that caused Ruth to gain his broad appeal was the uncertainty that surrounds his early life and his family. It allowed Ruth to exemplify the American success story, that even an uneducated, unsophisticated youth, without any family wealth or connections, can do something better than anyone else in the world. Montville notes that "the fog [surrounding his childhood] will make him forever accessible, universal. He will be the patron saint of American possibility." Similarly, the fact that Ruth played when a relatively small portion of his fans had the opportunity to see him play, in the era before television coverage of baseball, allowed his legend to grow through word of mouth and the hyperbole of sports reporters. Reisler notes that recent sluggers who surpassed Ruth's 60 home run mark, such as Mark McGwire and Barry Bonds, generated much less excitement than when Ruth repeatedly broke the single-season home run record in the 1920s; Ruth dominated a relatively small sports world, while Americans of the present era have many sports to choose to watch.
Legacy.
Creamer termed Ruth "a unique figure in the social history of the United States". Ruth has even entered the language: a dominant figure in a field, whether within or outside sports, is often referred to as "the Babe Ruth" of that field. Similarly, "Ruthian" has come to mean in sports, "colossal, dramatic, prodigious, magnificent; with great power."
More books, Montville noted in 2006, have been written about Ruth than about any other member of the Baseball Hall of Fame. At least five of these books (including Creamer's and Wagenheim's) were written in 1973 and 1974, timed to capitalize on the increase in public interest in Ruth as Henry Aaron approached his career home run mark, which he broke on April 8, 1974. Aaron stated as he approached Ruth's record, "I can't remember a day this year or last when I did not hear the name of Babe Ruth."
Montville suggests that Ruth is probably even more popular today than he was when his career home run record was broken by Aaron. The longball era that Ruth started continues in baseball, to the delight of the fans. Owners build ballparks to encourage home runs, which are featured on "SportsCenter" and "Baseball Tonight" each evening during the season. The questions of performance enhancing drug use which have dogged recent home run hitters such as McGwire and Bonds do nothing to diminish Ruth's reputation; his overindulgences with beer and hot dogs seem part of a simpler time.
Ruth has been named the greatest baseball player of all time in various surveys and rankings. In 1998, "The Sporting News" ranked him number one on the list of "Baseball's 100 Greatest Players". In 1999, baseball fans named Ruth to the Major League Baseball All-Century Team. He was named baseball's Greatest Player Ever in a ballot commemorating the 100th anniversary of professional baseball, in 1969. The Associated Press reported in 1993 that Muhammad Ali was tied with Babe Ruth as the most recognized athletes in America. In a 1999 ESPN poll, he was ranked as the second-greatest U.S. athlete of the century, behind Michael Jordan. In 1983, the United States Postal Service honored Ruth with the issuance of a twenty-cent stamp.
One long-term survivor of the craze over Ruth may be the Baby Ruth candy bar. Although the original company to market the confectionery, the Curtis Candy Company, maintained that the bar was named after Ruth Cleveland, daughter of former president Grover Cleveland, Ruth Cleveland had died in 1904 and the bar was first marketed in 1921, at the height of the Ruth craze. The slugger later sought to market candy bearing his name; he was refused a patent because of the existence of the Baby Ruth bar. Corporate files from 1921 are no longer extant; the brand has changed hands several times and is now owned by the Nestlé company. The Ruth estate licensed his likeness for use in an advertising campaign for Baby Ruth in 1995. Due to a marketing arrangement, in 2005, the Baby Ruth bar became the official candy bar of Major League Baseball.
Montville notes the continuing relevance of Babe Ruth in American culture, over three-quarters of a century after he last swung a bat in a major league game:
A biopic film, "The Babe", was released in 1992 and starred John Goodman in the title role.
External links.
 
 
 
 

</doc>
<doc id="4177" url="http://en.wikipedia.org/wiki?curid=4177" title="Barge">
Barge

A barge is a flat-bottomed boat, built mainly for river and canal transport of heavy goods. Some barges are not self-propelled and need to be towed or pushed by towboats. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway in the early industrial revolution, but were outcompeted in the carriage of high-value items due to the higher speed, falling costs, and route flexibility of rail.
Etymology.
"Barge" is attested from 1300, from Old French "barge", from Vulgar Latin "barga". The word originally could refer to any small boat; the modern meaning arose around 1480. "Bark" "small ship" is attested from 1420, from Old French "barque", from Vulgar Latin "barca" (400 AD). The more precise meaning "three-masted ship" arose in the 17th century, and often takes the French spelling for disambiguation. Both are probably derived from the Latin "barica", from Greek "baris" "Egyptian boat", from Coptic "bari" "small boat", hieroglyphic Egyptian and similar "ba-y-r" for "basket-shaped boat". By extension, the term "embark" literally means to board the kind of boat called a "barque".
The long pole used to maneuver or propel a barge have given rise to the saying "I wouldn't touch that [subject/thing] with a barge pole."
Types.
On the Great British canal system, the term 'barge' is used to describe a boat wider than a narrowboat, and the people who move barges are often known as lightermen. In the United States, deckhands perform the labor and are supervised by a leadman or the mate. The captain and pilot steer the towboat, which pushes one or more barges held together with rigging, collectively called 'the tow'. The crew live aboard the towboat as it travels along the inland river system or the intracoastal waterways. These towboats travel between ports and are also called line-haul boats.
Poles are used on barges to fend off the barge as it nears other vessels or a wharf. These are often called 'pike poles'. On shallow canals in the United Kingdom, long punt poles are used to manoeuvre or propel the barge.
Modern use.
Barges are used today for low-value bulk items, as the cost of hauling goods by barge is very low. Barges are also used for very heavy or bulky items; a typical barge measures 195 by 35 feet (59.4 m × 10.6 m), and can carry up to about 1500 tons of cargo. 
As an example, on June 26, 2006, a 565-ton catalytic cracking unit reactor was shipped by barge from the Tulsa Port of Catoosa in Oklahoma to a refinery in Pascagoula, Mississippi. Extremely large objects are normally shipped in sections and assembled onsite, but shipping an assembled unit reduced costs and avoided reliance on construction labor at the delivery site (which in this case was still recovering from Hurricane Katrina). Of the reactor's journey, only about 40 miles were traveled overland, from the final port to the refinery.
Self-propelled barges may be used as such when traveling downstream or upstream in placid waters; they are operated as an unpowered barge, with the assistance of a tugboat, when traveling upstream in faster waters. Canal barges are usually made for the particular canal in which they will operate.
Many barges, primarily Dutch Barges, which were originally designed for carrying cargo along the canals of Europe, are no longer large enough to compete in this industry with larger newer vessels. Many of these barges have been renovated and are now used as luxury Hotel Barges carrying holiday makers along the same canals they once carried grain or coal.
Towed or otherwise unpowered barges in the United States.
In primitive regions today and in all pre-development (lacking highways or railways) regions worldwide in times before industrial development and highways, barges were the predominant and most efficient means of inland transportation in many regions. This holds true today, for many areas of the world. 
In such pre-industrialized, or poorly developed infrastructure regions, many barges are purpose-designed to be powered on waterways by long slender poles — thereby becoming known on American waterways as poleboats as the extensive west of North America was settled using the vast tributary river systems of the Mississippi drainage basin. Poleboats use muscle power of "walkers" along the sides of the craft pushing against a pole against the streambed, canal, or lake bottom to move the vessel where desired. In settling the American west it was generally faster to navigate downriver from Brownsville, Pennsylvania, to the Ohio River confluence with the Mississippi and then pole upriver against the current to St. Louis than to travel overland on the rare primitive dirt roads for many decades after the American revolution.
Once the New York Central and Pennsylvania Railroads reached Chicago, that time dynamic changed, and American poleboats became less common, relegated to smaller rivers and more remote streams. On the Mississippi riverine system today, including that of other sheltered waterways, industrial barge trafficking in bulk raw materials such as coal, coke, timber, iron ore and other minerals is extremely common; in the developed world using huge cargo barges that connect in groups and trains-of-barges in ways that allow cargo volumes and weights considerably greater than those used by pioneers of modern barge systems and methods in the Victorian era.
Such barges need to be towed by tugboats or pushed by towboats. Canal barges, towed by draft animals on a waterway adjacent towpath were of fundamental importance in the early industrial revolution, whose major early engineering projects were efforts to build viaducts, aqueducts and especially canals to fuel and feed raw materials to nascent factories in the early industrial takeoff and take their goods to ports and cities for distribution.
The barge and canal system contended favorably with the railways in the early industrial revolution before around the 1850s–1860s — for example, the Erie Canal in New York State is credited by economic historians with giving the growth boost needed for New York City to eclipse Philadelphia as America's largest port and city — but such canal systems with their locks, need for maintenance and dredging, pumps and sanitary issues were eventually outcompeted in the carriage of high-value items by the railways due to the higher speed, falling costs, and route flexibility of rail transport. Barge and canal systems were nonetheless of great, perhaps even primary, economic importance until after World War I in Europe, particularly in the more developed nations of the Low Countries, France, Germany, Poland, and especially Great Britain which more or less made the system characteristically its own.

</doc>
<doc id="4178" url="http://en.wikipedia.org/wiki?curid=4178" title="Bill Schelter">
Bill Schelter

William Frederick Schelter (1947 – July 30, 2001) was a professor of mathematics at The University of Texas at Austin and a Lisp developer and programmer. Schelter is credited with the development of the GNU Common Lisp (gcl) implementation of Common Lisp and the GPL'd version of the computer algebra system Macsyma called Maxima. Schelter authored Austin Kyoto Common Lisp (AKCL) under contract with IBM. AKCL formed the foundation for Axiom, another computer algebra system. AKCL eventually became GNU Common Lisp. He is also credited with the first port of the GNU C compiler to the Intel 386 architecture, used in the original implementation of the Linux kernel .
Schelter obtained his Ph.D. at McGill University in 1972. His mathematical specialties were noncommutative ring theory and computational algebra and its applications, including automated theorem proving in geometry.
In the summer of 2001, age 54, he died suddenly of a heart attack while traveling in Russia.

</doc>
<doc id="4179" url="http://en.wikipedia.org/wiki?curid=4179" title="British English">
British English

British English is a broad term for the English Language as it is spoken and written in the United Kingdom. It may also be used in reference to all of the British Isles.
There are slight regional variations in formal written English in the United Kingdom. For example, although the words "wee" and "little" are interchangeable in some contexts, "wee" (as an adjective) is almost exclusively used by some people from some parts of Scotland and Northern Ireland, whereas in England and Wales, "little" is used predominantly. Nevertheless, there is a meaningful degree of uniformity in "written" English within the United Kingdom, and this could be described by the term "British English". The forms of "spoken" English, however, vary considerably more than in most other areas of the world where English is spoken, so a uniform concept of British English is more difficult to apply to the spoken language. According to Tom McArthur in the "Oxford Guide to World English", "For many people . . . especially in England ["British English"] is tautologous," and it shares "all the ambiguities and tensions in the word "British", and as a result can be used and interpreted in two ways, more broadly or more narrowly, within a range of blurring and ambiguity." The term "British English" is sometimes used as a synonym for "Commonwealth English"; that is, English as spoken and written in the Commonwealth of Nations, with the exception of those Commonwealth countries which have developed their own distinctive dialects such as Australia and Canada.
History.
English is a West Germanic language that originated from the Anglo-Frisian dialects brought to Britain by Germanic settlers from various parts of what is now northwest Germany and the northern Netherlands. The resident population at this time was generally speaking Common Brittonic—the insular variety of continental Celtic which was influenced by the Roman occupation. This group of languages (Welsh, Cornish, Cumbric) cohabited alongside English into the modern period, but due to their remoteness from the Germanic languages, influence on English was notably limited. However, the degree of influence remains debated, and it has recently been argued that its grammatical influence accounts for the substantial innovations noted between English and the other West Germanic languages. Initially, Old English was a diverse group of dialects, reflecting the varied origins of the Anglo-Saxon Kingdoms of England. One of these dialects, Late West Saxon, eventually came to dominate. The original Old English language was then influenced by two waves of invasion: the first was by speakers of the Scandinavian branch of the Germanic family, who conquered and colonised parts of Britain in the 8th and 9th centuries; the second was the Normans in the 11th century, who spoke Old Norman and ultimately developed an English variety of this called Anglo-Norman. These two invasions caused English to become "mixed" to some degree (though it was never a truly mixed language in the strictest sense of the word; mixed languages arise from the cohabitation of speakers of different languages, who develop a hybrid tongue for basic communication).
The more idiomatic, concrete and descriptive English is, the more it is from Anglo-Saxon origins. The more intellectual and abstract English is, the more it contains Latin and French influences (e.g. pig is the animal bred by the occupied Anglo-Saxons and pork is the animal eaten by the occupying Normans).
Cohabitation with the Scandinavians resulted in a significant grammatical simplification and lexical enrichment of the Anglo-Frisian core of English; the later Norman occupation led to the grafting onto that Germanic core of a more elaborate layer of words from the Romance branch of the European languages. This Norman influence entered English largely through the courts and government. Thus, English developed into a "borrowing" language of great flexibility and with a huge vocabulary.
Dialects.
Dialects and accents vary amongst the four countries of the United Kingdom, as well as within the countries themselves.
The major divisions are normally classified as English English (or English as spoken in England, which encompasses Southern English dialects, West Country dialects, East and West Midlands English dialects and Northern English dialects), Welsh English (not to be confused with the Welsh language), Irish English and Scottish English (not to be confused with the Scots language). The various British dialects also differ in the words that they have borrowed from other languages.
Following its last major survey of English Dialects (1949–1950), the University of Leeds has started work on a new project. In May 2007 the Arts and Humanities Research Council awarded a grant to a team led by Sally Johnson, Professor of Linguistics and Phonetics at Leeds University, to study British regional dialects.
Johnson's team are sifting through a large collection of examples of regional slang words and phrases turned up by the "Voices project" run by the BBC, in which they invited the public to send in examples of English still spoken throughout the country. The BBC Voices project also collected hundreds of news articles about how the British speak English from swearing through to items on language schools. This information will also be collated and analysed by Johnson's team both for content and for where it was reported. "Perhaps the most remarkable finding in the Voices study is that the English language is as diverse as ever, despite our increased mobility and constant exposure to other accents and dialects through TV and radio." Work by the team on this project is not expected to end before 2010.
Regional.
The form of English most commonly associated with the upper class in the southern counties of England is called Received Pronunciation (RP). It derives from a mixture of the Midland and Southern dialects which were spoken in London in the early modern period and is frequently used as a model for teaching English to foreign learners. Although speakers from elsewhere in England may not speak with an RP accent, it is now a class dialect more than a local dialect. It may also be referred to as "the Queen's (or King's) English", "Public School English", "Posh" or "BBC English" as this was originally the form of English used on radio and television, although a wider variety of accents can be heard these days. About 2% of Britons speak RP, and it has evolved quite markedly over the last 40 years.
In the South East there are significantly different accents; the Cockney accent spoken by some East Londoners is strikingly different from RP. The Cockney rhyming slang can be (and was initially intended to be) difficult for outsiders to understand, although the extent of its use is often somewhat exaggerated.
Estuary English has been gaining prominence in recent decades: it has some features of RP and some of Cockney. In London itself, the broad local accent is still changing, partly influenced by Caribbean speech. Immigrants to the UK in recent decades have brought many more languages to the country. Surveys started in 1979 by the Inner London Education Authority discovered over 100 languages being spoken domestically by the families of the inner city's schoolchildren. As a result, Londoners speak with a mixture of accents, depending on ethnicity, neighbourhood, class, age, upbringing, and sundry other factors.
Since the mass internal immigration to Northamptonshire in the 1940s and its position between several major accent regions, it has become a source of various accent developments. In Northampton the older accent has been influenced by overspill Londoners. There is an accent known locally as the Kettering accent, which is a transitional accent between the East Midlands and East Anglian. It is the last southern midland accent to use the broad "a" in words like "bath"/"grass" (i.e. barth/grarss). Conversely "crass"/"plastic" use a slender "a". A few miles northwest in Leicestershire the slender "a" becomes more widespread generally. In the town of Corby, five miles (8 km) north, one can find Corbyite, which unlike the Kettering accent, is largely influenced by the West Scottish accent.
In addition, most British people can to some degree temporarily "swing" their accent towards a more neutral form of English at will, to reduce difficulty where very different accents are involved, or when speaking to foreigners.
Glottal stop.
In a number of forms of spoken British English, it is common for the sound /t/ to be replaced by a glottal stop when it is in the intervocalic position, in a process called T-glottalisation. Once regarded as a Cockney feature, it has become much more widespread. It is still stigmatised when used in words like "later", but becoming very widespread at the end of words such as "not" (as in no/ʔ/ interested). Other consonants subject to this usage in Cockney English are "p", as in pa/ʔ/er, "k" as in ba/ʔ/er and "t" as in bu/ʔʔ/er.
Standardisation.
As with English around the world, the English language as used in the United Kingdom is governed by convention rather than formal code: there is no body equivalent to the Académie française or the Real Academia Española, and the authoritative dictionaries (for example, "Oxford English Dictionary", "Longman Dictionary of Contemporary English", "Chambers Dictionary", "Collins Dictionary") record usage rather than attempting to prescribe it. In addition, vocabulary and usage change with time: words are freely borrowed from other languages and other strains of English, and neologisms are frequent.
For historical reasons dating back to the rise of London in the 9th century, the form of language spoken in London and the East Midlands became standard English within the Court, and ultimately became the basis for generally accepted use in the law, government, literature and education in Britain. To a considerable extent, modern British spelling was standardised in Samuel Johnson's "A Dictionary of the English Language" (1755), although previous writers had also played a significant role in this and much has changed since 1755. Scotland, which underwent parliamentary union with England only in 1707, still has a few independent standards, especially within its separate legal system.
Since the early 20th century, numerous books by British authors intended as guides to English grammar and usage have been published, a few of which have achieved sufficient acclaim to have remained in print for long periods and to have been reissued in new editions after some decades. These include, most notably of all, Fowler's "Modern English Usage" and "The Complete Plain Words" by Sir Ernest Gowers. Detailed guidance on many aspects of writing British English for publication is included in style guides issued by various publishers including "The Times" newspaper, the Oxford University Press and the Cambridge University Press. The Oxford University Press guidelines were originally drafted as a single broadsheet page by Horace Henry Hart, and were at the time (1893) the first guide of their type in English; they were gradually expanded and eventually published, first as "Hart's Rules", and in 2002 as part of "The Oxford Manual of Style". Comparable in authority and stature to "The Chicago Manual of Style" for published American English, the Oxford Manual is a fairly exhaustive standard for published British English, to which writers can turn in the absence of any specific guidance issued by their publishing house.
Notes.
Citations

</doc>
<doc id="4181" url="http://en.wikipedia.org/wiki?curid=4181" title="Battle">
Battle

Generally, a battle is a conceptual component in the hierarchy of combat in warfare between two or more armed forces, or combatants. A war sometimes consists of many battles. Battles generally are well defined in duration, area, and force commitment. 
Wars and military campaigns are guided by strategy, whereas battles take place on a level of planning and execution known as operational mobility. German strategist Carl von Clausewitz stated that "the employment of battles ... to achieve the object of war" was the essence of strategy.
Etymology.
The definition of a battle cannot be arrived at solely through the names of historical battles, many of which are misnomers. The word "battle" is a loanword in English from the Old French "bataille", first attested in 1297, from Late Latin "battualia", meaning "exercise of soldiers and gladiators in fighting and fencing", from Late Latin (taken from Germanic) "battuere" "beat", from which the English word "battery" is also derived via Middle English "batri", and comes from the staged battles in the Colosseum in Rome that may have numbered 10,000 individuals.
Characteristics.
The defining characteristic of the fight as a concept in Military science has been a dynamic one through the course of military history, changing with the changes in the organisation, employment and technology of military forces. 
While the English military historian Sir John Keegan suggested an ideal definition of battle as "something which happens between two armies leading to the moral then physical disintegration of one or the other of them", the origins and outcomes of battles can rarely be summarized so neatly.
In general a battle during the 20th century was, and continues to be, defined by the combat between opposing forces representing major components of total forces committed to a military campaign, used to achieve specific military objectives. Where the duration of the battle is longer than a week, it is often for reasons of staff operational planning called an "operation". Battles can be planned, encountered, or forced by one force on the other when the latter is unable to withdraw from combat. 
A battle always has as its purpose the reaching of a mission goal by use of military force. A victory in the battle is achieved when one of the opposing sides forces the other to abandon its mission, or to surrender its forces, or routs the other, i.e., forces it to retreat or renders it militarily ineffective for further combat operations. However, a battle may end in a Pyrrhic victory, which ultimately favors the defeated party. If no resolution is reached in a battle, it can result in a stalemate. A conflict in which one side is unwilling to reach a decision by a direct battle using conventional warfare often becomes an insurgency.
Until the 19th century the majority of battles were of short duration, many lasting a part of a day. (The Battle of Nations (1813) and the Battle of Gettysburg (1863) were exceptional in lasting three days.) This was mainly due to the difficulty of supplying armies in the field, or conducting night operations. The means of prolonging a battle was typically by employment of siege warfare. Improvements in transportation and the sudden evolving of trench warfare, with its siege-like nature during World War I in the 20th century, lengthened the duration of battles to days and weeks. This created the requirement for unit rotation to prevent combat fatigue, with troops preferably not remaining in a combat area of operations for more than a month. Trench warfare had become largely obsolete in conflicts between advanced armies by the start of the Second World War.
The use of the term "battle" in military history has led to its misuse when referring to almost any scale of combat, notably by strategic forces involving hundreds of thousands of troops that may be engaged in either a single battle at one time (Battle of Leipzig) or multiple operations (Battle of Kursk). The space a battle occupies depends on the range of the weapons of the combatants. A "battle" in this broader sense may occupy a large piece of spacetime, as in the case of the Battle of Britain or the Battle of the Atlantic. Until the advent of artillery and aircraft, battles were fought with the two sides within sight, if not reach, of each other. The depth of the battlefield has also increased in modern warfare with inclusion of the supporting units in the rear areas; supply, artillery, medical personnel etc. often outnumber the front-line combat troops.
Battles are, on the whole, made up of a multitude of individual combats, skirmishes and small engagements within the context of which the combatants will usually only experience a small part of the events of the battle's entirety. To the infantryman, there may be little to distinguish between combat as part of a minor raid or as a major offensive, nor is it likely that he anticipates the future course of the battle; few of the British infantry who went over the top on the first day on the Somme, July 1, 1916, would have anticipated that they would be fighting the same battle in five months' time. Conversely, some of the Allied infantry who had just dealt a crushing defeat to the French at the Battle of Waterloo fully expected to have to fight again the next day (at the Battle of Wavre).
Battlespace.
Battlespace is a unified strategy to integrate and combine armed forces for the military theatre of operations, including air, information, land, sea and space. It includes the environment, factors and conditions that must be understood to successfully apply combat power, protect the force, or complete the mission. This includes enemy and friendly armed forces; facilities; weather; terrain; and the electromagnetic spectrum within the operational areas and areas of interest.
Factors.
Battles are decided by various factors. The number and quality of combatants and equipment, the skill of the commanders of each army, and the terrain advantages are among the most prominent factors. A unit may charge with high morale but less discipline and still emerge victorious. This tactic was effectively used by the early French Revolutionary Armies. 
Weapons and armour can be a decisive factor. On many occasions armies have achieved victories largely owing to the employment of more advanced weapons than those of their opponents. An extreme example was in the Battle of Omdurman, in which a large army of Sudanese Mahdists armed in a traditional manner were destroyed by an Anglo-Egyptian force equipped with Maxim guns.
On some occasions, simple weapons employed in an unorthodox fashion have proven advantageous, as with the Swiss pikemen who gained many victories through their ability to transform a traditionally defensive weapon into an offensive one. Likewise, the Zulus in the early 19th century were victorious in battles against their rivals in part because they adopted a new kind of spear, the iklwa. Even so, forces with inferior weapons have still emerged victorious at times, for example in the Wars of Scottish Independence and in the First Italo–Ethiopian War. Discipline within the troops is often of greater importance; at the Battle of Alesia, the Romans were greatly outnumbered but won because of superior training. 
Battles can also be determined by terrain. Capturing high ground, for example, has been the central strategy in innumerable battles. An army that holds the high ground forces the enemy to climb, and thus wear themselves down. Areas of dense vegetation, such as jungles and forest, act as force-multipliers, of benefit to inferior armies. Arguably, terrain is of less importance in modern warfare, due to the advent of aircraft, though terrain is still vital for camouflage, especially for guerrilla warfare. 
Generals and commanders also play a decisive role during combat. Hannibal, Julius Caesar, Khalid ibn Walid and Napoleon Bonaparte were all skilled generals and, consequently, their armies were extremely successful. An army that can trust the commands of their leaders with conviction in its success invariably has a higher morale than an army that doubts its every move. The British in the naval Battle of Trafalgar, for example, owed its success to the reputation of celebrated admiral Lord Nelson.
Types.
Battles can be fought on land, sea and in the modern age, in the air. Naval battles have occurred since before the 5th century BC. Air battles have been far less common, due to their late conception, the most prominent being the Battle of Britain in 1940. However since the Second World War land or sea battles have come to rely on air support. Indeed, during the Battle of Midway, five aircraft carriers were sunk without either fleet coming into direct contact.
There are numerous types of battles:
Battles frequently do not fit one particular type perfectly, and are usually hybrids of different types listed above.
A "decisive battle" is one of particular importance; often by bringing hostilities to an end, such as the Battle of Hastings or the Battle of Hattin, or as a turning point in the fortunes of the belligerents, such as the Battle of Stalingrad. A decisive battle can have political as well as military impact, changing the balance of power or boundaries between countries. The concept of the "decisive battle" became popular with the publication in 1851 of Edward Creasy's "The Fifteen Decisive Battles of the World". British military historians J.F.C. Fuller ("The Decisive Battles of the Western World") and B.H. Liddell Hart ("Decisive Wars of History"), among many others, have written books in the style of Creasy's work.
Land.
There is an obvious difference in the way battles have been fought throughout time. Early battles were probably fought between rival hunting bands as disorganized mobs. However, during the Battle of Megiddo, the first reliably documented battle in the fifteenth century BC, actual discipline was instilled in both armies. However, during the many wars of the Roman Empire, barbarians continued using mob tactics. 
As the Age of Enlightenment dawned, armies began to fight in highly disciplined lines. Each would follow the orders from their officers and fight as a single unit instead of individuals. Each army was successively divided into regiments, battalions, companies, and platoons. These armies would march, line up, and fire in divisions. 
Native Americans, on the other hand, did not fight in lines, utilizing instead guerrilla tactics. American colonists and European forces continued using disciplined lines, continuing into the American Civil War. 
A new style, during World War I, known as trench warfare, developed nearly half a century later. This also led to radio for communication between battalions. Chemical warfare also emerged with the use of poisonous gas during World War I. 
By World War II, the use of the smaller divisions, platoons and companies became much more important as precise operations became vital. Instead of the locked trench warfare of World War I, during World War II, a dynamic network of battles developed where small groups encountered other platoons. As a result, elite squads became much more recognized and distinguishable. 
Maneuver warfare also developed with an astonishing pace with the advent of the tank, replacing the archaic cannons of the Enlightenment Age. Artillery has since gradually replaced the use of frontal troops. Modern battles now continue to resemble those of World War II, though prominent innovations have been added. Indirect combat through the use of aircraft and missiles now constitutes a large portion of wars in place of battles, where battles are now mostly reserved for capturing cities.
Naval.
One significant difference of modern naval battles as opposed to earlier forms of combat is the use of marines, which introduced amphibious warfare. Today, a marine is actually an infantry regiment that sometimes fights solely on land and is no longer tied to the navy. A good example of an old naval battle is the Battle of Salamis. 
Most ancient naval battles were fought by fast ships using the battering ram to sink opposing fleets or steer close enough for boarding in hand-to-hand combat. Troops were often used to storm enemy ships as used by Romans and pirates. This tactic was usually used by civilizations that could not beat the enemy with ranged weaponry. 
Another invention in the late Middle Ages was the use of Greek fire by the Byzantines, which was used to set enemy fleets on fire. Empty demolition ships utilized the tactic to crash into opposing ships and set it afire with an explosion. After the invention of cannons, naval warfare became useful as support units for land warfare. 
During the 19th century, the development of mines led to a new type of naval warfare. The ironclad, first used in the American Civil War, resistant to cannons, soon made the wooden ship obsolete. The invention of military submarines, during World War I, brought naval warfare to both above and below the surface. With the development of military aircraft during World War II, battles were fought in the sky as well as below the ocean. Aircraft carriers have since become the central unit in naval warfare, acting as a mobile base for lethal aircraft.
Aerial.
Although the use of aircraft has for the most part always been used as a supplement to land or naval engagements, since their first major military use in World War I aircraft have increasingly taken on larger roles in warfare. During World War I, the primary use was for reconnaissance, and small-scale bombardment.
Aircraft began becoming much more prominent in the Spanish Civil War and especially World War II. Aircraft design began specializing, primarily into two types: bombers, which carried explosive payloads to bomb land targets or ships; and fighter-interceptors, which were used to either intercept incoming aircraft or to escort and protect bombers (engagements between fighter aircraft were known as dog fights). Some of the more notable aerial battles in this period include the Battle of Britain and the Battle of Midway.
Another important use of aircraft came with the development of the helicopter, which first became heavily used during the Vietnam War, and still continues to be widely used today to transport and augment ground forces.
Today, direct engagements between aircraft are rare - the most modern fighter-interceptors carry much more extensive bombing payloads, and are used to bomb precision land targets, rather than to fight other aircraft. Anti-aircraft batteries are used much more extensively to defend against incoming aircraft than interceptors. Despite this, aircraft today are much more extensively used as the primary tools for both army and navy, as evidenced by the prominent use of helicopters to transport and support troops, the use of aerial bombardment as the "first strike" in many engagements, and the replacement of the battleship with the aircraft carrier as the center of most modern navies.
Naming.
Battles are usually named after some feature of the battlefield geography, such as the name of a town, forest or river, commonly prefixed "Battle of...". Occasionally battles are named after the date on which they took place, such as The Glorious First of June. 
In the Middle Ages it was considered important to settle on a suitable name for a battle which could be used by the chroniclers. For example, after Henry V of England defeated a French army on October 25, 1415, he met with the senior French herald and they agreed to name the battle after the nearby castle and so it was called the Battle of Agincourt. 
In other cases, the sides adopted different names for the same battle, such as the Battle of Gallipoli which is known in Turkey as the Battle of Çanakkale. During the American Civil War, the Union tended to name the battles after the nearest watercourse, such as the Battle of Wilsons Creek and the Battle of Stones River, whereas the Confederates favoured the nearby towns, as in the Battles of Chancellorsville and Murfreesboro. Occasionally both names for the same battle entered the popular culture, such as the First and Second Battle of Bull Run, which are also referred to as the First and Second Battle of Manassas.
Sometimes in desert warfare, there is no nearby town name to use; map coordinates gave the name to the Battle of 73 Easting in the First Gulf War.
Some place names have become synonymous with the battles that took place there, such as the Passchendaele, Pearl Harbor, the Alamo, Thermopylae, or Waterloo. Military operations, many of which result in battle, are given codenames, which are not necessarily meaningful or indicative of the type or the location of the battle. Operation Market Garden and Operation Rolling Thunder are examples of battles known by their military codenames. 
When a battleground is the site of more than one battle in the same conflict, the instances are distinguished by ordinal number, such as the First and Second Battles of Bull Run. An extreme case are the twelve Battles of the Isonzo—First to Twelfth—between Italy and Austria-Hungary during the First World War.
Some battles are named for the convenience of military historians so that periods of combat can be neatly distinguished from one another. Following the First World War, the British Battles Nomenclature Committee was formed to decide on standard names for all battles and subsidiary actions. To the soldiers who did the fighting, the distinction was usually academic; a soldier fighting at Beaumont Hamel on November 13, 1916 was probably unaware he was taking part in what the committee would call the "Battle of the Ancre".
Many combats are too small to merit a name. Terms such as "action", "skirmish", "firefight", "raid" or "offensive patrol" are used to describe small-scale battle-like encounters. These combats often take place within the time and space of a battle and while they may have an objective, they are not necessarily "decisive". Sometimes the soldiers are unable to immediately gauge the significance of the combat; in the aftermath of the Battle of Waterloo, some British officers were in doubt as to whether the day's events merited the title of "battle" or would be passed off as merely an "action".
Effects.
Battles affect the individuals who take part, as well as the political actors. Personal effects of battle range from mild psychological issues to permanent and crippling injuries. Some battle-survivors have nightmares about the conditions they encountered, or abnormal reactions to certain sights or sounds. Some suffer flashbacks. Physical effects of battle can include scars, amputations, lesions, loss of bodily functions, blindness, paralysis — and death.
Battles also affect politics. A decisive battle can cause the losing side to surrender, while a Pyrrhic Victory such as the Battle of Asculum can cause the winning side to reconsider its long-term goals. Battles in civil wars have often decided the fate of monarchs or political factions. Famous examples include the War of the Roses, as well as the Jacobite Uprisings. Battles also affect the commitment of one side or the other to the continuance of a war, for example the Battle of Incheon and the Battle of Hue during the Tet Offensive.

</doc>
<doc id="4182" url="http://en.wikipedia.org/wiki?curid=4182" title="Berry Berenson">
Berry Berenson

Berinthia "Berry" Berenson Perkins (April 14, 1948 – September 11, 2001) was an American photographer, actress, and model. Perkins, who was the wife of actor Anthony Perkins, died in the September 11 attacks as a passenger on American Airlines Flight 11.
Early life.
Berenson was born in Murray Hill, Manhattan. Her father, Robert Lawrence Berenson, was an American career diplomat turned shipping executive; he was of Lithuanian Jewish descent, and his family's original surname was Valvrojenski. Her mother was born Countess Maria-Luisa Yvonne Radha de Wendt de Kerlor, better known as Gogo Schiaparelli, a socialite of Italian, Swiss, French, and Egyptian ancestry.
Her maternal grandmother was the Italian-born fashion designer Elsa Schiaparelli, and her maternal grandfather was Count Wilhelm de Wendt de Kerlor, a Theosophist and psychic medium. Her elder sister, Marisa Berenson, became a well-known model and actress. She also was a great-grandniece of Giovanni Schiaparelli, an Italian astronomer who believed he had discovered the supposed canals of Mars, and a second cousin, once removed, of art expert Bernard Berenson (1865–1959) and his sister Senda Berenson (1868–1954), an athlete and educator who was one of the first two women elected to the Basketball Hall of Fame.
Career.
Following a brief modeling career in the late 1960s, Berenson became a freelance photographer. By 1973, her photographs had been published in "Life", "Glamour", "Vogue" and "Newsweek".
She also appeared in several motion pictures, including "Cat People" with Malcolm McDowell. She starred opposite Anthony Perkins in the 1978 Alan Rudolph film "Remember My Name" and opposite Jeff Bridges in the 1979 film "Winter Kills".
Personal life and death.
On August 9, 1973, in Cape Cod, Massachusetts, Berenson married her "Remember My Name" costar Anthony Perkins. They had two sons: actor-musician Oz Perkins (born February 2, 1974) and folk/rock recording artist Elvis Perkins (born February 9, 1976). They remained married until Perkins' death from AIDS-related complications on September 12, 1992.
Berenson died at age 53 in the September 11 attacks aboard American Airlines Flight 11, one day before the ninth anniversary of Perkins' death. She was returning to her California home following a holiday on Cape Cod.
At the National September 11 Memorial & Museum, Berenson is memorialized at the North Pool, on Panel N-76.

</doc>
<doc id="4183" url="http://en.wikipedia.org/wiki?curid=4183" title="Botany">
Botany

Botany, also called plant science(s) or plant biology, is the science of plant life and a branch of biology. A botanist or plant scientist is a scientist who specializes in this field of study. The term "botany" comes from the Ancient Greek word ("botane") meaning "pasture", "grass", or "fodder"; is in turn derived from ("boskein"), "to feed" or "to graze". Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists study approximately 400,000 species of living organisms of which some 260,000 species are vascular plants and about 248,000 are flowering plants.
Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – edible, medicinal and poisonous plants, making it one of the oldest branches of science. Medieval physic gardens, often attached to monasteries, contained plants of medical importance. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of Carl Linnaeus that remains in use to this day.
In the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.
Modern botany is a broad, multidisciplinary subject with inputs from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which are the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods and textiles, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.
History.
Early botany.
Botany originated as herbalism, the study and use of plants for their medicinal properties. The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient sacred texts from India dating back to before 1100 BC, archaic Avestan writings, and works from China before it was unified in 221 BC.
Modern botany traces its roots back to Ancient Greece, specifically to Theophrastus (c. 371–287 BC), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the "Father of Botany". His major works, "Enquiry into Plants" and "On the Causes of Plants", constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries after they were written.
Another work from Ancient Greece that made an early impact on botany is "De Materia Medica", a five-volume encyclopedia about herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. "De Materia Medica" was widely read for more than 1,500 years. Important contributions from the medieval Muslim world include Ibn Wahshiyya's "Nabatean Agriculture", Abū Ḥanīfa Dīnawarī's (828–896) the "Book of Plants", and Ibn Bassal's "The Classification of Soils". In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.
In mid-16th century, "botanical gardens" were founded in a number of Italian universities – the Padua botanical garden in 1545 is usually considered to be the first which is still in its original location. These gardens continued the practical value of earlier "physic gardens", often associated with monasteries, in which plants were cultivated for medical use. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens and their medical uses demonstrated. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621. Throughout this period, botany remained firmly subordinate to medicine.
German physician Leonhart Fuchs (1501–1566) was one of "the three German fathers of botany", along with theologian Otto Brunfels (1489–1534) and physician Hieronymus Bock (1498–1554) (also called Hieronymus Tragus). Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.
Physician Valerius Cordus (1515–1544) authored a botanically and pharmacologically important herbal "Historia Plantarum" in 1544 and a pharmacopoeia of lasting importance, the "Dispensatorium" in 1546. Naturalist Conrad von Gesner (1516–1565) and herbalist John Gerard (1545–c. 1611) published herbals covering the medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522–1605) was considered the "father of natural history", which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells, a term he coined, in cork, and a short time later in living plant tissue.
Early modern botany.
During the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys. By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753 Carl von Linné (Carl Linnaeus) published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus. For the purposes of identification, Linnaeus's "Systema Sexuale" classified plants into 24 groups according to the number of their male sexual organs. The 24th group, "Cryptogamia", included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.
Increasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus had indicated. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later classification by Bentham and Hooker, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the "Origin of Species" in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.
Botany was greatly stimulated by the appearance of the first "modern" text book, Matthias Schleiden's "", published in English in 1849 as "Principles of Scientific Botany". Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831.
In 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.
Modern botany.
Building upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822–1884), August Weismann (1834–1914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters. The work of Katherine Esau (1898–1997) on plant anatomy is still a major foundation of modern botany. Her books "Plant Anatomy" and "Anatomy of Seed Plants" have been key plant structural biology texts for more than half a century.
The discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunkiær whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology. Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887–1943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.
Particularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues), the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled precise description of the rates of gas exchange between plants and the atmosphere. Innovations in statistical analysis by Ronald Fisher, Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research. The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones. The synthetic auxin 2,4-Dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.
20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown "in vitro" ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.
Modern morphology recognizes a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome. Furthermore, it emphasizes structural dynamics. Modern systematics aims to reflect and discover phylogenetic relationships between plants. Modern Molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species. The theoretical possibility of a practical method for identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.
Scope and importance.
The study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells. As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion. Plants are crucial to the future of human society as they provide food, oxygen, medicine, and products for people, as well as creating and preserving soil.
Historically, all living things were classified as either animals or plants and botany covered the study of all organisms not considered animals. Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.
The strictest definition of "plant" includes only the "land plants" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life, even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte. Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) – including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.
Paleobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.
Among the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.
Human nutrition.
Virtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them. Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level. The modern forms of the major staple foods, such as bananas, plantains, maize and other cereal grasses, and pulses, as well as flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral pants with the most desirable characteristics. Botanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to mankind's ability to feed the world and provide food security for future generations. Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems. Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant–people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany.
Plant biochemistry.
Plant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism. Others make specialized materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.
 box-shadow: 1px 1px 3px rgba(0,0,0,0.2);">
Plants make various photosynthetic pigments, some of which can be seen here through paper chromatography.
Xanthophylls
Chlorophyll "a"
Chlorophyll "b"
Plants and various other groups of photosynthetic eukaryotes collectively known as "algae" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll "a". Chlorophyll "a" (as well as its plant and green algal-specific cousin chlorophyll "b") absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O2) as a by-product.
The light energy captured by chlorophyll "a" is initially in the form of electrons (and later a proton gradient) that's used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesized. Some of the glucose is converted to starch which is stored in the chloroplast. Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.
Unlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesizing all their fatty acids, and most amino acids. The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out. 
Plants synthesize a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan from which the land plant cell wall is constructed.
Vascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibers that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.
The concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved pathways like Crassulacean acid metabolism and the carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common carbon fixation pathway. These biochemical strategies are unique to land plants.
Medicine and materials.
Phytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine).
Plants can synthesise useful coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.
Sugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer, can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel.
Plant ecology.
Plant ecology is the science of the functional relationships between plants and their habitats—the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species. The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.
Plants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilize mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources. They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.
Herbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection, honey bees, bats and other animals pollinate flowers and humans and other animals act as dispersal vectors to spread spores and seeds.
Plants, climate and environmental change.
Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.
Genetics.
Inheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in "Pisum sativum" (peas). What Mendel learned from studying plants has had far reaching benefits outside of botany. Similarly, "jumping genes" were discovered by Barbara McClintock while she was studying maize. Nevertheless there are some distinctive genetic differences between plants and other organisms.
Species boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, "Mentha" × "piperita", a sterile hybrid between "Mentha aquatica" and spearmint, "Mentha spicata". The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids. Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes. This is one of several methods used by plants to promote outcrossing. In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.
Unlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.
Most sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species. Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals. Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.
As in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.
Molecular genetics.
A considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, "Arabidopsis thaliana", a weedy species in the mustard family (Brassicaceae). The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. "Arabidopsis" was the first plant to have its genome sequenced, in 2000. The sequencing of some other relatively small genomes, of rice ("Oryza sativa") and "Brachypodium distachyon", has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.
Model plants such as "Arabidopsis thaliana" are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in plants. The single celled green alga "Chlamydomonas reinhardtii", while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study. A red alga "Cyanidioschyzon merolae" has also been used to study some basic chloroplast functions. Spinach, peas, soybeans and a moss "Physcomitrella patens" are commonly used to study plant cell biology.
"Agrobacterium tumefaciens", a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species. Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.
Epigenetics.
Epigenetics is the study of mitotically and/or meiotically heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence but cause the organism's genes to behave (or "express themselves") differently. One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable, while others are reset in the germ cells.
Epigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilized egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.
Unlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodeling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.
Evolution.
The chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as "blue-green algae") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a cyanobacterial resident.
The algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants. The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.
Nonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved "megaspory" – their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage. Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms. Gymnosperms produce "naked seeds" not fully enclosed in an ovary; modern representatives include conifers, cycads, "Ginkgo", and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary. Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.
Plant physiology.
Plant physiology encompasses all the internal chemical and physical activities of plants associated with life. Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues. Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.
Molecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur. Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulphur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals. Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.
Plant hormones.
Plants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of "Mimosa pudica", the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.
The hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light and gravity, and concluded "It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements". About the same time, the role of auxins (from the Greek auxein, to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went. The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later. This compound mediates the tropic responses of shoots and roots towards light and gravity. The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.
 Cytokinins are a class of plant hormones named for their control of cell division or cytokinesis. The natural cytokinin zeatin was discovered in corn, "Zea mays", and is a derivative of the purine adenine. Zeatin is produced in roots and transported to shoots in the xylem where it promotes cell division, bud development, and the greening of chloroplasts. The gibberelins, such as Gibberelic acid are diterpenes synthesised from acetyl CoA via the mevalonate pathway. They are involved in the promotion of germination and dormancy-breaking in seeds, in regulation of plant height by controlling stem elongation and the control of flowering. Abscisic acid (ABA) occurs in all land plants except liverworts, and is synthesised from carotenoids in the chloroplasts and other plastids. It inhibits cell division, promotes seed maturation, and dormancy, and promotes stomatal closure. It was so named because it was originally thought to control abscission. Ethylene is a gaseous hormone that is produced in all higher plant tissues from methionine. It is now known to be the hormone that stimulates or regulates fruit ripening and abscission, and it, or the synthetic growth regulator ethephon which is rapidly metabolised to produce ethylene, are used on industrial scale to promote ripening of cotton, pineapples and other climacteric crops.
Another class of phytohormones is the jasmonates, first isolated from the oil of "Jasminum grandiflorum" which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.
In addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.
Plant anatomy and morphology.
Plant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form. 
All plants are multicellular eukaryotes, their DNA stored in nuclei. The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin, larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales divide by construction of a phragmoplast as a template for building a cell plate late in cell division.
The bodies of vascular plants including clubmosses, ferns and seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll. Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis. The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.
The root system and the shoot system are interdependent – the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system. Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots. Stolons and tubers are examples of shoots that can grow roots. Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants. In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with "Saintpaulia", or even a single cell – which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.
In vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch, as in sugar beets and carrots.
Stems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering. Leaves gather sunlight and carry out photosynthesis. Large, flat, flexible, green leaves are called foliage leaves. Gymnosperms, such as conifers, cycads, "Ginkgo", and gnetophytes are seed-producing plants with open seeds. Angiosperms are seed-producing plants that produce flowers and have enclosed seeds. Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants. Some plants reproduce sexually, some asexually, and some via both means.
Although reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results. Furthermore, structures can be seen as processes, that is, process combinations.
Systematic botany.
Systematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history. It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carolus Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent – grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.
Kingdom Plantae belongs to Domain Eukarya and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural "genera"); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single world-wide name for each organism. For example, the tiger lily is "Lilium columbianum". "Lilium" is the genus, and "columbianum" the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).
The evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships. As an example, species of "Pereskia" are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an "Echinocactus". However, both "Pereskia" and "Echinocactus" have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.
Judging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history – such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) – and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent.
From the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having "direct access to the genetic basis of evolution." As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below – fungi are more closely related to animals than to plants.
In 1998 the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered. Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants. Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa. Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed.

</doc>
<doc id="4184" url="http://en.wikipedia.org/wiki?curid=4184" title="Bacillus thuringiensis">
Bacillus thuringiensis

Bacillus thuringiensis (or Bt) is a Gram-positive, soil-dwelling bacterium, commonly used as a biological pesticide. "B. thuringiensis" also occurs naturally in the gut of caterpillars of various types of moths and butterflies, as well on leaf surfaces, aquatic environments, animal feces, insect-rich environments, and flour mills and grain-storage facilities.
During sporulation, many Bt strains produce crystal proteins (proteinaceous inclusions), called δ-endotoxins, that have insecticidal action. This has led to their use as insecticides, and more recently to genetically modified crops using Bt genes. Many crystal-producing Bt strains, though, do not have insecticidal properties.
Discovery and mechanism of insecticidal action.
"B. thuringiensis" was first discovered in 1901 by Japanese biologist Ishiwata Shigetane. In 1911, "B. thuringiensis" was rediscovered in Germany by Ernst Berliner, who isolated it as the cause of a disease called "Schlaffsucht" in flour moth caterpillars. In 1976, Robert A. Zakharyan reported the presence of a plasmid in a strain of "B. thuringiensis" and suggested the plasmid's involvement in endospore and crystal formation. "B. thuringiensis" is closely related to "B.cereus", a soil bacterium, and "B.anthracis", the cause of anthrax; the three organisms differ mainly in their plasmids. Like other members of the genus, all three are aerobes capable of producing endospores. Upon sporulation, "B. thuringiensis" forms crystals of proteinaceous insecticidal δ-endotoxins (called crystal proteins or Cry proteins), which are encoded by "cry" genes. In most strains of "B. thuringiensis", the "cry" genes are located on a plasmid ("cry" is not a chromosomal gene in most strains).
Cry toxins have specific activities against insect species of the orders Lepidoptera (moths and butterflies), Diptera (flies and mosquitoes), Coleoptera (beetles), Hymenoptera (wasps, bees, ants and sawflies) and nematodes. Thus, "B. thuringiensis" serves as an important reservoir of Cry toxins for production of biological insecticides and insect-resistant genetically modified crops. When insects ingest toxin crystals, their alkaline digestive tracts denature the insoluble crystals, making them soluble and thus amenable to being cut with proteases found in the insect gut, which liberate the toxin from the crystal. The Cry toxin is then inserted into the insect gut cell membrane, paralyzing the digestive tract and forming a pore. The insect stops eating and starves to death; live Bt bacteria may also colonize the insect which can contribute to death. The midgut bacteria of susceptible larvae may be required for "B. thuringiensis" insecticidal activity.
In 1996 another class of insecticidal proteins in Bt was discovered; the vegetative insecticidal proteins (Vip). Vip proteins do not share sequence homology with Cry proteins, in general do not compete for the same receptors, and some kill different insects than do Cry proteins.
In 2000, a novel functional group of Cry protein, designated parasporin, was discovered from noninsecticidal "B. thuringiensis" isolates. The proteins of parasporin group are defined as "B. thuringiensis" and related bacterial parasporal proteins that are not hemolytic, but capable of preferentially killing cancer cells. As of January 2013, parasporins comprise six subfamilies (PS1 to PS6).
Use of spores and proteins in pest control.
Spores and crystalline insecticidal proteins produced by "B. thuringiensis" have been used to control insect pests since the 1920s and are often applied as liquid sprays. They are now used as specific insecticides under trade names such as DiPel and Thuricide. Because of their specificity, these pesticides are regarded as environmentally friendly, with little or no effect on humans, wildlife, pollinators, and most other beneficial insects, and are used in organic farming; however, the manuals for these products do contain many environmental and human health warnings, and a 2012 European regulatory peer review of five approved strains found, while data exist to support some claims of low toxicity to humans and the environment, the data are insufficient to justify many of these claims.
"Bacillus thuringiensis" serovar "israelensis" is widely used as a larvicide against mosquito larvae, where it is also considered an environmentally friendly method of mosquito control.
New strains of Bt are developed and introduced over time as insects develop resistance to Bt, or the desire occurs to force mutations to modify organism characteristics or to use homologous recombinant genetic engineering to improve crystal size and increase pesticidal activity or broaden the host range of Bt and obtain more effective formulations. Each new strain is given a unique number and registered with the U.S. EPA and allowances may be given for genetic modification depending on "its parental strains, the proposed pesticide use pattern, and the manner and extent to which the organism has been genetically modified". Formulations of Bt that are approved for organic farming in the US are listed at the website of the Organic Materials Review Institute (OMRI) and several university extension websites offer advice on how to use Bt spore or protein preparations in organic farming.
Use of Bt genes in genetic engineering of plants for pest control.
The Belgian company Plant Genetic Systems (now part of Bayer CropScience) was the first company (in 1985) to develop genetically modified crops (tobacco) with insect tolerance by expressing "cry" genes from "B. thuringiensis". The Bt tobacco was never commercialized; tobacco plants are used to test genetic modifications since they are easy to manipulate genetically and are not part of the food supply.
Usage.
In 1995, potato plants producing CRY 3A Bt toxin were approved safe by the Environmental Protection Agency, making it the first human-modified pesticide-producing crop to be approved in the USA, though many plants produce pesticides naturally, including tobacco, coffee plants, cocoa, and black walnut. This was the 'New Leaf' potato, and it was removed from the market in 2001 due to lack of interest. For current crops and their acreage under cultivation, see genetically modified crops.
In 1996, genetically modified maize producing Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.
The Bt genes engineered into crops and approved for release include, singly and stacked: Cry1A.105, CryIAb, CryIF, Cry2Ab, Cry3Bb1, Cry34Ab1, Cry35Ab1, mCry3A, and VIP, and the engineered crops include corn and cotton. Corn genetically modified to produce VIP was first approved in the US in 2010. Monsanto developed a soybean expressing Cry1Ac and the glyphosate-resistance gene for the Brazilian market, which completed the Brazilian regulatory process in 2010.
Insect resistance.
In November 2009, Monsanto scientists found the pink bollworm had become resistant to the first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, "Cry1Ac". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Monsanto immediately responded by introducing a second-generation cotton with multiple Bt proteins, which was rapidly adopted. Bollworm resistance to first-generation Bt cotton was also identified in Australia, China, Spain, and the United States.
Secondary pests.
Several studies have documented surges in "sucking pests" (which are not affected by Bt toxins) within a few years of adoption of Bt cotton. In China, the main problem has been with mirids, which have in some cases "completely eroded all benefits from Bt cotton cultivation”. The increase in sucking pests depended on local temperature and rainfall conditions and increased in half the villages studied. The increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. Another study in five provinces in China found the reduction in pesticide use in Bt cotton cultivars is significantly lower than that reported in research elsewhere, consistent with the hypothesis suggested by recent studies that more pesticide sprayings are needed over time to control emerging secondary pests, such as aphids, spider mites, and lygus bugs.
Similar problems have been reported in India, with both mealy bugs and aphids although a survey of small Indian farms between 2002 and 2008 concluded Bt cotton adoption has led to higher yields and lower pesticide use, decreasing over time.
Controversies.
There are controversies around GMOs on several levels, including whether making them is ethical, whether food produced with them is safe, whether such food should be labeled and if so how, whether agricultural biotech is needed to address world hunger now or in the future, and more specifically to GM crops—intellectual property and market dynamics; environmental effects of GM crops; and GM crops' role in industrial agricultural more generally. There are also issues specific to Bt transgenic crops.
Lepidopteran toxicity.
The most publicised problem associated with Bt crops is the claim that pollen from Bt maize could kill the monarch butterfly. The paper produced a public uproar and demonstrations against Bt maize; however by 2001 several follow-up studies coordinated by the USDA had proven that "the most common types of Bt maize pollen are not toxic to monarch larvae in concentrations the insects would encounter in the fields."
Wild maize genetic mixing.
A study published in "Nature" in 2001 reported Bt-containing maize genes were found in maize in its center of origin, Oaxaca, Mexico. In 2002, paper concluded, "the evidence available is not sufficient to justify the publication of the original paper." A significant controversy happened over the paper and "Nature"s unprecedented notice.
A subsequent large-scale study, in 2005, failed to find any evidence of genetic mixing in Oaxaca. A 2007 study found the "transgenic proteins expressed in maize were found in two (0.96%) of 208 samples from farmers' fields, located in two (8%) of 25 sampled communities." Mexico imports a substantial amount of maize from the US, and due to formal and informal seed networks among rural farmers, many potential routes are available for transgenic maize to enter into food and feed webs. One study found small-scale (about 1%) introduction of transgenic sequences in sampled fields in Mexico; it did not find evidence for or against this introduced genetic material being inherited by the next generation of plants. That study was immediately criticized, with the reviewer writing, "Genetically, any given plant should be either nontransgenic or transgenic, therefore for leaf tissue of a single transgenic plant, a GMO level close to 100% is expected. In their study, the authors chose to classify leaf samples as transgenic despite GMO levels of about 0.1%. We contend that results such as these are incorrectly interpreted as positive and are more likely to be indicative of contamination in the laboratory."
Colony collapse disorder.
As of 2007, a new phenomenon called colony collapse disorder (CCD) began affecting bee hives all over North America. Initial speculation on possible causes ranged from new parasites to pesticide use to the use of Bt transgenic crops. The Mid-Atlantic Apiculture Research and Extension Consortium found no evidence that pollen from Bt crops is adversely affecting bees. According to the USDA, "Genetically modified (GM) crops, most commonly Bt corn, have been offered up as the cause of CCD. But there is no correlation between where GM crops are planted and the pattern of CCD incidents. Also, GM crops have been widely planted since the late 1990s, but CCD did not appear until 2006. In addition, CCD has been reported in countries that do not allow GM crops to be planted, such as Switzerland. German researchers have noted in one study a possible correlation between exposure to Bt pollen and compromised immunity to Nosema." The actual cause of CCD was unknown in 2007, and scientists believe it may have multiple exacerbating causes.
Beta-exotoxins.
Some isolates of "B. thuringiensis" produce a class of insecticidal small molecules called beta-exotoxin, the common name for which is thuringiensin. A consensus document produced by the OECD says: "Beta-exotoxin and the other "Bacillus" toxins may contribute to the insecticidal toxicity of the bacterium to lepidopteran, dipteran, and coleopteran insects. Beta-exotoxin is known to be toxic to humans and almost all other forms of life and its presence is prohibited in" B. thuringiensis" microbial products. Engineering of plants to contain and express only the genes for δ-endotoxins avoids the problem of assessing the risks posed by these other toxins that may be produced in microbial preparations."

</doc>
<doc id="4185" url="http://en.wikipedia.org/wiki?curid=4185" title="Bacteriophage">
Bacteriophage

A bacteriophage (informally, "phage" ) is a virus that infects and replicates within a bacterium. The term is derived from 'bacteria' and the Greek φαγεῖν "phagein" "to devour". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have relatively simple or elaborate structures. Their genomes may encode as few as four genes, and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm. Bacteriophages are among the most common and diverse entities in the biosphere.
Phages are widely distributed in locations populated by bacterial hosts, such as soil or the intestines of animals. One of the densest natural sources for phages and other viruses is sea water, where up to 9×108 virions per milliliter have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.
They have been used for over 90 years as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria.
Classification.
Bacteriophages occur abundantly in the biosphere, with different virions, genomes and lifestyles. Phages are classified by the International Committee on Taxonomy of Viruses (ICTV) according to morphology and nucleic acid.
Nineteen families are currently recognised that infect bacteria and archaea. Of these, only two families have RNA genomes and only five families are enveloped. Of the viral families with DNA genomes, only two have single-stranded genomes. Eight of the viral families with DNA genomes have circular genomes, while nine have linear genomes. Nine families infect bacteria only, nine infect archaea only, and one ("Tectiviridae") infects both bacteria and archaea.
History.
Since ancient times, reports of river waters having the ability to cure infectious diseases, such as leprosy, have been documented. In 1896, Ernest Hanbury Hankin reported that something in the waters of the Ganges and Yamuna rivers in India had marked antibacterial action against cholera and could pass through a very fine porcelain filter. In 1915, British bacteriologist Frederick Twort, superintendent of the Brown Institution of London, discovered a small agent that infected and killed bacteria. He believed the agent must be one of the following:
Twort's work was interrupted by the onset of World War I and shortage of funding. Independently, French-Canadian microbiologist Félix d'Hérelle, working at the Pasteur Institute in Paris, announced on 3 September 1917, that he had discovered "an invisible, antagonistic microbe of the dysentery bacillus". For d’Hérelle, there was no question as to the nature of his discovery: "In a flash I had understood: what caused my clear spots was in fact an invisible microbe ... a virus parasitic on bacteria." D'Hérelle called the virus a bacteriophage or bacteria-eater (from the Greek "phagein" meaning to eat). He also recorded a dramatic account of a man suffering from dysentery who was restored to good health by the bacteriophages. It was D'Herelle who conducted much research into bacteriophages and introduced the concept of phage therapy.
In 1969, Max Delbrück, Alfred Hershey and Salvador Luria were awarded the Nobel Prize in Physiology and Medicine for their discoveries of the replication of viruses and their genetic structure.
Phage therapy.
Phages were discovered to be antibacterial agents and were used in Georgia and the United States during the 1920s and 1930s for treating bacterial infections. They had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reasons:
Their use has continued since the end of the Cold War in Georgia and elsewhere in Central and Eastern Europe. Globalyz Biotech is an international joint venture that commercializes bacteriophage treatment and its various applications across the globe. The company has successfully used bacteriophages in administering Phage therapy to patients suffering from bacterial infections, including: Staphylococcus (including MRSA), Streptococcus, Pseudomonas, Salmonella, skin and soft tissue, gastrointestinal, respiratory, and orthopedic infections. In 1923, the Eliava Institute was opened in Tbilisi, Georgia, to research this new science and put it into practice.
The first regulated randomized, double blind clinical trial was reported in the Journal of Wound Care in June 2009, which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous leg ulcers in human patients. The study was approved by the FDA as a Phase I clinical trial. Study results satisfactorily demonstrated safety of therapeutic application of bacteriophages, however it did not show efficacy. The authors explain that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin, silver) may have interfered with bacteriophage viability. Another regulated clinical trial in Western Europe (treatment of ear infections caused by "Pseudomonas aeruginosa") was reported shortly after in the journal Clinical Otolaryngology in August 2009.[14] The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis associated lung infections, among others. Meanwhile, Western scientists are developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes which degrade the biofilm matrix, phage structural proteins and also enzymes responsible for lysis of bacterial cell wall.
The water within some rivers traditionally thought to have healing powers, including India's Ganges River, may provide sources of naturally-occurring viral candidates for phage therapy.
Replication.
Bacteriophages may have a lytic cycle or a lysogenic cycle, and a few viruses are capable of carrying out both. With "lytic phages" such as the T4 phage, bacterial cells are broken open (lysed) and destroyed after immediate replication of the virion. As soon as the cell is destroyed, the phage progeny can find new hosts to infect. Lytic phages are more suitable for phage therapy. Some lytic phages undergo a phenomenon known as lysis inhibition, where completed phage progeny will not immediately lyse out of the cell if extracellular phage concentrations are high. This mechanism is not identical to that of temperate phage going dormant and is usually temporary.
In contrast, the "lysogenic cycle" does not result in immediate lysing of the host cell. Those phages able to undergo lysogeny are known as temperate phages. Their viral genome will integrate with host DNA and replicate along with it fairly harmlessly, or may even become established as a plasmid. The virus remains dormant until host conditions deteriorate, perhaps due to depletion of nutrients; then, the endogenous phages (known as prophages) become active. At this point they initiate the reproductive cycle, resulting in lysis of the host cell. As the lysogenic cycle allows the host cell to continue to survive and reproduce, the virus is reproduced in all of the cell’s offspring.
An example of a bacteriophage known to follow the lysogenic cycle and the lytic cycle is the phage lambda of "E. coli."
Sometimes prophages may provide benefits to the host bacterium while they are dormant by adding new functions to the bacterial genome in a phenomenon called lysogenic conversion. Examples are the conversion of harmless strains of "Corynebacterium diphtheriae" or "Vibrio cholerae" by bacteriophages to highly virulent ones, which cause Diphtheria or cholera, respectively. Strategies to combat certain bacterial infections by targeting these toxin-encoding prophages have been proposed.
Attachment and penetration.
To enter a host cell, bacteriophages attach to specific receptors on the surface of bacteria, including lipopolysaccharides, teichoic acids, proteins, or even flagella. This specificity means a bacteriophage can infect only certain bacteria bearing receptors to which they can bind, which in turn determines the phage's host range. Host growth conditions also influence the ability of the phage to attach and invade them. As phage virions do not move independently, they must rely on random encounters with the right receptors when in solution (blood, lymphatic circulation, irrigation, soil water, etc.).
Myovirus bacteriophages use a hypodermic syringe-like motion to inject their genetic material into the cell. After making contact with the appropriate receptor, the tail fibers flex to bring the base plate closer to the surface of the cell; this is known as reversible binding. Once attached completely, irreversible binding is initiated and the tail contracts, possibly with the help of ATP present in the tail, injecting genetic material through the bacterial membrane.
Podoviruses lack an elongated tail sheath similar to that of a myovirus, so they instead use their small, tooth-like tail fibers to enzymatically degrade a portion of the cell membrane before inserting their genetic material.
Synthesis of proteins and nucleic acid.
Within minutes, bacterial ribosomes start translating viral mRNA into protein. For RNA-based phages, RNA replicase is synthesized early in the process. Proteins modify the bacterial RNA polymerase so it preferentially transcribes viral mRNA. The host’s normal synthesis of proteins and nucleic acids is disrupted, and it is forced to manufacture viral products, instead. These products go on to become part of new virions within the cell, helper proteins that help assemble the new virions, or proteins involved in cell lysis. Walter Fiers (University of Ghent, Belgium) was the first to establish the complete nucleotide sequence of a gene (1972) and of the viral genome of bacteriophage MS2 (1976).
Virion assembly.
In the case of the T4 phage, the construction of new virus particles involves the assistance of helper proteins. The base plates are assembled first, with the tails being built upon them afterwards. The head capsids, constructed separately, will spontaneously assemble with the tails. The DNA is packed efficiently within the heads. The whole process takes about 15 minutes.
Release of virions.
Phages may be released via cell lysis, by extrusion, or, in a few cases, by budding. Lysis, by tailed phages, is achieved by an enzyme called endolysin, which attacks and breaks down the cell wall peptidoglycan. An altogether different phage type, the filamentous phages, make the host cell continually secrete new virus particles. Released virions are described as free, and, unless defective, are capable of infecting a new bacterium. Budding is associated with certain "Mycoplasma" phages. In contrast to virion release, phages displaying a lysogenic cycle do not kill the host but, rather, become long-term residents as prophage.
Genome structure.
Bacteriophage genomes are especially mosaic: the genome of any one phage species appears to be composed of numerous individual modules. These modules may be found in other phage species in different arrangements. Mycobacteriophages - bacteriophages with mycobacterial hosts - have provided excellent examples of this mosaicism. In these mycobacteriophages, genetic assortment may be the result of repeated instances of site-specific recombination and illegitimate recombination (the result of phage genome acquisition of bacterial host genetic sequences). It should be noted, however, that evolutionary mechanisms shaping the genomes of bacterial viruses vary between different families and depend on the type of the nucleic acid, characteristics of the virion structure, as well as the mode of the viral life cycle.
In the environment.
Metagenomics has allowed the in-water detection of bacteriophages that was not possible previously.
Bacteriophages have also been used in hydrological tracing and modelling in river systems, especially where surface water and groundwater interactions occur. The use of phages is preferred to the more conventional dye marker because they are significantly less absorbed when passing through ground waters and they are readily detected at very low concentrations.
Other areas of use.
Since 2006, the United States Food and Drug Administration (FDA) and USDA have approved several bacteriophage products. Intralytix introduced LMP-102, also called ListShield as a food additive to target and kill "Listeria monocytogenes". LMP-102 (ListShield by Intralytix) was approved for treating ready-to-eat (RTE) poultry and meat products. In that same year, the FDA approved LISTEX by Micreos (formerly EBI Food Safety) using bacteriophages on cheese to kill the "L. monocytogenes" bacteria, giving them generally recognized as safe (GRAS) status. In July 2007, the same bacteriophage were approved for use on all food products. In 2011 USDA confirmed that LISTEX is a clean label processing-aid and is included in USDA. Research in the field of food safety is continuing to see if lytic phages are a viable option to control other food-borne pathogens in various food products.
Government agencies in the West have for several years been looking to Georgia and the former Soviet Union for help with exploiting phages for counteracting bioweapons and toxins, such as anthrax and botulism. Developments are continuing among research groups in the US. Other uses include spray application in horticulture for protecting plants and vegetable produce from decay and the spread of bacterial disease. Other applications for bacteriophages are as biocides for environmental surfaces, e.g., in hospitals, and as preventative treatments for catheters and medical devices prior to use in clinical settings. The technology for phages to be applied to dry surfaces, e.g., uniforms, curtains, or even sutures for surgery now exists. Clinical trials reported in the "Lancet" show success in veterinary treatment of pet dogs with otitis.
Phage display is a different use of phages involving a library of phages with a variable peptide linked to a surface protein. Each phage's genome encodes the variant of the protein displayed on its surface (hence the name), providing a link between the peptide variant and its encoding gene. Variant phages from the library can be selected through their binding affinity to an immobilized molecule (e.g., botulism toxin) to neutralize it. The bound, selected phages can be multiplied by reinfecting a susceptible bacterial strain, thus allowing them to retrieve the peptides encoded in them for further study.
The SEPTIC bacterium sensing and identification method uses the ion emission and its dynamics during phage infection and offers high specificity and speed for detection.
Phage-ligand technology makes use of proteins, which are identified from bacteriophages, characterized and recombinantly expressed for various applications such as binding of bacteria and bacterial components (e.g. endotoxin) and lysis of bacteria.
Bacteriophages are also important model organisms for studying principles of evolution and ecology.
Model bacteriophages.
The following bacteriophages are extensively studied:

</doc>
<doc id="4187" url="http://en.wikipedia.org/wiki?curid=4187" title="Bactericide">
Bactericide

A bactericide or bacteriocide, sometimes abbreviated Bcidal, is a substance that kills bacteria. Bactericides are disinfectants, antiseptics, or antibiotics.
Bactericidal disinfectants.
The most used disinfectants are those applying
Bactericidal antiseptics.
As antiseptics (i.e., germicide agents that can be used on human or animal body, skin, mucoses, wounds and the like), few of the above-mentioned disinfectants can be used, under proper conditions (mainly concentration, pH, temperature and toxicity toward humans and animals). Among them, some important are
Others are generally not applicable as safe antiseptics, either because of their corrosive or toxic nature.
Bactericidal antibiotics.
Bactericidal antibiotics kill bacteria; bacteriostatic antibiotics slow their growth or reproduction.
Antibiotics that inhibit cell wall synthesis: the Beta-lactam antibiotics (penicillin derivatives (penams), cephalosporins (cephems), monobactams, and carbapenems) and vancomycin.
Also bactericidal are daptomycin, fluoroquinolones, metronidazole, nitrofurantoin, co-trimoxazole, telithromycin.
Aminoglycosidic antibiotics are usually considered bactericidal, although they may be bacteriostatic with some organisms

</doc>
<doc id="4188" url="http://en.wikipedia.org/wiki?curid=4188" title="Brion Gysin">
Brion Gysin

Brion Gysin (19 January 1916 – 13 July 1986) was a painter, writer, sound poet, and performance artist born in Taplow, Buckinghamshire.
He is best known for his discovery of the cut-up technique, used by his friend, the novelist William S. Burroughs. With the engineer Ian Sommerville he invented the Dreamachine, a flicker device designed as an art object to be viewed with the eyes closed. It was in painting and drawing, however, that Gysin devoted his greatest efforts, creating calligraphic works inspired by the cursive Japanese "grass" script and Arabic script. Burroughs later stated that "Brion Gysin was the only man I ever respected."
Biography.
Early years.
John Clifford Brian Gysin was born at Taplow House, England, a Canadian military hospital. His mother, Stella Margaret Martin, was a Canadian from Deseronto, Ontario. His father, Leonard Gysin, a captain with the Canadian Expeditionary Force, was killed in action eight months after his son's birth. Stella returned to Canada and settled in Edmonton, Alberta where her son became "the only Catholic day-boy at an Anglican boarding school". Graduating at fifteen, Gysin was sent to Downside School in Stratton-on-the-Fosse, near Bath, Somerset in England, a prestigious college run by the Benedictines and known as "the Eton of Catholic public schools". Despite attending a Catholic school, Gysin became an atheist.
Surrealism.
In 1934, he moved to Paris to study "La Civilisation Française", an open course given at the Sorbonne where he made literary and artistic contacts through Marie Berthe Aurenche, Max Ernst's second wife. He joined the Surrealist Group and began frequenting Valentine Hugo, Leonor Fini, Salvador Dalí, Picasso and Dora Maar. A year later, he had his first exhibition at the "Galerie Quatre Chemins" in Paris with Ernst, Picasso, Hans Arp, Hans Bellmer, Victor Brauner, Giorgio de Chirico, Dalí, Marcel Duchamp, René Magritte, Man Ray and Yves Tanguy. On the day of the preview, however, he was expelled from the Surrealist Group by André Breton, who ordered the poet Paul Éluard to take down his pictures. Gysin was 19 years old. His biographer, John Geiger, suggests the arbitrary expulsion "had the effect of a curse. Years later, he blamed other failures on the Breton incident. It gave rise to conspiracy theories about the powerful interests who seek control of the art world. He gave various explanations for the expulsion, the more elaborate involving 'insubordination' or "lèse majesté" towards Breton".
After World War II.
After serving in the U.S. army during World War II, Gysin published a biography of Josiah "Uncle Tom" Henson titled, "To Master, a Long Goodnight: The History of Slavery in Canada" (1946). A gifted draughtsman, he took an 18-month course learning the Japanese language (including calligraphy) that would greatly influence his artwork. In 1949, he was among the first Fulbright Fellows. His goal: to research the history of slavery at the University of Bordeaux and in the Archivo de Indias in Seville, Spain, a project that he later abandoned. He moved to Tangier, Morocco after visiting the city with novelist and composer Paul Bowles in 1950.
Morocco and the Beat Hotel.
In 1954 in Tangier, Gysin opened a restaurant called The 1001 Nights, with his friend Mohamed Hamri, who was the cook. Gysin hired the Master Musicians of Jajouka from the village of Jajouka to perform alongside entertainment that included acrobats, a dancing boy and fire eaters. The musicians performed there for an international clientele that included William S. Burroughs. Gysin lost the business in 1958, and the restaurant closed permanently. That same year, Gysin returned to Paris, taking lodgings in a flophouse located at 9 rue Gît-le-Coeur that would become famous as the Beat Hotel. Working on a drawing, he discovered a Dada technique by accident:
William Burroughs and I first went into techniques of writing, together, back in room No. 15 of the Beat Hotel during the cold Paris spring of 1958... Burroughs was more intent on Scotch-taping his photos together into one great continuum on the wall, where scenes faded and slipped into one another, than occupied with editing the monster manuscript... "Naked Lunch" appeared and Burroughs disappeared. He kicked his habit with apomorphine and flew off to London to see Dr Dent, who had first turned him on to the cure. While cutting a mount for a drawing in room No. 15, I sliced through a pile of newspapers with my Stanley blade and thought of what I had said to Burroughs some six months earlier about the necessity for turning painters' techniques directly into writing. I picked up the raw words and began to piece together texts that later appeared as "First Cut-Ups" in "Minutes to Go" (Two Cities, Paris 1960).
When Burroughs returned from London in September 1959, Gysin not only shared his discovery with his friend but the new techniques he had developed for it. Burroughs then put the techniques to use while completing "Naked Lunch" and the experiment dramatically changed the landscape of American literature. Gysin helped Burroughs with the editing of several of his novels including "Interzone", and wrote a script for a film version of "Naked Lunch", which was never produced. The pair collaborated on a large manuscript for Grove Press titled "The Third Mind" but it was determined that it would be impractical to publish it as originally envisioned. The book later published under that title incorporates little of this material. Interviewed for "The Guardian" in 1997, Burroughs explained that Gysin was "the only man that I've ever respected in my life. I've admired people, I've liked them, but he's the only man I've ever respected." In 1969, Gysin completed his finest novel, "The Process", a work judged by critic Robert Palmer as "a classic of 20th century modernism".
A consummate innovator, Gysin altered the cut-up technique to produce what he called permutation poems in which a single phrase was repeated several times with the words rearranged in a different order with each reiteration. An example of this is "I don't dig work, man/Man, work I don't dig." Many of these permutations were derived using a random sequence generator in an early computer program written by Ian Sommerville. Commissioned by the BBC in 1960 to produce material for broadcast, Gysin's results included "Pistol Poem", which was created by recording a gun firing at different distances and then splicing the sounds. That year, the piece was subsequently used as a theme for the Paris performance of Le Domaine Poetique, a showcase for experimental works by people like Gysin, François Dufrêne, Bernard Heidsieck, and Henri Chopin.
With Sommerville, he built the Dreamachine in 1961. Described as "the first art object to be seen with the eyes closed", the flicker device uses alpha waves in the 8-16 Hz range to produce a change of consciousness in receptive viewers.
Later years.
He also worked extensively with noted jazz soprano saxophonist Steve Lacy.
He recorded an album in 1986 with French musician Ramuntcho Matta, featuring himself singing/rapping his own texts, with performances by Don Cherry, Elli Medeiros, Steve Lacy, Lizzy Mercier Descloux and more. The album was reissued on CD in 1993 by Crammed Discs, under the title "Self-Portrait Jumping".
As a joke, Gysin contributed a recipe for marijuana fudge to a cookbook by Alice B. Toklas; it was unintentionally included for publication, becoming famous under the name Alice B. Toklas brownies.
A heavily edited version of his novel, "The Last Museum", was published posthumously in 1986 by Faber & Faber (London) and by Grove Press (New York).
Made an American Commander of the French Ordre des Arts et des Lettres in 1985, Gysin died of lung cancer a year later, on July 13, 1986. An obituary by Robert Palmer published in "The New York Times" fittingly described him as a man who "threw off the sort of ideas that ordinary artists would parlay into a lifetime career, great clumps of ideas, as casually as a locomotive throws off sparks".
Burroughs on the Gysin cut-up.
In a 1966 interview by Conrad Knickerbocker for The Paris Review, William S. Burroughs explained that Brion Gysin was, to his knowledge, "the first to create cut-ups".
INTERVIEWER: How did you become interested in the cut-up technique?
BURROUGHS: A friend, Brion Gysin, an American poet and painter, who has lived in Europe for thirty years, was, as far as I know, the first to create cut-ups. His cut-up poem, "Minutes to Go", was broadcast by the BBC and later published in a pamphlet. I was in Paris in the summer of 1960; this was after the publication there of "Naked Lunch". I became interested in the possibilities of this technique, and I began experimenting myself. Of course, when you think of it, "The Waste Land" was the first great cut-up collage, and Tristan Tzara had done a bit along the same lines. Dos Passos used the same idea in 'The Camera Eye' sequences in "USA". I felt I had been working toward the same goal; thus it was a major revelation to me when I actually saw it being done.
Influence.
According to José Férez Kuri, author of "Brion Gysin: Tuning in to the Multimedia Age" (2003) and co-curator of a major retrospective of the artist's work at The Edmonton Art Gallery in 1998, Gysin's wide range of "radical ideas would become a source of inspiration for artists of the Beat Generation, as well as for their successors (among them David Bowie, Mick Jagger, Keith Haring, and Laurie Anderson)". Other artists include Genesis P-Orridge, John Zorn (as displayed on the 2013's Dreamachines album) and Brian Jones.
Selected bibliography.
Gysin is the subject of John Geiger's biography, "Nothing Is True Everything Is Permitted: The Life of Brion Gysin", and features in "Chapel of Extreme Experience: A Short History of Stroboscopic Light and the Dream Machine", also by Geiger. "Man From Nowhere: Storming the Citadels of Enlightenment with William Burroughs and Brion Gysin", a biographical study of Burroughs and Gysin with a collection of homages to Gysin, was authored by Joe Ambrose, Frank Rynne, and Terry Wilson with contributions by Marianne Faithfull, John Cale, William S. Burroughs, John Giorno, Stanley Booth, Bill Laswell, Mohamed Hamri, Keith Haring and Paul Bowles. A monograph on Gysin was published in 2003 by Thames and Hudson.
Works.
Prose
Radio
Cinema
Music
Painting

</doc>
<doc id="4190" url="http://en.wikipedia.org/wiki?curid=4190" title="Bulgarian">
Bulgarian

Bulgarian refers to anything of or relating to Bulgaria and may refer directly to:
or 

</doc>
<doc id="4191" url="http://en.wikipedia.org/wiki?curid=4191" title="BCG vaccine">
BCG vaccine

Bacillus Calmette–Guérin (historically Vaccin Bilié de Calmette et Guérin commonly referred to as Bacille de Calmette et Guérin or BCG) is a vaccine against tuberculosis that is prepared from a strain of the attenuated (virulence-reduced) live bovine tuberculosis bacillus, "Mycobacterium bovis", that has lost its virulence in humans by being specially subcultured in a culture medium, usually Middlebrook 7H9. Because the living bacilli evolve to make the best use of available nutrients, they become less well-adapted to human blood and can no longer induce disease when introduced into a human host. Still, they are similar enough to their wild ancestors to provide some degree of immunity against human tuberculosis. The BCG vaccine can be anywhere from 0 to 80% effective in preventing tuberculosis for a duration of 15 years; however, its protective effect appears to vary according to geography and the lab in which the vaccine strain was grown.
It is on the World Health Organization's List of Essential Medicines, a list of the most important medication needed in a basic health system.
Medical uses.
The main use of BCG is for vaccination against tuberculosis. BCG vaccine can be implemented after the birth intradermally. BCG vaccination is recommended to be given intradermally by a nurse skilled in the technique. A previous BCG vaccination can cause a false positive Mantoux test, although a very high-grade reading is usually due to active disease.
The age of the patient and the frequency with which BCG is given has always varied from country to country.
Method of administration.
Except in neonates, a tuberculin skin test should always be done before administering BCG. A reactive tuberculin skin test is a contraindication to BCG. Someone with a positive tuberculin reaction is not given BCG, because the risk of severe local inflammation and scarring is high, not because of the common misconception that tuberculin reactors "are already immune" and therefore do not need BCG. People found to have reactive tuberculin skin tests should be screened for active tuberculosis.
BCG is given as a single intradermal injection at the insertion of the deltoid. If BCG is accidentally given subcutaneously, then a local abscess may form (a "BCG-oma") that can sometimes ulcerate, and may require treatment with antibiotics immediately, otherwise without treatment it could spread the infection causing severe damage to vital organs. However, it is important to note an abscess is not always associated with incorrect administration, and it is one of the more common complications that can occur with the vaccination. Numerous medical studies on treatment of these abscesses with antibiotics have been done with varying results, but the consensus is once pus is aspirated and analysed, provided no unusual bacilli are present, the abscess will generally heal on its own in a matter of weeks.
The characteristic raised scar BCG immunization leaves is often used as proof of prior immunization. This scar must be distinguished from that of small pox vaccination, which it may resemble.
Efficacy.
The most controversial aspect of BCG is the variable efficacy found in different clinical trials, which appears to depend on geography. Trials conducted in the UK have consistently shown a protective effect of 60 to 80%, but those conducted elsewhere have shown no protective effect, and efficacy appears to fall the closer one gets to the equator.
A 1994 systematic review found that the BCG reduces the risk of getting TB by about 50%. There are differences in effectiveness, depending on region due to factors such as genetic differences in the populations, changes in environment, exposure to other bacterial infections, and conditions in the lab where the vaccine is grown, including genetic differences between the strains being cultured and the choice of growth medium.
However, a systematic review and meta analysis conducted in 2014 demonstrated that the BCG vaccine reduced infection by 19-27% and reduced progression to active TB by 71%. The studies included in this review were limited to those that used Interferon gamma release assay.
The duration of protection of BCG is not clearly known. In those studies showing a protective effect, the data are inconsistent. The MRC study showed protection waned to 59% after 15 years and to zero after 20 years; however, a study looking at native Americans immunized in the 1930s found evidence of protection even 60 years after immunization, with only a slight waning in efficacy.
BCG seems to have its greatest effect in preventing miliary TB or TB meningitis, so it is still extensively used even in countries where efficacy against pulmonary tuberculosis is negligible.
Reasons.
A number of possible reasons for the variable efficacy of BCG in different countries have been proposed, but none have been proven, and none can explain the lack of efficacy in both low-TB burden countries (US) and high-TB burden countries (India). The reasons for variable efficacy have been discussed at length in a WHO document on BCG.
Adverse effects.
BCG immunization generally causes some pain and scarring at the site of injection. The main adverse effects are keloids—large, raised scars. The insertion of deltoid is most frequently used because the local complication rate is smallest when that site is used. Nonetheless, the buttock is an alternative site of administration because it provides better cosmetic outcomes.
BCG vaccine should be given intradermally. If given subcutaneously, it may induce local infection and spread to the regional lymph nodes, causing either suppurative and nonsuppurative lymphadenitis. Conservative management is usually adequate for nonsuppurative lymphadenitis. If suppuration occurs, it may need needle aspiration. For nonresolving suppuration, surgical excision is required, but not incision. Uncommonly, breast and gluteal abscesses can occur due to haematogenous and lymphangiomatous spread. Regional bone infection (BCG osteomyelitis or osteitis) and disseminated BCG infection are rare complications of BCG vaccination, but potentially life-threatening. Systemic antituberculous therapy may be helpful in severe complications.
If BCG is accidentally given to an immunocompromised patient (e.g., an infant with SCID), it can cause disseminated or life-threatening infection. The documented incidence of this happening is less than one per million immunizations given. In 2007, The WHO stopped recommending BCG for infants with HIV, even if there is a high risk of exposure to TB, because of the risk of disseminated BCG infection (which is approximately 400 per 100,000).
Manufacturers.
A number of different companies make BCG, sometimes using different genetic strains of the bacterium. This may result in different product characteristics. OncoTICE, used for bladder instillation for bladder cancer, was developed by Organon Laboratories (since acquired by Schering-Plough, and in turn acquired by Merck, Inc.). Pacis® BCG, made from the Montréal (Institut Armand-Frappier) strain, was first marketed by Urocor in about 2002. Urocor was since acquired by Dianon Systems. Evans Vaccines (a subsidiary of PowderJect Pharmaceuticals Plc, London: PJP). Statens Serum Institut in Denmark markets BCG vaccine prepared using Danish strain 1331. markets its vaccine, based on the Tokyo 172 substrain of Pasteur BCG, in 50 countries worldwide. Sanofi Pasteur's BCG vaccine products, made with the Glaxo 1077 strain, were due to noncompliance in the manufacturing process.
History.
The history of BCG is tied to that of smallpox. Jean Antoine Villemin first recognized bovine tuberculosis in 1854 and transmitted it, and Robert Koch first distinguished "Mycobacterium bovis" from "Mycobacterium tuberculosis". Following the success of vaccination in preventing smallpox, established during the 18th century, scientists thought to find a corollary in tuberculosis by drawing a parallel between bovine tuberculosis and cowpox: it was hypothesized that infection with bovine tuberculosis might protect against infection with human tuberculosis. In the late 19th century, clinical trials using "M. bovis" were conducted in Italy with disastrous results, because "M. bovis" was found to be just as virulent as "M. tuberculosis".
Albert Calmette, a French physician and bacteriologist, and his assistant and later colleague, Camille Guérin, a veterinarian, were working at the Institut Pasteur de Lille (Lille, France) in 1908. Their work included subculturing virulent strains of the tubercle bacillus and testing different culture media. They noted a glycerin-bile-potato mixture grew bacilli that seemed less virulent, and changed the course of their research to see if repeated subculturing would produce a strain that was attenuated enough to be considered for use as a vaccine. The research continued throughout World War I until 1919, when the now avirulent bacilli were unable to cause tuberculosis disease in research animals. They transferred to the Paris Pasteur Institute in 1919. The BCG vaccine was first used in humans in 1921.
Public acceptance was slow, and one disaster, in particular, did much to harm public acceptance of the vaccine. In the summer of 1930 in Lübeck, 240 infants were vaccinated in the first 10 days of life; almost all developed tuberculosis and 72 infants died. It was subsequently discovered that the BCG administered had been contaminated with a virulent strain that was being stored in the same incubator, and led to legal action being taken against the manufacturers of BCG.
Dr. R.G. Ferguson, working at the Fort Qu'Appelle Sanatorium in Saskatchewan, was among the pioneers in developing the practice of vaccination against tuberculosis. In 1928, BCG was adopted by the Health Committee of the League of Nations (predecessor to the WHO). Because of opposition, however, it did not become widely used until after World War II. From 1945 to 1948, relief organizations (International Tuberculosis Campaign or Joint Enterprises) vaccinated over 8 million babies in eastern Europe and prevented the predicted typical increase of TB after a major war.
BCG is very efficacious against tuberculous meningitis in the pediatric age group, but its efficacy against pulmonary tuberculosis appears to be variable. As of 2006, only a few countries do not use BCG for routine vaccination. Two countries that have never used it routinely are the USA and the Netherlands (in both countries, it is felt that having a reliable Mantoux test and being able to accurately detect active disease is more beneficial to society than vaccinating against a condition that is now relatively rare there). 
Research.
Recent research by the Imperial College London has focused on finding new cell-wall proteins that trigger an immune response and are suitable for use in a vaccine to provide long-term protection against "M. tuberculosis". The study has revealed a few such proteins, the most promising of which has been dubbed EspC; it elicits a very strong immune reaction, and is specific to "M. tuberculosis".

</doc>
<doc id="4192" url="http://en.wikipedia.org/wiki?curid=4192" title="Bunsen">
Bunsen

Bunsen may refer to:

</doc>
<doc id="4193" url="http://en.wikipedia.org/wiki?curid=4193" title="Common buzzard">
Common buzzard

The common buzzard ("Buteo buteo") is a medium-to-large bird of prey, whose range covers most of Europe and extends into Asia. It is usually resident year-round, except in the coldest parts of its range, and in the case of one subspecies.
Description.
The common buzzard measures between in length with a wingspan and a body mass of , making it a medium-sized raptor.
This broad-winged raptor has a wide variety of plumages, and in Europe can be confused with the similar rough-legged buzzard ("Buteo lagopus") and the only distantly related European honey buzzard ("Pernis apivorus"), which mimics the common buzzard's plumage for a degree of protection from northern goshawks . The plumage can vary in Britain from almost pure white to black, but is usually shades of brown, with a pale 'necklace' of feathers.
Systematics.
The common buzzard was first described by Linnaeus in his "Systema naturae" in 1758 as "Falco buteo". Buzzard subspecies fall into two groups.
The western "Buteo" group is mainly resident or short-distance migrants. They are:
The eastern "vulpinus" group includes
Two resident forms on islands close to Africa are often assigned to the first group, but appear to be distinct species, more closely related to the African long-legged buzzard, based on biogeography and preliminary mtDNA cytochrome "b" sequence data (Clouet & Wink 2000):
Behaviour.
The common buzzard breeds in woodlands, usually on the fringes, but favours hunting over open land. It eats mainly small mammals, and will come to carrion. A great opportunist, it adapts well to a varied diet of pheasant, rabbit, other small mammals to medium mammals, snakes and lizards, and can often be seen walking over recently ploughed fields looking for worms and insects.
Buzzards do not normally form flocks, but several may be seen together on migration or in good habitat. The Victorian writer on Dartmoor, William Crossing, noted he had on occasions seen flocks of 15 or more at some places. Though a rare occurrence, as many as 20 buzzards can be spotted in one field area, approximately 30 metres apart, so cannot be classed as a flock in the general sense, consisting of birds without a mate or territory. They are fiercely territorial, and, though rare, fights do break out if one strays onto another pair's territory, but dominant displays of aggression will normally see off the interloper. Pairs mate for life. To attract a mate (or impress his existing mate) the male performs a ritual aerial display before the beginning of spring. This spectacular display is known as 'the roller coaster'. He will rise high up in the sky, to turn and plummet downward, in a spiral, twisting and turning as he comes down. He then rises immediately upward to repeat the exercise.
The call is a plaintive "peea-ay", similar to a cat's meow.
Steppe buzzard.
The steppe buzzard, "B. (b.) vulpinus" breeds from east Europe eastward to the Far East, excluding Japan. It is a long-distance migrant, excepting some north Himalayan birds, and winters in Africa, India and southeastern Asia. In the open country favoured on the wintering grounds, steppe buzzards are often seen perched on roadside telephone poles.
The steppe buzzard is some times split off as a separate species, "B. vulpinus". Compared to the nominate form, it is slightly smaller (45–50 cm long), longer winged and longer tailed. There are two colour morphs: the rufous form which gives this subspecies its scientific name ("vulpes" is Latin for "fox"), and a dark grey form.
The tail of "vulpinus" is paler than the nominate form, and often quite rufous, recalling North American red-tailed hawk. The upper wings have pale primary patches, and the primary flight feathers are also paler when viewed from below. Adults have a black trailing edge to the wings, and both morphs often have plain underparts, lacking the breast band frequently seen in "B. b. buteo".
Forest buzzard.
The forest buzzard, "B. (b.) trizonatus", is another form sometimes upgraded to a full species, though most recent authorities have placed it as a subspecies of another species, the mountain buzzard, "B. oreophilus". This is a resident breeding species in woodlands in southern and eastern South Africa.
It is very similar to the abundant summer migrant steppe buzzard, but the adult can be distinguished with a good view by its whiter underparts and unbarred flanks. The juvenile differs from the same-age steppe buzzard by its white front and tear-shaped flank streaks.
The forest buzzard, as its name implies, inhabits evergreen woodlands, including introduced eucalyptus and pines, whereas the steppe buzzard prefers more open habitats. However, habitat alone is not a good indicator for these forms.

</doc>
<doc id="4194" url="http://en.wikipedia.org/wiki?curid=4194" title="Bohrium">
Bohrium

Bohrium is a chemical element with symbol Bh and atomic number 107. It is named after Danish physicist Niels Bohr. It is a synthetic element (an element that can be created in a laboratory but is not found in nature) and radioactive; the most stable known isotope, 270Bh, has a half-life of approximately 61 seconds.
In the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 7 elements. Chemistry experiments have confirmed that bohrium behaves as the heavier homologue to rhenium in group 7. The chemical properties of bohrium are characterized only partly, but they compare well with the chemistry of the other group 7 elements.
History.
Official discovery.
Bohrium was first convincingly synthesized in 1981 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of bismuth-209 with accelerated nuclei of chromium-54 to produce 5 atoms of the isotope bohrium-262:
The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report.
Proposed names.
The German group suggested the name "nielsbohrium" with symbol "Ns" to honor the Danish physicist Niels Bohr. The Soviet scientists at the Joint Institute for Nuclear Research in Dubna, Russia had suggested this name be given to element 105 (which was finally called dubnium) and the German team wished to recognise both Bohr and the fact that the Dubna team had been the first to propose the cold fusion reaction to solve the controversial problem of the naming of element 105. The Dubna team agreed with the German group's naming proposal for element 107.
There was an element naming controversy as to what the elements from 104 to 106 were to be called; the IUPAC adopted "unnilseptium" (symbol "Uns") as a temporary, systematic element name for this element. In 1994 a committee of IUPAC recommended that element 107 be named "bohrium", not "nielsbohrium", since there was no precedence for using a scientist's complete name in the naming of an element. This was opposed by the discoverers as there was some concern that the name might be confused with boron and in particular the distinguishing of the names of their respective oxyanions, "bohrate" and "borate". The matter was handed to the Danish branch of IUPAC which, despite this, voted in favour of the name "bohrium", and thus the name "bohrium" for element 107 was recognized internationally in 1997. The IUPAC subsequently decided that bohrium salts should be called "bohriates" instead of "bohrates".
Isotopes.
Bohrium has no stable or naturally-occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eleven different isotopes of bohrium have been reported with atomic masses 260–262, 264–267, 270–272, 274, one of which, bohrium-262, has a known metastable state. All of these decay only through alpha decay, although some unknown bohrium isotopes are predicted to undergo spontaneous fission.
Stability and half-lives.
The lighter isotopes usually have shorter half-lives; half-lives of under 100 ms for 260Bh, 261Bh, 262Bh, and 262mBh were observed. 264Bh, 265Bh, 266Bh, and 271Bh are more stable at around 1 s, and 267Bh and 272Bh have half-lives of about 10 s. The heaviest isotopes are the most stable, with 270Bh and 274Bh having measured half-lives of about 61 s and 54 s respectively. The unknown isotopes 273Bh and 275Bh are predicted to have even longer half-lives of around 90 minutes and 40 minutes respectively. Before its discovery, 274Bh was also predicted to have a long half-life of 90 minutes, but it was found to have a shorter half-life of only about 54 seconds.
The proton-rich isotopes with masses 260, 261, and 262 were directly produced by cold fusion, those with mass 262 and 264 were reported in the decay chains of meitnerium and roentgenium, while the neutron-rich isotopes with masses 265, 266, 267 were created in irradiations of actinide targets. The four most neutron-rich ones with masses 270, 271, 272, and 274 appear in the decay chains of 282113, 287115, 288115, and 294117 respectively. These eleven isotopes have half-lives ranging from 8 miliseconds to 1 minute.
Chemical properties.
Extrapolated.
Bohrium is the fourth member of the 6d series of transition metals and the heaviest member of group VII in the Periodic Table, below manganese, technetium and rhenium. All the members of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well.
The heavier members of the group are known to form volatile heptoxides M2O7, so bohrium should also form the volatile oxide Bh2O7. The oxide should dissolve in water to form perbohric acid, HBhO4.
Rhenium and technetium form a range of oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MO3Cl, so BhO3Cl should be formed in this reaction. Fluorination results in MO3F and MO2F3 for the heavier elements in addition to the rhenium compounds ReOF5 and ReF7. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties.
Bohrium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure ("c"/"a" = 1.62), similar to its lighter congener rhenium.
Experimental.
In 1995, the first report on attempted isolation of the element was unsuccessful.
In 2000, it was confirmed that although relativistic effects are important, the 107th element does behave like a typical group 7 element.
In 2000, a team at the PSI conducted a chemistry reaction using atoms of 267Bh produced in the reaction between 249Bk and 22Ne ions. The resulting atoms were thermalised and reacted with a HCl/O2 mixture to form a volatile oxychloride. The reaction also produced isotopes of its lighter homologues, technetium (as 108Tc) and rhenium (as 169Re). The isothermal adsorption curves were measured and gave strong evidence for the formation of a volatile oxychloride with properties similar to that of rhenium oxychloride. This placed bohrium as a typical member of group 7.

</doc>
<doc id="4195" url="http://en.wikipedia.org/wiki?curid=4195" title="Barbara Olson">
Barbara Olson

Barbara Kay Olson (born Barbara Kay Bracher; December 27, 1955 September 11, 2001) was a lawyer and conservative American television commentator who worked for CNN, Fox News Channel, and several other outlets. She was a passenger on American Airlines Flight 77 en route to a taping of Bill Maher's television show "Politically Incorrect" when it was flown into the Pentagon in the September 11 attacks.
Early life.
Olson was born Barbara Kay Bracher in Houston, Texas. (Her older sister, Toni Bracher-Lawrence, has been a member of the Houston City Council since 2004.) She graduated from Waltrip High School and earned a Bachelor of Arts from the University of Saint Thomas in Houston. She earned a Juris Doctor degree from Yeshiva University's Benjamin N. Cardozo School of Law.
Career.
As a newcomer, she achieved a surprising measure of success, working for HBO and Stacy Keach Productions. In the early 1990s, she worked as an associate at the Washington, D.C.-based law firm of Wilmer Cutler & Pickering where she did civil litigation for several years before becoming an Assistant U.S. Attorney. 
Olson's support in 1991 of Supreme Court nominee Clarence Thomas led to the formation of the Independent Women's Forum. At that time, Olson and friend Rosalie (Ricky) Gaull Silberman started an informal network of women who supported the Thomas nomination to the Supreme Court despite allegations of sexual harassment by Anita Hill, a former subordinate of Thomas at the Equal Employment Opportunity Commission. Olson, who had also worked under Thomas at the EEOC and was a close friend of Thomas, spoke out on his behalf during his contentious Senate confirmation hearings. Olson later helped edit "The Real Anita Hill", a book by David Brock that savaged Hill and portrayed the harassment claim as a political dirty trick (Brock later recanted his claims and apologized to Hill). The Independent Women's Forum continued on with a goal of remaining a high profile group of women to advocate for economic and political freedom and personal responsibility. 
In 1994, Olson became chief investigative counsel for the U.S. House of Representatives Committee on Oversight and Government Reform. In that position, she led the Travelgate and Filegate investigations into the Clinton administration. She was later a partner in the Washington, D.C. office of the Birmingham, Alabama law firm Balch & Bingham. 
Personal life.
She married Theodore Olson in 1996. Theodore went on to successfully represent presidential candidate George W. Bush in the Supreme Court case of "Bush v. Gore", and subsequently served as U.S. Solicitor General in the Bush administration. 
Olson was a frequent critic of the Bill Clinton administration and wrote a book about then First Lady Hillary Rodham Clinton, "Hell to Pay: The Unfolding Story of Hillary Rodham Clinton" (1999). Olson's second book, "The Final Days: The Last, Desperate Abuses of Power by the Clinton White House" was published posthumously.
She was a resident of Great Falls, Virginia.
Death and legacy.
Olson was a passenger on American Airlines Flight 77 on her way to a taping of "Politically Incorrect" in Los Angeles, when it was flown into the Pentagon in the September 11 attacks. At the National September 11 Memorial, Olson's name is located on Panel S-70 of the South Pool, along with those of other passengers of Flight 77.

</doc>
<doc id="4196" url="http://en.wikipedia.org/wiki?curid=4196" title="Barnard's Star">
Barnard's Star

Barnard's Star is a very low-mass red dwarf star about six light-years away from Earth in the constellation of Ophiuchus, the Snake-holder. Barnard's Star is the fourth-closest known individual star to the Sun (almost 6 light years away), after the three components of the Alpha Centauri system, and the closest star in the Northern Hemisphere. Despite its proximity, Barnard's Star, at a dim apparent magnitude of about nine, is not visible with the unaided eye; however, it is much brighter in the infrared than it is in visible light. The star is named for American astronomer E.E. Barnard. He was not the first to observe the star (it appeared on Harvard College University plates in 1888 and 1890), but in 1916 he measured its proper motion as 10.3 arcseconds (20,000 inverse radians) per year, which remains the largest-known proper motion of any star relative to the Solar System.
Barnard's Star has been the subject of much study, and it has probably received more attention from astronomers than any other class M dwarf star due to its proximity and favorable location for observation near the celestial equator. Historically, research on Barnard's Star has focused on measuring its stellar characteristics, its astrometry, and also refining the limits of possible extrasolar planets. Although Barnard's Star is an ancient star, some observations suggest that it still experiences star flare events.
Barnard's Star has also been the subject of some controversy. For a decade, from the early 1960s to the early 1970s, Peter van de Kamp claimed that there were one or more gas giants in orbit around it. Although the presence of small terrestrial planets around Barnard's Star remains a possibility, Van de Kamp's specific claims of large gas giants were refuted in the mid-1970s.
Barnard's Star is also notable as the target for Project Daedalus, a study on the possibility of fast, unmanned travel to nearby star systems.
Overview.
Barnard's Star is a red dwarf of the dim spectral type M4, and it is too faint to see without a telescope. Its apparent magnitude is 9.54. This compares with a magnitude of −1.5 for Sirius – the brightest star in the night sky – and about 6.0 for the faintest visible objects with the naked eye (this magnitude scale is logarithmic, and so the magnitude of 9.54 is only about 1/27th of the brightness of the faintest star that can be seen with the naked eye under good viewing conditions).
At seven to 12 billion years of age, Barnard's Star is considerably older than the Sun [4.567 billion], and it might be among the oldest stars in the Milky Way galaxy. Barnard's Star has lost a great deal of rotational energy, and the periodic slight changes in its brightness indicate that it rotates just once in 130 days (the Sun rotates in 25). Given its age, Barnard's Star was long assumed to be quiescent in terms of stellar activity. However in 1998, astronomers observed an intense stellar flare, surprisingly showing that Barnard's Star is a flare star. Barnard's Star has the variable star designation V2500 Ophiuchi. In 2003, Barnard's Star presented the first detectable change in the radial velocity of a star caused by its motion. Further variability in the radial velocity of Barnard's Star was attributed to its stellar activity.
The proper motion of Barnard's Star corresponds to a relative lateral speed ("sideways" relative to our line of sight to the Sun) of 90 km/s. The 10.3 seconds of arc it travels annually amounts to a quarter of a degree in a human lifetime, roughly half the angular diameter of the full Moon.
The radial velocity of Barnard's Star towards the Sun can be measured by its blue shift. Two measurements are given in catalogues: 106.8 km/s in SIMBAD, which refers to a 1967 compilation of older measurements, and 110.8 km/s in ARICNS and similar values in all modern astronomical references. These measurements, combined with proper motion, suggest a true velocity relative to the Sun of 139.7 and 142.7 km/s, respectively. Barnard's Star will make its closest approach to the Sun around AD 11,800, when it approaches to within about 3.75 light-years. However, at that time, Barnard's Star will not be the nearest star, since Proxima Centauri will have moved even closer to the Sun. Barnard's Star will still be too dim to be seen with the naked eye at the time of its closest approach, since its apparent magnitude will be about 8.5 then. After that it will gradually recede from the Sun.
Barnard's Star has approximately 14% of a solar mass, and it has a radius 15% to 20% of that of the Sun. In 2003, its radius was estimated as 0.20±0.008 of the solar radius, at the high end of the ranges that were typically calculated in the past, indicating that previous estimates of the radius of Barnard's Star probably underestimated the actual value. Thus, although Barnard's Star has roughly 150 times the mass of Jupiter, its radius is only 1.5 to 2.0 times larger, reflecting the tendency of objects in the brown dwarf range to be about the same size. Its effective temperature is 3,134(±102) kelvin, and it has a visual luminosity just 4/10,000ths of solar luminosity, corresponding to a bolometric luminosity of 34.6/10,000ths. Barnard's Star is so faint that if it were at the same distance from Earth as the Sun is, it would appear only 100 times brighter than a full moon, comparable to the brightness of the Sun at 80 Astronomical Units.
In a broad survey of the metallicity of M-class dwarf stars, Barnard's Star's was placed between −0.5 and −1.0 on the metallicity scale, which is roughly 10 to 32% of the value for the Sun. Metallicity, the proportion of stellar mass made up of elements heavier than helium, helps classify stars relative to the galactic population. Barnard's Star seems to be typical of the old, red dwarf population II stars, yet these are also generally metal-poor halo stars. While sub-solar, Barnard's Star's metallicity is higher than a halo star and is in keeping with the low end of the metal-rich disk star range; this, plus its high space motion, have led to the designation "Intermediate Population II star", between a halo and disk star.
Claims of a planetary system.
For a decade from 1963 to about 1973, a substantial number of astronomers accepted a claim by Peter van de Kamp that he had detected, by using astrometry, a perturbation in the proper motion of Barnard's Star consistent with its having one or more planets comparable in mass with Jupiter. Van de Kamp had been observing the star from 1938, attempting, with colleagues at the Swarthmore College observatory, to find minuscule variations of one micrometre in its position on photographic plates consistent with orbital perturbations (wobbles) in the star that would indicate a planetary companion; this involved as many as ten people averaging their results in looking at plates, to avoid systemic, individual errors. Van de Kamp's initial suggestion was a planet having about 1.6 the Jovian mass at a distance of 4.4 AU in a slightly eccentric orbit, and these measurements were apparently refined in a 1969 paper. Later that year, Van de Kamp suggested that there were two planets of 1.1 and 0.8 Jovian masses.
Other astronomers subsequently repeated Van de Kamp's measurements, and two important papers in 1973 undermined the claim of a planet or planets. George Gatewood and Heinrich Eichhorn, at a different observatory and using newer plate measuring techniques, failed to verify the planetary companion. Another paper published by John L. Hershey four months earlier, also using the Swarthmore observatory, found that changes in the astrometric field of various stars correlated to the timing of adjustments and modifications that had been carried out on the refractor telescope's objective lens; the planetary "discovery" was an artifact of maintenance and upgrade work. The affair has been discussed as part of a broader scientific review.
Van de Kamp never acknowledged any error and published a further confirmation of two planets' existence as late as 1982; he died in 1995. Wulff Heintz, Van de Kamp's successor at Swarthmore and an expert on double stars, questioned his findings and began publishing criticisms from 1976 onwards. The two men were reported to have become estranged from each other because of this.
Refining planetary boundaries.
While not completely ruling out the possibility of planets, null results for planetary companions continued throughout the 1980s and 1990s, the latest based on interferometric work with the Hubble Space Telescope in 1999. By refining the values of a star's motion, the mass and orbital boundaries for possible planets are tightened: in this way astronomers are often able to describe what types of planets "cannot" orbit a given star.
M dwarfs such as Barnard's Star are more easily studied than larger stars in this regard because their lower masses render perturbations more obvious. Gatewood was thus able to show in 1995 that planets with 10 times the mass of Jupiter (the lower limit for brown dwarfs) were impossible around Barnard's Star, in a paper which helped refine the negative certainty regarding planetary objects in general. In 1999, work with the Hubble Space Telescope further excluded planetary companions of 0.8 times the mass of Jupiter with an orbital period of less than 1,000 days (Jupiter's orbital period is 4,332 days), while Kuerster determined in 2003 that within the habitable zone around Barnard's Star, planets are not possible with an ""M" sin "i" value greater than 7.5 times the mass of the Earth, or with a mass greater than 3.1 times the mass of Neptune (much lower than van de Kamp's smallest suggested value).
Even though this research has greatly restricted the possible properties of planets around Barnard's Star, it has not ruled them out completely; terrestrial planets would be difficult to detect. NASA's Space Interferometry Mission, which was to begin searching for extrasolar Earth-like planets, was reported to have chosen Barnard's Star as an early search target. However, this mission was shut down in 2010. ESA's similar Darwin interferometry mission had the same goal, but was stripped of funding in 2007.
Project Daedalus.
Excepting the planet controversy, the best known study of Barnard's Star was part of Project Daedalus. Undertaken between 1973 and 1978, it suggested that rapid, unmanned travel to another star system is possible with existing or near-future technology. Barnard's Star was chosen as a target, partly because it was believed to have planets.
The theoretical model suggested that a nuclear pulse rocket employing nuclear fusion (specifically, electron bombardment of deuterium and helium-3) and accelerating for four years could achieve a velocity of 12% of the speed of light. The star could then be reached in 50 years, within a human lifetime. Along with detailed investigation of the star and any companions, the interstellar medium would be examined and baseline astrometric readings performed.
The initial Project Daedalus model sparked further theoretical research. In 1980, Robert Freitas suggested a more ambitious plan: a self-replicating spacecraft intended to search for and make contact with extraterrestrial life. Built and launched in Jovian orbit, it would reach Barnard's Star in 47 years under parameters similar to those of the original Project Daedalus. Once at the star, it would begin automated self-replication, constructing a factory, initially to manufacture exploratory probes and eventually to create a copy of the original spacecraft after 1,000 years.
The flare in 1998.
The observation of a stellar flare on Barnard's Star has added another element of interest to its study. Noted by William Cochran, University of Texas at Austin, based on changes in the spectral emissions on July 17, 1998 (during an unrelated search for planetary "wobbles"), it was four more years before the flare was fully analyzed. At that point Diane Paulson "et al.", now of Goddard Space Flight Center, suggested that the flare's temperature was 8000 K, more than twice the normal temperature of the star, although simply analyzing the spectra cannot precisely determine the flare's total output. Given the essentially random nature of flares, she noted "the star would be fantastic for amateurs to observe".
The flare was surprising because intense stellar activity is not expected around stars of such age. Flares are not completely understood, but are believed to be caused by strong magnetic fields which suppress plasma convection and lead to sudden outbursts: strong magnetic fields occur in rapidly rotating stars, while old stars tend to rotate slowly. An event of such magnitude around Barnard's Star is thus presumed to be a rarity. Research on the star's periodicity, or changes in stellar activity over a given timescale, also suggest it ought to be quiescent; 1998 research showed weak evidence for periodic variation in Barnard's Star's brightness, noting only one possible starspot over 130 days.
Stellar activity of this sort has created interest in using Barnard's Star as a proxy to understand similar stars. Photometric studies of its X-ray and UV emissions are hoped to shed light on the large population of old M dwarfs in the galaxy. Such research has astrobiological implications: given that the habitable zones of M dwarfs are close to the star, any planets would be strongly influenced by solar flares, winds, and plasma ejection events.
The star's neighborhood.
Barnard's Star shares much the same neighborhood as the Sun. The neighbors of Barnard's Star are generally of red dwarf size, the smallest and most common star type. Its closest neighbor is currently the red dwarf Ross 154, at 1.66 parsecs or 5.41 light years distance. The Sun and Alpha Centauri are, respectively, the next closest systems. From Barnard's Star, the Sun would appear on the diametrically opposite side of the sky at coordinates RA=, Dec=, in the eastern part of the constellation Monoceros. The absolute magnitude of the Sun is 4.83 and at a distance of 1.834 parsecs, it would be an impressively bright first-magnitude star, as Pollux is from the Earth.

</doc>
<doc id="4199" url="http://en.wikipedia.org/wiki?curid=4199" title="Bayer designation">
Bayer designation

A Bayer designation is a stellar designation in which a specific star is identified by a Greek letter, followed by the genitive form of its parent constellation's Latin name. The original list of Bayer designations contained 1,564 stars.
Most of the brighter stars were assigned their first systematic names by the German astronomer Johann Bayer in 1603, in his star atlas "Uranometria". Bayer assigned a lower-case Greek letter, such as alpha (α), beta (β), gamma (γ), etc., to each star he catalogued, combined with the Latin name of the star’s parent constellation in genitive (possessive) form. (See 88 modern constellations for the genitive forms.) For example, Aldebaran is designated "α Tauri" (pronounced "Alpha Tauri"), which means "Alpha of the constellation Taurus".
A single constellation may contain fifty or more stars, but the Greek alphabet has only twenty-four letters. When these ran out, Bayer began using Latin letters: upper case "A", followed by lower case "b" through "z" (omitting "j" and "v"), for a total of another 24 letters. Bayer never went beyond "z", but later astronomers added more designations using both upper and lower case Latin letters, the upper case letters following the lower case ones in general. Examples include "s Carinae" ("s" of the constellation Carina), "d Centauri" ("d" of the constellation Centaurus), "G Scorpii" ("G" of the constellation Scorpius), and "N Velorum" ("N" of the constellation Vela). The last upper-case letter used in this way was "Q".
Is Alpha always the brightest star?
For the most part, Bayer assigned Greek and Latin letters to stars in rough order of apparent brightness, from brightest to dimmest, within a particular constellation. Since in a majority of constellations the brightest star is designated Alpha (α), many people wrongly assume that Bayer meant to put the stars exclusively in order of their brightness, but in his day there was no way to measure stellar brightness precisely. Traditionally, the stars were assigned to one of six magnitude classes, and Bayer's catalog lists all the first-magnitude stars, followed by all the second-magnitude stars, and so on. Within each magnitude class, Bayer made no attempt to arrange stars by relative brightness.
Bayer did not always follow this rule; he sometimes assigned letters to stars according to their location within a constellation (for example: the northern, southern, eastern, or western part of a constellation), according to either the order in which they rise in the east, to historical or mythological information on specific stars within a constellation, or to his own arbitrary choosing.
Of the 88 modern constellations, there are at least 30 in which "Alpha" is not the brightest star, and four of those lack an alpha star altogether. (Constellations with no alpha include Vela and Puppis, both formerly part of Argo Navis whose alpha is Canopus in Carina.)
Bayer designations in Orion.
Orion provides a good example of Bayer's method. (The lower the magnitude, the brighter the star; additionally, there is a precise definition: a "2nd-magnitude" star ranks between 1.51 and 2.50, inclusive.) Bayer first designated the two 1st-magnitude stars, Betelgeuse and Rigel, as Alpha and Beta, with Betelgeuse (the shoulder) coming ahead of Rigel (the foot), even though the latter is usually the brighter. (Betelgeuse, a variable star, can at its maximum occasionally be brighter than Rigel.) He then repeated the procedure for the stars of the 2nd magnitude. As is evident from the map and chart, he again followed a "top-down" ("north-south") route.
Various Bayer designation arrangements.
This "First to Rise in the East" method is done in a number of other instances, even for Castor and Pollux of Gemini. Although Pollux is brighter than Castor, the latter was assigned "alpha" because it rises in the east ahead of the former. Bayer may also have assigned the stars Castor and Pollux in terms of historical or mythological knowledge. Both historically and mythologically, Castor's name is almost always mentioned first (Castor and Pollux) whenever the twins are mentioned, and that may have compelled him to assign "alpha" (α) to Castor and "beta" (β) to Pollux.
Although the brightest star in Draco is Eltanin (Gamma Draconis), Thuban was assigned "alpha" (α) by Bayer because Thuban, in history, was once the north pole star, 4,000 years ago. Almost every star with a history of being the North Star, including Vega, Alderamin, and Polaris, was designated as the "alpha" (α) of its parent constellation by Bayer.
Sometimes, indeed, there's no apparent order, as exemplified by the stars in Sagittarius, where Bayer's designations appear almost random to the modern eye. Alpha and Beta Sagittarii are perhaps the most anomalously designated stars in the sky; they are more than two magnitudes fainter than the brightest star (designated Epsilon), lie several degrees south of the main pattern (the "teapot" asterism), are more than 20 degrees off the ecliptic in a Zodiacal constellation, and do not even rise from Bayer's native Germany while Epsilon and several other brighter stars do. The order of the letters assigned in Sagittarius does correspond to the magnitudes as illustrated on Bayer's chart; but the latter don't agree with modern determinations of the magnitudes.
Bayer designations added by later astronomers generally were ordered by magnitude, but care was usually taken to avoid conflict with designations already assigned. In Libra, for example, the new designations sigma, tau, and upsilon were chosen to avoid conflict with Bayer's earlier designations, even though several stars with earlier letters are not as bright.
Bayer's miscellaneous labels.
Although Bayer did not use upper-case Latin letters (except "A") for "fixed stars", he did use them to label other items shown on his charts, such as neighboring constellations, miscellaneous astronomical objects, or reference lines like the Tropic of Cancer. In Cygnus, for example, Bayer's fixed stars run through "g", and on this chart Bayer employs "H" through "P" as miscellaneous labels, mostly for neighboring constellations. Bayer did not intend such labels as catalog designations, but some have survived to refer to astronomical objects: P Cygni for example is still used as a designation for Nova Cyg 1600. In charts for constellations that did not exhaust the Greek letters, Bayer sometimes used the left-over Greek letters for miscellaneous labels as well.
Revised Bayer designations.
Ptolemy designated three stars as "border stars", each shared by two constellations; and Bayer assigned these a Greek letter from both constellations: , , and . When the International Astronomical Union (IAU) outlined the official 88 constellations with definite boundaries in 1930, it declared that stars and other celestial objects can be assigned to only one constellation. Consequently, the redundant Bayer designations for those three stars were scrapped in favor of Beta Tauri, Alpha Andromedae, and Nu Boötis. 
Other cases of multiple Bayer designations arose when stars named by Bayer in one constellation were transferred to a different constellation. Bayer's Gamma and Omicron Scorpii, for example, were later reassigned from Scorpius to Libra and given the new names Sigma and Upsilon Librae. (To add to the confusion, the star now known as Omicron Scorpii was not named by Bayer but was assigned the designation o Scorpii (Latin lower case 'o') by Lacaille – which later astronomers misinterpreted as omicron once Bayer's omicron had been reassigned to Libra.) 
A few stars no longer lie (according to the modern constellation boundaries) within the constellation for which they are named. The proper motion of Rho Aquilae, for example, recently carried it across the boundary into Delphinus. Nonetheless, these designations have proved useful and are widely used today.
Bayer designation styles.
Greek letters can be used together with the standard 3-letter abbreviation of the constellation, as in α CMa or β Per. Or the two can be combined (α Canis Majoris). Earlier 4-letter abbreviations (such as α CMaj) are rarely used today.
Bayer designations are sometimes written out out in full, as in Alpha Canis Majoris or Beta Persei.
Other Bayer designations.
The Latin-letter extended designations are not as commonly used as the Greek-letter ones, but there are some exceptions such as h Persei (which is actually a star cluster) and P Cygni. Uppercase Latin Bayer designations in modern use do not go beyond Q; names such as R Leporis and W Ursae Majoris are variable star designations, not Bayer designations.
A further complication is the use of numeric superscripts to distinguish neighboring stars that Bayer (or a later astronomer) labeled with a common letter. Usually these are double stars (mostly optical doubles rather than true binary stars), but there are some exceptions such as the chain of stars π1, π2, π3, π4, π5 and π6 Orionis.

</doc>
<doc id="4200" url="http://en.wikipedia.org/wiki?curid=4200" title="Boötes">
Boötes

Boötes is a constellation in the northern sky, located between 0° and +60° declination, and 13 and 16 hours of right ascension on the celestial sphere. The name comes from the Greek Βοώτης, "Boōtēs", meaning herdsman or plowman (literally, ox-driver; from "boos", related to the Latin "bovis", “cow”). The "ö" in the name is a diaeresis, not an umlaut, meaning that each 'o' is to be pronounced separately.
One of the 48 constellations described by the 2nd century astronomer Ptolemy, Boötes is now one of the 88 modern constellations. It contains the fourth brightest star in the night sky, the orange-hued Arcturus. Boötes is home to many other bright stars, including eight above the fourth magnitude and an additional 21 above the fifth magnitude, making a total of 29 stars easily visible to the naked eye.
History and mythology.
In ancient Babylon the stars of Boötes were known as SHU.PA. They were apparently depicted as the god Enlil, who was the leader of the Babylonian pantheon and special patron of farmers.
The name "Boötes" was first used by Homer in his Odyssey as a celestial reference point for navigation, described as "late-setting" or "slow to set", translated as the "Plowman". Exactly whom Boötes is supposed to represent in Greek mythology is not clear. According to one version, he was a son of Demeter, Philomenus, twin brother of Plutus, a ploughman who drove the oxen in the constellation Ursa Major. This is corroborated by the constellation's name, which itself means "ox-driver" or "herdsman." The ancient Greeks saw the asterism now called the "Big Dipper" or "Plough" as a cart with oxen. This influenced the name's etymology, derived from the Greek for "noisy" or "ox-driver". Another myth associated with Boötes tells that he invented the plow and was memorialized for his ingenuity as a constellation.
Another myth associated with Boötes by Hyginus is that of Icarius, who was schooled as a grape farmer and winemaker by Dionysus. Icarius made wine so strong that those who drank it appeared poisoned, which caused shepherds to avenge their supposedly poisoned friends by killing Icarius. Maera, Icarius's dog, brought his daughter Erigone to her father's body, whereupon both she and the dog committed suicide. Zeus then chose to honor all three by placing them in the sky as constellations: Icarius as Boötes, Erigone as Virgo, and Maera as Canis Major or Canis Minor.
Following another reading, the constellation is identified with Arcas and also referred to as Arcas and Arcturus, son of Zeus and Callisto. Arcas was brought up by his maternal grandfather Lycaon, to whom one day Zeus went and had a meal. To verify that the guest was really the king of the gods, Lycaon killed his grandson and prepared a meal made from his flesh. Zeus noticed and became very angry, transforming Lycaon into a wolf and gave back life to his son. In the meantime Callisto had been transformed into a she-bear, by Zeus's wife, Hera, who was angry at Zeus's infidelity. This is corroborated by the Greek name for Boötes, "Arctophylax", which means "Bear Watcher". Callisto in form of a bear was almost killed by her son who was out hunting. Zeus rescued her, taking her into the sky where she became Ursa Major, "the Great Bear". The name Arcturus (the constellation's brightest star) comes from the Greek word meaning "guardian of the bear". Sometimes Arcturus is depicted as leading the hunting dogs of nearby Canes Venatici and driving the bears of Ursa Major and Ursa Minor.
Several former constellations were formed from stars now included in Boötes. Quadrans Muralis, the Quadrant, was a constellation created near Beta Boötis from faint stars. It was invented in 1795 by Jérôme Lalande, an astronomer who used a quadrant to perform detailed astronometric measurements. Lalande worked with Nicole-Reine Lepaute and others to predict the 1758 return of Halley's Comet. Quadrans Muralis was formed from the stars of eastern Boötes, western Hercules, and Draco. It was originally called "Le Mural" by Jean Fortin in his 1795 Atlas Céleste; it was not given the name "Quadrans Muralis" until Johann Bode's 1801 "Uranographia". The constellation was quite faint, with its brightest stars reaching the 5th magnitude. Mons Maenalus, representing the Maenalus mountains, was created by Johannes Hevelius in 1687 at the foot of the constellation's figure. The mountain was named for the son of Lycaon, Maenalus. The mountain, one of Diana's hunting grounds, was also holy to Pan.
Non-Western astronomy.
The stars of Boötes were incorporated into many different Chinese constellations. Arcturus was part of the most prominent of these, variously designated as the celestial king's throne ("Tian Wang") or the Blue Dragon's horn ("Daijiao"); the name "Daijiao", meaning "great horn", is more common. Arcturus was given such importance in Chinese celestial mythology because of its status marking the beginning of the lunar calendar, as well as its status as the brightest star in the northern night sky. Two constellations flanked "Daijiao", "Yousheti" to the right and "Zuosheti" to the left; they represented companions that orchestrated the seasons. "Zuosheti" was formed from modern Zeta, Omicron, and Pi Boötis, while "Yousheti" was formed from modern Eta, Tau, and Upsilon Boötis. "Dixi", the Emperor's ceremonial banquet mat, was north of Arcturus, consisting of the stars 12, 11, and 9 Boötis. Another northern constellation was "Qigong", the Seven Dukes, which was mostly across the Boötes-Hercules border. It included either Delta Boötis or Beta Boötis as its terminus.
The other Chinese constellations made up of the stars of Boötes existed in the modern constellation's north; they are all representations of weapons. "Tianqiang", the spear, was formed from Iota, Kappa, and Theta Boötis; "Genghe", variously representing a lance or shield, was formed from Epsilon, Rho, and Sigma Boötis. There were also two weapons made up of a singular star. "Xuange", the halberd, was represented by Lambda Boötis, and "Zhaoyao", either the sword or the spear, was represented by Gamma Boötis.
Two Chinese constellations have an uncertain placement in Boötes. "Kangchi", the lake, was placed south of Arcturus, though its specific location is disputed. It may have been placed entirely in Boötes, on either side of the Boötes-Virgo border, or on either side of the Virgo-Libra border. The constellation "Zhouding", a bronze tripod-mounted container used for food, was sometimes cited as the stars 1, 2, and 6 Boötis. However, it has also been associated with three stars in Coma Berenices.
Characteristics.
Boötes is a constellation bordered by Virgo to the south, Coma Berenices and Canes Venatici to the west, Ursa Major to the northwest, Draco to the northeast, and Hercules, Corona Borealis and Serpens Caput to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Boo'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 16 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates stretch from +7.36° to +55.1°. Covering 907 square degrees, Boötes culminates at midnight around 2 May and ranks 13th in area.
Colloquially, its pattern of stars has been likened to a kite or ice cream cone. However, depictions of Boötes have varied historically. Aratus described him circling the north pole, herding the two bears. Later ancient Greek depictions, described by Ptolemy, have him holding the reins of his hunting dogs (Canes Venatici) in his left hand, with a spear, club, or staff in his right hand. After Hevelius introduced Mons Maenalus in 1681, Boötes was often depicted standing on the Peloponnese mountain. By 1801, when Johann Bode published his "Uranographia", Boötes had acquired a sickle, which was also held in his left hand.
The placement of Arcturus has also been mutable through the centuries. Traditionally, Arcturus lay between his thighs, as Ptolemy depicted him. However, Germanicus Caesar deviated from this tradition by placing Arcturus "where his garment is fastened by a knot".
Notable features.
Stars.
In his "Uranometria", Johann Bayer used the Greek letters Alpha through to Omega and then A to k to label what he saw as the most prominent 35 stars in the constellation, with subsequent astronomers splitting Kappa, Mu, Nu and Pi as two stars each. Nu is also the same star as Psi Herculis. John Flamsteed numbered 54 stars for the constellation.
Located 36.7 light-years from Earth, Arcturus, or Alpha Boötis, is the brightest star in Boötes and the fourth brightest star in the sky at an apparent magnitude of −0.05; It is also the brightest star north of the celestial equator, just shading out Vega and Capella. Its name comes from the Greek for "bear-keeper". An orange giant of spectral class K1.5III, Arcturus is an ageing star that has exhausted its core supply of hydrogen and cooled and expanded to a diameter of 27 solar diameters, equivalent to approximately 32 million kilometers. Though its mass is approximately one solar mass, Arcturus shines with 133 times the luminosity of the Sun. Bayer located Arcturus above the herdsman's left knee in his "Uranometria". Nearby Eta Boötis, or Muphrid, is the uppermost star denoting the left leg. It is a 2.68-magnitude star 37 light-years distant with a spectral class of G0IV, indicating it has just exhausted its core hydrogen and is beginning to expand and cool. It is 9 times as luminous as the Sun and has 2.7 times its diameter. Analysis of its spectrum reveals that it is a spectroscopic binary. Muphrid and Arcturus lie only 3.3 light years away from each other. Viewed from Arcturus, Muphrid would have a visual magnitude of -2½, while Arcturus would be around visual magnitude -4½ when seen from Muphrid.
Marking the herdsman's head is Beta Boötis, or Nekkar, a yellow giant of magnitude 3.5 and spectral type G8IIIa. Like Arcturus, it has expanded and cooled off the main sequence—likely to have lived most of its stellar life as a blue-white B-type main sequence star. Its common name comes from the Arabic phrase for "ox-driver". It is 219 light-years away and has a luminosity of 58 L☉. Located 86 light-years distant, Gamma Boötis, or Seginus, is a white giant star of spectral class A7III, with a luminosity 34 times and diameter 3.5 times that of the Sun. It is a Delta Scuti variable, ranging between magnitudes 3.02 and 3.07 every 7 hours. Thes stars are short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. Delta Boötis is a wide double star with a primary of magnitude 3.5 and a secondary of magnitude 7.8. The primary is a yellow giant that has cooled and expanded to 10.4 times the diameter of the Sun. Of spectral class class G8IV, it is around 121 light-years away, while the secondary is a yellow main sequence star of spectral type G0V. The two are thought to take 120,000 years to orbit each other. Mu Boötis, known as Alkalurops, is a triple star popular with amateur astronomers. It has an overall magnitude of 4.3 and is 121 light-years away. Its name is from the Arabic phrase for "club" or "staff". The primary appears to be of magnitude 4.3 and is blue-white. The secondary appears to be of magnitude 6.5, but is actually a close double star itself with a primary of magnitude 7.0 and a secondary of magnitude 7.6. The secondary and tertiary stars have an orbital period of 260 years. The primary has an absolute magnitude of 2.6 and is of spectral class F0. The secondary and tertiary stars are separated by 2 arcseconds; the primary and secondary are separated by 109.1 arcseconds at an angle of 171 degrees. Nu Boötis is an optical double star. The primary is an orange giant of magnitude 5.0 and the secondary is a white star of magnitude 5.0. The primary is 870 light-years away and the secondary is 430 light-years.
Epsilon Boötis, also known as "Izar" or "Pulcherrima", is a close triple star popular with amateur astronomers and the most prominent binary star in Boötes. The primary is a yellow- or orange-hued magnitude 2.5 giant star, the secondary is a magnitude 4.6 blue-hued main-sequence star, and the tertiary is a magnitude 12.0 star. The system is 210 light-years away. The name "Izar" comes from the Arabic word for "girdle" or "loincloth", referring to its location in the constellation. The name "Pulcherrima" comes from the Latin phrase for "most beautiful", referring to its contrasting colors in a telescope. The primary and secondary stars are separated by 2.9 arcseconds at an angle of 341 degrees; the primary's spectral class is K0 and it has a luminosity of 200 L☉. To the naked eye, Izar has a magnitude of 2.37. Nearby Rho and Sigma Boötis denote the herdsman's waist. Rho is an orange giant of spectral type K3III located around 160 light-years from Earth. It is ever so slightly variable, wavering by 0.003 of a magnitude from its average of 3.57. Sigma, a yellow-white main sequence star of spectral type F3V, is suspected of varying in brightness from 4.45 to 4.49. It is around 52 light-years distant. 
Traditionally known as "Al Aulād al Dhiʼbah" (ألعولد ألذعب - "al aulād al dhiʼb"), "the Whelps of the Hyenas", Theta, Iota, Kappa and Lambda Boötis are a small group of stars in the far north of the constellation. The magnitude 4.05 Theta Boötis has a spectral type of F7 and an absolute magnitude of 3.8. Iota Boötis is a triple star with a primary of magnitude 4.8 and spectral class of A7, a secondary of magnitude 7.5, and a tertiary of magnitude 12.6. The primary is 97 light-years away. The primary and secondary stars are separated by 38.5 arcseconds, at an angle of 33 degrees. The primary and tertiary stars are separated by 86.7 arcseconds at an angle of 194 degrees. Both the primary and tertiary appear white in a telescope, but the secondary appears yellow-hued. Kappa Boötis is another wide double star. The primary is 155 light-years away and has a magnitude of 4.5. The secondary is 196 light-years away and has a magnitude of 6.6. The two components are separated by 13.4 arcseconds, at an angle of 236 degrees. The primary, with spectral class A7, appears white and the secondary appears bluish. An apparent magnitude 4.18 type A0p star, Lambda Boötis is the prototype of a class of chemically peculiar stars, only some of which pulsate as Delta Scuti type stars. The distinction between the Lambda Boötis stars as a class of stars with peculiar spectra, and the delta Scuti stars whose class describes pulsation in low-overtone pressure modes, is an important one. While many Lambda Boötis stars pulsate and are delta Scuti stars, not many delta Scuti stars have Lambda Boötis peculiarities, since the Lambda Boötis stars are a much rarer class whose members can be found both inside and outside the delta Scuti instability strip. Lambda Boötis stars are dwarf stars that can be either spectral class A or F. Like BL Boötis-type stars they are metal-poor. Scientists have had difficulty explaining the characteristics of Lambda Boötis stars, partly because only around 60 confirmed members exist, but also due to heterogeneity in the literature. Lambda has an absolute magnitude of 1.8.
There are two dimmer F-type stars, magnitude 4.83 12 Boötis, class F8; and magnitude 4.93 45 Boötis, class F5. Xi Boötis is a G8 yellow dwarf of magnitude 4.55, and absolute magnitude is 5.5. Two dimmer G-type stars are magnitude 4.86 31 Boötis, class G8, and magnitude 4.76 44 Boötis, class G0.
Of apparent magnitude 4.06, Upsilon Boötis has a spectral class of K5 and an absolute magnitude of −0.3. Dimmer than Upsilon Boötis is magnitude 4.54 Phi Boötis, with a spectral class of K2 and an absolute magnitude of −0.1. Just slightly dimmer than Phi at magnitude 4.60 is O Boötis, which, like Izar, has a spectral class of K0. O Boötis has an absolute magnitude of 0.2. The other four dim stars are magnitude 4.91 6 Boötis, class K4; magnitude 4.86 20 Boötis, class K3; magnitude 4.81 Omega Boötis, class K4; and magnitude 4.83 A Boötis, class K1.
There is one bright B-class star in Boötes; magnitude 4.93 Pi1 Boötis, also called Alazal. It has a spectral class of B9 and is 40 parsecs from Earth. There is also one M-type star, magnitude 4.81 34 Boötis. It is of class gM0.
Multiple stars.
Besides Pulcherrima and Alkalurops, there are several other binary stars in Boötes:
44 Boötis (i Boötis) is a double variable star 42 light-years away. It has an overall magnitude of 4.8 and appears yellow to the naked eye. The primary is of magnitude 5.3 and the secondary is of magnitude 6.1; their orbital period is 220 years. The secondary is itself an eclipsing variable star with a range of 0.6 magnitudes; its orbital period is 6.4 hours. It is a W Ursae Majoris variable that ranges in magnitude from a minimum of 7.1 to a maximum of 6.5 every 0.27 days. Both stars are G-type stars. Another eclipsing binary star is ZZ Boötis, which has two F2-type components of almost equal mass, and ranges in magnitude from a minimum of 6.79 to a maximum of 7.44 over a period of 5.0 days.
Variable stars.
Two of the brighter Mira-type variable stars in the constellation are R and S Boötis. Both are red giants that range greatly in in magnitude—from 6.2 to 13.1 over 223.4 days, and 7.8 to 13.8 over a period of 270.7 days respectively. Also red giants, V and W Boötis are semi-regular variable stars that range in magnitude from 7.0 to 12.0 over a period of 258 days, and magnitude 4.7 to 5.4 over 450 days respectively.
BL Boötis is the prototype of its class of pulsating variable stars, the anomalous Cepheids. These stars are somewhat similar to Cepheid variables, but they do not have the same relationship between their period and luminosity. Their periods are similar to RRAB variables; however, they are far brighter than these stars. BL Boötis is a member of the cluster NGC 5466. Anomalous Cepheids are metal poor and have masses not much larger than the Sun's, on average, 1.5 solar masses. BL Boötis type stars are a subtype of RR Lyrae variables.
T Boötis was a nova observed in April 1860 at a magnitude of 9.7. It has never been observed since, but that does not preclude the possibility of it being a highly irregular variable star or a recurrent nova.
Stars with planetary systems.
Extrasolar planets have been discovered encircling ten stars in Boötes as of 2012. Tau Boötis is orbited by a large planet, discovered in 1999. The host star itself is a magnitude 4.5 star of type F7V, 15.6 parsecs from Earth. It has a mass of 1.3 solar masses and a radius of 1.331 solar radii; a companion, GJ527B, orbits at a distance of 240 AU. Tau Boötis b, the sole planet discovered in the system, orbits at a distance of 0.046 AU every 3.31 days. Discovered through radial velocity measurements, it has a mass of 5.95 Jupiter masses. This makes it a hot Jupiter. The host star and planet are tidally locked, meaning that the planet's orbit and the star's particularly high rotation are synchronized. Furthermore, a slight variability in the host star's light may be caused by magnetic interactions with the planet. Carbon monoxide is present in the planet's atmosphere. Tau Boötis b does not transit its star, rather, its orbit is inclined 46 degrees. Like Tau Boötis b, HAT-P-4 b is also a hot Jupiter. It is noted for orbiting a particularly metal-rich host star and being of low density. Discovered in 2007, HAT-P-4 b has a mass of 0.68 Jupiter masses and a radius of 1.27 Jupiter radii. It orbits every 3.05 days at a distance of 0.04 AU. HAT-P-4, the host star, is an F-type star of magnitude 11.2, 310 parsecs from Earth. It is larger than the Sun, with a mass of 1.26 solar masses and a radius of 1.59 solar radii.
Boötes is also home to multiple-planet systems. HD 128311 is the host star for a two-planet system, consisting of HD 128311 b and HD 128311 c, discovered in 2002 and 2005, respectively. HD 128311 b is the smaller planet, with a mass of 2.18 Jupiter masses; it was discovered through radial velocity observations. It orbits at almost the same distance as Earth, at 1.099 AU; however, its orbital period is significantly longer at 448.6 days. The larger of the two, HD 128311 c, has a mass of 3.21 Jupiter masses and was discovered in the same manner. It orbits every 919 days inclined at 50°, and is 1.76 AU from the host star. The host star, HD 128311, is a K0V-type star located 16.6 parsecs from Earth. It is smaller than the Sun, with a mass of 0.84 solar masses and a radius of 0.73 solar radii; it also appears below the threshold of naked-eye visibility at an apparent magnitude of 7.51.
There are several single-planet systems in Boötes. HD 132406 is a sun-like star of spectral type G0V with an apparent magnitude of 8.45, 231.5 light-years from Earth. It has a mass of 1.09 solar masses and a radius of 1 solar radius. The star is orbited by a gas giant, HD 132406 b, discovered in 2007. HD 132406 orbits 1.98 AU from its host star with a period of 974 days and has a mass of 5.61 Jupiter masses. The planet was discovered by the radial velocity method. WASP-23 is a star with one orbiting planet, WASP-23 b. The planet, discovered by the transit method in 2010, orbits every 2.944 very close to its sun, at 0.0376 AU. It is smaller than Jupiter, at 0.884 Jupiter masses and 0.962 Jupiter radii. Its star is a K1V type star of apparent magnitude 12.7, far below naked-eye visibility, and smaller than the Sun at 0.78 solar masses and 0.765 solar radii. HD 131496 is also encircled by one planet, HD 131496 b. The star is of type K0 and is located 110 parsecs from Earth; it appears at a visual magnitude of 7.96. It is significantly larger than the Sun, with a mass of 1.61 solar masses and a radius of 4.6 solar radii. Its one planet, discovered in 2011 by the radial velocity method, has a mass of 2.2 Jupiter masses; its radius is as yet undetermined. HD 131496 b orbits at a distance of 2.09 AU with a period of 883 days.
Another single planetary system in Boötes is the HD 132563 system, a triple star system. The parent star, technically HD 132563B, is a star of magnitude 9.47, 96 parsecs from Earth. It is almost exactly the size of the sun, with the same radius and a mass only 1% greater. Its planet, HD 132563B b, was discovered in 2011 by the radial velocity method. 1.49 times the mass of Jupiter, it orbits 2.62 AU from its star with a period of 1544 days. Its orbit is somewhat elliptical, with an eccentricity of 0.22. HD 132563B b is one of very few planets found in triple star systems; it orbits the isolated member of the system, which is separated from the other components, a spectroscopic binary, by 400 AU. Also discovered through the radial velocity method, albeit a year earlier, is HD 136418 b, a 2-Jupiter mass planet that orbits the star HD 136418 at a distance of 1.32 AU with a period of 464.3 days. Its host star is a magnitude 7.88 G5-type star, 98.2 parsecs from Earth. It has a radius of 3.4 solar radii and a mass of 1.33 solar masses.
WASP-14 b is one of the most massive and dense exoplanets known, with a mass of 7.341 Jupiter masses and a radius of 1.281 Jupiter radii. Discovered via the transit method, it orbits 0.036 AU from its host star with a period of 2.24 days. WASP-14 b has a density of 4.6 grams per cubic centimeter, making it one of the densest exoplanets known. Its host star, WASP-14, is an F5V-type star of magnitude 9.75, 160 parsecs from Earth. It has a radius of 1.306 solar radii and a mass of 1.211 solar masses. It also has a very high proportion of lithium.
Deep-sky objects.
Boötes is in a part of the celestial sphere facing away from the plane of our home Milky Way galaxy, and so does not have open clusters or nebulae. Instead, it has one bright globular cluster and many faint galaxies. The globular cluster NGC 5466 has an overall magnitude of 9.1 and a diameter of 11 arcminutes. It is a very loose globular cluster with fairly few stars and may appear as a rich, concentrated open cluster in a telescope. NGC 5466 is classified as a Shapley-Sawyer Concentration Class 12 cluster, reflecting its sparsity. Its fairly large diameter means that it has a low surface brightness, so it appears far dimmer than the catalogued magnitude of 9.1 and requires a large amateur telescope to view. Only approximately 12 stars are resolved by an amateur instrument.
Boötes has two bright galaxies. NGC 5248 (Caldwell 45) is a type Sc galaxy (a variety of spiral galaxy) of magnitude 10.2. It measures 6.5 by 4.9 arcminutes. 50 million light-years from Earth, NGC 5248 is a member of the Virgo Cluster of galaxies; it has dim outer arms and obvious H II regions, dust lanes, and young star clusters. NGC 5676 is another type Sc galaxy of magnitude 10.9. It measures 3.9 by 2.0 arcminutes. Other galaxies include NGC 5008, a type Sc emission-line galaxy, NGC 5548, a type S Seyfert galaxy, NGC 5653, a type S HII galaxy, NGC 5778 (also classified as NGC 5825), a type E galaxy that is the brightest of its cluster, NGC 5886, and NGC 5888, a type SBb galaxy. NGC 5698 is a barred spiral galaxy, notable for being the host of the 2005 supernova SN 2005bc, which peaked at magnitude 15.3.
Further away lies the 250 million light-year diameter Boötes void, a huge space largely empty of galaxies. Discovered by Robert Kirshner and colleagues in 1981, it is roughly 700 million light years from Earth. Beyond it and within the bounds of the constellation, lie two superclusters at around 830 million and 1 billion light years distant.
Meteor showers.
Boötes is home to the Quadrantid meteor shower, the most prolific annual meteor shower. It was discovered in January 1835 and named in 1864 by Alexander Hershell. The radiant is located in northern Boötes near Kappa Boötis, in its namesake former constellation of Quadrans Muralis. Quadrantid meteors are dim, but have a peak visible hourly rate of approximately 100 per hour on January 3–4. The zenithal hourly rate of the Quadrantids is approximately 130 meteors per hour at their peak; it is also a very narrow shower. The Quadrantids are notoriously difficult to observe because of a low radiant and often inclement weather. The parent body of the meteor shower has been disputed for decades; however, Peter Jenniskens has proposed 2003 EH1, a minor planet, as the parent. 2003 EH1 may be linked to C/1490 Y1, a comet previously thought to be a potential parent body for the Quadrantids. 2003 EH1 is a short-period comet of the Jupiter family; 500 years ago, it experienced a catastrophic breakup event. It is now dormant. The Quadrantids had notable displays in 1982, 1985, and 2004. Meteors from this shower often appear to have a blue hue and travel at a moderate speed of 41.5–43 kilometers per second.
On April 28, 1984, a remarkable outburst of the normally placid Alpha Bootids was observed by visual observer Frank Witte from 00:00 to 2:30 UTC. In a 6 cm telescope, he observed 433 meteors in a field of view near Arcturus with a diameter of less than 1°. Peter Jenniskens comments that this outburst resembled a "typical dust trail crossing". The Alpha Bootids normally begin on April 14, peaking on April 27th and 28th, and finishing on May 12. Its meteors are slow-moving, with a velocity of 20.9 kilometers per second. They may be related to Comet 73P/Schwassmann-Wachmann 3, but this connection is only theorized.
The June Bootids, also known as the Iota Draconids, is a meteor shower associated with the comet 7P/Pons-Winnecke, first recognized on May 27, 1916 by William F. Denning. The shower, with its slow meteors, was not observed prior to 1916 because Earth did not cross the comet's dust trail until Jupiter perturbed Pons-Winnecke's orbit, causing it to come within 0.03 AU of Earth's orbit the first year the June Bootids were observed. In 1982, E. A. Reznikov discovered that the 1916 outburst was caused by material released from the comet in 1819. Another outburst of the June Bootids was not observed until 1998, because Comet Pons-Winnecke's orbit was not in a favorable position. However, on June 27, 1998, an outburst of meteors radiating from Boötes, later confirmed to be associated with Pons-Winnecke, was observed. They were incredibly long-lived, with trails of the brightest meteors lasting several seconds at times. Many fireballs, green-hued trails, and even some meteors that cast shadows were observed throughout the outburst, which had a maximum zenithal hourly rate of 200–300 meteors per hour. In 2002, two Russian astronomers determined that material ejected from the comet in 1825 was responsible for the 1998 outburst. Ejecta from the comet dating to 1819, 1825, and 1830 was predicted to enter Earth's atmosphere on June 23, 2004. The predictions of a shower less spectacular than the 1998 showing were borne out in a display that had a maximum zenithal hourly rate of 16–20 meteors per hour that night. The June Bootids are not expected to have another outburst in the next 50 years. Typically, only 1–2 dim, very slow meteors are visible per hour; the average June Bootid has a magnitude of 5.0. It is related to the Alpha Draconids and the Bootids-Draconids. The shower lasts from June 27 to July 5, with a peak on the night of June 28. The June Bootids are classified as a class III shower (variable), and has an average entry velocity of 18 kilometers per second. Its radiant is located 7 degrees north of Beta Boötis.
The Beta Bootids is a weak shower that begins on January 5, peaks on January 16, and ends on January 18. Its meteors travel at 43 kilometers/second. The January Bootids is a short, young meteor shower that begins on January 9, peaks from January 16 to January 18, and ends on January 18th. The Phi Bootids is another weak shower radiating from Boötes. It begins on April 16, peaks on April 30 and May 1, and ends on May 12. Its meteors are slow-moving, with a velocity of 15.1 km/s. They were discovered in 2006. The shower's peak hourly rate can be as high as 6 meteors per hour. Though named for a star in Boötes, the Phi Bootid radiant has moved into Hercules. The meteor stream is associated with three different asteroids: 1620 Geographos, 2062 Aten, and 1978 CA. The Lambda Bootids, part of the Bootid-Coronae Borealid Complex, are a weak annual shower with moderately fast meteors; 41.75 km/s. The complex includes the Lambda Bootids, as well as the Theta Coronae Borealids and Xi Coronae Borealids. All of the Bootid-Coronae Borealid showers are Jupiter family comet showers; the streams in the complex have highly inclined orbits.
There are several minor showers in Boötes, some of whose existence is yet to be verified. The Rho Bootids radiate from near the namesake star, and were hypothesized in 2010. The average Rho Bootid has an entry velocity of 43 km/s. It peaks in November and lasts for 3 days. The Rho Bootid shower is part of the SMA complex, a group of meteor showers related to the Taurids, which is in turn linked to the comet 2P/Encke. However, the link to the Taurid shower remains unconfirmed and may be a chance correlation. Another such shower is the Gamma Bootids, which were hypothesized in 2006. Gamma Bootids have an entry velocity of 50.3 km/s. The Nu Bootids, hypothesized in 2012, have faster meteors, with an entry velocity of 62.8 km/s.
References.
Citations
References

</doc>
<doc id="4203" url="http://en.wikipedia.org/wiki?curid=4203" title="Bernardino Ochino">
Bernardino Ochino

Bernardino Ochino (1487–1564) was an Italian, who was raised a Roman Catholic and later turned to Protestantism.
Biography.
Bernardino Ochino was born in Siena, the son of the barber Domenico Ochino, and at the age of 7 or 8, in around 1504, was entrusted to the order of Franciscan Friars. From 1510 he studied medicine at Perugia.
Transfer to the Capuchins.
At the age of 38, Ochino transferred himself in 1534 to the newly founded Order of Friars Minor Capuchin. By then he was the close friend of Juan de Valdés, Pietro Bembo, Vittoria Colonna, Pietro Martire, Carnesecchi. In 1538 he was elected vicar-general of his order. In 1539, urged by Bembo, he visited Venice and delivered a course of sermons showing a sympathy with justification by faith, which appeared more clearly in his "Dialogues" published the same year. He was suspected and denounced, but nothing ensued until the establishment of the Inquisition in Rome in June 1542, at the instigation of Cardinal Giovanni Pietro Carafa. Ochino received a citation to Rome, and set out to obey it about the middle of August. According to his own statement, he was deterred from presenting himself at Rome by the warnings of Cardinal Contarini, whom he found at Bologna, dying of poison administered by the reactionary party.
Escape to Geneva.
Ochino turned aside to Florence, and after some hesitation went across the Alps to Geneva. He was cordially received by John Calvin, and published within two years several volumes of "Prediche", controversial tracts rationalizing his change of religion. He also addressed replies to marchioness Vittoria Colonna, Claudio Tolomei, and other Italian sympathizers who were reluctant to go to the same length as himself. His own breach with the Roman Catholic Church was final.
Augsburg and England.
In 1545 Ochino became minister of the Italian Protestant congregation at Augsburg. From this time dates his contact with Caspar Schwenckfeld. He was compelled to flee when, in January 1547, the city was occupied by the imperial forces for the Diet of Augsburg.
Ochino found asylum in England, where he was made a prebendary of Canterbury Cathedral, received a pension from Edward VI's privy purse, and composed his major work, the "Tragoedie or Dialoge of the unjuste usurped primacie of the Bishop of Rome". This text, originally written in Latin, is extant only in the 1549 translation of Bishop John Ponet. The form is a series of dialogues. Lucifer, enraged at the spread of Jesus's kingdom, convokes the fiends in council, and resolves to set up the pope as antichrist. The state, represented by the emperor Phocas, is persuaded to connive at the pope's assumption of spiritual authority; the other churches are intimidated into acquiescence; Lucifer's projects seem fully accomplished, when Heaven raises up Henry VIII of England and his son for their overthrow.
Several of Ochino's "Prediche" were translated into English by Anna Cooke; and he published numerous controversial treatises on the Continent.
Zürich.
In 1553 the accession of Mary I drove Ochino from England. He went to Basel, where Lelio Sozzini and the lawyer Martino Muralto were sent to secure Ochino as pastor of the Italian church at Zürich, which Ochino accepted. The Italian congregation there was composed mainly of refugees from Locarno. There for 10 years Ochino wrote books which gave increasing evidence of his alienation from the orthodoxy around him. The most important of these was the "Labyrinth", a discussion of the freedom of the will, covertly undermining the Calvinistic doctrine of predestination.
In 1563 a long simmering storm burst on Ochino with the publication of his "Thirty Dialogues", in one of which his adversaries maintained that he had justified polygamy under the disguise of a pretended refutation. His dialogues on divorce and against the Trinity were also considered heretical.
Poland, and death.
Ochino was not given opportunity to defend himself, and was banished from Zürich. After being refused admission by other Protestant cities, he directed his steps towards Poland, at that time the most tolerant state in Europe. He had not resided there long when an edict appeared (August 8, 1564) banishing all foreign dissidents. Fleeing the country, he encountered the plague at Pińczów; three of his four children were carried off; and he himself, worn out by misfortune, died in solitude and obscurity at Slavkov in Moravia, about the end of 1564.
Legacy.
Ochino's reputation among Protestants was low. He was charged by Thomas Browne in 1643 with the authorship of the legendary-apocryphal heretical treatise "De tribus Impostoribus", as well as with having carried his alleged approval of polygamy into practice.
His biographer Karl Benrath justified him, representing him as a fervent evangelist and at the same time as a speculative thinker with a passion for free inquiry. The picture is of Ochino always learning and unlearning and arguing out difficult questions with himself in his dialogues, frequently without attaining to any absolute conviction. 

</doc>
<doc id="4204" url="http://en.wikipedia.org/wiki?curid=4204" title="Bay of Quinte">
Bay of Quinte

The Bay of Quinte is a long, narrow bay shaped like the letter "Z" on the northern shore of Lake Ontario in the province of Ontario, Canada. It is just west of the head of the Saint Lawrence River that drains the Great Lakes into the Gulf of Saint Lawrence. It is located about east of Toronto and west of Montreal.
The name "Quinte" is derived from "Kente", which was the name of an early French Catholic mission located on the south shore of what is now Prince Edward County. Officially, in the Mohawk language, the community is called "Kenhtè:ke" which means "the place of the bay". The Cayuga name is "Tayęda:ne:gęˀ or Detgayę:da:negęˀ", "land of two logs."
The Bay, as it is known locally, provides some of the best trophy Walleye angling in North America as well as most sport fish common to the great lakes. The bay is subject to algae blooms in late summer which are a naturally occurring phenomenon and do not indicate pollution other than from agricultural runoff. Zebra mussels as well as the other invasive species found in the great lakes are present.
The Quinte area played a vital role in bootlegging during Prohibition in the United States, with large volumes of booze being produced in the area, and shipped via boat on the Bay to Lake Ontario finally arriving in New York State where it was distributed. Illegal sales of liquor accounted for many fortunes in and around Belleville.
Tourism in the area is significant, especially in the summer months due to the Bay of Quinte and its fishing, local golf courses, provincial parks, and wineries.
Geography.
The northern side of the bay is defined by Ontario's mainland, while the southern side follows the shore of the Prince Edward County headland. Beginning in the east with the outlet to Lake Ontario, the bay runs west-southwest for to Picton (although this section is also called Adolphus Reach), where it turns north-northwest for another as far as Deseronto. From there it turns south-southwest again for another , running past Big Island on the south and Belleville on the north. The width of the bay rarely exceeds two kilometers. The bay ends at Trenton (Quinte West) and the Trent River, both also on the north side. The Murray Canal has been cut through the "Carrying Place", the few miles separating the end of the bay and Lake Ontario on the west side. The Trent River is part of the Trent-Severn Waterway, a canal connecting Lake Ontario to Lake Simcoe and then Georgian Bay on Lake Huron.
There are several sub-bays off the Bay of Quinte, including Hay Bay, Big Bay, and Muscote Bay.
Quinte Region.
Quinte is also a region comprising several communities situated along the Bay of Quinte, including Belleville which is the largest city in the Quinte Region, and represents a midpoint between Montreal, Ottawa, and Toronto.
The Greater Bay of Quinte area includes the municipalities of Brighton, Quinte West, Belleville, Prince Edward County, and Greater Napanee as well as the Native Tyendinaga Mohawk Territory. Overall population of the area exceeds 200,000.
Mohawks of the Bay of Quinte.
The Mohawks of the Bay of Quinte (Kenhtè:ke Kanyen'kehá:ka) on traditional Tyendinaga Mohawk Territory. Their reserve Band number 244, their current land base, is a 73 km² (18000-acre) on the Bay of Quinte in southeastern Ontario, Canada, east of Belleville and immediately to the west of Deseronto.
The community takes its name from a variant spelling of Mohawk leader Joseph Brant's traditional Mohawk name, Thayendanegea (standardized spelling Thayentiné:ken), which means 'two pieces of fire wood beside each other'. Officially, in the Mohawk language, the community is called "Kenhtè:ke" (Tyendinaga) which means "on the bay" the birthplace of Tekanawí:ta. The Cayuga name is Tyendinaga, "Tayęda:ne:gęˀ or Detgayę:da:negęˀ", "land of two logs.")
Education.
The Quinte Region, specifically the City of Belleville, is home to "Loyalist College of Applied Arts and Technology." Other post-secondary schools in the region include; "Maxwell College of Advanced Technology," "CDI College," "Ontario Business College," and "Quinte Literacy." Secondary Schools in the region include "Albert College" (private school) and "Sir James Whitney" (a school for the deaf and severely hearing-impaired).
Industry and employment.
The Quinte Region is home to a large number of national and international food processing manufacturers. Quinte also houses a large number of industries in the plastics & packaging sector, transportation sector, logistics sector and advanced manufacturing sector, including the following (just a few of over 350 industries located in the Bay of Quinte Region) :

</doc>
<doc id="4207" url="http://en.wikipedia.org/wiki?curid=4207" title="Bassoon">
Bassoon

The bassoon is a woodwind instrument in the double reed family that typically plays music written in the bass and tenor clefs, and occasionally the treble. Appearing in its modern form in the 19th century, the bassoon figures prominently in orchestral, concert band, and chamber music literature. The bassoon is a non-transposing instrument known for its distinctive tone color, wide range, variety of character and agility. Listeners often compare its warm, dark, reedy timbre to that of a male baritone voice. Someone who plays the bassoon is called a bassoonist.
Etymology.
The word bassoon comes from French "basson" and from Italian "bassone" ("basso" with the augmentative suffix "-one").
Range.
The range of the bassoon begins at B1 (the first one below the bass staff) and extends upward over three octaves, roughly to the G above the treble staff (G5). Higher notes are possible but difficult to produce, and rarely called for: orchestral and concert band parts rarely go higher than C5 or D5. Even Stravinsky's famously difficult opening solo in "The Rite of Spring" only ascends to D5.
A1 is possible with a special extension to the instrument—see "Extended techniques" below.
Construction.
The bassoon disassembles into six main pieces, including the reed. The bell (6), extending upward; the bass joint (or long joint) (5), connecting the bell and the boot; the boot (or butt) (4), at the bottom of the instrument and folding over on itself; the wing joint (3), which extends from boot to bocal; and the bocal (or crook) (2), a crooked metal tube that attaches the wing joint to a reed (1) (). Bassoons are double reed instruments like the oboe and the English horn.
A modern beginner's bassoon is generally made of maple, with medium-hardness types such as sycamore maple and sugar maple preferred. Less-expensive models are also made of materials such as polypropylene and ebonite, primarily for student and outdoor use; metal bassoons were made in the past but have not been produced by any major manufacturer since 1889. The bore of the bassoon is conical, like that of the oboe and the saxophone, and the two adjoining bores of the boot joint are connected at the bottom of the instrument with a U-shaped metal connector. Both bore and tone holes are precision-machined, and each instrument is finished by hand for proper tuning. The walls of the bassoon are thicker at various points along the bore; here, the tone holes are drilled at an angle to the axis of the bore, which reduces the distance between the holes on the exterior. This ensures coverage by the fingers of the average adult hand. Wooden instruments are lined with hard rubber along the interior of the wing and boot joints to prevent damage from moisture; wooden instruments are also stained and varnished. The end of the bell is usually fitted with a ring, either of metal, plastic or ivory. The joints between sections consist of a tenon fitting into a socket; the tenons are wrapped in either cork or string as a seal against air leaks. The bocal connects the reed to the rest of the instrument and is inserted into a socket at the top of the wing joint. Bocals come in many different lengths and styles, depending on the desired tuning and playing characteristics.
Folded upon itself, the bassoon stands tall, but the total sounding length is . Playing is facilitated by doubling the tube back on itself and by closing the distance between the widely spaced holes with a complex system of key work, which extends throughout nearly the entire length of the instrument. There are also short-reach bassoons made for the benefit of young or petite players.
Development.
Early history.
Music historians generally consider the dulcian to be the forerunner of the modern bassoon, as the two instruments share many characteristics: a double reed fitted to a metal crook, obliquely drilled tone holes and a conical bore that doubles back on itself. The origins of the dulcian are obscure, but by the mid-16th century it was available in as many as eight different sizes, from soprano to great bass. A full consort of dulcians was a rarity; its primary function seems to have been to provide the bass in the typical wind band of the time, either loud (shawms) or soft (recorders), indicating a remarkable ability to vary dynamics to suit the need. Otherwise, dulcian technique was rather primitive, with eight finger holes and two keys, indicating that it could play in only a limited number of key signatures.
The dulcian came to be known as "fagotto" in Italy. However, the usual etymology that equates "fagotto" with "bundle of sticks" is somewhat misleading, as the latter term did not come into general use until later. Some think it may resemble the Roman Fasces, a standard of bound sticks with an ax. A further discrepancy lies in the fact that the dulcian was carved out of a single block of wood—in other words, a single "stick" and not a bundle.
Circumstantial evidence indicates that the baroque bassoon was a newly invented instrument, rather than a simple modification of the old dulcian. The dulcian was not immediately supplanted, but continued to be used well into the 18th century by Bach and others. The man most likely responsible for developing the true bassoon was Martin Hotteterre (d.1712), who may also have invented the three-piece "flûte traversière" and the "hautbois" (baroque oboe). Some historians believe that sometime in the 1650s, Hotteterre conceived the bassoon in four sections (bell, bass joint, boot and wing joint), an arrangement that allowed greater accuracy in machining the bore compared to the one-piece dulcian. He also extended the compass down to B by adding two keys. An alternate view maintains Hotteterre was one of several craftsmen responsible for the development of the early bassoon. These may have included additional members of the Hotteterre family, as well as other French makers active around the same time. No original French bassoon from this period survives, but if it did, it would most likely resemble the earliest extant bassoons of Johann Christoph Denner and Richard Haka from the 1680s. Sometime around 1700, a fourth key (G♯) was added, and it was for this type of instrument that composers such as Antonio Vivaldi, Bach, and Georg Philipp Telemann wrote their demanding music. A fifth key, for the low E, was added during the first half of the 18th century. Notable makers of the 4-key and 5-key baroque bassoon include J.H. Eichentopf (c. 1678–1769), J. Poerschmann (1680–1757), Thomas Stanesby, Jr. (1668–1734), G.H. Scherer (1703–1778), and Prudent Thieriot (1732–1786).
Modern history.
Increasing demands on capabilities of instruments and players in the 19th century—particularly larger concert halls requiring greater volume and the rise of virtuoso composer-performers—spurred further refinement. Increased sophistication, both in manufacturing techniques and acoustical knowledge, made possible great improvements in the instrument's playability.
The modern bassoon exists in two distinct primary forms, the Buffet system and the Heckel system. Most of the world plays the Heckel system, while the Buffet system is primarily played in France, Belgium, and parts of Latin America.
Heckel (German) system.
The design of the modern bassoon owes a great deal to the performer, teacher, and composer Carl Almenräder. Assisted by the German acoustic researcher Gottfried Weber, he developed the 17-key bassoon with a range spanning four octaves. Almenräder's improvements to the bassoon began with an 1823 treatise describing ways of improving intonation, response, and technical ease of playing by augmenting and rearranging the keywork. Subsequent articles further developed his ideas. His employment at Schott gave him the freedom to construct and test instruments according to these new designs, and he published the results in "Caecilia", Schott's house journal. Almenräder continued publishing and building instruments until his death in 1846, and Ludwig van Beethoven himself requested one of the newly made instruments after hearing of the papers. In 1831, Almenräder left Schott to start his own factory with a partner, Johann Adam Heckel.
Heckel and two generations of descendants continued to refine the bassoon, and their instruments became the standard, with other makers following. Because of their superior singing tone quality (an improvement upon one of the main drawbacks of the Almenräder instruments), the Heckel instruments competed for prominence with the reformed Wiener system, a Boehm-style bassoon, and a completely keyed instrument devised by Charles-Joseph Sax, father of Adolphe Sax. F.W. Kruspe implemented a latecomer attempt in 1893 to reform the fingering system, but it failed to catch on. Other attempts to improve the instrument included a 24-keyed model and a single-reed mouthpiece, but both these had adverse effects on tone and were abandoned.
Coming into the 20th century, the Heckel-style German model of bassoon dominated the field. Heckel himself had made over 1,100 instruments by the turn of the 20th century (serial numbers begin at 3,000), and the British makers' instruments were no longer desirable for the changing pitch requirements of the symphony orchestra, remaining primarily in military band use.
Except for a brief 1940s wartime conversion to ball bearing manufacture, the Heckel concern has produced instruments continuously to the present day. Heckel bassoons are considered by many to be the best, although a range of Heckel-style instruments is available from several other manufacturers, all with slightly different playing characteristics. Companies that manufacture Heckel-system bassoons include: Wilhelm Heckel, Yamaha, Fox Products, W. Schreiber & Söhne, Püchner, Conn-Selmer, Linton, Moosmann Kohlert, Moennig/Adler, B.H. Bell, Walter, Leitzinger and Guntram Wolf. In addition, several factories in the People's Republic of China are producing inexpensive instruments under such labels as Laval, Haydn, and Lark, and these have been available in the West for some time now. However, they are generally of marginal quality and are usually avoided by serious players.
Because its mechanism is primitive compared to most modern woodwinds, makers have occasionally attempted to "reinvent" the bassoon. In the 1960s, Giles Brindley began to develop what he called the "logical bassoon," which aimed to improve intonation and evenness of tone through use of an electrically activated mechanism, making possible key combinations too complex for the human hand to manage. Brindley's logical bassoon was never marketed.
Buffet (French) system.
The Buffet system bassoon achieved its basic acoustical properties somewhat earlier than the Heckel. Thereafter it continued to develop in a more conservative manner. While the early history of the Heckel bassoon included a complete overhaul of the instrument in both acoustics and keywork, the development of the Buffet system consisted primarily of incremental improvements to the keywork. This minimalist approach deprived the Buffet of the improved consistency, ease of operation, and increased power found in the Heckel bassoons, but the Buffet is considered by some to have a more vocal and expressive quality. The conductor John Foulds lamented in 1934 the dominance of the Heckel-style bassoon, considering them too homogeneous in sound with the horn.
Compared to the Heckel bassoon, Buffet system bassoons have a narrower bore and simpler mechanism, requiring different fingerings for many notes. Switching between Heckel and Buffet requires extensive retraining. Buffet instruments are known for a reedier sound and greater facility in the upper registers, reaching e" and f" with far greater ease and less air pressure. French woodwind tone in general exhibits a certain amount of "edge," with more of a vocal quality than is usual elsewhere, and the Buffet bassoon is no exception. This type of sound can be beneficial in music by French composers, but has drawn criticism for being too intrusive. As with all bassoons, the tone varies considerably, depending on individual instrument and performer. In the hands of a lesser player, the Heckel bassoon can sound flat and woody, but good players succeed in producing a vibrant, singing tone. Conversely, a poorly played Buffet can sound buzzy and nasal, but good players succeed in producing a warm, expressive sound, different from—but not inferior to—the Heckel.
Though the United Kingdom once favored the French system, Buffet-system instruments are no longer made there and the last prominent British player of the French system retired in the 1980s. However, with continued use in some regions and its distinctive tone, the Buffet continues to have a place in modern bassoon playing, particularly in France. Buffet-model bassoons are currently made in Paris by Buffet Crampon and The Selmer Company. Some players, for example the late Gerald Corey in Canada, have learned to play both types and will alternate between them depending on the repertoire.
Use in ensembles.
Earlier ensembles.
Orchestras first used the bassoon to reinforce the bass line, and as the bass of the double reed choir (oboes and taille). Baroque composer Jean-Baptiste Lully and his "Les Petits Violons" included oboes and bassoons along with the strings in the 16-piece (later 21-piece) ensemble, as one of the first orchestras to include the newly invented double reeds. Antonio Cesti included a bassoon in his 1668 opera "Il pomo d'oro" (The Golden Apple). However, use of bassoons in concert orchestras was sporadic until the late 17th century when double reeds began to make their way into standard instrumentation. This was largely due to the spread of the "hautbois" to countries outside of France. Increasing use of the bassoon as a "basso continuo" instrument meant that it began to be included in opera orchestras, first in France and later in Italy, Germany and England. Meanwhile, composers such as Joseph Bodin de Boismortier, Michel Corrette, Johann Ernst Galliard, Jan Dismas Zelenka, Johann Friedrich Fasch and Telemann wrote demanding solo and ensemble music for the instrument. Antonio Vivaldi brought the bassoon to prominence by featuring it in 37 concerti for the instrument.
By the mid-18th century, the bassoon's function in the orchestra was still mostly limited to that of a continuo instrument—since scores often made no specific mention of the bassoon, its use was implied, particularly if there were parts for oboes or other winds. Beginning in the early Rococo era, composers such as Joseph Haydn, Michael Haydn, Johann Christian Bach, Giovanni Battista Sammartini and Johann Stamitz included parts that exploited the bassoon for its unique color, rather than for its perfunctory ability to double the bass line. Orchestral works with fully independent parts for the bassoon would not become commonplace until the Classical era. Wolfgang Amadeus Mozart's "Jupiter" symphony is a prime example, with its famous bassoon solos in the first movement. The bassoons were generally paired, as in current practice, though the famed Mannheim orchestra boasted four.
Another important use of the bassoon during the Classical era was in the "Harmonie", a chamber ensemble consisting of pairs of oboes, horns and bassoons; later, two clarinets would be added to form an octet. The "Harmonie" was an ensemble maintained by German and Austrian noblemen for private music-making, and was a cost-effective alternative to a full orchestra. Haydn, Mozart, Ludwig van Beethoven and Franz Krommer all wrote considerable amounts of music for the "Harmonie".
Modern ensembles.
The modern symphony orchestra typically calls for two bassoons, often with a third playing the contrabassoon. Some works call for four or more players. The first player is frequently called upon to perform solo passages. The bassoon's distinctive tone suits it for both plaintive, lyrical solos such as Maurice Ravel's "Boléro" and more comical ones, such as the grandfather's theme in "Peter and the Wolf". Its agility suits it for passages such as the famous running line (doubled in the violas and cellos) in the overture to "The Marriage of Figaro". In addition to its solo role, the bassoon is an effective bass to a woodwind choir, a bass line along with the cellos and double basses, and harmonic support along with the French horns.
A wind ensemble will usually also include two bassoons and sometimes contrabassoon, each with independent parts; other types of concert wind ensembles will often have larger sections, with many players on each of first or second parts; in simpler arrangements there will be only one bassoon part and no contrabassoon. The bassoon's role in the concert band is similar to its role in the orchestra, though when scoring is thick it often cannot be heard above the brass instruments also in its range. "La Fiesta Mexicana", by H. Owen Reed, features the instrument prominently, as does the transcription of Malcolm Arnold's "Four Scottish Dances", which has become a staple of the concert band repertoire.
The bassoon is also part of the standard wind quintet instrumentation, along with the flute, oboe, clarinet, and horn; it is also frequently combined in various ways with other woodwinds. Richard Strauss's "Duet-Concertino" pairs it with the clarinet as "concertante" instruments, with string orchestra in support.
The bassoon quartet has also gained favor in recent times. The bassoon's wide range and variety of tone colors make it ideally suited to grouping in like-instrument ensembles. Peter Schickele's "Last Tango in Bayreuth" (after themes from "Tristan und Isolde") is a popular work; Schickele's fictional alter ego P. D. Q. Bach exploits the more humorous aspects with his quartet "Lip My Reeds," which at one point calls for players to perform on the reed alone. It also calls for a low A at the very end of the prelude section in the fourth bassoon part. It is written so that the first bassoon does not play; instead, his or her role is to place an extension in the bell of the fourth bassoon so that the note can be played.
Jazz.
The bassoon is infrequently used as a jazz instrument and rarely seen in a jazz ensemble. It first began appearing in the 1920s, including specific calls for its use in Paul Whiteman's group, the unusual octets of Alec Wilder, and a few other session appearances. The next few decades saw the instrument used only sporadically, as symphonic jazz fell out of favor, but the 1960s saw artists such as Yusef Lateef and Chick Corea incorporate bassoon into their recordings; Lateef's diverse and eclectic instrumentation saw the bassoon as a natural addition, while Corea employed the bassoon in combination with flautist Hubert Laws.
More recently, Illinois Jacquet, Ray Pizzi, Frank Tiberi, and Marshall Allen have both doubled on bassoon in addition to their saxophone performances. Bassoonist Karen Borca, a performer of free jazz, is one of the few jazz musicians to play only bassoon; Michael Rabinowitz, the Spanish bassoonist Javier Abad, and James Lassen, an American resident in Bergen, Norway, are others. Katherine Young plays the bassoon in the ensembles of Anthony Braxton. Lindsay Cooper, Paul Hanson, the Brazilian bassoonist Alexandre Silverio, Trent Jacobs and Daniel Smith are also currently using the bassoon in jazz. French bassoonists Jean-Jacques Decreux and Alexandre Ouzounoff have both recorded jazz, exploiting the flexibility of the Buffet system instrument to good effect.
Popular music.
The bassoon is even rarer as a regular member of rock bands. However, several 1960s pop music hits feature the bassoon, including "The Tears of a Clown" by Smokey Robinson and the Miracles (the bassoonist was Charles R. Sirard), "Jennifer Juniper" by Donovan, "The Turtles" "Happy Together"(third verse,overdub), "59th Street Bridge Song" by Harpers Bizarre, and the oompah bassoon underlying The New Vaudeville Band's "Winchester Cathedral". From 1974 to 1978, the bassoon was played by Lindsay Cooper in the British avant-garde band Henry Cow. In the 1970s it was played, in the British medieval/progressive rock band Gryphon, by Brian Gulland, as well as by the American band Ambrosia, where it was played by drummer Burleigh Drummond. The Belgian Rock in Opposition-band Univers Zero is also known for its use of the bassoon.
In the 1990s, Madonna Wayne Gacy provided bassoon for the alternative metal band Marilyn Manson as did Aimee DeFoe, in what is self-described as "grouchily lilting garage bassoon" in the indie-rock band Blogurt from Pittsburgh, Pennsylvania. More recently, These New Puritans's 2010 album Hidden makes heavy use of the instrument throughout; their principal songwriter, Jack Barnett, claimed repeatedly to be "writing a lot of music for bassoon" in the run-up to its recording. In early 2011, American hip-hop artist Kanye West updated his Twitter account to inform followers that he recently added the bassoon to a yet unnamed song.
The rock band Better Than Ezra took their name from a passage in Ernest Hemingway's "A Moveable Feast" in which the author comments that listening to an annoyingly talkative person is still “better than Ezra learning how to play the bassoon,” referring to Ezra Pound.
British psych rock/prog rock band Knifeworld features the bassoon playing of Chloe Herrington, who also plays for experimental chamber rock orchestra Chrome Hoof.
Technique.
The bassoon is held diagonally in front of the player, but unlike the flute, oboe and clarinet, it cannot be supported by the player's hands alone. Some means of additional support is required; the most common ones are a seat strap attached to the base of the boot joint, which is laid across the chair seat prior to sitting down, or a neck strap or shoulder harness attached to the top of the boot joint. Occasionally a spike similar to those used for the cello or the bass clarinet is attached to the bottom of the boot joint and rests on the floor. It is possible to play while standing up if the player uses a neck strap or similar harness, or if the seat strap is tied to the belt. Sometimes a device called a "balance hanger" is used when playing in a standing position. This is installed between the instrument and the neck strap, and shifts the point of support closer to the center of gravity.
The bassoon is played with both hands in a stationary position, the left above the right, with five main finger holes on the front of the instrument (nearest the audience) plus a sixth that is activated by an open-standing key. Five additional keys on the front are controlled by the little fingers of each hand. The back of the instrument (nearest the player) has twelve or more keys to be controlled by the thumbs, the exact number varying depending on model.
To stabilize the right hand, many bassoonists use an adjustable comma-shaped apparatus called a "crutch," or a hand rest, which mounts to the boot joint. The crutch is secured with a thumb screw, which also allows the distance that it protrudes from the bassoon to be adjusted. Players rest the curve of the right hand where the thumb joins the palm against the crutch. The crutch also keeps the right hand from tiring and enables the player to keep the finger pads flat on the finger holes and keys.
An aspect of bassoon technique not found on any other woodwind is called "flicking". It involves the left hand thumb momentarily pressing, or 'flicking' the high A, C and D keys at the beginning of certain notes in the middle octave. This eliminates cracking, or brief multiphonics that happens without the use of this technique.
Flicking is not universal amongst bassoonists; some American players, principally on the East Coast, use it sparingly, if at all. The rest use it virtually 100% of the time—it has become in essence part of the fingering.
The alternative method is "venting", which requires that the register key be used as part of the full fingering as opposed to being open momentarily at the start of the note.
While flicking is used to higher notes, the whisper key is used for lower notes. From the A right below middle C and lower, the whisper key is pressed with the left thumb and held for the duration of the note. This prevents cracking, as low notes can sometimes crack into a higher octave. Both flicking and using the whisper key is especially important to ensure notes speak properly during slurring between high and low registers.
While bassoons are usually critically tuned at the factory, the player nonetheless has a great degree of flexibility of pitch control through the use of breath support, embouchure, and reed profile. Players can also use alternate fingerings to adjust the pitch of many notes. Similar to other woodwind instruments, the length of the bassoon can be increased to lower pitch or decreased to raise pitch. On the bassoon, this is done preferably by changing the bocal to one of a different length, (lengths are denoted by a number on the bocal, usually starting at 0 for the shortest length, and 3 for the longest, but there are some manufacturers who will use other numbers) but it is possible to push the bocal in or out to adjust the pitch.
Embouchure.
The bassoon embouchure is a very important aspect of producing a full, round bassoon tone, but can be difficult to obtain as a beginner. The bassoon embouchure is made by putting one's lips together as if one were whistling and then dropping the jaw down as in a yawning motion (without actually yawning or opening the mouth). Both sets of teeth should be covered by the lips in order to protect the reed. The reed is then placed in the mouth, forming a seal around the reed with the lips and facial muscles.
Extended techniques.
Many extended techniques can be performed on the bassoon, such as multiphonics, flutter-tonguing, circular breathing, double tonguing, and harmonics. In the case of the bassoon, flutter-tonguing may be accomplished by "gargling" in the back of the throat as well as by the conventional method of rolling Rs.
Also, using certain fingerings, notes may be produced on the instrument that sound lower pitches than the actual range of the instrument. These "impossible notes" tend to sound very gravelly and out of tune, but technically sound below the low B. Alternatively, lower notes can be produced by inserting a small paper or rubber tube into the end of the bell, which converts the lower B into a lower note such as an A natural; this lowers the pitch of the instrument, but has the positive effect of bringing the lowest register (which is typically quite sharp) into tune. A notable piece that calls for the use of a low A bell is Carl Nielsen's Wind Quintet, op. 43, which includes an optional low A for the final cadence of the work. Bassoonists sometimes use the end bell segment of an English horn or clarinet if one is available instead of a specially made extension. This often yields unsatisfactory results, though, as the resultant A can be quite sharp. The idea of using low A was begun by Richard Wagner, who wanted to extend the range of the bassoon. Many passages in his later operas require the low A as well as the B-flat above. (This is impossible on a normal bassoon using an A extension as the fingering for the B-flat yields the low A.) These passages are typically realized on the contrabassoon, as recommended by the composer. Some bassoons have been made to allow bassoonists to realize similar passages. These bassoons are made with a "Wagner bell," which is an extended bell with a key for both the low A and the low B-flat. Bassoons with Wagner bells suffer similar intonational deficiencies as a bassoon with an A extension. Another composer who has required the bassoon to be chromatic down to low A is Gustav Mahler. Richard Strauss also calls for the low A in his opera "Intermezzo".
Learning the bassoon.
The complicated fingering and the problem of reeds make the bassoon more difficult to learn than some of the other woodwind instruments. Cost is another factor in a person's decision to pursue the bassoon. Prices range from $8,000 up to $25,000 for a good-quality instrument. In North America, schoolchildren typically take up bassoon only after starting on another reed instrument, such as clarinet or saxophone.
Students in America often begin to pursue the study of bassoon performance and technique in the middle years of their music education. Students are often provided with a school instrument and encouraged to pursue lessons with private instructors. Students typically receive instruction in proper posture, hand position, embouchure, tone production, and reed making.
Reeds and reed construction.
Modern reeds.
Bassoon reeds, made of "Arundo donax" cane, are often made by the players themselves, although beginner bassoonists tend to buy their reeds from professional reed makers or use reeds made by their teachers. Reeds begin with a length of tube cane that is split into three or four pieces. The cane is then trimmed and "gouged" to the desired thickness, leaving the bark attached. After soaking, the gouged cane is cut to the proper shape and milled to the desired thickness, or "profile", by removing material from the bark side. This can be done by hand with a file; more frequently it is done with a machine or tool designed for the purpose. After the profiled cane has soaked once again it is folded over in the middle. Prior to soaking, the reed maker will have lightly scored the bark with parallel lines with a knife; this ensures that the cane will assume a cylindrical shape during the forming stage. On the bark portion, the reed maker binds on one, two, or three coils or loops of brass wire to aid in the final forming process. The exact placement of these loops can vary somewhat depending on the reed maker. The bound reed blank is then wrapped with thick cotton or linen thread to protect it, and a conical steel mandrel (which sometimes has been heated in a flame) is quickly inserted in between the blades. Using a special pair of pliers, the reed maker presses down the cane, making it conform to the shape of the mandrel. (The steam generated by the heated mandrel causes the cane to permanently assume the shape of the mandrel.) The upper portion of the cavity thus created is called the "throat," and its shape has an influence on the final playing characteristics of the reed. The lower, mostly cylindrical portion will be reamed out with a special tool, allowing the reed to fit on the bocal.
After the reed has dried, the wires are tightened around the reed, which has shrunk after drying, or replaced completely. The lower part is sealed (a nitrocellulose-based cement such as Duco may be used) and then wrapped with thread to ensure both that no air leaks out through the bottom of the reed and that the reed maintains its shape. The wrapping itself is often sealed with Duco or clear nail varnish (polish). The bulge in the wrapping is sometimes referred to as the "Turk's head"—it serves as a convenient handle when inserting the reed on the bocal.
To finish the reed, the end of the reed blank, originally at the center of the unfolded piece of cane, is cut off, creating an opening. The blades above the first wire are now roughly long. For the reed to play, a slight bevel must be created at the tip with a knife, although there is also a machine that can perform this function. Other adjustments with the knife may be necessary, depending on the hardness and profile of the cane and the requirements of the player. The reed opening may also need to be adjusted by squeezing either the first or second wire with the pliers. Additional material may be removed from the sides (the "channels") or tip to balance the reed. Additionally, if the "e" in the staff is sagging in pitch, it may be necessary to "clip" the reed by removing from its length.
Playing styles of individual bassoonists vary greatly; because of this, most advanced players will make their own reeds, in the process customizing them to their individual playing requirements. Many companies and individuals do offer reeds for sale, but even with store-bought reeds, the player must know how to make adjustments to suit his particular playing style.
Early reeds.
Little is known about the early construction of the bassoon reed, as few examples survive, and much of what is known is only what can be gathered from artistic representations. The earliest known written instructions date from the middle of the 17th century, describing the reed as being held together by wire or resined thread; the earliest actual reeds that survive are more than a century younger, a collection of 21 reeds from the late 18th-century Spanish "bajon".

</doc>
<doc id="4210" url="http://en.wikipedia.org/wiki?curid=4210" title="Bipedalism">
Bipedalism

Bipedalism is a form of terrestrial locomotion where an organism moves by means of its two rear limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped , meaning "two feet" (from the Latin "bi" for "two" and "ped" for "foot"). Types of bipedal movement include walking, running, or hopping.
Few modern species are habitual bipeds whose normal method of locomotion is two-legged. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare, hopping mice, pangolins and homininan apes, as well as various other extinct groups evolving the trait independently. In the Triassic period some groups of archosaurs (a group that includes the ancestors of crocodiles) developed bipedalism; among their descendants the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds descended from one group of exclusively bipedal dinosaurs.
A larger number of modern species utilize bipedal movement for a short time. Several non-archosaurian lizard species move bipedally when running, usually to escape from threats. Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment. Several arboreal primate species, such as gibbons and indriids, exclusively utilize bipedal locomotion during the brief periods they spend on the ground. Many animals rear up on their hind legs whilst fighting or copulating. A few animals commonly stand on their hind legs, in order to reach food, to keep watch, to threaten a competitor or predator, or to pose in courtship, but do not move bipedally.
Definition.
The word is derived from the Latin words "bi(s)" 'two (2)' and "ped-" 'foot', as contrasted with quadruped 'four feet'.
Advantages.
Limited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in giant pangolin), combat (in bears and the large monitor lizard) or camouflage (in certain species of octopus). The maximum bipedal speed appears less fast than the maximum speed of quadrupedal movement with a flexible backbone – both the ostrich and the red kangaroo can reach speeds of , while the cheetah can exceed .
Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, which could aid in escaping from predators.
Facultative and obligate bipedalism.
Zoologists often label behaviors, including bipedalism, as "facultative" (i.e. optional) or "obligate" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut — for example, humans normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome — very rare genetic neurological disorders rather than normal behavior. Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that "normal" humans can crawl on hands and knees. This article therefore avoids the terms "facultative" and "obligate", and focuses on the range of styles of locomotion "normally" used by various groups of animals.
Movement.
There are a number of states of movement commonly associated with bipedalism.
Bipedal animals.
The great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to utilize either form of locomotion, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.
Amphibians.
There are no known living or fossil bipedal amphibians.
Extant reptiles.
Many species of lizards become bipedal during high-speed, sprint locomotion, including the world's fastest lizard, the spiny-tailed iguana (genus "Ctenosaura").
Early reptiles and lizards.
The first known biped is the bolosaurid "Eudibamus" whose fossils date from 290 million years ago. Its long hindlegs, short forelegs, and distinctive joints all suggest bipedalism. The species was extinct before the dinosaurs appeared.
Archosaurs (include birds, crocodiles, and dinosaurs).
Birds.
All birds are bipeds when on the ground, a feature inherited from their dinosaur ancestors.
Other archosaurs.
Bipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians. All dinosaurs are believed to be descended from a fully bipedal ancestor, perhaps similar to "Eoraptor". Bipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodons. Some extinct members of the crocodilian line, a sister group to the dinosaurs and birds, also evolved bipedal forms - a crocodile relative from the triassic, "Effigia okeeffeae", is believed to be bipedal. Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion. Bipedalism also evolved independently among the dinosaurs. Dinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of fossils from the early dinosaur genus "Eoraptor" establishes its presence in the fossil record at this time. Paleontologists believe "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.
Mammals.
A number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion - for example humans, giant pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.
Bipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few mammals other than primates commonly move bipedally by an alternating gait rather than hopping. Exceptions are the ground pangolin and in some circumstances the tree kangaroo. 
Primates.
Most bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans). Many primates can stand upright on their hind legs without any support. 
Chimpanzees, bonobos, gibbons and baboons exhibit forms of bipedalism. Injured chimpanzees and bonobos have been capable of sustained bipedalism.
Geladas, although often quadrupedal, will move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion .
Three captive primates, one macaque Natasha and two chimps, Oliver and Poko (chimpanzee), were found to move bipedally . Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage. Oliver reverted to knuckle-walking after developing arthritis.
In addition, non-human primates often use bipedal locomotion when carrying food. One hypothesis for human bipedalism is thus that it evolved as a result of differentially successful survival from carrying food to share with group members, although there are other hypotheses, as discussed below.
Limited bipedalism.
Limited bipedalism in mammals.
Other mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver may also move bipedally if transporting wood for their dams). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will also use limited bipedalism when threatened, rearing up on its forelimbs while facing the attacker so its anal glands, capable of spraying an offensive oil, face its attacker.
Limited bipedalism in non-mammals.
Bipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the 'reared-up' running of lizards such as agamids and monitor lizards. Many reptile species will also temporarily adopt bipedalism while fighting. One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds. Bipedalism is rarely found outside terrestrial animals, though at least two types of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.
Evolution of human bipedalism.
There are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools. Bipedal specializations are found in "Australopithecus" fossils from 4.2-3.9 million years ago. The evolution of bipedalism was accompanied by significant evolutions in the spine including the forward movement in position of the foramen magnum, where the spinal cord leaves the cranium. Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as "Australopithecus africanus". This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make. Adapting bipedalism would have required less shoulder stability, which allowed the shoulder and other limbs to become more independent of each other and adapt for specific suspensory behaviors. In addition to the change in shoulder stability, changing locomotion would have increased the demand for shoulder mobility, which would have propelled the evolution of bipedalism forward. The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.
Possible reasons for the evolution of human bipedalism include freeing the hands for tool use and carrying, sexual dimorphism in food gathering, changes in climate and habitat (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun.
Savanna-based theory.
According to the savanna-based theory, hominines descended from the trees and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. It was also suggested in P.E. Wheeler's "The evolution of bipedality and loss of functional body hair in hominids", that a possible advantage of bipedalism in the savanna was reducing the amount of surface area of the body exposed to the sun, helping regulate body temperature. In fact, Elizabeth Vrba’s turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.
Rather, the bipedal adaptation hominines had already achieved was used in the savanna. The fossil record shows that early bipedal hominines were still adapted to climbing trees at the time they were also walking upright. Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence—the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.
Some of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous "Australopithecus afarensis", found in Hadar in Ethiopia, which may have been forested at the time of Lucy’s death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. “Little Foot,” the collection of "Australopithecus africanus" foot bones, has a divergent big toe as well as the ankle strength to walk upright. “Little Foot” could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be covered in thick vegetation and has only recently become the arid desert it is now.
Traveling efficiency hypothesis.
An alternative explanation is the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism.
Postural feeding hypothesis.
The postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University. This hypothesis asserts that chimpanzees were only bipedal when they ate. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was utilized by grabbing for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt hypothesises that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, "Australopithecus afarensis" has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the "Australopithecus" hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.
Provisioning model.
One theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as "male provisioning". Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction ("feminization") of the male canine teeth in early hominids such as "Sahelanthropus tchadensis" and "Ardipithecus ramidus", which along with low body size dimorphism in "Ardipithecus" and "Australopithecus", suggests a reduction in inter-male antagonistic behavior in early hominids. In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.
However, this model has generated some controversy, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that Australopithecus afarensis males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk. Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother - hampering the mother's freedom and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that "Australopithecus afarensis" lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a group living primate.
Early bipedalism in homininae model.
Recent studies of 4.4 million years old "Ardipithecus ramidus" suggest bipedalism, it is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. According to Richard Dawkins in his book "The Ancestor's Tale", chimps and bonobos are descended from "Australopithecus" gracile type species while gorillas are descended from Paranthropus. These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines who eventually became us (see Homininae). Early homininaes such as "Ardipithecus ramidus" may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.
Warning display (aposematic) model.
Joseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects. Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.
Other behavioural models.
There are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski & Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others ("e.g." Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins ("e.g." 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested ("e.g." Tanner 1981:165) that male phallic display could have been the initial incentive.
Thermoregulatory model.
The thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation. When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus.
Carrying models.
Charles Darwin wrote that "Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will" Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat "over considerable distances" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea as indeed did Lovejoy (1981) with his 'provisioning model' described above. Others, such as Nancy Tanner (1981) have suggested that infant carrying was key, whilst others have suggested stone tools and weapons drove the change.
Wading models.
The observation that large Primates, including especially the great apes, that predominantly move quadrupedally on dry land, tend to switch to bipedal locomotion in waist deep water, has led to the idea that the origin of human bipedalism may have been influenced by waterside environments. This idea has been promoted for several decades by Elaine Morgan, as part of the aquatic ape hypothesis, which also proposes that swimming, diving and aquatic food sources exerted a strong influence on many aspects of human evolution, including bipedalism. The "aquatic ape hypothesis" is not accepted by or considered a serious theory within the anthropological scholarly community. Others, however, cite bipedalism among a cluster of other adaptations unique among primates, including voluntary control of breathing, hairlessness, subcutaneous fat and several other traits that are difficult to explain with more conventional theories.
Other theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution or critical fallback foods) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism.
Physiology.
Bipedal movement occurs in a number of ways, and requires many mechanical and neurological adaptations. Some of these are described below.
Biomechanics.
Standing.
Energy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.
Walking.
Walking is characterized by an "inverted pendulum" movement in which the center of gravity vaults over a stiff leg with each step. Force plates can be used to quantify the whole-body kinetic & potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two. Interestingly, this model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.
In humans, walking is composed of several separate processes:
Running.
Running is characterized by a spring-mass movement. Kinetic and potential energy are in phase, and the energy is stored & released from a spring-like limb during foot contact. Again, the whole-body kinetics are similar to animals with more limbs.
Musculature.
Bipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms.
Respiration.
A biped also has the ability to breathe while it runs. Humans usually take a breath every other stride when their aerobic system is functioning. During a sprint, at which point the anaerobic system kicks in, breathing slows until the anaerobic system can no longer sustain a sprint.
Bipedal robots.
For nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot, those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.

</doc>
<doc id="4211" url="http://en.wikipedia.org/wiki?curid=4211" title="Bootstrapping">
Bootstrapping

In general parlance, bootstrapping usually refers to the starting of a self-sustaining process that is supposed to proceed without external input. In computer technology the term (usually shortened to booting) usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other software as needed.
The term appears to have originated in the early 19th century United States (particularly in the phrase "pull oneself over a fence by one's bootstraps"), to mean an absurdly impossible action, an adynaton.
Etymology.
Tall boots may have a tab, loop or handle at the top known as a bootstrap, allowing one to use fingers or a boot hook tool to help pulling the boots on. The saying "to pull oneself up by one's bootstraps" was already in use during the 19th century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the "Workingman's Advocate": "It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots." In 1860 it appeared in a comment on metaphysical philosophy: "The attempt of the mind to analyze itself [is] an effort analogous to one who would lift himself by his own bootstraps." Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.
The term is sometimes attributed to Rudolf Erich Raspe's story "", where the main character pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), but Baron Münchhausen (1720–1797), a recounter of tall tales, does not, in fact, pull himself out by his bootstraps – and there's no sign that anyone has found an explicit reference to bootstraps in the various versions of the Munchausen tales.
Applications.
Computing.
Software loading and execution.
Booting is the process of starting a computer, specifically in regards to starting its software. The process involves a chain of stages, in which at each stage a smaller simpler program loads and then executes the larger more complicated program of the next stage. It is in this sense that the computer "pulls itself up by its bootstraps", i.e. it improves itself by its own efforts. Booting is a chain of events that starts with execution of hardware-based procedures and may then hand-off to firmware and software which is loaded into main memory. Booting often involves processes such as performing self-tests, loading configuration settings, loading a BIOS, resident monitors, a hypervisor, an operating system, or utility software.
The computer term bootstrap began as a metaphor in the 1950s. In computers, pressing a bootstrap button caused a hardwired program to read a bootstrap program from an input unit. The computer would then execute the bootstrap program, which caused it to read more program instructions. It became a self-sustaining process that proceeded without external help from manually entered instructions. As a computing term, bootstrap has been used since at least 1953.
Software development.
Bootstrapping can also refer to the development of successively more complex, faster programming environments. The simplest environment will be, perhaps, a very basic text editor (e.g., ed) and an assembler program. Using these tools, one can write a more complex text editor, and a simple compiler for a higher-level language and so on, until one can have a graphical IDE and an extremely high-level programming language.
Historically, bootstrapping also refers to an early technique for computer program development on new hardware. The technique described in this paragraph has been replaced by the use of a cross compiler executed by a pre-existing computer. Bootstrapping in program development began during the 1950s when each program was constructed on paper in decimal code or in binary code, bit by bit (1s and 0s), because there was no high-level computer language, no compiler, no assembler, and no linker. A tiny assembler program was hand-coded for a new computer (for example the IBM 650) which converted a few instructions into binary or decimal code: A1. This simple assembler program was then rewritten in its just-defined assembly language but with extensions that would enable the use of some additional mnemonics for more complex operation codes. The enhanced assembler's source program was then assembled by its predecessor's executable (A1) into binary or decimal code to give A2, and the cycle repeated (now with those enhancements available), until the entire instruction set was coded, branch addresses were automatically calculated, and other conveniences (such as conditional assembly, macros, optimisations, etc.) established. This was how the early assembly program SOAP (Symbolic Optimal Assembly Program) was developed. Compilers, linkers, loaders, and utilities were then coded in assembly language, further continuing the bootstrapping process of developing complex software systems by using simpler software.
The term was also championed by Doug Engelbart to refer to his belief that organizations could better evolve by improving the process they use for improvement (thus obtaining a compounding effect over time). His SRI team that developed the NLS hypertext system applied this strategy by using the tool they had developed to improve the tool.
Compilers.
The development of compilers for new programming languages first developed in an existing language but then rewritten in the new language and compiled by itself, is another example of the bootstrapping notion. Using an existing language to bootstrap a new language is one way to solve the "chicken or the egg" causality dilemma.
Installers.
During the installation of computer programs it is sometimes necessary to update the installer or package manager itself. The common pattern for this is to use a small executable bootstrapper file (e.g. setup.exe) which updates the installer and starts the real installation after the update. Sometimes the bootstrapper also installs other prerequisites for the software during the bootstrapping process.
Overlay networks.
A bootstrapping node, also known as a rendezvous host, is a node in an overlay network that provides initial configuration information to newly joining nodes so that they may successfully join the overlay network.
Discrete event simulation.
A type of computer simulation called discrete event simulation represents the operation of a system as a chronological sequence of events. A technique called "bootstrapping the simulation model" is used, which bootstraps initial data points using a pseudorandom number generator to schedule an initial set of pending events, which schedule additional events, and with time, the distribution of event times approaches its steady state—the bootstrapping behavior is overwhelmed by steady-state behavior.
Artificial intelligence and machine learning.
Bootstrapping is a technique used to iteratively improve a classifier's performance. Seed AI is a hypothesized type of artificial intelligence capable of recursive self-improvement. Having improved itself, it would become better at improving itself, potentially leading to an exponential increase in intelligence. No such AI is known to exist, but it remains an active field of research.
Seed AI is a significant part of some theories about the technological singularity: proponents believe that the development of seed AI will rapidly yield ever-smarter intelligence (via bootstrapping) and thus a new era.
Research.
Bootstrapping is a database searching technique. One may perform an inexact search (using keywords, for instance) and retrieve numerous "hits", some of which will be on-target. When the researcher looks at a relevant document that comes through in the mix, subject headings will be located within the document. The researcher can then execute a new search using authorized subject headings that will yield more focused, pinpointed results.
Statistics.
Bootstrapping is a resampling technique used to obtain estimates of summary statistics.
Business.
Bootstrapping in business means starting a business without external help or capital. Such startups fund the development of their company through internal cash flow and are cautious with their expenses. Generally at the start of a venture, a small amount of money will be set aside for the bootstrap process. Bootstrapping can also be a supplement for econometric models. Bootstrapping was also expanded upon in the book "Bootstrap Business", by Richard Christiansen.
Biology.
Richard Dawkins in his book "River Out of Eden" used the computer bootstrapping concept to explain how biological cells differentiate: "Different cells receive different combinations of chemicals, which switch on different combinations of genes, and some genes work to switch other genes on or off. And so the bootstrapping continues, until we have the full repertoire of different kinds of cells."
Phylogenetics.
Bootstrapping analysis gives a way to judge the strength of support for clades on phylogenetic trees. A number is written by a node, which reflects the percentage of bootstrap trees which also resolve the clade at the endpoints of that branch.
Law.
Bootstrapping is a rule preventing the admission of hearsay evidence in conspiracy cases.
Linguistics.
Bootstrapping is a theory of language acquisition.
Physics.
Bootstrapping is using very general consistency criteria to determine the form of a quantum theory from some assumptions on the spectrum of particles.
Electronics.
Bootstrapping is a form of positive feedback in analog circuit design.
Electric power grid.
Typically an electric power grid is never brought down intentionally. Generators and power stations are started and shut down as necessary. A typical power station requires power for start up prior to being able to generate power. This power is obtained from the grid, so if the entire grid is down these stations cannot be started.
Therefore to get a grid started, there must be at least a small number of power stations that can start entirely on their own. A black start is the process of restoring a power station to operation without relying on external power. In the absence of grid power, one or more black starts are used to bootstrap the grid.
Cellular networks.
A Bootstrapping Server Function (BSF) is an intermediary element in cellular networks which provides application independent functions for mutual authentication of user equipment and servers unknown to each other and for 'bootstrapping' the exchange of secret session keys afterwards. The term 'bootstrapping' is related to building a security relation with a previously unknown device first and to allow installing security elements (keys) in the device and the BSF afterwards.

</doc>
<doc id="4213" url="http://en.wikipedia.org/wiki?curid=4213" title="Baltic languages">
Baltic languages

The Baltic languages are part of the Balto-Slavic branch of the Indo-European language family spoken by the Balts. Baltic languages are spoken mainly in areas extending east and southeast of the Baltic Sea in Northern Europe. They are usually considered a single family divided into two groups: Western Baltic, containing only extinct languages, and Eastern Baltic, containing two living languages, Lithuanian and Latvian. The range of the Eastern Balts once reached to the Ural mountains. Although related, the Lithuanian, the Latvian, and particularly the Old Prussian vocabularies differ substantially from one another and are not mutually intelligible. The now-extinct Old Prussian language is considered the most archaic of the Baltic languages.
Branches.
The Baltic languages are generally thought to form a single family with two branches, Eastern and Western. However, these are sometimes classified as independent branches of Balto-Slavic.
Eastern Baltic languages.
"("†"—Extinct language)"
Nadruvian was spoken near the isogloss of Eastern and Western Baltic, but is too poorly attested to classify.
Geographic distribution.
Speakers of modern Baltic languages are generally concentrated within the borders of Lithuania and Latvia, and in emigrant communities in the United States, Canada, Australia and states of the former Soviet Union. Historically the languages were spoken over a larger area: West to the mouth of the Vistula river in present-day Poland, at least as far East as the Dniepr river in present-day Belarus, perhaps even to Moscow, perhaps as far south as Kiev. Key evidence of Baltic language presence in these regions is found in hydronyms (names of bodies of water) in the regions that are characteristically Baltic. Use of hydronyms is generally accepted to determine the extent of these cultures' influence, but "not" the date of such influence. Historical expansion of the usage of Slavic languages in the South and East, and Germanic languages in the West reduced the geographic distribution of Baltic languages to a fraction of the area that they formerly covered.
Prehistory and history.
Although the various Baltic tribes were mentioned by ancient historians as early as 98 B.C., the first attestation of a Baltic language was in about 1350, with the creation of the "Elbing Prussian Vocabulary", a German to Prussian translation dictionary. It is also believed that Baltic languages are among the most archaic of the remaining Indo-European languages, despite their late attestation. Lithuanian was first attested in a hymnal translation in 1545; the first printed book in Lithuanian, a Catechism by Martynas Mažvydas was published in 1547 in Königsberg, Prussia (now Kaliningrad, Russia). Latvian appeared in a hymnal in 1530 and in a printed Catechism in 1585. One reason for the late attestation is that the Baltic peoples resisted Christianization longer than any other Europeans, which delayed the introduction of writing and isolated their languages from outside influence.
With the establishment of a German state in Prussia, and the eradication or flight of much of the Baltic Prussian population in the 13th century, the remaining Prussians began to be assimilated, and by the end of the 17th century, the Prussian language had become extinct.
During the years of the Polish–Lithuanian Commonwealth (1569–1795), official documents were written in Polish, Ruthenian and Latin, with Lithuanian being mostly an oral language, with small quantities of written documents. After the Partitions of Commonwealth, most of the Baltic lands were under the rule of the Russian Empire, where the native languages were sometimes prohibited from being written down, or used publicly (see Lithuanian press ban).
Relationship with other Indo-European languages.
The Baltic languages are of particular interest to linguists because they retain many archaic features, which are believed to have been present in the early stages of the Proto-Indo-European language. However, linguists have had a hard time establishing the precise relationship of the Baltic languages to other languages in the Indo-European family. Several of the extinct Baltic languages have a limited or nonexistent written record, their existence being known only from the records of ancient historians and personal or place names. All of the languages in the Baltic group (including the living ones) were first written down relatively late in their probable existence as distinct languages. These two factors combined with others have obscured the history of the Baltic languages, leading to a number of theories regarding their position in the Indo-European family.
The Baltic languages show a close relationship with the Slavic languages, and are grouped with them in a Balto-Slavic family by most scholars. This family is considered to have developed from a common ancestor, Proto-Balto-Slavic. Later on, several lexical, phonological and morphological dialectisms developed, separating the various Balto-Slavic languages from each other. Although it is generally agreed upon that the Slavic languages developed from a single more-or-less unified dialect (Proto-Slavic) that split off from common Balto-Slavic, there is more disagreement about the relationship between the Baltic languages.
The traditional view is that the Balto-Slavic languages split into two branches, Baltic and Slavic, with each branch developing as a single common language (Proto-Baltic and Proto-Slavic) for some time afterwards. Proto-Baltic is then thought to have split into East Baltic and West Baltic branches. However, more recent scholarship has suggested that there was no unified Proto-Baltic stage, but that Proto-Balto-Slavic split directly into three groups: Slavic, East Baltic and West Baltic. Under this view, the Baltic family is paraphyletic, and consists of all Balto-Slavic languages that are not Slavic. This would imply that Proto-Baltic, the last common ancestor of all Baltic languages, would be identical to Proto-Balto-Slavic itself, rather than distinct from it.
Finally, there is a minority of scholars who argue that Baltic descended directly from Proto-Indo-European, without an intermediate common Balto-Slavic stage. They argue that the many similarities and shared innovations between Baltic and Slavic are due to several millennia of contact between the groups, rather than shared heritage.

</doc>
<doc id="4214" url="http://en.wikipedia.org/wiki?curid=4214" title="Bioinformatics">
Bioinformatics

Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics and engineering to study and process biological data.
Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the fields of genetics and genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.
Introduction.
Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, and protein structures as well as molecular interactions.
History.
Paulien Hogeweg and Ben Hesper coined the term "bioinformatics" in 1970 to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).
Sequences.
Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics." Dayhoff compiled one of the first protein sequence databases, initially published as books and pioneered methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.
Genomes.
As whole genome sequences became available, again with the pioneering work of Frederick Sanger, it became evident that computer-assisted analysis would be insightful. The first analysis of this type, which had important input from cryptologists at the National Security Agency, was applied to the nucleotide sequences of the bacteriophages MS2 and PhiX174. As a proof of principle, this work showed that standard methods of cryptology could reveal intrinsic features of the genetic code such as the codon length and the reading frame. This work seems to have been ahead of its time—it was rejected for publication by numerous standard journals and finally found a home in the Journal of Theoretical Biology. The term bioinformatics was re-discovered and used to refer to the creation of databases such as GenBank in 1982. With the public availability of data tools for their analysis were quickly developed and described in journals such as Nucleic Acids Research which published specialized issues on bioinformatics tools as early as 1982.
Goals.
In order to study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures. The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:
The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, and the modeling of evolution.
Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.
Over the past few decades rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.
Approaches.
Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.
There are two fundamental ways of modelling a Biological system (e.g., living cell) both coming under Bioinformatic approaches.
A broad sub-category under bioinformatics is structural bioinformatics.
Relation to other fields.
Bioinformatics is a similar but distinct science from biological computation and computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology have similar aims and approaches, but differ in scale: bioinformatics organizes and analyzes basic biological data, whereas computational biology builds theoretical models of biological systems, just as mathematical biology does with mathematical models.
Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.
Sequence analysis.
Since the Phage Φ-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode polypeptides (proteins), RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides. These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research to sequence the first bacterial genome, "Haemophilus influenzae") does not produce entire chromosomes. Instead it generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly will usually contain numerous gaps that have to be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.
Another aspect of bioinformatics in sequence analysis is annotation. This involves computational gene finding to search for protein-coding genes, RNA genes, and other functional sequences within a genome. Not all of the nucleotides within a genome are part of genes. Within the genomes of higher organisms, large parts of the DNA do not serve any obvious purpose. This so-called junk DNA may, however, contain unrecognized functional elements. Bioinformatics helps to bridge the gap between genome and proteome projects — for example, in the use of DNA sequences for protein identification.
Genome annotation.
In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.
The first genome annotation software system was designed in 1995 by Owen White, who was part of the team at The Institute for Genomic Research that sequenced and analyzed the first genome of a free-living organism to be decoded, the bacterium "Haemophilus influenzae". White built a software system to find the genes (fragments of genomic sequence that encode proteins), the transfer RNAs, and to make initial assignments of function to those genes. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in "Haemophilus influenzae", are constantly changing and improving.
Computational evolutionary biology.
Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:
Future work endeavours to reconstruct the now more complex tree of life.
The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.
Comparative genomics.
The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectra of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov Chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.
Many of these studies are based on the homology detection and protein families computation.
Genetics of disease.
With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as infertility, breast cancer or Alzheimer's Disease. Genome-wide association studies are essential to pinpoint the mutations for such complex diseases.
Analysis of mutations in cancer.
In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known "point mutations". These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.
Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.
Gene and protein expression.
Analysis of gene expression.
The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.
Analysis of protein expression.
Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected.
Analysis of regulation.
Regulation is the complex orchestration of events starting with an extracellular signal such as a hormone and leading to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process. For example, promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods such as the Bi-CoPaM. The later, namely Bi-CoPaM, has been actually proposed to address various issues specific to gene discovery problems such as consistent co-expression of genes over multiple microarray datasets.
Structural bioinformatics.
Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy – a.k.a. Mad Cow Disease – prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of "secondary", "tertiary" and "quaternary" structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.
One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene "A", whose function is known, is homologous to the sequence of gene "B," whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.
One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.
Other techniques for predicting protein structure include protein threading and "de novo" (from scratch) physics-based modeling.
Network and systems biology.
"Network analysis" seeks to understand the relationships within biological networks such as metabolic or protein-protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically and/or functionally.
"Systems biology" involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.
Molecular interaction networks.
Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without performing protein–protein interaction experiments. A variety of methods have been developed to tackle the protein–protein docking problem, though it seems that there is still much work to be done in this field.
Other interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.
Others.
Literature analysis.
The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:
The area of research draws from statistics and computational linguistics.
High-throughput image analysis.
Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:
High-throughput single cell data analysis.
Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.
Biodiversity informatics.
Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, or species identification tools.
Databases.
Databases are essential for bioinformatics research and applications. There is a huge number of available databases covering almost everything from DNA and protein sequences, molecular structures, to phenotypes and biodiversity. Databases generally fall into one of three types. Some contain data resulting directly from empirical methods such as gene knockouts. Others consist of predicted data, and most contain data from both sources. There are meta-databases that incorporate data compiled from multiple other databases. Some others are specialized, such as those specific to an organism. These databases vary in their format, way of accession and whether they are public or not. Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.
Please keep in mind that this is a quick sampling and generally most computation data is supported by wet lab data as well.
Software and tools.
Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.
Open-source bioinformatics software.
Many free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative "in silico" experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide "de facto" standards and shared object models for assisting with the challenge of bioinformation integration.
The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioRuby, Bioclipse, EMBOSS, .NET Bio, Taverna workbench, and UGENE. In order to maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.
Web services in bioinformatics.
SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.
Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.
Bioinformatics workflow management systems.
A Bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to
Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril.
Education platforms.
Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal.
Conferences.
There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), Research in Computational Molecular Biology (RECOMB) and American Society of Mass Spectrometry (ASMS).

</doc>
<doc id="4216" url="http://en.wikipedia.org/wiki?curid=4216" title="Brian De Palma">
Brian De Palma

Brian Russell De Palma (born September 11, 1940) is an American film director and screenwriter. In a career spanning over 40 years, he is probably best known for his suspense and crime thriller films. He directed successful and popular films such as "Carrie", "Dressed to Kill", "Scarface", "The Untouchables", "Carlito's Way", and "". De Palma is credited with fostering the career of Al Pacino.
Early life.
De Palma, who is of Italian ancestry, was born in Newark, New Jersey, the son of Vivienne (née Muti) and Anthony Federico De Palma, an orthopedic surgeon. He was raised in Philadelphia, Pennsylvania and New Hampshire and attended various Protestant and Quaker schools, eventually graduating from Friends' Central School. He won a regional science-fair prize for a project titled "An Analog Computer to Solve Differential Equations".
1960s and early career.
Enrolled at Columbia as a physics student, De Palma became enraptured with the filmmaking process after viewing "Citizen Kane" and "Vertigo". De Palma subsequently enrolled at the newly coed Sarah Lawrence College as a graduate student in their theater department in the early 1960s, becoming one of the first male students among a female population. Once there, influences as various as drama teacher Wilford Leach, the Maysles brothers, Michelangelo Antonioni, Jean-Luc Godard, Andy Warhol and Alfred Hitchcock impressed upon De Palma the many styles and themes that would shape his own cinema in the coming decades. An early association with a young Robert De Niro resulted in "The Wedding Party". The film, which was co-directed with Leach and producer Cynthia Munroe, had been shot in 1963 but remained unreleased until 1969, when De Palma's star had risen sufficiently within the Greenwich Village filmmaking scene. De Niro was unknown at the time; the credits mistakenly display his name as "Robert ." The film is noteworthy for its invocation of silent film techniques and an insistence on the jump-cut for effect. De Palma followed this with various small films for the NAACP and The Treasury Department.
During the 1960s, De Palma began making a living producing documentary films, notably "The Responsive Eye", a 1966 movie about "The Responsive Eye" op-art exhibit curated by William Seitz for Museum of Modern Art in 1965. In an interview with Gelmis from 1969, De Palma described the film as "very good and very successful. It's distributed by Pathe Contemporary and makes lots of money. I shot it in four hours, with synched sound. I had two other guys shooting people's reactions to the paintings, and the paintings themselves."
"Dionysus in 69" (1969) was De Palma's other major documentary from this period. The film records The Performance Group's performance of Euripides' The Bacchae, starring, amongst others, De Palma regular William Finley. The play is noted for breaking traditional barriers between performers and audience. The film's most striking quality is its extensive use of the split-screen. De Palma recalls that he was "floored" by this performance upon first sight, and in 1973 recounts how he "began to try and figure out a way to capture it on film. I came up with the idea of split-screen, to be able to show the actual audience involvement, to trace the life of the audience and that of the play as they merge in and out of each other."
De Palma's most significant features from this decade are "Greetings" (1968) and "Hi, Mom!" (1970). Both films star Robert De Niro and espouse a Leftist revolutionary viewpoint common to their era. "Greetings" was entered into the 19th Berlin International Film Festival, where it won a Silver Bear award. His other major film from this period is the slasher comedy "Murder a la Mod". Each of these films contains experiments in narrative and intertextuality, reflecting De Palma's stated intention to become the "American Godard" while integrating several of the themes which permeated Hitchcock's work.
"Greetings" is about three New Yorkers dealing with the draft. The film is often considered the first to deal explicitly with the draft. The film is noteworthy for its use of various experimental techniques to convey its narrative in ultimately unconventional ways. Footage was sped up, rapid cutting was used to distance the audience from the narrative, and it was difficult to discern with whom the audience must ultimately align. "Greetings" ultimately grossed over $1 million at the box office and cemented De Palma's position as a bankable filmmaker.
After the success of his 1968 breakthrough, De Palma and his producing partner, Charles Hirsch, were given the opportunity by Sigma 3 to make an unofficial sequel of sorts, initially entitled "Son of Greetings", and subsequently released as "Hi, Mom!". While "Greetings" accentuated its varied cast, "Hi, Mom!" focuses on De Niro's character, Jon Rubin, an essential carry-over from the previous film. The film is ultimately significant insofar as it displays the first enunciation of De Palma's style in all its major traits – voyeurism, guilt, and a hyper-consciousness of the medium are all on full display, not just as hallmarks, but built into this formal, material apparatus itself.
These traits come to the fore in "Hi, Mom!"'s "Be Black, Baby" sequence. This sequence parodies cinéma vérité, the dominant documentary tradition of the 1960s, while simultaneously providing the audience with a visceral and disturbingly emotional experience. De Palma describes the sequence as a constant invocation of Brechtian distanciation: “First of all, I am interested in the medium of film itself, and I am constantly standing outside and making people aware that they are always watching a film. At the same time I am evolving it. In "Hi, Mom!" for instance, there is a sequence where you are obviously watching a ridiculous documentary and you are told that and you are aware of it, but it still sucks you in. There is a kind of Brechtian alienation idea here: you are aware of what you are watching at the same time that you are emotionally involved with it.”
"Be Black, Baby" was filmed in black and white stock on 16 mm, in low-light conditions that stress the crudity of the direct cinema aesthetic. It is precisely from this crudity that the film itself gains a credibility of “realism.” In an interview with Michael Bliss, De Palma notes “[Be Black, Baby] was rehearsed for almost three weeks... In fact, it's all scripted. But once the thing starts, they just go with the way it's going. I specifically got a very good documentary camera filmmaker (Robert Elfstrom) to just shoot it like a documentary to follow the action." Furthermore, "I wanted to show in "Hi, Mom!" how you can really involve an audience. You take an absurd premise – "Be Black, Baby" – and totally involve them and really frighten them at the same time. It's very Brechtian. You suck 'em in and annihilate 'em. Then you say, "It's just a movie, right? It's not real." It's just like television. You're sucked in all the time, and you're being lied to in a very documentary-like setting. The "Be Black, Baby" section of "Hi, Mom!" is probably the most important piece of film I've ever done."
Transition to Hollywood.
In the 1970s, De Palma went to Hollywood where he worked on bigger budget films. In 1970, De Palma left New York for Hollywood at age thirty to make "Get to Know Your Rabbit", starring Orson Welles and Tommy Smothers. Making the film was a crushing experience for De Palma as Tommy Smothers didn't like a lot of De Palma's ideas.
After several small, studio and independent released films that included stand-outs "Sisters", "Phantom of the Paradise", and "Obsession", a small film based on a novel called "Carrie" was released directed by Brian De Palma. The psychic thriller "Carrie" is seen by some as De Palma's bid for a blockbuster. In fact, the project was small, underfunded by United Artists, and well under the cultural radar during the early months of production, as Stephen King's source novel had yet to climb the bestseller list. De Palma gravitated toward the project and changed crucial plot elements based upon his own predilections, not the saleability of the novel. The cast was young and relatively new, though the stars Sissy Spacek and John Travolta had gained considerable attention for previous work in, respectively, film and episodic sitcoms. "Carrie" became a hit, the first genuine box-office success for De Palma. It garnered Spacek and Piper Laurie Oscar nominations for their performances. Preproduction for the film had coincided with the casting process for George Lucas's "", and many of the actors cast in De Palma's film had been earmarked as contenders for Lucas's movie, and vice-versa. The "shock ending" finale is effective even while it upholds horror-film convention, its suspense sequences are buttressed by teen comedy tropes, and its use of split-screen, split-diopter and slow motion shots tell the story visually rather than through dialogue.
The financial and critical success of "Carrie" allowed De Palma to pursue more personal material. "The Demolished Man" was a novel that had fascinated De Palma since the late 1950s and appealed to his background in mathematics and avant-garde storytelling. Its unconventional unfolding of plot (exemplified in its mathematical layout of dialogue) and its stress on perception have analogs in De Palma's filmmaking. He sought to adapt it on numerous occasions, though the project would carry a substantial price tag, and has yet to appear onscreen (Steven Spielberg's adaptation of Philip K. Dick's "Minority Report" bears striking similarities to De Palma's visual style and some of the themes of "The Demolished Man"). The result of his experience with adapting "The Demolished Man" was "The Fury", a science fiction psychic thriller that starred Kirk Douglas, Carrie Snodgress, John Cassavetes and Amy Irving. The film was admired by Jean-Luc Godard, who featured a clip in his mammoth Histoire(s) du cinéma, and Pauline Kael who championed both "The Fury" and De Palma. The film boasted a larger budget than "Carrie", though the consensus view at the time was that De Palma was repeating himself, with diminishing returns. As a film it retains De Palma's considerable visual flair, but points more toward his work in mainstream entertainments such as "The Untouchables" and "", the thematic complex thrillers for which he is now better known.
For many film-goers, De Palma's gangster films, most notably "Scarface" and "Carlito's Way", pushed the envelope of violence and depravity, and yet greatly vary from one another in both style and content and also illustrate De Palma's evolution as a film-maker. In essence, the excesses of "Scarface" contrast with the more emotional tragedy of "Carlito's Way". Both films feature Al Pacino in what has become a fruitful working relationship. In 1984, he directed the music video of Bruce Springsteen's song "Dancing in the Dark".
Later into the 1990s and 2000s, De Palma did other films. He attempted to do dramas and a few thrillers plus science fiction. Some of these movies ("Mission: Impossible, Carlito's Way") worked and some others ("Mission to Mars, Raising Cain, Snake Eyes, The Bonfire of the Vanities") failed at the box office. Of these films, "The Bonfire of the Vanities" would be De Palma's biggest box office disaster, losing millions. Another later movie from De Palma, "Redacted", unleashed a torrent of controversy over its subject of American involvement in Iraq, and supposed atrocities committed there. It received limited release in the United States.
In 2012, his film "Passion" was selected to compete for the Golden Lion at the 69th Venice International Film Festival.
De Palma today resides in Los Angeles.
Trademarks and style.
Themes.
De Palma's films can fall into two categories, his psychological thrillers ("Sisters", "Body Double", "Obsession", "Dressed to Kill", "Blow Out", "Raising Cain") and his mainly commercial films ("Scarface", "The Untouchables", "Carlito's Way", and "Mission: Impossible"). He has often produced "De Palma" films one after the other before going on to direct a different genre, but would always return to his familiar territory. Because of the subject matter and graphic violence of some of De Palma's films, such as "Dressed to Kill", "Scarface" and "Body Double", they are often at the center of controversy with the Motion Picture Association of America, film critics and the viewing public.
De Palma is known for quoting and referencing other director's work throughout his career. Michelangelo Antonioni's "Blowup" and Francis Ford Coppola's "The Conversation" plots were used for the basis of "Blow Out". "The Untouchables" finale shoot out in the train station is a clear borrow from the Odessa Steps sequence in Sergei Eisenstein's "The Battleship Potemkin". The main plot from "Rear Window" was used for "Body Double", while it also used elements of "Vertigo". "Vertigo" was also the basis for "Obsession". "Dressed to Kill" was a note-for-note homage to Hitchcock's "Psycho", including such moments as the surprise death of the lead actress and the exposition scene by the psychiatrist at the end.
Camera shots.
Film critics have often noted De Palma's penchant for unusual camera angles and compositions throughout his career. He often frames characters against the background using a canted angle shot. Split-screen techniques have been used to show two separate events happening simultaneously. To emphasize the dramatic impact of a certain scene De Palma has employed a 360-degree camera pan. Slow sweeping, panning and tracking shots are often used throughout his films, often through precisely-choreographed long takes lasting for minutes without cutting. Split focus shots, often referred to as "di-opt", are used by De Palma to emphasize the foreground person/object while simultaneously keeping a background person/object in focus. Slow-motion is frequently used in his films to increase suspense.
Personal life.
De Palma previously dated Margot Kidder in the early 1970s. He has been married and divorced three times, to actress Nancy Allen (1979–1983), producer Gale Anne Hurd (1991–1993), and Darnell Gregorio (1995–1997). He has one daughter from his marriage to Gale Anne Hurd, Lolita de Palma, born in 1991, and one daughter from his marriage to Darnell Gregorio, Piper De Palma, born in 1996.
Legacy.
De Palma is often cited as a leading member of the New Hollywood generation of film directors, a distinct pedigree who either emerged from film schools or are overtly cine-literate. His contemporaries include Martin Scorsese, Paul Schrader, John Milius, George Lucas, Francis Ford Coppola, John Carpenter, and Ridley Scott. His artistry in directing and use of cinematography and suspense in several of his films has often been compared to the work of Alfred Hitchcock. Psychologists have been intrigued by De Palma's fascination with pathology, by the aberrant behavior aroused in characters who find themselves manipulated by others.
De Palma has encouraged and fostered the filmmaking careers of directors such as Quentin Tarantino, Mark Romanek and Keith Gordon. Tarantino said – during interview with De Palma, that "Blow Out" is one of his all time favourite films, and that after watching "Scarface" he knew how to make his own film. Terrence Malick credits seeing De Palma's early films on college campus tours as a validation of independent film, and subsequently switched his attention from philosophy to filmmaking.
Critics who frequently admire De Palma's work include Pauline Kael, Roger Ebert and Armond White, among others. Kael wrote in her review of "Blow Out", "At forty, Brian De Palma has more than twenty years of moviemaking behind him, and he has been growing better and better. Each time a new film of his opens, everything he has done before seems to have been preparation for it." In his review of "Femme Fatale", Roger Ebert wrote about the director: "De Palma deserves more honor as a director. Consider also these titles: "Sisters", "Blow Out", "The Fury", "Dressed to Kill", "Carrie", "Scarface", "Wise Guys", "Casualties of War", "Carlito's Way", "Mission: Impossible". Yes, there are a few failures along the way ("Snake Eyes", "Mission to Mars", "The Bonfire of the Vanities"), but look at the range here, and reflect that these movies contain treasure for those who admire the craft as well as the story, who sense the glee with which De Palma manipulates images and characters for the simple joy of being good at it. It's not just that he sometimes works in the style of Hitchcock, but that he has the nerve to." White said in defense of De Palma's "Mission to Mars", "[The film] is a litmus test. It can be said with certainty that any reviewer who pans it does not understand movies, let alone like them."
Criticisms.
De Palma is frequently criticized for his filmmaking style. Julie Salamon has written that "many critics argued that De Palma dressed up his woman-hating wickedness so artfully that the intelligentsia didn't see him for what he was: a perverse misogynist." Feminist writer Jane Caputi responded to De Palma's statement that "I'm always attacked for having an erotic, sexist approach-- chopping up women, putting women in peril. I'm making suspense movies! What else is going to happen to them?" by saying "Things can only 'happen to' women in the femicidal grammar. We also can note with great irony just whom De Palma claims is being attacked."
David Thomson wrote in his entry for De Palma, "There is a self-conscious cunning in De Palma's work, ready to control everything except his own cruelty and indifference."
References.
Notes
Bibliography
Further reading

</doc>
<doc id="4218" url="http://en.wikipedia.org/wiki?curid=4218" title="North American B-25 Mitchell">
North American B-25 Mitchell

The North American B-25 Mitchell was an American twin-engined medium bomber manufactured by North American Aviation. It was used by many Allied air forces, in every theater of World War II, as well as many other air forces after the war ended, and saw service across four decades.
The B-25 was named in honor of General Billy Mitchell, a pioneer of U.S. military aviation. By the end of its production, nearly 10,000 B-25s in numerous models had been built. These included a few limited variations, such as the United States Navy's and Marine Corps' PBJ-1 patrol bomber and the United States Army Air Forces' F-10 photo reconnaissance aircraft.
Design and development.
The B-25 was a descendant of the earlier XB-21 (North American-39) project of the mid-1930s. Experience gained in developing that aircraft was eventually used by North American in designing the B-25 (called the NA-40 by the company). One NA-40 was built, with several modifications later being done to test a number of potential improvements. These improvements included Wright R-2600 radial engines, which would become standard on the later B-25.
In 1939, the modified and improved NA-40B was submitted to the United States Army Air Corps for evaluation. This aircraft was originally intended to be an attack bomber for export to the United Kingdom and France, both of which had a pressing requirement for such aircraft in the early stages of World War II. However, those countries changed their minds, opting instead for the also-new Douglas DB-7 (later to be used by the U.S. as the A-20 Havoc). Despite this loss of sales, the NA-40B re-entered the spotlight when the Army Air Corps evaluated it for use as a medium bomber. Unfortunately, the NA-40B was destroyed in a crash on 11 April 1939. Nonetheless, the type was ordered into production, along with the army's other new medium bomber, the Martin B-26 Marauder.
Early production.
To get orders for bombers, North American Aviation designed the XB-21. It lost out to the Douglas B-18A, but persisted and developed a more advanced design, which became the B-25 Mitchell bomber.
An improvement of the NA-40B, dubbed the NA-62, was the basis for the first actual B-25. Due to the pressing need for medium bombers by the army, no experimental or service-test versions were built. Any necessary modifications were made during production runs, or to existing aircraft at field modification centers around the world.
A significant change in the early days of B-25 production was a redesign of the wing. In the first nine aircraft, a constant-dihedral wing was used, in which the wing had a consistent, straight, slight upward angle from the fuselage to the wingtip. This design caused stability problems, and as a result, the dihedral angle was nullified on the outboard wing sections, giving the B-25 its slightly gull wing configuration. Less noticeable changes during this period included an increase in the size of the tail fins and a decrease in their inward cant.
North American was the largest producer of aircraft in the war by far. It was the first time a company had produced trainers, bombers and fighters simultaneously (the AT-6/SNJ Texan, B-25 Mitchell, and the P-51 Mustang). It produced B-25s at both its Inglewood main plant and an additional 6,608 planes at its Kansas City, Kansas plant at Fairfax Airport.
A descendant of the B-25 was the North American XB-28, meant to be a high-altitude version of the B-25. Despite this premise, the actual aircraft bore little resemblance to the Mitchell and in some respects more closely resembled the Martin B-26 Marauder.
Operational history.
Far East.
The majority of B-25s in American service were used in the Pacific. It fought on Papua New Guinea, in Burma and in the island hopping campaign in the central Pacific. It was in the Pacific that the aircraft’s potential as a ground-attack aircraft was discovered and developed. The jungle environment reduced the usefulness of standard-level bombing, and made low-level attack the best tactic. The ever-increasing amount of forward firing guns was a response to this operational environment, making the B-25 a formidable strafing aircraft. 
In Burma, the B-25 was often used to attack Japanese communication links, especially bridges in central Burma. It also helped supply the besieged troops at Imphal in 1944. 
In the Pacific, the B-25 proved itself to be a very capable anti-shipping weapon, sinking many ships. Later in the war, the distances between islands limited the usefulness of the B-25, although it was used against Guam and Tinian. It was also used against Japanese-occupied islands that had been bypassed by the main campaign, as happened in the Marshall Islands.
Middle East and Italy.
The first B-25s arrived in Egypt just in time to take part in the Battle of El Alamein. From there the aircraft took part in the rest of the campaign in North Africa, the invasion of Sicily and the advance up Italy. In Italy the B-25 was used in the ground-attack role, concentrating on attacks against road and rail links in Italy, Austria and the Balkans. The B-25 had a longer range than the Douglas A-20 Havoc and Douglas A-26 Invaders, allowing it to reach further into occupied Europe. The five bombardment groups that used the B-25 in the desert and Italy were the only U.S. units to use the B-25 in Europe.
Europe.
The U.S. Eighth Air Force, based in Britain, concentrated on long-range raids over Germany and occupied Europe. Although it did have a small number of units equipped with twin-engined aircraft, the B-25 was not amongst them. However, the RAF received nearly 900 Mitchells, using them to replace Douglas Bostons, Lockheed Venturas and Vickers Wellington bombers. The Mitchell entered active RAF service on 22 January 1943. At first it was used to bomb strategic targets in occupied Europe. After the D-Day invasion the RAF used its Mitchells to support the armies in Europe, moving several squadrons to forward airbases in France and Belgium.
USAAF.
The B-25 first gained fame as the bomber used in the 18 April 1942 Doolittle Raid, in which 16 B-25Bs led by Lieutenant Colonel Jimmy Doolittle attacked mainland Japan, four months after the bombing of Pearl Harbor. The mission gave a much-needed lift in spirits to the Americans, and alarmed the Japanese who had believed their home islands were inviolable by enemy forces. Although the amount of actual damage done was relatively minor, it forced the Japanese to divert troops for the home defense for the remainder of the war. 
The raiders took off from the carrier and successfully bombed Tokyo and four other Japanese cities without loss. Fifteen of the bombers subsequently crash-landed "en route" to recovery fields in Eastern China. These losses were the result of the task force being spotted by a Japanese vessel forcing the bombers to take off early, fuel exhaustion, stormy nighttime conditions with zero visibility, and lack of electronic homing aids at the recovery bases. Only one B-25 bomber landed intact, in Siberia where its five-man crew was interned and the aircraft confiscated. Of the 80 aircrew, 69 survived their historic mission and eventually made it back to American lines.
Following a number of additional modifications, including the addition of Plexiglas windows for the navigator and radio operator, heavier nose armament, and de-icing and anti-icing equipment, the B-25C was released to the Army. This was the second mass-produced version of the Mitchell, the first being the lightly armed B-25B used by the Doolittle Raiders. The B-25C and B-25D differed only in location of manufacture: -Cs at Inglewood, California, -Ds at Kansas City, Kansas. A total of 3,915 B-25Cs and -Ds were built by North American during World War II.
Although the B-25 was originally designed to bomb from medium altitudes in level flight, it was used frequently in the Southwest Pacific theatre on treetop-level strafing and missions with parachute-retarded fragmentation bombs against Japanese airfields in New Guinea and the Philippines. These heavily armed Mitchells, field-modified at Townsville, Australia, by Major Paul I. "Pappy" Gunn and North American tech rep Jack Fox, were also used on strafing and skip-bombing missions against Japanese shipping trying to resupply their land-based armies. 
Under the leadership of Lieutenant General George C. Kenney, B-25s of the Fifth and Thirteenth Air Forces devastated Japanese targets in the Southwest Pacific theater from 1942 to 1945, and played a significant role in pushing the Japanese back to their home islands. B-25s were also used with devastating effect in the Central Pacific, Alaska, North Africa, Mediterranean and China-Burma-India (CBI) theaters.
Use as a gunship.
Because of the urgent need for hard-hitting strafer aircraft, a version dubbed the B-25G was developed, in which the standard-length transparent nose and the bombardier were replaced by a shorter solid nose containing two fixed .50 in (12.7 mm) machine guns and a 75 mm (2.95 in) M4 cannon, one of the largest weapons fitted to an aircraft, similar to the experimental British Mosquito Mk. XVIII (57 mm gun), and German Ju 88P heavy cannon (up to a 75mm long-barrel "Bordkanone BK 7,5" cannon) carrying aircraft. The B-25G's cannon was manually loaded and serviced by the navigator, who was able to perform these operations without leaving his crew station just behind the pilot. This was possible due to the shorter nose of the G-model and the length of the M4, which allowed the breech to extend into the navigator's compartment.
The B-25G's successor, the B-25H, had even more firepower. The M4 gun was replaced by the lighter T13E1, designed specifically for the aircraft. The 75 mm (2.95 in) gun fired at a muzzle velocity of 2,362 ft/s (about 720 m/s). Due to its low rate of fire (approximately four rounds could be fired in a single strafing run) and relative ineffectiveness against ground targets, as well as substantial recoil, the 75 mm (2.95 in) gun was sometimes removed from both G and H models and replaced with two additional .50 in (12.7 mm) machine guns as a field modification. 
Besides that, the -H normally mounted four fixed forward-firing .50  (12.7 mm) machine guns in the nose, four more fixed ones in forward-firing cheek blisters, two more in the manned dorsal turret, one each in a pair of new waist positions, and a final pair in a new tail gunner's position. Company promotional material bragged the B-25H could "bring to bear 10 machine guns coming and four going, in addition to the 75 mm cannon, a brace of eight rockets and 3,000 lb (1,360 kg) of bombs."
The B-25H also featured a redesigned cockpit area, required by the dorsal turret having been relocated forward to the navigator's compartment – just aft of the cockpit and just ahead of the leading edge wing roots, thus requiring the addition of the waist and tail gun positions – and a heavily modified cockpit designed to be operated by a single pilot, the co-pilot's station and controls deleted, and the seat cut down and used by the navigator/cannoneer, the radio operator being moved to the aft compartment, operating the waist guns. A total of 405 B-25Gs and 1000 B-25Hs were built, the 248 of the latter being used by Navy as PBJ-1H.
The final, and the most built, version of the Mitchell, the B-25J, looked much like the earlier B, C and D, having reverted to the longer, glazed bombardier's nose, but with the -H version's relocated-forward dorsal manned turret. The less-than-successful 75 mm (2.95 in) cannon was deleted. Instead, 800 of this version were built with a solid nose containing eight .50  (12.7 mm) machine guns, while other J-models featured the earlier "greenhouse" style nose containing the bombardier's position.
Regardless of the nose style used, all J-models also included four .50 in (12.7 mm) light-barrel Browning AN/M2 guns in a pair of "fuselage package", flank-mount conformal gun pods each containing two Browning M2s, located directly beneath the pilot's and co-pilot's compartment along the external sides of the cockpit, with the co-pilot's seat and dual flight controls restored to their previous cockpit locations. The solid-nose B-25J variant carried a total of 18 .50 in (12.7 mm) light-barrel AN/M2 Browning M2 machine guns: eight in the nose, four in the flank-mount conformal gun pod packages, two in the dorsal turret, one each in the pair of waist positions, and a pair in the tail – with fourteen of the guns either aimed directly forward, or aimable to fire directly forward for strafing missions. No other main series production bomber of World War II carried as many guns.
The first 555 B-25Js (the B-25J-1-NC production block) were delivered without the fuselage package guns, because it was discovered that muzzle blast from these guns was causing severe stress in the fuselage; this problem was cured with heavier fuselage skin patches. Although later production runs returned these fuselage package guns to the aircraft, they were often removed as a field modification for the same reason. The later B-25J was additionally armed with eight 5 in (130 mm) high-velocity aircraft rockets (HVAR). In all, 4,318 B-25Js were built.
Flight characteristics.
The B-25 was a safe and forgiving aircraft to fly. With an engine out, 60° banking turns into the dead engine were possible, and control could be easily maintained down to 145 mph (230 km/h). However, the pilot had to remember to maintain engine-out directional control at low speeds after takeoff with rudder; if this maneuver was attempted with ailerons, the aircraft would snap out of control. The tricycle landing gear made for excellent visibility while taxiing. The only significant complaint about the B-25 was the extremely high noise level produced by its engines; as a result, many pilots eventually suffered from varying degrees of hearing loss. 
The high noise level was due to design and space restrictions in the engine cowlings which resulted in the exhaust "stacks" protruding directly from the cowling ring and partly covered by a small triangular fairing. This arrangement directed exhaust and noise directly at the pilot and crew compartments. Crew members and operators on the airshow circuit frequently comment that "the B-25 is the fastest way to turn aviation fuel directly into noise". Many B-25s now in civilian ownership have been modified with exhaust rings that direct the exhaust through the outboard bottom section of the cowling.
The Mitchell was an exceptionally sturdy aircraft that could withstand tremendous punishment. One well-known B-25C of the 321st Bomb Group was nicknamed "Patches" because its crew chief painted all the aircraft's flak hole patches with high-visibility zinc chromate primer. By the end of the war, this aircraft had completed over 300 missions, was belly-landed six times and sported over 400 patched holes. The airframe was so bent askew that straight-and-level flight required 8° of left aileron trim and 6° of right rudder, causing the aircraft to "crab" sideways across the sky.
An interesting characteristic of the B-25 was its ability to extend range by using one-quarter wing flap settings. Since the aircraft normally cruised in a slightly nose-high attitude, about 40 gal (150 l) of fuel was below the fuel pickup point and thus unavailable for use. The flaps-down setting gave the aircraft a more level flight attitude, which resulted in this fuel becoming available, thus slightly extending the aircraft's range.
By the time a separate United States Air Force was established in 1947, most B-25s had been consigned to long-term storage. However, a select number continued in service through the late 1940s and 1950s in a variety of training, reconnaissance and support roles. Its principal use during this period was for undergraduate training of multi-engine aircraft pilots slated for reciprocating engine or turboprop cargo, aerial refueling or reconnaissance aircraft. Still others were assigned to units of the Air National Guard in training roles in support of Northrop F-89 Scorpion and Lockheed F-94 Starfire operations. 
TB-25J-25-NC Mitchell, "44-30854", the last B-25 in the USAF inventory, assigned at March AFB, California as of March 1960, was flown to Eglin AFB, Florida, from Turner Air Force Base, Georgia, on 21 May 1960, the last flight by a USAF B-25, and presented by Brig. Gen. A. J. Russell, Commander of SAC's 822d Air Division at Turner AFB, to the Air Proving Ground Center Commander, Brig. Gen. Robert H. Warren, who in turn presented the bomber to Valparaiso, Florida Mayor Randall Roberts on behalf of the Niceville-Valparaiso Chamber of Commerce. Four of the original Tokyo Raiders were present for the ceremony, Col. Davy Jones, Col. Jack Simms, Lt. Col. Joseph Manske, and retired Master Sgt. Edwin W. Horton. It was donated back to the Air Force Armament Museum c. 1974 and marked as Doolittle's "40-2344".
Today, many B-25s are kept in airworthy condition by air museums and collectors.
U.S. Navy and USMC.
The PBJ-1 was a navalized version of the USAAF B-25, the designation standing for Patrol (P) Bomber (B) built by North American Aviation (J), first variant (-1). The PBJ had its origin in a deal cut in mid-1942 between the Navy and the USAAF. As part of the deal, 50 B-25Cs and 152 B-25Ds were transferred to the Navy from the USAAF. The bombers carried Navy bureau numbers (BuNos) beginning with BuNo 34998. The first PBJ-1s arrived in February 1943 and were used by Marine Corps pilots, beginning with Marine Bombing squadron 413 (VMB-413). Many of PBJs were equipped with a search radar with a retractable radome fitted in place of the ventral turret.
Large numbers of B-25H and J variants were delivered to the Navy as PBJ-1H and PBJ-1J respectively. These aircraft joined, but did not necessarily replace, the earlier PBJs. 
The PBJs were operated almost exclusively by the Marine Corps as land-based bombers. To operate them, the U.S. Marine Corps established a number of marine bomber squadrons (VMB), beginning with VMB-413, in March 1943 at MCAS Cherry Point, North Carolina. Eight VMB squadrons were flying PBJs by the end of 1943, forming the initial marine medium bombardment group. Four more squadrons were in the process of formation in late 1945, but had not yet deployed by the time the war ended. 
Operational use of the Marine Corps PBJ-1s began in March 1944. The marine PBJs operated from the Philippines, Saipan, Iwo Jima and Okinawa during the last few months of the Pacific war. Their primary mission was the long range interdiction of enemy shipping that was trying to run the blockade which was strangling Japan. The weapon of choice during these missions was usually the five-inch HVAR rocket, eight of which could be carried on underwing racks. 
Many of the PBJ-1C and D versions carried a rather ugly, bulbous antenna for an APS-3 search radar sticking out of the upper part of the transparent nose. On the PBJ-1H and J, the APS-3 search radar antenna was usually housed inside a ventral or wingtip radome. Some PBJ-1Js had their top turrets removed to save weight, especially towards the end of the war when Japanese fighters had become relatively scarce.
After World War Two, some PBJs were stationed at the navy's rocket laboratory at Inyokern, California, the present-day Naval Air Weapons Station China Lake, to test various air-to-ground rockets and arrangements, including a twin-barrel nose arrangement that could fire ten spin-stabilized five inch rockets in one salvo.
Royal Air Force.
The Royal Air Force (RAF) was an early customer for the B-25 via Lend-Lease. The RAF was the only force to use the B-25 on raids against Europe from bases in the United Kingdom, as the USAAF used the Martin B-26 Marauder and Boeing B-17 Flying Fortress for this purpose instead. 
The first Mitchells were designated Mitchell I by the RAF and were delivered in August 1941, to No 111 Operational Training Unit based in the Bahamas. These bombers were used exclusively for training and familiarization and never achieved operational status. The B-25Cs and Ds were designated Mitchell II, altogether, 167 B-25Cs and 371 B-25Ds were delivered to the RAF. 
A total of 93 Mitchell Is and IIs had been delivered to the RAF by the end of 1942 and served with No. 2 Group RAF, the RAF's tactical medium bomber force. The first RAF operation with the Mitchell II took place on 22 January 1943, when six aircraft from No. 180 Squadron RAF attacked oil installations at Ghent. After the invasion of Europe, all four Mitchell squadrons moved to bases in France and Belgium (Melsbroek) to support Allied ground forces. The British Mitchell squadrons were joined by No. 342 (Lorraine) Squadron of the French Air Force in April 1945. 
No 305 (Polish) Squadron flew Mitchell IIs from September to December 1943 before converting to Mosquitos. In addition to the 2nd Group, the B-25 was used by various second-line RAF units in the UK and abroad. In the Far East, No. 3 PRU, which consisted of Nos. 681 and 684 Squadrons, flew the Mitchell (primarily Mk IIs) on photographic reconnaissance sorties. 
The RAF was allocated 316 B-25Js which entered service as the Mitchell III. Deliveries took place between August 1944 and August 1945. However, only about 240 of these bombers actually reached Britain, with some being diverted to No. 111 OTU in the Bahamas, some crashing during delivery and some being retained in the United States.
Royal Canadian Air Force.
The Royal Canadian Air Force (RCAF) was an important user of the B-25 Mitchell, although most of the RCAF use of the 162 Mitchells delivered was postwar. The first B-25s for the RCAF had originally been diverted to Canada from RAF orders. These included one Mitchell I, 42 Mitchell IIs, and 19 Mitchell IIIs. No 13 (P) Squadron was formed unofficially at RCAF Rockcliffe in May 1944. They operated Mitchell IIs on high-altitude aerial photography sorties. The No. 5 OTU (Operational Training Unit) at Boundary Bay, British Columbia and Abbotsford, British Columbia operated the Mitchell in the training role together with B-24 Liberators for Heavy Conversion as part of the BCATP. The RCAF retained the Mitchell until October 1963. 
No 418 (Auxiliary) Squadron received its first Mitchell IIs in January 1947. It was followed by No 406 (auxiliary) which flew Mitchell IIIs from April 1947 to June 1958. No 418 Operated a mix of IIs and IIIs until March 1958. No 12 Squadron of Air Transport Command also flew Mitchell IIIs along with other types from September 1956 to November 1960. In 1951, the RCAF received an additional 75 B-25Js from USAF stocks to make good attrition and to equip various second line units.
Royal Australian Air Force.
The Australians got Mitchells by the spring of 1944. The joint Australian-Dutch No. 18 (Netherlands East Indies) Squadron RAAF had more than enough Mitchells for one squadron so the surplus went to re-equip the RAAF's No. 2 Squadron, replacing their Beauforts.
Dutch Air Force.
During World War II, the Mitchell served in fairly large numbers with the Air Force of the Dutch government-in-exile. They participated in combat both in the East Indies as well as on the European front. On 30 June 1941, the Netherlands Purchasing Commission, acting on behalf of the Dutch government-in-exile in London, signed a contract with North American Aviation for 162 B-25C aircraft. The bombers were to be delivered to the Netherlands East Indies to help deter any Japanese aggression into the region. 
In February 1942, the British Overseas Airways Corporation (BOAC) agreed to ferry 20 of the Dutch B-25s from Florida to Australia via Africa and India, and an additional ten via the South Pacific route from California. During March, five of the bombers on the Dutch order had reached Bangalore, India and 12 had reached Archerfield in Australia. It was agreed that the B-25s in Australia would be used as the nucleus of a new squadron, designated No. 18. This squadron would be staffed jointly by Australian and Dutch aircrews plus a smattering of aircrews from other nations, but would operate at least initially under Royal Australian Air Force command. 
The B-25s of No. 18 Squadron would be painted with the Dutch national insignia (at this time a rectangular Netherlands flag) and would carry NEIAF serials. Discounting the ten "temporary" B-25s delivered to 18 Squadron in early 1942, a total of 150 Mitchells were taken on strength by the NEIAF, 19 in 1942, 16 in 1943, 87 in 1944, and 28 in 1945. They flew bombing raids against Japanese targets in the East Indies. In 1944, the more capable B-25J Mitchell replaced most of the earlier C and D models. 
In June 1940, No. 320 Squadron RAF had been formed from personnel formerly serving with the Royal Dutch Naval Air Service who had escaped to England after the German occupation of the Netherlands. Equipped with various British aircraft, No. 320 Squadron flew anti-submarine patrols, convoy escort missions, and performed air-sea rescue duties. They acquired the Mitchell II in September 1943, performing operations over Europe against gun emplacements, railway yards, bridges, troops and other tactical targets. They moved to Belgium in October 1944, and transitioned to the Mitchell III in 1945. No. 320 Squadron was disbanded in August 1945. Following the war, B-25s were used in a vain attempt of the Dutch to retain control of Indonesia.
Soviet Air Force.
The U.S. supplied 862 B-25 (of B, D, G, and J types) aircraft to the Soviet Union under lend-lease during the Second World War via the Alaska–Siberia ALSIB ferry route. 
Other damaged aircraft arrived or crashed in the Far East of Russia, and one Doolittle Raid aircraft landed there short of fuel after attacking Japan. The lone airworthy aircraft to reach the Soviet Union was lost in a hangar fire in the early 50s while undergoing routine maintenance. In general, the B-25 was operated as a ground-support and tactical daylight bomber (as similar Douglas A-20 Havocs were used). It saw action in fights from Stalingrad (with B/D models) to the German surrender during May 1945 with (G/J types).
B-25s that remained in Soviet Air Force service after the war were assigned the NATO reporting name "Bank".
China Air Force.
Well over 100 B-25Cs and Ds were supplied to the Nationalist Chinese during the Second World War. In addition, a total of 131 B-25Js were supplied to China under Lend-Lease. 
The four squadrons of the 1st BG (1st, 2nd, 3rd, and 4th) of the 1st Medium Bomber Group were formed during the War. They formerly operated Russian-built Tupolev SB bombers, then transferred to the B-25. The 1st BG was under the command of CACW (Chinese-American Composite Wing) while operating B-25. Following the end of the war in the Pacific, these four bombardment squadrons were established to fight against the Communist insurgency that was rapidly spreading throughout the country. During the civil war, Chinese Mitchells fought alongside de Havilland Mosquitos. 
In December 1948, the Nationalists were forced to move to the island of Taiwan, taking many of their Mitchells with them. However, some B-25s were left behind and were impressed into service with the air force of the new People's Republic of China.
Brazilian Air Force.
During the war, the Força Aérea Brasileira (FAB) received a few B-25s under Lend-Lease. Brazil declared war against the Axis powers in August 1942 and participated in the war against the U-boats in the southern Atlantic. The last Brazilian B-25 was finally declared surplus in 1970.
Free French.
At least 21 Mitchell IIIs were issued by the Royal Air Force to No 342 Squadron, which was made up primarily of Free French aircrews. 
Following the liberation of France, this squadron was transferred to the newly formed French air force (Armée de l'Air) as GB I/20 Lorraine. These aircraft were operated by GB I/20 after the war, some being converted from bomber configuration into fast VIP transports. They were finally struck off charge in June 1947.
Empire State Building incident.
At 9:40 on Saturday, 28 July 1945, a USAAF B-25D crashed in thick fog into the north side of the Empire State Building between the 79th and 80th floors. Fourteen people died – eleven in the building and the three occupants of the aircraft including the pilot, Colonel William Smith. Betty Lou Oliver, an elevator attendant, survived the impact and a subsequent uncontrolled descent in the elevator.
Partly as a result of this incident, Towers 1 and 2 of the World Trade Center were designed to withstand an aircraft impact. However, this design was based on an impact by a Boeing 707 aircraft in common use in the late 1960s and early 1970s, not the larger Boeing 767, two of which, (American Airlines Flight 11 and United Airlines Flight 175), struck the towers on September 11, 2001, resulting in their subsequent collapse.
Variants.
Trainer variants.
Most models of the B-25 were used at some point as training aircraft.

</doc>
<doc id="4219" url="http://en.wikipedia.org/wiki?curid=4219" title="British Open (disambiguation)">
British Open (disambiguation)

The British Open is the Open Championship men's golf tournament.
British Open may also refer to:

</doc>
<doc id="4224" url="http://en.wikipedia.org/wiki?curid=4224" title="Bobby Charlton">
Bobby Charlton

Sir Robert "Bobby" Charlton CBE (born 11 October 1937) is an English former football player, regarded as one of the greatest midfielders of all time, and an essential member of the England team who won the World Cup and also won the Ballon d'Or in 1966. He played almost all of his club football at Manchester United, where he became renowned for his attacking instincts and passing abilities from midfield and his ferocious long-range shot. He was also well known for his fitness and stamina. His elder brother Jack, who was also in the World Cup-winning team, is a former defender for Leeds United and international manager.
Born in Ashington, Northumberland, Charlton made his debut for the Manchester United first-team in 1956, and over the next two seasons gained a regular place in the team, during which time he survived the Munich air disaster of 1958 after being rescued by Harry Gregg. After helping United to win the Football League in 1965, he won a World Cup medal with England in 1966 and another Football League title with United the following year. In 1968, he captained the Manchester United team that won the European Cup, scoring two goals in the final to help his team be the first English side to win the competition. He has scored more goals for England and United than any other player. Charlton held the record for most appearances for Manchester United (758), before being surpassed by Ryan Giggs.
He was selected for four World Cups (1958, 1962, 1966, and 1970), and helped England to win the competition in 1966. At the time of his retirement from the England team in 1970, he was the nation's most capped player, having turned out 106 times at the highest level. This record has since been eclipsed by Bobby Moore, Peter Shilton, David Beckham, Steven Gerrard and Ashley Cole.
He left Manchester United to become manager of Preston North End for the 1973–74 season. He changed to player-manager the following season. He next accepted a post as a director with Wigan Athletic, then became a member of Manchester United's board of directors in 1984 and remains one as of August 2014.
Early life.
Charlton is related to several professional footballers on his mother's side of the family: his uncles were Jack Milburn (Leeds United and Bradford City), George Milburn (Leeds United and Chesterfield), Jim Milburn (Leeds United and Bradford City) and Stan Milburn (Chesterfield, Leicester City and Rochdale), and legendary Newcastle United and England footballer Jackie Milburn, was his mother's cousin. However, Charlton credits much of the early development of his career to his grandfather Tanner and his mother Cissie. His elder brother, Jack, initially went to work applying to the Police Service before also becoming a professional footballer with Leeds United.
Club career.
On 9 February 1953, then a Bedlington Grammar School pupil, Charlton was spotted playing for East Northumberland schools by Manchester United chief scout Joe Armstrong. Charlton went on to play for England schoolboys and the 15-year-old signed with United on 1 January 1953, along with Wilf McGuinness, also aged 15. Initially his mother was reluctant to let him commit to an insecure football career, so he began an apprenticeship as an electrical engineer; however he went on to turn professional in October 1954.
Charlton became one of the famed Busby Babes, the collection of talented footballers who emerged through the system at Old Trafford in the 1940s, 1950s and 1960s as Matt Busby set about a long-term plan of rebuilding the club after the Second World War. He worked his way through the pecking order of teams, scoring regularly for the youth and reserve sides before he was handed his first team debut against Charlton Athletic in October 1956. At the same time, he was doing his National service with the Royal Army Ordnance Corps in Shrewsbury, where Busby had advised him to apply as it meant he could still play for Manchester United at the weekend. Also doing his army service in Shrewsbury at the same time was his United team-mate Duncan Edwards.
Charlton played 14 times for United in that first season, scoring twice on his debut and managing a total of 12 goals in all competitions, and including a hat-trick in a 5–1 away win over Charlton Athletic in the February. United won the league championship but were denied the 20th century's first "double" when they controversially lost the 1957 FA Cup Final to Aston Villa. Charlton, still only 19, was selected for the game, which saw United goalkeeper Ray Wood carried off with a broken cheekbone after a clash with Villa centre forward Peter McParland. Though Charlton was a candidate to go in goal to replace Wood (in the days before substitutes, and certainly before goalkeeping substitutes), it was teammate Jackie Blanchflower who ended up between the posts.
Charlton was an established player by the time the next season was fully underway, which saw United, as current League champions, become the first English team to compete in the European Cup. Previously, the Football Association had scorned the competition but United made progress, reaching the semi-finals where they lost to holders Real Madrid. Their reputation was further enhanced the next season as they reached the quarter finals to play Red Star Belgrade. In the first leg at home, United won 2–1. The return in Yugoslavia saw Charlton score twice as United stormed 3–0 ahead, although the hosts came back to earn a 3–3 draw. However, United maintained their aggregate lead to reach the last four and were in jubilant mood as they left to catch their flight home, thinking of an important League game against Wolves at the weekend. But the plane, carrying 44 passengers and crew (including the 17-strong Manchester United squad) crashed on take-off after re-fuelling at Munich. Charlton survived with minor injuries, but 23 people (eight of them Manchester United players) died as a result of their injures in the crash. Of the eight other players who survived, two of them were injured so badly that they never played again.
Further success with Manchester United came at last when they beat Leicester City 3–1 in the FA Cup final of 1963, with Charlton finally earning a winners' medal in his third final. Busby's post-Munich rebuilding programme continued to progress with two League championships within three seasons, with United taking the title in 1965 and 1967. A successful (though trophyless) season with Manchester United had seen him take the honours of "Football Writers' Association Footballer of the Year" and "European Footballer Of The Year" into the competition.
In 1968, Manchester United reached the European Cup final, ten seasons after Munich. Even though other clubs had taken part in the competition in the intervening decade, the team which got to this final was still the first English side to do so. On a highly emotional night at Wembley, Charlton scored twice in a 4–1 win after extra time against Benfica and, as United captain, lifted the trophy.
During the early 1970s, Manchester United were no longer competing among the top teams in England, and at several stages were battling against relegation. At times, Charlton was not on speaking terms with United's other superstars George Best and Denis Law, and Best refused to play in Charlton's testimonial match against Celtic, saying that "to do so would be hypocritical". Charlton left Manchester United at the end of the 1972–73 season, having scored 249 goals and set a club record of 758 appearances, a record which Ryan Giggs broke in the 2008 UEFA Champions League Final.
His last game was against Chelsea at Stamford Bridge on 28 April 1973, and before the game the BBC cameras for "Match of the Day" captured the Chelsea chairman handing Charlton a commemorative cigarette case. The match ended in a 1-0 defeat. His final goal came a month earlier, on 31 March, in a 2-0 win at Southampton, also in the First Division.
International career.
Charlton's emergence as the country's leading young football talent was completed when he was called up to join the England squad for a British Home Championship game against Scotland at Hampden Park on 19 April 1958, just over two months after he had survived the Munich air disaster.
Charlton was handed his debut as England romped home 4–0, with the new player gaining even more admirers after scoring a magnificent thumping volley dispatched with authority after a cross by the left winger Tom Finney. He scored both goals in his second game as England beat Portugal 2–1 in a friendly at Wembley; and overcame obvious nerves on a return to Belgrade to play his third match against Yugoslavia. England lost that game 5–0 and Charlton played poorly.
1958 World Cup.
He was selected for the squad which competed at the 1958 World Cup in Sweden, but didn't kick a ball, something at which critics expressed surprise and bewilderment, even allowing for his lacklustre performance in Belgrade.
In 1959 he scored a hat-trick as England demolished the US 8–1; and his second England hat-trick came in 1961 in an 8–0 thrashing of Mexico. He also managed to score in every British Home Championship tournament he played in except 1963 in an association with the tournament which lasted from 1958 to 1970 and included 16 goals and ten tournament victories (five shared).
1962 World Cup.
He played in qualifiers for the 1962 World Cup in Chile against Luxembourg and Portugal and was named in the squad for the finals themselves. His goal in the 3–1 group win over Argentina was his 25th for England in just 38 appearances, and he was still only 24 years old, but his individual success could not be replicated by that of the team, which was eliminated in the quarter final by Brazil, who went on to win the tournament.
By now, England were coached by Alf Ramsey who had managed to gain sole control of the recruitment and team selection procedure from the committee-based call-up system which had lasted up to the previous World Cup. Ramsey had already cleared out some of the older players who had been reliant on the loyalty of the committee for their continued selection – it was well known that decorum on the pitch at club level had been just as big a factor in playing for England as ability and form. Luckily for Charlton, he had all three.
A hat-trick in the 8–1 rout of Switzerland in June 1963 took Charlton's England goal tally to 30, equalling the record jointly held by Tom Finney and Nat Lofthouse and Charlton's 31st goal against Wales in October the same year gave him the record alone. Charlton's role was developing from traditional inside-forward to what today would be termed an attacking midfield player, with Ramsey planning to build the team for the 1966 World Cup around him. When England beat the USA 10-0 in a friendly on 27 May 1964, he scored one goal, his 33rd at senior level for England.
His goals became a little less frequent, and indeed Jimmy Greaves, playing purely as a striker, would overtake Charlton's England tally in October 1964. Nevertheless, he was still scoring and creating freely and as the tournament was about to start, he was expected to become one of its stars and galvanise his established reputation as one of the world's best footballers.
1966 World Cup.
England drew the opening game of the tournament 0–0 with Uruguay, and Charlton scored the first goal in the 2–0 win over Mexico. This was followed by an identical scoreline against France, allowing England to qualify for the quarter finals.
England defeated Argentina 1–0 – the game was the only one in which Charlton received a caution – and faced Portugal in the semi finals. This turned out to be one of Charlton's most important games for England.
Charlton opened the scoring with a crisp side-footed finish after a run by Roger Hunt had forced the Portuguese goalkeeper out of his net; his second was a sweetly struck shot after a run and pull-back from Geoff Hurst. Charlton and Hunt were now England's joint-highest scorers in the tournament with three each, and a final against West Germany beckoned.
The final turned out to be one of Charlton's quieter days; he and a young Franz Beckenbauer effectively marked each other out of the game. England won 4–2 after extra time.
Euro 1968.
Charlton's next England game was his 75th as England beat Northern Ireland; 2 caps later and he had become England's second most-capped player, behind the veteran Billy Wright, who was approaching his 100th appearance when Charlton was starting out and ended with 105 caps.
Weeks later he scored his 45th England goal in a friendly against Sweden, breaking the record of 44 set the previous year by Jimmy Greaves. He was then in the England team which made it to the semi-finals of the 1968 European Championships where they were knocked out by Yugoslavia in Florence. During the match Charlton struck a Yugoslav post. England defeated the Soviet Union 2–0 in the third place match.
In 1969, Charlton was awarded the OBE for services to football. More milestones followed as he won his 100th England cap on 21 April 1970 against Northern Ireland, and was made captain by Ramsey for the occasion. Inevitably, he scored. This was his 48th goal for his country – his 49th and final goal would follow a month later in a 4–0 win over Colombia during a warm-up tour for the 1970 World Cup, designed to get the players adapted to altitude conditions. Charlton's inevitable selection by Ramsey for the tournament made him the first – and still, to date, only – England player to feature in four World Cup squads.
1970 World Cup.
Shortly before the World Cup Charlton was involved in the Bogotá Bracelet incident in which he and Bobby Moore were accused of stealing a bracelet from a jewellery store. Moore was later arrested and detained for four days before being granted a conditional release, while Charlton was not arrested.
England began the tournament with two victories in the group stages, plus a memorable defeat against Brazil. Charlton played in all three, though was substituted for Alan Ball in the final game of the group against Czechoslovakia. Ramsey, confident of victory and progress to the quarter final, wanted Charlton to rest.
England duly reached the last eight where they again faced West Germany. Charlton controlled the midfield and suppressed Franz Beckenbauer's runs from deep as England coasted to a 2–0 lead. Beckenbauer pulled a goal back for the Germans and Ramsey replaced the ageing and tired Charlton with Colin Bell who further tested the German keeper Maier and also provided a great cross for Geoff Hurst who uncharacteristically squandered the chance. West Germany, who had a habit of coming back from behind, eventually scored twice – a back header from Uwe Seeler made it 2–2. In extra-time, Geoff Hurst had a goal mysteriously ruled out after which Gerd Müller's goal won the match 2-3. England were out and, after a record 106 caps and 49 goals, Charlton decided to end his international career at the age of 32. On the flight home from Mexico, he asked Ramsey not to consider him again. His brother Jack, two years his senior but 71 caps his junior, did likewise.
Despite popular opinion the substitution did not change the game as Franz Beckenbauer had scored before Charlton left the field, hence Charlton had failed to cancel out the German. Charlton himself conceded that the substitution did not affect the game in a BBC documentary. His caps record lasted until 1973 when Bobby Moore overtook him, and Charlton currently lies sixth in the all-time England appearances list behind Moore, Ashley Cole, Steven Gerrard, David Beckham and Peter Shilton, whose own England career began in the first game after Charlton's had ended. As of May 2014, Charlton's goalscoring record still stands.
Munich air disaster.
The aeroplane which took the United players and staff home from Zemun Airport needed to stop in Munich to refuel. This was carried out in worsening weather, and by the time the refuelling was complete and the call was made for the passengers to re-board the aircraft, the wintry showers had taken hold and snow had settled heavily on the runway and around the airport. There were two aborted take-offs which led to concern on board, and the passengers were advised by a stewardess to disembark again while a minor technical error was fixed.
The team was back in the airport terminal barely ten minutes when the call to reconvene on the plane came, and a number of passengers began to feel nervous. Charlton and teammate Dennis Viollet swapped places with Tommy Taylor and David Pegg, who had decided they would be safer at the back of the plane.
The plane clipped the fence at the end of the runway on its next take-off attempt and a wing tore through a nearby house, setting it alight. The wing and part of the tail came off and hit a tree and a wooden hut, the plane spinning along the snow until coming to a halt. It had been cut in half.
Charlton, strapped into his seat, had fallen out of the cabin and when United goalkeeper Harry Gregg (who had somehow got through a hole in the plane unscathed and begun a one-man rescue mission) found him, he thought he was dead. That said, he grabbed both Charlton and Viollet by their trouser waistbands and dragged them away from the plane in constant fear that it would explode. Gregg returned to the plane to try to help the appallingly injured Busby and Blanchflower, and when he turned around again, he was relieved to see that Charlton and Viollet, both of whom he had presumed to be dead, had got out of their detached seats and were looking into the wreckage.
Charlton suffered cuts to his head and severe shock and was in hospital for a week. Seven of his teammates had perished at the scene, including Taylor and Pegg, with whom he and Viollet had swapped seats prior to the fatal take-off attempt. Club captain Roger Byrne was also killed, along with Mark Jones, Billy Whelan, Eddie Colman and Geoff Bent. Duncan Edwards died a fortnight later from the injuries he had sustained. In total, the crash claimed 23 lives. Initially, ice on the wings was blamed, but a later inquiry declared that slush on the runway had made a safe take-off almost impossible.
Charlton was the first injured survivor to leave hospital. Harry Gregg and Bill Foulkes were not hospitalized since they escaped uninjured. He arrived back in England on 14 February 1958, eight days after the crash. As he convalesced with family in Ashington, he spent some time kicking a ball around with local youths, and a famous photograph of him was taken. He was still only 20 years old, yet now there was an expectation that he would help with the rebuilding of the club as Busby's aides tried to piece together what remained of the season. He returned to playing in an FA cup tie against West Bromwich Albion on 1 March; the game was a draw and United won the replay 1–0.
Not unexpectedly, United went out of the European Cup to Milan in the semi-finals to a 5–2 aggregate defeat and fell behind in the League. Yet somehow they reached their second consecutive FA Cup final, and the big day at Wembley coincided with Busby's return to work. However, his words could not inspire a side which was playing on a nation's goodwill and sentiment, and Nat Lofthouse scored twice to give Bolton Wanderers side a 2–0 win.
Management career and directorships.
Charlton became the manager of Preston North End in 1973, signing his former United and England team-mate Nobby Stiles as player-coach. His first season ended in relegation and although he began playing again he left Preston early in the 1975–76 season after a disagreement with the board over the transfer of John Bird to Newcastle United. He was awarded the CBE that year and began a casual association with the BBC for punditry on matches which continued for many years. In early 1976, he scored once in 3 league appearances for Waterford United.
He joined Wigan Athletic as a director, and was briefly caretaker manager there in 1983. He then spent some time playing in South Africa. He also built up several businesses in areas such as travel, jewellery and hampers, and ran soccer schools in the UK, the US, Canada, Australia and China. In 1984, he was invited to become member of the board of directors at Manchester United, partly because of his football knowledge and partly because it was felt that the club needed a "name" on the board after the resignation of Sir Matt Busby. He remains a director of Manchester United as of 2014 and his continued presence was a factor in placating many fans opposed to the club's takeover by Malcolm Glazer.
Personal life and retirement.
He met his wife, Norma Ball, at an ice rink in Manchester in 1959 and they married in 1961. They have two daughters – Suzanne and Andrea. Suzanne was a weather forecaster for the BBC during the 1990s. They now have grandchildren, including Suzanne's son Robert, who is named in honour of his grandfather.
In 2007, while publicising his forthcoming autobiography, Charlton revealed that he had a long-running feud with his brother, Jack. They have rarely spoken since a falling-out between his wife Norma and his mother Cissie (who died on 25 March 1996 at the age of 83). Charlton did not see his mother after 1992 as a result of the feud.
Jack presented him with his BBC Sports Personality of the Year Lifetime Achievement Award on 14 December 2008. He said that he was 'knocked out' as he was presented the award by his brother. He received a standing ovation as he stood waiting for his prize.
Charlton helped to promote Manchester's bids for the 1996 and 2000 Olympic Games and the 2002 Commonwealth Games, England's bid for the 2006 FIFA World Cup and London's successful bid for the 2012 Summer Olympics. He received a knighthood in 1994 and was an Inaugural Inductee to the English Football Hall of Fame in 2002. On accepting his award he commented "I'm really proud to be included in the National Football Museum's Hall of Fame. It's a great honour. If you look at the names included I have to say I couldn't argue with them. They are all great players and people I would love to have played with." He is also the (honorary) president of the National Football Museum, an organisation about which he said "I can't think of a better Museum anywhere in the world." On 14 December 2008 Charlton was awarded the prestigious BBC Sports Personality of the Year Lifetime Achievement Award.
On 2 March 2009, Charlton was given the freedom of the city of Manchester, stating "I'm just so proud, it's fantastic. It's a great city. I have always been very proud of it." 
Charlton is involved in a number of charitable activities including fund raising for cancer hospitals. Charlton became involved in the cause of land mine clearance after visits to Bosnia and Cambodia and supports the Mines Advisory Group as well as founding his own charity Find a Better Way which funds research into improved civilian landmine clearance.
In January 2011 Charlton was voted the 4th greatest Manchester United player of all time by the readers of Inside United and ManUtd.com, behind Ryan Giggs (who topped the poll), Eric Cantona and George Best.
He is a member of the Laureus World Sports Academy. On 6 February 2012 Sir Bobby Charlton was taken to hospital after falling ill, and subsequently had a gallstone removed. This prevented him from collecting a Lifetime Achievement award at the Laureus World Sports Awards.
In the 2011 film "United", centred on the successes of the Busby Babes and the decimation of the team in the Munich crash, Charlton was portrayed by actor Jack O'Connell.
External links.
 

</doc>
<doc id="4227" url="http://en.wikipedia.org/wiki?curid=4227" title="Barry Lyndon">
Barry Lyndon

Barry Lyndon is a 1975 British-American period drama film written, produced and directed by Stanley Kubrick, based on the 1844 novel "The Luck of Barry Lyndon" by William Makepeace Thackeray. It stars Ryan O'Neal, Marisa Berenson, Patrick Magee and Hardy Krüger. The film recounts the exploits of a fictional 18th-century Irish adventurer. Most of the exteriors were shot on location in Ireland. At the 1975 Academy Awards, the film won four Oscars in production categories.
The film, which had a modest commercial success and a mixed critical reception on initial release, is now regarded as one of Kubrick's finest films. In numerous polls, such as "Village Voice" (1999), "Sight & Sound" (2002), and "Time" (2005), it has been rated one of the greatest films ever made. Director Martin Scorsese has cited "Barry Lyndon" as his favorite Kubrick film. Quotations from its script have also appeared in such disparate works as Ridley Scott's "The Duellists", Scorsese's "The Age of Innocence", and Wes Anderson's "Rushmore".
Plot.
Act I.
An omniscient (though unreliable) narrator (voiced by Michael Hordern) states that in 1750s Ireland, the father of Redmond Barry (Ryan O'Neal) is killed in a duel over a disputed horse sale. The widow (Marie Kean), disdaining offers of marriage, devotes herself to her only son.
As a teenager, Barry falls in love with his older cousin, Nora Brady (Gay Hamilton). Though she seduces him, she later drops Barry (who has no money) for the well-off English Captain John Quin (Leonard Rossiter). Nora and her family plan to relieve their poverty with an advantageous marriage, but Barry refuses to accept the situation and shoots Quin in a duel.
Barry flees to Dublin, but en route is robbed of purse and equipment by Captain Feeney (Arthur O'Sullivan), an infamous highwayman. Broke, Barry joins the British army, whereupon he reunites with Captain Grogan (Godfrey Quigley), a family friend, who informs him that, in fact, he did not kill Quin — Barry's dueling pistol was loaded with tow. The duel was staged by Nora's family to get rid of Barry so that their family finances would be secured through the marriage of Nora and Quin.
Barry's regiment is sent to Germany to fight in the Seven Years' War, where Captain Grogan is fatally wounded by the French in a skirmish at the Battle of Minden. Barry deserts the army, stealing an officer courier's uniform, horse, and identification papers. En route to neutral Holland he encounters the Prussian Captain Potzdorf (Hardy Krüger), who, seeing through his disguise, offers him the choice of being turned back over to the British where he will be shot as a deserter, or enlisting in the Prussian army. Barry enlists in his second army and later receives a special commendation from Frederick the Great for saving Potzdorf's life in a battle.
After the war ends in 1763, Barry is employed by Captain Potzdorf's uncle in the Prussian Ministry of Police to become the servant of the Chevalier de Balibari (Patrick Magee), a professional gambler. The Prussians suspect he is a spy and send Barry as an undercover agent to verify this. Barry reveals himself to the Chevalier right away and they become confederates cheating at cards.
After he and the Chevalier cheat the Prince of Tübingen at the cardtable, the Prince accuses the Chevalier (without proof) and refuses to pay his debt unless the Chevalier demands satisfaction. When Barry relays this to his Prussian handlers, they (still suspecting that the Chevalier is a spy) are wary of allowing another meeting between the Chevalier and the Prince. So, the Prussians arrange for the Chevalier to be expelled from the country. Barry conveys this plan to the Chevalier, who flees in the night. The next morning, Barry, under disguise as the Chevalier, is escorted from Prussian territory by Prussian officers.
For the next few years, Barry and the Chevalier travel the spas and parlors of Europe, profiting from their gambling with Barry enforcing reluctant debtors with a duel. Seeing that his life is going nowhere, Barry decides to marry into wealth. At a gambling table in Belgium, he encounters the beautiful and wealthy Countess of Lyndon (Marisa Berenson). He seduces and later marries her after the death of her elderly husband, Sir Charles Lyndon (Frank Middlemass).
 Intermission 
Act II.
In 1773, Barry takes the Countess' last name in marriage and settles in England to enjoy her wealth, still with no money of his own. Lord Bullingdon (Dominic Savage), Lady Lyndon's 10-year-old son by Sir Charles, does not approve of the marriage and quickly comes to hate Barry, aware that Barry is merely a "common opportunist" and does not love his mother. The Countess bears Barry a son, Bryan Patrick, but the marriage is unhappy: Barry is openly unfaithful and enjoys spending his wife's money in self-indulgent spending sprees while keeping his wife in dull seclusion.
Some years later, Barry's mother comes to live with him at the Lyndon estate. She warns her son that his position is precarious: If Lady Lyndon were to die, all her wealth would go to her first-born son Lord Bullingdon (now a young man, played by Leon Vitali), leaving Barry penniless. Barry's mother advises him to obtain a noble title to protect himself. To further this goal, he cultivates the acquaintance of the influential Lord Wendover (André Morell) and begins to expend even larger sums of money to ingratiate himself to high society. All this effort is wasted, however, during a birthday party for Lady Lyndon, where Lord Bullingdon announces his hatred of his stepfather and his intention to leave the family estate for as long as his mother remains married to Barry. Angered, Barry assaults Bullingdon before the guests. This public display of cruelty loses Barry all the powerful friends he has worked so hard to make and he is shunned socially. Bullingdon makes good on his announcement and leaves the estate and England itself for parts unknown.
In contrast to his mistreatment of his stepson, Barry proves a compassionate and doting father to Bryan, with whom he spends all his time after Bullingdon's departure. He cannot refuse his son anything, and succumbs to Bryan's insistence on receiving a full-grown horse for his ninth birthday. The spoiled Bryan disobeys his parents' direct instructions that Bryan ride the horse only in the presence of his father, and is thrown by the horse. Bryan dies a few days later from his injuries.
The grief-stricken Barry turns to alcohol, while Lady Lyndon seeks solace in religion, assisted by the Reverend Samuel Runt (Murray Melvin), who had been tutor first to Lord Bullingdon and then to Bryan. Left in charge of the families' affairs while Barry and Lady Lyndon grieve, Barry's mother dismisses the Reverend, both because the family no longer needs (nor can afford, due to Barry's spending debts) a tutor and for fear that his influence worsens Lady Lyndon's condition. Plunging even deeper into grief, Lady Lyndon later attempts suicide (though she ingests only enough poison to make herself ill). The Reverend and the family's accountant and emissary Graham (Philip Stone) then seek out Lord Bullingdon. Upon hearing of these events, Lord Bullingdon returns to England where he finds Barry in a local tavern getting drunk and mourning the loss of his son rather than being with Lady Lyndon. Bullingdon demands "satisfaction" (a challenge to a duel) for Barry's public assault.
The duel with pistols is held in an abandoned chapel. A coin-toss gives Bullingdon the right of first fire, but his pistol misfires as it is being cocked. Barry, reluctant to shoot Bullingdon, magnanimously fires into the ground, but the unmoved Bullingdon refuses to let the duel end. In the second round, Bullingdon shoots Barry in his left leg. At a nearby inn, a surgeon informs Barry that the leg will need to be amputated below the knee if he is to survive.
While Barry is recovering, Bullingdon takes control of the estate. He sends a very nervous Graham to the inn with a proposition: Bullingdon will grant Barry an annuity of 500 guineas per year for life on the conditions that he leave England forever and end his marriage to Lady Lyndon. Otherwise, with his credit and bank accounts exhausted, Barry's creditors and bill collectors will assuredly see that he is jailed. Defeated, Barry accepts. The narrator states that Barry goes first to Ireland with his mother, then to the European continent to resume his former profession of gambler (though without his former success) and that he never sees Lady Lyndon again. The final scene (set in December 1789) shows the middle-aged Lady Lyndon signing Barry's annuity cheque as Bullingdon looks on.
Cast.
Critic Tim Robey suggests, that the film "makes you realise that the most undervalued aspect of Kubrick's genius could well be his way with actors." He adds that the supporting cast is a "glittering procession of cameos, not from star names but from vital character players."
The cast featured Leon Vitali as the older Lord Bullingdon, who would then become Kubrick's personal assistant, working as the casting director on his following films, and supervising film-to-video transfers for Kubrick. Their relationship lasted until Kubrick's death. The film's cinematographer, John Alcott, appears at the men's club in the non-speaking role of the man asleep in a chair near the title character when Lord Bullingdon challenges Barry to a duel. Kubrick's daughter Vivian also appears (in an uncredited role) as a guest at Bryan's birthday party.
Kubrick stalwarts Patrick Magee (who had played the handicapped writer in "A Clockwork Orange") and Philip Stone (who had played Alex's father in "A Clockwork Orange", and would go on to play the dead caretaker Grady in "The Shining") are featured as the Chevalier du Balibari and as Graham, respectively.
Production.
Development.
After "", Kubrick made plans for a film about Napoleon Bonaparte. During pre-production, however, Sergei Bondarchuk and Dino De Laurentiis' "Waterloo" was released and subsequently failed at the box office. As a result, Kubrick's financiers pulled their funding for the film and he turned his attention to his next film, "A Clockwork Orange". Subsequently, Kubrick showed an interest in Thackeray's "Vanity Fair" but dropped the project when a serialised version for television was produced. He told an interviewer, "At one time, "Vanity Fair" interested me as a possible film but, in the end, I decided the story could not be successfully compressed into the relatively short time-span of a feature film...as soon as I read "Barry Lyndon" I became very excited about it."
Having garnered Oscar nominations for "Dr. Strangelove", "2001: A Space Odyssey" and "A Clockwork Orange", Kubrick's reputation in the early 1970s was that of "a perfectionist auteur who loomed larger over his movies than any concept or star." His studio—Warner Bros.—was therefore "eager to bankroll" his next project, which Kubrick kept "shrouded in secrecy" from the press partly due to the furor surrounding the controversially violent "A Clockwork Orange" (particularly in the UK) and partly due to his "long-standing paranoia about the tabloid press."
Having felt compelled to set aside his plans for a film about Napoleon Bonaparte, Kubrick set his sights on Thackeray's 1844 "satirical picaresque about the fortune-hunting of an Irish rogue," "Barry Lyndon", the setting of which allowed Kubrick to take advantage of the copious period research he had done for the now-aborted "Napoleon". At the time, Kubrick merely announced only that his next film would star Ryan O'Neal (deemed "a seemingly un-Kubricky choice of leading man") and Marisa Berenson, a former "Vogue" and "Time" magazine cover model, and be shot largely in Ireland. So heightened was the secrecy surrounding the film that "Even Berenson, when Kubrick first approached her, was told only that it was to be an 18th-century costume piece [and] she was instructed to keep out of the sun in the months before production, to achieve the period-specific pallor he required."
Principal photography.
Principal photography took 300 days, from spring 1973 through early 1974, with a break for Christmas.
Many of the film's exteriors were shot in Ireland, playing "itself, England, and Prussia during the Seven Years' War." Drawing inspiration from "the landscapes of Watteau and Gainsborough," Kubrick and cinematographer Alcott also relied on the "scrupulously researched art direction" of Ken Adam and Roy Walker. Alcott, Adam and Walker would be among those who would win Oscars for their "amazing work" on the film.
Several of the interior scenes were filmed in Powerscourt House, a famous 18th-century mansion in County Wicklow, Republic of Ireland. The house was destroyed in an accidental fire several months after filming (November 1974), so the film serves as a record of the lost interiors, particularly the "Saloon" which was used for more than one scene. The Wicklow Mountains are visible, for example, through the window of the Saloon during a scene set in Berlin. Other locations included Kells Priory (the English Redcoat encampment) Blenheim Palace, Castle Howard (exteriors of the Lyndon estate), Corsham Court (various interiors and the music room scene), Petworth House (chapel, and so on.), Stourhead (lake and temple), Longleat, and Wilton House (interior and exterior) in England, Dunrobin Castle (exterior and garden as Spa) in Scotland, Dublin Castle in Ireland (the chevalier's home), Ludwigsburg Palace near Stuttgart and Frederick the Great's Neues Palais at Potsdam near Berlin (suggesting Berlin's main street Unter den Linden as construction in Potsdam had just begun in 1763). Some exterior shots were also filmed at Waterford Castle (now a luxury hotel and golf course) and Little Island, Waterford. Moorstown Castle in Tipperary also featured. Several scenes were filmed at Castletown House outside Carrick-on-Suir, Co.Tipperary, and at Youghal, Co. Cork.
Cinematography.
The film—as with "almost every Kubrick film"—is a "showcase for [a] major innovation in technique." While "2001: A Space Odyssey" had featured "revolutionary effects," and "The Shining" would later feature heavy use of the Steadicam, "Barry Lyndon" saw a considerable number of sequences shot "without recourse to electric light." Cinematography was overseen by director of photography John Alcott (who won an Oscar for his work), and is particularly noted for the technical innovations that made some of its most spectacular images possible. To achieve photography without electric lighting "[f]or the many densely furnished interior scenes... meant shooting by candlelight," which is known to be difficult in still photography, "let alone with moving images."
Kubrick was "determined not to reproduce the set-bound, artificially lit look of other costume dramas from that time." After "tinker[ing] with different combinations of lenses and film stock," the production got hold of three super-fast 50mm lenses (Carl Zeiss Planar 50mm f/0.7) developed by Zeiss for use by NASA in the Apollo moon landings," which Kubrick had discovered in his search for low-light solutions. These super-fast lenses "with their huge aperture (the film actually features the lowest f-stop in film history) and fixed focal length" were problematic to mount, and were extensively modified into three versions by Cinema Products Corp. for Kubrick so to gain a wider angle of view, with input from optics expert Richard Vetter of Todd-AO. This allowed Kubrick and Alcott to shoot scenes lit with actual candles to an average lighting volume of only three candela, "recreating the huddle and glow of a pre-electrical age." In addition, Kubrick had the entire film push-developed by one stop.
Although Kubrick's express desire was to avoid electric lighting where possible, most shots were achieved with conventional lenses and lighting, but were lit to deliberately mimic natural light rather than for compositional reasons. In addition to potentially seeming more realistic, these methods also gave a particular period look to the film which has often been likened to 18th-century paintings (which were, of course, depicting a world devoid of electric lighting), in particular owing "a lot to William Hogarth, with whom Thackeray had always been fascinated."
According to critic Tim Robey, the film has a "stately, painterly, often determinedly static quality." For example, to help light some interior scenes, lights were placed outside and aimed through the windows, which were covered in a diffuse material to scatter the light evenly through the room rather than being placed inside for maximum use as most conventional films do. A sign of this method occurs in the scene where Barry duels Lord Bullingdon. Though it appears to be lit entirely with natural light, one can see that the light coming in through the cross-shaped windows in the abandoned chapel appears blue in color, while the main lighting of the scene coming in from the side is not. This is because the light through the cross-shaped windows is daylight from the sun, which when recorded on the film stock used by Kubrick showed up as blue-tinted compared to the incandescent electric light coming in from the side.
Despite such slight tinting effects, this method of lighting not only gave the look of natural daylight coming in through the windows, but it also protected the historic locations from the damage caused by mounting the lights on walls or ceilings and the heat from the lights. This helped the film "fit... perfectly with Kubrick's gilded-cage aesthetic - the film is consciously a museum piece, its characters pinned to the frame like butterflies."
Music.
The film's period setting allowed Kubrick to indulge his penchant for classical music, and the film score uses pieces by Johann Sebastian Bach (an arrangement of the Concerto for violin and oboe in C minor), Antonio Vivaldi (Cello Concerto in E-Minor, a transcription of the Cello Sonata in E Minor RV 40), Giovanni Paisiello, Wolfgang Amadeus Mozart, and Franz Schubert (German Dance No. 1 in C major, Piano Trio in E-Flat, Opus 100 and Impromptu No. 1 in C minor), as well as the Hohenfriedberger March. The piece most associated with the film, however, is the main title music: George Frideric Handel's stately "Sarabande" from the Suite in D minor HWV 437. Originally for solo harpsichord, the versions for the main and end titles are performed very romantically with orchestral strings, harpsichord, and timpani. It is used at various points in the film, in various arrangements, to indicate the implacable working of impersonal fate.
The score also includes Irish folk music, including Seán Ó Riada's song "Women of Ireland", arranged by Paddy Moloney and performed by The Chieftains.
Reception.
The film "was not the commercial success Warner Bros. had been hoping for" within the United States, although it fared better in Europe. This mixed reaction saw the film (in the words of one retrospective review) "greeted, on its release, with dutiful admiration – but not love. Critics... rail[ed] against the perceived coldness of Kubrick's style, the film's self-conscious artistry and slow pace. Audiences, on the whole, rather agreed..." This "air of disappointment" factored into Kubrick's decision to next film Stephen King's "The Shining" – a project that would not only please him artistically, but also be more likely to succeed financially. Still, several other critics, including Gene Siskel, praised the film's technical quality and strong narrative, and Siskel himself counted it as one of the five best films of the year.
In recent years, the film has gained a more positive reaction. As of March 2012 it holds a 94% "Certified Fresh" rating on Rotten Tomatoes based on 49 reviews. Roger Ebert added the film to his 'Great Movies' list on September 9, 2009, writing, "It defies us to care, it asks us to remain only observers of its stately elegance", and it "must be one of the most beautiful films ever made."
Awards.
In 1976, at the 48th Academy Awards, the film won four awards, for Best Art Direction (Ken Adam, Roy Walker, Vernon Dixon), Best Cinematography (John Alcott), Best Costume Design (Milena Canonero, Ulla-Britt Söderlund) and Best Musical Score (Leonard Rosenman, "for his arrangements of Schubert and Handel".) Kubrick was nominated three times, for Best Director, Best Picture, and Best Adapted Screenplay.
Kubrick won the British Academy of Film and Television Arts Award for Best Direction. John Alcott won for Best Cinematography. "Barry Lyndon" was also nominated for Best Film, Art Direction, and Costume Design.
Source novel.
Stanley Kubrick based his adapted screenplay on William Makepeace Thackeray's "The Luck of Barry Lyndon" (republished as the novel "Memoirs of Barry Lyndon, Esq.)," a picaresque tale written and published in serial form in 1844. 
"Barry Lyndon" departs from its source novel in several ways. In Thackeray's writings, events are related in the first person by Barry himself. A comic tone pervades the work, as Barry proves both a raconteur and an unreliable narrator. Kubrick's film, by contrast, presents the story objectively. Though the film contains voice-over (by actor Michael Hordern), the comments expressed are not Barry's, but those of an omniscient, although not entirely impartial, narrator. This change in perspective alters the tone of the story; Thackeray tells a jaunty, humorous tale, but Kubrick's telling is essentially tragic, albeit with a satirical tone. Kubrick felt that using a first-person narrative would not be useful in a film adaptation:
"I believe Thackeray used Redmond Barry to tell his own story in a deliberately distorted way because it made it more interesting. Instead of the omniscient author, Thackeray used the imperfect observer, or perhaps it would be more accurate to say the dishonest observer, thus allowing the reader to judge for himself, with little difficulty, the probable truth in Redmond Barry's view of his life. This technique worked extremely well in the novel but, of course, in a film you have objective reality in front of you all of the time, so the effect of Thackeray's first-person story-teller could not be repeated on the screen. It might have worked as comedy by the juxtaposition of Barry's version of the truth with the reality on the screen, but I don't think that Barry Lyndon should have been done as a comedy."
Kubrick also changed the plot. For example, the novel does not include a final duel. By adding this episode, Kubrick establishes dueling as the film's central motif: the film begins with a duel where Barry's father is shot dead, and duels recur throughout the film.

</doc>
<doc id="4230" url="http://en.wikipedia.org/wiki?curid=4230" title="Cell (biology)">
Cell (biology)

The cell (from Latin "cella", meaning "small room") is the basic structural, functional, and biological unit of all known living organisms. Cells are the smallest unit of life that can replicate independently, and are often called the "building blocks of life". The study of cells is called cell biology. 
Cells consist of a protoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids. Organisms can be classified as unicellular (consisting of a single cell; including most bacteria) or multicellular (including plants and animals). While the number of cells in plants and animals varies from species to species, humans contain about 100 trillion (1014) cells. Most plant and animal cells are visible only under the microscope, with dimensions between 1 and 100 micrometres.
The cell was discovered by Robert Hooke in 1665. The cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that all cells come from preexisting cells, that vital functions of an organism occur within cells, and that all cells contain the hereditary information necessary for regulating cell functions and for transmitting information to the next generation of cells. Cells emerged on Earth at least 3.5 billion years ago.
Anatomy.
There are two types of cells, eukaryotes, which contain a nucleus, and prokaryotes, which do not. Prokaryotic cells are usually single-celled organisms, while eukaryotic cells can be either single-celled or part of multicellular organisms.
Prokaryotic cells.
Prokaryotic cells were the first form of life on Earth, as they have signaling and self-sustaining processes. They are simpler and smaller than eukaryotic cells, and lack membrane-bound organelles such as the nucleus. Prokaryotes include two of the domains of life, bacteria and archaea. The DNA of a prokaryotic cell consists of a single chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid. Most of the prokaryotes are smallest of all organisms. Most prokaryotes range from 0.5 to 2.0 µm in diameter.
A prokaryotic cell has three architectural regions:
Eukaryotic cells.
Plants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound compartments in which specific metabolic activities take place. Most important among these is a cell nucleus, a membrane-delineated compartment that houses the eukaryotic cell's DNA. This nucleus gives the eukaryote its name, which means "true nucleus". Other differences include:
Subcellular components.
All cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, a salty cytoplasm takes up most of the cell volume. All cells (except red blood cells which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin) possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary components of the cell, then briefly describes their function.
Membrane.
The cell membrane, or plasma membrane, surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is said to be 'semi-permeable', in that it can either let a substance (molecule or ion) pass through freely, pass through to a limited extent or not pass through at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones.
Cytoskeleton.
The cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microfilaments, intermediate filaments and microtubules. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis.The subunit protein of microfilaments is a small, monomeric protein called actin. The subunit of microtubules is a dimeric molecule called tubulin. Intermediate filaments are heteropolymers whose subunits varies among the cell types in different tissues. But some of the subunit protein of intermediate filaments include vimentin, desmin, lamin (lamins A, B and C), keratin (multiple acidic and basic keratins), neurofilament proteins (NF - L, NF - M).
Genetic material.
Two different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.
Prokaryotic genetic material is organized in a simple circular DNA molecule (the bacterial chromosome) in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different, linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).
A human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from the nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.
Foreign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome.
Organelles.
Organelles are parts of the cell which are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function). Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.
There are several types of organelles in a cell. Some (such as the nucleus and golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles.
Structures outside the cell membrane.
Many cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the impermeable cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes.
Cell wall.
Many types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of pectin, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan.
Prokaryotic.
Capsule.
A gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as "Bacillus anthracis" or hyaluronic acid as in streptococci. (See Bacterial capsule.)
Capsules are not marked by normal staining protocols and can be detected by India ink or methyl blue; which allows for higher contrast between the cells for observation.
Flagella.
Flagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. Are most commonly found in bacteria cells but are found in animal cells as well.
Fimbriae (pili).
They are short and thin hair-like filaments, formed of protein called pilin (antigenic). Fimbriae are responsible for attachment of bacteria to specific receptors of human cell (adherence). There are special types of pili called (sex pili) involved in conjunction. (See Pilus.)
Cellular processes.
Growth and metabolism.
Between successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.
Complex sugars consumed by the organism can be broken down into a less chemically complex sugar molecule called glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a form of energy, through two different pathways.
The first pathway, glycolysis, requires no oxygen and is referred to as anaerobic metabolism. Each reaction produces ATP and NADH, which are used in cellular functions, as well as two pyruvate molecules that derived from the original glucose molecule. In prokaryotes, all energy is produced by glycolysis.
The second pathway, called the Krebs cycle or citric acid cycle, is performed only by eukaryotes and involves further breakdown of the pyruvate produced in glycolysis. It occurs inside the mitochondria and generates much more energy than glycolysis, mostly through oxidative phosphorylation.
Replication.
Cell division involves a single cell (called a "mother cell") dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.
DNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.
In meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins for carrying out the job.
Protein synthesis.
Cells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.
Transcription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.
Movement or motility.
Unicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia. 
In multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps – protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.
Multicellularity.
Cell specialization.
Multicellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.
In complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but different cell type due to the differential regulation of the genes they contain.
Most distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell–cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).
Origin of multicellularity.
Multicellularity has evolved independently at least 25 times, including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, "Magnetoglobus multicellularis" or "Methanosarcina". However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants. It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae. Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships. 
The first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.
The evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.
Origins.
The origin of cells has to do with the origin of life, which began the history of life on Earth.
Origin of the first cell.
There are several theories about the origin of small molecules that led to life on the early Earth. They may have been carried to Earth on meteorites (see Murchison meteorite), created at deep-sea vents, or synthesized by lightning in a reducing atmosphere (see Miller–Urey experiment). There is little experimental data defining what the first self-replicating forms were. RNA is thought to be the earliest self-replicating molecule, as it is capable of both storing genetic information and catalyzing chemical reactions (see RNA world hypothesis), but some other entity with the potential to self-replicate could have preceded RNA, such as clay or peptide nucleic acid.
Cells emerged at least 3.5 billion years ago. The current belief is that these cells were heterotrophs. The early cell membranes were probably more simple and permeable than modern ones, with only a single fatty acid chain per lipid. Lipids are known to spontaneously form bilayered vesicles in water, and could have preceded RNA, but the first cell membranes could also have been produced by catalytic RNA, or even have required structural proteins before they could form.
Origin of eukaryotic cells.
The eukaryotic cell seems to have evolved from a symbiotic community of prokaryotic cells. DNA-bearing organelles like the mitochondria and the chloroplasts are descended from ancient symbiotic oxygen-breathing proteobacteria and cyanobacteria, respectively, which were endosymbiosed by an ancestral archaean prokaryote.
There is still considerable debate about whether organelles like the hydrogenosome predated the origin of mitochondria, or vice versa: see the hydrogen hypothesis for the origin of eukaryotic cells.

</doc>
<doc id="4231" url="http://en.wikipedia.org/wiki?curid=4231" title="Buffy the Vampire Slayer (film)">
Buffy the Vampire Slayer (film)

Buffy the Vampire Slayer is a 1992 American action/comedy horror film about a Valley girl cheerleader named Buffy who learns that it is her fate to hunt vampires. The film starred Kristy Swanson, Donald Sutherland, Paul Reubens, Rutger Hauer, Luke Perry and Hilary Swank. It was a moderate success at the box office, but received mixed reception from critics. The film was taken in a different direction from the one that its writer, Joss Whedon, intended, but several years later he was able to create the darker and acclaimed TV series of the same name.
Plot.
High school senior Buffy Summers (Kristy Swanson) is introduced as a stereotypical, shallow cheerleader at Hemery High School in Los Angeles. She is a carefree popular girl whose main concerns are shopping and spending time with her airhead friends and her boyfriend, Jeffrey. While at school one day, she is approached by a man named Merrick Jamison-Smythe (Donald Sutherland). He informs her that she is The Slayer, or chosen one, and he is a Watcher whose duty it is to guide and train her. She initially rebukes his claims, but finally becomes convinced that he is right when he is able to describe a recurring dream of hers in detail. In addition, Buffy is exhibiting uncanny abilities not known to her, including heightened agility, senses, and endurance, yet she repeatedly tries Merrick's patience with her frivolous nature and sharp-tongued remarks.
Meanwhile Oliver Pike (Luke Perry), and best friend Benny (David Arquette), who resented Buffy and her friends due to differing social circles, are out drinking when they are attacked by vampires. Benny is turned but Oliver is saved by Merrick. As a vampire, Benny visits Oliver and tries to get him to join him. Later, when Oliver and his boss are discussing Benny, Oliver tells him to run if he sees him.
After several successful outings, Buffy is drawn into conflict with a local vampire king named Lothos (Rutger Hauer), who has killed a number of past Slayers. During an encounter with Lothos' main minion Amilyn (Paul Reubens) and his gang of vampires, Buffy, Oliver, and Merrick fight against them in the forest as Amilyn loses his arm. Amilyn flees the fight to talk to Lothos who now realizes Buffy is the slayer. After this encounter, Buffy and Oliver start a friendship, which eventually becomes romantic and Oliver becomes Buffy's partner in fighting the undead.
During a basketball game, Buffy and Oliver find out that one of the players is a minion of Lothos. After a quick chase to a parade float storage yard, Buffy finally confronts Lothos, shortly after she and Oliver take down his gang. Lothos puts Buffy in a hypnotic trance, which is broken due to Merrick's intervention. Lothos turns on Merrick and impales him with the stake he attempted to use on him. Lothos leaves, saying that Buffy is not ready. As Merrick dies, he tells Buffy to do things her own way rather than live by the rules of others. Because of her new life, responsibilities, and heartbreak, Buffy becomes emotionally shocked and has a falling out with her friends as she outgrows their immature, selfish behavior, and starts dropping her Slayer responsibilities.
At the senior dance, Buffy meets up with Oliver and as they start to dance and eventually kiss, Lothos leads the remainder of his minions to the school and attacks the students and the attending faculty. Buffy confronts the vampires outside while Oliver fights the vampiric Benny. After overpowering the vampires, she confronts Lothos inside the school and stabs Amilyn. Lothos hypnotises Buffy again but she uses a cross and hairspray to create a makeshift flame-thrower and burns Lothos before heading back into the gym. Buffy sees everybody recover from the attack, but Lothos emerges again getting into a fight with Buffy, who then stakes him.
The film ends with Buffy and Oliver leaving the dance on a motorcycle, and a news crew interviewing the students and the principal about the attack during the credits.
Continuity with the television show.
Many of the details given in the film differ from the continuity of the later television series. For example, Buffy's history is dissimilar, and both the vampires' and Slayer's abilities are depicted differently. The vampires in the film die like humans, while in the TV show they turn to dust. Joss Whedon has expressed his disapproval with the movie's interpretation of the script, stating, "I finally sat down and had written it and somebody had made it into a movie, and I felt like — well, that's not quite her. It's a start, but it's not quite the girl."
According to the "Official Buffy Watcher's Guide", Whedon wrote the pilot to the TV series as a sequel to his original script, which is why the show makes references to events that did not occur in the film. In 1999, Dark Horse Comics released a graphic novel adaptation of Whedon's original script under the title, "The Origin". Whedon stated: "The "Origin" comic, though I have issues with it, CAN pretty much be accepted as canonical. They did a cool job of combining the movie script with the series, that was nice, and using the series Merrick and not a certain OTHER thespian who shall remain hated."
Box office.
The film debuted at #5 at the North American box office and eventually grossed $16,624,456 against a $7 million production budget.
Home releases.
The film was released on VHS and Laserdisc in the U.S. in 1992 by Fox Video and re-released in 1995 under the "Twentieth Century Fox Selections" banner. It was released on DVD in the US in 2001 and on BluRay in 2011.
Soundtrack.
The soundtrack was released on July 28, 1992.
The soundtrack does not include every song played in the film, which also included "In the Wind" by War Babies and "Inner Mind" by Eon.
Remake.
On May 25, 2009, "The Hollywood Reporter" reported that Roy Lee and Doug Davison of Vertigo Entertainment were working with Fran Rubel Kuzui and Kaz Kuzui on a re-envisioning or relaunch of the "Buffy" film for the big screen. The film would not be a sequel or prequel to the existing film or television franchise and Joss Whedon would have no involvement in the project. None of the characters, cast, or crew from the television series would be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Whedon into taking the reins of the project. On November 22, 2010, "The Hollywood Reporter" confirmed that Warner Bros. had picked up the movie rights to the remake. The film was set for release sometime in 2012. 20th Century Fox, which usually holds rights to the more successful "Buffy"/"Angel" television franchise, will retain merchandising and some distribution rights.
The idea of the remake caused wrath among fans of the TV series, since Whedon is not involved and the project does not have any connection with the show and will not conform to the continuity maintained with the "Buffy the Vampire Slayer Season Eight" and "Season Nine" comic book titles. Not only the fandom, but the main cast members of both "Buffy" and "Angel" series, expressed disagreement with the report on Twitter and in recent interviews. Sarah Michelle Gellar said, "I think it's a horrible idea. To try to do a "Buffy" without Joss Whedon... to be incredibly non-eloquent: that's the dumbest idea I've ever heard." Proposed shooting locations included Black Wood and other areas in rural England, due to budgetary constraints and the potential setting as being outside of the city, an unusual change for the franchise.
In December 2011, more than a year after the official reboot announcement, the "Los Angeles Times" site reported that Whit Anderson, the writer picked for the new "Buffy" movie, had her script rejected by the producers behind the project, and that a new writer was being sought. Sources also stated that "If you're going to bring it back, you have to do it right. [Anderson] came in with some great ideas and she had reinvented some of the lore and it was pretty cool but in the end there just wasn't enough on the page."

</doc>
<doc id="4232" url="http://en.wikipedia.org/wiki?curid=4232" title="Barter">
Barter

Barter is a system of exchange by which goods or services are directly exchanged for other goods or services without using a medium of exchange, such as money. It is distinguishable from gift economies in that the reciprocal exchange is immediate and not delayed in time. It is usually bilateral, but may be multilateral (i.e., mediated through barter organizations) and usually exists parallel to monetary systems in most developed countries, though to a very limited extent. Barter usually replaces money as the method of exchange in times of monetary crisis, such as when the currency may be either unstable (e.g., hyperinflation or deflationary spiral) or simply unavailable for conducting commerce.
The inefficiency of barter in archaic society has been used by economists since Adam Smith to explain the emergence of money, the economy, and hence the discipline of economics itself.However, no present or past society has ever been seen through ethnographic studies to use pure barter without any medium of exchange, nor the emergence of money from barter. 
Since the 1830s, direct barter in western market economies has been aided by exchanges which frequently utilize alternative currencies based on the labour theory of value, and designed to prevent profit taking by intermediators. Examples include the Owenite socialists, the Cincinnati Time store, and more recently Ithaca HOURS (Time banking) and the LETS system.
Economic theory.
Adam Smith on the origin of money.
Adam Smith, the father of modern economics, sought to demonstrate that markets (and economies) pre-existed the state, and hence should be free of government regulation. He argued (against conventional wisdom) that money was not the creation of governments. Markets emerged, in his view, out of the division of labour, by which individuals began to specialize in specific crafts and hence had to depend on others for subsistence goods. These goods were first exchanged by barter. Specialization depended on trade, but was hindered by the "double coincidence of wants" which barter requires, i.e., for the exchange to occur, each participant must want what the other has. To complete this hypothetical history, craftsmen would stockpile one particular good, be it salt or metal, that they thought no one would refuse. This is the origin of money according to Smith. Money, as a universally desired medium of exchange, allows each half of the transaction to be separated.
Barter is characterized in Adam Smith's "The Wealth of Nations" by a disparaging vocabulary: "higgling, haggling, swapping, dickering." It has also been characterized as negative reciprocity, or "selfish profiteering."
Anthropologists have argued, in contrast, "that when something resembling barter "does" occur in stateless societies it is almost always between strangers, people who would otherwise be enemies." Barter occurred between strangers, not fellow villagers, and hence cannot be used to naturalisticly explain the origin of money without the state. Since most people engaged in trade knew each other, exchange was fostered through the extension of credit. Marcel Mauss, author of 'The Gift', argued that the first economic contracts were to "not" act in one's economic self-interest, and that before money, exchange was fostered through the processes of reciprocity and redistribution, not barter. Everyday exchange relations in such societies are characterized by generalized reciprocity, or a non-calculative familial "communism" where each takes according to their needs, and gives as they have.
Limitations.
Barter's limits are usually explained in terms of its inefficiencies in easing exchange in comparison to the functions of money:
History.
Silent trade.
Other anthropologists have questioned whether barter is typically between "total" strangers, a form of barter known as "silent trade". Silent trade, also called silent barter, dumb barter ("dumb" here used in its old meaning of "mute"), or depot trade, is a method by which traders who cannot speak each other's language can trade without talking. However, Benjamin Orlove has shown that while barter occurs through "silent trade" (between strangers), it also occurs in commercial markets as well. "Because barter is a difficult way of conducting trade, it will occur only where there are strong institutional constraints on the use of money or where the barter symbolically denotes a special social relationship and is used in well-defined conditions. To sum up, multipurpose money in markets is like lubrication for machines - necessary for the most efficient function, but not necessary for the existence of the market itself."
In his analysis of barter between coastal and inland villages in the Trobriand Islands, Keith Hart highlighted the difference between highly ceremonial gift exchange between community leaders, and the barter that occurs between individual households. The haggling that takes place between strangers is possible because of the larger temporary political order established by the gift exchanges of leaders. From this he concludes that barter is "an atomized interaction predicated upon the presence of society" (i.e. that social order established by gift exchange), and not typical between complete strangers.
Times of monetary crisis.
As Orlove noted, barter may occur in commercial economies, usually during periods of monetary crisis. During such a crisis, currency may be in short supply, or highly devalued through hyperinflation. In such cases, money ceases to be the universal medium of exchange or standard of value. Money may be in such short supply that it becomes an item of barter itself rather than the means of exchange. Barter may also occur when people cannot afford to keep money (as when hyperinflation quickly devalues it).
Exchanges.
Economic historian Karl Polanyi has argued that where barter is widespread, and cash supplies limited, barter is aided by the use of credit, brokerage, and money as a unit of account (i.e. used to price items). All of these strategies are found in ancient economies including Ptolemaic Egypt. They are also the basis for more recent barter exchange systems.
While one-to-one bartering is practiced between individuals and businesses on an informal basis, organized barter exchanges have developed to conduct third party bartering which helps overcome some of the limitations of barter. A barter exchange operates as a broker and bank in which each participating member has an account that is debited when purchases are made, and credited when sales are made.
Modern barter and trade has evolved considerably to become an effective method of increasing sales, conserving cash, moving inventory, and making use of excess production capacity for businesses around the world. Businesses in a barter earn trade credits (instead of cash) that are deposited into their account. They then have the ability to purchase goods and services from other members utilizing their trade credits – they are not obligated to purchase from those whom they sold to, and vice-versa. The exchange plays an important role because they provide the record-keeping, brokering expertise and monthly statements to each member. Commercial exchanges make money by charging a commission on each transaction either all on the buy side, all on the sell side, or a combination of both. Transaction fees typically run between 8 and 15%.
Utopian socialism.
The Owenite socialists in Britain and the United States in the 1830s were the first to attempt to organize barter exchanges. Owenism developed a "theory of equitable exchange" as a critique of the exploitative wage relationship between capitalist and labourer, by which all profit accrued to the capitalist. To counteract the uneven playing field between employers and employed, they proposed "schemes of labour notes based on labour time, thus institutionalizing Owen's demand that human labour, not money, be made the standard of value." This alternate currency eliminated price variability between markets, as well as the role of merchants who bought low and sold high. The system arose in a period where paper currency was an innovation. Paper currency was an I.O.U. circulated by a bank (a promise to pay, not a payment in itself). Both merchants and an unstable paper currency created difficulties for direct producers.
An alternate currency, denominated in labour time, would prevent profit taking by middlemen; all goods exchanged would be priced only in terms of the amount of labour that went into them as expressed in the maxim 'Cost the limit of price'. It became the basis of exchanges in London, and in America, where the idea was implemented at the New Harmony communal settlement by Josiah Warren in 1826, and in his Cincinnati 'Time store' in 1827. Warren ideas were adopted by other Owenites and currency reformers, even though the labour exchanges were relatively short lived.
In England, about 30 to 40 cooperative societies sent their surplus goods to an "exchange bazaar" for direct barter in London, which later adopted a similar labour note. The British Association for Promoting Cooperative Knowledge established an "equitable labour exchange" in 1830. This was expanded as the National Equitable Labour Exchange in 1832 on Grays Inn Road in London. These efforts became the basis of the British cooperative movement of the 1840s. In 1848, the socialist and first self-designated anarchist Pierre-Joseph Proudhon postulated a system of "time chits". In 1875, Karl Marx wrote of "Labor Certificates" ("Arbeitszertifikaten") in his Critique of the Gotha Program of a "certificate from society that [the labourer] has furnished such and such an amount of labour", which can be used to draw "from the social stock of means of consumption as much as costs the same amount of labour."
Twentieth century experiments.
The first exchange system was the Swiss WIR Bank. It was founded in 1934 as a result of currency shortages after the stock market crash of 1929. "WIR" is both an abbreviation of Wirtschaftsring and the word for "we" in German, reminding participants that the economic circle is also a community.
In Spain (particularly the Catalonia region) there is a growing number of exchange markets. These barter markets or swap meets work without money. Participants bring things they do not need and exchange them for the unwanted goods of another participant. Swapping among three parties often helps satisfy tastes when trying to get around the rule that money is not allowed.
Michael Linton originated the term "local exchange trading system" (LETS) in 1983 and for a time ran the Comox Valley LETSystems in Courtenay, British Columbia. LETS networks use interest-free local credit so direct swaps do not need to be made. For instance, a member may earn credit by doing childcare for one person and spend it later on carpentry with another person in the same network. In LETS, unlike other local currencies, no scrip is issued, but rather transactions are recorded in a central location open to all members. As credit is issued by the network members, for the benefit of the members themselves, LETS are considered mutual credit systems.
Modern developments.
According to the International Reciprocal Trade Association, the industry trade body, more than 450,000 businesses transacted $10 billion globally in 2008 – and officials expect trade volume to grow by 15% in 2009.
It is estimated that over 450,000 businesses in the United States were involved in barter exchange activities in 2010. There are approximately 400 commercial and corporate barter companies serving all parts of the world. There are many opportunities for entrepreneurs to start a barter exchange. Several major cities in the U.S. and Canada do not currently have a local barter exchange. There are two industry groups in the United States, the (NATE) and the International Reciprocal Trade Association (IRTA). Both offer training and promote high ethical standards among their members. Moreover, each has created its own currency through which its member barter companies can trade. NATE's currency is the known as the BANC and IRTA's currency is called Universal Currency (UC). In Canada, the largest barter exchange is Tradebank, founded in 1987. In the United States, the largest barter exchange and corporate trade group is International Monetary Systems, founded in 1985, now with representation in various countries. In Australia and New Zealand the largest barter exchange is Bartercard, founded in 1991, with offices in the United Kingdom,United States, Cyprus,UAE and Thailand.
Corporate barter focuses on larger transactions, which is different from a traditional, retail oriented barter exchange. Corporate barter exchanges typically use media and advertising as leverage for their larger transactions. It entails the use of a currency unit called a "trade-credit". The trade-credit must not only be known and guaranteed, but also be valued in an amount the media and advertising could have been purchased for had the "client" bought it themselves (contract to eliminate ambiguity and risk).
Soviet bilateral trade is occasionally called "barter trade", because although the purchases were denominated in U.S. dollars, the transactions were credited to an international clearing account, avoiding the use of hard cash.
Tax implications.
In the United States, Karl Hess used bartering to make it harder for the IRS to seize his wages and as a form of tax resistance. Hess explained how he turned to barter in an op-ed for "The New York Times" in 1975. However the IRS now requires barter exchanges to be reported as per the Tax Equity and Fiscal Responsibility Act of 1982. Barter exchanges are considered taxable revenue by the IRS and must be reported on a 1099-B form. According to the IRS, "The fair market value of goods and services exchanged must be included in the income of both parties."
Other countries though do not have the reporting requirement that the U.S. does concerning proceeds from barter transactions, but taxation is handled the same way as a cash transaction. If one barters for a profit, one pays the appropriate tax; if one generates a loss in the transaction, they have a loss. Bartering for business is also taxed accordingly as business income or business expense. Many barter exchanges require that one register as a business.

</doc>
<doc id="4233" url="http://en.wikipedia.org/wiki?curid=4233" title="Berthe Morisot">
Berthe Morisot

Berthe Morisot (; January 14, 1841 – March 2, 1895) was a painter and a member of the circle of painters in Paris who became known as the Impressionists. She was described by Gustave Geffroy in 1894 as one of "les trois grandes dames" of Impressionism alongside Marie Bracquemond and Mary Cassatt.
In 1864, she exhibited for the first time in the highly esteemed Salon de Paris. Sponsored by the government, and judged by Academicians, the Salon was the official, annual exhibition of the Académie des beaux-arts in Paris. Her work was selected for exhibition in six subsequent Salons until, in 1874, she joined the "rejected" Impressionists in the first of their own exhibitions, which included Paul Cézanne, Edgar Degas, Claude Monet, Camille Pissarro, Pierre-Auguste Renoir, and Alfred Sisley. It was held at the studio of the photographer Nadar.
She became the sister-in-law of her friend and colleague, Édouard Manet, when she married his brother, Eugène.
Education.
Morisot was born in Bourges, Cher, France, into a successful bourgeois family. According to family tradition, the family had included one of the most prolific Rococo painters of the ancien régime, Fragonard, whose handling of color and expressive, confident brushwork influenced later painters. Both Berthe and her sister, Edma Morisot, chose to become painters.
Berthe Morisot's family moved to Paris when she was a child. Once Berthe settled on pursuing art, her family did not impede her career. She registered as a copyist at the Louvre. By age twenty, she had met and befriended the important, and pivotal, landscape painter of the Barbizon School, Camille Corot, who excelled in figure painting as well. The older artist instructed Berthe and her sister in painting and introduced them to other artists and teachers. Under Corot's influence, Morisot took up the plein air method of working.
Manet and impressionism.
Morisot's first appearance in the Salon de Paris came at the age of twenty-three in 1864, with the acceptance of two landscape paintings. She continued to show regularly in the Salon, to generally favorable reviews, until 1873, the year before the first Impressionist exhibition. Morisot exhibited with the Impressionists from 1874 onwards, only missing the exhibition in 1878 when her daughter was born.
Meanwhile, in 1868 Morisot became acquainted with Édouard Manet. He took a special interest in Morisot, as is evident from his warm portrayal of her in several paintings, including a striking portrait study of Morisot in a black veil, while in mourning for her father's death (displayed at the top of the article). Correspondence between them bespeaks affection. He once gave her an easel as a Christmas present. He also interfered in one of her Salon submissions when he was engaged to transport it. Manet mistook one of Morisot's self-criticisms as an invitation to add his corrections, which he did, much to Morisot's dismay.
Although traditionally Manet has been related as the master and Morisot as the follower, there is evidence that their relationship was a reciprocating one. Morisot had developed her own distinctive artistic style. Records of paintings show Manet's appreciation of certain stylistic and compositional decisions that Morisot originated. He incorporated some of these characteristics into his own work.
It was Morisot who persuaded Manet to attempt plein air painting, which she had been practicing since having been introduced to it by Corot.
She also drew Manet into the circle of painters who soon became known as the Impressionists. In 1874, Morisot married Manet's brother, Eugene, and they had one daughter, Julie. Julie Manet became the subject for many of her mother's paintings and a book of her memoirs "Growing Up with the Impressionists: The Diary of Julie Manet," was published in 1987.
Subjects.
Morisot painted what she experienced on a daily basis. Her paintings reflect the 19th-century cultural restrictions of her class and gender. She avoided urban and street scenes as well as the nude figure and, like her fellow female Impressionist Mary Cassatt, focused on domestic life and portraits in which she could use family and personal friends as models. Paintings like "The Cradle" (1872), in which she depicted current trends for nursery furniture, reflect her sensitivity to fashion and advertising, both of which would have been apparent to her female audience. Her works also include landscapes, portraits, garden settings and boating scenes.
Death.
Berthe Morisot died on March 2, 1895, in Paris, of pneumonia contracted while attending to her daughter Julie's similar illness. She was interred in the Cimetière de Passy.
In popular culture.
She was portrayed by actress Marine Delterme in the eponymous 2012 French biographical TV film directed by Caroline Champetier.
Art market.
At a Christie's auction in February 2013, "After Lunch" (1881), a portrait of a young redhead in a straw hat and purple dress, sold for $10.9 million, roughly three times its high estimate. The painting set a record at the time as the most expensive work ever sold by a female artist at auction at the time, topping an earlier record set with $10.7 million having been paid for a sculpture by Louise Bourgeois in 2012.

</doc>
<doc id="4237" url="http://en.wikipedia.org/wiki?curid=4237" title="Barnard College">
Barnard College

Barnard College is a private women's liberal arts college and a member of the Seven Sisters. Founded in 1889, it has been affiliated with Columbia University since 1900. Barnard's campus stretches along Broadway between 116th and 120th Streets in the Morningside Heights neighborhood in the borough of Manhattan, in New York City. It is directly adjacent to Columbia's campus and near several other academic institutions and has been used by Barnard since 1898.
History.
Barnard College was founded to provide an undergraduate education for women comparable to that of Columbia University and other Ivy League schools, most of which admitted only men for undergraduate study into the 1960s. The college was named after Frederick Augustus Porter Barnard, an American educator and mathematician, who served as the president of the then-Columbia College from 1864 to 1889. He advocated equal educational privileges for men and women, preferably in a coeducational setting, and began proposing in 1879 that Columbia admit women. The board of trustees repeatedly rejected Barnard's suggestion, but in 1883 agreed to create a detailed syllabus of study for women. While they could not attend Columbia classes, those who passed examinations based on the syllabus would receive a degree. The first such woman graduate received her bachelor's degree in 1887. A former student of the program, Annie Nathan Meyer, and other prominent New York women persuaded the board in 1889 to create a women's college connected to Columbia.
Barnard College's original 1889 home was a rented brownstone at 343 Madison Avenue, where a faculty of six offered instruction to 14 students in the School of Arts, as well as to 22 "specials", who lacked the entrance requirements in Greek and so enrolled in science. When Columbia University announced in 1892 its impending move to Morningside Heights, Barnard built a new campus on 119th-120th Streets with gifts from Mary E. Brinckerhoff, Elizabeth Milbank Anderson and Martha T. Fiske. Milbank, Brinckerhoff, and Fiske Halls, built in 1897–1898, were listed on the National Register of Historic Places in 2003. Ella Weed supervised the college in its first four years; Emily James Smith succeeded her as Barnard's first dean. As the college grew it needed additional space, and in 1903 it received the three blocks south of 119th Street from Anderson who had purchased a former portion of the Bloomingdale Asylum site from the New York Hospital. By the mid-20th century Barnard had succeeded in its original goal of providing an elite education to women. Between 1920 and 1974, only the much larger Hunter College and University of California, Berkeley produced more women graduates who later received doctorate degrees.
Students' Hall, now known as Barnard Hall, was built in 1916. Brooks and Hewitt Halls were built in 1906–1907 and 1926–1927, respectively. They were listed on the National Register of Historic Places in 2003.
Relationship with Columbia University.
The relationship between Barnard College and Columbia University is complicated. The college's front gates state "Barnard College of Columbia University". Barnard describes itself as "both an independently incorporated educational institution and an official college of Columbia University", and advises students to state "Barnard College, Columbia University" or "Barnard College of Columbia University" on résumés. Columbia describes Barnard as an affiliated institution that is a faculty of the university or is "in partnership with" it. An academic journal describes Barnard as a former affiliate that became a school within the university. Facebook includes Barnard students and alumnae within the Columbia interest group. All Barnard faculty are granted tenure by the college and Columbia, and Barnard graduates receive Columbia University diplomas signed by both the Barnard and Columbia presidents.
Smith and Columbia president Seth Low worked to open Columbia classes to Barnard students. By 1900 they could attend Columbia classes in philosophy, political science, and several scientific fields. That year Barnard formalized an affiliation with the university which made available to its students the instruction and facilities of Columbia. Many top women attended the college; Franz Boas, who taught at both Columbia and Barnard in the early 1900s, was among those faculty members who reportedly found Barnard students superior to their male Columbia counterparts. From 1955 Columbia and Barnard students could register for the other school's classes with the permission of the instructor; from 1973 no permission was needed.
Columbia president William J. McGill predicted in 1970 that Barnard College and Columbia College would merge within five years, and Columbia's financial difficulties during the 1970s increased its desire to merge, but Barnard resisted doing so because of the university's large debt. After a decade of failed negotiations for a merger with Barnard akin to the one between Harvard College and Radcliffe College, Columbia College instead began admitting women in 1983. Applications to Columbia rose 56% that year, making admission more selective, and nine Barnard students transferred to Columbia. Eight students admitted to both Columbia and Barnard chose Barnard, while 78 chose Columbia.
The Columbia-Barnard affiliation continued, however, despite Columbia College's decision. Barnard pays Columbia about $5 million a year under the terms of the "interoperate relationship", which the two schools renegotiate every 15 years. Despite the affiliation Barnard is legally and financially separate from Columbia, with an independent faculty and board of trustees. It is responsible for its own separate admissions, health, security, guidance and placement services, and has its own alumnae association. Nonetheless, Barnard students participate in the academic, social, athletic and extracurricular life of the broader University community on a reciprocal basis. The affiliation permits the two schools to share some academic resources; for example, only Barnard has an urban studies department, and only Columbia has a computer science department. Most Columbia classes are open to Barnard students and vice versa. Barnard students and faculty are represented in the University Senate, and student organizations such as the "Columbia Daily Spectator" are open to all students. Barnard students play on Columbia athletics teams, and Barnard uses Columbia email, telephone and network services.
Admissions.
Admissions to Barnard is considered most selective by "U.S. News & World Report". It is the most selective women's college in the nation; in 2008, Barnard had the lowest acceptance rate of the five Seven Sisters that remain single-sex in admissions.
The class of 2017's admission rate was 20.5%, a new record low. The class of 2016 set the admission rate at a 21%, with 5,440 applications received.
For the class of 2015, 5,154 applications were received, setting the admission rate at 24.9%.
For the class of 2014, the admit rate was 27.8%, with 4,618 applications received.
For the class of 2013, 90.3% ranked in first or second decile at their high school (of the 35.0% ranked by their schools). The average GPA of the class of 2013 was 94.6 on a 100-pt. scale and 3.84 on a 4.0 scale.
For the class of 2012, the admission rate was 28.5% of the 4,273 applications received. The early-decision admission rate was 47.7%, out of 392 applications. The median SAT Combined was 2060, with median subscores of 660 in Math, 690 in Critical Reading, and 700 in Writing. The Median ACT score was 30. Of the women in the class of 2012, 89.4% ranked in first or second decile at their high school (of the 41.3% ranked by their schools). The average GPA of the class of 2012 was 94.3 on a 100-point scale and 3.88 on a 4.0 scale.
For the class of 2011, Barnard College admitted 28.7% of those who applied. The median ACT score was 30, while the median combined SAT score was 2100.
Academic Ranking.
In the 2014 U.S. News & World Report rankings, Barnard was ranked as the 32nd best liberal arts college in the country. The ranking came under widespread criticism, as it only accounted for institution-specific resources. Greg Brown, chief operating officer at Barnard, said, "I believe that our ranking is lower than it should be, primarily because the methodology simply can't account for the Barnard-Columbia relationship. Because the Columbia relationship doesn't fit neatly into any of the survey categories, it is essentially ignored. Rankings are inherently limited in this way."
In 1998, then president Judith Shapiro compared the ranking service to the "equivalent of "Sport's Illustrated" swimsuit issue." According to Shapiro's letter, "Such a ranking system certainly does more harm than good in terms of educating the public." On June 19, 2007, following a meeting of the Annapolis Group, which represents over 100 liberal arts colleges, Barnard announced that it would no longer participate in the U.S. News annual survey, and that they would fashion their own way to collect and report common data.
Barnard Library.
Barnard's Wollman Library is located in Adele Lehman Hall. Its collection includes over 300,000 volumes which support the undergraduate curriculum. It also houses an archival collection of official and student publications, photographs, letters and other material that documents Barnard's history from its founding in 1889 to the present day. Barnard's rare books collections include the Overbury Collection, the personal library of Nobel prize-winning poet Gabriela Mistral, and a small collection of other rare books. The Overbury Collection consists of 3,300 items, including special and first edition books as well as manuscript materials by and about American women authors. Alumnae Books is a collection of books donated by Barnard alumnae authors. Conflicting accounts list either Richard B. Snow or Philip M. Chu as the architect of Lehman Hall... as well as of the Amherst College library and one of the libraries at Princeton University. The building opened in 1959.
Barnard Library Zine Collection.
Birthed from a proposal by longtime zinester Jenna Freedman, Barnard collects zines in an effort to document the third-wave feminism and Riot Grrrl culture. The Zine Collection complements Barnard's women's studies research holdings because it gives room to voices of girls and women otherwise under or not at all represented in the book stacks. According to its Library collection development policy, Barnard's zines are "written by New York City and other urban women with an emphasis on zines by women of color. (In this case the word "woman" includes anyone who identifies as female and some who don't believe in binary gender.) The zines are personal and political publications on activism, anarchism, body image, third wave feminism, gender, parenting, queer community, riotgrrrl, sexual assault, and other topics."
Barnard's collection documents movements and trends in feminist thought through the personal work of artists, writers, and activists. Currently, the Barnard Zine Collection has over 4,000 items, including zines about race, gender, sexuality, childbirth, motherhood, politics, and relationships. Barnard attempts to collect two copies of each zine, one of which circulates with the second copy archived for preservation. To facilitate circulation, Barnard zines are cataloged in CLIO (the Columbia/Barnard OPAC) and OCLC's Worldcat.
Culture and student life.
Student organizations.
Every Barnard student is part of the Student Government Association (SGA), which elects a representative student government. SGA aims to facilitate the expression of opinions on matters that directly affect the Barnard community. Members of the Executive Board and the Representative Council of SGA promote these goals through active communication between students, faculty, and administration. The Executive Board includes the President of SGA, Vice President, Vice President for Campus Life, Vice President for Communications,and Vice President of Finance. Members of the Representative Council include the Senior Representative to the Board of Trustees, Junior Representative to the Board of Trustees, University Senator, Representative for Campus Policy, Representative for Academic Affairs, Representative for Diversity, Representative for Student Services, Representative for Student Interests, Representative for College Relations, Representative for Arts and Culture, Representative for Campus Affairs, and Representative for Information and Technology. In addition to these members the President and Vice President of each Class Council also sit on the Representative Council.
Student groups include theatre and vocal music groups, language clubs, literary magazines, a freeform radio station called WBAR, a biweekly magazine called the "Barnard Bulletin", community service groups, and others. Barnard students can also join extracurricular activities or organizations at Columbia University, while Columbia University students are allowed in most, but not all, Barnard organizations.
Barnard's McIntosh Activities Council (commonly known as McAC), named after the first President of Barnard, Millicent McIntosh, organizes various community focused events on campus, such as Big Sub and Midnight Breakfast. McAC is made up of five sub-committees which are the Multicultural committee, the Wellness committee, the Network committee, the Community committee, and the Action committee. Each committee has a different focus, such as hosting and publicizing multicultural events (Multicultural), having health and wellness related events (Wellness), giving students opportunities to be involved with Alumnae and various professionals (Network), planning events that bring the entire student body together (Community), and planning community service events that give back to the surrounding community (Action).
In 2011, Barnard's SGA and McAC will work together to bring back the Greek Games, an old but quite famous Barnard tradition.
Barnard College officially banned sororities in 1913, but Barnard students continue to participate in Columbia's five National Panhellenic Conference sororities—Alpha Chi Omega, Alpha Omicron Pi, Delta Gamma, Kappa Alpha Theta, and Sigma Delta Tau—and the National Pan-Hellenic Council Sororities- Alpha Kappa Alpha (Lambda chapter) and Delta Sigma Theta (Rho chapter) as well as other sororities in the Multicultural Greek Council. Two National Panhellenic Conference organizations were founded at Barnard College. The Alpha Omicron Pi Fraternity, founded on January 2, 1897, left campus during the 1913 ban but returned to establish its Alpha chapter in 2013. The Alpha Epsilon Phi, founded on October 24, 1909, is no longer on campus. As of 2010, Barnard does not fully recognize the National Panhellenic Conference sororities at Columbia, but it does provide some funding to account for Barnard students living in Columbia housing through these organizations.
Athletics.
Barnard athletes compete in the Ivy League (NCAA Division I) through the Columbia/Barnard Athletic Consortium, which was established in 1983. Through this arrangement, Barnard is the only women's college offering Division I athletics. There are 15 intercollegiate teams, and students also compete at the intramural and club levels.
From 1975–1983, before the establishment of the Columbia/Barnard Athletic Consortium, Barnard students competed as the "Barnard Bears". Prior to 1975, students referred to themselves as the "Barnard honeybears".
Seven Sisters—student collaborations.
Established within the Barnard Student Government Association (SGA), The Seven Sisters Governing Board represents Barnard College as part of the Seven Sisters Coalition, which is a group of representatives from student councils of the historic Seven Sisters colleges. The reps on the coordinating board of Seven Sisters Coalition are rotating every year to hold the annual Seven Sisters Conference in a serious but informal setting. The first Seven Sisters Conference was hosted by SGA student representatives at Barnard College in 2009. In fall 2013, the conference was hosted by Vassar college during the first weekend of November. The major topic focused on inner college collaborations and differences in student government structures among Seven Sisters Colleges. The Seven Sisters Coordinating Board of Barnard brought six Barnard student representatives to attend the Fall Semester conference, which was hosted at Vassar College in the past fall semester. Based on the Coalition Coordinating Board Constitution established in February 2013, Students delegates were initiating projects in the aspects of public relations,alumni outreach and website management to promote the presence and development of the seven sisters culture. Meanwhile, The Barnard delegates engaged in discussions about the various structures of the student governments among the historic seven sisters colleges.
Sustainability.
Barnard College has issued a statement affirming its commitment to environmental sustainability, a major part of which is the goal of reducing its greenhouse gas emissions by 30% by 2017. Student EcoReps work as a resource on environmental issues for students in Barnard's residence halls, while the student-run Earth Coalition works on outreach initiatives such as local park clean-ups, tutoring elementary school students in environmental education, and sponsoring environmental forums. Barnard earned a "C-" for its sustainability efforts on the College Sustainability Report Card 2009 published by the Sustainable Endowments Institute. Its highest marks were in Student Involvement and Food and Recycling, receiving a "B" in both categories.
Nine Ways of Knowing.
Nine Ways of Knowing are liberal arts requirements. Students must take one year of one laboratory science, study a single foreign language for four semesters, and complete one 3-credit course in each of the following categories: reason and value, social analysis, historical studies, cultures in comparison, quantitative and deductive reasoning, literature, and visual and performing arts. The use of AP or IB credit to fulfill these requirements is very limited, but Nine Ways of Knowing courses may overlap with major or minor requirements. In addition to the Nine Ways of Knowing, students must complete a first-year seminar, a first-year English course, and one semester of physical education.
Controversies.
In the spring of 1960 Columbia University president Grayson Kirk complained to the president of Barnard that Barnard students were wearing inappropriate clothing. The garments in question were pants and Bermuda shorts. The administration forced the student council to institute a dress code. Students would be allowed to wear shorts and pants only at Barnard and only if the shorts were no more than two inches above the knee and the pants were not tight. Barnard women crossing the street to enter the Columbia campus wearing shorts or pants were required to cover themselves with a long coat similar to a jilbab.
In March 1968, "The New York Times" ran an article on students who cohabited, identifying one of the persons they interviewed as a student at Barnard College from New Hampshire named "Susan". Barnard officials searched their records for women from New Hampshire and were able to determine that "Susan" was the pseudonym of a student (Linda LeClair) who was living with her boyfriend, a student at Columbia University. She was called before Barnard's student-faculty administration judicial committee, where she faced the possibility of expulsion. A student protest included a petition signed by 300 other Barnard women, admitting that they too had broken the regulations against cohabitating. The judicial committee reached a compromise and the student was allowed to remain in school, but was denied use of the college cafeteria and barred from all social activities. The student briefly became a focus of intense national attention. She eventually dropped out of Barnard.
In October 2011, the Barnard administration issued a controversial policy that mandated that every student must pay full-time tuition as of fall 2012, regardless of how many credits were taken. Students, families, and faculty alike responded with a petition on Change.org and a protest from students.

</doc>
<doc id="4240" url="http://en.wikipedia.org/wiki?curid=4240" title="Order of Saint Benedict">
Order of Saint Benedict

The Order of Saint Benedict (OSB; Latin: "Ordo Sancti Benedicti"), also knownin reference to the colour of its members' habitsas the Black Monks, is a Roman Catholic religious order of independent monastic communities that observe the Rule of Saint Benedict. Each community (monastery, priory or abbey) within the order maintains its own autonomy, while the order itself represents their mutual interests. The terms "Order of Saint Benedict" and "Benedictine Order" are, however, also used to refer to Benedictine communities "in toto", sometimes giving the incorrect impression that there exists a generalate or motherhouse with jurisdiction over them.
Internationally, the order is governed by the Benedictine Confederation, a body, established in 1883 by Pope Leo XIII's Brief "Summum semper", whose head is known as the Abbot Primate. Individuals whose communities are members of the order generally add the initials "OSB" after their names.
Historical development.
The monastery at Subiaco in Italy, established by Saint Benedict of Nursia circa 529, was the first of the dozen monasteries he founded. There is no evidence, however, that he intended to found an order and the Rule of St Benedict presupposes the autonomy of each community. As most monasteries founded during the Middle Ages adopted the Rule of St Benedict, it became a standard form of Western monasticism despite the absence of a Benedictine order.
Today, Benedictine monasticism is fundamentally different from other Western religious orders insofar as its individual communities are not part of a religious order with "Generalates" and "Superiors General". Rather, in modern times, the various autonomous houses have formed themselves loosely into congregations (for example, Cassinese, English, Solesmes, Subiaco, Camaldolese, Sylvestrines) that in turn are represented in the Benedictine Confederation that came into existence through Pope Leo XIII's Apostolic Brief "Summum semper" on July 12, 1883. This organization facilitates dialogue of Benedictine communities with each other and the relationship between Benedictine communities and other religious orders and the church at large.
The Rule of Saint Benedict is also used by a number of religious orders that began as reforms of the Benedictine tradition such as the Cistercians and Trappists although none of these groups are part of the Benedictine Confederation.
The largest number of Benedictines are Roman Catholics, but there are also some within the Anglican Communion and occasionally within other Christian denominations as well, for example, within the Lutheran Church.
England.
In the English Reformation, all monasteries were dissolved and their lands confiscated by the Crown, forcing their Catholic members to flee into exile on the Continent. During the 19th century they were able to return to England, including to Selby Abbey in Yorkshire, one of the few great monastic churches to survive the Dissolution.
St. Mildred's Priory, on the Isle of Thanet, Kent, was built in 1027 on the site of an abbey founded in 670 by the daughter of the first Christian King of Kent. Currently the priory is home to a community of Benedictine nuns. Four of the most notable English abbeys are the Basilica of St Gregory the Great at Downside, commonly known as Downside Abbey, Ealing Abbey in Ealing, West London, St. Lawrence's in Yorkshire (Ampleforth Abbey), and Worth Abbey. Prinknash Abbey, used by Henry VIII as a hunting lodge, was officially returned to the Benedictines four hundred years later, in 1928. During the next few years, so-called Prinknash Park was used as a home until it was returned to the order.
Since the Oxford Movement, there has also been a modest flourishing of Benedictine monasticism in the Anglican Church and Protestant Churches. Anglican Benedictine Abbots are invited guests of the Benedictine Abbot Primate in Rome at Abbatial gatherings at Sant'Anselmo. There are an estimated 2,400 celibate Anglican Religious (1,080 men and 1,320 women) in the Anglican Communion as a whole, some of whom have adopted the Rule of St. Benedict. For a full list of all historic Benedictine houses in England and Wales, see below.
France.
Monasticism had been introduced into the region of modern France during the Roman era by Saint Martin of Tours, who founded the first monastery in Western Europe. The Rule of St. Benedict was promoted by various rulers of France, especially the House of Capet. Figures such as Benedict of Aniane were authorized by the Emperor Louis the Pious and his successors to promote its adoption by monasteries throughout the Holy Roman Empire. It expanded throughout the next millennium, growing through periods of revival and decay over the centuries. Monasteries were among the institutions of the Catholic Church swept away during the French Revolution.
Monasteries were again allowed to form in the 19th century under the Bourbon Restoration. Later that century, under the Third French Republic, laws were enacted preventing religious teaching. The original intent was to allow secular schools. Thus in 1880 and 1882, Benedictine teaching monks were effectively exiled; this was not completed until 1901.
Benedictine vow and life.
Section 17 in chapter 58 of the Rule of Saint Benedict states the solemn promise candidates for reception into a Benedictine community are required to make: a promise of stability (i.e. to remain in the same community), "conversatio morum" (an idiomatic Latin phrase suggesting "conversion of manners"; see below) and obedience (to the community's superior, seen as holding the place of Christ within it). This solemn commitment tends to be referred to as the "Benedictine vow" and is the Benedictine antecedent and equivalent of the evangelical counsels professed by candidates for reception into a religious order.
Much scholarship over the last fifty years has been dedicated to the translation and interpretation of "conversatio morum". The older translation "conversion of life" has generally been replaced with phrases such as "[conversion to] a monastic manner of life", drawing from the Vulgate's use of "conversatio" as a translation of "citizenship" or "homeland" in Philippians 3:20. Some scholars have claimed that the vow formula of the Rule is best translated as "to live in this place as a monk, in obedience to its rule and abbot."
Benedictine abbots and abbesses have full jurisdiction of their abbey and thus absolute authority over the monks or nuns who are resident. This authority includes the power to assign duties, to decide which books may or may not be read, to regulate comings and goings, and to punish and to excommunicate, in the sense of an enforced isolation from the monastic community.
A tight communal timetablethe horariumis meant to ensure that the time given by God is not wasted but used in God's service, whether for prayer, work, meals, spiritual reading or sleep.
Although Benedictines do not take a vow of silence, hours of strict silence are set, and at other time silence is maintained as much as is practically possible. Social conversations tend to be limited to communal recreation times. But such details, like the many other details of the daily routine of a Benedictine house that the Rule of St Benedict leaves to the discretion of the superior, are set out in its customary.
In the Roman Catholic Church, according to the norms of the 1983 Code of Canon Law, a Benedictine abbey is a "religious institute" and its members are therefore members of the consecrated life. While Canon Law 588 §1 explains that Benedictine monks are "neither clerical nor lay", they can, however, be ordained. Benedictine Oblates endeavor to embrace the spirit of the Benedictine vow in their own life in the world.

</doc>
<doc id="4241" url="http://en.wikipedia.org/wiki?curid=4241" title="Bayezid I">
Bayezid I

Bayezid I (; ; nicknamed "Yıldırım" (Ottoman Turkish: ییلدیرم), "The Thunderbolt"; 1354 – 8 March 1403) was the Sultan of the Ottoman Empire from 1389 to 1402. He was the son of Murad I and Gülçiçek Hatun.
Biography.
The first major role of Bayezid was as governor of Kütahya, ciry that was conquered from the Germiyanids. He was an impetuous soldier, earning the nickname of Thunderbolt in a battle against the Karamanids.
Bayezid ascended to the throne following the death of his father Murad I, who was killed by Serbian knight Miloš Obilić during (15 June), or immediately after (16 June), the Battle of Kosovo in 1389, by which Serbia became a vassal of the Ottoman Empire. Immediately after obtaining the throne, he had his younger brother strangled to avoid a plot. In 1390, Bayezid took as a wife Princess Olivera Despina, the daughter of Prince Lazar of Serbia, who also lost his life in Kosovo. Bayezid recognized Stefan Lazarević, the son of Lazar, as the new Serbian leader (later despot), with considerable autonomy.
The upper Serbia resisted the ottomans until general Pashayigit captured the city of Skopje in 1391, converting he city in an important base of operations.
Meanwhile, the sultan suppressed a revolt of the Anatolian beyliks against him from 1389 to 1390. The petty emirates of Aydin, Saruhan , Aydin, Saruhan and Hamid were annexed.He couldn't conquer Karaman however because the emir Sulayman allied with the ruler of Sivas, Kadi Burhan al-Din.
From 1389 to 1395 he conquered Bulgaria and northern Greece. In 1394 he crossed the River Danube to attack Wallachia, ruled at that time by Mircea the Elder. The Ottomans were superior in number, but on 10 October 1394 (or 17 May 1395), in the Battle of Rovine, on forested and swampy terrain, the Wallachians won the fierce battle and prevented Bayezid's army from advancing beyond the Danube.
Meanwhile, he begin the reunification of the Turkish Anatolia, conquering the beyliks of Aydin, Saruhan in 1390, the beyliks of Aydin, Saruhanand Kastamonu in 1391; and finally the great emirate of Karaman and the ex-emirate of Burhan-ad-Din in Tokat, Sivas and Kayseri(1397–98). Next he occuped the cities of Malatya and Elbistan, in a war with the mamluk sultan of Egypt.
In 1394, Bayezid laid siege to Constantinople, the capital of the Byzantine Empire. Anadoluhisarı fortress was built between 1393 and 1394 as part of preparations for the Second Ottoman Siege of Constantinople, which took place in 1395. On the urgings of the Byzantine emperor Manuel II Palaeologus a new crusade was organized to defeat him. This proved unsuccessful: in 1396 the Christian allies, under the leadership of the King of Hungary and future Holy Roman Emperor (in 1433) Sigismund, were defeated in the Battle of Nicopolis. Bayezid built the magnificent Ulu Cami in Bursa, to celebrate this victory.
Thus, the siege of Constantinople continued, lasting until 1402. The beleaguered Byzantines had their reprieve when Bayezid fought the Timurid in the East. At this time, the empire of Bayezid included Thrace (except Constantinople), Macedonia, Bulgaria, and parts of Serbia in Europe. In Asia, his domains extended to the Taurus Mountains. His army was considered one of the best in the Islamic world. In 1400, the Central Asian warlord Timur succeeded in rousing the local Turkic beyliks that had been vassals of the Ottomans to join him in his attack on Bayezid, who was also considered one of the most powerful rulers in the Muslim world during that period. In the fateful Battle of Ankara, on 20 July 1402, Bayezid was captured by Timur and the Ottoman army was defeated. Many writers claim that Bayezid was mistreated by the Timurids. However, writers and historians from Timur's own court reported that Bayezid was treated well, and that Timur even mourned his death. One of Bayezid's sons, Mustafa Çelebi, was captured with him and held captive in Samarkand until 1405.
Four of Bayezid's sons, specifically Süleyman Çelebi, İsa Çelebi, Mehmed Çelebi, and Musa Çelebi, however, escaped from the battlefield and later started a civil war for the Ottoman throne known as the Ottoman Interregnum. After Mehmed's victory, his coronation as Mehmed I, and the death of all four but Mehmed, Bayezid's other son Mustafa Çelebi emerged from hiding and began two failed rebellions against his brother Mehmed and, after Mehmed's death, his nephew Murat II.
Legacy.
A commando battalion in the Pakistan Army is named Yaldaram Battalion after him.
Yildirim Beyazit University, a state university in Turkey, is also named after him.
Marriages and progeny.
His mother was Valide Sultan Gülçiçek Hatun who was of ethnic Greek descent.
In fiction.
The defeat of Bayezid became a popular subject for later Western writers, composers, and painters. They embellished the legend that he was taken by Timur to Samarkand with a cast of characters to create an oriental fantasy that has maintained its appeal. Christopher Marlowe's play "Tamburlaine the Great" was first performed in London in 1587, three years after the formal opening of English-Ottoman trade relations when William Harborne sailed for Constantinople as an agent of the Levant Company. In 1648, the play "Le Gran Tamerlan et Bejezet" by Jean Magnon appeared in London, and in 1725, Handel's "Tamerlano" was first performed and published in London; Vivaldi's version of the story, "Bajazet", was written in 1735. Magnon had given Bayezid an intriguing wife and daughter; the Handel and Vivaldi renditions included, as well as Tamerlane and Bayezid and his daughter, a prince of Byzantium and a princess of Trebizond (Trabzon) in a passionate love story. A cycle of paintings in Schloss Eggenberg, near Graz in Austria, translated the theme to a different medium; this was completed in the 1670s shortly before the Ottoman army attacked the Habsburgs in central Europe. Bayezid (spelled Bayazid) is a central character in the Robert E. Howard story "Lord of Samarcand."
External links.
[aged 47–48]

</doc>
<doc id="4242" url="http://en.wikipedia.org/wiki?curid=4242" title="Bayezid II">
Bayezid II

Bayezid II or Sultân Bayezid-î Velî (December 3, 1447 – May 26, 1512) (Ottoman Turkish: بايزيد ثانى "Bāyezīd-i sānī", Turkish:"II. Bayezid" or "II. Beyazıt") was the eldest son and successor of Mehmed II, ruling as Sultan of the Ottoman Empire from 1481 to 1512. During his reign, Bayezid II consolidated the Ottoman Empire and thwarted a Safavid rebellion soon before abdicating his throne to his son, Selim I. He is most notable for evacuating Jews from Spain after the proclamation of the Alhambra Decree and resettling them throughout the Ottoman Empire.
Early life.
Bayezid II was the son of Mehmed II (1432–81). His mother's identity is undetermined; there are two main theories, that she was Emine Gülbahar Hatun or Sitti Mükrime Hatun.
Bayezid II married Ayşe Hatun, who was the mother of his eldest son Şehzade Ahmet, as well as Bayezid II's heir and successor, Selim I and nephew of Sitti Mükrime Hatun, another wife of Mehmed II.
Fight for the throne.
Bayezid II's overriding concern was the quarrel with his brother Cem, who claimed the throne and sought military backing from the Mamluks in Egypt. Having been defeated by his brother's armies, Cem sought protection from the Knights of St. John in Rhodes. Eventually, the Knights handed Cem over to Pope Innocent VIII (1484–1492). The Pope thought of using Cem as a tool to drive the Turks out of Europe, but, as the papal crusade failed to come to fruition, Cem was left to languish and die in a Neapolitan prison.
Reign.
Bayezid II ascended the Ottoman throne in 1481. Like his father, Bayezid II was a patron of western and eastern culture and unlike many other Sultans, worked hard to ensure a smooth running of domestic politics, which earned him the epithet of "the Just". Throughout his reign, Bayezid II engaged in numerous campaigns to conquer the Venetian possessions in Morea, accurately defining this region as the key to future Ottoman naval power in the Eastern Mediterranean. The last of these wars ended in 1501 with Bayezid II in control of the whole Peloponnese. Rebellions in the east, such as that of the Qizilbash, plagued much of Bayezid II's reign and were often backed by the Shah of Persia, Ismail, who was eager to promote Shi'ism to undermine the authority of the Ottoman state. Ottoman authority in Anatolia was indeed seriously threatened during this period, and at one point Bayezid II's grand vizier, Ali Pasha, was killed in battle against rebels.
Jewish and Muslim immigration.
In July 1492, the new state of Spain expelled its Jewish and Muslim populations as part of the Spanish Inquisition. Bayezid II sent out the Ottoman Navy under the command of Admiral Kemal Reis to Spain in 1492 in order to evacuate them safely to Ottoman lands. He sent out proclamations throughout the empire that the refugees were to be welcomed. He granted the refugees the permission to settle in the Ottoman Empire and become Ottoman citizens. He ridiculed the conduct of Ferdinand II of Aragon and Isabella I of Castile in expelling a class of people so useful to their subjects. "You venture to call Ferdinand a wise ruler," he said to his courtiers — "he who has impoverished his own country and enriched mine!" Bayezid addressed a firman to all the governors of his European provinces, ordering them not only to refrain from repelling the Spanish refugees, but to give them a friendly and welcome reception. He threatened with death all those who treated the Jews harshly or refused them admission into the empire. Moses Capsali, who probably helped to arouse the sultan's friendship for the Jews, was most energetic in his assistance to the exiles. He made a tour of the communities, and was instrumental in imposing a tax upon the rich, to ransom the Jewish victims of the persecutions then prevalent.
The Muslims and Jews of al-Andalus (Iberia) contributed much to the rising power of the Ottoman Empire by introducing new ideas, methods and craftsmanship. The first printing press in Constantinople was established by the Sephardic Jews in 1493. It is reported that under Bayezid's reign, Jews enjoyed a period of cultural flourishing, with the presence of such scholars as the Talmudist and scientist Mordecai Comtino; astronomer and poet Solomon ben Elijah Sharbiṭ ha-Zahab; Shabbethai ben Malkiel Cohen, and the liturgical poet Menahem Tamar.
Succession.
On September 14, 1509, Constantinople was devastated by an earthquake. Bayezid II's final years saw a succession battle between his sons Selim I and Ahmet. Ahmet unexpectedly captured Karaman, an Ottoman city, and began marching to Constantinople to exploit his triumph. Fearing for his safety, Selim staged a revolt in Thrace but was defeated by Bayezid and forced to flee back to the Crimean Peninsula. Bayezid II developed fears that Ahmet might in turn kill him to gain the throne and refused to allow his son to enter Constantinople.
Selim returned from Crimea and, with support from the Janissaries, forced his father to abdicate the throne on April 25, 1512. Beyazid departed for retirement in his native Demotika, but he died on May 26, 1512 at Büyükçekmece before reaching his destination, and only a month after his abdication. He was buried next to the Bayezid Mosque in Istanbul.

</doc>
<doc id="4243" url="http://en.wikipedia.org/wiki?curid=4243" title="Boxing">
Boxing

Boxing is a combat sport in which two people engage in a contest of strength, speed, reflexes, endurance, and will, by throwing punches at each other, usually with gloved hands.
Amateur boxing is both an Olympic and Commonwealth sport and is a common fixture in most international games—it also has its own World Championships. Boxing is supervised by a referee over a series of one- to three-minute intervals called rounds. The result is decided when an opponent is deemed incapable to continue by a referee, is disqualified for breaking a rule, resigns by throwing in a towel, or is pronounced the winner or loser based on the judges' scorecards at the end of the contest.
While people have fought in hand-to-hand combat since before the dawn of history, the origin of boxing as an organized sport may be its acceptance by the ancient Greeks as an Olympic game in BC 688. Boxing evolved from 16th- and 18th-century prizefights, largely in Great Britain, to the forerunner of modern boxing in the mid-19th century, again initially in Great Britain and later in the United States.
History.
Early history.
"See also Ancient Greek boxing
The earliest known depiction of boxing comes from a Sumerian relief from the 3rd millennium BC. Later depictions from the 2nd millennium BC are found in reliefs from the Mesopotamian nations of Assyria and Babylonia, and in Hittite art from Asia Minor. The earliest evidence for fist fighting with any kind of gloves can be found on Minoan Crete (c. 1500–900 BC), and on Sardinia, if we consider the boxing statues of Prama mountains (c. 2000–1000 BC).
Boxing was a popular spectator sport in Ancient Rome. In order for the fighters to protect themselves against their opponents they wrapped leather thongs around their fists. Eventually harder leather was used and the thong soon became a weapon. The Romans even introduced metal studs to the thongs to make the cestus which then led to a more sinister weapon called the myrmex (‘limb piercer’). Fighting events were held at Roman Amphitheatres. The Roman form of boxing was often a fight until death to please the spectators who gathered at such events. However, especially in later times, purchased slaves and trained combat performers were valuable commodities, and their lives were not given up without due consideration. Often slaves were used against one another in a circle marked on the floor. This is where the term ring came from. In 393 AD, during the Roman gladiator period, boxing was abolished due to excessive brutality. It was not until the late 17th century that boxing re-surfaced in London.
Modern boxing.
Broughton's rules (1743).
Records of Classical boxing activity disappeared after the fall of the Western Roman Empire when the wearing of weapons became common once again and interest in fighting with the fists waned. However, there are detailed records of various fist-fighting sports that were maintained in different cities and provinces of Italy between the 12th and 17th centuries. There was also a sport in ancient Rus called Kulachniy Boy or "Fist Fighting".
As the wearing of swords became less common, there was renewed interest in fencing with the fists. The sport would later resurface in England during the early 16th century in the form of bare-knuckle boxing sometimes referred to as prizefighting. The first documented account of a bare-knuckle fight in England appeared in 1681 in the "London Protestant Mercury", and the first English bare-knuckle champion was James Figg in 1719. This is also the time when the word "boxing" first came to be used. It should be noted, that this earliest form of modern boxing was very different. Contests in Mr. Figg's time, in addition to fist fighting, also contained fencing and cudgeling. On 6 January 1681, the first recorded boxing match took place in Britain when Christopher Monck, 2nd Duke of Albemarle (and later Lieutenant Governor of Jamaica) engineered a bout between his butler and his butcher with the latter winning the prize.
Early fighting had no written rules. There were no weight divisions or round limits, and no referee. In general, it was extremely chaotic. The first boxing rules, called the Broughton's rules, were introduced by champion Jack Broughton in 1743 to protect fighters in the ring where deaths sometimes occurred. Under these rules, if a man went down and could not continue after a count of 30 seconds, the fight was over. Hitting a downed fighter and grasping below the waist were prohibited. Broughton also invented and encouraged the use of "mufflers", a form of padded gloves, which were used in training and exhibitions. The first paper on boxing was published in the early 1700s by a successful Cornish Wrestler from Bunnyip, Cornwall, named Sir Thomas Parkyns, who was also a Physics student of Sir Isaac Newton. The paper was actually a single page in his extensive Wrestling & Fencing manual that entailed a system of headbutting, punching, eye gouging, chokes, and hard throws not common in modern Boxing. Parkyns added the techniques described in his paper to his own fighting style.
These rules did allow the fighters an advantage not enjoyed by today's boxers; they permitted the fighter to drop to one knee to begin a 30-second count at any time. Thus a fighter realizing he was in trouble had an opportunity to recover. However, this was considered "unmanly" and was frequently disallowed by additional rules negotiated by the Seconds of the Boxers. Intentionally going down in modern boxing will cause the recovering fighter to lose points in the scoring system. Furthermore, as the contestants did not have heavy leather gloves and wristwraps to protect their hands, they used different punching technique to preserve their hands because the head was a common target to hit full out as almost all period manuals have powerful straight punches with the whole body behind them to the face (including forehead) as the basic blows.
London Prize Ring rules (1838).
In 1838, the London Prize Ring rules were codified. Later revised in 1853, they stipulated the following:
Marquess of Queensberry rules (1867).
In 1867, the Marquess of Queensberry rules were drafted by John Chambers for amateur championships held at Lillie Bridge in London for Lightweights, Middleweights and Heavyweights. The rules were published under the patronage of the Marquess of Queensberry, whose name has always been associated with them.
There were twelve rules in all, and they specified that fights should be "a fair stand-up boxing match" in a 24-foot-square or similar ring. Rounds were three minutes with one-minute rest intervals between rounds. Each fighter was given a ten-second count if he were knocked down, and wrestling was banned.
The introduction of gloves of "fair-size" also changed the nature of the bouts. An average pair of boxing gloves resembles a bloated pair of mittens and are laced up around the wrists.
The gloves can be used to block an opponent's blows. As a result of their introduction, bouts became longer and more strategic with greater importance attached to defensive maneuvers such as slipping, bobbing, countering and angling. Because less defensive emphasis was placed on the use of the forearms and more on the gloves, the classical forearms outwards, torso leaning back stance of the bare knuckle boxer was modified to a more modern stance in which the torso is tilted forward and the hands are held closer to the face.
Modern.
Through the late nineteenth century, the martial art of boxing or prizefighting was primarily a sport of dubious legitimacy. Outlawed in England and much of the United States, prizefights were often held at gambling venues and broken up by police. Brawling and wrestling tactics continued, and riots at prizefights were common occurrences. Still, throughout this period, there arose some notable bare knuckle champions who developed fairly sophisticated fighting tactics.
The English case of "R v. Coney" in 1882 found that a bare-knuckle fight was an assault occasioning actual bodily harm, despite the consent of the participants. This marked the end of widespread public bare-knuckle contests in England.
The first world heavyweight champion under the Queensberry Rules was "Gentleman Jim" Corbett, who defeated John L. Sullivan in 1892 at the Pelican Athletic Club in New Orleans.
The first instance of film censorship in the United States occurred in 1897 when several states banned the showing of prize fighting films from the state of Nevada, where it was legal at the time.
Throughout the early twentieth century, boxers struggled to achieve legitimacy. They were aided by the influence of promoters like Tex Rickard and the popularity of great champions such as John L. Sullivan.
Rules.
The "Marquess of Queensberry rules" have been the general rules governing modern boxing since their publication in 1867.
A boxing match typically consists of a determined number of three-minute rounds, a total of up to 12 rounds (formerly 15). A minute is typically spent between each round with the fighters in their assigned corners receiving advice and attention from their coach and staff. The fight is controlled by a referee who works within the ring to judge and control the conduct of the fighters, rule on their ability to fight safely, count knocked-down fighters, and rule on fouls.
Up to three judges are typically present at ringside to score the bout and assign points to the boxers, based on punches that connect, defense, knockdowns, and other, more subjective, measures. Because of the open-ended style of boxing judging, many fights have controversial results, in which one or both fighters believe they have been "robbed" or unfairly denied a victory. Each fighter has an assigned corner of the ring, where his or her coach, as well as one or more "seconds" may administer to the fighter at the beginning of the fight and between rounds. Each boxer enters into the ring from their assigned corners at the beginning of each round and must cease fighting and return to their corner at the signaled end of each round.
A bout in which the predetermined number of rounds passes is decided by the judges, and is said to "go the distance". The fighter with the higher score at the end of the fight is ruled the winner. With three judges, unanimous and split decisions are possible, as are draws. A boxer may win the bout before a decision is reached through a knock-out ; such bouts are said to have ended "inside the distance". If a fighter is knocked down during the fight, determined by whether the boxer touches the canvas floor of the ring with any part of their body other than the feet as a result of the opponent's punch and not a slip, as determined by the referee, the referee begins counting until the fighter returns to his or her feet and can continue.
Should the referee count to ten, then the knocked-down boxer is ruled "knocked out" (whether unconscious or not) and the other boxer is ruled the winner by knockout (KO). A "technical knock-out" (TKO) is possible as well, and is ruled by the referee, fight doctor, or a fighter's corner if a fighter is unable to safely continue to fight, based upon injuries or being judged unable to effectively defend themselves. Many jurisdictions and sanctioning agencies also have a "three-knockdown rule", in which three knockdowns in a given round result in a TKO. A TKO is considered a knockout in a fighter's record. A "standing eight" count rule may also be in effect. This gives the referee the right to step in and administer a count of eight to a fighter that he feels may be in danger, even if no knockdown has taken place. After counting the referee will observe the fighter, and decide if he is fit to continue. For scoring purposes, a standing eight count is treated as a knockdown.
In general, boxers are prohibited from hitting below the belt, holding, tripping, pushing, biting, or spitting. The boxer's shorts are raised so the opponent is not allowed to hit to the groin area with intent to cause pain or injury. Failure to abide by the former may result in a foul. They also are prohibited from kicking, head-butting, or hitting with any part of the arm other than the knuckles of a closed fist (including hitting with the elbow, shoulder or forearm, as well as with open gloves, the wrist, the inside, back or side of the hand). They are prohibited as well from hitting the back, back of the neck or head (called a "rabbit-punch") or the kidneys. They are prohibited from holding the ropes for support when punching, holding an opponent while punching, or ducking below the belt of their opponent (dropping below the waist of your opponent, no matter the distance between).
If a "clinch" – a defensive move in which a boxer wraps his or her opponents arms and holds on to create a pause – is broken by the referee, each fighter must take a full step back before punching again (alternatively, the referee may direct the fighters to "punch out" of the clinch). When a boxer is knocked down, the other boxer must immediately cease fighting and move to the furthest neutral corner of the ring until the referee has either ruled a knockout or called for the fight to continue.
Violations of these rules may be ruled "fouls" by the referee, who may issue warnings, deduct points, or disqualify an offending boxer, causing an automatic loss, depending on the seriousness and intentionality of the foul. An intentional foul that causes injury that prevents a fight from continuing usually causes the boxer who committed it to be disqualified. A fighter who suffers an accidental low-blow may be given up to five minutes to recover, after which they may be ruled knocked out if they are unable to continue. Accidental fouls that cause injury ending a bout may lead to a "no contest" result, or else cause the fight to go to a decision if enough rounds (typically four or more, or at least three in a four-round fight) have passed.
Unheard of these days, but common during the early 20th Century in North America, a "newspaper decision (NWS)" might be made after a no decision bout had ended. A "no decision" bout occurred when, by law or by pre-arrangement of the fighters, if both boxers were still standing at the fight's conclusion and there was no knockout, no official decision was rendered and neither boxer was declared the winner. But this did not prevent the pool of ringside newspaper reporters from declaring a consensus result among themselves and printing a newspaper decision in their publications. Officially, however, a "no decision" bout resulted in neither boxer winning or losing. Boxing historians sometimes use these unofficial newspaper decisions in compiling fight records for illustrative purposes only. Often, media outlets covering a match will personally score the match, and post their scores as an independent sentence in their report.
Professional vs. amateur boxing.
Throughout the 17th through 19th centuries, boxing bouts were motivated by money, as the fighters competed for prize money, promoters controlled the gate, and spectators bet on the result. The modern Olympic movement revived interest in amateur sports, and amateur boxing became an Olympic sport in 1908. In their current form, Olympic and other amateur bouts are typically limited to three or four rounds, scoring is computed by points based on the number of clean blows landed, regardless of impact, and fighters wear protective headgear, reducing the number of injuries, knockdowns, and knockouts. Currently scoring blows in amateur boxing are subjectively counted by ringside judges, but the Australian Institute for Sport has demonstrated a prototype of an Automated Boxing Scoring System, which introduces scoring objectivity, improves safety, and arguably makes the sport more interesting to spectators. Professional boxing remains by far the most popular form of the sport globally, though amateur boxing is dominant in Cuba and some former Soviet republics. For most fighters, an amateur career, especially at the Olympics, serves to develop skills and gain experience in preparation for a professional career.
Amateur boxing.
Amateur boxing may be found at the collegiate level, at the Olympic Games and Commonwealth Games, and in many other venues sanctioned by amateur boxing associations. Amateur boxing has a point scoring system that measures the number of clean blows landed rather than physical damage. Bouts consist of three rounds of three minutes in the Olympic and Commonwealth Games, and three rounds of three minutes in a national ABA (Amateur Boxing Association) bout, each with a one-minute interval between rounds.
Competitors wear protective headgear and gloves with a white strip or circle across the knuckle. There are cases however, where white ended gloves are not required but any solid color may be worn. The white end just is a way to make it easier for judges to score clean hits. Each competitor must have their hands properly wrapped, pre-fight, for added protection on their hands and for added cushion under the gloves. Gloves worn by the fighters must be twelve ounces in weight unless, the fighters weigh under 165 pounds, thus allowing them to wear 10 ounce gloves. A punch is considered a scoring punch only when the boxers connect with the white portion of the gloves. Each punch that lands cleanly on the head or torso with sufficient force is awarded a point. A referee monitors the fight to ensure that competitors use only legal blows. A belt worn over the torso represents the lower limit of punches – any boxer repeatedly landing low blows below the belt is disqualified. Referees also ensure that the boxers don't use holding tactics to prevent the opponent from swinging. If this occurs, the referee separates the opponents and orders them to continue boxing. Repeated holding can result in a boxer being penalized or ultimately disqualified. Referees will stop the bout if a boxer is seriously injured, if one boxer is significantly dominating the other or if the score is severely imbalanced. Amateur bouts which end this way may be noted as "RSC" (referee stopped contest) with notations for an outclassed opponent (RSCO), outscored opponent (RSCOS), injury (RSCI) or head injury (RSCH).now as per the new rules headgear is removed as protection in the upcoming games
Professional boxing.
Professional bouts are usually much longer than amateur bouts, typically ranging from ten to twelve rounds, though four round fights are common for less experienced fighters or club fighters. There are also some two- and three-round professional bouts, especially in Australia. Through the early twentieth century, it was common for fights to have unlimited rounds, ending only when one fighter quit, benefiting high-energy fighters like Jack Dempsey. Fifteen rounds remained the internationally recognized limit for championship fights for most of the twentieth century until the early 1980s, when the death of boxer Duk Koo Kim eventually prompted the World Boxing Council and other organizations sanctioning professional boxing to reduce the limit to twelve rounds.
Headgear is not permitted in professional bouts, and boxers are generally allowed to take much more damage before a fight is halted. At any time, however, the referee may stop the contest if he believes that one participant cannot defend himself due to injury. In that case, the other participant is awarded a technical knockout win. A technical knockout would also be awarded if a fighter lands a punch that opens a cut on the opponent, and the opponent is later deemed not fit to continue by a doctor because of the cut. For this reason, fighters often employ cutmen, whose job is to treat cuts between rounds so that the boxer is able to continue despite the cut. If a boxer simply quits fighting, or if his corner stops the fight, then the winning boxer is also awarded a technical knockout victory. In contrast with amateur boxing, professional male boxers have to be bare chested.
Boxing styles.
Definition of style.
"Style" is often defined as the strategic approach a fighter takes during a bout. No two fighters' styles are alike, as it is determined by that individual's physical and mental attributes. There are three main styles in boxing: out-fighter ("boxer"), brawler (or "slugger"), and In-fighter ("swarmer"). These styles may be divided into several special subgroups, such as counter puncher, etc. The main philosophy of the styles is, that each style has an advantage over one, but disadvantage over the other one. It follows the rock-paper-scissors scenario - boxer beats brawler, swarmer beats boxer, and brawler beats swarmer.
Boxer/out-fighter.
A classic "boxer" or stylist (also known as an "out-fighter") seeks to maintain distance between himself and his opponent, fighting with faster, longer range punches, most notably the jab, and gradually wearing his opponent down. Due to this reliance on weaker punches, out-fighters tend to win by point decisions rather than by knockout, though some out-fighters have notable knockout records. They are often regarded as the best boxing strategists due to their ability to control the pace of the fight and lead their opponent, methodically wearing him down and exhibiting more skill and finesse than a brawler. Out-fighters need reach, hand speed, reflexes, and footwork.
Notable out-fighters include Muhammad Ali, Larry Holmes, Joe Calzaghe, Floyd Mayweather Jr., Wilfredo Gómez, 
Salvador Sanchez, Cecilia Brækhus, Gene Tunney, Ezzard Charles, Willie Pep, Meldrick Taylor, Ricardo Lopez, Roy Jones, Jr., and Sugar Ray Leonard, Miguel Vazquez. This style was also used by fictional boxer Apollo Creed.
Boxer-puncher.
A boxer-puncher is a well-rounded boxer who is able to fight at close range with a combination of technique and power, often with the ability to knock opponents out with a combination and in some instances a single shot. Their movement and tactics are similar to that of an out-fighter (although they are generally not as mobile as an out-fighter), but instead of winning by decision, they tend to wear their opponents down using combinations and then move in to score the knockout. A boxer must be well rounded to be effective using this style.
Notable boxer-punchers include Muhammad Ali, Manny Pacquiao, Wladimir Klitschko, Lennox Lewis, Joe Louis, Wilfredo Gómez, Oscar de la Hoya, Archie Moore, Miguel Cotto, Nonito Donaire, Sam Langford, Henry Armstrong, Sugar Ray Robinson, Tony Zale, Carlos Monzón, Alexis Argüello, Erik Morales, Terry Norris, Marco Antonio Barrera, Naseem Hamed, Thomas Hearns and Victor Ortiz.
Counter puncher.
Counter punchers are slippery, defensive style fighters who often rely on their opponent's mistakes in order to gain the advantage, whether it be on the score cards or more preferably a knockout. They use their well-rounded defense to avoid or block shots and then immediately catch the opponent off guard with a well placed and timed punch. A fight with a skilled counter-puncher can turn into a war of attrition, where each shot landed is a battle in itself. Thus, fighting against counter punchers requires constant feinting and the ability to avoid telegraphing ones attacks. To be truly successful using this style they must have good reflexes, a high level of prediction and awareness, pinpoint accuracy and speed, both in striking and in footwork.
Notable counter punchers include Muhammad Ali, Vitali Klitschko, Floyd Mayweather, Jr., Evander Holyfield, Max Schmeling, Chris Byrd, Jim Corbett, Jack Johnson, Bernard Hopkins, Laszlo Papp, Jerry Quarry, Anselmo Moreno, James Toney, Marvin Hagler, Juan Manuel Márquez, Humberto Soto, Roger Mayweather, Pernell Whitaker and Sergio Gabriel Martinez.
Counter punchers usually wear their opponents down by causing them to miss their punches. The more the opponent misses, the faster they tire, and the psychological effects of being unable to land a hit will start to sink in. The counter puncher often tries to outplay their opponent entirely, not just in a physical sense, but also in a mental and emotional sense. This style can be incredibly difficult, especially against seasoned fighters, but winning a fight without getting hit is often worth the pay-off. They usually try to stay away from the center of the ring, in order to outmaneuver and chip away at their opponents. A large advantage in counter-hitting is the forward momentum of the attacker, which drives them further into your return strike. As such, knockouts are more common than one would expect from a defensive style.
Brawler/slugger.
A brawler is a fighter who generally lacks finesse and footwork in the ring, but makes up for it through sheer punching power. Mainly Irish, Irish-American, Puerto Rican, Mexican, and Mexican-American boxers popularized this style. Many brawlers tend to lack mobility, preferring a less mobile, more stable platform and have difficulty pursuing fighters who are fast on their feet. They may also have a tendency to ignore combination punching in favor of continuous beat-downs with one hand and by throwing slower, more powerful single punches (such as hooks and uppercuts). Their slowness and predictable punching pattern (single punches with obvious leads) often leaves them open to counter punches, so successful brawlers must be able to absorb substantial amounts of punishment. However not all brawler/slugger fighters are not mobile, some can move around and switch styles if needed but still have the brawler/slugger style such as Wilfredo Gómez, Prince Naseem Hamed and Danny García.
A brawler's most important assets are power and chin (the ability to absorb punishment while remaining able to continue boxing). Examples of this style include George Foreman, Danny García, Wilfredo Gómez, Sonny Liston, John L. Sullivan, Max Baer, Prince Naseem Hamed, Ray Mancini, David Tua, Arturo Gatti, Micky Ward, Michael Katsidis, James Kirkland, Marcos Maidana, Jake Lamotta, Manny Pacquiao, and Ireland's John Duddy. This style of boxing was also used by fictional boxers Rocky Balboa and James "Clubber" Lang.
Brawlers tend to be more predictable and easy to hit but usually fare well enough against other fighting styles because they train to take punches very well. They often have a higher chance than other fighting styles to score a knockout against their opponents because they focus on landing big, powerful hits, instead of smaller, faster attacks. Oftentimes they place focus on training on their upper body instead of their entire body, to increase power and endurance. They also aim to intimidate their opponents because of their power, stature and ability to take a punch.
Swarmer/in-fighter.
In-fighters/swarmers (sometimes called "pressure fighters") attempt to stay close to an opponent, throwing intense flurries and combinations of hooks and uppercuts. A successful in-fighter often needs a good "chin" because swarming usually involves being hit with many jabs before they can maneuver inside where they are more effective. In-fighters operate best at close range because they are generally shorter and have less reach than their opponents and thus are more effective at a short distance where the longer arms of their opponents make punching awkward. However, several fighters tall for their division have been relatively adept at in-fighting as well as out-fighting.
The essence of a swarmer is non-stop aggression. Many short in-fighters utilize their stature to their advantage, employing a bob-and-weave defense by bending at the waist to slip underneath or to the sides of incoming punches. Unlike blocking, causing an opponent to miss a punch disrupts his balance, permits forward movement past the opponent's extended arm and keeps the hands free to counter. A distinct advantage that in-fighters have is when throwing uppercuts where they can channel their entire bodyweight behind the punch; Mike Tyson was famous for throwing devastating uppercuts. Marvin Hagler was known for his hard "chin", punching power, body attack and the stalking of his opponents. Some in-fighters, like Mike Tyson, have been known for being notoriously hard to hit. The key to a swarmer is aggression, endurance, chin, and bobbing-and-weaving.
Notable in-fighters include Julio César Chávez, Miguel Cotto, Joe Frazier, Danny García, Mike Tyson, Manny Pacquiao, Saúl Álvarez, Rocky Marciano, Jack Dempsey, Wayne McCullough, Harry Greb, David Tua and Ricky Hatton.
Combinations of styles.
All fighters have primary skills with which they feel most comfortable, but truly elite fighters are often able to incorporate auxiliary styles when presented with a particular challenge. For example, an out-fighter will sometimes plant his feet and counter punch, or a slugger may have the stamina to pressure fight with his power punches.
Style matchups.
There is a generally accepted rule of thumb about the success each of these boxing styles has against the others. In general, an in-fighter has an advantage over an out-fighter, an out-fighter has an advantage over a brawler, and a brawler has an advantage over an in-fighter; these form a cycle with each style being stronger relative to one, and weaker relative to another, with none dominating, as in rock-paper-scissors. Naturally, many other factors, such as the skill level and training of the combatants, determine the outcome of a fight, but the widely held belief in this relationship among the styles is embodied in the cliché amongst boxing fans and writers that "styles make fights."
Brawlers tend to overcome swarmers or in-fighters because, in trying to get close to the slugger, the in-fighter will invariably have to walk straight into the guns of the much harder-hitting brawler, so, unless the former has a very good chin and the latter's stamina is poor, the brawler's superior power will carry the day. A famous example of this type of match-up advantage would be George Foreman's knockout victory over Joe Frazier in their original bout "The Sunshine Showdown".
Although in-fighters struggle against heavy sluggers, they typically enjoy more success against out-fighters or boxers. Out-fighters prefer a slower fight, with some distance between themselves and the opponent. The in-fighter tries to close that gap and unleash furious flurries. On the inside, the out-fighter loses a lot of his combat effectiveness, because he cannot throw the hard punches. The in-fighter is generally successful in this case, due to his intensity in advancing on his opponent and his good agility, which makes him difficult to evade. For example, the swarming Joe Frazier, though easily dominated by the slugger George Foreman, was able to create many more problems for the boxer Muhammad Ali in their three fights. Joe Louis, after retirement, admitted that he hated being crowded, and that swarmers like untied/undefeated champ Rocky Marciano would have caused him style problems even in his prime.
The boxer or out-fighter tends to be most successful against a brawler, whose slow speed (both hand and foot) and poor technique makes him an easy target to hit for the faster out-fighter. The out-fighter's main concern is to stay alert, as the brawler only needs to land one good punch to finish the fight. If the out-fighter can avoid those power punches, he can often wear the brawler down with fast jabs, tiring him out. If he is successful enough, he may even apply extra pressure in the later rounds in an attempt to achieve a knockout. Most classic boxers, such as Muhammad Ali, enjoyed their best successes against sluggers.
An example of a style matchup was the historical fight of Julio César Chávez, a swarmer or in-fighter, against Meldrick Taylor, the boxer or out-fighter (see Julio César Chávez vs. Meldrick Taylor). The match was nicknamed "Thunder Meets Lightning" as an allusion to punching power of Chávez and blinding speed of Taylor. Chávez was the epitome of the "Mexican" style of boxing. Taylor's hand and foot speed and boxing abilities gave him the early advantage, allowing him to begin building a large lead on points. Chávez remained relentless in his pursuit of Taylor and due to his greater punching power Chávez slowly punished Taylor. Coming into the later rounds, Taylor was bleeding from the mouth, his entire face was swollen, the bones around his eye socket had been broken, he had swallowed a considerable amount of his own blood, and as he grew tired, Taylor was increasingly forced into exchanging blows with Chávez, which only gave Chávez a greater chance to cause damage. While there was little doubt that Taylor had solidly won the first three quarters of the fight, the question at hand was whether he would survive the final quarter. Going into the final round, Taylor held a secure lead on the scorecards of two of the three judges. Chávez would have to knock Taylor out to claim a victory, whereas Taylor merely needed to stay away from the Mexican legend. However, Taylor did not stay away, but continued to trade blows with Chávez. As he did so, Taylor showed signs of extreme exhaustion, and every tick of the clock brought Taylor closer to victory unless Chávez could knock him out.
With about a minute left in the round, Chávez hit Taylor squarely with several hard punches and stayed on the attack, continuing to hit Taylor with well-placed shots. Finally, with about 25 seconds to go, Chávez landed a hard right hand that caused Taylor to stagger forward towards a corner, forcing Chávez back ahead of him. Suddenly Chávez stepped around Taylor, positioning him so that Taylor was trapped in the corner, with no way to escape from Chávez' desperate final flurry. Chávez then nailed Taylor with a tremendous right hand that dropped the younger man. By using the ring ropes to pull himself up, Taylor managed to return to his feet and was given the mandatory 8-count. Referee Richard Steele asked Taylor twice if he was able to continue fighting, but Taylor failed to answer. Steele then concluded that Taylor was unfit to continue and signaled that he was ending the fight, resulting in a TKO victory for Chávez with only two seconds to go in the bout.
Equipment.
Since boxing involves forceful, repetitive punching, precautions must be taken to prevent damage to bones in the hand. Most trainers do not allow boxers to train and spar without wrist wraps and boxing gloves. Hand wraps are used to secure the bones in the hand, and the gloves are used to protect the hands from blunt injury, allowing boxers to throw punches with more force than if they did not utilize them. Gloves have been required in competition since the late nineteenth century, though modern boxing gloves are much heavier than those worn by early twentieth-century fighters. Prior to a bout, both boxers agree upon the weight of gloves to be used in the bout, with the understanding that lighter gloves allow heavy punchers to inflict more damage. The brand of gloves can also affect the impact of punches, so this too is usually stipulated before a bout.
A mouth guard is important to protect the teeth and gums from injury, and to cushion the jaw, resulting in a decreased chance of knockout. Both fighters must wear soft soled shoes to reduce the damage from accidental (or intentional) stepping on feet. While older boxing boots more commonly resembled those of a professional wrestler, modern boxing shoes and boots tend to be quite similar to their amateur wrestling counterparts.
Boxers practice their skills on two basic types of punching bags. A small, tear-drop-shaped "speed bag" is used to hone reflexes and repetitive punching skills, while a large cylindrical "heavy bag" filled with sand, a synthetic substitute, or water is used to practice power punching and body blows. In addition to these distinctive pieces of equipment, boxers also utilize sport-nonspecific training equipment to build strength, speed, agility, and stamina. Common training equipment includes free weights, rowing machines, jump rope, and medicine balls.
Boxing matches typically take place in a boxing ring, a raised platform surrounded by ropes attached to posts rising in each corner. The term "ring" has come to be used as a metaphor for many aspects of prize fighting in general.
Technique.
Stance.
The modern boxing stance differs substantially from the typical boxing stances of the 19th and early 20th centuries. The modern stance has a more upright vertical-armed guard, as opposed to the more horizontal, knuckles-facing-forward guard adopted by early 20th century hook users such as Jack Johnson.
In a fully upright stance, the boxer stands with the legs shoulder-width apart and the rear foot a half-step in front of the lead man. Right-handed or orthodox boxers lead with the left foot and fist (for most penetration power). Both feet are parallel, and the right heel is off the ground. The lead (left) fist is held vertically about six inches in front of the face at eye level. The rear (right) fist is held beside the chin and the elbow tucked against the ribcage to protect the body. The chin is tucked into the chest to avoid punches to the jaw which commonly cause knock-outs and is often kept slightly offcenter. Wrists are slightly bent to avoid damage when punching and the elbows are kept tucked in to protect the ribcage. Some boxers fight from a crouch, leaning forward and keeping their feet closer together. The stance described is considered the "textbook" stance and fighters are encouraged to change it around once it's been mastered as a base. Case in point, many fast fighters have their hands down and have almost exaggerated footwork, while brawlers or bully fighters tend to slowly stalk their opponents.
Left-handed or southpaw fighters use a mirror image of the orthodox stance, which can create problems for orthodox fighters unaccustomed to receiving jabs, hooks, or crosses from the opposite side. The southpaw stance, conversely, is vulnerable to a straight right hand.
North American fighters tend to favor a more balanced stance, facing the opponent almost squarely, while many European fighters stand with their torso turned more to the side. The positioning of the hands may also vary, as some fighters prefer to have both hands raised in front of the face, risking exposure to body shots.
Modern boxers can sometimes be seen tapping their cheeks or foreheads with their fists in order to remind themselves to keep their hands up (which becomes difficult during long bouts). Boxers are taught to push off with their feet in order to move effectively. Forward motion involves lifting the lead leg and pushing with the rear leg. Rearward motion involves lifting the rear leg and pushing with the lead leg. During lateral motion the leg in the direction of the movement moves first while the opposite leg provides the force needed to move the body.
Punches.
There are four basic punches in boxing: the jab, cross, hook and uppercut. Any punch other than a jab is considered a power punch. If a boxer is right-handed (orthodox), his left hand is the lead hand and his right hand is the rear hand. For a left-handed boxer or southpaw, the hand positions are reversed. For clarity, the following discussion will assume a right-handed boxer.
These different punch types can be thrown in rapid succession to form combinations or "combos". The most common is the jab and cross combination, nicknamed the "one-two combo". This is usually an effective combination, because the jab blocks the opponent's view of the cross, making it easier to land cleanly and forcefully.
A large, swinging circular punch starting from a cocked-back position with the arm at a longer extension than the hook and all of the fighter's weight behind it is sometimes referred to as a "roundhouse", "haymaker", or sucker-punch. Relying on body weight and centripetal force within a wide arc, the roundhouse can be a powerful blow, but it is often a wild and uncontrolled punch that leaves the fighter delivering it off balance and with an open guard.
Wide, looping punches have the further disadvantage of taking more time to deliver, giving the opponent ample warning to react and counter. For this reason, the haymaker or roundhouse is not a conventional punch, and is regarded by trainers as a mark of poor technique or desperation. Sometimes it has been used, because of its immense potential power, to finish off an already staggering opponent who seems unable or unlikely to take advantage of the poor position it leaves the puncher in.
Another unconventional punch is the rarely used bolo punch, in which the opponent swings an arm out several times in a wide arc, usually as a distraction, before delivering with either that or the other arm.
An illegal punch to the back of the head or neck is known as a rabbit punch.
Defense.
There are several basic maneuvers a boxer can use in order to evade or block punches, depicted and discussed below.
Philly Shell or Shoulder roll defense -This is actually a variation of the cross-arm defense. The lead arm (left for an orthodox fighter and right for a southpaw) is placed across the torso usually somewhere in between the belly button and chest and the lead hand rests on the opposite side of the fighter's torso. The back hand is placed on the side of the face (right side for orthodox fighters and left side for southpaws). The lead shoulder is brought in tight against the side of the face (left side for orthodox fighters and right side for southpaws). This style is used by fighters who like to counterpunch.
To execute this guard a fighter must be very athletic and experienced. This style is so effective for counterpunching because it allows fighters to slip punches by rotating and dipping their upper body and causing blows to glance off the fighter. After the punch glances off, the fighter's back hand is in perfect position to hit their out-of-position opponent. The shoulder lean is used in this stance. To execute the shoulder lean a fighter rotates and ducks (to the right for orthodox fighters and to the left for southpaws) when their opponents punch is coming towards them and then rotates back towards their opponent while their opponent is bringing their hand back.
The fighter will throw a punch with their back hand as they are rotating towards their undefended opponent. The weakness to this style is that when a fighter is stationary and not rotating they are open to be hit so a fighter must be athletic and well conditioned to effectively execute this style. To beat this style, fighters like to jab their opponents shoulder causing the shoulder and arm to be in pain and to demobilize that arm. Fighters that used this defense include Sugar Ray Robinson, Ken Norton (also used this defense), Pernell Whitaker, James Toney, and Floyd Mayweather Jr.
Less common strategies.
Floyd Mayweather, Jr. employed the use of a check hook against Ricky Hatton, which sent Hatton flying head first into the corner post and being knocked down. Hatton managed to get himself to his feet after the knockdown but was clearly dazed and it was only a matter of moments before Mayweather landed a flurry of punches which sent Hatton crashing to the canvas, giving Mayweather a TKO victory in the 10th round and handing Hatton his first defeat.
Ring corner.
In boxing, each fighter is given a corner of the ring where he rests in between rounds and where his trainers stand. Typically, three men stand in the corner besides the boxer himself; these are the trainer, the assistant trainer and the cutman. The trainer and assistant typically give advice to the boxer on what he is doing wrong as well as encouraging him if he is losing. The cutman is a cutaneous doctor responsible for keeping the boxer's face and eyes free of cuts and blood. This is of particular importance because many fights are stopped because of cuts that threaten the boxer's eyes.
In addition, the corner is responsible for stopping the fight if they feel their fighter is in grave danger of permanent injury. The corner will occasionally throw in a white towel to signify a boxer's surrender (the idiomatic phrase "to throw in the towel", meaning to give up, derives from this practice). This can be seen in the fight between Diego Corrales and Floyd Mayweather. In that fight, Corrales' corner surrendered despite Corrales' steadfast refusal.
Medical concerns.
Knocking a person unconscious or even causing concussion may cause permanent brain damage. There is no clear division between the force required to knock a person out and the force likely to kill a person. Since 1980, more than 200 amateur boxers, professional boxers and Toughman fighters have died due to ring or training injuries. In 1983, the "Journal of the American Medical Association" called for a ban on boxing. The editor, Dr. George Lundberg, called boxing an "obscenity" that "should not be sanctioned by any civilized society." Since then, the British, Canadian and Australian Medical Associations also have called for bans on boxing.
Supporters of the ban state that boxing is the only sport where hurting the other athlete is the goal. Dr. Bill O'Neill, boxing spokesman for the British Medical Association, has supported the BMA's proposed ban on boxing: "It is the only sport where the intention is to inflict serious injury on your opponent, and we feel that we must have a total ban on boxing." Opponents respond that such a position is misguided opinion, stating that amateur boxing is scored solely according to total connecting blows with no award for "injury". They observe that many skilled professional boxers have had rewarding careers without inflicting injury on opponents by accumulating scoring blows and avoiding punches winning rounds scored 10-9 by the 10-point must system, and they note that there are many other sports where concussions are much more prevalent. In 2007, one study of amateur boxers showed that protective headgear did not prevent brain damage, and another found that amateur boxers faced a high risk of brain damage. The Gothenburg study analyzed temporary levels of neurofiliment light in cerebral spinal fluid which they conclude is evidence of damage, even though the levels soon subside. More comprehensive studies of neurologiocal function on larger samples performed by Johns Hopkins University and accident rates analyzed by National Safety Council show amateur boxing is a comparatively safe sport.
In 1997, the American Association of Professional Ringside Physicians was established to create medical protocols through research and education to prevent injuries in boxing.
Professional boxing is forbidden in Norway, Iceland, Iran and North Korea. It was banned in Sweden until 2007 when the ban was lifted but strict restrictions, including four three-minute rounds for fights, were imposed. It was banned in Albania from 1965 till the fall of Communism in 1991; it is now legal.
Boxing Hall of Fame.
The sport of boxing has two internationally recognized boxing halls of fame; the International Boxing Hall of Fame (IBHOF) and the World Boxing Hall of Fame (WBHF), with the IBHOF being the more widely recognized boxing hall of fame.
The WBHF was founded by Everett L. Sanders in 1980. Since its inception the WBHOF has never had a permanent location or museum, which has allowed the more recent IBHOF to garner more publicity and prestige. Among the notable names in the WBHF are Ricardo "Finito" Lopez, Gabriel "Flash" Elorde, Michael Carbajal, Khaosai Galaxy, Henry Armstrong, Jack Johnson, Roberto Durán, George Foreman, Ceferino Garcia and Salvador Sanchez. Boxing's International Hall of Fame was inspired by a tribute an American town held for two local heroes in 1982. The town, Canastota, New York, (which is about east of Syracuse, via the New York State Thruway), honored former world welterweight/middleweight champion Carmen Basilio and his nephew, former world welterweight champion Billy Backus. The people of Canastota raised money for the tribute which inspired the idea of creating an official, annual hall of fame for notable boxers.
The International Boxing Hall of Fame opened in Canastota in 1989. The first inductees in 1990 included Jack Johnson, Benny Leonard, Jack Dempsey, Henry Armstrong, Sugar Ray Robinson, Archie Moore, and Muhammad Ali. Other world-class figures include Salvador Sanchez, Fabio Martella, Roberto "Manos de Piedra" Durán, Ricardo Lopez, Gabriel "Flash" Elorde, Vicente Saldivar, Ismael Laguna, Eusebio Pedroza, Carlos Monzón, Azumah Nelson, Rocky Marciano, Pipino Cuevas and Ken Buchanan. The Hall of Fame's induction ceremony is held every June as part of a four-day event.
The fans who come to Canastota for the Induction Weekend are treated to a number of events, including scheduled autograph sessions, boxing exhibitions, a parade featuring past and present inductees, and the induction ceremony itself.
Boxer rankings.
There are various organizations and websites, that rank boxers in both weight class and pound-for-pound manner.

</doc>
<doc id="4246" url="http://en.wikipedia.org/wiki?curid=4246" title="Bollywood">
Bollywood

Bollywood is the Hindi-language film industry based in Mumbai (Bombay), Maharashtra, India. The term is often incorrectly used to refer to the whole of Indian cinema; however, it is only a part of the large Indian film industry, which includes other production centres producing films in multiple languages. Bollywood is one of the largest film producers in India and one of the largest centres of film production in the world.
Bollywood is more formally referred to as Hindi cinema. 
Etymology.
The name "Bollywood" is a portmanteau derived from Bombay (the former name for Mumbai) and Hollywood, the center of the American film industry. However, unlike Hollywood, Bollywood does not exist as a physical place. Though some deplore the name, arguing that it makes the industry look like a poor cousin to Hollywood, it has its own entry in the "Oxford English Dictionary".
The naming scheme for "Bollywood" was inspired by "Tollywood", the name that was used to refer to the cinema of West Bengal. Dating back to 1932, "Tollywood" was the earliest Hollywood-inspired name, referring to the Bengali film industry based in Tollygunge, Calcutta, whose name is reminiscent of "Hollywood" and was the center of the cinema of India at the time. It was this "chance juxtaposition of two pairs of rhyming syllables," Holly and Tolly, that led to the portmanteau name "Tollywood" being coined. The name "Tollywood" went on to be used as a nickname for the Bengali film industry by the popular Kolkata-based "Junior Statesman" youth magazine, establishing a precedent for other film industries to use similar-sounding names, eventually leading to the term "Bollywood" being coined. However, more popularly, Tollywood is now used to refer to the Telugu Film Industry in Telangana & Andhra Pradesh.
The term "Bollywood" itself has origins in the 1970s, when India overtook America as the world's largest film producer. Credit for the term has been claimed by several different people, including the lyricist, filmmaker and scholar Amit Khanna, and the journalist Bevinda Collaco.
History.
"Raja Harishchandra" (1913), by Dadasaheb Phalke, is known as the first silent feature film made in India. By the 1930s, the industry was producing over 200 films per annum. The first Indian sound film, Ardeshir Irani's "Alam Ara" (1931), was a major commercial success. There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries quickly switched to sound filming.
The 1930s and 1940s were tumultuous times: India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Most Bollywood films were unabashedly escapist, but there were also a number of filmmakers who tackled tough social issues, or used the struggle for Indian independence as a backdrop for their plots.
In 1937, Ardeshir Irani, of "Alam Ara" fame, made the first colour film in Hindi, "Kisan Kanya". The next year, he made another colour film, a version of "Mother India". However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were the staple fare at the cinema.
Golden Age.
Following India's independence, the period from the late 1940s to the 1960s is regarded by film historians as the "Golden Age" of Hindi cinema. Some of the most critically acclaimed Hindi films of all time were produced during this period. Examples include the Guru Dutt films "Pyaasa" (1957) and "Kaagaz Ke Phool" (1959) and the Raj Kapoor films "Awaara" (1951) and "Shree 420" (1955). These films expressed social themes mainly dealing with working-class urban life in India; "Awaara" presented the city as both a nightmare and a dream, while "Pyaasa" critiqued the unreality of city life. Some of the most famous epic films of Hindi cinema were also produced at the time, including Mehboob Khan's "Mother India" (1957), which was nominated for the Academy Award for Best Foreign Language Film, and K. Asif's "Mughal-e-Azam" (1960). "Madhumati" (1958), directed by Bimal Roy and written by Ritwik Ghatak, popularised the theme of reincarnation in Western popular culture. Other acclaimed mainstream Hindi filmmakers at the time included Kamal Amrohi and Vijay Bhatt. Successful actors at the time included Dev Anand, Dilip Kumar, Raj Kapoor and Guru Dutt, while successful actresses included Nargis, Vyjayanthimala, Meena Kumari, Nutan, Madhubala, Waheeda Rehman and Mala Sinha.
While commercial Hindi cinema was thriving, the 1950s also saw the emergence of a new Parallel Cinema movement. Though the movement was mainly led by Bengali cinema, it also began gaining prominence in Hindi cinema. Early examples of Hindi films in this movement include Chetan Anand's "Neecha Nagar" (1946) and Bimal Roy's "Do Bigha Zamin" (1953). Their critical acclaim, as well as the latter's commercial success, paved the way for Indian neorealism and the "Indian New Wave". Some of the internationally acclaimed Hindi filmmakers involved in the movement included Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal and Vijaya Mehta.
Ever since the social realist film "Neecha Nagar" won the Grand Prize at the first Cannes Film Festival, Hindi films were frequently in competition for the Palme d'Or at the Cannes Film Festival throughout the 1950s and early 1960s, with some of them winning major prizes at the festival. Guru Dutt, while overlooked in his own lifetime, had belatedly generated international recognition much later in the 1980s. Dutt is now regarded as one of the greatest Asian filmmakers of all time, alongside the more famous Indian Bengali filmmaker Satyajit Ray. The 2002 "Sight & Sound" critics' and directors' poll of greatest filmmakers ranked Dutt at No. 73 on the list. Some of his films are now included among the greatest films of all time, with "Pyaasa" (1957) being featured in Time magazine's "All-TIME" 100 best movies list, and with both "Pyaasa" and "Kaagaz Ke Phool" (1959) tied at #160 in the 2002 "Sight & Sound" critics' and directors' poll of all-time greatest films. Several other Hindi films from this era were also ranked in the "Sight & Sound" poll, including Raj Kapoor's "Awaara" (1951), Vijay Bhatt's "Baiju Bawra" (1952), Mehboob Khan's "Mother India" (1957) and K. Asif's "Mughal-e-Azam" (1960) all tied at #346 on the list.
Modern cinema.
In the late 1960s and early 1970s, romance movies and action films starred actors like Rajesh Khanna, Dharmendra, Sanjeev Kumar and Shashi Kapoor and actresses like Sharmila Tagore, Mumtaz and Asha Parekh. In the mid-1970s, romantic confections made way for gritty, violent films about gangsters (see Indian mafia) and bandits. Amitabh Bachchan, the star known for his "angry young man" roles, rode the crest of this trend with actors like Mithun Chakraborty, Anil Kapoor and Sunny Deol, which lasted into the early 1990s. Actresses from this era included Hema Malini, Jaya Bachchan and Rekha.
Some Hindi filmmakers such as Shyam Benegal continued to produce realistic Parallel Cinema throughout the 1970s, alongside Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani and Vijaya Mehta. However, the 'art film' bent of the Film Finance Corporation came under criticism during a Committee on Public Undertakings investigation in 1976, which accused the body of not doing enough to encourage commercial cinema. The 1970s thus saw the rise of commercial cinema in the form of enduring films such as "Sholay" (1975), which consolidated Amitabh Bachchan's position as a lead actor. The devotional classic "Jai Santoshi Ma" was also released in 1975. Another important film from 1975 was "Deewar", directed by Yash Chopra and written by Salim-Javed. A crime film pitting "a policeman against his brother, a gang leader based on real-life smuggler Haji Mastan", portrayed by Amitabh Bachchan; it was described as being "absolutely key to Indian cinema" by Danny Boyle. The most internationally acclaimed Hindi film of the 1980s was Mira Nair's "Salaam Bombay!" (1988), which won the Camera d'Or at the 1988 Cannes Film Festival and was nominated for the Academy Award for Best Foreign Language Film.
During the late 1980s and early 1990s, the pendulum swung back toward family-centric romantic musicals with the success of such films as "Qayamat Se Qayamat Tak" (1988), "Maine Pyar Kiya" (1989), "Dil" (1990), "Hum Aapke Hain Kaun" (1994) and "Dilwale Dulhania Le Jayenge" (1995), making stars of a new generation of actors (such as Aamir Khan, Salman Khan and Shahrukh Khan) and actresses (such as Sridevi, Madhuri Dixit, Juhi Chawla and Kajol). In that point of time, action and comedy films were also successful, with actors like Govinda and actresses such as Raveena Tandon and Karisma Kapoor appearing in popular comedy films, and stunt actor Akshay Kumar gaining popularity for performing dangerous stunts in action films in his well known Khiladi (film series) and other action films. Furthermore, this decade marked the entry of new performers in arthouse and independent films, some of which succeeded commercially, the most influential example being "Satya" (1998), directed by Ram Gopal Varma and written by Anurag Kashyap. The critical and commercial success of "Satya" led to the emergence of a distinct genre known as "Mumbai noir", urban films reflecting social problems in the city of Mumbai. This led to a resurgence of Parallel Cinema by the end of the decade. These films often featured actors like Nana Patekar, Manoj Bajpai, Manisha Koirala, Tabu and Urmila Matondkar, whose performances were usually critically acclaimed.
The 2000s saw a growth in Bollywood's popularity across the world. This led the nation's filmmaking to new heights in terms of production values, cinematography and innovative story lines as well as technical advances in areas such as special effects and animation. Some of the largest production houses, among them Yash Raj Films and Dharma Productions were the producers of new modern films. Some popular films of the decade were "Koi... Mil Gaya" (2003), "Kal Ho Naa Ho" (2003), "Veer-Zaara" (2004), "Dhoom" (2004), "Hum Tum" (2004), "Dhoom 2" (2006), "Krrish" (2006), and "Jab We Met" (2007). These films starred established actors. However, the mid-2000s also saw the rise of a new generation of popular actors like Hrithik Roshan, Saif Ali Khan, Shahid Kapoor, and Abhishek Bachchan, as well as a new generation of popular actresses like Rani Mukerji, Preity Zinta, Aishwarya Rai, Kareena Kapoor, and Priyanka Chopra.
In the early 2010s, established actors like Salman Khan and Akshay Kumar became known for making big-budget "masala" entertainers like "Dabangg" and "Rowdy Rathore" opposite younger actresses like Sonakshi Sinha. These films were often not the subject of critical acclaim, but were nonetheless major commercial successes. While most stars from the 2000s continued their successful careers into the next decade, the 2010s also saw the rise of a new generation of actors like Ranbir Kapoor, Imran Khan, Ranveer Singh, and Arjun Kapoor, as well as actresses like Vidya Balan, Katrina Kaif, Deepika Padukone, Anushka Sharma, and Parineeti Chopra.
The Hindi film industry has preferred films that appeal to all segments of the audience (see the discussion in Ganti, 2004, cited in references), and has resisted making films that target narrower audiences. It was believed that aiming for a broad spectrum would maximise box office receipts. However, filmmakers may be moving towards accepting some box-office segmentation, between films that appeal to rural Indians, and films that appeal to urban and international audiences.
Influences for Bollywood.
Gokulsing and Dissanayake identify six major influences that have shaped the conventions of Indian popular cinema:
Influence of Bollywood.
Perhaps the biggest influence of Bollywood has been on nationalism in India itself, where along with rest of Indian cinema, it has become part and parcel of the 'Indian story'. In the words of the economist and Bollywood biographer Lord Meghnad Desai,
Cinema actually has been the most vibrant medium for telling India its own story, the story of its struggle for independence, its constant struggle to achieve national integration and to emerge as a global presence.
In the 2000s, Bollywood began influencing musical films in the Western world, and played a particularly instrumental role in the revival of the American musical film genre. Baz Luhrmann stated that his musical film "Moulin Rouge!" (2001) was directly inspired by Bollywood musicals. The film incorporated an Indian-themed play based on the ancient Sanskrit drama "Mṛcchakatika" and a Bollywood-style dance sequence with a song from the film "China Gate". The critical and financial success of "Moulin Rouge!" renewed interest in the then-moribund Western musical genre, and subsequently films such as "Chicago, The Producers, Rent", "Dreamgirls", "Hairspray", "", "Across the Universe", "The Phantom of the Opera", "Enchanted" and "Mamma Mia!" were produced, fuelling a renaissance of the genre.
A. R. Rahman, an Indian film composer, wrote the music for Andrew Lloyd Webber's "Bombay Dreams", and a musical version of "Hum Aapke Hain Koun" has played in London's West End. The Bollywood musical "Lagaan" (2001) was nominated for the Academy Award for Best Foreign Language Film, and two other Bollywood films "Devdas" (2002) and "Rang De Basanti" (2006) were nominated for the BAFTA Award for Best Foreign Language Film. Danny Boyle's "Slumdog Millionaire" (2008), which has won four Golden Globes and eight Academy Awards, was also directly inspired by Bollywood films, and is considered to be a "homage to Hindi commercial cinema". The theme of reincarnation was also popularised in Western popular culture through Bollywood films, with "Madhumati" (1958) inspiring the Hollywood film "The Reincarnation of Peter Proud" (1975), which in turn inspired the Bollywood film "Karz" (1980), which in turn influenced another Hollywood film "Chances Are" (1989). The 1975 film "Chhoti Si Baat" is believed to have inspired "Hitch" (2005), which in turn inspired the Bollywood film "Partner" (2007).
The influence of Bollywood "filmi" music can also be seen in popular music elsewhere in the world. In 1978, technopop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced an electronic album "Cochin Moon" based on an experimental fusion between electronic music and Bollywood-inspired Indian music. Devo's 1988 hit song "Disco Dancer" was inspired by the song "I am a Disco Dancer" from the Bollywood film "Disco Dancer" (1982). The 2002 song "Addictive", sung by Truth Hurts and produced by DJ Quik and Dr. Dre, was lifted from Lata Mangeshkar's "Thoda Resham Lagta Hai" from "Jyoti" (1981). The Black Eyed Peas' Grammy Award winning 2005 song "Don't Phunk with My Heart" was inspired by two 1970s Bollywood songs: "Ye Mera Dil Yaar Ka Diwana" from "Don" (1978) and "Ae Nujawan Hai Sub" from "Apradh" (1972). Both songs were originally composed by Kalyanji Anandji, sung by Asha Bhosle, and featured the dancer Helen. Also in 2005, the Kronos Quartet re-recorded several R. D. Burman compositions, with Asha Bhosle as the singer, into an album "You've Stolen My Heart: Songs from R.D. Burman's Bollywood", which was nominated for "Best Contemporary World Music Album" at the 2006 Grammy Awards. "Filmi" music composed by A. R. Rahman (who would later win two Academy Awards for the "Slumdog Millionaire" soundtrack) has frequently been sampled by musicians elsewhere in the world, including the Singaporean artist Kelly Poon, the Uzbek artist Iroda Dilroz, the French rap group La Caution, the American artist Ciara, and the German band Löwenherz, among others. Many Asian Underground artists, particularly those among the overseas Indian diaspora, have also been inspired by Bollywood music.
Genre conventions.
Bollywood films are mostly musicals and are expected to contain catchy music in the form of song-and-dance numbers woven into the script. A film's success often depends on the quality of such musical numbers. Indeed, a film's music is often released before the movie and helps increase the audience.
Indian audiences expect full value for their money, with a good entertainer generally referred to as "paisa" "vasool", (literally, "money's worth"). Songs and dances, love triangles, comedy and dare-devil thrills are all mixed up in a three-hour extravaganza with an intermission. They are called "masala" films, after the Hindi word for a spice mixture. Like "masalas", these movies are a mixture of many things such as action, comedy, romance and so on. Most films have heroes who are able to fight off villains all by themselves.
Bollywood plots have tended to be melodramatic. They frequently employ formulaic ingredients such as star-crossed lovers and angry parents, love triangles, family ties, sacrifice, corrupt politicians, kidnappers, conniving villains, courtesans with hearts of gold, long-lost relatives and siblings separated by fate, dramatic reversals of fortune, and convenient coincidences.
There have always been Indian films with more artistic aims and more sophisticated stories, both inside and outside the Bollywood tradition (see Parallel Cinema). They often lost out at the box office to movies with more mass appeal. Bollywood conventions are changing, however. A large Indian diaspora in English-speaking countries, and increased Western influence at home, have nudged Bollywood films closer to Hollywood models.
Film critic Lata Khubchandani writes, "our earliest films ... had liberal doses of sex and kissing scenes in them. Strangely, it was after Independence the censor board came into being and so did all the strictures." Plots now tend to feature Westernised urbanites dating and dancing in clubs rather than centring on pre-arranged marriages. Though these changes can widely be seen in contemporary Bollywood, traditional conservative ways of Indian culture continue to exist in India outside the industry and an element of resistance by some to western-based influences. Despite this, Bollywood continues to play a major role in fashion in India. Some studies into fashion in India have revealed that some people are unaware that the changing nature of fashion in Bollywood films are often influenced by globalisation; many consider the clothes worn by Bollywood actors as authentically Indian.
Cast and crew.
Bollywood employs people from all parts of India. It attracts thousands of aspiring actors and actresses, all hoping for a break in the industry. Models and beauty contestants, television actors, theatre actors and even common people come to Mumbai with the hope and dream of becoming a star. Just as in Hollywood, very few succeed. Since many Bollywood films are shot abroad, many foreign extras are employed too.
Very few non-Indian actors are able to make a mark in Bollywood, though many have tried from time to time. There have been some exceptions, of which one recent example is the hit film "Rang De Basanti", where the lead actress is Alice Patten, an Englishwoman. "Kisna", "Lagaan", and "" also featured foreign actors. Of late, Emma Brown Garett, an Australian born actress, has starred in a few Indian films.
Bollywood can be very clannish, and the relatives of film-industry insiders have an edge in getting coveted roles in films or being part of a film's crew. However, industry connections are no guarantee of a long career: competition is fierce and if film industry scions do not succeed at the box office, their careers will falter. Some of the biggest stars, such as Rajesh Khanna, Dharmendra, Amitabh Bachchan, Shahrukh Khan and Akshay Kumar have succeeded despite a lack of any show business connections. For film clans, see List of Hindi film clans.
Sound.
Sound in Bollywood films was once rarely recorded on location (otherwise known as sync sound). Therefore, the sound was usually created (or re-created) entirely in the studio, with the actors reciting their lines as their images appear on-screen in the studio in the process known as "looping in the sound" or ADR—with the foley and sound effects added later. This created several problems, since the sound in these films usually occurs a frame or two earlier or later than the mouth movements or gestures. The actors had to act twice: once on-location, once in the studio—and the emotional level on set is often very difficult to re-create. Commercial Indian films, not just the Hindi-language variety, are known for their lack of ambient sound, so there is a silence underlying everything instead of the background sound and noises usually employed in films to create aurally perceivable depth and environment.
The ubiquity of ADR in Bollywood cinema became prevalent in the early 1960s with the arrival of the Arriflex 3 camera, which required a blimp (cover) in order to shield the sound of the camera, for which it was notorious, from on-location filming. Commercial Indian filmmakers, known for their speed, never bothered to blimp the camera, and its excessive noise required that everything had to be re-created in the studio. Eventually, this became the standard for Indian films.
The trend was bucked in 2001, after a 30-year hiatus of synchronised sound, with the film "Lagaan", in which producer-star Aamir Khan insisted that the sound be done on location. This opened up a heated debate on the use and economic feasibility of on-location sound, and several Bollywood films have employed on-location sound since then.
Bollywood song and dance.
Bollywood film music is called filmi music (from Hindi, meaning "of films"). Songs from Bollywood movies are generally pre-recorded by professional playback singers, with the actors then lip synching the words to the song on-screen, often while dancing. While most actors, especially today, are excellent dancers, few are also singers. One notable exception was Kishore Kumar, who starred in several major films in the 1950s while also having a stellar career as a playback singer. K. L. Saigal, Suraiyya, and Noor Jehan were also known as both singers and actors. Some actors in the last thirty years have sung one or more songs themselves; for a list, see Singing actors and actresses in Indian cinema.
Playback singers are prominently featured in the opening credits and have their own fans who will go to an otherwise lackluster movie just to hear their favourites. Going by the quality as well as the quantity of the songs they rendered, most notable singers of Bollywood are Lata Mangeshkar, Asha Bhosle, Geeta Dutt, Shamshad Begum, Kavita Krishnamurthy, Sadhana Sargam and Alka Yagnik among female playback singers; and K. L. Saigal, Talat Mahmood, Mukesh, Mohammed Rafi, Manna Dey, Hemant Kumar, Kishore Kumar, Kumar Sanu, Udit Narayan and Sonu Nigam among male playback singers. Kishore Kumar and Mohammed Rafi are often considered arguably the finest of the singers that have lent their voice to Bollywood songs, followed by Lata Mangeshkar, who, through the course of a career spanning over six decades, has recorded thousands of songs for Indian movies. The composers of film music, known as music directors, are also well-known. Their songs can make or break a film and usually do. Remixing of film songs with modern beats and rhythms is a common occurrence today, and producers may even release remixed versions of some of their films' songs along with the films' regular soundtrack albums.
The dancing in Bollywood films, especially older ones, is primarily modelled on Indian dance: classical dance styles, dances of historic northern Indian courtesans (tawaif), or folk dances. In modern films, Indian dance elements often blend with Western dance styles (as seen on MTV or in Broadway musicals), though it is usual to see Western pop "and" pure classical dance numbers side by side in the same film. The hero or heroine will often perform with a troupe of supporting dancers. Many song-and-dance routines in Indian films feature unrealistically instantaneous shifts of location or changes of costume between verses of a song. If the hero and heroine dance and sing a duet, it is often staged in beautiful natural surroundings or architecturally grand settings. This staging is referred to as a "picturisation".
Songs typically comment on the action taking place in the movie, in several ways. Sometimes, a song is worked into the plot, so that a character has a reason to sing. Other times, a song is an externalisation of a character's thoughts, or presages an event that has not occurred yet in the plot of the movie. In this case, the event is often two characters falling in love. The songs are also often referred to as a "dream sequence", and anything can happen that would not normally happen in the real world.
Previously song and dance scenes often used to be shot in Kashmir, but due to political unrest in Kashmir since the end of the 1980s, those scenes have since then often been shot in Western Europe, particularly in Switzerland and Austria.
Bollywood films have always used what are now called "item numbers". A physically attractive female character (the "item girl"), often completely unrelated to the main cast and plot of the film, performs a catchy song and dance number in the film. In older films, the "item number" may be performed by a courtesan ("tawaif") dancing for a rich client or as part of a cabaret show. The actress Helen was famous for her cabaret numbers. In modern films, item numbers may be inserted as discotheque sequences, dancing at celebrations, or as stage shows.
For the last few decades Bollywood producers have been releasing the film's soundtrack, as tapes or CDs, before the main movie release, hoping that the music will pull audiences into the cinema later. Often the soundtrack is more popular than the movie. In the last few years some producers have also been releasing music videos, usually featuring a song from the film. However, some promotional videos feature a song which is not included in the movie.
Dialogues and lyrics.
The film script or lines of dialogue (called "dialogues" in Indian English) and the song lyrics are often written by different people.
Dialogues are usually written in an unadorned Hindi that would be understood by the largest possible audience. Some movies, however, have used regional dialects to evoke a village setting, or old-fashioned, courtly, Persian-influenced Urdu in Mughal era historical films. Jyotika Virdi, in her book "The cinematic imagiNation" , wrote about the presence of Urdu in Hindi films: "Urdu is often used in film titles, screenplay, lyrics, the language of love, war, and martyrdom." However, she further discussed its decline over the years: "The extent of Urdu used in commercial Hindi cinema has not been stable ... the decline of Urdu is mirrored in Hindi films ... It is true that many Urdu words have survived and have become part of Hindi cinema's popular vocabulary. But that is as far as it goes." Contemporary mainstream movies also make great use of English. According to "Bollywood Audiences Editorial", "English has begun to challenge the ideological work done by Urdu." Some movie scripts are first written in Latin script. Characters may shift from one language to the other to express a certain atmosphere (for example, English in a business setting and Hindi in an informal one).
Cinematic language, whether in dialogues or lyrics, is often melodramatic and invokes God, family, mother, duty, and self-sacrifice liberally. Song lyrics are often about love. Bollywood song lyrics, especially in the old movies, frequently use the poetic vocabulary of court Urdu, with many Persian loanwords. Another source for love lyrics is the long Hindu tradition of poetry about the amours of Krishna, Radha, and the gopis, as referenced in films such as "Jhanak Jhanak Payal Baje" and "Lagaan".
Music directors often prefer working with certain lyricists, to the point that the lyricist and composer are seen as a team. This phenomenon is compared to the pairings of American composers and songwriters that created old-time Broadway musicals.
Finances.
Bollywood films are multi-million dollar productions, with the most expensive productions costing up to 1 billion rupees (roughly USD 20 million). The latest Science fiction movie "Ra.One" was made at an immense budget of 135 crores (roughly USD 27 million), making it the most expensive movie ever produced in Bollywood. Sets, costumes, special effects, and cinematography were less than world-class up until the mid-to-late 1990s, although with some notable exceptions. As Western films and television gain wider distribution in India itself, there is an increasing pressure for Bollywood films to attain the same production levels, particularly in areas such as action and special effects. Recent Bollywood films have employed international technicians to improve in these areas, such as "Krrish" (2006) which has action choreographed by Hong Kong based Tony Ching. The increasing accessibility to professional action and special effects, coupled with rising film budgets, has seen an explosion in the action and sci-fi genres.
Sequences shot overseas have proved a real box office draw, so Mumbai film crews are increasingly filming in Australia, Canada, New Zealand, the United Kingdom, the United States, continental Europe and elsewhere. Nowadays, Indian producers are winning more and more funding for big-budget films shot within India as well, such as "Lagaan", "Devdas" and other recent films.
Funding for Bollywood films often comes from private distributors and a few large studios. Indian banks and financial institutions were forbidden from lending money to movie studios. However, this ban has now been lifted. As finances are not regulated, some funding also comes from illegitimate sources, such as the Mumbai underworld. The Mumbai underworld has been known to be involved in the production of several films, and are notorious for patronising several prominent film personalities. On occasion, they have been known to use money and muscle power to get their way in cinematic deals. In January 2000, Mumbai mafia hitmen shot Rakesh Roshan, a film director and father of star Hrithik Roshan. In 2001, the Central Bureau of Investigation seized all prints of the movie "Chori Chori Chupke Chupke" after the movie was found to be funded by members of the Mumbai underworld.
Another problem facing Bollywood is widespread copyright infringement of its films. Often, bootleg DVD copies of movies are available before the prints are officially released in cinemas. Manufacturing of bootleg DVD, VCD, and VHS copies of the latest movie titles is a well established 'small scale industry' in parts of South Asia and South East Asia. The Federation of Indian Chambers of Commerce and Industry (FICCI) estimates that the Bollywood industry loses $100 million annually in loss of revenue from pirated home videos and DVDs. Besides catering to the homegrown market, demand for these copies is large amongst some sections of the Indian diaspora, too. (In fact, bootleg copies are the only way people in Pakistan can watch Bollywood movies, since the Government of Pakistan has banned their sale, distribution and telecast). Films are frequently broadcast without compensation by countless small cable TV companies in India and other parts of South Asia. Small convenience stores run by members of the Indian diaspora in the US and the UK regularly stock tapes and DVDs of dubious provenance, while consumer copying adds to the problem. The availability of illegal copies of movies on the Internet also contributes to the piracy problem.
Satellite TV, television and imported foreign films are making huge inroads into the domestic Indian entertainment market. In the past, most Bollywood films could make money; now fewer tend to do so. However, most Bollywood producers make money, recouping their investments from many sources of revenue, including selling ancillary rights. There are also increasing returns from theatres in Western countries like the United Kingdom, Canada, and the United States, where Bollywood is slowly getting noticed. As more Indians migrate to these countries, they form a growing market for upscale Indian films.
For a comparison of Hollywood and Bollywood financial figures, see . It shows tickets sold in 2002 and total revenue estimates. Bollywood sold 3.6 billion tickets and had total revenues (theatre tickets, DVDs, television and so on.) of US$1.3 billion, whereas Hollywood films sold 2.6 billion tickets and generated total revenues (again from all formats) of US$51 billion.
Advertising.
Many Indian artists used to make a living by hand-painting movie billboards and posters (The well-known artist M.F. Hussain used to paint film posters early in his career). This was because human labour was found to be cheaper than printing and distributing publicity material. Now, a majority of the huge and ubiquitous billboards in India's major cities are created with computer-printed vinyl. The old hand-painted posters, once regarded as ephemera, are becoming increasingly collectible as folk art.
Releasing the film music, or music videos, before the actual release of the film can also be considered a form of advertising. A popular tune is believed to help pull audiences into the theatres.
Bollywood publicists have begun to use the Internet as a venue for advertising. Most of the better-funded film releases now have their own websites, where browsers can view trailers, stills, and information about the story, cast, and crew.
Bollywood is also used to advertise other products. Product placement, as used in Hollywood, is widely practised in Bollywood.
Bollywood movie stars appear in print and television advertisements for other products, such as watches or soap (see Celebrity endorsement). Advertisers say that a star endorsement boosts sales.
Awards.
The Filmfare Awards ceremony is one of the most prominent film events given for Hindi films in India. The Indian screen magazine "Filmfare" started the first Filmfare Awards in 1954, and awards were given to the best films of 1953. The ceremony was referred to as the "Clare Awards" after the magazine's editor. Modelled after the poll-based merit format of the Academy of Motion Picture Arts and Sciences, individuals may submit their votes in separate categories. A dual voting system was developed in 1956. Like the Oscars, the Filmfare awards are frequently accused of bias towards commercial success rather than artistic merit.
As the Filmfare, the National Film Awards were introduced in 1954. Since 1973, the Indian government has sponsored the National Film Awards, awarded by the government run Directorate of Film Festivals (DFF). The DFF screens not only Bollywood films, but films from all the other regional movie industries and independent/art films. These awards are handed out at an annual ceremony presided over by the President of India. Under this system, in contrast to the National Film Awards, which are decided by a panel appointed by Indian Government, the Filmfare Awards are voted for by both the public and a committee of experts.
Additional ceremonies held within India are:
Ceremonies held overseas are:
Most of these award ceremonies are lavishly staged spectacles, featuring singing, dancing, and numerous celebrities.
Popularity and appeal.
Besides being popular among the India diaspora, such far off locations as Nigeria to Egypt to Senegal and to Russia generations of non-Indian fans have grown up with Bollywood during the years, bearing witness to the cross-cultural appeal of Indian movies. Over the last years of the twentieth century and beyond, Bollywood progressed in its popularity as it entered the consciousness of Western audiences and producers, with Western actors now actively seeking roles in Bollywood movies.
Africa.
Historically, Hindi films have been distributed to some parts of Africa, largely by Lebanese businessmen. "Mother India" (1957), for example, continued to be played in Nigeria decades after its release. Indian movies have also gained ground so as to alter the style of Hausa fashions, songs have also been copied by Hausa singers and stories have influenced the writings of Nigerian novelists. Stickers of Indian films and stars decorate taxis and buses in Northern Nigeria, while posters of Indian films adorn the walls of tailor shops and mechanics' garages in the country. Unlike in Europe and North America where Indian films largely cater to the expatriate Indian market yearning to keep in touch with their homeland, in West Africa, as in many other parts of the world, such movies rose in popularity despite the lack of a significant Indian audience, where movies are about an alien culture, based on a religion wholly different, and, for the most part, a language that is unintelligible to the viewers. One such explanation for this lies in the similarities between the two cultures. Other similarities include wearing turbans; the presence of animals in markets; porters carrying large bundles, chewing sugar cane; youths riding Bajaj motor scooters; wedding celebrations, and so forth. With the strict Muslim culture, Indian movies were said to show "respect" toward women, where Hollywood movies were seen to have "no shame". In Indian movies women were modestly dressed, men and women rarely kiss, and there is no nudity, thus Indian movies are said to "have culture" that Hollywood films lack. The latter choice was a failure because "they don't base themselves on the problems of the people," where the former is based socialist values and on the reality of developing countries emerging from years of colonialism. Indian movies also allowed for a new youth culture to follow without such ideological baggage as "becoming western."
Several Bollywood personalities have avenued to the continent for both shooting movies and off-camera projects. The film "Padmashree Laloo Prasad Yadav" (2005) was one of many movies shot in South Africa. "Dil Jo Bhi Kahey" (2005) was shot almost entirely in Mauritius, which has a large ethnically Indian population.
Ominously, however, the popularity of old Bollywood versus a new, changing Bollywood seems to be diminishing the popularity on the continent. The changing style of Bollywood has begun to question such an acceptance. The new era features more sexually explicit and violent films. Nigerian viewers, for example, commented that older films of the 1950s and 1960s had culture to the newer, more westernised picturizations. The old days of India avidly "advocating decolonization ... and India's policy was wholly influenced by his missionary zeal to end racial domination and discrimination in the African territories" were replaced by newer realities. The emergence of Nollywood, Africa's local movie industry has also contributed to the declining popularity of Bollywood films. A greater globalised world worked in tandem with the sexualisation of Indian films so as to become more like American films, thus negating the preferred values of an old Bollywood and diminishing Indian soft power.
Additionally, classic Bollywood actors like Kishore Kumar and Amitabh Bachchan have historically enjoyed popularity in Egypt and Somalia. In Ethiopia, Bollwood movies are shown alongside Hollywood productions in Piazza theatres, such as the Cinema Ethiopia in Addis Ababa. In the Maghreb, Bollywood films are also broadcast, though local aesthetics tend much more toward expressive or auteur cinema than commercial fare.
Asia.
Bollywood films are widely watched in South Asian countries, including Bangladesh, Nepal, Pakistan and Sri Lanka.
Many Pakistanis watch Bollywood films, as they understand Hindi (due to its linguistic similarity to Urdu). Pakistan banned the legal import of Bollywood movies in 1965. However, trade in pirated DVDs and illegal cable broadcasts ensured the continued popularity of Bollywood releases in Pakistan. Exceptions were made for a few films, such as the 2006 colorised re-release of the classic "Mughal-e-Azam" or the 2006 film "Taj Mahal". Early in 2008, the Pakistani government eased the ban and allowed the import of even more movies; 16 were screened in 2008. Continued easing followed in 2009 and 2010. The new policy is opposed by nationalists and representatives of Pakistan's small film industry but is embraced by cinema owners, who are making profits after years of low receipts.
Bollywood movies are popular in Afghanistan due to the country's proximity to the Indian subcontinent and cultural perspectives present in the movies. A number of Bollywood movies were filmed inside Afghanistan while some dealt with the country, including "Dharmatma", "Kabul Express", "Khuda Gawah" and "Escape From Taliban". Hindi films have been popular in Arab countries, including Palestine, Jordan, Egypt and the Gulf countries.
Imported Indian films are usually subtitled in Arabic upon the film's release. Since the early 2000s, Bollywood has progressed in Israel. Special channels dedicated to Indian films have been displayed on cable television. Bollywood films are popular in Southeast Asia (particularly in Maritime Southeast Asia) and Central Asia (particularly in Uzbekistan and Tajikistan).
Bollywood films are widely appreciated in East Asian countries such as China, Japan, South Korea and etc. Some Hindi movies had success in the China and South Korea, Japan in the 1940s and 1950s and are popular till today. The most popular Hindi films in that country were "Dr. Kotnis Ki Amar Kahani" (1946), "Awaara" (1951) and "Do Bigha Zamin" (1953). Raj Kapoor was a famous movie star in China, and the song "Awara Hoon" ("I am a Tramp") was popular in the country. Since then, Hindi films significantly declined in popularity in China, until the Academy Award nominated "Lagaan" (2001) became the first Indian film to have a nation-wide release there in decades. The Chinese filmmaker He Ping was impressed by "Lagaan", especially its soundtrack, and thus hired the film's music composer A. R. Rahman to score the soundtrack for his film "Warriors of Heaven and Earth" (2003). Several older Hindi films have a cult following in Japan, particularly the films directed by Guru Dutt.
Indian films are the most popular foreign films in Tajikistan, and Hindi-Urdu departments are very large in the country.
Europe.
The awareness of Hindi cinema is substantial in the United Kingdom, where they frequently enter the UK top ten. Many films, such as "Kabhi Khushi Kabhie Gham" (2001) have been set in London. Bollywood is also appreciated in France, Germany, the Netherlands, and the Scandinavian countries. Various Bollywood movies are dubbed in German and shown on the German television channel RTL II on a regular basis.
Bollywood films are particularly popular in the former Soviet Union. Bollywood films have been dubbed into Russian, and shown in prominent theatres such as Mosfilm and Lenfilm.
Ashok Sharma, Indian Ambassador to Suriname, who has served three times in the Commonwealth of Independent States region during his diplomatic career said:
The film "Mera Naam Joker" (1970), sought to cater to such an appeal and the popularity of Raj Kapoor in Russia, when it recruited Russian actress Kseniya Ryabinkina for the movie. In the contemporary era, "" (2005) was shot entirely in Russia. After the collapse of the Soviet film distribution system, Hollywood occupied the void created in the Russian film market. This made things difficult for Bollywood as it was losing market share to Hollywood. However, Russian newspapers report that there is a renewed interest in Bollywood among young Russians.
North America.
Bollywood has experienced a marked growth in revenue in Canada and the United States, particularly popular amongst the South Asian communities in large cities, such as Toronto, Chicago, and New York City. Yash Raj Films, one of India's largest production houses and distributors, reported in September 2005 that Bollywood films in the United States earn around $100 million a year through theatre screenings, video sales and the sale of movie soundtracks. In other words, films from India do more business in the United States than films from any other non-English speaking country. Numerous films in the mid-1990s and onwards have been largely, or entirely, shot in New York, Los Angeles, Vancouver and Toronto. Bollywood's immersion in the traditional Hollywood domain was further tied with such films as "The Guru" (2002) and "" (2007) trying to popularise the Bollywood-theme for Hollywood.
Oceania.
Bollywood is not as successful in the Oceanic countries and Pacific Islands such as New Guinea. However, it ranks second to Hollywood in countries such as Fiji, with its large Indian minority, Australia and New Zealand.
Australia is one of the countries where there is a large South Asian Diaspora. Bollywood is popular amongst non-Asians in the country as well. Since 1997 the country has provided a backdrop for an increasing number of Bollywood films. Indian filmmakers have been attracted to Australia's diverse locations and landscapes, and initially used it as the setting for song-and-dance sequences, which demonstrated the contrast between the values. However, nowadays, Australian locations are becoming more important to the plot of Bollywood films. Hindi films shot in Australia usually incorporate aspects of Australian lifestyle. The Yash Raj Film "Salaam Namaste" (2005) became the first Indian film to be shot entirely in Australia and was the most successful Bollywood film of 2005 in the country. This was followed by "Heyy Babyy" (2007) "Chak De! India" (2007) and "Singh Is Kinng" (2008) which turned out to be box office successes. Following the release of "Salaam Namaste", on a visit to India the then prime minister John Howard also sought, having seen the film, to have more Indian movies shooting in the country to boost tourism, where the Bollywood and cricket nexus, was further tightened with Steve Waugh's appointment as tourism ambassador to India. Australian actress Tania Zaetta, who co-starred in "Salaam Namaste", among other Bollywood films, expressed her keenness to expand her career in Bollywood.
South America.
Bollywood movies are not influential in many countries of South America, though Bollywood culture and dance is recognised. However, due to significant South Asian diasporic communities in Suriname and Guyana, Hindi language movies are popular. In 2006, "Dhoom 2" became the first Bollywood film to be shot in Rio de Janeiro, Brazil.
In January 2012, it was announced that UTV Motion Pictures would be releasing movies in Peru, starting with "Guzaarish".
Plagiarism.
Constrained by rushed production schedules and small budgets, some Bollywood writers and musicians have been known to resort to plagiarism. Ideas, plot lines, tunes or riffs have been copied from other Indian film industries or foreign films (including Hollywood and other Asian films) without acknowledgement of the original source. This has led to criticism towards the film industry.
Before the 1990s, this could be done with impunity. Copyright enforcement was lax in India and few actors or directors ever saw an official contract. The Hindi film industry was not widely known to non-Indian audiences (excluding the Soviet states), who would not even be aware that their material was being copied. Audiences may also not have been aware of the plagiarism since many audiences in India were unfamiliar with foreign films and music. While copyright enforcement in India is still somewhat lenient, Bollywood and other film industries are much more aware of each other now and Indian audiences are more familiar with foreign movies and music. Organizations like the India EU Film Initiative seek to foster a community between film makers and industry professional between India and the EU.
One of the common justifications of plagiarism in Bollywood in the media is that producers often play a safer option by remaking popular Hollywood films in an Indian context. Screenwriters generally produce original scripts, but due to financial uncertainty and insecurity over the success of a film many were rejected. Screenwriters themselves have been criticised for lack of creativity which happened due to tight schedules and restricted funds in the industry to employ better screenwriters. Certain filmmakers see plagiarism in Bollywood as an integral part of globalisation where American and western cultures are firmly embedding themselves into Indian culture, which is manifested, amongst other mediums, in Bollywood films. Vikram Bhatt, director of films such as "Raaz", a remake of "What Lies Beneath", and "Kasoor", a remake of "Jagged Edge", has spoken about the strong influence of American culture and desire to produce box office hits based along the same lines in Bollywood. He said, "Financially, I would be more secure knowing that a particular piece of work has already done well at the box office. Copying is endemic everywhere in India. Our TV shows are adaptations of American programmes. We want their films, their cars, their planes, their Diet Cokes and also their attitude. The American way of life is creeping into our culture." Mahesh Bhatt has said, "If you hide the source, you're a genius. There's no such thing as originality in the creative sphere".
There have been very few cases of film copyright violations taken to court because of serious delays in the legal process, and due to the long time they take to decide a case. There have been some notable cases of conflict though. The makers of "Partner" (2007) and "Zinda" (2005) have been targeted by the owners and distributors of the original films, "Hitch" and "Oldboy". American Studio Twentieth Century Fox brought the Mumbai-based B.R. Films to court over its forthcoming "Banda Yeh Bindaas Hai", allegedly an illegal remake of its 1992 film "My Cousin Vinny". B.R. Films eventually settled out of court by paying the studio at a cost of about $200,000, paving the way for the film's release. Some on the other hand do comply with copyright law, with Orion Pictures in 2008 securing the rights to remake the Hollywood film "Wedding Crashers".

</doc>
<doc id="4248" url="http://en.wikipedia.org/wiki?curid=4248" title="Bowls">
Bowls

Bowls or lawn bowls is a sport in which the objective is to roll biased balls so that they stop close to a smaller ball called a "jack" or "kitty". It is played on a pitch which may be flat (for "flat-green bowls") or convex or uneven (for "crown-green bowls"). It is normally played outdoors (although there are many indoor venues) and the outdoor surface is either natural grass, artificial turf, or cotula (in New Zealand).
History.
It has been traced certainly to the 13th century, and conjecturally to the 12th. William Fitzstephen (d. about 1190), in his biography of Thomas Becket, gives a graphic sketch of the London of his day and, writing of the summer amusements of the young men, says that on holidays they were "exercised in Leaping, Shooting, Wrestling, Casting of Stones [in jactu lapidum], and Throwing of Javelins fitted with Loops for the Purpose, which they strive to fling before the Mark; they also use Bucklers, like fighting Men." It is commonly supposed that by jactus lapidum, Fitzstephen meant the game of bowls, but though it is possible that round stones may sometimes have been employed in an early variety of the game - and there is a record of iron bowls being used, though at a much later date, on festive occasions at Nairn, - nevertheless the inference seems unwarranted. The jactus lapidum of which he speaks may have been more akin to shotput. It is beyond dispute, however, that the game, at any rate in a rudimentary form, was played in the 13th century. A manuscript of that period in the royal library, Windsor (No. 20, E iv.), contains a drawing representing two players aiming at a small cone instead of an earthenware ball or jack. The world's oldest surviving bowling green is the Southampton Old Bowling Green, which was first used in 1299.
Another manuscript of the same century has a crude but spirited picture which brings us into close touch with the existing game. Three figures are introduced and a jack. The first player's bowl has come to rest just in front of the jack; the second has delivered his bowl and is following after it with one of those eccentric contortions still not unusual on modern greens, the first player meanwhile making a repressive gesture with his hand, as if to urge the bowl to stop short of his own; the third player is depicted as in the act of delivering his bowl. A 14th-century manuscript, Book of Prayers, in the Francis Douce collection in the Bodleian Library at Oxford contains a drawing in which two persons are shown, but they bowl to no mark. Strutt (Sports and Pastimes) suggests that the first player's bowl may have been regarded by the second player as a species of jack; but in that case it is not clear what was the first player's target. In these three earliest illustrations of the pastime it is worth noting that each player has one bowl only, and that the attitude in delivering it was as various five or six hundred years ago as it is to-day. In the third he stands almost upright; in the first he kneels; in the second he stoops, halfway between the upright and the kneeling position.
The game eventually came under the ban of king and parliament, both fearing it might jeopardise the practice of archery, then so important in battle. Statutes forbidding it and other sports were enacted in the reigns of Edward III, Richard II and other monarchs. Even when, on the invention of gunpowder and firearms, the bow had fallen into disuse as a weapon of war, the prohibition was continued. The discredit attaching to bowling alleys, first established in London in 1455, probably encouraged subsequent repressive legislation, for many of the alleys were connected with taverns frequented by the dissolute and gamesters. The word "bowls" occurs for the first time in the statute of 1511 in which Henry VIII confirmed previous enactments against unlawful games. By a further act of 1541—which was not repealed until 1845—artificers, labourers, apprentices, servants and the like were forbidden to play bowls at any time except Christmas, and then only in their master's house and presence. It was further enjoined that any one playing bowls outside his own garden or orchard was liable to a penalty of 6s. 8d.(6 shillings and 8 pence), while those possessed of lands of the yearly value of £100 might obtain licences to play on their own private greens.
In 1864 William Wallace Mitchell (1803–1884), a Glasgow Cotton Merchant, published his "Manual of Bowls Playing" following his work as the secretary formed in 1849 by Scottish bowling clubs which became the basis of the rules of the modern game.
Young Mitchell was only 11 when he played on Kilmarnock Bowling green, the oldest club in Scotland, instituted in 1740.
The patenting of the first lawn mower in 1830, in Britain, is strongly believed to have been the catalyst, world-wide, for the preparation of modern-style greens, sporting ovals, playing fields, pitches, grass courts, etc. This is turn led to the codification of modern rules for many sports, including lawn bowls, most football codes, lawn tennis and others.
National Bowling Associations were established in the late 1800s. In the then Victorian Colony (now State of Victoria in Australia), the (Royal) Victorian Bowling Association was formed in 1880 and The Scottish Bowling Association was established in 1892, although there had been a failed attempt in 1848 by 200 Scottish clubs.
Today the sport is played in over 40 countries with more than 50 member national authorities.
The home of the modern game is still Scotland with the World Bowls centre in Edinburgh at Caledonia House,1 Redheughs Rigg, South Gyle, Edinburgh, EH12 9DQ.
Game.
Lawn bowls is usually played on a large, rectangular, precisely levelled and manicured grass or synthetic surface known as a bowling green which is divided into parallel playing strips called rinks. In the simplest competition, singles, one of the two opponents flips a coin to see who wins the "mat" and begins a segment of the competition (in bowling parlance, an "end"), by placing the mat and rolling the jack to the other end of the green to serve as a target. Once it has come to rest, the jack is aligned to the centre of the rink and the players take turns to roll their bowls from the mat towards the jack and thereby build up the "head".
A bowl may curve outside the rink boundary on its path, but must come to rest within the rink boundary to remain in play. Bowls falling into the ditch are dead and removed from play, except in the event when one has "touched" the jack on its way. "Touchers" are marked with chalk and remain alive in play even though they are in the ditch. Similarly if the jack is knocked into the ditch it is still alive unless it is out of bounds to the side resulting in a "dead" end which is replayed, though according to international rules the jack is "respotted" to the centre of the rink and the end is continued. After each competitor has delivered all of their bowls (four each in singles and pairs, three each in triples, and two bowls each in fours), the distance of the closest bowls to the jack is determined (the jack may have been displaced) and points, called "shots", are awarded for each bowl which a competitor has closer than the opponent's nearest to the jack. For instance, if a competitor has bowled two bowls closer to the jack than their opponent's nearest, they are awarded two shots. The exercise is then repeated for the next end, a game of bowls typically being of twenty-one ends.
Lawn bowls is played on grass and variations from green to green are common. Greens come in all shapes and sizes, fast, slow, big crown, small crown and so on.
Scoring.
Scoring systems vary from competition to competition. Games can be decided when:
Games to a specified number of ends may also be drawn. The draw may stand, or the opponents may be required to play an extra end to decide the winner. These provisions are always published beforehand in the event's Conditions of Play.
In the Laws of the Sport of Bowls
the winner in a singles game is the first player to score 21 shots. In all other disciplines (pairs, triples, fours) the winner is the team who has scored the most shots after 21/25 ends of play. Often local tournaments will play shorter games (often 10 or 12 ends). Some competitions use a "set" scoring system, with the first to seven points awarded a set in a best-or-three or best-of-five set match. As well as singles competition, there can be two (pairs), three (triples) and four-player (fours) teams. In these, teams bowl alternately, with each player within a team bowling all their bowls, then handing over to the next player. The team captain or "skip" always plays last and is instrumental in directing his team's shots and tactics. The current method of scoring in the professional tour (World Bowls Tour) is sets. Each set consists of nine ends and the player with the most shots at the end of a set wins the set. If the score is tied the set is halved. If a player wins two sets, or gets a win and a tie, that player wins the game. If each player wins a set, or both sets end tied, there is a 3-end tiebreaker to determine a winner.
Bias of bowls.
Bowls are designed to travel a curved path because of a weight bias which was originally produced by inserting weights in one side of the bowl. This is no longer permitted by the rules and bias is now produced entirely by the shape of the bowl. A bowler determines the bias direction of the bowl in his hand by a dimple or symbol on one side. Regulations determine the minimum bias allowed, and the range of diameters (11.6 to 13.1 cm), but within these rules bowlers can and do choose bowls to suit their own preference. They were originally made from lignum vitae, a dense wood giving rise to the term "woods" for bowls, but are now more typically made of a hard plastic composite material.
Bowls were once only available coloured black or brown but they are now available in a variety of colours. They have unique symbol markings engraved on them for identification. Since many bowls look the same, coloured, adhesive stickers or labels are also used to mark the bowls of each team in bowls matches. Some local associations agree on specific colours for stickers for each of the clubs in their area. Provincial or national colours are often assigned in national and international competitions. These stickers are used by officials to distinguish teams.
Bowls have symbols unique to the set of four for identification. The side of the bowl with a larger symbol within a circle indicates the side away from the bias. That side with a smaller symbol within a smaller circle is the bias side toward which the bowl will turn. It is not uncommon for players to deliver a "wrong bias" shot from time to time and see their carefully aimed bowl crossing neighbouring rinks rather than heading towards their jack.
When bowling there are several types of delivery. "Draw" shots are those where the bowl is rolled to a specific location without causing too much disturbance of bowls already in the head. For a right-handed bowler, "forehand draw" or "finger peg" is initially aimed to the right of the jack, and curves in to the left. The same bowler can deliver a "backhand draw" or "thumb peg" by turning the bowl over in his hand and curving it the opposite way, from left to right. In both cases, the bowl is rolled as close to the jack as possible, unless tactics demand otherwise. A "drive" or "fire" or "strike" involves bowling with force with the aim of knocking either the jack or a specific bowl out of play - and with the drive's speed, there is virtually no noticeable (or, at least, much less) curve on the shot. An "upshot" or "yard on" shot involves delivering the bowl with an extra degree of weight (often referred to as "controlled" weight or "rambler"), enough to displace the jack or disturb other bowls in the head without killing the end. A "block" shot is one that is intentionally placed short to defend from a drive or to stop an oppositions draw shot. The challenge in all these shots is to be able to adjust line and length accordingly, the faster the delivery, the narrower the line or "green".
Variations of play.
Particularly in team competition there can be a large number of bowls on the green towards the conclusion of the end, and this gives rise to complex tactics. Teams "holding shot" with the closest bowl will often make their subsequent shots not with the goal of placing the bowl near the jack, but in positions to make it difficult for opponents to get their bowls into the head, or to places where the jack might be deflected to if the opponent attempts to disturb the head.
There are many different ways to set up the game. Crown Green Bowling utilises the entire green. A player can send the jack anywhere on the green in this game and the green itself is more akin to a golf green, with lots of undulation. It is only played with two bowls each, the Jack also has a bias and is only slightly smaller than the Bowls. The game is played usually to 21-up in Singles and Doubles format with some competitions playing to 31-up. The Panel (Professional Crown Green Bowls) is played at the Red Lion, Westhoughton daily and is played to 41-up with greenside betting throughout play. The game of Crown Green Bowls is looking to grow with the introduction of the Portuguese Masters in October and recent interest from Sky to re-televise the sport.
Singles, triples and fours and Australian pairs are some ways the game can be played. In singles, two people play against each other and the first to reach 21, 25 or 31 shots (as decided by the controlling body) is the winner. In one variation of singles play, each player uses two bowls only and the game is played over 21 ends. A player concedes the game before the 21st end if the score difference is such that it is impossible to draw equal or win within the 21 ends. If the score is equal after 21 ends, an extra end is played to decide the winner. An additional scoring method is set play. This comprises two sets over nine ends. Should a player win a set each, they then play a further 3 ends that will decide the winner.
Pairs allows both people on a team to play Skip and Lead. The lead throws two bowls, the skip delivers two, then the lead delivers his remaining two, the skip then delivers his remaining two bowls. Each end, the leads and skips switch positions. This is played over 21 ends or sets play. Triples is with three players while Fours is with four players in each team and is played over 21 ends.
Another pairs variation is 242 pairs (also known as Australian Pairs). In the first end of the game the A players lead off with 2 bowls each, then the B players play 4 bowls each, before the A players complete the end with their final 2 bowls. The A players act as lead and skip in the same end. In the second end the roles are reversed with the A players being in the middle. This alternating pattern continues through the game which is typically over 15 ends.
Short Mat Bowls is an all-year sport unaffected by weather conditions and it does not require a permanent location as the rink mats can be rolled up and stowed away. This makes it particularly appropriate for small communities as it can be played in village halls, schools, sports and social clubs, hotels and so on. where space is restricted and is also required for other purposes: it is even played on North Sea oil rigs where space is really at a premium.
Bowls are played by the blind and paraplegic. Blind bowlers are extremely skilful. A string is run out down the centre of the lane & wherever the jack lands it is moved across to the string and the length is called out by a sighted marker, when the woods are sent the distance from the jack is called out, in yards, feet and inches-the position in relation to the jack is given using the clock,12.00 is behind the jack. The world's best are a match for the best club level sighted bowlers .
Competitions.
There is a World Indoor Bowls Championships and also World Bowls Events.
Bowls is one of the "core sports" that must be included at each edition of the Commonwealth Games. With the exception of the 1966 Games, the sport has been included in all Games since their inception in 1930. Glasgow, Scotland hosted the 2014 Commonwealth Games, with Jo Edwards (New Zealand) and Darren Burnett (Scotland) winning the singles gold medals. Gold Coast, Australia will host the 2018 Commonwealth Games.

</doc>
<doc id="4249" url="http://en.wikipedia.org/wiki?curid=4249" title="Barcelonnette">
Barcelonnette

Barcelonnette () is a commune of France and a subprefecture in the department of Alpes-de-Haute-Provence, in the Provence-Alpes-Côte d'Azur region. It is located in the southern French Alps, at the crossroads between Provence, Piedmont and the Dauphiné, and is the largest town in the Ubaye Valley. The town's inhabitants are known as "Barcelonnettes".
Toponymy.
Barcelonnette was founded and named in 1231, by Ramon Berenguer IV, Count of Provence. While the town's name is generally seen as a diminutive form of Barcelona in Spain, Albert Dauzat and Charles Rostaing point out an earlier attestation of the name "Barcilona" in Barcelonnette in around 1200, and suggest that it is derived instead from two earlier stems signifying a mountain, *"bar" and *"cin" (the latter of which is also seen in the name of Mont Cenis).
In the Vivaro-Alpine dialect of Occitan, the town is known as "Barcilona de Provença" or more rarely "Barciloneta" according to the classical norm; under the Mistralian norm it is called "Barcilouna de Prouvença" or "Barcilouneto". In "Valéian" (the dialect of Occitan spoken in the Ubaye Valley), it is called "Barcilouna de Prouvença" or "Barcilounéta". "Barcino Nova" is the town's Latin name meaning "new Barcelona"; "Barcino" was the Roman name for Barcelona in Spain from its foundation by Emperor Augustus in 10 BC, and it was only changed to "Barcelona" in the Middle Ages.
The inhabitants of the town are called "Barcelonnettes", or "Vilandroises" in Valéian.
History.
Origins.
The Barcelonnette region was populated by Ligures from the 1st millennium BC onwards, and the arrival of the Celts several centuries later led to the formation of a mixed Celto-Ligurian people, the Vesubians. Polybius described the Vesubians as belligerent but nonetheless civilised and mercantile, and Julius Caesar praised their bravery. The work "History of the Gauls" also places the Vesubians in the Ubaye Valley.
Following the Roman conquest of Provence, Barcelonnette was included in a small province with modern Embrun as its capital and governed by Albanus Bassalus. This was integrated soon afterwards into Gallia Narbonensis. In 36 AD, Emperor Nero transferred Barcelonnette to the province of the Cottian Alps. The town was known as "Rigomagensium" under the Roman Empire and was the capital of a civitas (a provincial subdivision), though no Roman money has yet been found in the canton of Barcelonnette.
Medieval town.
The town of Barcelonnette was founded in 1231 by Ramon Berenguer IV, Count of Provence. According to Charles Rostaing, this act of formal "foundation", according certain privileges to the town, was a means of regenerating the destroyed town of "Barcilona". The town was afforded a "consulat" (giving it the power to administer and defend itself) in 1240.
Control of the area in the Middle Ages swung between the Counts of Savoy and of Provence. In 1388, after Count Louis II of Provence had left to conquer Naples, the Count of Savoy Amadeus VIII took control of Barcelonnette; however, it returned to Provençal control in 1390, with the d'Audiffret family as its lords. On the death of Louis II in 1417 it reverted to Savoy, and, although Count René again retook the area for Provence in 1471, it had returned to Savoyard dominance by the start of the 16th century, by which point the County of Provence had become united with the Kingdom of France due to the death of Count Charles V in 1481.
Ancien Régime.
During Charles V's invasion of Provence in 1536, Francis I of France sent the Count of Fürstenberg's 6000 "Landsknechte" to ravage the area in a scorched earth policy. Barcelonnette and the Ubaye Valley remained under French sovereignty until the second Treaty of Cateau-Cambrésis on 3 April 1559.
In 1588 the troops of François, Duke of Lesdiguières entered the town and set fire to the church and convent during their campaign against the Duke of Savoy. In 1600, after the Treaty of Vervins, conflict returned between Henry IV of France and Savoy, and Lesdiguières retook Barcelonnette until the conclusion of the Treaty of Lyon on 17 January the following year. In 1628, during the War of the Mantuan Succession, Barcelonnette and the other towns of the Ubaye Valley were pillaged and burned by Jacques du Blé d'Uxelles and his troops, as they passed through towards Italy to the Duke of Mantua's aid. The town was retaken by the Duke of Savoy in 1630; and in 1691 it was captured by the troops of the Marquis de Vins during the War of the League of Augsburg.
Between 1614 and 1713, Barcelonnette was the seat of one of the four prefectures under the jurisdiction of the Senate of Nice. At this time, the community of Barcelonnette successfully purchased the "seigneurie" of the town as it was put to auction by the Duke of Savoy; it thereby gained its own justicial powers. In 1646, a college was founded in Barcelonnette.
A "significant" part of the town's inhabitants had, by the 16th century, converted to Protestantism, and were repressed during the French Wars of Religion.
The "viguerie" of Barcelonnette (also comprising Saint-Martin and Entraunes) was reattached to France in 1713 as part of a territorial exchange with the Duchy of Savoy during the Treaties of Utrecht. The town remained the site of a "viguerie" until the French Revolution. A decree of the council of state on 25 December 1714 reunited Barcelonnete with the general government of Provence.
Revolution.
Barcelonnette was one of few settlements in Haute-Provence to acquire a Masonic Lodge before the Revolution, in fact having two:
In March 1789, riots took place as a result of a crisis in wheat production. In July, the Great Fear of aristocratic reprisal against the ongoing French Revolution struck France, arriving in the Barcelonnette area on 31 July 1789 (when the news of the storming of the Bastille first reached the town) before spreading towards Digne.
This agitation continued in the Ubaye Valley; a new revolt broke out on 14 June, and famine was declared in April 1792. The patriotic society of the commune was one of the first 21 created in Alpes-de-Haute-Provence, in spring 1792, by the envoys of the departmental administration. Around a third of the male population attended at the club. Another episode of political violence occurred in August 1792. 
Barcelonnette was the seat of the District of Barcelonnette from 1790 to 1800.
Modern history.
In December 1851, the town was home to a movement of republican resistance towards Napoleon III's coup. Though only a minority of the population, the movement rebelled on Sunday 7 December, the day after the news of the coup arrived. Town officials and gendarmes were disarmed and placed in the maison d'arrêt. A committee of public health was created on 8 December; on 9 December the inhabitants of Jausiers and its surroundings formed a colony under the direction of general councillor Brès, and Mayor Signoret of Saint-Paul-sur-Ubaye. This was stopped, however, on 10 December before it could reach Barcelonnette, as the priest of the subprefecture had intervened. On 11 December, several officials escaped and found refuge in L'Argentière in Piedmont. The arrival of troops on 16 December put a final end to the republican resistance without bloodshed, and 57 insurgents were tried; 38 were condemned to deportation (though several were pardoned in April).
Between 1850 and 1950, Barcelonnette was the source of a wave of emigration to Mexico. Among these emigrants was Jean Baptiste Ebrard, founder of the Liverpool department store chain in Mexico; Marcelo Ebrard, the head of government of Mexico City from 2006 to 2012, is also descended from them. On the edges of Barcelonnette and Jausiers there are several houses and villas of colonial style (known as "maisons mexicaines"), constructed by emigrants to Mexico who returned to France between 1870 and 1930. A plaque in the town commemorates the deaths of ten Mexican citizens who returned to Barcelonnette to fight in the First World War.
During the Second World War, 26 Jews were arrested in Barcelonnette before being deported. The 89th "compagnie de travailleurs étrangers" (Company of Foreign Workers), consisting of foreigners judged as undesirable by the Third Republic and the Vichy regime and committed to forced labour, was established in Barcelonnette.
The 11th Battalion of "Chasseurs alpins" was garrisoned at Barcelonnette between 1948 and 1990.
Geography.
Barcelonnette is situated in the wide and fertile Ubaye Valley, of which it is the largest town. It lies at an elevation of 1132 m (3717 ft) on the right bank of the Ubaye River, and is surrounded by mountains which reach peaks of over 3000 m; the tallest of these is the Needle of Chambeyron at 3412 m. Barcelonnette is situated 210 km from Turin, 91 km from Nice and 68 km from Gap.
Biodiversity.
As a result of its relief and geographic situation, the Ubaye Valley has an "abundance of plant and animal species". The fauna is largely constituted of golden eagles, marmots, ibex and vultures, and the flora includes a large proportion of larches, génépis and white asphodels.
Climate.
The Ubaye Valley has an alpine climate and winters are harsh as a result of the altitude, but there are only light winds as a result of the relief. There are on average almost 300 days of sun and 700 mm of rain per year.
Hazards.
None of the 200 communes of the department is entirely free of seismic risk; the canton of Barcelonnette is placed in zone 1b (low risk) by the determinist classification of 1991 based on seismic history, and zone 4 (average risk) according to the probabilistic EC8 classification of 2011. The commune is also vulnerable to avalanches, forest fires, floods, and landslides. Barcelonnette is also exposed to the possibility of a technological hazard in that road transport of dangerous materials is allowed to pass through on the RD900.
The town has been subject to several orders of natural disaster: floods and mudslides in 1994 and 2008, and landslides in 1996 and 1999. The strongest recorded earthquakes in the region occurred on 5 April 1959, with its epicentre at Saint-Paul-sur-Ubaye and a recorded intensity of 6.5 at Barcelonnette, and on 17 February 1947, with its epicentre at Prazzo over the Italian border.
Architecture.
The subprefecture has been situated since 1978 in a "maison mexicaine", the Villa l'Ubayette, constructed between 1901 and 1903.
Population.
In 1471, the community of Barcelonnette (including several surrounding parishes) comprised 421 fires (households). In 1765, it had 6674 inhabitants, but emigration, particularly to Mexico, slowed the town's growth in the period before the Second World War. According to the census of 2007, Barcelonnette has a population of 2766 (municipal population) or 2939 (total) across a total of 16.42 km2. The town is characterised by low population density. Between 1990 and 1999 the town's annual mean population growth was -0.6%, though between 1999 and 2007 this increased to an average of -0.1%.
Economy.
The city is mainly a tourist and resort centre, serving many ski lodges. The Pra Loup resort is 7 km from Barcelonnette; Le Sauze is 5 km away. It and the Ubaye Valley are served by the Barcelonnette - Saint-Pons Airport. Notably, Barcelonnette is the only subprefecture of France not be served by rail transport; the Ubaye line which would have linked Chorges to Barcelonnette was never completed as a result of the First World War and the construction of the Serre-Ponçon Dam between 1955 and 1961.
Education.
An "école normale" (an institute for training primary school teachers) was founded in Barcelonnette in 1833, and remained there until 1888 when it was transferred to Digne. The "lycée André-Honnorat de Barcelonnette", originally the "collège Saint-Maurice" and renamed after the politician André Honnorat in 1919, is located in the town; Pierre-Gilles de Gennes and Carole Merle both studied there. Currently, three schools exist in Barcelonnette: a public nursery school, a public elementary school, and a private school (under a contract by which the teachers are paid by the national education system).
In 2010 the "lycée André-Honnorat" opened a boarding school aimed at gifted students of poorer social backgrounds, in order to give them better conditions in which to study. It is located in the "Quartier Craplet", formerly the garrison of the 11th Battalion of "Chasseurs Alpins" and then the French Army's "Centre d'instruction et d'entraînement au combat en montagne" (CIECM).
International links.
Barcelonnette is twinned with:
It is also the site of a Mexican honorary consulate.

</doc>
<doc id="4251" url="http://en.wikipedia.org/wiki?curid=4251" title="Bahá'í Faith">
Bahá'í Faith

The Bahá'í Faith ( "Baha'iyyah") ) is a monotheistic religion emphasizing the spiritual unity of all humankind. Three core principles establish a basis for Bahá'í teachings and doctrine: the unity of God, that there is only one God who is the source of all creation; the unity of religion, that all major religions have the same spiritual source and come from the same God; and the unity of humanity, that all humans have been created equal, and that diversity of race and culture are seen as worthy of appreciation and acceptance. According to the Bahá'í Faith's teachings, the human purpose is to learn to know and love God through such methods as prayer, reflection and being of service to humanity.
The Bahá'í Faith was founded by Bahá'u'lláh in 19th-century Persia. Bahá'u'lláh was exiled for his teachings, from Persia to the Ottoman Empire, and died while officially still a prisoner. After Bahá'u'lláh's death, under the leadership of his son, `Abdu'l-Bahá, the religion spread from its Persian and Ottoman roots, and gained a footing in Europe and America, and was consolidated in Iran, where it suffers intense persecution. After the death of `Abdu'l-Bahá, the leadership of the Bahá'í community entered a new phase, evolving from a single individual to an administrative order with both elected bodies and appointed individuals. There are probably more than 5 million Bahá'ís around the world in more than 200 countries and territories.
In the Bahá'í Faith, religious history is seen to have unfolded through a series of divine messengers, each of whom established a religion that was suited to the needs of the time and to the capacity of the people. These messengers have included Abrahamic figures, Moses, Jesus, Muhammad as well as Dharmic ones, Krishna, Buddha, and others. For Bahá'ís, the most recent messengers are the Báb and Bahá'u'lláh. In Bahá'í belief, each consecutive messenger prophesied of messengers to follow, and Bahá'u'lláh's life and teachings fulfilled the end-time promises of previous scriptures. Humanity is understood to be in a process of collective evolution, and the need of the present time is for the gradual establishment of peace, justice and unity on a global scale.
Etymology.
The word "Bahá'í " is used either as an adjective to refer to the Bahá'í Faith or as a term for a follower of Bahá'u'lláh. The word is not a noun meaning the religion as a whole. It is derived from the Arabic "Bahá"' (), meaning "glory" or "splendor". The term "Bahaism" (or "Baha'ism") is still used, mainly in a pejorative sense.
Beliefs.
Three core principles establish a basis for Bahá'í teachings and doctrine: the unity of God, the unity of religion, and the unity of humanity. From these postulates stems the belief that God periodically reveals his will through divine messengers, whose purpose is to transform the character of humankind and to develop, within those who respond, moral and spiritual qualities. Religion is thus seen as orderly, unified, and progressive from age to age.
God.
The Bahá'í writings describe a single, personal, inaccessible, omniscient, omnipresent, imperishable, and almighty God who is the creator of all things in the universe. The existence of God and the universe is thought to be eternal, without a beginning or end. Though inaccessible directly, God is nevertheless seen as conscious of creation, with a will and purpose that is expressed through messengers termed Manifestations of God.
Bahá'í teachings state that God is too great for humans to fully comprehend, or to create a complete and accurate image of, by themselves. Therefore, human understanding of God is achieved through his revelations via his Manifestations. In the Bahá'í religion God is often referred to by titles and attributes (for example, the All-Powerful, or the All-Loving), and there is a substantial emphasis on monotheism; such doctrines as the Trinity are seen as compromising, if not contradicting, the Bahá'í view that God is single and has no equal. The Bahá'í teachings state that the attributes which are applied to God are used to translate Godliness into human terms and also to help individuals concentrate on their own attributes in worshipping God to develop their potentialities on their spiritual path. According to the Bahá'í teachings the human purpose is to learn to know and love God through such methods as prayer, reflection, and being of service to others.
Religion.
Bahá'í notions of progressive religious revelation result in their accepting the validity of the well known religions of the world, whose founders and central figures are seen as Manifestations of God. Religious history is interpreted as a series of dispensations, where each "manifestation" brings a somewhat broader and more advanced revelation, suited for the time and place in which it was expressed. Specific religious social teachings (for example, the direction of prayer, or dietary restrictions) may be revoked by a subsequent manifestation so that a more appropriate requirement for the time and place may be established. Conversely, certain general principles (for example, neighbourliness, or charity) are seen to be universal and consistent. In Bahá'í belief, this process of progressive revelation will not end; however, it is believed to be cyclical. Bahá'ís do not expect a new manifestation of God to appear within 1000 years of Bahá'u'lláh's revelation.
Bahá'í beliefs are sometimes described as syncretic combinations of earlier religious beliefs. Bahá'ís, however, assert that their religion is a distinct tradition with its own scriptures, teachings, laws, and history. While the religion was initially seen as a sect of Islam, most religious specialists now see it as an independent religion, with its religious background in Shi'a Islam being seen as analogous to the Jewish context in which Christianity was established. Muslim institutions and clergy, both Sunni and Shia, consider Bahá'ís to be deserters or apostates from Islam, which has led to Bahá'ís being persecuted. Bahá'ís, themselves, describe their faith as an independent world religion, differing from the other traditions in its relative age and in the appropriateness of Bahá'u'lláh's teachings to the modern context. Bahá'u'lláh is believed to have fulfilled the messianic expectations of these precursor faiths.
Human beings.
The Bahá'í writings state that human beings have a "rational soul", and that this provides the species with a unique capacity to recognize God's station and humanity's relationship with its creator. Every human is seen to have a duty to recognize God through His messengers, and to conform to their teachings. Through recognition and obedience, service to humanity and regular prayer and spiritual practice, the Bahá'í writings state that the soul becomes closer to God, the spiritual ideal in Bahá'í belief. When a human dies, the soul passes into the next world, where its spiritual development in the physical world becomes a basis for judgment and advancement in the spiritual world. Heaven and Hell are taught to be spiritual states of nearness or distance from God that describe relationships in this world and the next, and not physical places of reward and punishment achieved after death.
The Bahá'í writings emphasize the essential equality of human beings, and the abolition of prejudice. Humanity is seen as essentially one, though highly varied; its diversity of race and culture are seen as worthy of appreciation and acceptance. Doctrines of racism, nationalism, caste, social class, and gender-based hierarchy are seen as artificial impediments to unity. The Bahá'í teachings state that the unification of humanity is the paramount issue in the religious and political conditions of the present world.
Teachings.
Summary.
Shoghi Effendi, the appointed head of the religion from 1921 to 1957, wrote the following summary of what he considered to be the distinguishing principles of Bahá'u'lláh's teachings, which, he said, together with the laws and ordinances of the "Kitáb-i-Aqdas" constitute the bedrock of the Bahá'í Faith:
Social principles.
The following principles are frequently listed as a quick summary of the Bahá'í teachings. They are derived from transcripts of speeches given by `Abdu'l-Bahá during his tour of Europe and North America in 1912. The list is not authoritative and a variety of such lists circulate.
With specific regard to the pursuit of world peace, Bahá'u'lláh prescribed a world-embracing collective security arrangement as necessary for the establishment of a lasting peace.
Mystical teachings.
Although the Bahá'í teachings have a strong emphasis on social and ethical issues, there exist a number of foundational texts that have been described as mystical. The "Seven Valleys" is considered Bahá'u'lláh's "greatest mystical composition." It was written to a follower of Sufism, in the style of `Attar, a Muslim poet, and sets forth the stages of the soul's journey towards God. It was first translated into English in 1906, becoming one of the earliest available books of Bahá'u'lláh to the West. The "Hidden Words" is another book written by Bahá'u'lláh during the same period, containing 153 short passages in which Bahá'u'lláh claims to have taken the basic essence of certain spiritual truths and written them in brief form.
Covenant.
The Bahá'í teachings speak of both a "Greater Covenant", being universal and endless, and a "Lesser Covenant", being unique to each religious dispensation. The Lesser Covenant is viewed as an agreement between a Messenger of God and his followers and includes social practices and the continuation of authority in the religion. At this time Bahá'ís view Bahá'u'lláh's revelation as a binding lesser covenant for his followers; in the Bahá'í writings being firm in the covenant is considered a virtue to work toward. The Greater Covenant is viewed as a more enduring agreement between God and humanity, where a Manifestation of God is expected to come to humanity about every thousand years, at times of turmoil and uncertainty.
With unity as an essential teaching of the religion, Bahá'ís follow an administration they believe is divinely ordained, and therefore see attempts to create schisms and divisions as efforts that are contrary to the teachings of Bahá'u'lláh. Schisms have occurred over the succession of authority, but any Bahá'í divisions have had relatively little success and have failed to attract a sizeable following. The followers of such divisions are regarded as Covenant-breakers and shunned, essentially excommunicated.
Canonical texts.
The "canonical texts" are the writings of the Báb, Bahá'u'lláh, `Abdu'l-Bahá, Shoghi Effendi and the Universal House of Justice, and the authenticated talks of `Abdu'l-Bahá. The writings of the Báb and Bahá'u'lláh are considered as divine revelation, the writings and talks of `Abdu'l-Bahá and the writings of Shoghi Effendi as authoritative interpretation, and those of the Universal House of Justice as authoritative legislation and elucidation. Some measure of divine guidance is assumed for all of these texts. Some of Bahá'u'lláh's most important writings include the Kitáb-i-Aqdas, literally the "Most Holy Book", which is his book of laws, the Kitáb-i-Íqán, literally the "Book of Certitude", which became the foundation of much of Bahá'í belief, the Gems of Divine Mysteries, which includes further doctrinal foundations, and the Seven Valleys and the Four Valleys which are mystical treatises.
History.
Bahá'í history follows a sequence of leaders, beginning with the Báb's May 23, 1844 declaration in Shiraz, Iran, and ultimately resting on an administrative order established by the central figures of the religion. The Bahá'í community was mostly confined to the Persian and Ottoman empires until after the death of Bahá'u'lláh in 1892, at which time he had followers in 13 countries of Asia and Africa. Under the leadership of his son, `Abdu'l-Bahá, the religion gained a footing in Europe and America, and was consolidated in Iran, where it still suffers intense persecution. After the death of `Abdu'l-Bahá in 1921, the leadership of the Bahá'í community entered a new phase, evolving from a single individual to an administrative order with both elected bodies and appointed individuals.
The Báb.
On May 23, 1844 Siyyid `Alí-Muhammad of Shiraz, Iran proclaimed that he was "the Báb" ( "the Gate"), referring to his later claim to the station of Mahdi, the Twelfth Imam of Shi`a Islam. His followers were therefore known as Bábís. As the Báb's teachings spread, which the Islamic clergy saw as a threat, his followers came under increased persecution and torture. The conflicts escalated in several places to military sieges by the Shah's army. The Báb himself was imprisoned and eventually executed in 1850.
Bahá'ís see the Báb as the forerunner of the Bahá'í Faith, because the Báb's writings introduced the concept of "He whom God shall make manifest", a Messianic figure whose coming, according to Bahá'ís, was announced in the scriptures of all of the world's great religions, and whom Bahá'u'lláh, the founder of the Bahá'í Faith, claimed to be in 1863. The Báb's tomb, located in Haifa, Israel, is an important place of pilgrimage for Bahá'ís. The remains of the Báb were brought secretly from Iran to the Holy Land and eventually interred in the tomb built for them in a spot specifically designated by Bahá'u'lláh. The main written works translated into English of the Báb's are collected in Selections from the Writings of the Báb out of the estimated 135 works.
Bahá'u'lláh.
Mírzá Husayn `Alí Núrí was one of the early followers of the Báb, and later took the title of Bahá'u'lláh. He was arrested and imprisoned for this involvement in 1852. Bahá'u'lláh relates that in 1853, while incarcerated in the dungeon of the Síyáh-Chál in Tehran, he received the first intimations that he was the one anticipated by the Báb.
Shortly thereafter he was expelled from Tehran to Baghdad, in the Ottoman Empire; then to Constantinople (now Istanbul); and then to Adrianople (now Edirne). In 1863, at the time of his banishment from Baghdad to Constantinople, Bahá'u'lláh declared his claim to a divine mission to his family and followers. Tensions then grew between him and Subh-i-Azal, the appointed leader of the Bábís who did not recognize Bahá'u'lláh's claim. Throughout the rest of his life Bahá'u'lláh gained the allegiance of most of the Bábís, who came to be known as Bahá'ís. Beginning in 1866, he began declaring his mission as a Messenger of God in letters to the world's religious and secular rulers, including Pope Pius IX, Napoleon III, and Queen Victoria.
In 1868 Bahá'u'lláh was banished by Sultan Abdülâziz a final time to the Ottoman penal colony of `Akká, in present-day Israel. Towards the end of his life, the strict and harsh confinement was gradually relaxed, and he was allowed to live in a home near `Akká, while still officially a prisoner of that city. He died there in 1892. Bahá'ís regard his resting place at Bahjí as the Qiblih to which they turn in prayer each day.
Bahá'u'lláh wrote many written works taken as scripture in the religion of which only a fraction have been translated into English. There have been 15,000 works both small and large noted - the most significant of which are the Most Holy Book, the Book of Certitude, the Hidden Words, and the Seven Valleys. There is also a series of compilation volumes of smaller works the most significant of which is the Gleanings from the Writings of Bahá'u'lláh.
`Abdu'l-Bahá.
`Abbás Effendi was Bahá'u'lláh's eldest son, known by the title of `Abdu'l-Bahá (Servant of Bahá). His father left a Will that appointed `Abdu'l-Bahá as the leader of the Bahá'í community, and designated him as the "Centre of the Covenant", "Head of the Faith", and the sole authoritative interpreter of Bahá'u'lláh's writings. `Abdu'l-Bahá had shared his father's long exile and imprisonment, which continued until `Abdu'l-Bahá's own release as a result of the Young Turk Revolution in 1908. Following his release he led a life of travelling, speaking, teaching, and maintaining correspondence with communities of believers and individuals, expounding the principles of the Bahá'í Faith.
It is estimated that `Abdu'l-Bahá wrote over 27,000 works mostly in the form of letters of which only a fraction have been translated into English. Among the more well known are The Secret of Divine Civilization, the Tablet to Auguste-Henri Forel, and Some Answered Questions. Additionally notes taken of a number of his talks were published in various volumes like Paris Talks during his journeys to the West.
Bahá'í administration.
Bahá'u'lláh's "Kitáb-i-Aqdas" and "The Will and Testament of `Abdu'l-Bahá" are foundational documents of the Bahá'í administrative order. Bahá'u'lláh established the elected Universal House of Justice, and `Abdu'l-Bahá established the appointed hereditary Guardianship and clarified the relationship between the two institutions. In his Will, `Abdu'l-Bahá appointed his eldest grandson, Shoghi Effendi, as the first "Guardian" of the Bahá'í Faith, serving as head of the religion until his death, for 36 years.
Shoghi Effendi throughout his lifetime translated Bahá'í texts; developed global plans for the expansion of the Bahá'í community; developed the Bahá'í World Centre; carried on a voluminous correspondence with communities and individuals around the world; and built the administrative structure of the religion, preparing the community for the election of the Universal House of Justice. He died in 1957 under conditions that did not allow for a successor to be appointed.
At local, regional, and national levels, Bahá'ís elect members to nine-person Spiritual Assemblies, which run the affairs of the religion. There are also appointed individuals working at various levels, including locally and internationally, which perform the function of propagating the teachings and protecting the community. The latter do not serve as clergy, which the Bahá'í Faith does not have. The Universal House of Justice, first elected in 1963, remains the successor and supreme governing body of the Bahá'í Faith, and its 9 members are elected every five years by the members of all National Spiritual Assemblies. Any male Bahá'í, 21 years or older, is eligible to be elected to the Universal House of Justice; all other positions are open to male and female Bahá'ís.
International plans.
In 1937, Shoghi Effendi launched a seven-year plan for the Bahá'ís of North America, followed by another in 1946. In 1953, he launched the first international plan, the Ten Year World Crusade. This plan included extremely ambitious goals for the expansion of Bahá'í communities and institutions, the translation of Bahá'í texts into several new languages, and the sending of Bahá'í pioneers into previously unreached nations. He announced in letters during the Ten Year Crusade that it would be followed by other plans under the direction of the Universal House of Justice, which was elected in 1963 at the culmination of the Crusade. The House of Justice then launched a nine-year plan in 1964, and a series of subsequent multi-year plans of varying length and goals followed, guiding the direction of the international Bahá'í community.
Annually, on April 21, the Universal House of Justice sends a ‘Ridván’ message to the worldwide Bahá’í community, which generally gives an update on the progress made concerning the current plan, and provides further guidance for the year to come. The Bahá'ís around the world are currently being encouraged to focus on capacity building through children's classes, youth groups, devotional gatherings, and a systematic study of the religion known as study circles. Further focuses are involvement in social action and participation in the prevalent discourses of society. The years from 2001 until 2021 represent four successive five-year plans, culminating in the centennial anniversary of the passing of `Abdu'l-Bahá.
Demographics.
A Bahá'í published document reported 4.74 million Bahá'ís in 1986 growing at a rate of 4.4%. Bahá'í sources since 1991 usually estimate the worldwide Bahá'í population to be above 5 million. The "World Christian Encyclopedia" estimated 7.1 million Bahá'ís in the world in 2000, representing 218 countries, and 7.3 million in 2010 with the same source. They further state: "The Baha'i Faith is the only religion to have grown faster in every United Nations region over the past 100 years than the general population; Baha’i was thus the fastest-growing religion between 1910 and 2010, growing at least twice as fast as the population of almost every UN region." This source's only systematic flaw was to consistently have a higher estimate of Christians than other cross-national data sets.
From its origins in the Persian and Ottoman Empires, by the early 20th century there were a number of converts in South and South East Asia, Europe, and North America. During the 1950s and 1960s, vast travel teaching efforts brought the religion to almost every country and territory of the world. By the 1990s, Bahá'ís were developing programs for systematic consolidation on a large scale, and the early 21st century saw large influxes of new adherents around the world. The Bahá'í Faith is currently the largest religious minority in Iran, Panama, and Belize; the second largest international religion in Bolivia, Zambia, and Papua New Guinea; and the third largest international religion in Chad and Kenya.
According to "The World Almanac and Book of Facts 2004":
The Bahá'í religion was listed in "The Britannica Book of the Year" (1992–present) as the second most widespread of the world's independent religions in terms of the number of countries represented. According to "Britannica", the Bahá'í Faith (as of 2002) is established in 247 countries and territories; represents over 2,100 ethnic, racial, and tribal groups; has scriptures translated into over 800 languages; and has an estimated seven million adherents worldwide. Additionally, Bahá'ís have self-organized in most of the nations of the world.
The Bahá'í religion was ranked by the FP magazine as the world's second fastest growing religion by percentage (1.7%) in 2007.
Social practices.
Laws.
The laws of the Bahá'í Faith primarily come from the "Kitáb-i-Aqdas", written by Bahá'u'lláh. The following are a few examples of basic laws and religious observances.
While some of the laws from the Kitáb-i-Aqdas are applicable at the present time and may be enforced to a degree by the administrative institutions, Bahá'u'lláh has provided for the progressive application of other laws that are dependent upon the existence of a predominantly Bahá'í society. The laws, when not in direct conflict with the civil laws of the country of residence, are binding on every Bahá'í, and the observance of personal laws, such as prayer or fasting, is the sole responsibility of the individual.
Marriage.
The purpose of marriage in the Bahá'i faith is mainly to foster spiritual harmony, fellowship and unity between a man and a woman and to provide a stable and loving environment for the rearing of children. The Bahá'í teachings on marriage call it a "fortress for well-being and salvation" and place marriage and the family as the foundation of the structure of human society. Bahá'u'lláh highly praised marriage, discouraged divorce and homosexuality, and required chastity outside of marriage; Bahá'u'lláh taught that a husband and wife should strive to improve the spiritual life of each other. Interracial marriage is also highly praised throughout Bahá'í scripture.
Bahá'ís intending to marry are asked to obtain a thorough understanding of the other's character before deciding to marry. Although parents should not choose partners for their children, once two individuals decide to marry, they must receive the consent of all living biological parents, even if one partner is not a Bahá'í. The Bahá'í marriage ceremony is simple; the only compulsory part of the wedding is the reading of the wedding vows prescribed by Bahá'u'lláh which both the groom and the bride read, in the presence of two witnesses. The vows are "We will all, verily, abide by the Will of God."
Work.
Monasticism is forbidden, and Bahá'ís attempt to ground their spirituality in ordinary daily life. Performing useful work, for example, is not only required but considered a form of worship. Bahá'u'lláh prohibited a mendicant and ascetic lifestyle. The importance of self-exertion and service to humanity in one's spiritual life is emphasised further in Bahá'u'lláh's writings, where he states that work done in the spirit of service to humanity enjoys a rank equal to that of prayer and worship in the sight of God.
Places of worship.
Most Bahá'í meetings occur in individuals' homes, local Bahá'í centers, or rented facilities. Worldwide, there are currently seven Bahá'í Houses of Worship, with an eighth under construction in Chile, and a further seven planned as of April 2012. Bahá'í writings refer to an institution called a "Mashriqu'l-Adhkár" (Dawning-place of the Mention of God), which is to form the center of a complex of institutions including a hospital, university, and so on. The first ever Mashriqu'l-Adhkár in `Ishqábád, Turkmenistan, has been the most complete House of Worship.
Calendar.
The Bahá'í calendar is based upon the calendar established by the Báb. The year consists of 19 months, each having 19 days, with four or five intercalary days, to make a full solar year. The Bahá'í New Year corresponds to the traditional Persian New Year, called Naw Rúz, and occurs on the vernal equinox, March 21, at the end of the month of fasting. Bahá'í communities gather at the beginning of each month at a meeting called a Feast for worship, consultation and socializing.
Each of the 19 months is given a name which is an attribute of God; some examples include Bahá’ (Splendour), ‘Ilm (Knowledge), and Jamál (Beauty). The Bahá'í week is familiar in that it consists of seven days, with each day of the week also named after an attribute of God. Bahá'ís observe 11 Holy Days throughout the year, with work suspended on 9 of these. These days commemorate important anniversaries in the history of the religion.
Symbols.
The symbols of the religion are derived from the Arabic word Bahá’ ( "splendor" or "glory"), with a numerical value of 9, which is why the most common symbol is the nine-pointed star. The ringstone symbol and calligraphy of the Greatest Name are also often encountered. The former consists of two five-pointed stars interspersed with a stylized Bahá’ whose shape is meant to recall the three onenesses, while the latter is a calligraphic rendering of the phrase Yá Bahá'u'l-Abhá ( "O Glory of the Most Glorious!").The five-pointed star is the symbol of the Bahá'í Faith. In the Bahá'í Faith, the star is known as the Haykal (), and it was initiated and established by the Báb. The Báb and Bahá'u'lláh wrote various works in the form of a pentagram.
Socio-economic development.
Since its inception the Bahá'í Faith has had involvement in socio-economic development beginning by giving greater freedom to women, promulgating the promotion of female education as a priority concern, and that involvement was given practical expression by creating schools, agricultural coops, and clinics.
The religion entered a new phase of activity when a message of the Universal House of Justice dated October 20, 1983 was released. Bahá'ís were urged to seek out ways, compatible with the Bahá'í teachings, in which they could become involved in the social and economic development of the communities in which they lived. Worldwide in 1979 there were 129 officially recognized Bahá'í socio-economic development projects. By 1987, the number of officially recognized development projects had increased to 1482.
United Nations.
Bahá'u'lláh wrote of the need for world government in this age of humanity's collective life. Because of this emphasis the international Bahá'í community has chosen to support efforts of improving international relations through organizations such as the League of Nations and the United Nations, with some reservations about the present structure and constitution of the UN. The Bahá'í International Community is an agency under the direction of the Universal House of Justice in Haifa, and has consultative status with the following organizations:
The Bahá'í International Community has offices at the United Nations in New York and Geneva and representations to United Nations regional commissions and other offices in Addis Ababa, Bangkok, Nairobi, Rome, Santiago, and Vienna. In recent years an Office of the Environment and an Office for the Advancement of Women were established as part of its United Nations Office. The Bahá'í Faith has also undertaken joint development programs with various other United Nations agencies. In the 2000 Millennium Forum of the United Nations a Bahá'í was invited as the only non-governmental speaker during the summit.
Persecution.
Bahá'ís continue to be persecuted in Islamic countries, as Islamic leaders do not recognize the Bahá'í Faith as an independent religion, but rather as apostasy from Islam. The most severe persecutions have occurred in Iran, where over 200 Bahá'ís were executed between 1978 and 1998, and in Egypt. The rights of Bahá'ís have been restricted to greater or lesser extents in numerous other countries, including Afghanistan, Indonesia, Iraq, Morocco, and several countries in sub-Saharan Africa.
Iran.
The marginalization of the Iranian Bahá'ís by current governments is rooted in historical efforts by Muslim clergy to persecute the religious minority. When the Báb started attracting a large following, the clergy hoped to stop the movement from spreading by stating that its followers were enemies of God. These clerical directives led to mob attacks and public executions. Starting in the twentieth century, in addition to repression that impacted individual Bahá'ís, centrally directed campaigns that targeted the entire Bahá'í community and its institutions were initiated. In one case in Yazd in 1903 more than 100 Bahá'ís were killed. Bahá'í schools, such as the Tarbiyat boys' and girl's schools in Tehran, were closed in the 1930s and 40s, Bahá'í marriages were not recognized and Bahá'í texts were censored.
During the reign of Mohammad Reza Pahlavi, to divert attention from economic difficulties in Iran and from a growing nationalist movement, a campaign of persecution against the Bahá'ís was instituted. An approved and coordinated anti-Bahá'í campaign (to incite public passion against the Bahá'ís) started in 1955 and it included the spreading of anti-Bahá'í propaganda on national radio stations and in official newspapers. In the late 1970s the Shah's regime consistently lost legitimacy due to criticism that it was pro-Western. As the anti-Shah movement gained ground and support, revolutionary propaganda was spread which alleged that some of the Shah's advisors were Bahá'ís. Bahá'ís were portrayed as economic threats, and as supporters of Israel and the West, and societal hostility against the Bahá'ís increased.
Since the Islamic Revolution of 1979 Iranian Bahá'ís have regularly had their homes ransacked or have been banned from attending university or from holding government jobs, and several hundred have received prison sentences for their religious beliefs, most recently for participating in study circles. Bahá'í cemeteries have been desecrated and property has been seized and occasionally demolished, including the House of Mírzá Buzurg, Bahá'u'lláh's father. The House of the Báb in Shiraz, one of three sites to which Bahá'ís perform pilgrimage, has been destroyed twice.
According to a US panel, attacks on Bahá'ís in Iran have increased since Mahmoud Ahmadinejad became president. The United Nations Commission on Human Rights revealed an October 2005 confidential letter from Command Headquarters of the Armed Forces of Iran ordering its members to identify Bahá'ís and to monitor their activities. Due to these actions, the Special Rapporteur of the United Nations Commission on Human Rights stated on March 20, 2006, that she "also expresses concern that the information gained as a result of such monitoring will be used as a basis for the increased persecution of, and discrimination against, members of the Bahá'í faith, in violation of international standards. The Special Rapporteur is concerned that this latest development indicates that the situation with regard to religious minorities in Iran is, in fact, deteriorating."
On May 14, 2008, members of an informal body known as the "Friends" that oversaw the needs of the Bahá'í community in Iran were arrested and taken to Evin prison. The Friends court case has been postponed several times, but was finally underway on January 12, 2010. Other observers were not allowed in the court. Even the defence lawyers, who for two years have had minimal access to the defendants, had difficulty entering the courtroom. The chairman of the U.S. Commission on International Religious Freedom said that it seems that the government has already predetermined the outcome of the case and is violating international human rights law. Further sessions were held on February 7, 2010, April 12, 2010 and June 12, 2010. On August 11, 2010 it became known that the court sentence was 20 years imprisonment for each of the seven prisoners which was later reduced to ten years. After the sentence, they were transferred to Gohardasht prison. In March 2011 the sentences were reinstated to the original 20 years.
On January 3, 2010, Iranian authorities detained ten more members of the Baha'i minority, reportedly including Leva Khanjani, granddaughter of Jamaloddin Khanjani, one of seven Baha'i leaders jailed since 2008 and in February, they arrested his son, Niki Khanjani.
The Iranian government claims that the Bahá'í Faith is not a religion, but is instead a political organization, and hence refuses to recognize it as a minority religion. However, the government has never produced convincing evidence supporting its characterization of the Bahá'í community. Also, the government's statements that Bahá'ís who recanted their religion would have their rights restored, attest to the fact that Bahá'ís are persecuted solely for their religious affiliation. The Iranian government also accuses the Bahá'í Faith of being associated with Zionism because the Bahá'í World Centre is located in Haifa, Israel. These accusations against the Bahá'ís have no basis in historical fact, and the accusations are used by the Iranian government to use the Bahá'ís as "scapegoats". In fact it was the Iranian leader Naser al-Din Shah Qajar who banished Bahá'u'lláh from Persia to the Ottoman Empire and Bahá'u'lláh was later exiled by the Ottoman Sultan, at the behest of the Persian Shah, to territories further away from Iran and finally to Acre in Syria, which only a century later was incorporated into the state of Israel.
Egypt.
Bahá'í institutions and community activities have been illegal under Egyptian law since 1960. All Bahá'í community properties, including Bahá'í centers, libraries, and cemeteries, have been confiscated by the government and fatwas have been issued charging Bahá'ís with apostasy.
The Egyptian identification card controversy began in the 1990s when the government modernized the electronic processing of identity documents, which introduced a de facto requirement that documents must list the person's religion as Muslim, Christian, or Jewish (the only three religions officially recognized by the government). Consequently, Bahá'ís were unable to obtain government identification documents (such as national identification cards, birth certificates, death certificates, marriage or divorce certificates, or passports) necessary to exercise their rights in their country unless they lied about their religion, which conflicts with Bahá'í religious principle. Without documents, they could not be employed, educated, treated in hospitals, travel outside of the country, or vote, among other hardships. Following a protracted legal process culminating in a court ruling favorable to the Bahá'ís, the interior minister of Egypt released a decree on April 14, 2009, amending the law to allow Egyptians who are not Muslim, Christian, or Jewish to obtain identification documents that list dash in place of one of the three recognized religions. The first identification cards were issued to two Bahá'ís under the new decree on August 8, 2009.

</doc>
<doc id="4257" url="http://en.wikipedia.org/wiki?curid=4257" title="Burgundians">
Burgundians

The Burgundians (; ; ; ) were an East Germanic tribe which may have emigrated from mainland Scandinavia to the Baltic island of Bornholm, and from there to the Vistula basin, in middle modern Poland. A part of the Burgundian tribes migrated further westward, where they may have participated in the 406 Crossing of the Rhine, after which they settled in the Rhine Valley and established the Kingdom of the Burgundians. Another part of Burgundians stayed in their previous homeland in Oder-Vistula basin and formed a contingent in Attila's Hunnic army by 451.
Their name survives in the regional appellation, Burgundy.
Name.
The name of the Burgundians has since remained connected to the area of modern France that still bears their name: see the later history of Burgundy. Between the 6th and 20th centuries, however, the boundaries and political connections of this area have changed frequently; with none of the changes having had anything to do with the original Burgundians. The name "Burgundians" used here and generally used by English writers to refer to the Burgundiones is a later formation and more precisely refers to the inhabitants of the territory of Burgundy which was named from the people called Burgundiones. The descendants of the Burgundians today are found primarily in historical Burgundy and among the west Swiss.
History.
Background.
The Burgundians had a tradition of Scandinavian origin which finds support in place-name evidence and archaeological evidence (Stjerna) and many consider their tradition to be correct (e.g. Musset, p. 62). The Burgundians are believed to have then emigrated to the Baltic island of Bornholm ("the island of the Burgundians" in Old Norse). However, by about 250 the population of Bornholm had largely disappeared from the island. Most cemeteries ceased to be used, and those that were still used had few burials (Stjerna, in Nerman 1925:176). In "Þorsteins saga Víkingssonar" ("The Saga of Thorstein, Viking's Son"), the Veseti settled in an island or holm, which was called Borgund's holm, i.e. Bornholm. Alfred the Great's translation of "Orosius" uses the name "Burgenda land". The poet and early mythologist Viktor Rydberg (1828–1895), ("Our Fathers' Godsaga") asserted from an early medieval source, "Vita Sigismundi", that they themselves retained oral traditions about their Scandinavian origin.
Early Roman sources such as Tacitus and Pliny the Elder knew little concerning the Germanic peoples east of the Elbe river, or on the Baltic Sea. Pliny (IV.28) however mentions them among the Vandalic or Eastern Germanic Germani peoples, including also the Goths. By the end of the 4th century the Burgundians appeared in the Vistula basin, in central modern Poland. Claudius Ptolemy lists them as living between the Suevus (probably the Oder) and Vistula rivers, north of the Lugii, and south of the coast dwelling tribes. Jordanes later reported that during the 3rd century, the Burgundians living in the Vistula basin were almost annihilated by Fastida, king of the Gepids, whose kingdom was at the mouth of the Vistula.
In the late 3rd century, the Burgundians appear on the east bank of the Rhine, confronting Roman Gaul. Zosimus (1.68) reports them being defeated by the emperor Probus in 278 in Gaul. At this time they were led by a Vandal king. A few years later, Claudius Mamertinus mentions them along with the Alamanni, a Suebic people. These two peoples had moved into the Agri Decumates on the eastern side of the Rhine, an area today referred to still as Swabia, at times attacking Roman Gaul together and sometimes fighting each other. He also mentions that the Goths had previously defeated the Burgundians.
Ammianus Marcellinus, on the other hand, claimed that the Burgundians were descended from Romans. The Roman sources do not speak of any specific migration from Poland by the Burgundians (although other Vandalic peoples are more clearly mentioned as having moved west in this period), and so there have historically been some doubts about the link between the eastern and western Burgundians.
In 369/370, the Emperor Valentinian I enlisted the aid of the Burgundians in his war against the Alemanni.
Approximately four decades later, the Burgundians appear again. Following Stilicho's withdrawal of troops to fight Alaric I the Visigoth in AD 406-408, the northern tribes crossed the Rhine and entered the Empire in the "Völkerwanderung", or Germanic migrations. Among them were the Alans, Vandals, the Suevi, and possibly some Burgundians. A part of Burgundians migrated westwards and settled as "foederati" in the Roman province of Germania Secunda along the Middle Rhine. Another part of Burgundians stayed in their previous homeland in Oder-Vistula interfluvial and formed a contingent in Attila's Hunnic army by 451.
Kingdom.
Establishment.
In 411, the Burgundian king Gundahar (or "Gundicar") set up a puppet emperor, Jovinus, in cooperation with Goar, king of the Alans. With the authority of the Gallic emperor that he controlled, Gundahar settled on the left (Roman) bank of the Rhine, between the river Lauter and the Nahe, seizing Worms, Speyer, and Strassburg. Apparently as part of a truce, the Emperor Honorius later officially "granted" them the land, (Prosper, a. 386) with its capital at the old Celtic Roman settlement of Borbetomagus (present Worms).
Despite their new status as "foederati", Burgundian raids into Roman Upper Gallia Belgica became intolerable and were ruthlessly brought to an end in 436, when the Roman general Aëtius called in Hun mercenaries who overwhelmed the Rhineland kingdom in 437. Gundahar was killed in the fighting, reportedly along with the majority of the Burgundian tribe. (Prosper; "Chronica Gallica 452"; Hydatius; and Sidonius Apollinaris)
The destruction of Worms and the Burgundian kingdom by the Huns became the subject of heroic legends that were afterwards incorporated in the "Nibelungenlied"—on which Wagner based his Ring Cycle—where King Gunther (Gundahar) and Queen Brünhild hold their court at Worms, and Siegfried comes to woo Kriemhild. (In Old Norse sources the names are "Gunnar", "Brynhild", and "Gudrún" as normally rendered in English.) In fact, the "Etzel" of the "Nibelungenlied" is based on Attila the Hun.
Settlement in Savoy.
For reasons not cited in the sources, the Burgundians were granted "foederati" status a second time, and in 443 were resettled by Aëtius in the region of "Sapaudia". ("Chronica Gallica 452") Though the precise geography is uncertain, "Sapaudia" corresponds to the modern-day Savoy, and the Burgundians probably lived near "Lugdunum", known today as Lyon. (Wood 1994, Gregory II, 9) A new king Gundioc or "Gunderic", presumed to be Gundahar's son, appears to have reigned following his father's death. (Drew, p. 1) The historian Pline tells us that Gonderic reigned the areas of Saône, Dauphiny, Savoie and a part of Provence. He set up Vienne as the capital of the kingdom of Burgundy. In all, eight Burgundian kings of the house of Gundahar ruled until the kingdom was overrun by the Franks in 534.
As allies of Rome in its last decades, the Burgundians fought alongside Aëtius and a confederation of Visigoths and others in the battle against Attila at the Battle of Châlons (also called "The Battle of the Catalaunian Fields") in 451. The alliance between Burgundians and Visigoths seems to have been strong, as Gundioc and his brother Chilperic I accompanied Theodoric II to Spain to fight the Sueves in 455. (Jordanes, "Getica", 231)
Aspirations to the Empire.
Also in 455, an ambiguous reference "infidoque tibi Burdundio ductu" (Sidonius Apollinaris in "Panegyr. Avit". 442.) implicates an unnamed treacherous Burgundian leader in the murder of the emperor Petronius Maximus in the chaos preceding the sack of Rome by the Vandals. The Patrician Ricimer is also blamed; this event marks the first indication of the link between the Burgundians and Ricimer, who was probably Gundioc's brother-in-law and Gundobad's uncle, (John Malalas, 374)
The Burgundians, apparently confident in their growing power, negotiated in 456 a territorial expansion and power sharing arrangement with the local Roman senators. (Marius of Avenches)
In 457, Ricimer overthrew another emperor, Avitus, raising Majorian to the throne. This new emperor proved unhelpful to Ricimer and the Burgundians. The year after his ascension, Majorian stripped the Burgundians of the lands they had acquired two years earlier. After showing further signs of independence, he was murdered by Ricimer in 461.
Ten years later, in 472, Ricimer–who was by now the son-in-law of the Western Emperor Anthemius–was plotting with Gundobad to kill his father-in-law; Gundobad beheaded the emperor (apparently personally). ("Chronica Gallica 511"; John of Antioch, fr. 209; Jordanes, "Getica", 239) Ricimer then appointed Olybrius; both died, surprisingly of natural causes, within a few months. Gundobad seems then to have succeeded his uncle as Patrician and king-maker, and raised Glycerius to the throne. (Marius of Avenches; John of Antioch, fr. 209)
In 474, Burgundian influence over the empire seems to have ended. Glycerius was deposed in favor of Julius Nepos, and Gundobad returned to Burgundy, presumably at the death of his father Gundioc. At this time or shortly afterward, the Burgundian kingdom was divided between Gundobad and his brothers, Godigisel, Chilperic II, and Gundomar I. (Gregory, II, 28)
Consolidation of the Kingdom.
According to Gregory of Tours, the years following Gundobad's return to Burgundy saw a bloody consolidation of power. Gregory states that Gundobad murdered his brother Chilperic, drowning his wife and exiling their daughters (one of whom was to become the wife of Clovis the Frank, and was reputedly responsible for his conversion). This is contested by, e.g., Bury, who points out problems in much of Gregory's chronology for the events.
C.500, when Gundobad and Clovis were at war, Gundobad appears to have been betrayed by his brother Godegisel, who joined the Franks; together Godegisel's and Clovis' forces "crushed the army of Gundobad." (Marius a. 500; Gregory, II, 32) Gundobad was temporarily holed up in Avignon, but was able to re-muster his army and sacked Vienne, where Godegisel and many of his followers were put to death. From this point, Gundobad appears to have been the sole king of Burgundy. (e.g., Gregory, II, 33) This would imply that his brother Gundomar was already dead, though there are no specific mentions of the event in the sources.
Either Gundobad and Clovis reconciled their differences, or Gundobad was forced into some sort of vassalage by Clovis' earlier victory, as the Burgundian king appears to have assisted the Franks in 507 in their victory over Alaric II the Visigoth.
During the upheaval, sometime between 483-501, Gundobad began to set forth the "Lex Gundobada" (see below), issuing roughly the first half, which drew upon the "Lex Visigothorum". (Drew, p. 1) Following his consolidation of power, between 501 and his death in 516, Gundobad issued the second half of his law, which was more originally Burgundian.
Fall.
The Burgundians were extending their power over southeastern Gaul; that is, northern Italy, western Switzerland, and southeastern France. In 493 Clovis, king of the Franks, married the Burgundian princess Clotilda (daughter of Chilperic), who converted him to the Catholic faith.
At first allied with Clovis' Franks against the Visigoths in the early 6th century, the Burgundians were eventually conquered at Autun by the Franks in 532 after a first attempt in the Battle of Vézeronce. The Burgundian kingdom was made part of the Merovingian kingdoms, and the Burgundians themselves were by and large absorbed as well.
Language.
The Burgundian language belonged to the East Germanic language group. It appears to have become extinct during the late sixth century.
Little is known of the language. Some proper names of Burgundians are recorded, and some words used in the area in modern times are thought to be derived from the ancient Burgundian language, but it is often difficult to distinguish these from Germanic words of other origin, and in any case the modern form of the words is rarely suitable to infer much about the form in the old language.
Culture.
Religion.
Somewhere in the east the Burgundians had converted to the Arian form of Christianity from their native Germanic polytheism. Their Arianism proved a source of suspicion and distrust between the Burgundians and the Catholic Western Roman Empire. Divisions were evidently healed or healing circa AD 500, however, as Gundobad, one of the last Burgundian kings, maintained a close personal friendship with Avitus, the bishop of Vienne. Moreover, Gundobad's son and successor, Sigismund, was himself a Catholic, and there is evidence that many of the Burgundian people had converted by this time as well, including several female members of the ruling family.
Law.
The Burgundians left three legal codes, among the earliest from any of the Germanic tribes.
The Liber Constitutionum sive Lex Gundobada ("The Book of the Constitution following the Law of Gundobad"), also known as the "Lex Burgundionum", or more simply the "Lex Gundobada" or the "Liber", was issued in several parts between 483 and 516, principally by Gundobad, but also by his son, Sigismund. (Drew, p. 6–7) It was a record of Burgundian customary law and is typical of the many Germanic law codes from this period. In particular, the "Liber" borrowed from the "Lex Visigothorum" (Drew, p. 6) and influenced the later "Lex Ribuaria". (Rivers, p. 9) The "Liber" is one of the primary sources for contemporary Burgundian life, as well as the history of its kings.
Like many of the Germanic tribes, the Burgundians' legal traditions allowed the application of separate laws for separate ethnicities. Thus, in addition to the "Lex Gundobada", Gundobad also issued (or codified) a set of laws for Roman subjects of the Burgundian kingdom, the "Lex Romana Burgundionum" ("The Roman Law of the Burgundians").
In addition to the above codes, Gundobad's son Sigismund later published the "Prima Constitutio". 

</doc>
<doc id="4260" url="http://en.wikipedia.org/wiki?curid=4260" title="Dots and Boxes">
Dots and Boxes

Dots and Boxes (also known as Boxes, Squares, Paddocks, Pigs in a Pen, Square-it, Dots and Dashes, Dots, Smart Dots, Dot Boxing, or, simply, the Dot Game) is a pencil and paper game for two players (or sometimes, more than two) first published in 1889 by Édouard Lucas.
Starting with an empty grid of dots, players take turns, adding a single horizontal or vertical line between two unjoined adjacent dots. A player who completes the fourth side of a 1×1 box earns one point and takes another turn. (The points are typically recorded by placing in the box an identifying mark of the player, such as an initial). The game ends when no more lines can be placed. The winner of the game is the player with the most points.
The board may be of any size. When short on time, 2×2 boxes (created by a square of 9 dots) is good for beginners, and 5×5 is good for experts.
The diagram on the right shows a game being played on the 2×2 board. The second player (B) plays the mirror image of the first player's move, hoping to divide the board into two pieces and tie the game. The first player (A) makes a "sacrifice" at move 7; B accepts the sacrifice, getting one box. However, B must now add another line, and connects the center dot to the center-right dot, causing the remaining boxes to be joined together in a "chain" as shown at the end of move 8. With A's next move, A gets them all, winning 3–1.
Strategy.
For most novice players, the game begins with a phase of more-or-less random connecting of dots, where the only strategy is to avoid adding the third side to any box. This continues until all the remaining (potential) boxes are joined together into "chains" – groups of one or more adjacent boxes in which any move gives all the boxes in the chain to the opponent. At this point, players typically take all available boxes, then open the smallest available chain to their opponent. For example, a novice player faced with a situation like position 1 in the diagram on the left, in which some boxes can be captured, may take all the boxes in the chain, resulting in position 2. But with their last move, they have to open the next (and larger) chain, and the novice loses the game.
A more experienced player faced with position 1 will instead play the "double-cross strategy", taking all but 2 of the boxes in the chain and leaving position 3. The opponent will take these last two boxes, but will then be forced to open the next chain. By moving to position 3, player A wins. The same double-cross strategy applies however many long chains there are: a player using this strategy will take all but two of the boxes in each chain, and take all the boxes in the last chain. If the chains are long enough then this player will certainly win.
The next level of strategic complexity, between experts who would both use the double-cross strategy if they were allowed to do so, is a battle for "control": An expert player tries to force their opponent to be the one to open the first long chain, because the player who first opens a long chain usually loses. Against a player who doesn't understand the concept of a sacrifice, the expert simply has to make the correct number of sacrifices to encourage the opponent to hand him the first chain long enough to ensure a win. If the other player also knows to offer sacrifices, the expert also has to manipulate the number of available sacrifices through earlier play.
In combinatorial game theory dots and boxes is an impartial game, and many positions can be analyzed using Sprague–Grundy theory. However, dots and boxes lacks the normal play convention of most impartial games where the last player to move wins, which complicates the analysis considerably.
Unusual grids.
Dots and boxes need not be played on a rectangular grid. It can be played on a triangular grid or a hexagonal grid. There is also a variant in Bolivia when it is played in a Chakana or Inca Cross grid, which adds more complications to the game.
Dots-and-boxes has a dual form called "strings-and-coins". This game is played on a network of coins (vertices) joined by strings (edges). Players take turns to cut a string. When a cut leaves a coin with no strings, the player pockets the coin and takes another turn. The winner is the player who pockets the most coins. Strings-and-coins can be played on an arbitrary graph.
A variant played in Poland allows a player to claim a region of several squares as soon as its boundary is completed.
In the Netherlands it is called "kamertje verhuren" ("rent-a-room"), where the outer border already has lines.
Board game.
A board game version of Dots and Boxes is available through Shapeways. Instead of drawing lines on paper players place tiles in slots edge up in a specially designed board. When a spot on the board is surrounded by a player they place one of their tiles on the board with their side facing up. The board does not come with playing pieces but was designed to be used with "Scrabble" tiles. One player plays with the lettered side and the other is represented by the blank side.

</doc>
<doc id="4261" url="http://en.wikipedia.org/wiki?curid=4261" title="Big Brother (Nineteen Eighty-Four)">
Big Brother (Nineteen Eighty-Four)

Big Brother is a fictional character in George Orwell's novel "Nineteen Eighty-Four". He is the enigmatic dictator of Oceania, a totalitarian state wherein the ruling Party wields total power "for its own sake" over the inhabitants.
In the society that Orwell describes, every citizen is under constant surveillance by the authorities, mainly by telescreens(with the exception of the Proles). The people are constantly reminded of this by the phrase "Big Brother is watching you", the maxim ubiquitous on display. However, in the nature of doublethink, this phrase is also meant to mean that Big Brother is a benevolent protector of all citizens.
Since the publication of "Nineteen Eighty-Four", the term "Big Brother" has entered the lexicon as a synonym for abuse of government power, particularly in respect to civil liberties, often specifically related to mass surveillance.
Purported origins.
In the essay section of his novel "1985", Anthony Burgess states that Orwell got the idea for Big Brother from advertising billboards for educational correspondence courses from a company called "Bennett's", current during World War II. The original posters showed J. M. Bennett himself: a kindly-looking old man offering guidance and support to would-be students with the phrase "Let me be your father" attached. After Bennett's death, his son took over the company, and the posters were replaced with pictures of the son (who looked imposing and stern in contrast to his father's kindly demeanour) with the text "Let me be your big brother."
Additional speculation from Douglas Kellner of UCLA argued that Big Brother represents Joseph Stalin.
Appearance in the novel.
Existence.
In the novel, it is never made totally clear whether Big Brother is or had been a real person, or was simply a creation by the Party to personify itself. 
In Party propaganda, Big Brother is presented as one of the founders of the Party, along with Goldstein. At one point, Winston Smith, the protagonist of Orwell's novel, tries "to remember in what year he had first heard mention of Big Brother. He thought it must have been at some time in the sixties, but it was impossible to be certain. In the Party histories, of course, Big Brother figured as the leader and guardian of the Revolution since its very earliest days. His exploits had been gradually pushed backwards in time until already they extended into the fabulous world of the forties and the thirties, when the capitalists in their strange cylindrical hats still rode through the streets of London..." In the year 1984, Big Brother appears on posters and telescreens as a handsome man in his mid-40s, but he may be long dead, if he ever existed at all.
In the book "The Theory and Practice of Oligarchical Collectivism", read by Winston Smith and purportedly written by Goldstein, Big Brother is referred to as infallible and all-powerful. No-one has ever seen him and there is a reasonable certainty that he will never die. He is simply "the guise in which the Party chooses to exhibit itself to the world", since the emotions of love, fear and reverence are more easily focussed on an individual (if only a face on the hoardings and a voice on the telescreens), than an organisation. When Winston Smith is later arrested, O'Brien repeats that Big Brother will never die. When Smith asks if Big Brother exists, O'Brien describes him as "the embodiment of the Party" and says that he will exist as long as the Party exists. When Winston asks "Does Big Brother exist the same way I do?" (meaning is Big Brother an actual human being), O'Brien replies "You do not exist" (meaning that Smith is now an unperson; an example of doublethink).
Cult of personality.
A spontaneous ritual of devotion to Big Brother ("BB") is illustrated at the end of the "Two Minutes Hate":
Though Oceania's Ministry of Truth, Ministry of Plenty, and Ministry of Peace each have names with meanings deliberately opposite to their real purpose, the Ministry of Love is perhaps the most straightforward: "rehabilitated thought criminals" leave the Ministry as loyal subjects who have been brainwashed into adoring Big Brother.
Legacy.
Since the publication of "Nineteen Eighty-Four" the phrase "Big Brother" has come into common use to describe any prying or overly-controlling authority figure, and attempts by government to increase surveillance.
Ukrainian-American comedian Yakov Smirnoff makes frequent reference to both Big Brother and other Orwellian traits in his Russian Reversal jokes.
The magazine "Book" ranked Big Brother No. 59 on its 100 Best Characters in Fiction Since 1900 list. "Wizard" magazine rated him the 75th greatest villain of all time.
The worldwide reality television show "Big Brother" is based on the novel's concept of people being under constant surveillance. In 2000, after the U.S. version of the CBS program "Big Brother" premiered, the Estate of George Orwell sued CBS and its production company "Orwell Productions, Inc." in federal court in Chicago for copyright and trademark infringement. The case was "Estate of Orwell v. CBS", 00-c-5034 (ND Ill). On the eve of trial, the case settled worldwide to the parties' "mutual satisfaction"; the amount that CBS paid to the Orwell Estate was not disclosed. CBS had not asked the Estate for permission. Under current laws the novel will remain under copyright protection until 2020 in the European Union and until 2044 in the United States.
The iconic image of Big Brother (played by David Graham) played a key role in Apple's 1984 television commercial introducing the Macintosh. The Orwell Estate viewed the Apple commercial as a copyright infringement, and sent a cease-and-desist letter to Apple and its advertising agency. The commercial was never televised again. Subsequent (now posthumous) ads featuring Steve Jobs (for a variety of products including audio books) have mimicked the format and appearance of that original ad campaign, with the appearance of Steve Jobs nearly identical to that of Big Brother. In 2008, the Simpsons animated television series spoofed the Apple Big Brother commercial in an episode entitled "Mypods and Boomsticks."
The December 2002 issue of "Gear" magazine featured a story about technologies and trends that could violate personal privacy moving society closer to a "Big Brother" state and utilised a recreation of the movie poster from the film version of "1984" created by Dallmeierart.com.
In 2011, media analyst and political activist Mark Dice published a non-fiction book titled "Big Brother: The Orwellian Nightmare Come True" which analyses the parallels between elements of the storyline in "Nineteen Eighty-Four", and current government programs, technology, and cultural trends.
Computer company Microsoft patented in 2011 a product distribution system with a camera or capture device that monitors the viewers that consume the product, allowing the provider to take "remedial action" if the actual viewers do not match the distribution license. The system has been compared with "1984"'s telescreen surveillance system.

</doc>
<doc id="4266" url="http://en.wikipedia.org/wiki?curid=4266" title="Binary search algorithm">
Binary search algorithm

In computer science, a binary search or half-interval search algorithm finds the position of a specified input value (the search "key") within an array sorted by key value. For binary search, the array should be arranged in ascending or descending order. In each step, the algorithm compares the search key value with the key value of the middle element of the array. If the keys match, then a matching element has been found and its index, or position, is returned. Otherwise, if the search key is less than the middle element's key, then the algorithm repeats its action on the sub-array to the left of the middle element or, if the search key is greater, on the sub-array to the right. If the remaining array to be searched is empty, then the key cannot be found in the array and a special "not found" indication is returned.
A binary search halves the number of items to check with each iteration, so locating an item (or determining its absence) takes logarithmic time. A binary search is a dichotomic divide and conquer search algorithm.
Overview.
Searching a sorted collection is a common task. A dictionary is a sorted list of word definitions. Given a word, one can find its definition. A telephone book is a sorted list of people's names, addresses, and telephone numbers. Knowing someone's name allows one to quickly find their telephone number and address.
If the list to be searched contains more than a few items (a dozen, say) a binary search will require far fewer comparisons than a linear search, but it imposes the requirement that the list be sorted. Similarly, a hash search can be faster than a binary search but imposes still greater requirements. If the contents of the array are modified between searches, maintaining these requirements may even take more time than the searches. And if it is known that some items will be searched for "much" more often than others, "and" it can be arranged so that these items are at the start of the list, then a linear search may be the best.
More generally, algorithm allows searching over argument of any monotonic function for a point, at which function reaches the arbitrary value (enclosed between minimum and maximum at the given range).
Examples.
Example: The list to be searched: L = 1 3 4 6 8 9 11. The value to be found: X = 4.
 Compare X to 6. X is smaller. Repeat with L = 1 3 4.
 Compare X to 3. X is bigger. Repeat with L = 4.
 Compare X to 4. They are equal. We're done, we found X.
This is called Binary Search: each iteration of (1)-(4) the length of the list we are looking in gets cut in half; therefore, the total number of iterations cannot be greater than logN.
Number guessing game.
This rather simple game begins something like "I'm thinking of an integer between forty and sixty inclusive, and to your guesses I'll respond 'Higher', 'Lower', or 'Yes!' as might be the case."
Supposing that "N" is the number of possible values (here, twenty-one, as "inclusive" was stated), then at most formula_1 questions are required to determine the number, since each question halves the search space. Note that one less question (iteration) is required than for the general algorithm, since the number is already constrained to be within a particular range.
Even if the number to guess can be arbitrarily large, in which case there is no upper bound "N", the number can be found in at most formula_2 steps (where "k" is the (unknown) selected number) by first finding an upper bound with one-sided binary search. For example, if the number were 11, the following sequence of guesses could be used to find it: 1 (Higher), 2 (Higher), 4 (Higher), 8 (Higher), 16 (Lower), 12 (Lower), 10 (Higher). Now we know that the number must be 11 because it is higher than 10 and lower than 12.
One could also extend the method to include negative numbers; for example the following guesses could be used to find −13: 0, −1, −2, −4, −8, −16, −12, −14. Now we know that the number must be −13 because it is lower than −12 and higher than −14.
Word lists.
People typically use a mixture of the binary search and interpolative search algorithms when searching a telephone book, after the initial guess we exploit the fact that the entries are sorted and can rapidly find the required entry. For example when searching for Smith, if Rogers and Thomas have been found, one can flip to a page about halfway between the previous guesses. If this shows Samson, it can be concluded that Smith is somewhere between the Samson and Thomas pages so these can be divided.
Applications to complexity theory.
Even if we do not know a fixed range the number "k" falls in, we can still determine its value by asking formula_3 simple yes/no questions of the form "Is "k" greater than "x"?" for some number "x". As a simple consequence of this, if you can answer the question "Is this integer property "k" greater than a given value?" in some amount of time then you can find the value of that property in the same amount of time with an added factor of formula_4. This is called a "reduction", and it is because of this kind of reduction that most complexity theorists concentrate on decision problems, algorithms that produce a simple yes/no answer.
For example, suppose we could answer "Does this "n" x "n" matrix have permanent larger than "k"?" in O("n2") time. Then, by using binary search, we could find the (ceiling of the) permanent itself in O("n2" log "p") time, where "p" is the value of the permanent. Notice that "p" is not the size of the input, but the "value" of the output; given a matrix whose maximum item (in absolute value) is "m", "p" is bounded by formula_5. Hence log "p" = O("n" log "n" + log "m"). A binary search could find the permanent in O("n"3 log "n" + "n"2 log "m").
Algorithm.
Recursive.
A straightforward implementation of binary search is recursive. The initial call uses the indices of the entire array to be searched. The procedure then calculates an index midway between the two indices, determines which of the two subarrays to search, and then does a recursive call to search that subarray. Each of the calls is tail recursive, so a compiler need not make a new stack frame for each call. The variables codice_1 and codice_2 are the lowest and highest inclusive indices that are searched.
It is invoked with initial codice_1 and codice_2 values of codice_5 and codice_6 for a zero based array of length N.
The number type "int" shown in the code has an influence on how the midpoint calculation can be implemented correctly. With unlimited numbers, the midpoint can be calculated as codice_7. In practical programming, however, the calculation is often performed with numbers of a limited range, and then the intermediate result codice_8 might overflow. With limited numbers, the midpoint can be calculated correctly as codice_9.
Iterative.
The binary search algorithm can also be expressed iteratively with two index limits that progressively narrow the search range.
Deferred detection of equality.
The above iterative and recursive versions take three paths based on the key comparison: one path for less than, one path for greater than, and one path for equality. (There are two conditional branches.) The path for equality is taken only when the record is finally matched, so it is rarely taken. That branch path can be moved outside the search loop in the deferred test for equality version of the algorithm. The following algorithm uses only one conditional branch per iteration.
The deferred detection approach foregoes the possibility of early termination on discovery of a match, so the search will take about log2("N") iterations. On average, a "successful" early termination search will not save many iterations. For large arrays that are a power of 2, the savings is about two iterations. Half the time, a match is found with one iteration left to go; one quarter the time with two iterations left, one eighth with three iterations, and so forth. The infinite series sum is 2.
The deferred detection algorithm has the advantage that if the keys are not unique, it returns the smallest index (the starting index) of the region where elements have the search key. The early termination version would return the first match it found, and that match might be anywhere in region of equal keys.
Performance.
With each test that fails to find a match at the probed position, the search is continued with one or other of the two sub-intervals, each at most half the size. More precisely, if the number of items, "N", is odd then both sub-intervals will contain ("N"−1)/2 elements, while if "N" is even then the two sub-intervals contain "N"/2−1 and "N"/2 elements.
If the original number of items is "N" then after the first iteration there will be at most "N"/2 items remaining, then at most "N"/4 items, at most "N"/8 items, and so on. In the worst case, when the value is not in the list, the algorithm must continue iterating until the span has been made empty; this will have taken at most ⌊log2("N")+1⌋ iterations, where the ⌊ ⌋ notation denotes the floor function that rounds its argument down to an integer. This worst case analysis is tight: for any "N" there exists a query that takes exactly ⌊log2("N")+1⌋ iterations. When compared to linear search, whose worst-case behaviour is "N" iterations, we see that binary search is substantially faster as "N" grows large. For example, to search a list of one million items takes as many as one million iterations with linear search, but never more than twenty iterations with binary search. However, a binary search can only be performed if the list is in sorted order.
Average performance.
log2("N")−1 is the expected number of probes in an average successful search, and the worst case is log2("N"), just one more probe. If the list is empty, no probes at all are made.
Thus binary search is a logarithmic algorithm and executes in O(log "N") time. In most cases it is considerably faster than a linear search. It can be implemented using iteration, or recursion. In some languages it is more elegantly expressed recursively; however, in some C-based languages tail recursion is not eliminated and the recursive version requires more stack space.
Binary search can interact poorly with the memory hierarchy (i.e. caching), because of its random-access nature. For in-memory searching, if the span to be searched is small, a linear search may have superior performance simply because it exhibits better locality of reference. For external searching, care must be taken or each of the first several probes will lead to a disk seek. A common method is to abandon binary searching for linear searching as soon as the size of the remaining span falls below a small value such as 8 or 16 or even more in recent computers. The exact value depends entirely on the machine running the algorithm.
Notice that for multiple searches "with a fixed value for N", then (with the appropriate regard for integer division), the first iteration always selects the middle element at "N"/2, and the second always selects either "N"/4 or 3"N"/4, and so on. Thus if the array's key values are in some sort of slow storage (on a disc file, in virtual memory, not in the cpu's on-chip memory), keeping those three keys in a local array for a special preliminary search will avoid accessing widely separated memory. Escalating to seven or fifteen such values will allow further levels at not much cost in storage. On the other hand, if the searches are frequent and not separated by much other activity, the computer's various storage control features will more or less automatically promote frequently accessed elements into faster storage.
When multiple binary searches are to be performed for the same key in related lists, fractional cascading can be used to speed up successive searches after the first one.
Even though in theory binary search is almost always faster than linear search, in practice even on small arrays (around 64 items or less) it might be infeasible to ever use binary search. On large unsorted arrays, it only makes sense to binary search if the number of searches is large enough, because the initial time to sort the array is comparable to log(n) linear searches 
Variations.
There are many, and they are easily confused.
Exclusive or inclusive bounds.
The most significant differences are between the "exclusive" and "inclusive" forms of the bounds. In the "exclusive" bound form the span to be searched is "(L+1)" to "(R−1)", and this may seem clumsy when the span to be searched could be described in the "inclusive" form, as "L" to "R". Although the details differ the two forms are equivalent as can be seen by transforming one version into the other. The inclusive bound form can be attained by replacing all appearances of "L" by "(L−1)" and "R" by "(R+1)" then rearranging. Thus, the initialisation of "L := 0" becomes "(L−1) := 0" or "L := 1", and "R := N+1" becomes "(R+1) := N+1" or "R := N". So far so good, but note now that the changes to "L" and "R" are no longer simply transferring the value of "p" to "L" or "R" as appropriate but now must be "(R+1) := p" or "R := p−1", and "(L−1) := p" or "L := p+1".
Thus, the gain of a simpler initialisation, done once, is lost by a more complex calculation, and which is done for every iteration. If that is not enough, the test for an empty span is more complex also, as compared to the simplicity of checking that the value of "p" is zero. Nevertheless, the inclusive bound form is found in many publications, such as Donald Knuth. "The Art of Computer Programming", Volume 3: "Sorting and Searching", Third Edition.
Another common variation uses inclusive bounds for the left bound, but exclusive bounds for the right bound. This is derived from the fact that the bounds in a language with zero-based arrays can be simply initialized to 0 and the size of the array, respectively. This mirrors the way array slices are represented in some programming languages.
Midpoint and width.
A different variation involves abandoning the "L" and "R" pointers and using a current position "p" and a width "w". At each iteration, the position "p" is adjusted and the width "w" is halved. Knuth states, "It is possible to do this, but only if extreme care is paid to the details."
Search domain.
There is no particular requirement that the array being searched has the bounds 1 to "N". It is possible to search a specified range, elements "first" to "last" instead of 1 to "N". All that is necessary is that the initialization of the bounds be "L := first−1" and "R := last+1", then all proceeds as before.
The elements of the list are not necessarily all unique. If one searches for a value that occurs multiple times in the list, the index returned will be of the first-encountered equal element, and this will not necessarily be that of the first, last, or middle element of the run of equal-key elements but will depend on the positions of the values. Modifying the list even in seemingly unrelated ways such as adding elements elsewhere in the list may change the result.
If the location of the first and/or last equal element needs to be determined, this can be done efficiently with a variant of the binary search algorithms which perform only one inequality test per iteration. See deferred detection of equality.
Noisy search.
Several algorithms closely related to or extending binary search exist. For instance, noisy binary search solves the same class of projects as regular binary search, with the added complexity that any given test can return a false value at random. (Usually, the number of such erroneous results are bounded in some way, either in the form of an average error rate, or in the total number of errors allowed per element in the search space.) Optimal algorithms for several classes of noisy binary search problems have been known since the late seventies, and more recently, optimal algorithms for noisy binary search in quantum computers (where several elements can be tested at the same time) have been discovered.
Exponential search.
An exponential search (also called a one-sided search) searches from a starting point within the array and either expects that the element formula_6 being sought is nearby or the upper (lower) bound on the array is unknown. Starting with a step size of 1 and doubling with each step, the method looks for a number >= (<=) formula_6. Once the upper (lower) bound is found, then the method proceeds with a binary search. The complexity of the search is formula_8 if the sought element is in the "n"th array position. This depends only on formula_9 and not on the size of the array.
Interpolated search.
An interpolated search tries to guess the location of the element formula_6 you're searching for, typically by calculating a midpoint based on the lowest and highest value and assuming a fairly even distribution of values. When formula_6 has been determined an exponential search is performed.
Implementation issues.
Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky… — Donald Knuth
When Jon Bentley assigned it as a problem in a course for professional programmers, he found that an astounding ninety percent failed to code a binary search correctly after several hours of working on it, and another study shows that accurate code for it is only found in five out of twenty textbooks. Furthermore, Bentley's own implementation of binary search, published in his 1986 book "Programming Pearls", contains an error that remained undetected for over twenty years.
Arithmetic.
In a practical implementation, the variables used to represent the indices will often be of finite size, hence only capable of representing a finite range of values. For example, 32-bit unsigned integers can only hold values from 0 to 4294967295. 32-bit signed integers can only hold values from -2147483648 to 2147483647. If the binary search algorithm is to operate on large arrays, this has three implications:
Language support.
Many standard libraries provide a way to do a binary search:

</doc>
<doc id="4267" url="http://en.wikipedia.org/wiki?curid=4267" title="Belle and Sebastian">
Belle and Sebastian

Belle and Sebastian are an indie pop band formed in Glasgow in January 1996. They are often compared with acts such as The Smiths and Nick Drake. The name "Belle and Sebastian" comes from "Belle et Sébastien", a 1965 children's book by French writer Cécile Aubry later adapted for television. Though consistently lauded by critics, Belle and Sebastian's "wistful pop" has enjoyed only limited commercial success.
After releasing a number of albums and EPs on Jeepster Records, they are now signed to Rough Trade Records in the United Kingdom and Matador Records in the United States.
History.
Formation and early years (1996–1998).
Belle and Sebastian were formed in Glasgow, Scotland in 1996 by Stuart Murdoch and Stuart David. Together, with Stow College music professor Alan Rankine, they recorded some demos; these demos were picked up by the college's Music Business course that produces and releases one single each year on the college's label, Electric Honey. As the band had a number of songs already and the label was extremely impressed with the demos, Belle and Sebastian were allowed to record a full-length album, which was titled "Tigermilk". Murdoch once described the band as a "product of botched capitalism".
"Tigermilk" was recorded in three days and originally only one thousand copies were pressed on vinyl. These original copies now sell for up to £400. The warm reception the album received inspired Murdoch and David to turn the band into a full-time project, recruiting Stevie Jackson (guitar and vocals), Isobel Campbell (cello/vocals), Chris Geddes (keys) and Richard Colburn (drums) to fill out the group.
After the success of the debut album, Belle and Sebastian were signed to Jeepster Records in August 1996 and "If You're Feeling Sinister", their second album, was released on 18 November. The album was named by "Spin" as one of the 100 greatest albums between 1985 and 2005, and is widely considered the band's masterpiece. Just before the recording of "Sinister", Sarah Martin (violin/vocals) joined the band. Following this a series of EPs were released in 1997. The first of these was "Dog on Wheels", which contained four demo tracks recorded before the real formation of the band. In fact, the only long-term band members to play on the songs were Murdoch, David, and Mick Cooke, who played trumpet on the EP but would not officially join the band until a few years later. It charted at No. 59 in the UK singles chart.
The "Lazy Line Painter Jane" EP followed in July. The track was recorded in the church where Murdoch lived and features vocals from Monica Queen. The EP narrowly missed out of the UK top 40, peaking at No. 41. The last of the 1997 EPs was October's "3.. 6.. 9 Seconds of Light". The EP was made Single of the Week in both the "NME" and "Melody Maker" and reached No. 32 in the charts, thus becoming the band's first top 40 single.
Critical acclaim (1998–2000).
The band released their third LP, "The Boy with the Arab Strap" in 1998, and it reached No. 12 in the UK charts. "Arab Strap" is often described by critics as the band's best album, and garnered positive reviews from "Rolling Stone" and the "Village Voice," among others; however, the album has its detractors, including "Pitchfork Media", who gave the album a particularly poor review, calling it a "parody" of their earlier work (Pitchfork has since removed the review from their website). During the recording of the album, long-time studio trumpet-player Mick Cooke was asked to join the band as a full member. The "This Is Just a Modern Rock Song" EP followed later that year.
In 1999 the band was awarded with Best Newcomer (for their third album) at the BRIT Awards, upsetting better-known acts such as Steps and 5ive. That same year, the band hosted their own festival, the Bowlie Weekender. "Tigermilk" was also given a full release by Jeepster before the band started work on their next LP. The result was "Fold Your Hands Child, You Walk Like a Peasant", which became the band's first top 10 album in the UK. A stand-alone single, "Legal Man", reached No. 15 and gave them their first appearance on Top of the Pops.
As the band's popularity and recognition was growing worldwide, their music began appearing in films and on television. The 2000 film "High Fidelity" mentions the band and features a clip from the song "Seymour Stein" from "The Boy with the Arab Strap". Also, the title track from "Arab Strap" was played over the end credits of the UK television series "Teachers".
Line-up and label changes (2000–2005).
Stuart David soon left the band to concentrate on his side project, Looper, and his book writing, which included his "The Idle Thoughts of a Daydreamer". He was replaced by Bobby Kildea of V-Twin. The "Jonathan David" single, sung by Stevie Jackson, was released in June 2001 and was followed by "I'm Waking Up to Us" in November. "I'm Waking Up to Us" saw the band use an outside producer (Mike Hurst) for the first time. Most of 2002 was spent touring and recording a soundtrack album, "Storytelling" (for "Storytelling" by Todd Solondz). Campbell left the band in the spring of 2002, in the middle of the band's North American tour.
The band left Jeepster in 2002, signing a four-album deal with Rough Trade Records. Their first album for Rough Trade, "Dear Catastrophe Waitress", was released in 2003, and was produced by Trevor Horn. The album showed a markedly more "produced" sound compared to their first four LPs, as the band was making a concerted effort to produce more "radio-friendly" music. The album was warmly received and is credited with restoring the band's "indie cred". The album also marked the return of Murdoch as the group's primary songwriter, following the poorly received "Fold Your Hands Child, You Walk Like a Peasant" and "Storytelling", both of which were more collaborative than the band's early work. A documentary DVD, "Fans Only", was released by Jeepster in October 2003, featuring promotional videos, live clips and unreleased footage. A single from the album, "Step into My Office, Baby" followed in November 2003; it would be their first single taken from an album.
The Thin Lizzy-inspired "I'm a Cuckoo" was the second single from the album. It achieved their highest chart position yet, reaching No. 14 in the UK. The "Books" EP followed, a double A-side single led by "Wrapped Up in Books" from "Dear Catastrophe Waitress" and the new "Your Cover's Blown". This EP became the band's third top 20 UK release, and the band was nominated for both the Mercury Music Prize and an Ivor Novello Award. In January 2005, B&S was voted Scotland's greatest band in a poll by The List, beating Simple Minds, Idlewild, Travis, Franz Ferdinand, and The Proclaimers, among others.
Return to success (2005–2010).
In April 2005, members of the band visited Israel and the Palestinian territories with the UK charity War on Want; the group subsequently recorded a song inspired by the trip titled "The Eighth Station of the Cross Kebab House", which would first appear on the digital-download version of the charity album and would later have a physical release as a B-side on 2006's "Funny Little Frog" single. "Push Barman to Open Old Wounds", a compilation of the Jeepster singles and EPs, was released in May 2005 while the band were recording their seventh album in California. The result of the sessions was "The Life Pursuit", produced by Tony Hoffer. The album, originally intended to be a double album, became the band's highest-charting album upon its release in February 2006, peaking at No. 8 in the UK and No. 65 on the US "Billboard" 200. "Funny Little Frog", which preceded it, also proved to be their highest-charting single, debuting at No. 13.
On 6 July 2006, the band played a historic show with the Los Angeles Philharmonic at the Hollywood Bowl. The opening act at the 18,000 seat sell-out concert was The Shins. The members of the band see this as a landmark event, with Stevie Jackson saying, "This is the biggest thrill of my entire life". In October 2006, members of the band helped put together a CD collection of new songs for children titled "Colours Are Brighter", with the involvement of major bands such as Franz Ferdinand and The Flaming Lips.
On 18 November 2008 the band released "The BBC Sessions", which features songs from the period of 1996–2001 (including the last recordings featuring Isobel Campbell before she left the band), along with a second disc featuring a recording of a live performance in Belfast from Christmas 2001.
Recent years (2010–present).
On 17 July 2010, the band performed their first UK gig in almost four years to a crowd of around 30,000 at Latitude Festival in Henham Park, Southwold. They performed two new songs, "I Didn't See It Coming" and "I'm Not Living in the Real World".
Their eighth studio album, released in the UK and internationally on 25 September 2010, was titled "Belle and Sebastian Write About Love". The first single from the album, as well as the record's title track "Write About Love", was released in the US on 7 September 2010. "Write About Love" entered the UK albums chart in its first week of release, peaking at No. 8 as of 19 October 2010.
In December 2010 Belle and Sebastian curated the sequel to the "Bowlie Weekender" in the form of "Bowlie 2" presented by All Tomorrow's Parties.
In 2013, Pitchfork TV released an hour-long documentary in February, directed by RJ Bentler which focused on the band's 1996 album If You're Feeling Sinister, as well as the formation and early releases of the band. The documentary featured interviews with every member that was present on the album, as well as several archival photos and videos from the band's early days. The band compiled a second compilation album "The Third Eye Centre" which included the b-sides and rarities released after "Push Barman to Open Old Wounds", from the albums "Dear Catastrophe Waitress", "The Life Pursuit", and "Write About Love". In an interview at the end of 2013, Mick Cooke confirmed he had left the band on good terms.
The band received an 'Outstanding Contribution To Music Award' at the NME Awards 2014.
Recently, the band has returned to the studio, recording in Atlanta, Georgia for their ninth studio album, along with announcing tour dates for various festivals and concerts across the world during 2014.
Discography.
Compilation albums
Live albums

</doc>
<doc id="4279" url="http://en.wikipedia.org/wiki?curid=4279" title="Broadcast domain">
Broadcast domain

A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer. A broadcast domain can be within the same LAN segment or it can be bridged to other LAN segments.
In terms of current popular technologies: Any computer connected to the same Ethernet repeater or switch is a member of the same broadcast domain. Further, any computer connected to the same set of inter-connected switches/repeaters is a member of the same broadcast domain. Routers and other higher-layer devices form boundaries between broadcast domains.
This is as compared to a collision domain, which would be all nodes on the same set of inter-connected repeaters, divided by switches and learning bridges. Collision domains are generally smaller than, and contained within, broadcast domains.
While some layer two network devices are able to divide the collision domains, broadcast domains are only divided by layer 3 network devices such as routers or layer 3 switches. Separating VLANs divides broadcast domains as well, but provides no means to network these without layer 3 functionality.
Further explanation.
The distinction between broadcast and collision domains comes about because simple Ethernet and similar systems use a shared transmission system. In simple Ethernet (without switches or bridges), data frames are transmitted to all other nodes on a network. Each receiving node checks the destination address of each frame, and simply ignores any frame not addressed to its own MAC.
Switches act as buffers, receiving and analyzing the frames from each connected network segment. Frames destined for nodes connected to the originating segment are not forwarded by the switch. Frames destined for a specific node on a different segment are sent only to that segment. Only broadcast frames are forwarded to all other segments. This reduces unnecessary traffic and collisions.
In such a switched network, transmitted frames may not be received by all other reachable nodes. Nominally, only broadcast frames will be received by all other nodes. Collisions are localized to the network segment they occur on. Thus, the broadcast domain is the entire inter-connected layer two network, and the segments connected to each switch/bridge port are each a collision domain.
Not all network systems or media feature broadcast/collision domains. For example, PPP links.
Broadcast domain control.
With a sufficiently sophisticated switch, it is possible to create a network in which the normal notion of a broadcast domain is strictly controlled. One implementation of this concept is termed a "private VLAN". Another implementation is possible with Linux and iptables. One helpful analogy is that by creating multiple VLANs, the number of broadcast domains increases, but the size of each broadcast domain decreases. This is because a virtual LAN (or VLAN) is technically a broadcast domain.
This is achieved by designating one or more "server" or "provider" nodes, either by MAC address or switch port. Broadcast frames are allowed to originate from these sources, and are sent to all other nodes. Broadcast frames from all other sources are directed only to the server/provider nodes. Traffic from other sources not destined to the server/provider nodes ("peer-to-peer" traffic) is blocked.
The result is a network based on a nominally shared transmission system; like Ethernet, but in which "client" nodes cannot communicate with each other, only with the server/provider. A common application is Internet providers. Allowing direct data link layer communication between customer nodes exposes the network to various security attacks, such as ARP spoofing. Controlling the broadcast domain in this fashion provides many of the advantages of a point-to-point network, using commodity broadcast-based hardware.

</doc>
<doc id="4282" url="http://en.wikipedia.org/wiki?curid=4282" title="Beechcraft">
Beechcraft

Beechcraft Corporation is an American manufacturer of general aviation and military aircraft, ranging from light single-engined aircraft to twin-engined turboprop transports, and military trainers. A brand of Textron Aviation since 2014, it has also been a division of Raytheon and later a brand of Hawker Beechcraft.
History.
Beech Aircraft Company was founded in Wichita, Kansas, in 1932 by Walter Beech and his wife Olive Ann Beech. The company began operations in an idle Cessna factory. With designer Ted Wells, they developed the first aircraft under the Beechcraft name, the classic Model 17 Staggerwing, which first flew in November 1932. Over 750 Staggerwings were built, with 270 manufactured for the United States Army Air Forces during World War II.
Beechcraft was not Beech's first company, as he had previously formed Travel Air in 1924 and the design numbers used at Beechcraft followed the sequence started at Travel Air, and were then continued at Curtiss-Wright, after Travel Air had been absorbed into the much larger company in 1929. Beech became President of the Curtiss-Wright's airplane division and VP of sales, but became dissatisfied with being so far removed from aircraft production and quit to form Beechcraft, using the original Travel Air facilities and employing many of the same people. Model numbers prior to 11/11000 were built under the Travel Air name, while Curtiss-Wright built the CW-12, 14, 15 and 16 as well as previous successful Travel Air models (mostly the model 4).
In 1942 Beech won its first Army-Navy "E" Award production award and became one of the elite five percent of war contracting firms in the country to win five straight awards for production efficiency, mostly for the production of the Beechcraft Model 18 which remains in widespread use worldwide. Beechcraft ranked 69th among United States corporations in the value of World War II military production contracts.
After the war, the Staggerwing was replaced by the revolutionary Beechcraft Bonanza with a distinctive V-tail. Perhaps the best known Beech aircraft, the single-engined Bonanza has been manufactured in various models since 1947. The Bonanza has had the longest production run of any airplane, past or present, in the world. Other important Beech aircraft are the King Air/Super King Air line of twin-engined turboprops, in production since 1964, the Baron, a twin-engined variant of the Bonanza, and the Beechcraft Model 18, originally a business transport and commuter airliner from the late 1930s through the 1960s, which remains in active service as a cargo transport.
In 1950, Olive Ann Beech was installed as president and CEO of the company, after the sudden death of her husband from a heart attack on 29 November of that year. She continued as CEO until Beech was purchased by Raytheon Company on 8 February 1980. Ted Wells had been replaced as Chief Engineer by Herbert Rawdon, who remained at the post until his retirement in the early 1960s.
In 1994, Raytheon merged Beechcraft with the Hawker product line it had acquired in 1993 from British Aerospace, forming Raytheon Aircraft Company. In 2002, the Beechcraft brand was revived to again designate the Wichita-produced aircraft. In 2006, Raytheon sold Raytheon Aircraft to Goldman Sachs creating Hawker Beechcraft. Since its inception Beechcraft has resided in Wichita, Kansas, also the home of chief competitor Cessna, the birthplace of Learjet and of Stearman, whose trainers were used in large numbers during WW2.
The entry into bankruptcy of Hawker Beechcraft on May 3, 2012 ended with its emergence on February 16, 2013 as a new entity, Beechcraft Corporation, with the Hawker Beechcraft name being retired. The new and much smaller company will produce the King Air line of aircraft as well as the T-6 and AT-6 military trainer/attack aircraft, the piston-powered single-engined Bonanza and twin-engined Baron aircraft. The jet line was discontinued, but the new company would continue to support the aircraft already produced with parts, plus engineering and airworthiness documentation.
By October 2013, the company, now financially turned around, was up for sale.
On December 26, 2013, Textron agreed to purchase Beechcraft, including the discontinued Hawker jet line, for $1.4 billion. The sale was expected to be concluded in the first half of 2014, pending government approval. Textron CEO Scott Donnelly indicated that Beechcraft and Cessna would be combined to form a new light aircraft manufacturing concern that will result in US$65M-$85M in annual savings over keeping the companies separate. Textron's initial plan is to keep both Beechcraft and Cessna as separate brands.
Facilities.
Beech Factory Airport house Beechcraft's head office, manufacturing facility and runway for test flights.

</doc>
