<doc id="7088" url="http://en.wikipedia.org/wiki?curid=7088" title="List of cryptographers">
List of cryptographers

List of cryptographers.
Modern.
See also: for an exhaustive list.

</doc>
<doc id="7089" url="http://en.wikipedia.org/wiki?curid=7089" title="Chocolate">
Chocolate

Chocolate is a typically sweet, usually brown, food preparation of "Theobroma cacao" seeds, roasted and ground, often flavored, as with vanilla. It is made in the form of a liquid, paste or in a block or used as a flavoring ingredient in other sweet foods. Cacao has been cultivated by many cultures for at least three millennia in Mexico and Central America. The earliest evidence of use traces to the Mokaya, with evidence of chocolate beverages dating back to 1900 BC. In fact, the majority of Mesoamerican people made chocolate beverages, including the Mayans and Aztecs, who made it into a beverage known as "xocolātl" , a Nahuatl word meaning "bitter water". The seeds of the cacao tree have an intense bitter taste and must be fermented to develop the flavor.
After fermentation, the beans are dried, cleaned, and roasted. The shell is removed to produce cacao nibs, which are then ground to cocoa mass, pure chocolate in rough form. Because the cocoa mass is usually liquefied before being molded with or without other ingredients, it is called chocolate liquor. The liquor also may be processed into two components: cocoa solids and cocoa butter. Unsweetened baking chocolate (bitter chocolate) contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate consumed today is in the form of sweet chocolate, a combination of cocoa solids, cocoa butter or other fat, and sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk but no cocoa solids.
Cocoa solids are one of the richest sources of flavanol antioxidants. They also contain alkaloids such as theobromine, phenethylamine and caffeine. These have physiological effects on the body and are linked to serotonin levels in the brain. Some research has found that chocolate, eaten in moderation, can lower blood pressure. The presence of theobromine renders chocolate toxic to some animals, especially dogs and cats.
Chocolate has become one of the most popular food types and flavors in the world, and a vast number of foodstuffs involving chocolate have been created. Chocolate chip cookies have become very common, and very popular, in most parts of Europe and North America. Gifts of chocolate molded into different shapes have become traditional on certain holidays. Chocolate is also used in cold and hot beverages such as chocolate milk and hot chocolate.
Although cocoa originated in the Americas, today Western Africa produces almost two-thirds of the world's cocoa, with Côte d'Ivoire growing almost half of it.
European Union regulations require dark chocolate to have at least 60% cocoa solids, milk chocolate 25%, and white chocolate none.
Etymology.
The word "chocolate" entered the English language from Spanish. How the word came into Spanish is less certain, and there are competing explanations. Perhaps the most cited explanation is that "chocolate" comes from Nahuatl, the language of the Aztecs, from the word "chocolātl", which many sources say derived from "xocolātl" , combining "xococ", sour or bitter, and "ātl", water or drink. However, as William Bright noted, the word "chocolatl" does not occur in central Mexican colonial sources, making this an unlikely derivation. Santamaria gives a derivation from the Yucatec Maya word "chokol" meaning hot, and the Nahuatl "atl" meaning water. Sophie and Michael D. Coe agree with this etymology.
Pointing to various sources dating from the time of the Spanish conquest, they identify "cacahuatl" (cacao water) as the original Nahuatl word for the cold beverage consumed by the Aztecs. Noting that using a word with "caca" in it to describe a thick brown beverage would not have gone over well with most speakers of Spanish because "caca" means faeces in Spanish, the Coes suggest that the Spanish colonisers combined the Nahuatl "atl" with the Yucatec Maya "chocol", for, unlike the Aztec, the Maya tended to drink chocolate heated. The Spanish preferred the warm Mayan preparation of the beverage to the cold Aztec one, so the colonisers substituted "chocol" in place of the culturally unacceptable "caca".
More recently, Dakin and Wichmann derive it from another Nahuatl term, "chicolatl" from eastern Nahuatl, meaning "beaten drink". They derive this term from the word for the frothing stick, "chicoli". However, the Coes write that "xicalli" referred to the gourd out of which the beverage was consumed and that the use of a frothing stick (known as a "molinillo") was a product of creolisation between the Spanish and Aztec; the original frothing method used by the indigenous people was simply pouring the drink from a height into another vessel.
History.
Mesoamerican usage.
Chocolate has been prepared as a drink for nearly all of its history. For example, one vessel found at an Olmec archaeological site on the Gulf Coast of Veracruz, Mexico, dates chocolate's preparation by pre-Olmec peoples as early as 1750 BC. On the Pacific coast of Chiapas, Mexico, a Mokaya archaeological site provides evidence of cacao beverages dating even earlier, to 1900 BC. The residues and the kind of vessel in which they were found indicate the initial use of cacao was not simply as a beverage, but the white pulp around the cacao beans was likely used as a source of fermentable sugars for an alcoholic drink.
An early Classic-period (460–480 AD) Mayan tomb from the site in Rio Azul had vessels with the Maya glyph for cacao on them with residue of a chocolate drink, suggests the Maya were drinking chocolate around 400 AD. Documents in Maya hieroglyphs stated chocolate was used for ceremonial purposes, in addition to everyday life. Mayans grew cacao trees in their backyards, and used the cacao seeds the trees produced to make a frothy, bitter drink.
By the 15th century, the Aztecs gained control of a large part of Mesoamerica and adopted cacao into their culture. They associated chocolate with Quetzalcoatl, who, according to one legend, was cast away by the other gods for sharing chocolate with humans, and identified its extrication from the pod with the removal of the human heart in sacrifice. In contrast to the Mayans, who liked their chocolate warm, the Aztecs drank it cold, seasoning it with a broad variety of additives, including the petals of the "Cymbopetalum penduliflorum" tree, chile pepper, allspice, vanilla, and honey.
The Aztecs were not able to grow cacao themselves, as their home in the Mexican highlands was unsuitable for it, so chocolate was a luxury imported into the empire. Those who lived in areas ruled by the Aztecs were required to offer cacao seeds in payment of the tax they deemed "tribute". Cocoa beans were often used as currency. For example, the Aztecs used a system in which one turkey cost 100 cacao beans and one fresh avocado was worth three beans.
European adaptation.
Until the 16th century, no European had ever heard of the popular drink from the Central and South American peoples. Christopher Columbus and his son Ferdinand encountered the cacao bean on Columbus's fourth mission to the Americas on 15 August 1502, when he and his crew seized a large native canoe that proved to contain cacao beans among other goods for trade. Spanish conquistador Hernán Cortés may have been the first European to encounter it, as the frothy drink was part of the after-dinner routine of Montezuma. Jose de Acosta, a Spanish Jesuit missionary who lived in Peru and then Mexico in the later 16th century, wrote of its growing influence on the Spaniards:
Loathsome to such as are not acquainted with it, having a scum or froth that is very unpleasant taste. Yet it is a drink very much esteemed among the Indians, where with they feast noble men who pass through their country. The Spaniards, both men and women that are accustomed to the country are very greedy of this Chocolate. They say they make diverse sorts of it, some hot, some cold, and some temperate, and put therein much of that "chili"; yea, they make paste thereof, the which they say is good for the stomach and against the catarrh.
While Columbus had taken cacao beans with him back to Spain, chocolate made no impact until Spanish friars introduced it to the Spanish court. After the Spanish conquest of the Aztecs, chocolate was imported to Europe. There, it quickly became a court favorite. It was still served as a beverage, but the Spanish added sugar, as well as honey, to counteract the natural bitterness. By 1602, chocolate had made its way from Spain to Austria. By 1662, the bishop of Rome had declared that religious fasts were not broken by consuming chocolate drinks. Within about a hundred years, chocolate established a foothold throughout Europe.
The new craze for chocolate brought with it a thriving slave market, as between the early 1600s and late 1800s, the laborious and slow processing of the cacao bean was manual. Cacao plantations spread, as the English, Dutch, and French colonized and planted. With the depletion of Mesoamerican workers, largely to disease, cacao production was often the work of poor wage laborers and African slaves. Wind-powered and horse-drawn mills were used to speed production, but chocolate would remain a treat for the elite and the wealthy until the arrival of the Industrial Revolution brought steam-powered engines to speed the processing of the bean. The first steam-driven chocolate mill was created by a French inventor named Debuisson in the early 1700s.
As the processes for chocolate making sped the production, new techniques and approaches revolutionized the texture and taste. In 1815, Dutch chemist Coenraad Van Houten introduced alkaline salts to chocolate, which reduced its bitterness. A few years thereafter, in 1828, he created a press to remove about half the natural fat (cocoa butter or cacao butter) from chocolate liquor, which made chocolate both cheaper to produce and more consistent in quality. This innovation introduced the modern era of chocolate. Known as "Dutch cocoa", this machine-pressed chocolate was instrumental in the transformation of chocolate to its solid form when, in 1847, Joseph Fry learned to make chocolate moldable by adding back melted cacao butter. Milk had sometimes been used as an addition to chocolate beverages since the mid-17th century, but in 1875 Daniel Peter invented milk chocolate by mixing a powdered milk developed by Henri Nestlé with the liquor. In 1879, the texture and taste of chocolate was further improved when Rudolphe Lindt invented the conching machine.
Besides Nestlé, a number of notable chocolate companies had their start in the late 19th and early 20th centuries. Cadbury was manufacturing boxed chocolates in England by 1868. In 1893, Milton S. Hershey purchased chocolate processing equipment at the World's Columbian Exposition in Chicago, and soon began the career of Hershey's chocolates with chocolate-coated caramels.
Types.
Several types of chocolate can be distinguished. Pure, unsweetened chocolate contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate consumed today is in the form of sweet chocolate, combining chocolate with sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. In the U.K. and Ireland, milk chocolate must contain a minimum of 20% total dry cocoa solids; in the rest of the European Union, the minimum is 25%. "White chocolate" contains cocoa butter, sugar, and milk, but no cocoa solids. Chocolate contains alkaloids such as theobromine and phenethylamine, which have some physiological effects in humans and has been linked to serotonin levels in the brain, but the presence of theobromine renders it toxic to some animals, such as dogs and cats. Dark chocolate has been promoted for unproven health benefits, as it seems to possess substantial amounts of antioxidants that reduce the formation of free radicals.
White chocolate, although similar in texture to that of milk and dark chocolate, does not contain any cocoa solids. Because of this, many countries do not consider white chocolate as chocolate at all. Although white chocolate was first introduced by Hebert Candies in 1955, Mars, Incorporated, was the first to produce it in the United States. Because it does not contain any cocoa solids, white chocolate does not contain any theobromine, so it can be consumed by animals. 
Dark chocolate is produced by adding fat and sugar to the cacao mixture. The U.S. Food and Drug Administration calls this "sweet chocolate", and requires a 15% concentration of chocolate liquor. European rules specify a minimum of 35% cocoa solids. Dark chocolate, with its high cocoa content, is a rich source of epicatechin and gallic acid, which are thought to possess cardioprotective properties. Dark chocolate has also been said to reduce the possibility of a heart attack when consumed regularly in small amounts. Semisweet chocolate is a dark chocolate with a low sugar content. Bittersweet chocolate is chocolate liquor to which some sugar (typically a third), more cocoa butter, vanilla, and sometimes lecithin have been added. It has less sugar and more liquor than semisweet chocolate, but the two are interchangeable in baking.
Unsweetened chocolate is pure chocolate liquor, also known as bitter or baking chocolate. It is unadulterated chocolate: the pure, ground, roasted chocolate beans impart a strong, deep chocolate flavor.
Raw chocolate, often referred to as raw cacao, is always dark and a minimum of 75% cacao. Because the act of processing results in the loss of certain vitamins and minerals (such as magnesium), some consider raw cacao to be a more nutritious form of chocolate.
Chocolate may have whitish spots on the dark chocolate part, called chocolate bloom; it is an indication that sugar and/or fat has separated due to poor storage. It is not toxic.
Production.
Roughly two-thirds of the entire world's cocoa is produced in West Africa, with 43% sourced from Ivory Coast, where child labor is a common practice to obtain the product.
According to the World Cocoa Foundation, some 50 million people around the world depend on cocoa as a source of livelihood. In the UK, most chocolatiers purchase their chocolate from them, to melt, mold and package to their own design. The WCF's report titled 'Cocoa Market Update (2012)' the Ivory Coast is the largest producer of cocoa in the world. 
Production costs can be decreased by reducing cocoa solids content or by substituting cocoa butter with another fat. Cocoa growers object to allowing the resulting food to be called "chocolate", due to the risk of lower demand for their crops. The sequencing in 2010 of the genome of the cacao tree may allow yields to be improved.
The two main jobs associated with creating chocolate candy are chocolate makers and chocolatiers. Chocolate makers use harvested cacao beans and other ingredients to produce couverture chocolate (covering). Chocolatiers use the finished couverture to make chocolate candies (bars, truffles, etc.).
Cacao varieties.
Chocolate is made from cocoa beans, the dried and partially fermented seeds of the cacao tree ("Theobroma cacao"), a small (4– to 8-m-tall (15– to 26-ft-tall) evergreen tree native to the deep tropical region of the Americas. Recent genetic studies suggest the most common genotype of the plant originated in the Amazon basin and was gradually transported by humans throughout South and Central America. Early forms of another genotype have also been found in what is now Venezuela. The scientific name, "Theobroma", means "food of the deities". The fruit, called a cacao pod, is ovoid, 15–30 cm (or 6–12 in) long and 8–10 cm (3–4 in) wide, ripening yellow to orange, and weighing about 500 g (1 lb) when ripe.
Cacao trees are small, understory trees that need rich, well-drained soils. They naturally grow within 20° of either side of the equator because they need about 2000 mm of rainfall a year, and temperatures in the range of 21 to 32°C. Cacao trees cannot tolerate a temperature lower than 15°C (59°F).
The three main varieties of cacao beans used in chocolate are criollo, forastero, and trinitario.
Criollo.
Representing only 5% of all cocoa beans grown, criollo is the rarest and most expensive cocoa on the market, and is native to Central America, the Caribbean islands and the northern tier of South American states. The genetic purity of cocoas sold today as criollo is disputed, as most populations have been exposed to the genetic influence of other varieties.
Criollos are particularly difficult to grow, as they are vulnerable to a variety of environmental threats and produce low yields of cocoa per tree. The flavor of criollo is described as delicate yet complex, low in classic chocolate flavor, but rich in "secondary" notes of long duration.
Forastero.
The most commonly grown bean is forastero, a large group of wild and cultivated cacaos, most likely native to the Amazon basin. The African cocoa crop is entirely of the forastero variety. They are significantly hardier and of higher yield than criollo. The source of most chocolate marketed, forastero cocoas are typically strong in classic "chocolate" flavor, but have a short duration and are unsupported by secondary flavors, producing "quite bland" chocolate.
Trinitario.
Trinitario is a natural hybrid of criollo and forastero. Trinitario originated in Trinidad after an introduction of forastero to the local criollo crop. Nearly all cacao produced over the past five decades is of the forastero or lower-grade trinitario varieties.
Processing.
Cacao pods are harvested by cutting them from the tree using a machete, or by knocking them off the tree using a stick. The beans with their surrounding pulp are removed from the pods and placed in piles or bins, allowing access to micro-organisms so fermentation of the pectin-containing material can begin. Yeasts produce ethanol, lactic acid bacteria produce lactic acid, and acetic acid bacteria produce acetic acid. The fermentation process, which takes up to seven days, also produces several flavor precursors, eventually resulting in the familiar chocolate taste.
It is important to harvest the pods when they are fully ripe, because if the pod is unripe, the beans will have a low cocoa butter content, or sugars in the white pulp will be insufficient for fermentation, resulting in a weak flavor. After fermentation, the beans must be quickly dried to prevent mold growth. Climate and weather permitting, this is done by spreading the beans out in the sun from five to seven days.
The dried beans are then transported to a chocolate manufacturing facility. The beans are cleaned (removing twigs, stones, and other debris), roasted, and graded. Next, the shell of each bean is removed to extract the nib. Finally, the nibs are ground and liquefied, resulting in pure chocolate in fluid form: chocolate liquor. The liquor can be further processed into two components: cocoa solids and cocoa butter.
Blending.
Chocolate liquor is blended with the cocoa butter in varying quantities to make different types of chocolate or couvertures. The basic blends of ingredients for the various types of chocolate (in order of highest quantity of cocoa liquor first), are:
Usually, an emulsifying agent, such as soy lecithin, is added, though a few manufacturers prefer to exclude this ingredient for purity reasons and to remain GMO-free, sometimes at the cost of a perfectly smooth texture. Some manufacturers are now using PGPR, an artificial emulsifier derived from castor oil that allows them to reduce the amount of cocoa butter while maintaining the same mouthfeel.
The texture is also heavily influenced by processing, specifically conching (see below). The more expensive chocolate tends to be processed longer and thus have a smoother texture and mouthfeel, regardless of whether emulsifying agents are added.
Different manufacturers develop their own "signature" blends based on the above formulas, but varying proportions of the different constituents are used. The finest, plain dark chocolate couvertures contain at least 70% cocoa (both solids and butter), whereas milk chocolate usually contains up to 50%. High-quality white chocolate couvertures contain only about 35% cocoa butter.
Producers of high-quality, small-batch chocolate argue that mass production produces bad-quality chocolate. Some mass-produced chocolate contains much less cocoa (as low as 7% in many cases), and fats other than cocoa butter. Vegetable oils and artificial vanilla flavor are often used in cheaper chocolate to mask poorly fermented and/or roasted beans.
In 2007, the Chocolate Manufacturers Association in the United States, whose members include Hershey, Nestlé, and Archer Daniels Midland, lobbied the Food and Drug Administration (FDA) to change the legal definition of chocolate to let them substitute partially hydrogenated vegetable oils for cocoa butter, in addition to using artificial sweeteners and milk substitutes. Currently, the FDA does not allow a product to be referred to as "chocolate" if the product contains any of these ingredients. 
Conching.
The penultimate process is called conching. A conche is a container filled with metal beads, which act as grinders. The refined and blended chocolate mass is kept in a liquid state by frictional heat. Chocolate prior to conching has an uneven and gritty texture. The conching process produces cocoa and sugar particles smaller than the tongue can detect, hence the smooth feel in the mouth. The length of the conching process determines the final smoothness and quality of the chocolate. High-quality chocolate is conched for about 72 hours, and lesser grades about four to six hours. After the process is complete, the chocolate mass is stored in tanks heated to about 45–50°C (113–122°F) until final processing.
Tempering.
The final process is called tempering. Uncontrolled crystallization of cocoa butter typically results in crystals of varying size, some or all large enough to be clearly seen with the naked eye. This causes the surface of the chocolate to appear mottled and matte, and causes the chocolate to crumble rather than snap when broken. The uniform sheen and crisp bite of properly processed chocolate are the result of consistently small cocoa butter crystals produced by the tempering process.
The fats in cocoa butter can crystallize in six different forms (polymorphous crystallization). The primary purpose of tempering is to assure that only the best form is present. The six different crystal forms have different properties.
As a solid piece of chocolate, the cocoa butter fat particles are in a crystalline rigid structure that gives the chocolate its solid appearance. Once heated, the crystals of the polymorphic cocoa butter are able to break apart from the rigid structure and allow the chocolate to obtain a more fluid consistency as the temperature increases - the melting process. When the heat is removed, the cocoa butter crystals become rigid again and come closer together, allowing the chocolate to solidify.
The temperature in which the crystals obtain enough energy to break apart from their rigid conformation would depend on the milk fat content in the chocolate and the shape of the fat molecules, as well as the form of the cocoa butter fat. Chocolate with a higher fat content will melt at a lower temperature.
Making chocolate considered "good" is about forming as many type V crystals as possible. This provides the best appearance and texture and creates the most stable crystals, so the texture and appearance will not degrade over time. To accomplish this, the temperature is carefully manipulated during the crystallization.
Generally, the chocolate is first heated to 45°C to melt all six forms of crystals. Next, the chocolate is cooled to about 27°C, which will allow crystal types IV and V to form. At this temperature, the chocolate is agitated to create many small crystal "seeds" which will serve as nuclei to create small crystals in the chocolate. The chocolate is then heated to about 31°C to eliminate any type IV crystals, leaving just type V. After this point, any excessive heating of the chocolate will destroy the temper and this process will have to be repeated. However, other methods of chocolate tempering are used. The most common variant is introducing already tempered, solid "seed" chocolate. The temper of chocolate can be measured with a chocolate temper meter to ensure accuracy and consistency. A sample cup is filled with the chocolate and placed in the unit which then displays or prints the results.
Two classic ways of manually tempering chocolate are:
Chocolate tempering machines (or temperers) with computer controls can be used for producing consistently tempered chocolate, particularly for large volume applications.
Storage.
Chocolate is very sensitive to temperature and humidity. Ideal storage temperatures are between 15 and 17°C (59 and 63°F), with a relative humidity of less than 50%. Various types of "blooming" effects can occur if chocolate is stored or served improperly. Fat bloom is caused by storage temperature fluctuating or exceeding 24°C, while sugar bloom is caused by temperature below 15°C or excess humidity. To distinguish between different types of bloom, one can rub the surface of the chocolate lightly, and if the bloom disappears, it is fat bloom. One can get rid of bloom by retempering the chocolate or using it for any use that requires melting the chocolate.
Chocolate is generally stored away from other foods, as it can absorb different aromas. Ideally, chocolates are packed or wrapped, and placed in proper storage with the correct humidity and temperature. Additionally, chocolate is frequently stored in a dark place or protected from light by wrapping paper.
If refrigerated or frozen without containment, chocolate can absorb enough moisture to cause a whitish discoloration, the result of fat or sugar crystals rising to the surface. Moving chocolate from one temperature extreme to another, such as from a refrigerator on a hot day, can result in an oily texture. Although visually unappealing, chocolate suffering from bloom is perfectly safe for consumption.
Labeling.
Some manufacturers provide the percentage of chocolate in a finished chocolate confection as a label quoting percentage of "cocoa" or "cacao". It should be noted that this refers to the combined percentage of both cocoa solids and cocoa butter in the bar, not just the percentage of cocoa solids. The Belgian AMBAO certification mark indicates that no non-cocoa vegetable fats have been used in making the chocolate.
Chocolates that are organic or fair trade certified carry labels accordingly.
In the United States, some large chocolate manufacturers lobbied the federal government to permit confections containing cheaper hydrogenated vegetable oil in place of cocoa butter to be sold as "chocolate". In June 2007, as a response to consumer concern after the proposed change, the FDA reiterated "Cacao fat, as one of the signature characteristics of the product, will remain a principal component of standardized chocolate."
Manufacturers.
Chocolate manufacturers produce a range of products from chocolate bars to fudge. Large manufacturers of chocolate products include Cadbury (the world's largest confectionery manufacturer), Guylian, The Hershey Company, Lindt & Sprüngli, Mars, Incorporated, Milka, Neuhaus and Suchard.
Guylian is best known for its chocolate Sea Shells; Cadbury for its Dairy Milk and Creme Egg. The Hershey Company, the largest chocolate manufacturer in North America, produces the Hershey Bar and Hershey's Kisses. Mars Incorporated, a large privately owned U.S. corporation, produces Mars Bar, Milky Way, M&M's, Twix, and Snickers. Lindt is known for its truffle balls and Gold Easter Bunnies.
Food conglomerates Nestlé SA and Kraft Foods both have chocolate brands. Nestlé acquired Rowntree's in 1988 and now markets chocolates under their own brand, including Smarties and Kit Kat; Kraft Foods through its 1990 acquisition of Jacobs Suchard, now owns Milka and Suchard. In February 2010, Kraft also acquired British-based Cadbury.; Fry's, Trebor Basset, the fair-trade brand Green & Black's, also belongs to the group.
The chocolate industry.
The chocolate industry, a steadily growing, $50 billion-a-year worldwide business centered on the sale and consumption of chocolate, is prevalent on five of the seven continents. Europe accounts for 45% of the world's chocolate revenue. Big Chocolate, as it is also called, is essentially an oligopoly between major international chocolate companies in Europe and the U.S. The U.S. companies, such as Mars and Hershey’s alone, generate $13 billion a year in chocolate sales and account for two-thirds of U.S. manufacturers. Despite the expanding reach of the chocolate industry internationally, cocoa farmers and labourers in the Ivory Coast are unaware of the uses of the beans. The high cost of chocolate in the Ivory Coast also means that it is inaccessible to the majority of the population, who are unaware of what it tastes like.
Slavery.
In 2009, Salvation Army International Development (SAID) UK noted that 12,000 children have been trafficked on cocoa farms in the Ivory Coast of Africa, where half of the world's chocolate is made. SAID UK states that it is these child slaves who are likely to be working in "harsh and abusive" conditions for the production of chocolate, and an increasing number of health-food and anti-slavery organisations are now highlighting and campaigning against the use of trafficking in the chocolate industry.
In popular culture.
Religious and cultural links.
Chocolate is associated with festivals such as Easter, when moulded chocolate rabbits and eggs are traditionally given in Christian communities, and Hanukkah, when chocolate coins are given in Jewish communities. Chocolate hearts and chocolate in heart-shaped boxes are popular on Valentine's Day and are often presented along with flowers and a greeting card. Chocolate is an acceptable gift on other holidays and on occasions such as birthdays.
Many confectioners make holiday-specific chocolate candies. Chocolate Easter eggs or rabbits and Santa Claus figures are two examples. Such confections can be solid, hollow, or filled with sweets or fondant.
Books and film.
Chocolate has been the center of several successful book and film adaptations. In 1964, Roald Dahl published a children's novel titled "Charlie and the Chocolate Factory". The novel centers on a poor boy named Charlie Bucket who takes a tour through the greatest chocolate factory in the world, owned by Willy Wonka. Two film adaptations of the novel were produced. The first was "Willy Wonka & the Chocolate Factory", a 1971 film which later became a cult classic, and spawned the real world Willy Wonka Candy Company, which produces chocolate products to this day. Thirty-four years later, a second film adaptation was produced, titled "Charlie and the Chocolate Factory". The 2005 film was very well received by critics and was one of the highest grossing films that year, earning over US$470,000,000 worldwide. "Charlie and the Chocolate Factory" was also recognized at the 78th Academy Awards, where it was nominated for Best Costume Design for Gabriella Pesucci.
"Like Water for Chocolate" ("Como agua para chocolate"), a 1989 love story by novelist Laura Esquivel, was adapted to film in 1992. The plot incorporates magical realism with Mexican cuisine, and the title is a double entendre in its native language, referring both to a recipe for hot chocolate and to an idiom that is a metaphor for sexual arousal. The film earned 11 Ariel Awards from the Academia Mexicana de Artes y Ciencias Cinematográficas, including Best Picture.
"Chocolat", a 1999 novel by Joanne Harris, tells the story of Vianne Rocher, a young mother, whose confections change the lives of the townspeople. The 2000 film adaptation, "Chocolat", also proved successful, grossing over US$150,000,000 worldwide, and receiving Academy Award and Golden Globe nominations for Best Picture, Best Actress, and Best Original Score.

</doc>
<doc id="7100" url="http://en.wikipedia.org/wiki?curid=7100" title="Cornet">
Cornet

The cornet is a brass instrument very similar to the trumpet, distinguished by its conical bore, compact shape, and mellower tone quality. The most common cornet is a transposing instrument in B. It is not related to the renaissance and early baroque cornett.
History.
The cornet was initially derived from the post horn around 1820 in France. Among the first manufacturers of modern cornets was Parisian Jean Asté in 1828. Cornets first appeared as separate instrumental parts in 19th century French compositions.
This instrument could not have been developed without the improvement of piston valves by Heinrich Stölzel and Friedrich Blühmel. In the early 19th century these two instrument makers almost simultaneously invented the valves still used today. They jointly applied for a patent and were granted this for a period of ten years. The first notable virtuoso player was Jean-Baptiste Arban, who studied the cornet extensively and published "La grande méthode complète de cornet à piston et de saxhorn", commonly referred to as the "Arban method", in 1864. Up until the early 20th century, the trumpet and cornet coexisted in musical ensembles. Symphonic repertoire often involves separate parts for trumpet and cornet. As several instrument builders made improvements to both instruments, they started to look and sound more alike. The modern day cornet is used in brass bands, concert bands, and in specific orchestral repertoire that requires a more mellow sound.
The name cornet derives from corne, meaning "horn", itself from Latin cornus. While not musically related, instruments of the Zink family (which includes serpents) are named "cornetto- " with a tonal or pitch related Latin word following the hyphen to describe the particular variant. The 11th edition of the "Encyclopædia Britannica" referred to serpents as "old wooden cornets". The Roman/Etruscan cornu (or simply "horn") is the lingual ancestor of these. It is a predecessor of the post horn from which the cornet evolved and was used like a bugle to signal orders on the battlefield.
The instrument was once sometimes referred to as a cornopean, referencing the earliest cornets with the Stölzel valve system.
Relationship to trumpet.
The cornet was invented by adding valves to the post horn in 1814. The valves allowed for melodic playing throughout the register of the cornet. Trumpets were slower to adopt the new valve technology, so for the next 100 years or more, composers often wrote separate parts for trumpet and cornet. The trumpet would play fanfare-like passages, while the cornet played more melodic passages. The modern trumpet has valves that allow it to play the same notes and fingerings as the cornet.
Cornets and trumpets made in a given key (usually the key of B) play at the same pitch, and the technique for playing the instruments is nearly identical. However, cornets and trumpets are not entirely interchangeable, as they differ in timbre. Also available, but usually seen only in the brass band, is an E soprano model, pitched a fourth above the standard B.
Unlike the trumpet, which has a cylindrical bore up until the bell section, the tubing of the cornet has a mostly conical bore, starting very narrow at the mouthpiece and gradually widening towards the bell. Cornets following the 1913 patent of E.A. Couturier can have a continuously conical bore. The conical bore of the cornet is primarily responsible for its characteristic warm, mellow tone, which can be distinguished from the more penetrating sound of the trumpet. The conical bore of the cornet also makes it more agile than the trumpet when playing fast passages, but correct pitching is often less assured. The cornet is often preferred for young beginners as it is easier to hold, with its centre of gravity much closer to the player.
The cornet mouthpiece has a shorter and narrower shank than that of a trumpet so it can fit the cornet's smaller mouthpiece receiver. The cup size is often deeper than that of a trumpet mouthpiece.
One variety is the short model traditional cornet, also known as a "Shepherd's Crook" shaped model. These are most often large–bore instruments with a rich mellow sound. There is also a long-model cornet, usually with a smaller bore and a brighter sound, which is closer to a trumpet in appearance. The Shepherd's Crook model is preferred by cornet traditionalists. The long-model cornet is generally used in concert bands in the United States, but has found little following in British-style brass and concert bands.
Echo cornet.
The echo cornet is an obsolete variant which has a mute chamber (or echo chamber) mounted to the side acting as a second bell when the fourth valve is pressed. The second bell has a sound similar to that of a harmon mute and is typically used to play echo phrases, whereupon the player imitates the sound from the primary bell using the echo chamber.
Playing technique.
Like the trumpet and all other modern brass wind instruments, the cornet makes a sound when the player vibrates ("buzzes") the lips in the mouthpiece, creating a vibrating column of air in the tubing. The frequency of the air column's vibration can be modified by changing the lip aperture or "embouchure". In addition, the column of air can be lengthened by engaging one or more valves, thus lowering the pitch. Double and triple touguing are also possible.
Without valves, the player could only produce a harmonic series of notes like those played by the bugle and other "natural" brass instruments. These notes are far apart for most of the instrument's range, making diatonic and chromatic playing impossible except in the extreme high register. The valves change the length of the vibrating column and provide the cornet with the ability to play chromatically.
Ensembles with cornets.
Brass band.
British style brass band ensembles consist completely of brass instruments (except for the percussion section). The cornet is the leading melodic instrument in this ensemble and trumpets are never used. The ensemble consists of about thirty musicians, including nine B cornets and one E cornet (soprano cornet) in the higher registers. In England, companies such as Besson and Boosey and Hawkes specialized in these instruments. In America, 19th century manufacturers such as Graves and Company, Hall and Quinby, E.G. Wright and the Boston Musical Instrument Manufactury built lines of instruments for this format of ensemble.
Concert band.
The cornet also features in the British-style concert band, unlike the American concert band or wind band, where it is replaced by the trumpet. This slight difference in instrumentation derives from the British concert band's heritage in military bands, where the highest brass instrument is always the cornet. There are usually four to six B cornets present in a concert band, but no E instrument, as this role is taken by the E clarinet.
Fanfare orkest.
Fanfare orkesten ("fanfare orchestras"), only found in the Netherlands, Belgium, Northern France and Lithuania use the complete saxhorn family of instruments. The standard instrumentation includes both the cornet and the trumpet; however, in recent decades, the cornet has largely been replaced by the trumpet.
Jazz ensemble.
In old style jazz bands the cornet was preferred to the trumpet, but from the swing era onwards it has been largely replaced by the louder, more piercing trumpet. Likewise the cornet has been largely phased out of big bands by a growing taste for louder and more aggressive instruments, especially since the advent of bebop in the post World War II era.
Legendary jazz pioneer Buddy Bolden played the cornet, and Louis Armstrong started off on the cornet but later switched to the trumpet. Cornetists such as Bubber Miley and Rex Stewart contributed substantially to the Duke Ellington Orchestra's early sound. Other influential jazz cornetists include King Oliver, Bix Beiderbecke, Ruby Braff and Nat Adderley. Notable performances on cornet by players generally associated with the trumpet include Freddie Hubbard's on "Empyrean Isles" by Herbie Hancock and Don Cherry's on "The Shape of Jazz to Come" by Ornette Coleman.
Notable cornetists.
Today.
Influential contemporary cornet players include:

</doc>
<doc id="7102" url="http://en.wikipedia.org/wiki?curid=7102" title="CAMP">
CAMP

CAMP may stand for:

</doc>
<doc id="7103" url="http://en.wikipedia.org/wiki?curid=7103" title="CGMP">
CGMP

CGMP is an acronym. It can refer to:

</doc>
<doc id="7104" url="http://en.wikipedia.org/wiki?curid=7104" title="Cotton Mather">
Cotton Mather

Cotton Mather, FRS (February 12, 1663 – February 13, 1728; A.B. 1678, Harvard College; A.M. 1681, honorary doctorate 1710, University of Glasgow) was a socially and politically influential New England Puritan minister, prolific author and pamphleteer; he is often remembered for his scientific role in early hybridization experiments, his stance as an early proponent of inoculation in America, and for his role in the Salem witch trials.
Biography.
He was the son of Increase Mather, and grandson of both John Cotton and Richard Mather, all also prominent Puritan ministers. Mather was named after his maternal grandfather, John Cotton. He attended Boston Latin School, where his name was posthumously added to its Hall of Fame, and graduated from Harvard in 1678 at age 15. After completing his post-graduate work, he joined his father as assistant pastor of Boston's original North Church. In 1685 Mather assumed full responsibilities as pastor at the Church.
Cotton Mather wrote more than 450 books and pamphlets, and his ubiquitous literary works made him one of the most influential religious leaders in America. Mather set the moral tone in the colonies, and sounded the call for second- and third-generation Puritans, whose parents had left England for the New England colonies of North America, to return to the theological roots of Puritanism.
The most important of these, "Magnalia Christi Americana" (1702), comprises seven distinct books, many of which depict biographical and historical narratives to which later American writers, such as Nathaniel Hawthorne, Elizabeth Drew Stoddard, and Harriet Beecher Stowe, would look in describing the cultural significance of New England for later generations after the American Revolution. Mather's text thus is one of the more important documents in American history, because it reflects a particular tradition of seeing and understanding the significance of place. Mather, as a Puritan thinker and social conservative, drew on the language of the Bible to speak to contemporary audiences. In particular, Mather's review of the American experiment sought to explain signs of his time and the types of individuals drawn to the colonies as predicting the success of the venture. From his religious training, Mather viewed the importance of texts for elaborating meaning and for bridging different moments of history—linking, for instance, the Biblical stories of Noah and Abraham with the arrival of such eminent leaders as John Eliot; John Winthrop; and his own father, Increase Mather.
Through his writings the intellectual and physical struggles of first- through third-generation Puritans created an elevated appraisal in the minds of Americans about America's appointed place among other nations. The unease and self-deception that characterized that period of colonial history would be revisited in many forms at political and social moments of crisis (such as the Salem witch trials, which coincided with frontier warfare and economic competition among Indians and French and other European settlers) and during lengthy periods of cultural definition (such as the American Renaissance of the late 18th- and early 19th-century literary, visual, and architectural movements, which sought to capitalize on unique American identities).
Highly influential because of his prolific writing, Mather was a force to be reckoned with in secular, as well as in spiritual, matters. After the fall of James II of England, in 1688, Mather was among the leaders of the successful revolt against James's governor of the consolidated Dominion of New England, Sir Edmund Andros.
Cotton Mather was not known for writing in a neutral, unbiased perspective. Many, if not all, of his writings had bits and pieces of his own personal life in them or were written for personal reasons. According to literary historian Sacvan Bercovitch:
Mather also influenced early American science. In 1716, because of observations of corn varieties, he conducted one of the first recorded experiments with plant hybridization. This observation was memorialized in a letter to his friend James Petiver:
In November 1713, Mather's wife, newborn twins, and two-year-old daughter all succumbed during a measles epidemic. Of Mather's three wives and 15 children, only his last wife and two children survived him. Mather was buried on Copp's Hill, near Old North Church.
Boyle's influence on Mather.
A huge influence throughout Mather's career was Robert Boyle. While coming to terms with who he was, Mather read Robert Boyle's book "The Usefulness of Experimental Natural Philosophy". Mather read Boyle's work closely throughout the 1680s and his early works on science and religion borrowed greatly from it. He even uses almost identical language to Boyle.
Mather's relationship with his father and the after effects in Mather's Works.
Cotton Mather's relationship with his well-known father, Increase Mather, is thought by some to have been a strained and difficult one. Increase Mather was a pastor of the Old North Church and president of Harvard College; he led an accomplished life that Cotton was determined to live up to. Despite Cotton Mather's efforts, he never became quite as well known and successful in politics as his father. He did surpass his father's output as a writer, writing over 400 books. One of the most public displays of their strained relationship emerged during the Salem Witch Trials. Despite the fact that Increase Mather did not support the trials, Cotton Mather documented them. Cotton helped convince Elihu Yale to make a donation to a new college in New Haven that would come to be Yale College.
Smallpox inoculation controversy.
The practice of smallpox inoculation (as opposed to the later practice of vaccination) was developed possibly in 8th-century India or 10th-century China. Spreading its reach in seventeenth-century Turkey, inoculation or, rather, variolation, involved infecting a person through a cut in the skin with exudate from a patient with a relatively mild case of smallpox (variola), in order to bring about a manageable and recoverable infection that would provide later immunity.
By the beginning of the eighteenth century, the Royal Society in England was discussing the practice of inoculation, and the smallpox epidemic in 1713 spurred further interest. It was not until 1721, however, that England recorded its first case of inoculation.
Early New England.
Smallpox was a serious threat in colonial America, most devastating to Native Americans, but also to Anglo-American settlers. New England suffered smallpox epidemics in 1677, 1689–90, and 1702. It was highly contagious, and mortality could reach as high as 30 percent or more.
Boston had been plagued by smallpox outbreaks in 1690 and 1702. During this era, public authorities in Massachusetts dealt with the threat primarily by means of quarantine. Incoming ships were quarantined in Boston harbor, and any smallpox patients in town were held under guard or in a "pesthouse."
In 1706 his slave, Onesimus, explained to Cotton Mather how he had been inoculated as a child in Africa. Mather was fascinated by the idea. By July 1716, Mather had read an endorsement of inoculation by Dr. Emanuel Timonius of Constantinople in the "Philosophical Transactions". Mather then declared, in a letter to Dr. John Woodward of Gresham College in London, that he planned to press Boston's doctors to adopt the practice of inoculation should smallpox reach the colony again.
By 1721, a whole generation of young Bostonians was vulnerable and memories of the last epidemic's horrors had by and large disappeared. On April 22 of that year, the HMS "Seahorse" arrived from the West Indies carrying smallpox on board. Despite attempts to protect the town through quarantine, eight known cases of smallpox appeared in Boston by May 27, and by mid-June, the disease was spreading at an alarming rate. As a new wave of smallpox hit the area and continued to spread, many residents fled to outlying rural settlements. The combination of exodus, quarantine, and outside traders' fears disrupted business in the capital of the Bay Colony for weeks. Guards were stationed at the House of Representatives to keep Bostonians from entering without special permission. The death toll reached 101 in September, and the Selectmen, powerless to stop it, "severely limited the length of time funeral bells could toll." As one response, legislators delegated a thousand pounds from the treasury to help the people who, under these conditions, could no longer support their families.
On June 6, 1721, Mather sent an abstract of reports on inoculation by Timonius and Jacobus Pylarinus to local physicians, urging them to consult about the matter. He received no response. Next, Mather pleaded his case to Dr. Zabdiel Boylston, who tried the procedure on his only son and two slaves—one grown and one a boy. All recovered in about a week. Boylston inoculated seven more people by mid-July.
The epidemic peaked in October 1721, with 411 deaths; by February 26, 1722, Boston was, once again, free of smallpox. The total number of cases since April 1721 came to 5,889, with 844 deaths—more than three-quarters of all the deaths in Boston during 1721. Meanwhile, Dr. Boylston had inoculated 242 people, with only six resulting in death.
Inoculation debate.
Boylston and Mather's inoculation crusade "raised a horrid Clamour" amongst the people of Boston. Both Boylston and Mather were "Object[s] of their Fury; their furious Obloquies and Invectives," which Mather acknowledges in his diary. Boston's Selectmen, consulting a doctor who claimed that the practice caused many deaths and only spread the infection, forbade Boylston from performing it again.
"The New-England Courant" published writers who opposed the practice. The editorial stance was that the Boston populace feared that inoculation spread, rather than prevented, the disease; however, some historians, notably H. W. Brands, have argued that this position was a result of editor-in-chief James Franklin's (Benjamin Franklin's brother) contrarian positions.
Public discourse ranged in tone from organized arguments by Reverend John Williams from Boston, who posted that "several arguments proving that inoculating the smallpox is not contained in the law of Physick, either natural or divine, and therefore unlawful," to more slanderous attacks, such as those put forth in a pamphlet by Dr. William Douglass of Boston entitled "The Abuses and Scandals of Some Late Pamphlets in Favour of Inoculation of the Small Pox" (1721), on the qualifications of inoculation's proponents. (Douglass was exceptional at the time for holding a medical degree from Europe.) At the extreme, in November 1721, someone hurled a lighted grenade into Cotton Mather's house.
Medical opposition.
Several opponents of smallpox inoculation, among them John Williams, stated that there were only two laws of physick (medicine): sympathy and antipathy. In his estimation, inoculation was neither a sympathy toward a wound or a disease, or an antipathy toward one, but the creation of one. For this reason, its practice violated the natural laws of medicine, transforming health care practitioners into those who harm rather than heal.
As with many colonists, Williams' Puritan beliefs were enmeshed in every aspect of his life, and he used the Bible to state his case. He quoted when Jesus said: "It is not the healthy who need a doctor, but the sick."
In contrast, Dr. William Douglass proposed a more secular argument against inoculation, stressing the importance of reason over passion and urging the public to be pragmatic in their choices. In addition, he demanded that ministers leave the practice of medicine to physicians, and not meddle in areas where they lacked expertise. According to Douglass, smallpox inoculation was "a medical experiment of consequence," one not to be undertaken lightly. He believed that not all learned individuals were qualified to doctor others, and while ministers took on several roles in the early years of the colony, including that of caring for the sick, they were now expected to stay out of state and civil affairs.
Douglass also felt that inoculation caused more deaths than it prevented. The only reason Cotton Mather had success in it, he said, was because Mather had used it on children, who are naturally more resilient. Douglass vowed to always speak out against "the wickedness of spreading infection."
Speak out he did: "The battle between these two prestigious adversaries [Douglass and Mather] lasted far longer than the epidemic itself, and the literature accompanying the controversy was both vast and venomous." In the end, Douglass grew to accept inoculation, but he stood his ground on the need for professional standards.
Puritan resistance.
Generally, Puritan pastors favored the inoculation experiments. For example, Cotton's father Increase Mather was joined by prominent Puritan pastors Benjamin Colman and William Cooper in openly propagating the use of inoculations. "One of the classic assumptions of the Puritan mind was that the will of God was to be discerned in nature as well as in revelation." Nevertheless Williams questioned whether the smallpox "is not one of the strange works of God; and whether inoculation of it be not a fighting with the most High." He also asked his readers if the smallpox epidemic may have been given to them by God as "punishment for sin," and warned that attempting to shield themselves from God's fury (via inoculation), would only serve to "provoke him more." The Puritans found meaning in affliction, and they did not yet know why God was showing them disfavor through smallpox. Not to address their errant ways before attempting a cure could set them back in their "errand."
Many Puritans believed that creating a wound and inserting poison was doing violence and therefore was antithetical to the healing art. They grappled with adhering to the Ten Commandments, with being proper church members and good caring neighbors. The apparent contradiction between harming or murdering a neighbor through inoculation and the Sixth Commandment--"thou shalt not kill"—seemed insoluble and hence stood as one of the main objections against the procedure.[citation?]
Williams maintained that because the subject of inoculation could not be found in the Bible, it was not the will of God, and therefore "unlawful." He also explained that inoculation violated The Golden Rule, because if one neighbor voluntarily infected another with disease, he was not doing unto others as he would have done to him. With the Bible as the Puritans' source for all decision-making, lack of scriptural evidence concerned many, and Williams vocally scorned Rev. Mather for not being able to reference an inoculation edict directly from the Bible. Obviously, Mather, the Puritan pastor, disagreed and most, if not all, Puritan pastors stood with Mather.
Inoculation defended.
With the smallpox epidemic catching speed and racking up a staggering death toll, a solution to the crisis was becoming more urgently needed by the day. The use of quarantine and various other efforts, such as balancing the body's humors, did not slow the disease's spread. As news rolled in from town to town and correspondence arrived from overseas, reports of horrific stories of suffering and loss due to smallpox stirred mass panic among the people. "By circa 1700, smallpox had become among the most devastating of epidemic diseases circulating in the Atlantic world."
Cotton Mather strongly challenged the perception that inoculation was against the will of God and argued that the procedure was not outside of Puritan principles. He wrote that "whether a Christian may not employ this Medicine (let the matter of it be what it will) and humbly give Thanks to God's good Providence in discovering of it to a miserable World; and humbly look up to His Good Providence (as we do in the use of any other Medicine) It may seem strange, that any wise Christian cannot answer it. And how strangely do Men that call themselves Physicians betray their Anatomy, and their Philosophy, as well as their Divinity in their invectives against this Practice?" The Puritan minister began to embrace the sentiment that smallpox was an inevitability for anyone, both the good and the wicked, yet God had provided them with the means to save themselves. Mather reported that, from his view, "none that have used it ever died of the Small Pox, tho at the same time, it were so malignant, that at least half the People died, that were infected With it in the Common way."
While Cotton Mather was experimenting with the procedure, prominent Puritan pastors Benjamin Colman and William Cooper expressed public and theological support for them. The practice of smallpox inoculation was eventually accepted by the general population due to first-hand experiences and personal relationships. Although many were initially wary of the concept, it was because people were able to witness the procedure's consistently positive results, within their own community of ordinary citizens, that it became widely utilized and supported. One important change in the practice after 1721 was regulated quarantine of inoculees.
Inoculation visibly and directly aided man's control of the disease, the level of infection, mortality rates and the spreading of the epidemic. Planned inoculation led to better observation of the body's responses and allowed people the ability to time the onset of the pox and control the disease's intensity. For example, by inoculating in the months of milder climate, one had a better chance of fighting the infection and becoming immune instead of the alternative: natural exposure to the disease during harsher weather, when the body's defenses were already challenged.
Additionally, by the 1750s, innovations and experience with inoculation focused on better insertion of pox fluid and preparation of body to withstand the disease. By controlling the point and time of infection, bodies could be conditioned to optimal state before contracting smallpox, therefore providing a better opportunity to fight and achieve immunity. Dependent upon a person's constitution, by adhering to a specific diet or purging, one could physically handle the infection more successfully. It was also discovered that inoculation produced less scarring and physical defects than a common, naturally contracted case.
The aftermath.
Although Cotton Mather and Dr. Boylston were able to demonstrate the efficacy of the practice, the debate over inoculation would continue even beyond the epidemic of 1721–22. After overcoming considerable difficulty and achieving notable success, Boylston traveled to London in 1725 where he published his results and was elected to the Royal Society in 1726.
The responses of the Boston clergymen to the reproaches put forth by the anti-inoculation camp highlighted seminal changes the Puritan church was undergoing at the time. By prescribing recent advances in medicine, the Boston ministers modified the doctrine of theological pathogenesis in an attempt to maintain the old order according to which it was the clergy's duty and privilege to interpret illnesses and their cures. However, the contradiction of simultaneously upholding tradition and embracing innovations was impossible to resolve and, as a consequence, the clergy continued to lose influence over secular affairs in eighteenth-century New England.
In the end, lives were saved by inoculation, and the epidemic was halted. Even today, the procedure is credited with ending the devastation caused by the early epidemics, and vaccination, in many ways an updated and modernized form of the procedure, continues to be recommended by the Centers for Disease Control for at-risk populations, such as potential victims of bioterrorism, and research scientists who work with surviving strains of the virus.
Salem witch trials of 1692, The Mather Influence.
Mather's contemporary critic, Robert Calef, considered him responsible for laying the very groundwork that inspired the Salem witch trials:
The historian Charles Wentworth Upham, writing in 1869 says that both Mathers "are answerable... more than almost any other... for the opinions of their time. It was indeed a superstitious age, but made much more so by their operations, influence, and writings, beginning with Increase Mather's movement at the assembly of Ministers in 1681 and ending with Cotton Mather's dealings with the Goodwin children, and the account thereof which he printed [1689] and circulated far and wide." Upham refers to the afflicted in Salem as the "imitators" of the Goodwin children.
Mather was influential in the construction of the court for the trials from the beginning. Sir William Phips, governor of the newly chartered Province of Massachusetts Bay, appointed his lieutenant governor, William Stoughton, as head of a special witchcraft tribunal and then as chief justice of the colonial courts, where he presided over the witch trials. According to Bancroft, Mather had been influential in gaining politically unpopular Stoughton his appointment as lieutenant governor under Phips by appealing to his politically powerful father, Increase Mather. "Intercession had been made by Cotton Mather for the advancement of William Stoughton, a man of cold affections, proud, self-willed and covetous of distinction." Apparently Mather saw in Stoughton an ally for church-related matters. Bancroft quotes Mather's reaction to Stoughton's appointment as follows: 
Mather claims he did not attend the trials in Salem (though his father attended the trial of George Burroughs). Two contemporaries—Thomas Brattle and Robert Calef—place him at executions (see below). Mather began to publicize and celebrate the trials well before they were put to an end: "If in the midst of the many Dissatisfaction among us, the publication of these Trials may promote such a pious Thankfulness unto God, for Justice being so far executed among us, I shall Re-joyce that God is Glorified..." " (Wonders of the Invisible World)." He calls himself a historian not an advocate, but writes in such a way that clearly presumes the guilt of the accused and adding insults e.g. calling Martha Carrier a rampant hag (Wonders of the Invisible World).
Use of spectral evidence.
Mather's most fatal influence over the trials was in composing the answer to the question of whether or not to allow spectral evidence, that is, allowing the afflicted girls to claim that some invisible ghost of the defendant was tormenting them, and for this to be considered evidence of witchcraft by the defendant, even if the defendant denied it and professed their own strongly held Christian beliefs. An opinion on the matter was sought from the most esteemed ministers of the area and Cotton Mather took credit for their response when anonymously celebrating himself years later: "drawn up at their desire, by Cotton Mather the younger, as I have been informed." (Book of The Life of Sir William Phips first published anonymously in London in 1697) And Mather then included the letter, but, for his own reasons (surely not brevity, Magnalia is huge) left out the first, second, and eighth sections, which would seem most encouraging to the judges to carry-on with their work.
The original full version of the letter, called "Return of the Several Ministers" dated June 15, 1692, and had already been reprinted in the fall 1692 in the final two pages of Increase Mather's "Cases of Conscience". It is a curious document and remains a source of confusion and argument. Calef calls it "perfectly Ambidexter, giving as great as greater Encouragement to proceed in those dark methods, then cautions against them... indeed the Advice then given, looks most like a thing of his Composing, as carrying both Fire to increase and Water to quench the Conflagration." Regarding his view on Spectral evidence, Upham concludes that "Cotton Mather never in any public writing 'denounced the admission' of it, never advised its absolute exclusion; but on the contrary recognized it as a ground of 'presumption' ...[and once admitted] nothing could stand against it. Character, reason, common sense, were swept away." Allowing such evidence resulted in the shrieking circus that the trials became.
The later rejection of Spectral evidence in the trials of January 1693 resulted in no convictions.
Bancroft notes that Mather considered witches "among the poor, and vile, and ragged beggars upon Earth," and Bancroft asserts that Mather considered the people against the witch trials to be witch advocates.
Calef places Mather at the scene of the execution of Mr. Burroughs (and four others who were killed after Mather spoke) and shows him playing a direct and influential role:
Post-trial.
In the years after the trials, Cotton Mather remained unrepentant for his role. Of the principal actors in the trial, whose lives are recorded after, only Cotton Mather and his ally William Stoughton never admitted any guilt. Indeed, in the years after the trial Mather became an increasingly vehement defender of the trial. At the request of then Lt.-Gov. William Stoughton, Mather wrote "Wonders of the Invisible World" written during the trials and published in 1693. The book contained a few of Mather's sermons, the conditions of the colony and a description of witch trials in Europe. Mather somewhat clarified the contradictory advice he had given in "Return of the Several Ministers," by defending the use of spectral evidence. "Wonders of the Invisible World" appeared at the same time as Increase Mather's "Cases of Conscience."
Cotton Mather did not sign his name or support his father's book initially, and seemed more concerned with blow-back on himself and the judges than ensuring the protection of the innocent:
The last event in Cotton Mather's involvement with witchcraft was his attempt to cure Mercy Short and Margaret Rule. Boston merchant Robert Calef began his eight-year campaign against the influential Mathers. Calef's book was inspired by the fear that Mather would succeed in once again stirring up new witchcraft trials, and the need to bear witness to the horrible experiences of New Englanders in 1692. He quotes the public apologies of the men on the jury and one of the judges. Upon reading Calef's "More Wonders of the Invisible World," Increase Mather publicly burned the book in Harvard Yard.
19th Century Revision of Cotton Mather, Poole vs. Upham.
In 1869, William Frederick Poole quoted from various school textbooks of the time demonstrating they were in agreement on Cotton Mather's role in the Witch Trials:
Poole was not a historian, but a famous librarian, and a lover of literature, including Mather's "Magnalia" "and other books and tracts, numbering nearly 400 [which] were never so prized by collectors as today." Poole announces his intention to redeem Mather's name, using as a springboard a harsh critique of a recently published tome by Charles Wentworth Upham called "Salem Witchcraft Volumes I and II With an Account of Salem Village and a History of Opinions on Witchcraft and Kindred Subjects." Upham's book runs to almost 1,000 pages and a quick search of the name Mather (referring to either father, son, or ancestors) shows that it occurs 96 times; Poole's critique, in book form, runs less than 70 pages but the name "Mather" occurs many times that. Upham shows a balanced and complicated view of Cotton Mather such as this first mention: "One of Cotton Mather's most characteristic productions is the tribute to his venerated master. It flows from a heart warm with gratitude." Upham's book refers to Robert Calef 25 times with the majority of these regarding documents compiled by Calef in the mid-1690s and stating: "Although zealously devoted to the work of exposing the enormities connected with the witchcraft prosecutions, there is no ground to dispute the veracity of Calef as to matters of fact." He goes on to say that Calef's collection of writings "gave a shock to Mather's influence, from which it never recovered." Thus, Poole's critique might better be understood as aimed at Calef, Mather's contemporary, who saw fit to ascribe to him, and his influence, the largest portion of blame.
Calef produced only the one book; he is self-effacing and apologetic for his limitations, and on the title page he is listed not as author but "collector". Poole, champion of literature, cannot accept Calef whose "faculties, as indicated by his writings appear to us to have been of an inferior order;..." and his book "in our opinon, has a reputation much beyond its merits."
Poole refers to Calef as Mather's "personal enemy" and opens a line, "Without discussing the character and motives of Calef..." and he does not follow up on this suggestiveness, to discuss any motive or reason to impune Calef's character.
Upham responded to Poole in a book running five times as long and sharing the same title (referring to Poole as "the Reviewer.") Many of Poole's arguments were addressed, but both authors emphasize the importance of Cotton Mather's difficult and contradictory view on spectral evidence, as copied in the final pages of Increase Mather's "" called "The Return of Several Ministers."
In 1914, the historian George Lincoln Burr sided with Upham in a note on Thomas Brattle's letter, "The strange suggestion of W. F. Poole that Brattle here means Cotton Mather himself, is adequately answered by Upham..." Burr also reprinted Calef in full and dug deep into the historical record for information on the man and concludes "...that he had else any grievance against the Mathers or their colleagues there is no reason to think." Burr finds that a comparison between Calef's work and original documents in the historical record collections "testify to the care and exactness..."
20th century and ongoing revision.
Poole's views have found followers and the revision of Cotton Mather continues, while Calef's book is arguably less well-known.
Chadwick Hansen's "Witchcraft at Salem," published in 1969, defined Mather as a positive influence on the Salem Trials. Hansen considered Mather's handling of the Goodwin Children to be sane and temperate. Hansen also noted that Mather was more concerned with helping the affected children than witch-hunting. Mather treated the affected children through prayer and fasting.
Mather tried to convert the Catholic Goodwife Glover before she was executed, accused of practicing witchcraft on the Goodwin children. Hansen claimed Mather acted as a moderating influence in the trials by opposing the death penalty for those who confessed—or feigned confession—such as Tituba and Dorcas Good. Most interesting was Mather's decision not to tell the community of the others whom Goodwife Glover claimed practiced witchcraft. Robert Calef's observation of Mather's dealings with Margaret Rule suggested Mather's motivation for keeping quiet was blackmail, with Mather perceived as drawing information from her through leading questions, and possibly having a prurient interest--"Smutty" in Mather's words—in his intimate dealings with afflicted young women. Hansen also claims that most negative impressions of Cotton Mather stem from his defense of the ongoing trials in "Wonders of the Invisible World." After others had lamented the roles they played in the executions of nineteen and imprisonment of hundreds, Mather remained the chief defender of the trials, which diminishes the view of him as a moderate influence.
Historian Bernard Rosenthal laments that Mather is so often portrayed as the rabid witch hunter. Rosenthal suggests that Mather might have had guilty feelings—feigned or not—for choosing not to restrain the judges during the trial, though he was in the best position to do so. Historian Larry Gragg highlights Mather's cloudy thinking and confusion between sympathy for the possessed, and the boundlessness of spectral evidence when Mather stated, "the devil have sometimes represented the shapes of persons not only innocent, but also the very virtuous." And writing in the early 1980s, historian John Demos seemed to consider Mather a moderating influence on the trials.
Major works.
"Boston Ephemeris".
The Boston Ephemeris was an almanac written by Mather in 1686. The content was similar to what is known today as the Farmer's Almanac. This was particularly important because it shows that Cotton Mather had influence in mathematics during the time of Puritan New England. This almanac contained a significant amount of astronomy, celestial within the text of the Almanac the positions and motions of these celestial bodies, which he must have calculated by hand.
"The Biblia Americana".
When Cotton Mather died, he had an abundance of unfinished writings left behind, including one entitled "The Biblia Americana". Mather believed that "Biblia Americana" was the best thing he had ever written, believing it to be his masterwork (Hovey 533).
"Biblia Americana" contained Cotton Mather's thoughts and opinions on the Bible and how he interpreted it. "Biblia Americana" is incredibly large and Mather worked on it from 1693–1728, when he died. Mather tried to convince others that philosophy and science could work together with religion instead of against it. People did not have to choose one or the other and in "Biblia Americana" Mather looked at the Bible through a scientific perspective, the complete opposite of when he wrote "The Christian Philosopher", in which he decided to approach science in a religious manner (Smolinski 280–281).
"Pillars of Salt".
The Puritan execution sermon, preached on the occasion of a public hanging, then quickly printed up in pamphlet form and sold for a few pence, was an early form of true-crime literature. Mather's first published sermon, which appeared in 1686, concerned the crime and punishment of James Morgan, a reprobate who in a drunken rage impaled a man with an iron spit. Thirteen years later, following the execution of a Boston woman named Sarah Threeneedles for killing her baby, Mather issued "Pillars of Salt". This compilation of a dozen accounts (half of which, including the case of Morgan, had been previously published) stands as a landmark work, a Puritan precursor of the true-crime miscellanies that, stripped of all religious intent, would become a staple of the genre in subsequent centuries. In 2008 The Library of America reprinted the entirety of "Pillars of Salt" in its two-century retrospective of American True Crime.
"Magnalia Christi Americana".
"Magnalia Christi Americana", considered Mather's greatest work, was published in 1702, when he was 39. The book includes several biographies of saints, and describes the process of the New England settlement.(Meyers 23–24) It is composed of seven total books, including Pietas in Patriam: the life of His Excellency Sir William Phips, originally published anonymously in London in 1697. Despite being one of Mather's most well-known works, many have openly criticized it, labeling it as hard to follow and understand, and poorly paced and organized. However, other critics have praised Mather's works, believing it to be one of the best efforts at properly documenting the establishment of America and growth of the people (Halttunen 311).
"The Christian Philosopher".
In 1721 "The Christian Philosopher" was published. Written by Mather, it was the first systematic book on science published in America. Mather attempted to show how Newtonian science and religion were in harmony. It was in part based on Robert Boyle's "The Christian Virtuoso" (1690).[13]
Mather also took inspiration from "Hayy ibn Yaqdhan", a philosophical novel by Abu Bakr Ibn Tufail (whom he refers to as "Abubekar"), a 12th-century Islamic philosopher. Despite condemning the 'Mahometans' as infidels, he viewed the protagonist of the novel, Hayy, as a model for his ideal Christian philosopher and monotheistic scientist. Mather also viewed Hayy as a noble savage and applied this in the context of attempting to understand the Native American Indians in order to convert them to Puritan Christianity.[14]

</doc>
<doc id="7105" url="http://en.wikipedia.org/wiki?curid=7105" title="Cordwainer Smith">
Cordwainer Smith

Cordwainer Smith (pronounced "CORDwainer") was the pseudonym used by American author Paul Myron Anthony Linebarger (July 11, 1913 – August 6, 1966) for his science fiction works. Linebarger was a noted East Asia scholar and expert in psychological warfare. ("Cordwainer" is an archaic word for "A worker in cordwain or cordovan leather; a shoemaker", and a "smith" is "One who works in iron or other metals; esp. a blacksmith or farrier": two kinds of skilled workers with traditional materials.)
Linebarger also employed the literary pseudonyms "Carmichael Smith" (for his political thriller "Atomsk"), "Anthony Bearden" (for his poetry) and "Felix C. Forrest" (for the novels "Ria" and "Carola"). He died of a heart attack in 1966 at Johns Hopkins University Medical Center in Baltimore, Maryland, at age 53.
Early life and education.
Linebarger was born in Milwaukee, Wisconsin. His father was Paul M. W. Linebarger, a lawyer and political activist with close ties to the leaders of the Chinese revolution of 1911. As a result of those connections, Linebarger's godfather was Sun Yat-sen, considered the father of Chinese nationalism.
As a child, Linebarger was blinded in his right eye; the vision in his remaining eye was impaired by infection. His father moved his family to France and then Germany while Sun Yat-sen was struggling against contentious warlords in China. As a result, Linebarger was familiar with six languages by adulthood.
At the age of 23, he received a PhD in Political Science from Johns Hopkins University.
Career.
From 1937 to 1946, Linebarger held a faculty appointment at Duke University, where he began producing highly regarded works on Far Eastern affairs.
While retaining his professorship at Duke after the beginning of World War II, Linebarger began serving as a second lieutenant of the United States Army, where he was involved in the creation of the Office of War Information and the Operation Planning and Intelligence Board. He also helped organize the Army's first psychological warfare section. In 1943, he was sent to China to coordinate military intelligence operations. When he later pursued his interest in China, Linebarger became a close confidant of Chiang Kai-shek. By the end of the war, he had risen to the rank of major.
In 1947, Linebarger moved to the Johns Hopkins University's School of Advanced International Studies in Washington, DC, where he served as Professor of Asiatic Studies. He used his experiences in the war to write the book "Psychological Warfare" (1948). It is regarded by many in the field as a classic text.
He eventually rose to the rank of colonel in the reserves. He was recalled to advise the British forces in the Malayan Emergency and the U.S. Eighth Army in the Korean War. While he was known to call himself a "visitor to small wars", he refrained from becoming involved in Vietnam, but is known to have done undocumented work for the Central Intelligence Agency. He traveled extensively and became a member of the Foreign Policy Association, and was called upon to advise then–U.S. President John F. Kennedy.
Marriage and family.
In 1936, Linebarger married Margaret Snow. They had a daughter in 1942 and another in 1947. They divorced in 1949.
In 1950, Linebarger married again to Genevieve Collins; they had several children. They were married until his death from a heart attack in 1966, in Baltimore, Maryland. Linebarger had expressed a wish to retire to Australia, which he had visited in his travels, but died at age 53.
Colonel Linebarger is buried in Arlington National Cemetery, Section 35, Grave Number 4712. After her death, his widow, Genevieve Collins Linebarger, was interred with him on November 16, 1981.
Case history debate.
Linebarger was long rumored to have been the original for "Kirk Allen," the fantasy-haunted subject of "The Jet-Propelled Couch," a chapter in psychologist Robert M. Lindner's best-selling 1954 collection, "The Fifty-Minute Hour." According to Cordwainer Smith scholar Alan C. Elms, this speculation first reached print in Brian Aldiss's 1973 history of science fiction, "Billion Year Spree"; Aldiss, in turn, claimed to have gotten the information from Leon Stover. More recently, both Elms and librarian Lee Weinstein have gathered circumstantial evidence to support the case for Linebarger's being "Allen," but both concede there is no direct proof that Linebarger was ever a patient of Lindner's or that he suffered from a disorder similar to that of "Kirk Allen." 
Science fiction style.
A notable characteristic of Linebarger's science fiction is that most of his stories are set in the same universe, with a unified chronology. Some anthologies of Linebarger's fiction include a chart, with each of his stories inserted into the appropriate slot in the timeline. All his writings suggest a rich universe developing over a long period of time, but leave much to be guessed at by the reader.
Linebarger's stories are unusual, sometimes being written in narrative styles closer to traditional Chinese stories than to most English-language fiction, as well as reminiscent of the Genji tales of Lady Murasaki. The total volume of his science fiction output is relatively small, because of his time-consuming profession and his early death.
Smith's works consist of: a single novel, originally published in two volumes in edited form as "The Planet Buyer", also known as "The Boy Who Bought Old Earth" (1964) and "The Underpeople" (1968), and later restored to its original form as "Norstrilia" (1975); and 32 short stories (collected in "The Rediscovery of Man" (1993), including two versions of the short story "War No. 81-Q").
Linebarger's cultural links to China are partially expressed in the pseudonym "Felix C. Forrest", which he used in addition to "Cordwainer Smith": his godfather Sun Yat-Sen suggested to Linebarger that he adopt the Chinese name "Lin Bai-lo" (), which may be roughly translated as "Forest of Incandescent Bliss". In his later years, Linebarger proudly wore a tie with the Chinese characters for this name embroidered on it.
As an expert in psychological warfare, Linebarger was very interested in the newly developing fields of psychology and psychiatry. He used many of their concepts into his fiction. His fiction often has religious overtones or motifs, particularly evident in characters who have no control over their actions. James P. Jordan argued for the importance of Anglicanism to Linebarger's works back to 1949. But Linebarger's daughter Rosana Hart has indicated that he did not become an Anglican until 1950, and was not strongly interested in religion until later still. The introduction to the collection "Rediscovery of Man" notes that from around 1960 Linebarger became more devout and expressed this in his writing. Linebarger's works are sometimes included in analyses of Christianity in fiction, along with the works of authors such as C. S. Lewis and J.R.R. Tolkien.
Most of Smith's stories are set in an era starting some 14,000 years in the future. The Instrumentality of Mankind rules Earth and goes on to control other planets later inhabited by humanity. The Instrumentality attempts to revive old cultures and languages in a process known as the Rediscovery of Man. This rediscovery can be seen as the initial period when humankind emerges from a mundane utopia and the nonhuman Underpeople gain freedom from slavery. It may also be viewed as part of a continuing process begun by the Instrumentality, encompassing the whole cycle, where mankind is constantly at risk of falling back into bad old ways.
For years, Cordwainer Smith had a pocket notebook which he had filled with ideas about The Instrumentality and additional stories in the series. But while in a small boat in a lake or bay in the mid 60s, he leaned over the side, and his notebook fell out of his breast pocket into the water, where it was lost forever. Another story claims that he accidentally left the notebook in a restaurant in Rhodes in 1965. With the book gone, he felt empty of ideas, and decided to start a new series which was an allegory of Mid-Eastern politics.
Smith's stories describe a long future history of Earth. The settings range from a postapocalyptic landscape with walled cities, defended by agents of the Instrumentality, to a state of sterile utopia, in which freedom can be found only deep below the surface, in long-forgotten and buried anthropogenic strata. These features may place Smith's works within the Dying Earth subgenre of science fiction. They are ultimately more optimistic and distinctive.
Smith's most celebrated short story is his first-published, "Scanners Live in Vain", which led many of its earliest readers to assume that "Cordwainer Smith" was a new pen name for one of the established giants of the genre. It was selected as one of the best science fiction short stories of the pre-Nebula Award period by the Science Fiction and Fantasy Writers of America. It was selected for "The Science Fiction Hall of Fame Volume One, 1929-1964".
Linebarger's stories feature strange and vivid creations, such as:
Published fiction.
Short stories.
Titles marked with an asterisk * are independent stories not related to the Instrumentality universe.

</doc>
<doc id="7110" url="http://en.wikipedia.org/wiki?curid=7110" title="CSS (disambiguation)">
CSS (disambiguation)

CSS is Cascading Style Sheets, a language used to describe the style of document presentations in web development.
CSS may also refer to:

</doc>
<doc id="7112" url="http://en.wikipedia.org/wiki?curid=7112" title="Colorado Front Range">
Colorado Front Range

The Colorado Front Range is a colloquial geographic term for the most populous region of the state of Colorado in the United States. The area is located just east of the foothills of the Front Range, aligned in a north-south configuration on the western edge of the Great Plains, where they meet the Rockies. Geologically, the region lies mostly within the Colorado Piedmont, in the valley of the South Platte and Arkansas rivers on the east side of the Rockies. The region contains the largest cities and the majority of the population of Colorado.
The Colorado Front Range communities include (in a roughly north-to-south order):
The addition of the adjacent Wyoming city of Cheyenne to the Colorado Front Range forms the full Front Range Urban Corridor.

</doc>
<doc id="7118" url="http://en.wikipedia.org/wiki?curid=7118" title="Churnsike Lodge">
Churnsike Lodge

Churnsike Lodge () was an early Victorian hunting lodge situated in the parish of Greystead, West Northumberland, England. It was built in 1850 as a shooting lodge and was part of the Hesleyside estate (Hesleyside house is situated in the North Tyne valley near Bellingham). When the estate was sold in 1889, Churnsike Lodge was purchased by the Chesters Estate (near Hexham, Northumberland). The "cairnsyke" estate comprised several thousand acres of grouse moor and is referred to in the sale catalogue of 1889 as the "Finest grouse moor in the Kingdom". The property included stables for 6 horses, a gamekeepers bothy and well-appointed dog kennels which housed the Irthing head and Kielder hounds (headed by famous fox hunter William Dodd, as referred to in "Wanny Blossoms"). Situated 10 miles north of Gilsland and 13 miles west of Bellingham, the former grouse moor is now part of the Wark forest but there are areas still not covered by conifers where grouse can be found. Churnsike Lodge is now in private ownership.

</doc>
<doc id="7119" url="http://en.wikipedia.org/wiki?curid=7119" title="William Kidd">
William Kidd

Captain William Kidd (c. 22 January 1645 – 23 May 1701) was a Scottish sailor who was tried and executed for piracy after returning from a voyage to the Indian Ocean. Some modern historians deem his piratical reputation unjust, as there is evidence that Kidd acted only as a privateer. Kidd's fame springs largely from the sensational circumstances of his questioning before the English Parliament and the ensuing trial. His actual depredations on the high seas, whether piratical or not, were both less destructive and less lucrative than those of many other contemporary pirates and privateers.
Biography.
Captain William Kidd was either one of the most notorious pirates in the history of the world or one of its most unjustly vilified and prosecuted privateers in an age typified by the rationalisation of empire. Despite the legends and fiction surrounding this character, his actual career was punctuated by only a handful of skirmishes followed by a desperate quest to clear his name.
Kidd was born in Dundee, Scotland, January 1645. He gave the city as his place of birth and said he was aged 41 in testimony under oath at the High Court of the Admiralty in October 1695 or 1694. Researcher Dr David Dobson later identified his baptism documents from Dundee in 1645. His father was Captain John Kyd, who was lost at sea. A local society supported the family financially . Richard Zacks in the biography The Pirate Hunter (2002) says Kidd came from Dundee. Reports that Kidd came from Greenock have been dismissed by Dr. Dobson, who found neither the name Kidd nor Kyd in baptismal records. The myth that his "father was thought to have been a Church of Scotland minister" is also discounted. There is no mention of the name in comprehensive Church of Scotland records for the period. A contrary view is presented here Kidd later settled in the new colony of New York. It was here that he befriended many prominent colonial citizens, including three governors. There is some information that suggests he was a seaman's apprentice on a pirate ship much earlier than his own more famous seagoing exploits.
The first records of his life date from 1689, when he was a member of a French-English pirate crew that sailed in the Caribbean. Kidd and other members of the crew mutinied, ousted the captain off the ship, and sailed to the British colony of Nevis. There they renamed the ship "Blessed William". Kidd became captain, either the result of an election of the ship's crew or because of appointment by Christopher Codrington, governor of the island of Nevis. Captain Kidd and "Blessed William" became part of a small fleet assembled by Codrington to defend Nevis from the French, with whom the English were at war. In either case, he must have been an experienced leader and sailor by that time. As the governor did not want to pay the sailors for their defensive services, he told them they could take their pay from the French. Kidd and his men attacked the French island of Mariegalante, destroyed the only town, and looted the area, gathering for themselves something around 2,000 pounds Sterling.
During the War of the Grand Alliance, on orders from the provinces of New York and Massachusetts, Kidd captured an enemy privateer, which duty he was commissioned to perform off the New England coast. Shortly thereafter, Kidd was awarded £150 for successful privateering in the Caribbean. One year later, Captain Robert Culliford, a notorious pirate, stole Kidd's ship while he was ashore at Antigua in the West Indies. In 1695, William III of England replaced the corrupt governor Benjamin Fletcher, known for accepting bribes of one hundred dollars to allow illegal trading of pirate loot, with Richard Coote, Earl of Bellomont. In New York City, Kidd was active in the building of Trinity Church, New York.
On 16 May 1691, Kidd married Sarah Bradley Cox Oort, an English woman in her early twenties, who had already been twice widowed and was one of the wealthiest women in New York, largely because of her inheritance from her first husband.
Preparing his expedition.
On 11 December 1695, Belmont, who was now governing New York, Massachusetts, and New Hampshire, asked the "trusty and well beloved Captain Kidd" to attack Thomas Tew, John Ireland, Thomas Wake, William Maze, and all others who associated themselves with pirates, along with any enemy French ships. This request, if turned down, would have been viewed as disloyalty to the crown, the perception of which carried much social stigma, making it difficult for Kidd to have done so. The request preceded the voyage which established Kidd's reputation as a pirate, and marked his image in history and folklore.
Four-fifths of the cost for the venture was paid for by noble lords, who were among the most powerful men in England: the Earl of Orford, the Baron of Romney, the Duke of Shrewsbury and Sir John Somers. Kidd was presented with a letter of marque, signed personally by King William III of England. This letter reserved 10% of the loot for the Crown, and Henry Gilbert's "The Book of Pirates" suggests that the King may have fronted some of the money for the voyage himself. Kidd and an acquaintance, Colonel Robert Livingston, orchestrated the whole plan and paid for the rest. Kidd had to sell his ship "Antigua" to raise funds.
The new ship, "Adventure Galley", was well suited to the task of catching pirates; weighing over 284 tons burthen, she was equipped with 34 cannon, oars, and 150 men. The oars were a key advantage as they enabled "Adventure Galley" to manoeuvre in a battle when the winds had calmed and other ships were dead in the water. Kidd took pride in personally selecting the crew, choosing only those he deemed to be the best and most loyal officers.
Because of Kidd's refusal to salute, the Navy vessel's captain retaliated by pressing much of Kidd's crew into naval service, this despite rampant protests. Thus short-handed, Kidd sailed for New York City, capturing a French vessel en route (which was legal under the terms of his commission). To make up for the lack of officers, Kidd picked up replacement crew in New York, the vast majority of whom were known and hardened criminals, some undoubtedly former pirates.
Among Kidd's officers was his quartermaster, Hendrick van der Heul. The quartermaster was considered 'second in command' to the captain in pirate culture of this era. It is not clear, however, if van der Heul exercised this degree of responsibility because Kidd was nominally a privateer. Van der Heul is also noteworthy because he may have been African or of African-American descent. A contemporary source describes him as a ""small black Man"." However, the meaning of this term is not certain as, in late seventeenth-century usage, the term "negro" would have been normally used, and the phrase "black Man" could mean either dark-skinned (but still "white") or black-haired. If van der Heul was indeed of African ancestry, this fact would make him the highest ranking black pirate so far identified. Van der Heul went on to become a master's mate on a merchant vessel, and was never convicted of piracy.
Hunting for pirates.
In September 1696, Kidd weighed anchor and set course for the Cape of Good Hope. A third of his crew soon perished on the Comoros due to an outbreak of cholera, the brand-new ship developed many leaks, and he failed to find the pirates he expected to encounter off Madagascar. Kidd then sailed to the Strait of Bab-el-Mandeb at the southern entrance of the Red Sea, one of the most popular haunts of rovers on the Pirate Round. Here he again failed to find any pirates. According to Edward Barlow, a captain employed by the English East India Company, Kidd attacked a Mughal convoy here under escort by Barlow's East Indiaman, and was repelled. If the report is true, this marked Kidd's first foray into piracy.
As it became obvious his ambitious enterprise was failing, he became understandably desperate to cover its costs. But, once again, Kidd failed to attack several ships when given a chance, including a Dutchman and New York privateer. Some of the crew deserted Kidd the next time "Adventure Galley" anchored offshore, and those who decided to stay on made constant open-threats of mutiny.
Kidd killed one of his own crewmen on 30 October 1697. While Kidd's gunner, William Moore, was on deck sharpening a chisel, a Dutch ship appeared in sight. Moore urged Kidd to attack the Dutchman, an act not only piratical but also certain to anger the Dutch-born King William. Kidd refused, calling Moore a lousy dog. Moore retorted, "If I am a lousy dog, you have made me so; you have brought me to ruin and many more." Kidd snatched up and heaved an ironbound bucket at Moore. Moore fell to the deck with a fractured skull and died the following day.
While seventeenth century English admiralty law allowed captains great leeway in using violence against their crew, outright murder was not permitted. But Kidd seemed unconcerned, later explaining to his surgeon that he had "good friends in England, that will bring me off for that."
Accusations of piracy.
Acts of savagery on Kidd's part were reported by escaped prisoners, who told stories of being hoisted up by the arms and "drubbed" with a drawn cutlass. On one occasion, crew members ransacked the trading ship "Mary" and tortured several of its crew members while Kidd and the other captain, Thomas Parker, conversed privately in Kidd's cabin. When Kidd found out what had happened, he was outraged and forced his men to return most of the stolen property.
Kidd was declared a pirate very early in his voyage by a Royal Navy officer to whom he had promised "thirty men or so". Kidd sailed away during the night to preserve his crew, rather than subject them to Royal Navy impressment.
On 30 January 1698, he raised French colours and took his greatest prize, an Armenian ship, the 400 ton "Quedagh Merchant", which was loaded with satins, muslins, gold, silver, an incredible variety of East Indian merchandise, as well as extremely valuable silks. The captain of "Quedagh Merchant" was an Englishman named Wright, who had purchased passes from the French East India Company promising him the protection of the French Crown. After realising the captain of the taken vessel was an Englishman, Kidd tried to persuade his crew to return the ship to its owners, but they refused, claiming that their prey was perfectly legal as Kidd was commissioned to take French ships, and that an Armenian ship counted as French if it had French passes. In an attempt to maintain his tenuous control over his crew, Kidd relented and kept the prize. When this news reached England, it confirmed Kidd's reputation as a pirate, and various naval commanders were ordered to "pursue and seize the said Kidd and his accomplices" for the "notorious piracies" they had committed.
Kidd kept the French passes of "Quedagh Merchant", as well as the vessel itself. While the passes were at best a dubious defence of his capture, British admiralty and vice-admiralty courts (especially in North America) heretofore had often winked at privateers' excesses into piracy, and Kidd may have been hoping that the passes would provide the legal fig leaf that would allow him to keep "Quedagh Merchant" and her cargo. Renaming the seized merchantman "Adventure Prize", he set sail for Madagascar.
On 1 April 1698, Kidd reached Madagascar. Here he found the first pirate of his voyage, Robert Culliford (the same man who had stolen Kidd’s ship years before), and his crew aboard "Mocha Frigate". Two contradictory accounts exist of how Kidd reacted to his encounter with Culliford. According to "The General History of the Pirates", published more than 25 years after the event by an author whose very identity remains in dispute, Kidd made peaceful overtures to Culliford: he "drank their Captain's health," swearing that "he was in every respect their Brother," and gave Culliford "a Present of an Anchor and some Guns." This account appears to be based on the testimony of Kidd's crewmen Joseph Palmer and Robert Bradinham at his trial. The other version was presented by Richard Zacks in his 2002 book "The Pirate Hunter: The True Story of Captain Kidd." According to Zacks, Kidd was unaware that Culliford had only about 20 crew with him, and felt ill manned and ill equipped to take "Mocha Frigate" until his two prize ships and crews arrived, so he decided not to molest Culliford until these reinforcements came. After "Adventure Prize" and "Rouparelle" came in, Kidd ordered his crew to attack Culliford's "Mocha Frigate". However, his crew, despite their previous eagerness to seize any available prize, refused to attack Culliford and threatened instead to shoot Kidd. Zacks does not refer to any source for his version of events.
Both accounts agree that most of Kidd's men now abandoned him for Culliford. Only 13 remained with "Adventure Galley." Deciding to return home, Kidd left the "Adventure Galley" behind, ordering her to be burnt because she had become worm-eaten and leaky. Before burning the ship, he was able to salvage every last scrap of metal, such as hinges. With the loyal remnant of his crew, he returned to the Caribbean aboard the "Adventure Prize."
Trial and execution.
Prior to Kidd returning to New York City, he learned that he was a wanted pirate, and that several English men-of-war were searching for him. Realizing that "Adventure Prize" was a marked vessel, he cached it in the Caribbean Sea and continued toward New York aboard a sloop. He deposited some of his treasure on Gardiners Island, hoping to use his knowledge of its location as a bargaining tool.
Bellomont (an investor) was away in Boston, Massachusetts. Aware of the accusations against Kidd, Bellomont was justifiably afraid of being implicated in piracy himself, and knew that presenting Kidd to England in chains was his best chance to save himself. He lured Kidd into Boston with false promises of clemency, then ordered him arrested on 6 July 1699. Kidd was placed in Stone Prison, spending most of the time in solitary confinement. His wife, Sarah, was also imprisoned. The conditions of Kidd's imprisonment were extremely harsh, and appear to have driven him at least temporarily insane.
He was eventually (after over a year) sent to England for questioning by Parliament. The new Tory ministry hoped to use Kidd as a tool to discredit the Whigs who had backed him, but Kidd refused to name names, naively confident his patrons would reward his loyalty by interceding on his behalf. There is speculation that he probably would have been spared had he talked. Finding Kidd politically useless, the Tory leaders sent him to stand trial before the High Court of Admiralty in London for the charges of piracy on high seas and the murder of William Moore. Whilst awaiting trial, Kidd was confined in the infamous Newgate Prison and wrote several letters to King William requesting clemency.
Kidd had two lawyers to assist in his defence. He was shocked to learn at his trial that he was charged with murder. He was found guilty on all charges (murder and five counts of piracy). He was hanged on 23 May 1701, at 'Execution Dock', Wapping, in London. During the execution, the hangman's rope broke and Kidd was hanged on the second attempt. His body was gibbeted over the River Thames at Tilbury Point—as a warning to future would-be pirates—for three years.
His associates Richard Barleycorn, Robert Lamley, William Jenkins, Gabriel Loffe, Able Owens, and Hugh Parrot were convicted, but pardoned just prior to hanging at Execution Dock.
Kidd's Whig backers were embarrassed by his trial. Far from rewarding his loyalty, they participated in the effort to convict him by depriving him of the money and information which might have provided him with some legal defence. In particular, the two sets of French passes he had kept were missing at his trial. These passes (and others dated 1700) resurfaced in the early twentieth century, misfiled with other government papers in a London building. These passes call the extent of Kidd's guilt into question. Along with the papers, many goods were brought from the ships and soon auctioned off as "pirate plunder." They were never mentioned in the trial. Nevertheless, none of these items would have prevented his conviction for murdering Moore.
As to the accusations of murdering Moore, on this he was mostly sunk on the testimony of the two former crew members, Palmer and Bradinham, who testified against him in exchange for pardons. A deposition Palmer gave when he was captured in Rhode Island two years earlier contradicted his testimony and may have supported Kidd's assertions, but Kidd was unable to obtain the deposition.
A broadside song "Captain Kidd's Farewell to the Seas, or, the Famous Pirate's Lament" was printed shortly after his execution and popularised the common belief that Kidd had confessed to the false charges.
Mythology and legend.
The belief that Kidd had left buried treasure contributed considerably to the growth of his legend. The 1701 broadside song "Captain Kid's Farewell to the Seas, or, the Famous Pirate's Lament" lists "Two hundred bars of gold, and rix dollars manifold, we seized uncontrolled". This belief made its contributions to literature in Edgar Allan Poe's "The Gold-Bug", Washington Irving's "The Devil and Tom Walker", Robert Louis Stevenson's "Treasure Island" and Nelson DeMille's "Plum Island". It also gave impetus to the constant treasure hunts conducted on Oak Island in Nova Scotia, in Suffolk County, Long Island in New York where Gardiner's Island is located, Charles Island in Milford, Connecticut, the Thimble Islands in Connecticut, Cockenoe Island in Westport, Connecticut and on the island of Grand Manan in the Bay of Fundy.
Captain Kidd did bury a small cache of treasure on Gardiners Island in a spot known as Cherry Tree Field; however, it was removed by Governor Bellomont and sent to England to be used as evidence against him.
Kidd also visited Block Island around 1699, where he was supplied by Mrs. Mercy (Sands) Raymond, daughter of the mariner James Sands. The story has it that, for her hospitality, Mrs. Raymond was bid to hold out her apron, into which Kidd threw gold and jewels until it was full. After her husband Joshua Raymond died, Mercy moved with her family to northern New London, Connecticut (later Montville), where she bought much land. The Raymond family was thus said to have been "enriched by the apron".
On Grand Manan in the Bay of Fundy, as early as 1875, reference was made to searches on the West side of the island for treasure allegedly buried by Kidd during his time as a privateer. For nearly 200 years, this remote area of the island has been called "Money Cove".
In 1983, Cork Graham and Richard Knight went looking for Captain Kidd's buried treasure off the Vietnamese island of Phú Quốc. Knight and Graham were caught, convicted of illegally landing on Vietnamese territory, and assessed each a $10,000 fine. They were imprisoned for 11 months until they paid the fine.
"Quedagh Merchant" found.
For years, people and treasure hunters have tried to locate "Quedagh Merchant". It was reported on 13 December 2007 that "wreckage of a pirate ship abandoned by Captain Kidd in the 17th century has been found by divers in shallow waters off the Dominican Republic." The waters in which the ship was found were less than ten feet deep and were only off Catalina Island, just to the south of La Romana on the Dominican coast. The ship is believed to be "the remains of "Quedagh Merchant"". Charles Beeker, the director of Academic Diving and Underwater Science Programs in Indiana University (Bloomington)'s School of Health, Physical Education, and Recreation, was one of the experts leading the Indiana University diving team. He said that it was "remarkable that the wreck has remained undiscovered all these years given its location," and given that the ship has been the subject of so many prior failed searches. Captain Kidd's cannon, an artefact from the shipwreck was added to a permanent exhibit at The Children's Museum of Indianapolis in 2011.

</doc>
<doc id="7120" url="http://en.wikipedia.org/wiki?curid=7120" title="Calreticulin">
Calreticulin

Calreticulin also known as calregulin, CRP55, CaBP3, calsequestrin-like protein, and endoplasmic reticulum resident protein 60 (ERp60) is a protein that in humans is encoded by the "CALR" gene.
Calreticulin is a multifunctional protein that binds Ca2+ ions (a second messenger in signal transduction), rendering it inactive. The Ca2+ is bound with low affinity, but high capacity, and can be released on a signal (see inositol triphosphate). Calreticulin is located in storage compartments associated with the endoplasmic reticulum.
The term "Mobilferrin" is considered to be the same as calreticulin by some sources.
Function.
Calreticulin binds to misfolded proteins and prevents them from being exported from the endoplasmic reticulum to the Golgi apparatus.
A similar quality-control chaperone, calnexin, performs the same service for soluble proteins as does calreticulin. Both proteins, calnexin and calreticulin, have the function of binding to oligosaccharides containing terminal glucose residues, thereby targeting them for degradation. In normal cellular function, trimming of glucose residues off the core oligosaccharide added during N-linked glycosylation is a part of protein processing. If "overseer" enzymes note that residues are misfolded, proteins within the RER will re-add glucose residues so that other calreticulin/calnexin can bind to these proteins and prevent them from proceeding to the Golgi. This leads these aberrantly folded proteins down a path whereby they are targeted for degradation.
Studies on transgenic mice reveal that calreticulin is a cardiac embryonic gene that is essential during development.
Transcription regulation.
Calreticulin is also found in the nucleus, suggesting that it may have a role in transcription regulation. Calreticulin binds to the synthetic peptide KLGFFKR, which is almost identical to an amino acid sequence in the DNA-binding domain of the superfamily of nuclear receptors. The amino terminus of calreticulin interacts with the DNA-binding domain of the glucocorticoid receptor and prevents the receptor from binding to its specific glucocorticoid response element. Calreticulin can inhibit the binding of androgen receptor to its hormone-responsive DNA element and can inhibit androgen receptor and retinoic acid receptor transcriptional activities in vivo, as well as retinoic acid-induced neuronal differentiation. Thus, calreticulin can act as an important modulator of the regulation of gene transcription by nuclear hormone receptors.
Clinical significance.
Calreticulin binds to antibodies in certain sera of systemic lupus and Sjogren patients that contain anti-Ro/SSA antibodies. Systemic lupus erythematosus is associated with increased autoantibody titers against calreticulin, but calreticulin is not a Ro/SS-A antigen. Earlier papers referred to calreticulin as an Ro/SS-A antigen, but this was later disproven. Increased autoantibody titer against human calreticulin is found in infants with complete congenital heart block of both the IgG and IgM classes.
In 2013, two groups detected calreticulin mutations in a majority of JAK2-negative/MPL-negative patients with essential thrombocytosis and primary myelofibrosis, which makes "CALR" mutations the second most common in myeloproliferative neoplasms. All mutations (insertions or deletions) affected the last exon, generating a reading frame shift of the resulting protein, that creates a novel terminal peptide and causes a loss of endoplasmic reticulum KDEL retention signal.
Role in cancer.
Calreticulin (CRT) is expressed in many cancer cells and plays a role to promote macrophages to engulf hazardous cancerous cells. The reason why most of the cells are not destroyed is the presence of another molecule with signal CD47, which blocks CRT. Hence antibodies that block CD47 might be useful as a cancer treatment. In mice models of myeloid leukemia and non-Hodgkin’s lymphoma, anti-CD47 were effective in clearing cancer cells while normal cells were unaffected.
Interactions.
Calreticulin has been shown to interact with Perforin and NK2 homeobox 1.

</doc>
<doc id="7122" url="http://en.wikipedia.org/wiki?curid=7122" title="Crannog">
Crannog

A crannog (; ; ) is typically a partially or entirely artificial island, usually built in lakes, rivers and estuarine waters of Scotland and Ireland. Unlike the prehistoric pile dwellings around the Alps that were built on the shores and were inundated only later on, crannogs were built in the water, thus forming artificial islands.
Old Croghan Mans were used as dwellings over five millennia, from the European Neolithic Period to as late as the 17th/early 18th century, although in Scotland there is currently no convincing evidence in the archaeological record of Early and Middle Bronze Age or Norse Period use. The earliest radiocarbon determinations obtained from key sites such as Oakbank in Loch Tay and Redcastle, Beauly Firth approach the Late Bronze Age - Early Iron Age transition at their widest interpretation at 2 standard deviations or a 95.4% confidence level: they fall "after" around 800 BC and so could be considered Late Bronze Age by only the narrowest of margins. Crannogs have been variously interpreted as free-standing wooden structures, as at Loch Tay, although more commonly they exist as brush, stone or timber mounds that can be revetted with timber piles. However, in areas such as the Western Isles of Scotland, timber was unavailable from the Neolithic era onwards. As a result, completely stone crannogs supporting drystone architecture are common here. Today, crannogs typically appear as small, circular islets, often in diameter, covered in dense vegetation due to their inaccessibility to grazing livestock.
Etymology and uncertain meanings.
The Irish word derives from Old Irish , that referred to a wooden structure or vessel, stemming from "crann", which means "tree", plus a diminutive ending—literally "young tree". The modern sense of the term first appears sometime around the 12th century; its popularity spread in the medieval period along with the terms "isle", "ylle", "inis", "eilean", "oileán". There is some confusion on what the term "crannog" originally referred to, the structure atop the island or the island itself. The additional meanings of 'crannog' can be variously related as "structure/piece of wood; wooden pin; crow's nest; pulpit; driver's box on a coach and vessel/box/chest" for crannóg. The Scottish Gaelic form is and has the additional meanings of "pulpit" and "churn". Thus there is no real consensus on what the term "crannog" actually implies, although the modern adoption in the English language broadly refers to a partially or completely artificial islet that saw use from the prehistoric to the Post-Medieval period in Ireland and Scotland.
Location.
Crannogs are widespread in Ireland with an estimated 1,200 examples, while Scotland has 347 sites officially listed as such. The actual number in Scotland varies considerably depending on definition—between about 350 to 500, due to the use of the term "island dun" for well over one hundred Hebridean examples—a distinction that has created a divide between mainland Scottish crannog and Hebridean islet settlement studies. Previously unknown crannogs in Scotland and Ireland are still being found as underwater surveys continue to investigate loch beds for completely submerged examples. The largest concentrations of crannogs in Ireland are found in the Drumlin Belt of the Midlands, North and Northwest. In Scotland, crannogs favour a western or 'Atlantic distribution', with high concentrations in Argyll and Dumfries and Galloway. In reality, the Western Isles contain the highest density of lake-settlement in Scotland, yet they are recognised under varying terms besides "crannog". One lone Welsh example at Llangorse Lake exists, likely a product of Irish influence across the Irish Sea.
Reconstructed Irish crannógs are located in Craggaunowen, Ireland; the Irish National Heritage Park, in Wexford, Ireland; and in Scotland at the "Scottish Crannog Centre" at Loch Tay, Perthshire. This centre offers guided tours and hands-on activities, including wool-spinning, wood-turning and making fire, holds events to celebrate wild cooking and crafts, and hosts yearly Midsummer, Lughnasadh and Samhain festivals.
Types and problems with definition.
Crannogs took on many different forms and methods of construction based on what was available in the immediate landscape. The classic image of a prehistoric crannog stems from both post-medieval illustrations and highly influential excavations such as Milton Loch in Scotland by C.M. Piggot after World War II. The Milton Loch interpretation is of a small islet surrounded or defined at its edges by timber piles and a gangway, topped by a typical Iron Age roundhouse. The choice of a small islet as a home may seem odd today, yet waterways were the main channels for both communication and travel until the 19th century in much of Ireland and especially Highland Scotland. Crannogs are traditionally interpreted as simple prehistorical farmsteads. They are also interpreted as boltholes in times of danger, as status symbols with limited access and as inherited locations of power that imply a sense of legitimacy and ancestry towards ownership of the surrounding landscape.
A strict definition of a crannog, which has long been debated, requires the use of timber. Sites in the Western Isles do not satisfy this criterion, although their inhabitants shared the common habit of living on water. If not classed as "true" crannogs, small occupied islets (often at least partially artificial in nature) may be referred to as "island duns", although rather confusingly, 22 islet-based sites are classified as "proper" crannogs due to the different interpretations of the inspectors or excavators who drew up field reports Hebridean island dwellings or crannogs were commonly built on both natural and artificial islets, usually reached by a stone causeway. The visible structural remains are traditionally interpreted as duns, or in more recent terminology as "Atlantic roundhouses". This terminology has recently become popular when describing the entire range of robust, drystone structures that existed in later prehistoric Atlantic Scotland.
The majority of crannog excavations were poorly conducted (by modern standards) in the late 19th and early 20th centuries by early antiquarians, or were indeed purely accidental finds as lochs were drained during the improvements to increase usable farmland or pasture. In some early digs, labourers merely hauled away tons of materials with little regard to anything that was not of immediate economic value. Conversely, the vast majority of early attempts at proper excavation failed to accurately measure or record stratigraphy, thereby failing to provide a secure context for artefact finds. Thus only extremely limited interpretations are possible. Preservation and conservation techniques for waterlogged materials such as logboats or structural material were all but non-existent, and a number of extremely important finds perished as a result: in some instances dried out for firewood. From about 1900 to the late 1940s there was very little crannog excavation in Scotland, while Ireland did witness some important and highly influential contributions. In contrast, relatively few crannogs have been excavated since the Second World War, although this number has steadily grown, especially since the early 1980s, and may soon surpass pre-war totals. The overwhelming majority of crannogs show multiple phases of occupation and re-use, often extending over centuries. Thus the re-occupiers may have viewed crannogs as a legacy that remained alive in local tradition and memory. Crannog reoccupation is important and significant, especially in the many instances of crannogs built near natural islets that were often completely unused.
This long chronology of use has been verified both by radiocarbon dating and more precisely by dendrochronology. Interpretations of crannog function are not static through time; instead they appear to change in both the archaeological and historic records. Rather than largely simple domestic residences in prehistory, medieval crannogs were increasingly seen as strongholds of the upper class or regional 'political players' such as Gaelic chieftains like the O'Boylans and McMahons in County Monaghan and the Kingdom of Airgíalla until the 17th century. In Scotland, their medieval and post-medieval use is also documented into the early 18th century. Whether this increase in 'status' is real, or just a by-product of increasingly complex material assemblages remains to be convincingly validated.
History.
The earliest construction of a crannog is the completely artificial Neolithic islet of Eilean Domhnuill, Loch Olabhat on North Uist in Scotland. Eilean Domhnuill has produced radiocarbon dates ranging from 3650 to 2500 BC while Irish crannogs appear from middle Bronze Age layers at Ballinderry (1200–600 BC). Prior to the Bronze Age, the existence of artificial island settlement in Ireland is not as clear. While lakeside settlements are evident in Ireland from 4500 BC, these settlements are not crannogs as they were not intended to be islands. Despite having a tremendous chronology, their use was not at all consistent or unchanging. Crannog construction and occupation was at its peak in Scotland from about 800 BC to AD 200. Not surprisingly, crannogs have useful defensive properties, although there appears to be more significance to prehistoric use than simple defense as very few weapons or evidence for destruction appear in excavations of prehistoric crannogs. In Ireland, crannogs were at their zenith during the Early Historic period when they were the homes and retreats of kings, lords, prosperous farmers and occasionally socially marginalised groups such as monastic hermits or metalsmiths who could work in isolation. However, despite earlier concepts of a strict Early Historic evolution, Irish excavations are increasingly uncovering examples, which date from the "missing" Iron Age in Ireland.
Construction.
The construction techniques for a crannog (prehistoric or otherwise) are as varied as the multitude of finished forms witnessed in the archaeological record. Island settlement in Scotland and Ireland is manifest through the entire range of possibilities ranging from entirely natural, small islets to completely artificial islets, therefore definitions will invariably remain contentious. For 'crannogs' in the strict sense, typically this effort began on a shallow reef or rise in the lochbed. When timber was available, many were surrounded by a circle of wooden piles with axe-sharpened bases that were driven into the bottom, forming a circular enclosure that helped to retain the main mound and prevent erosion. The piles could also be joined together by mortise and tenon, or large holes cut to carefully accept specially shaped timbers designed to interlock and provide structural rigidity. On other examples, interior surfaces were built up with any mixture of clay, peat, stone, timber or brush- whatever was available. In some instances, more than one structure was built on crannogs. Other types of crannogs saw the occupants add large stones to the waterline of small natural islets, extending and enlarging them over successive phases of renewal. Larger crannogs could be occupied by extended families or communal groups, and access was either by logboats or coracles while evidence for timber or stone causeways exists on a large number of crannogs. The causeways themselves may have been slightly submerged; this has been interpreted as a device to make access difficult yet this can also simply be a by-product of loch level fluctuations over the ensuing centuries or indeed millennia. Organic remains are often found in excellent condition on these water-logged sites. The bones of cattle, deer, and swine have been found in excavated crannogs while remains of wooden utensils and even dairy products can remain completely preserved for several millennia.

</doc>
<doc id="7123" url="http://en.wikipedia.org/wiki?curid=7123" title="Calendar date">
Calendar date

A calendar date is a reference to a particular day represented within a calendar system. The calendar date allows the specific day to be identified. The number of days between two dates may be calculated. For example, "24 2015" is ten days after "14 2015" in the Gregorian calendar. The date of a particular event depends on the observed time zone. For example the air attack on Pearl Harbor that began at 7:48 a.m. Hawaiian time on December 7, 1941 took place at 3:18 a.m. December 8 in Japan (Japan Standard Time).
A particular day may be represented by a different date in another calendar as in the Gregorian calendar and the Julian calendar, which have been used simultaneously in different places. In most calendar systems, the date consists of three parts: the "day of month", "month", and the "year". There may also be additional parts, such as the "day of week". Years are usually counted from a particular starting point, usually called the epoch, with era referring to the particular period of time (Note the different use of the terms in geology).
The most widely used epoch is a conventional birthdate of Jesus (which was established by Dionysius Exiguus in the sixth century). A date without the year part may also be referred to as a "date" or "calendar date" (such as "28 " rather than "28 2015"). As such, it defines the day of an annual event, such as a birthday or Christmas on 24/25 December.
Many computer systems internally store points in time in Unix time format or some other system time format.
The date (Unix) command—internally using the C date and time functions—can be used to convert that internal representation of a point in time
to most of the date representations shown here.
Date format.
There is a large variety of formats for dates in use, which differ in the order of date components (e.g. 31/05/1999, 05/31/1999, 1999/05/31), component separators (e.g. 31.05.1999 vs. 31/05/1999), whether leading zeros are included (e.g. 31/5/1999 vs. 31/05/1999), whether all four digits of the year are written (e.g., 31.05.1999 vs. 31.05.99), and whether the month is represented numerically or by name (e.g. 31 May 1999 vs. 31.05.1999).
Gregorian, day-month-year (DMY).
This [[little-endian]] sequence is common to the majority of the world's countries. This date format originates from the custom of writing the date as "the 8th day of November in the year of our Lord 2003" in Western religious and legal documents. The format has shortened over time but the order of the elements has remained constant.
Gregorian, year-month-day (YMD).
In this format the most significant data item is written before lesser data items i.e. year before month before day. It is consistent with the big-endianness of the [[Indian decimal numbering system]], which progresses from the highest to the lowest order magnitude. That is, using this format textual orderings and chronological orderings are identical. This form is standard in Greater China, Iran, Japan, South Korea, Belgium, Lithuania, Hungary, Norway and Sweden; and some other countries to a limited extent.
Examples for the 9th of November 2003:
It is also extended through the universal big-endian format clock time: 9 November 2003, 18h 14m 12s, or 2003/11/9/18:14:12 or (ISO 8601) 2003-11-09T18:14:12.
Gregorian, month-day-year (MDY).
This sequence is used primarily in the United States. This date format was commonly used alongside the little-endian form in the United Kingdom until the mid-20th century and can be found in both defunct and modern print media such as the "[[London Gazette]]" and "[[The Times]]", respectively. In the United States, it is said as Sunday, November 9, although usage of "the" isn't uncommon (e.g. "Sunday, November the 9th", and even "November the 9th, Sunday", are also possible and readily understood).
The modern convention is to avoid using the ordinal (th, st, rd, nd) form of numbers when the day follows the month (July 4 or July 4, 1776); though this was common in the past, and the ordinal is still sometimes used (4th of July or July 4th).
Standards.
There are several standards that specify date formats:
Usage overloading.
Many numerical forms can create confusion when used in international correspondence, particularly when abbreviating the year to its final two digits.
For example, "7/8" could refer to either 7 August or July 8. In the [[United States]], dates are rarely written in purely numerical forms in formal writing, although they are very common elsewhere; when numerical forms are used, the month appears first. In the United Kingdom, while it is regarded as acceptable albeit less common to write "month-name day, year", this order is never used when written numerically. However, as an exception, the American shorthand "9/11" is widely understood as referring to the [[September 11 attacks|11 September 2001 terrorist attacks]].
When numbers are used to represent months, a significant amount of confusion can arise from the ambiguity of a date order; especially when the numbers representing the day, month or year are low, it can be impossible to tell which order is being used. This can be clarified by using four digits to represent years, and naming the month; for example, "Feb" instead of "02". The ISO 8601 date order with four-digit years: YYYY-MM-DD (introduced in ISO 2014), is specifically chosen to be unambiguous. The ISO 8601 standard also has the advantage of being language independent and is therefore useful when there may be no language context and a universal application is desired (expiration dating on export products, for example). Many Internet sites use YYYY-MM-DD, and those using other conventions often use -MMM- for the month to further clarify and avoid ambiguity (2001-MAY-09, 9-MAY-2001, MAY 09 2001, etc.).
In addition, the [[International Organization for Standardization]] considers its ISO 8601 standard to make sense from a logical perspective. Mixed units, for example feet and inches, or pounds and ounces, are normally written with the largest unit first, in decreasing order. Numbers are also written in that order, so the digits of 2006 indicate, in order, the millennium, the century within the millennium, the decade within the century, and the year within the decade. The only date order that is consistent with these well-established conventions is year-month-day. A plain text list of dates with this format can be easily sorted by [[file manager]]s, [[word processor]]s, [[spreadsheet]]s and other software tools with built-in sorting functions. Some database systems use an eight-digit YYYYMMDD representation to handle date values. Naming folders with YYYY-MM-DD at the beginning allows them to be listed in date order when sorting by name - especially useful for organising photograph libraries.
An early U.S. [[Federal Information Processing Standard]] recommended 2-digit years. This is now widely recognized as a bad idea, because of the [[year 2000 problem]]. Some U.S. government agencies now use ISO 8601 with 4-digit years.
When transitioning from one date notation to another, people often write both styles; for example [[Old Style and New Style dates]] in the transition from the Julian to the Gregorian calendar.
Advantages for ordering in sequence.
One of the advantages of using the ISO 8601 date format is that the [[lexicographical order]] ([[ASCIIbetical]]) of the representations is equivalent to the chronological order of the dates, assuming that all dates are in the same time zone. Thus dates can be sorted using simple string comparison algorithms, and indeed by any left to right [[collation]]. For example:
 1998-02-28 (28 February 1998) sorts before
 1999-03-01 (1 March 1999) which sorts before
 2000-01-30 (30 January 2000)
The YYYY-MM-DD layout is the only common format that can provide this. Sorting other date representations involves some [[parsing]] of the date strings. This also works when a time in 24-hour format is included after the date, as long as all times are understood to be in the same time zone.
ISO 8601 is used widely where concise, human readable yet easily computable and unambiguous dates are required, although many applications store dates internally as [[UNIX time]] and only convert to ISO 8601 for display. It is worth noting that all modern computer [[Operating Systems]] retain date information of files outside of their titles, allowing the user to choose which format they prefer and have them sorted thus, irrespective of the files' names.
Specialized usage.
Day and year only.
The U.S. military sometimes uses a system, which they call "Julian date format" that indicates the year and the actual day out of the 365 days of the year (and thus a designation of the month would not be needed). For example, "11 December 1999" can be written in some contexts as "1999345" or "99345", for the 345th day of 1999. This system is most often used in US military logistics, since it makes the process of calculating estimated shipping and arrival dates easier. For example: say a tank engine takes an estimated 35 days to ship by sea from the US to Korea. If the engine is sent on 99104, it should arrive on 99139. Note that outside of the US military and some US government agencies, including the [[Internal Revenue Service]], this format is usually referred to as "[[ordinal date]]", rather than "Julian date" 
Such ordinal date formats are also used by many computer programs (especially those for mainframe systems). Using a three-digit [[Julian date|Julian day number]] saves one byte of computer storage over a two-digit month plus two-digit day, for example, "January 17" is 017 in Julian versus 0117 in month-day format. [[OS/390]] or its successor, [[z/OS]], display dates in yy.ddd format for most operations.
[[UNIX time]], which stores time as a number in seconds since the UNIX Epoch (1970-01-01).
Another "ordinal" date system ("ordinal" in the sense of advancing in value by one as the date advances by one day) is in common use in astronomical calculations and referencing and uses the same name as this "logistics" system. The continuity of representation of period regardless of the time of year being considered is highly useful to both groups of specialists. The astronomers describe their system as also being a "[[Julian date]]" system. Unlike the system described above, the astronomical system does not consider years, it only counts days. Thus it is unperturbed by complications such as leap years.
Week number used.
Companies in Europe often use year, week number and day for planning purposes.
So, for example, an event in a project can happen on w43 (week 43) or w43-1 (Monday, week 43) or, if the year needs to be indicated, on w0543 or w543 (year 2005 week 43).
The ISO does present a standard for [[ISO week date|identifying weeks]], but as it does not match up with Gregorian calendar (the beginning and ending days of a given year do not match up), this standard is somewhat more problematic than the other standards for dates.
Expressing dates in spoken English.
In [[English language|English]] outside of North America, full dates are written as "7 December 1941" (or "7th December 1941") and spoken as "the seventh of December, nineteen forty-one" (exceedingly common usage of "the" and "of"), with the occasional usage of "December 7, 1941" ("December the seventh, nineteen forty-one"). In common with continental European usage, however, all-numeric dates are invariably ordered dd/mm/yyyy.
In the [[United States]], the usual written form is "December 7, 1941", spoken as "December seventh, nineteen forty-one" or colloquially "December the seventh, nineteen forty-one". [[Ordinal number (linguistics)|Ordinal]] numerals, however, are not always used when writing and pronouncing dates, and "December seven, nineteen forty-one" is also an accepted pronunciation of the date written "December 7, 1941". A notable exception to this rule is the [[Fourth of July]] (U.S. [[Independence Day (United States)|Independence Day]]).
External links.
[[Category:Calendars]]

</doc>
<doc id="7124" url="http://en.wikipedia.org/wiki?curid=7124" title="Cist">
Cist

A cist ( or ; also kist ;
from or Germanic "Kiste") is a small stone-built coffin-like box or ossuary used to hold the bodies of the dead. Examples can be found across Europe and in the Middle East.
A cist may have been associated with other monuments, perhaps under a cairn or long barrow. Several cists are sometimes found close together within the same cairn or barrow. Often ornaments have been found within an excavated cist, indicating the wealth or prominence of the interred individual.
In Devonshire a local word for a cist in Modern Cornish is "kistvaen". There are numerous Dartmoor kistvaens.
In the Welsh language (whose origins, like Cornish, are from the ancient British or Brythonic language line), "cist" is also used for such ancient graves, but in modern use, can also mean a chest, a coffer, a box, or even the boot / trunk of a car.

</doc>
<doc id="7125" url="http://en.wikipedia.org/wiki?curid=7125" title="Center (group theory)">
Center (group theory)

In abstract algebra, the center of a group "G", denoted "Z"("G"), is the set of elements that commute with every element of "G". In set-builder notation,
The center is a subgroup of "G", which by definition is abelian (that is, commutative). As a subgroup, it is always normal, and indeed characteristic, but it need not be fully characteristic. The quotient group "G" / "Z"("G") is isomorphic to the group of inner automorphisms of "G".
A group "G" is abelian if and only if "Z"("G") = "G". At the other extreme, a group is said to be centerless if "Z"("G") is trivial, i.e. consists only of the identity element.
The elements of the center are sometimes called central.
As a subgroup.
The center of "G" is always a subgroup of "G". In particular:
Furthermore the center of "G" is always a normal subgroup of "G", as it is closed under conjugation.
Conjugacy classes and centralisers.
By definition, the center is the set of elements for which the conjugacy class of each element is the element itself, i.e. ccl(g) = {g}.
The center is also the intersection of all the centralizers of each element of G. As centralizers are subgroups, this again shows that the center is a subgroup.
Conjugation.
Consider the map "f": "G" → Aut("G") from "G" to the automorphism group of "G" defined by "f"("g") = ϕ"g", where ϕ"g" is the automorphism of "G" defined by 
The function "f" is a group homomorphism, and its kernel is precisely the center of "G", and its image is called the inner automorphism group of "G", denoted Inn("G"). By the first isomorphism theorem we get
The cokernel of this map is the group formula_4 of outer automorphisms, and these form the exact sequence
Higher centers.
Quotienting out by the center of a group yields a sequence of groups called the upper central series:
The kernel of the map formula_18 is the "i"th center of "G" (second center, third center, etc.), and is denoted formula_19 Concretely, the formula_20-st center are the terms that commute with all elements up to an element of the "i"th center. Following this definition, one can define the 0th center of a group to be the identity subgroup. This can be continued to transfinite ordinals by transfinite induction; the union of all the higher centers is called the hypercenter.
The ascending chain of subgroups
stabilizes at "i" (equivalently, formula_22) if and only if formula_23 is centerless.

</doc>
<doc id="7129" url="http://en.wikipedia.org/wiki?curid=7129" title="Commonwealth of England">
Commonwealth of England

The Commonwealth, or Commonwealth of England, was the period from 1649 onwards when England, along later with Ireland and Scotland, was ruled as a republic following the end of the Second English Civil War and the trial and execution of Charles I. The republic's existence was initially declared through "An Act declaring England to be a Commonwealth", adopted by the Rump Parliament on 19 May 1649. Power in the early Commonwealth was vested primarily in the Parliament and a Council of State. During the period, fighting continued, particularly in Ireland and Scotland, between the parliamentary forces and those opposed to them, as part of what is now referred to as the Third English Civil War.
In 1653, after the forcible dissolution of the Rump Parliament, Oliver Cromwell was declared Lord Protector of a united "Commonwealth of England, Scotland and Ireland" under the terms of the Instrument of Government, inaugurating the period now usually known as the Protectorate. After Cromwell's death, and following a brief period of rule under his son, Richard Cromwell, the Protectorate Parliament was dissolved in 1659 and the Rump Parliament recalled, the start of a process that led ultimately to the restoration of the monarchy in 1660. The term Commonwealth is sometimes used for the whole of 1649 to 1660 – a period referred to by monarchists as the Interregnum – although for other historians, the use of the term is limited to the years prior to Cromwell’s formal assumption of power in 1653.
1649–1653.
Rump Parliament.
The Rump was created by Pride's Purge of those members of the Long Parliament who did not support the political position of the Grandees in the New Model Army. Just before and after the execution of King Charles I on 30 January 1649, the Rump passed a number of acts of Parliament creating the legal basis for the republic. With the abolition of the monarchy, Privy Council and the House of Lords, it had unchecked executive, as well as legislative, power. The English Council of State, which replaced the Privy Council, took over many of the executive functions of the monarchy. It was selected by the Rump, and most of its members were MPs. Ultimately, however, the Rump depended on the support of the Army with which it had a very uneasy relationship. After the Execution of Charles I, the House of Commons abolished the monarchy, the House of Lords, and the established Church of England. It declared the people of England ""and of all the Dominions and Territoryes thereunto belonging" to be henceforth under the governance of a "Commonwealth and Free State"", effectively a republic.
Structure.
In Pride's Purge, all members of parliament (including most of the political Presbyterians) who would not accept the need to bring the King to trial had been removed. Thus the Rump never had more than two hundred members (less than half the number of the Commons in the original Long Parliament). They included: supporters of religious independents who did not want an established church and some of whom had sympathies with the Levellers; Presbyterians who were willing to countenance the trial and execution of the King; and later admissions, such as formerly excluded MPs who were prepared to denounce the Newport Treaty negotiations with the King.
Most Rumpers were gentry, though there was a higher proportion of lesser gentry and lawyers than in previous parliaments. Less than one-quarter of them were regicides. This left the Rump as basically a conservative body whose vested interests in the existing land ownership and legal systems made it unlikely to want to reform them.
Issues and achievements.
For the first two years of the Commonwealth, the Rump faced economic depression and the risk of invasion from Scotland and Ireland. By 1653 Cromwell and the Army had largely eliminated these threats.
There were many disagreements amongst factions of the Rump. Some wanted a republic, but others favoured retaining some type of monarchical government. Most of England's traditional ruling classes regarded the Rump as an illegal government made up of regicides and upstarts. However, they were also aware that the Rump might be all that stood in the way of an outright military dictatorship. High taxes, mainly to pay the Army, were resented by the gentry. Limited reforms were enough to antagonise the ruling class but not enough to satisfy the radicals.
Despite its unpopularity, the Rump was a link with the old constitution, and helped to settle England down and make it secure after the biggest upheaval in its history. By 1653, both France and Spain had recognised England's new government.
Reforms.
Though the Church of England was retained, episcopacy was suppressed and the Act of Uniformity 1558 was repealed in September 1650. Mainly on the insistence of the Army, many independent churches were tolerated, although everyone still had to pay tithes to the established church.
Some small improvements were made to law and court procedure; for example, all court proceedings were now conducted in English rather than in Law French or Latin. However, there were no widespread reforms of the common law. This would have upset the gentry, who regarded the common law as reinforcing their status and property rights.
The Rump passed many restrictive laws to regulate people's moral behaviour, such as closing down theatres and requiring strict observance of Sunday. This antagonised most of the gentry.
Dismissal.
Cromwell, aided by Thomas Harrison, forcibly dismissed the Rump on 20 April 1653, for reasons that are unclear. Theories are that he feared the Rump was trying to perpetuate itself as the government, or that the Rump was preparing for an election which could return an anti-Commonwealth majority. Many former members of the Rump continued to regard themselves as England's only legitimate constitutional authority. The Rump had not agreed to its own dissolution when it was dispersed by Cromwell, and legislation from the period immediately before the Civil War—the Act against dissolving the Long Parliament without its own consent (11 May 1641) -- gave them the legal basis for this view.
Barebone's Parliament, July–December 1653.
The dissolution of the Rump was followed by a short period in which Cromwell and the Army ruled alone. Nobody had the constitutional authority to call an election, but Cromwell did not want to impose a military dictatorship. Instead, he ruled through a 'nominated assembly' which he believed would be easy for the Army to control, since Army officers did the nominating.
Barebone's Parliament was opposed by former Rumpers and ridiculed by many gentry as being an assembly of 'inferior' people. However, over 110 of its 140 members were lesser gentry or of higher social status. (An exception was Praise-God Barebone, a Baptist merchant after whom the Assembly got its derogatory nickname.) Many were well educated.
The assembly reflected the range of views of the officers who nominated it. The Radicals (approximately forty) included a hard core of Fifth Monarchists who wanted to be rid of Common Law and any state control of religion. The Moderates (approximately 60) wanted some improvements within the existing system and might move to either the radical or conservative side depending on the issue. The Conservatives (approximately 40) wanted to keep the status quo (since Common Law protected the interests of the gentry, and tithes and advowsons were valuable property).
Cromwell saw Barebone's Parliament as a temporary legislative body which he hoped would produce reforms and develop a constitution for the Commonwealth. However, members were divided over key issues, only 25 had previous parliamentary experience, and although many had some legal training, there were no qualified lawyers.
Cromwell seems to have expected this group of 'amateurs' to produce reform without management or direction. When the radicals mustered enough support to defeat a bill which would have preserved the status quo in religion, the conservatives, together with many moderates, surrendered their authority back to Cromwell who sent soldiers to clear the rest of the Assembly. Barebone's Parliament was over.
The Protectorate, 1653–1659.
Commonwealth government by a Council of State and Parliament, was divided in two by The Protectorate when the executive was vested in a Lord Protector, who governed under a written constitution that mandated that the Lord Protector summon triennial parliaments that should sit for several months each year.
In 1653, Oliver Cromwell became Lord Protector under England's first written constitution Instrument of Government, and then under the second and last written constitution, known as the Humble Petition and Advice of 1657.
On 12 April 1654, under the terms of the Tender of Union, the "Ordinance for uniting Scotland into one Commonwealth with England" was issued by the Lord Protector and proclaimed in Scotland by the military governor of Scotland, General George Monck, 1st Duke of Albemarle. The ordinance declared that "the people of Scotland should be united with the people of England into one Commonwealth and under one Government" and decreed that a new "Arms of the Commonwealth", incorporating the Saltire, should be placed on "all the public seals, seals of office, and seals of bodies civil or corporate, in Scotland" as "a badge of this Union".
On the death of Oliver Cromwell in 1658, his son, Richard Cromwell, inherited the title Lord Protector, but internal divisions among the republican party lead to his resignation, the end of the Protectorate and a second period of Commonwealth government by a Council of State and Parliament.
1659–1660.
The Protectorate might have continued if Cromwell's son Richard, who was made Lord Protector on his father's death, had been capable of carrying on his father's policies. Richard Cromwell's main weakness was that he did not have the confidence of the New Model Army.
After seven months the Grandees in the New Model Army removed him and, on 6 May 1659, they reinstalled the Rump Parliament. Charles Fleetwood was appointed a member of the Committee of Safety and of the Council of State, and one of the seven commissioners for the army. On 9 June he was nominated lord-general (commander-in-chief) of the army. However, his power was undermined in parliament, which chose to disregard the army's authority in a similar fashion to the pre–Civil War parliament. The Commons on 12 October 1659, cashiered General John Lambert and other officers, and installed Fleetwood as chief of a military council under the authority of the Speaker. The next day Lambert ordered that the doors of the House be shut and the members kept out. On 26 October a "Committee of Safety" was appointed, of which Fleetwood and Lambert were members. Lambert was appointed major-general of all the forces in England and Scotland, Fleetwood being general. Lambert was now sent, by the Committee of Safety, with a large force to meet George Monck, who was in command of the English forces in Scotland, and either negotiate with him or force him to come to terms.
It was into this atmosphere that General George Monck marched south with his army from Scotland. Lambert's army began to desert him, and he returned to London almost alone. On 21 February 1660, Monck reinstated the Presbyterian members of the Long Parliament 'secluded' by Pride, so that they could prepare legislation for a new parliament. Fleetwood was deprived of his command and ordered to appear before parliament to answer for his conduct. On 3 March Lambert was sent to the Tower, from which he escaped a month later. Lambert tried to rekindle the civil war in favour of the Commonwealth by issuing a proclamation calling on all supporters of the "Good Old Cause" to rally on the battlefield of Edgehill. But he was recaptured by Colonel Richard Ingoldsby, a regicide who hoped to win a pardon by handing Lambert over to the new regime. The Long Parliament dissolved itself on 16 March.
On 4 April 1660, in response to a secret message sent by Monck, Charles II issued the Declaration of Breda, which made known the conditions of his acceptance of the crown of England. Monck organised the Convention Parliament, which met for the first time on 25 April. On 8 May it proclaimed that King Charles II had been the lawful monarch since the execution of Charles I in January 1649. Charles returned from exile on 23 May. He entered London on 29 May, his birthday. To celebrate "his Majesty's Return to his Parliament" May 29 was made a public holiday, popularly known as Oak Apple Day. He was crowned at Westminster Abbey on 23 April 1661.

</doc>
<doc id="7131" url="http://en.wikipedia.org/wiki?curid=7131" title="Charles Evers">
Charles Evers

James Charles Evers (born September 11, 1922), is the older brother of slain civil rights activist Medgar Evers. In 1969, he became the first African American since the Reconstruction era to have been elected as mayor in a Mississippi city, Fayette in Jefferson County. Thereafter, he ran for governor in 1971 and the United States Senate in 1978, both times as an Independent candidate.
Early life and education.
Born in Decatur in Newton County in east central Mississippi, Evers was reared by devoutly Christian parents. He had a younger brother Medgar, with whom he was close. They attended segregated schools, which were typically underfunded in Mississippi.
During World War II, Charles and Medgar Evers both served in the United States Army. Charles fell in love with a Filipino woman while stationed overseas. He could not marry her and bring her home to his native Mississippi because of her "white" skin color. Mississippi had enshrined Jim Crow rules in its constitution of 1890, which prohibited interracial marriages. It also had effectively disfranchised blacks and poor whites by requiring payment of a poll tax and passing a literacy test to register to vote. This status lasted until after passage of the federal Voting Rights Act of 1965, which authorized federal oversight and enforcement of constitutional rights.
Before and after the war, Evers participated in bootlegging operations, prostitution, and numbers in Mississippi and Chicago. He revealed this part of his past in 1971 prior to his campaign for governor. He said he wasn't proud of it, but was proud that he had changed his life, leaving crime behind.
Career.
In Mississippi about 1951, brothers Charles and Medgar Evers grew interested in African freedom movements. They were interested in Jomo Kenyatta and the rise of the Kikuyu tribal resistance to colonialism in Kenya, known as the "Mau-Mau" Rebellion as it moved to open violence. Along with his brother, Charles became active in the Regional Council of Negro Leadership (RCNL), a civil rights organization that also promoted self-help and business ownership. He drew inspiration from Dr. T. R. M. Howard, the president of the RCNL, who was one of the wealthiest blacks in the state. Between 1952 and 1955, Evers often spoke at the RCNL's annual conferences in Mound Bayou on such issues as voting rights.
Around 1956, Evers' entrepreneurial gifts and his civil rights activism landed him in trouble in Philadelphia, Mississippi. He left town and moved to Chicago, Illinois. There, he fell into a life of hustling, running numbers for organized crime, and managing prostitutes. 
In 1963, Byron De La Beckwith, member of a KKK chapter, shot his brother Medgar Evers in Mississippi as he arrived home from work. Evers died in an ambulance on the way to the hospital. Charles Evers was shocked and deeply upset by his brother's death. Over the opposition of more establishment figures in the National Association for the Advancement of Colored People (NAACP) such as Roy Wilkins, Charles took over Medgar's post as head of the NAACP in Mississippi. 
In 1969, following passage of the federal Voting Rights Act of 1965, Charles Evers was elected mayor of Fayette, Mississippi, the first African-American mayor in his state since Reconstruction. Blacks had been closed out of politics since the late 19th century because of extended disfranchisement through violence and intimidation, capped by the 1890 state constitution. Fayette had a majority of black residents. It had no industry, so had attracted few residents who had grown up outside the area. Its white community was known to be hostile toward blacks. 
Evers' election as mayor had enormous symbolic significance statewide and national resonance. The NAACP named Evers the 1969 Man of the Year. John Updike mentioned Evers in his popular novel "Rabbit Redux". Evers popularized the slogan, "Hands that picked cotton can now pick the mayor."
Evers served many terms as mayor of Fayette. Admired by some, he alienated others with his inflexible stands on various issues. Evers did not like to share or delegate power. Kenneth Middleton, a political rival who finally defeated Evers in a mayoral election, used the slogan: "We've seen what Fayette can do for one man. Now let's see what one man can do for Fayette."
In 1971 Evers ran but was defeated in the gubernatorial general election by Democrat William "Bill" Waller, 601,222 (77 percent) to 172,762 (22.1 percent); Waller had been the original prosecutor of Byron De La Beckwith. 
In 1978, Evers ran as an Independent for the US Senate seat vacated by James O. Eastland. He finished in third place behind his opponents, Democrat Maurice Dantin and Republican Thad Cochran. He received 24 percent of the vote—more than likely siphoning off African-American votes that would have otherwise gone to Dantin. Cochran won the election with a plurality of 45 percent of the vote and still holds the seat today. That was a period when white conservative voters were making their shift to supporting Republicans rather than Democrats.
In 1983, Evers ran as an Independent for governor of Mississippi but lost to the Democrat Bill Allain. Republican Leon Bramlett of Clarksdale, also known as a college All-American football player, finished second with 39 percent of the vote.
Evers later attracted controversy for his support of judicial nominee Charles W. Pickering, a fellow Republican, who was nominated by President George H. W. Bush for a seat on the US Court of Appeals. Evers criticized the NAACP and other organizations for opposing Pickering despite what he claimed was a record of supporting the civil rights movement in Mississippi.
Evers has befriended a range of people from sharecroppers to presidents. He was an informal adviser to politicians as diverse as Lyndon B. Johnson, George C. Wallace, Ronald W. Reagan and Robert F. Kennedy. On the other hand, Evers has severely criticized black leaders who, he believes, are charlatans or have not "paid the price". Charles Evers has been highly critical of such black community leaders as Roy Wilkins, Stokely Carmichael, H. Rap Brown and Louis Farrakhan.
Books.
He has written two autobiographies or memoirs: "Evers" (1971), written with Grace Halsell and self-published; and "Have No Fear" (2009).

</doc>
<doc id="7143" url="http://en.wikipedia.org/wiki?curid=7143" title="Code division multiple access">
Code division multiple access

Code division multiple access (CDMA) is a channel access method used by various radio communication technologies.
CDMA is an example of multiple access, which is where several transmitters can send information simultaneously over a single communication channel. This allows several users to share a band of frequencies (see bandwidth). To permit this to be achieved without undue interference between the users, CDMA employs spread-spectrum technology and a special coding scheme (where each transmitter is assigned a code).
CDMA is used as the access method in many mobile phone standards such as cdmaOne, CDMA2000 (the 3G evolution of cdmaOne), and WCDMA (the 3G standard used by GSM carriers), which are often referred to as simply "CDMA".
History.
The technology of code division multiple access channels has long been known. In the USSR, the first work devoted to this subject was published in 1935 by professor . It was shown that through the use of linear methods, there are three types of signal separation: frequency, time and compensatory. The technology of CDMA was used in 1957, when the young military radio engineer Leonid Kupriyanovich in Moscow, made an experimental model of a wearable automatic mobile phone, called LK-1 by him, with a base station. LK-1 has a weight of 3 kg, 20–30 km operating distance, and 20–30 hours of battery life. The base station, as described by the author, could serve several customers. In 1958, Kupriyanovich made the new experimental "pocket" model of mobile phone. This phone weighed 0.5 kg. To serve more customers, Kupriyanovich proposed the device, named by him as correllator. In 1958, the USSR also started the development of the "Altai" national civil mobile phone service for cars, based on the Soviet MRT-1327 standard. The phone system weighed . It was placed in the trunk of the vehicles of high-ranking officials and used a standard handset in the passenger compartment. The main developers of the Altai system were VNIIS (Voronezh Science Research Institute of Communications) and GSPI (State Specialized Project Institute). In 1963 this service started in Moscow and in 1970 Altai service was used in 30 USSR cities.
Steps in CDMA modulation.
CDMA is a spread-spectrum multiple access technique. A spread spectrum technique spreads the bandwidth of the data uniformly for the same transmitted power. A spreading code is a pseudo-random code that has a narrow ambiguity function, unlike other narrow pulse codes. In CDMA a locally generated code runs at a much higher rate than the data to be transmitted. Data for transmission is combined via bitwise XOR (exclusive OR) with the faster code. The figure shows how a spread spectrum signal is generated. The data signal with pulse duration of formula_1 (symbol period) is XOR’ed with the code signal with pulse duration of formula_2 (chip period). (Note: bandwidth is proportional to formula_3, where formula_4 = bit time.) Therefore, the bandwidth of the data signal is formula_5 and the bandwidth of the spread spectrum signal is formula_6. Since formula_2 is much smaller than formula_1, the bandwidth of the spread spectrum signal is much larger than the bandwidth of the original signal. The ratio formula_9 is called the spreading factor or processing gain and determines to a certain extent the upper limit of the total number of users supported simultaneously by a base station.
Each user in a CDMA system uses a different code to modulate their signal. Choosing the codes used to modulate the signal is very important in the performance of CDMA systems. The best performance will occur when there is good separation between the signal of a desired user and the signals of other users. The separation of the signals is made by correlating the received signal with the locally generated code of the desired user. If the signal matches the desired user's code then the correlation function will be high and the system can extract that signal. If the desired user's code has nothing in common with the signal the correlation should be as close to zero as possible (thus eliminating the signal); this is referred to as cross-correlation. If the code is correlated with the signal at any time offset other than zero, the correlation should be as close to zero as possible. This is referred to as auto-correlation and is used to reject multi-path interference.
An analogy to the problem of multiple access is a room (channel) in which people wish to talk to each other simultaneously. To avoid confusion, people could take turns speaking (time division), speak at different pitches (frequency division), or speak in different languages (code division). CDMA is analogous to the last example where people speaking the same language can understand each other, but other languages are perceived as noise and rejected. Similarly, in radio CDMA, each group of users is given a shared code. Many codes occupy the same channel, but only users associated with a particular code can communicate.
In general, CDMA belongs to two basic categories: synchronous (orthogonal codes) and asynchronous (pseudorandom codes).
Code division multiplexing (synchronous CDMA).
The digital modulation method is analogous to those used in simple radio transceivers. In the analogue case, a low frequency data signal is time multiplied with a high frequency pure sine wave carrier, and transmitted. This is effectively a frequency convolution (Weiner-Kinchin Theorem) of the two signals, resulting in a carrier with narrow sidebands. In the digital case, the sinusoidal carrier is replaced by Walsh functions. These are binary square waves that form a complete orthonormal set. The data signal is also binary and the time multiplication is achieved with a simple XOR function. This is usually a Gilbert cell mixer in the circuitry.
Synchronous CDMA exploits mathematical properties of orthogonality between vectors representing the data strings. For example, binary string "1011" is represented by the vector (1, 0, 1, 1). Vectors can be multiplied by taking their dot product, by summing the products of their respective components (for example, if u = (a, b) and v = (c, d), then their dot product u·v = ac + bd). If the dot product is zero, the two vectors are said to be "orthogonal" to each other. Some properties of the dot product aid understanding of how W-CDMA works. If vectors "a" and "b" are orthogonal, then formula_10 and:
Each user in synchronous CDMA uses a code orthogonal to the others' codes to modulate their signal. An example of four mutually orthogonal digital signals is shown in the figure. Orthogonal codes have a cross-correlation equal to zero; in other words, they do not interfere with each other. In the case of IS-95 64 bit Walsh codes are used to encode the signal to separate different users. Since each of the 64 Walsh codes are orthogonal to one another, the signals are channelized into 64 orthogonal signals. The following example demonstrates how each user's signal can be encoded and decoded.
Example.
Start with a set of vectors that are mutually orthogonal. (Although mutual orthogonality is the only condition, these vectors are usually constructed for ease of decoding, for example columns or rows from Walsh matrices.) An example of orthogonal functions is shown in the picture on the right. These vectors will be assigned to individual users and are called the "code", "chip code", or "chipping code". In the interest of brevity, the rest of this example uses codes, v, with only two bits.
Each user is associated with a different code, say v. A 1 bit is represented by transmitting a positive code, v, and a 0 bit is represented by a negative code, –v. For example, if v = (v0, v1) = (1, –1) and the data that the user wishes to transmit is (1, 0, 1, 1), then the transmitted symbols would be 
(v, –v, v, v) = (v0, v1, –v0, –v1, v0, v1, v0, v1) = (1, –1, –1, 1, 1, –1, 1, –1). For the purposes of this article, we call this constructed vector the "transmitted vector".
Each sender has a different, unique vector v chosen from that set, but the construction method of the transmitted vector is identical.
Now, due to physical properties of interference, if two signals at a point are in phase, they add to give twice the amplitude of each signal, but if they are out of phase, they subtract and give a signal that is the difference of the amplitudes. Digitally, this behaviour can be modelled by the addition of the transmission vectors, component by component.
If sender0 has code (1, –1) and data (1, 0, 1, 1), and sender1 has code (1, 1) and data (0, 0, 1, 1), and both senders transmit simultaneously, then this table describes the coding steps:
Because signal0 and signal1 are transmitted at the same time into the air, they add to produce the raw signal:
This raw signal is called an interference pattern. The receiver then extracts an intelligible signal for any known sender by combining the sender's code with the interference pattern, the receiver combines it with the codes of the senders. The following table explains how this works and shows that the signals do not interfere with one another:
Further, after decoding, all values greater than 0 are interpreted as 1 while all values less than zero are interpreted as 0. For example, after decoding, data0 is (2, –2, 2, 2), but the receiver interprets this as (1, 0, 1, 1). Values of exactly 0 means that the sender did not transmit any data, as in the following example:
Assume signal0 = (1, –1, –1, 1, 1, –1, 1, –1) is transmitted alone. The following table shows the decode at the receiver:
When the receiver attempts to decode the signal using sender1's code, the data is all zeros, therefore the cross correlation is equal to zero and it is clear that sender1 did not transmit any data.
Asynchronous CDMA.
When mobile-to-base links cannot be precisely coordinated, particularly due to the mobility of the handsets, a different approach is required. Since it is not mathematically possible to create signature sequences that are both orthogonal for arbitrarily random starting points and which make full use of the code space, unique "pseudo-random" or "pseudo-noise" (PN) sequences are used in "asynchronous" CDMA systems. A PN code is a binary sequence that appears random but can be reproduced in a deterministic manner by intended receivers. These PN codes are used to encode and decode a user's signal in Asynchronous CDMA in the same manner as the orthogonal codes in synchronous CDMA (shown in the example above). These PN sequences are statistically uncorrelated, and the sum of a large number of PN sequences results in "multiple access interference" (MAI) that is approximated by a Gaussian noise process (following the central limit theorem in statistics). Gold codes are an example of a PN suitable for this purpose, as there is low correlation between the codes. If all of the users are received with the same power level, then the variance (e.g., the noise power) of the MAI increases in direct proportion to the number of users. In other words, unlike synchronous CDMA, the signals of other users will appear as noise to the signal of interest and interfere slightly with the desired signal in proportion to number of users.
All forms of CDMA use spread spectrum process gain to allow receivers to partially discriminate against unwanted signals. Signals encoded with the specified PN sequence (code) are received, while signals with different codes (or the same code but a different timing offset) appear as wideband noise reduced by the process gain.
Since each user generates MAI, controlling the signal strength is an important issue with CDMA transmitters. A CDM (synchronous CDMA), TDMA, or FDMA receiver can in theory completely reject arbitrarily strong signals using different codes, time slots or frequency channels due to the orthogonality of these systems. This is not true for Asynchronous CDMA; rejection of unwanted signals is only partial. If any or all of the unwanted signals are much stronger than the desired signal, they will overwhelm it. This leads to a general requirement in any asynchronous CDMA system to approximately match the various signal power levels as seen at the receiver. In CDMA cellular, the base station uses a fast closed-loop power control scheme to tightly control each mobile's transmit power.
Advantages of asynchronous CDMA over other techniques.
Efficient practical utilization of the fixed frequency spectrum.
In theory CDMA, TDMA and FDMA have exactly the same spectral efficiency but practically, each has its own challenges – power control in the case of CDMA, timing in the case of TDMA, and frequency generation/filtering in the case of FDMA.
TDMA systems must carefully synchronize the transmission times of all the users to ensure that they are received in the correct time slot and do not cause interference. Since this cannot be perfectly controlled in a mobile environment, each time slot must have a guard-time, which reduces the probability that users will interfere, but decreases the spectral efficiency. Similarly, FDMA systems must use a guard-band between adjacent channels, due to the unpredictable doppler shift of the signal spectrum because of user mobility. The guard-bands will reduce the probability that adjacent channels will interfere, but decrease the utilization of the spectrum.
Flexible allocation of resources.
Asynchronous CDMA offers a key advantage in the flexible allocation of resources i.e. allocation of a PN codes to active users. In the case of CDM (synchronous CDMA), TDMA, and FDMA the number of simultaneous orthogonal codes, time slots and frequency slots respectively are fixed hence the capacity in terms of number of simultaneous users is limited. There are a fixed number of orthogonal codes, time slots or frequency bands that can be allocated for CDM, TDMA, and FDMA systems, which remain underutilized due to the bursty nature of telephony and packetized data transmissions. There is no strict limit to the number of users that can be supported in an asynchronous CDMA system, only a practical limit governed by the desired bit error probability, since the SIR (Signal to Interference Ratio) varies inversely with the number of users. In a bursty traffic environment like mobile telephony, the advantage afforded by asynchronous CDMA is that the performance (bit error rate) is allowed to fluctuate randomly, with an average value determined by the number of users times the percentage of utilization. Suppose there are 2N users that only talk half of the time, then 2N users can be accommodated with the same "average" bit error probability as N users that talk all of the time. The key difference here is that the bit error probability for N users talking all of the time is constant, whereas it is a "random" quantity (with the same mean) for 2N users talking half of the time.
In other words, asynchronous CDMA is ideally suited to a mobile network where large numbers of transmitters each generate a relatively small amount of traffic at irregular intervals. CDM (synchronous CDMA), TDMA, and FDMA systems cannot recover the underutilized resources inherent to bursty traffic due to the fixed number of orthogonal codes, time slots or frequency channels that can be assigned to individual transmitters. For instance, if there are N time slots in a TDMA system and 2N users that talk half of the time, then half of the time there will be more than N users needing to use more than N time slots. Furthermore, it would require significant overhead to continually allocate and deallocate the orthogonal code, time slot or frequency channel resources. By comparison, asynchronous CDMA transmitters simply send when they have something to say, and go off the air when they don't, keeping the same PN signature sequence as long as they are connected to the system.
Spread-spectrum characteristics of CDMA.
Most modulation schemes try to minimize the bandwidth of this signal since bandwidth is a limited resource. However, spread spectrum techniques use a transmission bandwidth that is several orders of magnitude greater than the minimum required signal bandwidth. One of the initial reasons for doing this was military applications including guidance and communication systems. These systems were designed using spread spectrum because of its security and resistance to jamming. Asynchronous CDMA has some level of privacy built in because the signal is spread using a pseudo-random code; this code makes the spread spectrum signals appear random or have noise-like properties. A receiver cannot demodulate this transmission without knowledge of the pseudo-random sequence used to encode the data. CDMA is also resistant to jamming. A jamming signal only has a finite amount of power available to jam the signal. The jammer can either spread its energy over the entire bandwidth of the signal or jam only part of the entire signal.
CDMA can also effectively reject narrow band interference. Since narrow band interference affects only a small portion of the spread spectrum signal, it can easily be removed through notch filtering without much loss of information. Convolution encoding and interleaving can be used to assist in recovering this lost data. CDMA signals are also resistant to multipath fading. Since the spread spectrum signal occupies a large bandwidth only a small portion of this will undergo fading due to multipath at any given time. Like the narrow band interference this will result in only a small loss of data and can be overcome.
Another reason CDMA is resistant to multipath interference is because the delayed versions of the transmitted pseudo-random codes will have poor correlation with the original pseudo-random code, and will thus appear as another user, which is ignored at the receiver. In other words, as long as the multipath channel induces at least one chip of delay, the multipath signals will arrive at the receiver such that they are shifted in time by at least one chip from the intended signal. The correlation properties of the pseudo-random codes are such that this slight delay causes the multipath to appear uncorrelated with the intended signal, and it is thus ignored.
Some CDMA devices use a rake receiver, which exploits multipath delay components to improve the performance of the system.
A rake receiver combines the information from several correlators, each one tuned to a different path delay, producing a stronger version of the signal than a simple receiver with a single correlation tuned to the path delay of the strongest signal.
Frequency reuse is the ability to reuse the same radio channel frequency at other cell sites within a cellular system. In the FDMA and TDMA systems frequency planning is an important consideration. The frequencies used in different cells must be planned carefully to ensure signals from different cells do not interfere with each other. In a CDMA system, the same frequency can be used in every cell, because channelization is done using the pseudo-random codes. Reusing the same frequency in every cell eliminates the need for frequency planning in a CDMA system; however, planning of the different pseudo-random sequences must be done to ensure that the received signal from one cell does not correlate with the signal from a nearby cell.
Since adjacent cells use the same frequencies, CDMA systems have the ability to perform soft hand offs. Soft hand offs allow the mobile telephone to communicate simultaneously with two or more cells. The best signal quality is selected until the hand off is complete. This is different from hard hand offs utilized in other cellular systems. In a hard hand off situation, as the mobile telephone approaches a hand off, signal strength may vary abruptly. In contrast, CDMA systems use the soft hand off, which is undetectable and provides a more reliable and higher quality signal.
Collaborative CDMA.
In a recent study, a novel collaborative multi-user transmission and detection scheme called Collaborative CDMA has been investigated for the uplink that exploits the differences between users’ fading channel signatures to increase the user capacity well beyond the spreading length in multiple access interference (MAI) limited environment. The authors show that it is possible to achieve this increase at a low complexity and high bit error rate performance in ﬂat fading channels, which is a major research challenge for overloaded CDMA systems. In this approach, instead of using one sequence per user as in conventional CDMA, the authors group a small number of users to share the same spreading sequence and enable group spreading and despreading operations. The new collaborative multi-user receiver consists of two stages: group multi-user detection (MUD) stage to suppress the MAI between the groups and a low complexity maximum-likelihood detection stage to recover jointly the co-spread users’ data using minimum Euclidean distance measure and users’ channel gain coefﬁcients. In CDM signal security is high.

</doc>
<doc id="7144" url="http://en.wikipedia.org/wiki?curid=7144" title="Content-control software">
Content-control software

Content-control software, content filtering software, secure web gateways, censorware, Content Security and Control, web filtering software, content-censoring software, and content-blocking software are terms describing software designed to restrict or control the content a reader is authorised to access, especially when utilised to restrict material delivered over the Internet via the Web, e-mail, or other means. Content-control software determines what content will be available or perhaps more often what content will be blocked.
Such restrictions can be applied at various levels: a government can attempt to apply them nationwide (see Internet censorship), or they can, for example, be applied by an ISP to its clients, by an employer to its personnel, by a school to its students, by a library to its visitors, by a parent to a child's computer, or by an individual user to his or her own computer.
The motive is often to prevent access to content which the computer's owner(s) or other authorities may consider objectionable. When imposed without the consent of the user, content control can be characterised as a form of internet censorship. Some content-control software includes time control functions that empowers parents to set the amount of time that child may spend accessing the Internet or playing games or other computer activities.
In some countries, such software is ubiquitous. In Cuba, if a computer user at a government controlled Internet cafe types certain words, the word processor or browser is automatically closed, and a "state security" warning is given.
Terminology.
This article uses the term "content control", a term also used on occasion by CNN, "Playboy" magazine the "San Francisco Chronicle" and the "New York Times". However, two more controversial alternative terms, "censorware" and "web filtering", are often used. "Nannyware" has also been used in both product marketing and by the media. Industry research company Gartner uses "secure web gateway" (SWG) to describe the market segment.
Companies that make products that selectively block Web sites do not refer to these products as censorware, and prefer terms such as "Internet filter" or "URL Filter"; in the specialized case of software specifically designed to allow parents to monitor and restrict the access of their children, "parental control software" is also used. Some products log all sites that a user accesses and rates them based on content type for reporting to an "accountability partner" of the person's choosing, and the term accountability software is used. Internet filters, parental control software, and/or accountability software may also be combined into one product.
Those critical of such software, however, use the term "censorware" freely: consider the Censorware Project, for example. The use of the term "censorware" in editorials criticizing makers of such software is widespread and covers many different varieties and applications: Xeni Jardin used the term in a 9 March 2006 editorial in the New York Times when discussing the use of American-made filtering software to suppress content in China; in the same month a high school student used the term to discuss the deployment of such software in his school district.
In general, outside of editorial pages as described above, traditional newspapers do not use the term censorware in their reporting, preferring instead to use less overtly controversial terms such as "content filter", "content control", or "web filtering"; the New York Times and the "Wall Street Journal" both appear to follow this practice. On the other hand, Web-based newspapers such as CNET use the term in both editorial and journalistic contexts, for example "Windows Live to Get Censorware."
Types of filtering.
Filters can be implemented in many different ways: by software on a personal computer, via network infrastructure such as proxy servers, DNS servers, or firewalls that provide Internet access.
Browser based filters: Browser based content filtering solution is the most lightweight solution to do the content filtering, and is implemented via a third party browser extension.
E-mail filters: E-mail filters act on information contained in the mail body, in the mail headers such as sender and subject, and e-mail attachments to classify, accept, or reject messages. Bayesian filters, a type of statistical filter, are commonly used. Both client and server based filters are available.
Client-side filters: This type of filter is installed as software on each computer where filtering is required. This filter can typically be managed, disabled or uninstalled by anyone who has administrator-level privileges on the system.
Content-limited (or filtered) ISPs: Content-limited (or filtered) ISPs are Internet service providers that offer access to only a set portion of Internet content on an opt-in or a mandatory basis. Anyone who subscribes to this type of service is subject to restrictions. The type of filters can be used to implement government, regulatory or parental control over subscribers.
Network-based filtering: This type of filter is implemented at the transport layer as a transparent proxy, or at the application layer as a web proxy. Filtering software may include data loss prevention functionality to filter outbound as well as inbound information. All users are subject to the access policy defined by the institution. The filtering can be customized, so a school district's high school library can have a different filtering profile than the district's junior high school library.
Search-engine filters: Many search engines, such as Google and Alta Vista offer users the option of turning on a safety filter. When this safety filter is activated, it filters out the inappropriate links from all of the search results. If users know the actual URL of a website that features explicit or adult content, they have the ability to access that content without using a search engine. Engines like Lycos, Yahoo, and Bing offer kid-oriented versions of their engines that permit only children friendly websites.
Reasons for filtering.
Internet service providers (ISPs) that block material containing pornography, or controversial religious, political, or news-related content en route are often utilised by parents who do not permit their children to access content not conforming to their personal beliefs. Content filtering software can, however, also be used to block malware and other content that is or contains hostile, intrusive, or annoying material including adware, spam, computer viruses, worms, trojan horses, and spyware.
Most content control software is marketed to organizations or parents. It is, however, also marketed on occasion to facilitate self-censorship, for example by people struggling with addictions to online pornography, gambling, chat rooms, etc. Self-censorship software may also be utilised by some of the more extreme conservatives in order to avoid viewing content they consider 'immoral' or 'inappropriate'. A number of accountability software products are marketed as "self-censorship" or "accountability software". These are often promoted by religious media and at religious gatherings.
Criticism.
Filtering errors.
Overblocking: Utilising a filter that is overly zealous at filtering content, or mislabels content not intended to be censored can result in over blocking, or over-censoring. Over blocking can filter out material that should be acceptable under the filtering policy in effect, for example health related information may unintentionally be filtered along with porn-related material because of the Scunthorpe problem. Filter administrators may prefer to err on the side of caution by accepting over blocking to prevent any risk of access to sites that they determine to be undesirable. Content-control software was mentioned as blocking access to Beaver College before its name change to Arcadia University. Another example was the filtering of Horniman Museum. As well, over-blocking may encourage users to bypass the filter entirely.
Underblocking: Whenever new information is uploaded to the Internet, filters can under block, or under-censor, content if the parties responsible for maintaining the filters do not update them quickly and accurately, and a blacklisting rather than a whitelisting filtering policy is in place.
Morality and opinion.
Many would disapprove of government filtering viewpoints on moral or political issues, agreeing that this could become support for propaganda. Many would also find it unacceptable that an ISP, whether by law or by the ISP's own choice, should deploy such software without allowing the users to disable the filtering for their own connections. In the United States, the First Amendment has been cited in calls to criminalise forced internet censorship. (See section below)
Without adequate governmental supervision, content-filtering software could enable private companies to censor as they please. (See Religious or political censorship, below). Government utilisation or encouragement of content-control software is a component of Internet Censorship (not to be confused with Internet Surveillance, in which content is monitored and not necessarilly restricted). The governments of countries such as the People's Republic of China, and Cuba are current examples of countries in which this ethically controversial activity is alleged to have taken place.
Legal actions.
In 1998, a United States federal district court in Virginia ruled that the imposition of mandatory filtering in a public library violates the First Amendment of the U.S. Bill of Rights.
In 1996 the US Congress passed the Communications Decency Act, banning indecency on the Internet. Civil liberties groups challenged the law under the First Amendment and in 1997 the Supreme Court ruled in their favor. Part of the civil liberties argument, especially from groups like the Electronic Frontier Foundation, was that parents who wanted to block sites could use their own content-filtering software, making government involvement unnecessary.
In the late 1990s, groups such as the Censorware Project began reverse-engineering the content-control software and decrypting the blacklists to determine what kind of sites the software blocked. This led to legal action alleging violation of the "Cyber Patrol" license agreement. They discovered that such tools routinely blocked unobjectionable sites while also failing to block intended targets. (See Over-zealous filtering, below).
Some content-control software companies responded by claiming that their filtering criteria were backed by intensive manual checking. The companies' opponents argued, on the other hand, that performing the necessary checking would require resources greater than the companies possessed and that therefore their claims were not valid.
The Motion Picture Association successfully obtained a UK ruling enforcing ISPs to use content-control software to prevent copyright infringement by their subscribers.
Religious, anti-religious, and political censorship.
Many types of content-control software have been shown to block sites based on the religious and political leanings of the company owners. Examples include blocking several religious sites (including the Web site of the Vatican), many political sites, and sites about gay/lesbians. "X-Stop" was shown to block sites such as the Quaker web site, the National Journal of Sexual Orientation Law, the Heritage Foundation, and parts of The Ethical Spectacle. CYBERsitter blocks out sites like National Organization for Women. Nancy Willard, an academic researcher and attorney, pointed out that many U.S. public schools and libraries use the same filtering software that many Christian organizations use. Cyber Patrol, a product developed by The Anti-Defamation League and Mattel's The Learning Company, has been found to block not only political sites it deems to be engaging in 'hate speech' but also human rights web sites, such as Amnesty International's web page about Israel and gay-rights web sites, such as glaad.org.
Content labeling.
Content labeling may be considered another form of content-control software. In 1994, the Internet Content Rating Association (ICRA) — now part of the Family Online Safety Institute — developed a content rating system for online content providers. Using an online questionnaire a webmaster describes the nature of his web content. A small file is generated that contains a condensed, computer readable digest of this description that can then be used by content filtering software to block or allow that site.
ICRA labels come in a variety of formats. These include the World Wide Web Consortium's Resource Description Framework (RDF) as well as Platform for Internet Content Selection (PICS) labels used by Microsoft's Internet Explorer Content Advisor.
ICRA labels are an example of self-labeling. Similarly, in 2006 the Association of Sites Advocating Child Protection (ASACP) initiated the self-labeling initiative. ASACP members were concerned that various forms of legislation being proposed in the United States were going to have the effect of forcing adult companies to label their content. The RTA label, unlike ICRA labels, does not require a webmaster to fill out a questionnaire or sign up to use. Like ICRA the RTA label is free. Both labels are recognized by a wide variety of content-control software.
The Voluntary Content Rating (VCR) system was devised by Solid Oak Software for their CYBERsitter filtering software, as an alternative to the PICS system, which some critics deemed too complex. It employs HTML metadata tags embedded within web page documents to specify the type of content contained in the document. Only two levels are specified, "mature" and "adult", making the specification extremely simple.
Use in public libraries.
United States.
The use of Internet filters or content-control software varies widely in public libraries in the United States, since Internet use policies are established by the local library board. Many libraries adopted Internet filters after Congress conditioned the receipt of universal service discounts on the use of Internet filters through the Children's Internet Protection Act (CIPA). Other libraries do not install content control software, believing that acceptable use policies and educational efforts address the issue of children accessing age-inappropriate content while preserving adult users' right to freely access information. Some libraries use Internet filters on computers used by children only. Some libraries that employ content-control software allow the software to be deactivated on a case-by-case basis on application to a librarian; libraries that are subject to CIPA are required to have a policy that allows adults to request that the filter be disabled without having to explain the reason for their request.
Many legal scholars believe that a number of legal cases, in particular "Reno v. American Civil Liberties Union", established that the use of content-control software in libraries is a violation of the First Amendment. The Children's Internet Protection Act [CIPA] and the June 2003 case "United States v. American Library Association" found CIPA constitutional as a condition placed on the receipt of federal funding, stating that First Amendment concerns were dispelled by the law's provision that allowed adult library users to have the filtering software disabled, without having to explain the reasons for their request. The plurality decision left open a future "as-applied" Constitutional challenge, however. In November 2006, a lawsuit was filed against the North Central Regional Library District (NCRL) in Washington State for its policy of refusing to disable restrictions upon requests of adult patrons, but CIPA was not challenged in that matter. In May 2010, the Washington State Supreme Court provided an opinion after it was asked to certify a question referred by the United States District Court for the Eastern District of Washington: “Whether a public library, consistent with Article I, § 5 of the Washington Constitution, may filter Internet access for all patrons without disabling Web sites containing constitutionally-protected speech upon the request of an adult library patron.” The Washington State Supreme Court ruled that NCRL’s internet filtering policy did not violate Article I, Section 5 of the Washington State Constitution. The Court said: “It appears to us that NCRL’s filtering policy is reasonable and accords with its mission and these policies and is viewpoint neutral. It appears that no article I, section 5 content-based violation exists in this case. NCRL’s essential mission is to promote reading and lifelong learning. As NCRL maintains, it is reasonable to impose restrictions on Internet access in order to maintain an environment that is conducive to study and contemplative thought.” The case now returns to federal court.
In March 2007, Virginia passed a law similar to CIPA that requires public libraries receiving state funds to use content-control software. Like CIPA, the law requires libraries to disable filters for an adult library user when requested to do so by the user.
Australia.
The Australian Internet Safety Advisory Body has information about "practical advice on Internet safety, parental control and filters for the protection of children, students and families" that also includes public libraries.
NetAlert, the software made available free of charge by the Australian government, was allegedly cracked by a 16-year-old student, Tom Wood, less than a week after its release in August 2007. Wood supposedly bypassed the $84 million filter in about half an hour to highlight problems with the government's approach to Internet content filtering.
The Australian Government has introduced legislation that requires ISP's to "restrict access to age restricted content (commercial MA15+ content and R18+ content) either hosted in Australia or provided from Australia" that was due to commence from 20 January 2008, known as Cleanfeed.
Cleanfeed is a proposed mandatory ISP level content filtration system. It was proposed by the Beazley led Australian Labor Party opposition in a 2006 press release, with the intention of protecting children who were vulnerable due to claimed parental computer illiteracy. It was announced on 31 December 2007 as a policy to be implemented by the Rudd ALP government, and initial tests in Tasmania have produced a 2008 report. Cleanfeed is funded in the current budget, and is moving towards an Expression of Interest for live testing with ISPs in 2008. Public opposition and criticism have emerged, led by the EFA and gaining irregular mainstream media attention, with a majority of Australians reportedly "strongly against" its implementation. Criticisms include its expense, inaccuracy (it will be impossible to ensure only illegal sites are blocked) and the fact that it will be compulsory, which can be seen as an intrusion on free speech rights. Another major criticism point has been that although the filter is claimed to stop certain materials, the underground rings dealing in such materials will not be affected. The filter might also provide a false sense of security for parents, who might supervise children less while using the Internet, achieving the exact opposite effect. Cleanfeed is a responsibility of Senator Conroy's portfolio.
Denmark.
In Denmark it is stated policy that it will "prevent inappropriate Internet sites from being accessed from children's libraries across Denmark." "'It is important that every library in the country has the opportunity to protect children against pornographic material when they are using library computers. It is a main priority for me as Culture Minister to make sure children can surf the net safely at libraries,' states Brian Mikkelsen in a press-release of the Danish Ministry of Culture."
Bypassing filters.
Content filtering in general can "be bypassed entirely by tech-savvy individuals". Blocking content on circumvention advice "[will not]...guarantee that users won't eventually be able to find a way around the filter."
Some software may be bypassed successfully by using alternative protocols such as FTP or telnet or HTTPS, conducting searches in a different language, using a proxy server or a circumventor such as Psiphon. Also cached web pages returned by Google or other searches could bypass some controls as well. Web syndication services may provide alternate paths for content. Some of the more poorly designed programs can be shut down by killing their processes: for example, in Microsoft Windows through the Windows Task Manager, or in Mac OS X using Force Quit or Activity Monitor. Numerous workarounds and counters to workarounds from content-control software creators exist.
Google services are often blocked by filters, but these may most often be bypassed by using "https://" in place of "http://" since content filtering software is not able to interpret content under secure connections (in this case SSL).
Many content filters have an option which allows authorized people to bypass the content filter. This is especially useful in environments where the computer is being supervised and the content filter is aggressively blocking Web sites that need to be accessed.
An encrypted VPN can be used as means of bypassing content control software, especially if the content control software is installed on an Internet gateway or firewall.
Sometimes, an antivirus software with web protection may stop the content-control filter.
Products and services.
Some ISPs offer parental control options. Some offer security software which includes parental controls. Mac OS X v10.4 offers parental controls for several applications (Mail, Finder, iChat, Safari & Dictionary). Microsoft's Windows Vista operating system also includes content-control software.
Content filtering technology exists in two major forms: application gateway or packet inspection. For HTTP access the application gateway is called a web-proxy or just a proxy. Such web-proxies can inspect both the initial request and the returned web page using arbitrarily complex rules and will not return any part of the page to the requester until a decision is made. In addition they can make substitutions in whole or for any part of the returned result. Packet inspection filters do not initially interfere with the connection to the server but inspect the data in the connection as it goes past, at some point the filter may decide that the connection is to be filtered and it will then disconnect it by injecting a TCP-Reset or similar faked packet. The two techniques can be used together with the packet filter monitoring a link until it sees an HTTP connection starting to an IP address that has content that needs filtering. The packet filter then redirects the connection to the web-proxy which can perform detailed filtering on the website without having to pass through all unfiltered connections. This combination is quite popular because it can significantly reduce the cost of the system.
Gateway-based content control software may be more difficult to bypass than desktop software as the user does not have physical access to the filtering device. However, many of the techniques in the Bypassing filters section still work.

</doc>
<doc id="7145" url="http://en.wikipedia.org/wiki?curid=7145" title="Chambered cairn">
Chambered cairn

A chambered cairn is a burial monument, usually constructed during the Neolithic, consisting of a sizeable (usually stone) chamber around and over which a cairn of stones was constructed. Some chambered cairns are also passage-graves. They are found throughout Britain and Ireland, with the largest number in Scotland.
Typically, the chamber is larger than a cist, and will contain a larger number of interments, which are either excarnated bones or inhumations (cremations). Most were situated near a settlement, and served as that community's "graveyard".
Scotland.
Background.
During the early Neolithic (4000-3300 BC) architectural forms are highly regionalised with timber and earth monuments predominating in the east and stone-chambered cairns in the west. During the later Neolithic (3300-2500 BC) massive circular enclosures and the use of grooved ware and Unstan ware pottery emerge. Scotland has a particularly large number of chambered cairns; they are found in various different types described below. Along with the excavations of settlements such as Skara Brae, Links of Noltland, Barnhouse, Rinyo and Balfarg and the complex site at Ness of Brodgar these cairns provide important clues to the character of civilization in Scotland in the Neolithic. However the increasing use of cropmarks to identify Neolithic sites in lowland areas has tended to diminish the relative prominence of these cairns.
In the early phases bones of numerous bodies are often found together and it has been argued that this suggests that in death at least, the status of individuals was played down. During the late Neolithic henge sites were constructed and single burials began to become more commonplace; by the Bronze Age it is possible that even where chambered cairns were still being built they had become the burial places of prominent individuals rather than of communities as a whole.
Clyde-Carlingford court cairns.
The Clyde or Clyde-Carlingford type are principally found in northern and western Ireland and southwestern Scotland. They first were identified as a separate group in the Firth of Clyde region, hence the name. Over 100 have been identified in Scotland alone. Lacking a significant passage, they are a form of gallery grave. The burial chamber is normally located at one end of a rectangular or trapezoidal cairn, while a roofless, semi-circular forecourt at the entrance provided access from the outside (although the entrance itself was often blocked), and gives this type of chambered cairn its alternate name of court tomb or court cairn. These forecourts are typically fronted by large stones and it is thought the area in front of the cairn was used for public rituals of some kind. The chambers were created from large stones set on end, roofed with large flat stones and often sub-divided by slabs into small compartments. They are generally considered to be the earliest in Scotland.
Examples include Cairn Holy I and Cairn Holy II near Newton Stewart, a cairn at Port Charlotte, Islay, which dates to 3900-4000 BC, and Monamore, or Meallach's Grave, Arran, which may date from the early fifth millennium BC.
Orkney-Cromarty.
The Orkney-Cromarty group is by far the largest and most diverse. It has been subdivided into Yarrows, Camster and Cromarty subtypes but the differences are extremely subtle. The design is of dividing slabs at either side of a rectangular chamber, separating it into compartments or stalls. The number of these compartments ranges from 4 in the earliest examples to over 24 in an extreme example on Orkney. The actual shape of the cairn varies from simple circular designs to elaborate 'forecourts' protruding from each end, creating what look like small amphitheatres. It is likely that these are the result of cultural influences from mainland Europe, as they are similar to designs found in France and Spain.
Examples include Midhowe on Rousay and Unstan Chambered Cairn from the Orkney Mainland, both of which date from the mid 4th millennium BC and were probably in use over long periods of time. When the latter was excavated in 1884, grave goods were found that gave their name to Unstan ware pottery. Blackhammer cairn on Rousay is another example dating from the 3rd millennium BC.
The Grey Cairns of Camster in Caithness are examples of this type from mainland Scotland. The Tomb of the Eagles on South Ronaldsay is a stalled cairn that shows some similarities with the later Maeshowe type. It was in use for 800 years or more and numerous bird bones were found here, predominantly White-tailed Sea Eagle.
Maeshowe.
The Maeshowe group, named after the famous Orkney monument, is among the most elaborate. They appear relatively late and only in Orkney and it is not clear why the use of cairns continued in the north when their construction had largely ceased elsewhere in Scotland. They consist of a central chamber from which lead small compartments, into which burials would be placed. The central chambers are tall and steep-sided and have corbelled roofing faced with high quality stone.
In addition to Maeshowe itself, which was constructed c. 2700 BC, there are various other examples from the Orkney Mainland. These include Quanterness chambered cairn (3250 BC) in which the remains of 157 individuals were found when excavated in the 1970s, Cuween Hill near Finstown which was found to contain the bones of men, dogs and oxen and Wideford Hill cairn, which dates from 2000 BC.
Examples from elsewhere in Orkney are the Vinquoy cairn, found at an elevated location on the north end of the island of Eday and Quoyness on Sanday constructed about 2900 BC and which is surrounded by an arc of Bronze Age mounds. The central chamber of Holm of Papa Westray South cairn is over 20 metres long.
Bookan.
The Bookan type is named after a cairn found to the north-west of the Ring of Brodgar in Orkney, which is now a dilapidated oval mound, about 16 metres in diameter. Excavations in 1861 indicated a rectangular central chamber surrounded by five smaller chambers. Because of the structure's unusual design, it was originally presumed to be an early form. However, later interpretations and further excavation work in 2002 suggested that they have more in common with the later Maeshowe type rather than the stalled Orkney-Cromarty cairns.
Huntersquoy cairn on Eday is a Bookan type cairn with an upper and lower storey.
Shetland.
The Shetland or Zetland group are relatively small passage graves, that are round or heel-shaped in outline. The whole chamber is cross or trefoil-shaped and there are no smaller individual compartments. An example is to be found on the uninhabited island of Vementry on the north side of the West Mainland, where it appears that the cairn may have originally been circular and its distinctive heel shape added as a secondary development, a process repeated elsewhere in Shetland. This probably served to make the cairn more distinctive and the forecourt area more defined.
Hebridean.
Like the Shetland cairn the Hebridean group appear relatively late in the Neolithic. They are largely found in the Outer Hebrides, although a mixture of cairn types are found here. These passage graves are usually larger than the Shetland type and are round or have funnel-shaped forecourts, although a few are long cairns - perhaps originally circular but with later tails added. They often have a polygonal chamber and a short passage to one end of the cairn.
The Rubha an Dùnain peninsula on the island of Skye provides an example from the 2nd or 3rd millennium BC. Barpa Langass on North Uist is the best preserved chambered cairn in the Hebrides.
Bronze Age.
In addition to the increasing prominence of individual burials, during the Bronze Age regional differences in architecture in Scotland became more pronounced. The Clava cairns date from this period, with about 50 cairns of this type in the Inverness area. Corrimony chambered cairn near Drumnadrochit is an example dated to 2000 BC or older. The only surviving evidence of burial was a stain indicating the presence of a single body. The cairn is surrounded by a circle of 11 standing stones. The cairns at Balnuaran of Clava are of a similar date. The largest of three is the north-east cairn, which was partially reconstructed in the 19th century and the central cairn may have been used as a funeral pyre.
Glebe cairn in Kilmartin Glen in Argyll dates from 1700 BC and has two stone cists inside one of which a jet necklace was found during 19th century excavations. There are numerous prehistoric sites in the vicinity including Nether Largie North cairn, which was entirely removed and rebuilt during excavations in 1930.
Wales.
Chambered long cairns.
There are 18 SAM listed:

</doc>
<doc id="7147" url="http://en.wikipedia.org/wiki?curid=7147" title="Canadian whisky">
Canadian whisky

Canadian whisky is a type of whisky produced in Canada. Most Canadian whiskies are blended multi-grain liquors containing a large percentage of corn spirits, and are typically lighter and smoother than other whisky styles. Several hundred years ago, when Canadian distillers began adding small amounts of highly-flavourful rye grain to their mashes people began demanding this new rye-flavoured whisky, referring to it simply as "rye." Today, as for the past two centuries the terms "rye whisky" and "Canadian whisky" are used interchangeably in Canada and refer to exactly the same product.
Characteristics.
While the lighter and smoother Canadian whiskies are the most widely familiar, the range of products is actually broad and includes some robust whiskies as well.
Historically, in Canada, whisky that had some rye grain added to the mash bill to give it more flavour came to be called “rye”. Although some Canadian whiskies are still labelled as “rye”, Canadian “rye” whisky usually contains high-proof grain whisky blended with lower-proof rye-grain whisky and Canadian-made "bourbon-style" corn whisky as flavouring. Occasionally barley whisky is also used for flavouring. Flavour may also be derived in other ways, such as flavour development from the aging process. It is a common misconception that Canadian whiskies are primarily made using just rye grain. The use of rye grain is not dictated by law, and the primary grain used to make most Canadian whisky is corn.
Regulations.
Canada's Food and Drugs Act require that whisky labeled as "Canadian Whisky" be mashed, distilled and aged three years in Canada. As with Scotch whisky and Irish whiskey, the alcohol content of the spirits used may exceed 90%. Thus, much of the spirits used in making a Canadian whisky, prior to aging, may have less grain-derived flavour than typical single malts or U.S. "straight" whiskeys. To improve marketability, it may contain caramel (as may Scotch) and flavouring, in addition to the distilled mash spirits.
All spirits used in making a Canadian whisky must be aged for at least three years in wooden barrels of not greater than 700 L capacity (a requirement similar to that for Scotch and Irish whisky and longer than for American straight whisky). The final whisky must contain at least 40 percent alcohol by volume. No distinction is made between the quality of the barrels - new or used, charred or uncharred may be filled for aging.
Labelling.
Canadian whisky is recognized internationally as an indigenous product of Canada. Products labelled as Canadian whisky must satisfy the laws of Canada that regulate the manufacture of Canadian whisky for consumption in Canada. When sold in another country, Canadian whisky is typically also required to conform to the local product requirements that apply to whisky in general when sold in that country, which may in some aspects involve stricter or less stringent standards than the Canadian law. Canadian products aged less than three years or not in "small wood" barrels (or failing to meet the domestic Canadian whisky standards in some other way) cannot be called "Canadian whisky" within Canada and in some other countries (such as the U.S.).
History of illicit export to U.S..
Canadian whisky featured prominently in rum-running into the U.S. during Prohibition. Hiram Walker's distillery in Windsor, Ontario, directly across the Detroit River from Detroit, Michigan, easily served bootleggers using small, fast smuggling boats.

</doc>
<doc id="7148" url="http://en.wikipedia.org/wiki?curid=7148" title="Collective noun">
Collective noun

In linguistics, a collective noun is a collection of things taken as a whole. For example, in the phrase "a pride of lions", "pride" is a collective noun.
Most collective nouns in everyday speech, such as "group", are mundane and are not specific to a kind of object. For example, the terms "group of people", "group of dogs", and "group of ideas" are all correct uses. Others, especially words belonging to the large subset of collective nouns known as "terms of venery" (words for groups of animals), are specific to one kind of constituent object. For example, "pride" as a term of venery refers to lions, but not to dogs or cows.
Derivational collectives.
Derivation accounts for many collective words. Because derivation is a slower and less productive word formation process than the more overtly syntactical morphological methods, there are fewer collectives formed this way. As with all derived words, derivational collectives often differ semantically from the original words, acquiring new connotations and even new denotations.
The English endings "-age" and "-ade" often signify a collective. Sometimes the relationship is easily recognizable: "baggage, drainage, blockade." However, even though the etymology is plain to see, the derived words take on quite a special meaning.
German uses the prefix "Ge-" to create collectives. The root word often undergoes umlaut and suffixation as well as receiving the "Ge-" prefix. Nearly all nouns created in this way are of neuter gender. Examples include:
Dutch has a similar pattern, but sometimes uses the (unproductive) circumfix "ge- -te":
In Swedish one example is the different words for mosquitos in the collective form and in the individual form:
Metonymic merging of grammatical number.
Two good examples of collective nouns are "team" and "government", which are both words referring to groups of (usually) people. Both "team" and "government" are count nouns. (Consider: "one team", "two teams", "most teams"; "one government", "two governments", "many governments"). However, confusion often stems from the fact that plural verb forms are often used in British English with the singular forms of these count nouns (for example: "The team have finished the project."). Conversely, in the English language as a whole, singular verb forms can often be used with nouns ending in "-s" that were once considered plural (for example: "Physics is my favorite academic subject"). This apparent "number mismatch" is actually a quite natural and logical feature of human language, and its mechanism is a subtle metonymic shift in the thoughts underlying the words.
In British English, it is generally accepted that collective nouns can take either singular or plural verb forms depending on the context and the metonymic shift that it implies. For example, "the team is in the dressing room" ("formal agreement") refers to "the team" as an ensemble, while "the team are fighting among themselves" ("notional agreement") refers to "the team" as individuals. This is also British English practice with names of countries and cities in sports contexts; for example, "Germany have won the competition.", "Madrid have lost three consecutive matches.", etc. In American English, collective nouns almost invariably take singular verb forms (formal agreement). In cases where a metonymic shift would be otherwise revealed nearby, the whole sentence may be recast to avoid the metonymy. (For example, "The team are fighting among themselves" may become "the team "members" are fighting among themselves" or simply "The team is fighting.") See Comparison of American and British English - Formal and notional agreement.
A good example of such a metonymic shift in the singular-to-plural direction (which, generally speaking, only occurs in British English) is the following sentence: "The team have finished the project." In that sentence, the underlying thought is of the individual members of the team working together to finish the project. Their accomplishment is collective, and the emphasis is not on their individual identities, yet they are at the same time still discrete individuals; the word choice "team have" manages to convey both their collective and discrete identities simultaneously. A good example of such a metonymic shift in the plural-to-singular direction is the following sentence: "Mathematics is my favorite academic subject." The word "mathematics" may have originally been plural in concept, referring to mathematic endeavors, but metonymic shift—that is, the shift in concept from "the endeavors" to "the whole set of endeavors"—produced the usage of "mathematics" as a singular entity taking singular verb forms. (A true mass-noun sense of "mathematics" followed naturally.)
Nominally singular pronouns can be collective nouns taking plural verbs, according to the same rules that apply to other collective nouns. For example, it is correct British English or American English usage to say: "None are so fallible as those who are sure they're right." In this case, the plural verb is used because the context for "none" suggests more than one thing or person.
Terms of venery (words for groups of animals).
The tradition of using "terms of venery" or "nouns of assembly"—collective nouns that are specific to certain kinds of animals—stems from an English hunting tradition of the Late Middle Ages.
The fashion of a consciously developed hunting language came to England from France. It is marked by an extensive proliferation of specialist vocabulary, applying different names to the same feature in different animals. 
These elements can be shown to have already been part of French and English hunting terminology by the beginning of the 14th century. In the course of the 14th century, it became a courtly fashion to extend the vocabulary, and by the 15th century, this tendency had reached exaggerated proportions.
The "Venerie" of Twiti (early 14th century) distinguished three types of droppings of animals, and three different terms for herds of animals. Gaston Phoebus (14th century) had five terms for droppings of animals, which were extended to seven in the "Master of the Game" (early 15th century). The focus on collective terms for groups of animals emerges in the later 15th century. Thus, a list of collective nouns in Egerton MS 1995, dated to c. 1452 under the heading of "termis of venery &c." extends to 70 items, and the list in the "Book of Saint Albans" (1486) runs to 165 items, many of which, even though introduced by "the compaynys of beestys and fowlys", do not relate to venery but to human groups and professions and are clearly humorous. ("a Doctryne of doctoris, a Sentence of Juges, a Fightyng of beggers, an uncredibilite of Cocoldis, a Melody of harpers, a Gagle of women, a Disworship of Scottis" etc.)
The "Book of Saint Albans" became very popular during the 16th century and was reprinted frequently. Gervase Markham edited and commented on the list in his "The Gentleman's Academic" in 1595. The book's popularity had the effect of perpetuating many of these terms as part of the Standard English lexicon, even though they have long ceased to have any practical application. 
Even in their original context of medieval venery, the terms were of the nature of kennings, intended as a mark of erudition of the gentlemen able to use them correctly rather than for practical communication. The popularity of these terms in the early modern and modern period has resulted in the addition of numerous light-hearted, humorous or "facetious" collective nouns.

</doc>
<doc id="7158" url="http://en.wikipedia.org/wiki?curid=7158" title="Carat (mass)">
Carat (mass)

The carat is a unit of mass equal to 200 mg (0.2 g; 0.007055 oz) and is used for measuring gemstones and pearls.
The current definition, sometimes known as the metric carat, was adopted in 1907 at the Fourth General Conference on Weights and Measures, and soon afterwards in many countries around the world. The carat is divisible into one hundred "points" of two milligrams each. Other subdivisions, and slightly different mass values, have been used in the past in different locations.
In terms of diamonds, a paragon is a flawless stone of at least 100 carat (20 g).
The ANSI X.12 EDI standard abbreviation for the carat is CD.
Etymology.
First attested in English in the mid-15th century, the word "carat" came from Italian "carato", which came from Greek "kerátion" (κεράτιον) meaning carob seed (literally "small horn") and potentially from Arabic "qīrāṭ" (قيراط).
and was a unit of weight
though it was not likely used to measure gold in classical times.
The Latin word for carat is "siliqua".
This common belief that carat derives from carob seeds stems from the assumption that the seeds had unusually low variability in mass.
However, one group of researchers have found that carob seeds in fact have typical variability compared to the seeds of other species.
This was not the only reason. It is said that, in order to keep regional buyers and sellers of gold honest, potential customers could retrieve their own carob seeds on their way to the market, to check the tolerances of the seeds used by the merchant. If this precaution was not taken, the potential customers would be at the mercy of "2 sets of carob seeds". One set of "heavier" carob seeds would be used when buying from a customer (making the seller's gold appear to be less). Another, lighter set of carob seeds would be used when the merchant wanted to sell to a customer.
In the past, each country had its own carat. It was often used for weighing gold. Starting in the 1570s, it was used to measure weights of diamonds.
Historical definitions.
UK Board of Trade.
In the United Kingdom the original Board of Trade carat was exactly (≈ 3.170) grains; in 1888, the Board of Trade carat was changed to exactly (≈ 3.168) grains. Despite its being a non-metric unit, a number of metric countries have used this unit for its limited range of application.
The Board of Trade carat was divisible into four "diamond grains", but measurements were typically made in multiples of carat.
Refiners' carats.
There were also two varieties of "refiners' carats" once used in the United Kingdom — the pound carat and the ounce carat. The pound troy was divisible into 24 "pound carats" of 240 grains troy each; the pound carat was divisible into four "pound grains" of 60 grains troy each; and the pound grain was divisible into four "pound quarters" of 15 grains troy each. Likewise, the ounce troy was divisible into 24 "ounce carats" of 20 grains troy each; the ounce carat was divisible into four "ounce grains" of 5 grains troy each; and the ounce grain was divisible into four "ounce quarters" of grains troy each.
Greco-Roman.
The "solidus" was also a Roman weight unit. There is literary evidence that the weight of 72 coins of the type called "solidus" was exactly 1 Roman pound, and that the weight of 1 "solidus" was 24 "siliquae". The weight of a Roman pound is generally believed to have been 327.45 g or possibly up to 5 g less. Therefore, the metric equivalent of 1 "siliqua" was approximately 189 mg. The Greeks had a similar unit of the same value.
Gold fineness in carats comes from carats and grains of gold in a solidus of coin. One solidus = 24 carats, 1 carat = 4 grains, is preserved right up to this day. Woolhouse's "Measures, Weights and Moneys of all Nations" gives gold fineness in carats of 4 grains, and silver in (pound) of 12 ounces each 20 dwt.

</doc>
<doc id="7160" url="http://en.wikipedia.org/wiki?curid=7160" title="European Conference of Postal and Telecommunications Administrations">
European Conference of Postal and Telecommunications Administrations

The European Conference of Postal and Telecommunications Administrations (CEPT) was established on June 26, 1959, as a coordinating body for European state telecommunications and postal organizations. The acronym comes from the French version of its name "Conférence européenne des administrations des postes et des télécommunications".
CEPT was responsible for the creation of the European Telecommunications Standards Institute (ETSI) in 1988.
CEPT is organised into three main components: 
Member countries.
"As of October 2007: 48 countries."
Albania, Andorra, Austria, Azerbaijan, Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Macedonia, Malta, Moldova, Monaco, Montenegro, Netherlands, Norway, Poland, Portugal, Romania, Russian Federation, San Marino, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, Ukraine, United Kingdom, Vatican City,

</doc>
<doc id="7162" url="http://en.wikipedia.org/wiki?curid=7162" title="Tramlink">
Tramlink

Tramlink is a light rail/tram system in south London, England. It began operation in May 2000 as Croydon Tramlink, serving the London Borough of Croydon and London Borough of Merton. It is owned by London Tramlink, an arm of Transport for London (TfL) and operated as a concession by Tram Operations Limited, part of FirstGroup.
Tramlink serves seven National Rail stations and has one interchange with the London Underground, at Wimbledon for the District line, and one with London Overground, at West Croydon for the East London Line. One of the factors leading to its creation was that the London Borough of Croydon has no London Underground service.
Tramlink runs on a mixture of street track shared with other traffic, dedicated track in public roads, and off-street track consisting of new rights-of-way, former railway lines, and one section of alignment (not track) shared with a third rail electrified Network Rail line.
History.
Construction.
In 1990 Croydon Council with the then London Regional Transport (LRT) put the project to Parliament and the Croydon Tramlink Act 1994 resulted, which gave LRT the power to build and run Tramlink.
In 1996 Tramtrack Croydon Limited (TCL) won a 99-year Private Finance Initiative (PFI) contract to design, build, operate and maintain Tramlink. TCL was a partnership comprising First Group, Bombardier Transportation (the builders of the system's trams), Sir Robert McAlpine and Amey (who built the system), and Royal Bank of Scotland and 3i (who arranged the finances). TCL kept the revenue generated by Tramlink and LRT had to pay compensation to TCL for any changes to the fares and ticketing policy introduced later.
TCL subcontracted operations to CentreWest Buses, now part of First London.
Former lines re-used.
 There are four routes: Route 1 – Elmers End to Croydon; Route 2 – Beckenham Junction to Croydon; Route 3 – New Addington to Wimbledon; and Route 4 – Therapia Lane to Elmers End
Route 2 runs parallel to the Crystal Palace to Beckenham Junction line of the Southern network between Birkbeck and Beckenham Junction – the National Rail track had been singled some years earlier.
From Elmers End to Woodside, route 1 and route 4 (and route 2 from Arena) follow the former British Rail branch line to Addiscombe, then diverge to reach Addiscombe tram stop, 500 metres east of the demolished Addiscombe railway station. At Woodside the old station buildings stand disused, and the original platforms have been replaced by accessible low platforms.
From Woodside to near Sandilands (routes 1, 2 and 4) and from near Sandilands almost to Lloyd Park (route 3), Tramlink follows the former Woodside and South Croydon Railway, including the Park Hill (or Sandilands) tunnels.
The section of Route 3 between Wimbledon and West Croydon mostly follows the single-track British Rail route, closed on 31 May 1997 so that it could be converted for Tramlink. Within this section, from near Phipps Bridge to near Reeves Corner, route 3 follows the Surrey Iron Railway, giving Tramlink a claim to the one of the world's oldest railway alignments – beside Mitcham tram stop had its name long before Tramlink. A partial obstruction near this point has necessitated the use of interlaced track.
A Victorian footbridge beside Waddon New Road was dismantled to make way for the flyover over the West Croydon to Sutton railway line. The footbridge has been re-erected at Corfe Castle station on the Swanage Railway (although some evidence suggests that this was a similar footbridge removed from the site of Merton Park Railway Station).
Buyout by Transport for London.
In March 2008 TfL announced that it had reached agreement to buy TCL for £98m. The purchase was finalised on 28 June 2008. The background to this purchase relates to the requirement that TfL (who took over from London Regional Transport in 2000) compensates TCL for the consequences of any changes to the fares and ticketing policy introduced since 1996. In 2007 that payment was £4m, with an annual increase in rate.
In October 2008 TfL introduced a new livery, using the blue, white and green of the routes on TfL maps, to distinguish the trams from buses operating in the area.
Current system.
Stops.
The tram stops have low platforms, above rail level. They are unstaffed and have automated ticket machines. In general, access between the platforms involves crossing the tracks by pedestrian level crossing.
There are 39 stops, most being long. They are virtually level with the doors and are all wider than . This allows wheelchairs, prams, pushchairs and the elderly to board the tram easily with no steps. In street sections, the stop is integrated with the pavement.
Tramlink uses some former main-line stations on the Wimbledon–West Croydon and Elmers End–Coombe Road stretches of line. The railway platforms have been demolished and rebuilt to Tramlink specifications, except at Elmers End and Wimbledon where the track level was raised to meet the higher main-line platforms to enable cross-platform interchange.
Thirty-eight stops opened in the phased introduction of tram services in May 2000. Centrale tram stop in Tamworth Road opened on 10 December 2005, increasing journey times slightly. As turnround times were already quite tight this raised the issue of buying an extra tram to maintain punctuality. Partly for this reason but also to take into account the planned restructuring of services (subsequently introduced in July 2006), TfL issued tenders for a new tram. However, nothing resulted from this.
All stops have disabled access, raised paving, CCTV, a Passenger Help Point, a Passenger Information Display (PID), litter bins, a ticket machine, a noticeboard and lamp-posts, and most also have seats and a shelter.
The PIDs display the destinations and expected arrival times of the next two trams. They can also display any message the controllers want to display, such as information on delays or even instructions to vandals to stop placing objects on the track.
Routes.
Tramlink is shown on the "London Connections" map but not on the tube map. The original routes were Line 1 Wimbledon to Elmers End, Line 2 Croydon to Beckenham Junction, and Line 3 Croydon to New Addington. On 23 July 2006 the network was restructured, with route 1 from Elmers End to Croydon, route 2 from Beckenham Junction to Croydon and route 3 from New Addington to Wimbledon. In June 2012 route 4 from Therapia Lane to Elmers End was introduced.
Change in Route Colours.
When TfL took over a new network map was designed, combining Routes 1 and 2 as one service, coloured "Trams Green" (lime). (Originally, Line 1 was yellow, Line 2 red, and Line 3 a darker (District line) green.) Trams from Elmers End on Route 1 change their numbers in central Croydon to Route 2 (Beckenham Junction) and the reverse in the other direction, but this is likely to change in light of the introduction of Route 4.
Fares and ticketing.
TfL Bus Passes are valid on Tramlink, as are Travelcards that include any of zones 3, 4, 5 and 6.
Cash fares and pay-as-you-go Oyster Card fares are the same as on London Buses, although special fares may apply when using Tramlink feeder buses.
When using Oyster Cards, passengers must touch in on the platform before boarding the tram. Special arrangements apply at Wimbledon station, where the Tramlink stop is within the National Rail and London Underground station.
Rolling stock.
Tramlink is operated with 30 vehicles. The original fleet comprised 24 articulated low floor Flexity Swift CR4000 trams built by Bombardier Transportation in Vienna numbered beginning at 2530, continuing from the highest-numbered tram 2529 on London's former tram network, which closed in 1952. In 2006, the CR4000 fleet was refurbished, including a repaint into a new livery.
, it was reported that four more trams were planned, and to avoid the extra costs of a short production run Tramlink was seeking to lease these from Edinburgh Trams, where the construction of new track and depot faced long delays, but with rolling stock due for delivery from early in 2010. The Edinburgh Tram will be manufactured by CAF of Spain. To accommodate the extra services, some sections of single-track line may be doubled.
In January 2011 Tramtrack Croydon began tendering for the supply of ten new or second-hand trams from the end of summer 2011. The trams will be used between Therapia Lane and Elmers End. On 18 August 2011 TfL announced that Stadler Rail had won a £16 million contract to supply six Variobahn trams similar to those used by Bybanen in Bergen, Norway. They entered service in 2012. In August 2013, TfL ordered an additional four Variotrams for delivery in 2015, for use on the Wimbledon to Croydon link, this will bring the total Variotram fleet up to ten.
Future developments.
Projected extensions.
The Mayor's Transport Strategy for London states that extensions to the network could be developed at relatively modest cost where there is potential demand from existing and new development to support concentrated passenger movements, and where Tramlink technology might be cost effective. Proposal 4D7 says that "The Mayor will explore the potential for extending the Tramlink network where doing so could help meet the objectives of the Transport Strategy cost effectively" and sought initial views on the viability of a number of extensions by summer 2002.
An initial review of potential extensions has been prepared and discussed with interested parties. TfL now wishes to carry out initial development and evaluation work on the following routes:
Other extension proposals include Lewisham, Bromley town centre, Biggin Hill Airport/Village and a local spur/loop to penetrate further into Purley Way retail/industrial park.
Starting in the west, there are two corridors into Sutton town centre. The first, principally between Wimbledon and Sutton, was in view even before Tramlink opened: the trams were delivered with this as "line 4" on their destination blinds.
Extension D / Route 5.
Tramlink route 5 is the only extension being formally developed, linking Harrington Road with Crystal Palace, and Crystal Palace with Beckenham Junction, both terminating at Crystal Palace Parade. There were three options on how to get to the Parade: on-street, off-street and a mixture of the two. Following recent consultation the off-street option is favoured, with trams running along existing railway as far as Crystal Palace Station, and then round the western edge of Crystal Palace Park (within the park's perimeter) to the bus terminus near the parade. TfL has stated that due to lack of funding the plans for this extension will not be taken forward, but also says that it is committed to including new proposals for extensions to the tram as part of a future bid to Government.
Extension A.
The Sutton to Wimbledon proposal utilises the existing line between Wimbledon and Morden Road, but the cramped terminus inside Wimbledon station is barely adequate for its present function. If another service is to serve Wimbledon a new terminus will be needed. Diverging from the present route, the Sutton line might adopt a segregated alignment within the highway along Morden Road, serving Morden station interchange. It would probably use Aberconway Road to reach Morden Hall Road before using the spacious St Helier Avenue as the direct route to St Helier, Rose Hill. St Helier Hospital is an important local traffic objective, despite the need to deviate from the direct route into Sutton via Angel Hill. A number of variants in Sutton Town centre are to be examined to see how the shopping centre, station and office complex can be accessed.
The alignment is served by busy bus routes and would give Tramlink direct with the Northern line at Morden. A south-to-east curve may also be considered at Morden Road to permit direct links from St Helier to Mitcham and Croydon.
In July 2013, Mayor Boris Johnson affirmed that there is a reasonable business case for Tramlink to cover the Wimbledon - Sutton corridor. A map has been released showing the planned route. It would leave the existing route just to the east of Morden Road and head along the A24 and A297 to Rosehill Roundabout, then the B2230 through Sutton town centre, ending at the station. A loop via St Helier Hospital and a possible extension to Royal Marsden Hospital also are shown. Stops would be at Morden Hall, Ivy Lodge, Boxley Road, Langdon Road, Middleton Road, Rosehill Roundabout, St Helier Hospital (on loop), Rosehill Park, Sutton Tennis Centre, Angel Hill, Sutton Green, High Street North, Crown Road (northbound only), St Nicholas Road (northbound only), Throwley Way (southbound only) and Sutton Station
Extension B.
The other Sutton proposal, to Tooting, is more ambitious and contains many more challenges than Sutton/Wimbledon link. Apart from workshop/depot facilities and a curve required to link the line into the existing system, this extension would share no infrastructure with it. If "line 4" is realised ahead of this proposal, the Tooting line would have the St Helier to Sutton section in common. North of St Helier, the alignment is likely to fit across parkland and open space to take in the Willow Lane Industrial Estate before serving Mitcham town centre. There would be some commonality here with the short separate proposal for a spur from Mitcham Junction to Mitcham town centre. From here, the extension would seek to use the pedestrianised town centre before sharing the carriageway with all traffic in London Road south of Figge’s Marsh, with room for segregation beyond the junction with Streatham Road. The most difficult leg arises immediately the Merton/Wandsworth boundary is crossed and the most effective way of reaching Tooting Broadway from this point will stir much debate.
North and south from Croydon.
To the north and south of Croydon are some busy bus corridors, which derive from earlier tram routes. These include the Purley – Croydon – Streatham corridor, which is proposed for conversion to tram operation.
To the south of Croydon, the proposal is for the new route to diverge from the central Croydon loop and use a highway alignment, probably South End and Brighton Road, to Purley. Beyond Purley, an extension to Coulsdon will be investigated. As this would be close to the M23 motorway, a possibility would be the construction of a park and ride site. Finding a good alignment will be more difficult south of Purley, where Brighton Road is the A23 trunk road.
To the north of Croydon, it is proposed to use a highway alignment based on London Road. To the south of Thornton Heath Pond, the use of a shared carriageway is a possibility. North of this point the road becomes the A23 again, but there are likely to be some opportunities for trambaan type segregation to Norbury and between Norbury and Streatham, although Norbury is a pinch point. The proposal is to terminate the line at Streatham railway station, providing an interchange to the extended East London Line.
Other extensions.
Work currently commissioned will check out proposals to extend to Biggin Hill, Bromley town centre, Lewisham, and Purley Way. If initial examination shows promise, further work could follow to firm up more detailed routings.
Accidents and incidents.
On 7 September 2008 a bus on route 468 collided with tram 2534 in George Street, Croydon, and one person was killed. A BMW car was also involved. The victim was thought to have been a pedestrian waiting to cross the road, but he was a passenger thrown through the upper front window of the bus. The driver of the bus was convicted of causing death by dangerous driving.
On 13 September 2008 tram 2530 collided with a cyclist at Morden Hall Park footpath crossing between Morden Road and Phipps Bridge tram stops. The cyclist sustained injuries from which he later died.
On 15 November 2010, a boy aged seven was hit by a tram at Fieldway tram stop while crossing the tracks on his way to school. He was taken to St George's Hospital with serious leg injuries.
On 5 April 2011, a woman tripped over and was dragged under a moving tram. She was taken to hospital in a serious condition. She is believed to have been running to catch the tram outside East Croydon Station when she tripped and fell.
On 8 August 2011, track and overhead line equipment between Reeves Corner and Church Street were severely damaged by fire when the House of Reeves store 40 metres away was set alight during the riots in London. Services were suspended when rioting and looting began in the area at around 21:30. The fire was at the junction between the lines to Reeves Corner, Church Street and Centrale tram stops, meaning that all trams were blocked from getting into Croydon from the west.
On 17 February 2012, a tram derailed at East Croydon station, causing major delays.
Onboard announcements.
The onboard announcements are by BBC news reader (and tram enthusiast) Nicholas Owen. The announcer system is as follows: e.g. "This tram is for Wimbledon, The next stop will be Merton Park."

</doc>
<doc id="7163" url="http://en.wikipedia.org/wiki?curid=7163" title="Catenary">
Catenary

In physics and geometry, a catenary[p] is the curve that an idealized hanging chain or cable assumes under its own weight when supported only at its ends. The curve has a U-like shape, superficially similar in appearance to a parabola, but it is not a parabola: it is a (scaled, rotated) graph of the hyperbolic cosine. The curve appears in the design of certain types of arches and as a cross section of the catenoid—the shape assumed by a soap film bounded by two parallel circular rings.
The catenary is also called the "alysoid", "chainette", or, particularly in the material sciences, "funicular".
Mathematically, the catenary curve is the graph of the hyperbolic cosine function. The surface of revolution of the catenary curve, the catenoid, is a minimal surface, specifically a minimal surface of revolution. The mathematical properties of the catenary curve were first studied by Robert Hooke in the 1670s, and its equation was derived by Leibniz, Huygens and Johann Bernoulli in 1691.
Catenaries and related curves are used in architecture and engineering, in the design of bridges and arches, so that forces do not result in bending moments.
Note also the wider meaning of the word 'catenary' used since mid-1990s in the offshore oil and gas industry of steel catenary riser.
History.
The word "catenary" is derived from the Latin word "catena," which means "chain". The English word "catenary" is usually attributed to Thomas Jefferson,
who wrote in a letter to Thomas Paine on the construction of an arch for a bridge:
It is often said that Galileo thought the curve of a hanging chain was parabolic. In his "Two New Sciences" (1638), Galileo says that a hanging cord is an approximate parabola, and he correctly observes that this approximation improves as the curvature gets smaller and is almost exact when the elevation is less than 45°. That the curve followed by a chain is not a parabola was proven by Joachim Jungius (1587–1657); this result was published posthumously in 1669.
The application of the catenary to the construction of arches is attributed to Robert Hooke, whose "true mathematical and mechanical form" in the context of the rebuilding of St Paul's Cathedral alluded to a catenary. Some much older arches approximate catenaries, an example of which is the Arch of Taq-i Kisra in Ctesiphon.
In 1671, Hooke announced to the Royal Society that he had solved the problem of the optimal shape of an arch, and in 1675 published an encrypted solution as a Latin anagram in an appendix to his "Description of Helioscopes,"
where he wrote that he had found "a true mathematical and mechanical form of all manner of Arches for Building." He did not publish the solution to this anagram in his lifetime, but in 1705 his executor provided it as "Ut pendet continuum flexile, sic stabit contiguum rigidum inversum," meaning "As hangs a flexible cable so, inverted, stand the touching pieces of an arch."
In 1691 Gottfried Leibniz, Christiaan Huygens, and Johann Bernoulli derived the equation in response to a challenge by Jakob Bernoulli. David Gregory wrote a treatise on the catenary in 1697.
Euler proved in 1744 that the catenary is the curve which, when rotated about the "x"-axis, gives the surface of minimum surface area (the catenoid) for the given bounding circles. Nicolas Fuss gave equations describing the equilibrium of a chain under any force in 1796.
Inverted catenary arch.
Catenary arches are often used in the construction of kilns. To create the desired curve, the shape of a hanging chain of the desired dimensions is transferred to a form which is then used as a guide for the placement of bricks or other building material.
The Gateway Arch in St. Louis, Missouri, United States is sometimes said to be an (inverted) catenary, but this is incorrect. It is close to a more general curve called a flattened catenary, with equation , which is a catenary if . While a catenary is the ideal shape for a freestanding arch of constant thickness, the Gateway Arch is narrower near the top. According to the U.S. National Historic Landmark nomination for the arch, it is a "weighted catenary" instead. Its shape corresponds to the shape that a weighted chain, having lighter links in the middle, would form.
Catenary bridges.
In free-hanging chains, the force exerted is uniform with respect to length of the chain, and so the chain follows the catenary curve. The same is true of a simple suspension bridge or "catenary bridge," where the roadway follows the cable.
A stressed ribbon bridge is a more sophisticated structure with the same catenary shape.
However in a suspension bridge with a suspended roadway, the chains or cables support the weight of the bridge, and so do not hang freely. In most cases the roadway is flat, so when the weight of the cable is negligible compared with the weight being supported, the force exerted is uniform with respect to horizontal distance, and the result is a parabola, as discussed below (although the term "catenary" is often still used, in an informal sense). If the cable is heavy then the resulting curve is between a catenary and a parabola.
Anchoring of marine objects.
The catenary produced by gravity provides an advantage to heavy anchor rodes. An anchor rode (or anchor line) usually consists of chain or cable or both. Anchor rodes are used by ships, oilrigs, docks, floating wind turbines, and other marine equipment which must be anchored to the seabed.
When the rode is slack, the catenary curve presents a lower angle of pull on the anchor or mooring device than would be the case if it were nearly straight. This enhances the performance of the anchor and raises the level of force it will resist before dragging. To maintain the catenary shape in the presence of wind, a heavy chain is needed, so that only larger ships in deeper water can rely on this effect. Smaller boats must rely on the performance of the anchor itself.
Mathematical description.
Equation.
The equation of a catenary in Cartesian coordinates has the form
where cosh is the hyperbolic cosine function. All catenary curves are similar to each other, having eccentricity = . Changing the parameter "a" is equivalent to a uniform scaling of the curve.
The Whewell equation for the catenary is
Differentiating gives
and eliminating formula_4 gives the Cesàro equation
The radius of curvature is then
which is the length of the line normal to the curve between it and the "x"-axis.
Relation to other curves.
When a parabola is rolled along a straight line, the roulette curve traced by its focus is a catenary. The envelope of the directrix of the parabola is also a catenary. The involute from the vertex, that is the roulette formed traced by a point starting at the vertex when a line is rolled on a catenary, is the tractrix.
Another roulette, formed by rolling a line on a catenary, is another line. This implies that square wheels can roll perfectly smoothly if the road has evenly spaced bumps in the shape of a series of inverted catenary curves. The wheels can be any regular polygon except a triangle, but the catenary must have parameters corresponding to the shape and dimensions of the wheels.
Geometrical properties.
Over any horizontal interval, the ratio of the area under the catenary to its length equals "a", independent of the interval selected. The catenary is the only plane curve other than a horizontal line with this property. Also, the geometric centroid of the area under a stretch of catenary is the midpoint of the perpendicular segment connecting the centroid of the curve itself and the x-axis.
Science.
A charge in a uniform electric field moves along a catenary (which tends to a parabola if the charge velocity is much less than the speed of light "c").
The surface of revolution with fixed radii at either end that has minimum surface area is a catenary revolved about the x-axis.
Analysis.
Model of chains and arches.
In the mathematical model the chain (or cord, cable, rope, string, etc.) is idealized by assuming that it is so thin that it can be regarded as a curve and that it is so flexible any force of tension exerted by the chain is parallel to the chain. The analysis of the curve for an optimal arch is similar except that the forces of tension become forces of compression and everything is inverted.
An underlying principle is that the chain may be considered a rigid body once it has attained equilibrium. Equations which define the shape of the curve and the tension of the chain at each point may be derived by a careful inspection of the various forces acting on a segment using the fact that these forces must be in balance if the chain is in static equilibrium.
Let the path followed by the chain be given parametrically by
r = ("x", "y") = ("x"("s"), "y"("s")) where "s" represents arc length and r is the position vector. This is the natural parameterization and has the property that 
where u is a unit tangent vector. 
A differential equation for the curve may be derived as follows. Let c be the lowest point on the chain, called the "vertex" of the catenary,
 and measure the parameter "s" from c. Assume r is to the right of c since the other case is implied by symmetry. The forces acting on the section of the chain from c to r are the tension of the chain at c, the tension of the chain at r, and the weight of the chain. The tension at c is tangent to the curve at c and is therefore horizontal, and it pulls the section to the left so it may be written (−"T"0, 0) where "T"0 is the magnitude of the force. The tension at r is parallel to the curve at r and pulls the section to the right, so it may be written "T"u=("T"cos φ, "T"sin φ), where "T" is the magnitude of the force and φ is the angle between the curve at r and the "x"-axis (see tangential angle). Finally, the weight of the chain is represented by (0, −λ"gs") where λ is the mass per unit length, "g" is the acceleration of gravity and "s" is the length of chain between c and r.
The chain is in equilibrium so the sum of three forces is 0, therefore
and
and dividing these gives
It is convenient to write 
which is the length of chain whose weight is equal in magnitude to the tension at c. Then 
is an equation defining the curve.
The horizontal component of the tension, "T"cos φ = "T"0 is constant and the vertical component of the tension, "T"sin φ = λ"gs" is proportional to the length of chain between the r and the vertex.
Derivation of equations for the curve.
The differential equation given above can be solved to produce equations for the curve.
From
the formula for arc length gives
Then
and
The second of these equations can be integrated to give
and by shifting the position of the "x"-axis, β can be taken to be 0. Then
The "x"-axis thus chosen is called the "directrix" of the catenary.
It follows that the magnitude of the tension at a point "T" = λ"gy" which is proportional to the distance between the point and the directrix.
The integral of expression for "dx"/"ds" can be found using standard techniques giving
and, again, by shifting the position of the "y"-axis, α can be taken to be 0. Then
The "y"-axis thus chosen passes though the vertex and is called the "axis" of the catenary.
These results can be used to eliminate "s" giving
Alternative derivation.
The differential equation can be solved using a different approach.
From
it follows that
and
Integrating gives,
and
As before, the "x" and "y"-axes can be shifted so α and β can be taken to be 0. Then
and taking the reciprocal of both sides
Adding and subtracting the last two equations then gives the solution
and
Determining parameters.
In general the parameter "a" and the position of the axis and directrix are not given but must be determined from other information. Typically, the information given is that the 
catenary is suspended at given points "P"1 and "P"2 and with given length "s". The equation can be determined in this case as follows:
Relabel if necessary so that "P"1 is to the left of "P"2 and let "h" be the horizontal and "v" be the vertical distance from "P"1 to "P"2. Translate the axes so that the vertex of the catenary lies on the y-axis and its height "a" is adjusted so the catenary satisfies the standard equation of the curve
and let the coordinates of "P"1 and "P"2 be ("x"1, "y"1) and ("x"2, "y"2) respectively. The curve passes through these points, so the difference of height is
and the length of the curve from "P"1 to "P"2 is
When s2−v2 is expanded using these expressions the result is
so
This is a transcendental equation in "a" and must be solved numerically. It can be shown with the methods of calculus that there is at most one solution with "a">0 and so there is at most one position of equilibrium.
Generalizations with vertical force.
Nonuniform chains.
If the density of the chain is variable then the analysis above can be adapted to produce equations for the curve given the density, or given the curve to find the density.
Let "w" denote the weight per unit length of the chain, then the weight of the chain has magnitude
where the limits of integration are c and r. Balancing forces as in the uniform chain produces
and
and therefore
Differentiation then gives
In terms of φ and the radius of curvature ρ this becomes
Suspension bridge curve.
A similar analysis can be done to find the curve followed by the cable supporting a suspension bridge with a horizontal roadway. If the weight of the roadway per unit length is "w" and the weight of the cable and the wire supporting the bridge is negligible in comparison, then the weight on the cable from c to r is "wx" where "x" is the horizontal distance between c to r. Proceeding as before gives the differential equation
This is solved by simple integration to get
and so the cable follows a parabola. If the weight of the cable and supporting wires are not negligible then the analysis is more complex.
Catenary of equal strength.
In a catenary of equal strength, cable is strengthened according to the magnitude of the tension at each point, so its resistance to breaking is constant along its length. Assuming that the strength of the cable is proportional to its density per unit length, the weight, "w", per unit length of the chain can be written "T"/"c", where "c" is constant, and the analysis for nonuniform chains can be applied.
In this case the equations for tension are
Combining gives
and by differentiation
where ρ is the radius of curvature.
The solution to this is
In this case, the curve has vertical asymptotes and this limits the span to π"c". Other relations are
The curve was studied 1826 by Davies Gilbert and, apparently independently, by Gaspard-Gustave Coriolis in 1836.
Elastic catenary.
In an elastic catenary, the chain is replaced by a spring which can stretch in response to tension. The spring is assumed to stretch in accordance with Hooke's Law. Specifically, if "p" is the natural length of a section of spring, then the length of the spring with tension "T" applied has length
where "E" is a constant equal to "kp", where "k" is the stiffness of the spring. In the catenary the value of "T" is variable, but ratio remains valid at a local level, so
The curve followed by an elastic spring can now be derived following a similar method as for the inelastic spring.
The equations for tension of the spring are 
and
from which
where "p" is the natural length of the segment from c to r and λ0 is the mass per unit length of the spring with no tension and "g" is the acceleration of gravity. Write
so
Then 
and 
from which
and 
Integrating gives the parametric equations
Again, the x and y-axes can be shifted so α and β can be taken to be 0. So
are parametric equations for the curve. At the rigid limit where "E" is large, the shape of the curve reduces to that of a non-elastic chain.
Other generalizations.
Chain under a general force.
With no assumptions have been made regarding the force G acting on the chain, the following analysis can be made.
First, let T=T("s") be the force of tension as a function of "s". The chain is flexible so it can only exert a force parallel to itself. Since tension is defined as the force that the chain exerts on itself, T must be parallel to the chain. In other words,
where "T" is the magnitude of T and u is the unit tangent vector.
Second, let G=G("s") be the external force per unit length acting on a small segment of a chain as a function of "s". The forces acting on the segment of the chain between "s" and "s"+Δ"s" are the force of tension T("s"+Δ"s") at one end of the segment, the nearly opposite force −T("s") at the other end, and the external force acting on the segment which is
approximately GΔ"s". These forces must balance so
Divide by Δ"s" and take the limit as Δ"s" → 0 to obtain
These equations can be used as the starting point in the analysis of a flexible chain acting under any external force. In the case of the standard catenary, G = (0, −λ"g") where the chain has mass λ per unit length and "g" is the acceleration of gravity.

</doc>
<doc id="7164" url="http://en.wikipedia.org/wiki?curid=7164" title="Color temperature">
Color temperature

The color temperature of a light source is the temperature of an ideal black-body radiator that radiates light of comparable hue to that of the light source. Color temperature is a characteristic of visible light that has important applications in lighting, photography, videography, publishing, manufacturing, astrophysics, horticulture, and other fields. In practice, color temperature is only meaningful for light sources that do in fact correspond somewhat closely to the radiation of some black body, i.e., those on a line from reddish/orange via yellow and more or less white to blueish white; it does not make sense to speak of the color temperature of, e.g., a green or a purple light. Color temperature is conventionally stated in the unit of absolute temperature, the kelvin, having the unit symbol K.
Color temperatures over are called "cool colors" (bluish white), while lower color temperatures (2,700–3,000 K) are called "warm colors" (yellowish white through red). This relation, however, is a psychological one in contrast to the physical relation implied by Wien's displacement law, according to which the spectral peak is shifted towards shorter wavelengths (resulting in a more blueish white) for higher temperatures.
Categorizing different lighting.
The color temperature of the electromagnetic radiation emitted from an ideal black body is defined as its surface temperature in kelvins, or alternatively in "mireds" (micro-reciprocal kelvins). This permits the definition of a standard by which light sources are compared.
To the extent that a hot surface emits thermal radiation but is not an ideal black-body radiator, the color temperature of the light is not the actual temperature of the surface. An incandescent lamp's light is thermal radiation, and the bulb approximates an ideal black-body radiator, so its color temperature is essentially the temperature of the filament.
Many other light sources, such as fluorescent lamps, or LEDs (light emitting diodes) emit light primarily by processes other than thermal radiation. This means that the emitted radiation does not follow the form of a black-body spectrum. These sources are assigned what is known as a correlated color temperature (CCT). CCT is the color temperature of a black-body radiator which to human color perception most closely matches the light from the lamp. Because such an approximation is not required for incandescent light, the CCT for an incandescent light is simply its unadjusted temperature, derived from the comparison to a black-body radiator.
The Sun.
The Sun closely approximates a black-body radiator. The effective temperature, defined by the total radiative power per square unit, is about 5,780 K. The color temperature of sunlight above the atmosphere is about 5,900 K.
As the Sun crosses the sky, it may appear to be red, orange, yellow or white depending on its position. The changing color of the Sun over the course of the day is mainly a result of scattering of light and is not due to changes in black-body radiation. The blue color of the sky is caused by Rayleigh scattering of the sunlight from the atmosphere, which tends to scatter blue light more than red light.
Daylight has a spectrum similar to that of a black body with a correlated color temperature of 6,500 K (D65 viewing standard) or 5,500 K (daylight-balanced photographic film standard).
For colors based on black-body theory, blue occurs at higher temperatures, while red occurs at lower, cooler, temperatures. This is the opposite of the cultural associations attributed to colors, in which "red" is "hot", and "blue" is "cold".
Color temperature applications.
Lighting.
For lighting building interiors, it is often important to take into account the color temperature of illumination. For example, a warmer (i.e., lower color temperature) light is often used in public areas to promote relaxation, while a cooler (higher color temperature) light is used to enhance concentration in offices.
CCT dimming for LED technology is regarded as a difficult task, since binning, age and temperature drift effects of LEDs change the actual color value output. Here feedback loop systems are used for example with color sensors, to actively monitor and control the color output of multiple color mixing LEDs.
Aquaculture.
In fishkeeping, color temperature has different functions and foci, for different branches.
Digital photography.
In digital photography, "color temperature" is sometimes used interchangeably with "white balance", which allow a remapping of color values to simulate variations in ambient color temperature. Most digital cameras and RAW image software provide presets simulating specific ambient values (e.g., sunny, cloudy, tungsten, etc.) while others allow explicit entry of white balance values in kelvins. These settings vary color values along the blue–yellow axis, while some software includes additional controls (sometimes labeled "tint") adding the magenta–green axis, and are to some extent arbitrary and subject to artistic interpretation.
Photographic film.
Photographic emulsion film sometimes appears to exaggerate the color of the light, as it does not adapt to lighting color as human visual perception does. An object that appears to the eye to be white may turn out to look very blue or orange in a photograph. The color balance may need to be corrected while shooting or while printing to achieve a neutral color print.
Photographic film is made for specific light sources (most commonly daylight film and tungsten film), and used properly, will create a neutral color print. Matching the sensitivity of the film to the color temperature of the light source is one way to balance color. If tungsten film is used indoors with incandescent lamps, the yellowish-orange light of the tungsten incandescent lamps will appear as white (3,200 K) in the photograph.
Filters on a camera lens, or color gels over the light source(s) may also be used to correct color balance. When shooting with a bluish light (high color temperature) source such as on an overcast day, in the shade, in window light or if using tungsten film with white or blue light, a yellowish-orange filter will correct this. For shooting with daylight film (calibrated to 5,600 K) under warmer (low color temperature) light sources such as sunsets, candlelight or tungsten lighting, a bluish (e.g., #80A) filter may be used.
If there is more than one light source with varied color temperatures, one way to balance the color is to use daylight film and place color-correcting gel filters over each light source.
Photographers sometimes use color temperature meters. Color temperature meters are usually designed to read only two regions along the visible spectrum (red and blue); more expensive ones read three regions (red, green, and blue). However, they are ineffective with sources such as fluorescent or discharge lamps, whose light varies in color and may be harder to correct for. Because it is often greenish, a magenta filter may correct it. More sophisticated colorimetry tools can be used where such meters are lacking.
Desktop publishing.
In the desktop publishing industry, it is important to know a monitor’s color temperature. Color matching software, such as Apple's ColorSync for Mac OS, will measure a monitor's color temperature and then adjust its settings accordingly. This enables on-screen color to more closely match printed color. Common monitor color temperatures, along with matching standard illuminants in parentheses, are as follows:
D50 is scientific shorthand for a standard illuminant: the daylight spectrum at a correlated color temperature of 5,000 K. Similar definitions exist for D55, D65 and D75. Designations such as "D50" are used to help classify color temperatures of light tables and viewing booths. When viewing a color slide at a light table, it is important that the light be balanced properly so that the colors are not shifted towards the red or blue.
Digital cameras, web graphics, DVDs, etc., are normally designed for a 6,500 K color temperature. The sRGB standard commonly used for images on the Internet stipulates (among other things) a 6,500 K display whitepoint.
TV, video, and digital still cameras.
The NTSC and PAL TV norms call for a compliant TV screen to display an electrically black and white signal (minimal color saturation) at a color temperature of 6,500 K. On many consumer-grade televisions, there is a very noticeable deviation from this requirement. However, higher-end consumer-grade televisions can have their color temperatures adjusted to 6,500 K by using a preprogrammed setting or a custom calibration. Current versions of ATSC explicitly call for the color temperature data to be included in the data stream, but old versions of ATSC allowed this data to be omitted. In this case, current versions of ATSC cite default colorimetry standards depending on the format. Both of the cited standards specify a 6,500 K color temperature.
Most video and digital still cameras can adjust for color temperature by zooming into a white or neutral colored object and setting the manual "white balance" (telling the camera that "this object is white"); the camera then shows true white as white and adjusts all the other colors accordingly. White-balancing is necessary especially when indoors under fluorescent lighting and when moving the camera from one lighting situation to another. Most cameras also have an automatic white balance function that attempts to determine the color of the light and correct accordingly. While these settings were once unreliable, they are much improved in today's digital cameras, and will produce an accurate white balance in a wide variety of lighting situations.
Artistic application via control of color temperature.
Video camera operators can white-balance objects which aren't white, downplaying the color of the object used for white-balancing. For instance, they can bring more warmth into a picture by white-balancing off something light blue, such as faded blue denim; in this way white-balancing can serve in place of a filter or lighting gel when those are not available.
Cinematographers do not "white balance" in the same way as video camera operators; they can use techniques such as filters, choice of film stock, pre-flashing, and after shooting, color grading (both by exposure at the labs and also digitally). Cinematographers also work closely with set designers and lighting crews to achieve the desired effects.
For artists, most pigments and papers have a cool or warm cast, as the human eye can detect even a minute amount of saturation. Gray mixed with yellow, orange or red is a "warm gray". Green, blue, or purple, create "cool grays". Note that this sense of "temperature" is the reverse of that of real temperature; bluer is described as "cooler" even though it corresponds to a higher-temperature black body.
Lighting designers sometimes select filters by color temperature, commonly to match light that is theoretically white. Since fixtures using discharge type lamps produce a light of considerably higher color temperature than tungsten lamps, using the two in conjunction could potentially produce a stark contrast, so sometimes fixtures with HID lamps, commonly producing light of 6,000–7,000 K, are fitted with 3,200 K filters to emulate tungsten light. Fixtures with color mixing features or with multiple colors, (if including 3,200 K) are also capable of producing tungsten like light. Color temperature may also be a factor when selecting lamps, since each is likely to have a different color temperature.
Correlated color temperature.
Motivation.
Black-body radiators are the reference by which the whiteness of light sources is judged. A black body can be described by its color temperature, whose hues are depicted above. By analogy, nearly Planckian light sources such as certain fluorescent or high-intensity discharge lamps can be judged by their "correlated" color temperature (CCT); the color temperature of the Planckian radiator that best approximates them. The question is: what is the relationship between the light source's relative spectral power distribution and its correlated color temperature?
Background.
The notion of using Planckian radiators as a yardstick against which to judge other light sources is not a new one. In 1923, writing about "grading of illuminants with reference to quality of color…the temperature of the source as an index of the quality of color", Priest essentially described CCT as we understand it today, going so far as to use the term "apparent color temperature", and astutely recognized three cases:
Several important developments occurred in 1931. In chronological order:
These developments paved the way for the development of new chromaticity spaces that are more suited to the estimation of correlated color temperatures and chromaticity differences. Bridging the concepts of color difference and color temperature, Priest made the observation that the eye is sensitive to constant differences in "reciprocal" temperature:
Priest proposed to use "the scale of temperature as a scale for arranging the chromaticities of the several illuminants in a serial order". Over the next few years, Judd published three more significant papers:
The first verified the findings of Priest, Davis, and Judd, with a paper on sensitivity to change in color temperature.
The second proposed a new chromaticity space, guided by a principle that has become the holy grail of color spaces: perceptual uniformity (chromaticity distance should be commensurate with perceptual difference). By means of a projective transformation, Judd found a more "uniform chromaticity space" (UCS) in which to find the CCT. Judd determined the "nearest color temperature" by simply finding the nearest point on the Planckian locus to the chromaticity of the stimulus on Maxwell's color triangle, depicted aside. The transformation matrix he used to convert X,Y,Z tristimulus values to R,G,B coordinates was:
From this, one can find these chromaticities:
The third depicted the locus of the isothermal chromaticities on the CIE 1931 "x,y" chromaticity diagram. Since the isothermal points formed normals on his UCS diagram, transformation back into the xy plane revealed them still to be lines, but no longer perpendicular to the locus.
Calculation.
Judd's idea of determining the nearest point to the Planckian locus on a uniform chromaticity space is current. In 1937, MacAdam suggested a "modified uniform chromaticity scale diagram", based on certain simplifying geometrical considerations:
This (u,v) chromaticity space became the CIE 1960 color space, which is still used to calculate the CCT (even though MacAdam did not devise it with this purpose in mind). Using other chromaticity spaces, such as u'v', leads to non-standard results that may nevertheless be perceptually meaningful.
The distance from the locus (i.e., degree of departure from a black body) is traditionally indicated in units of formula_4; positive for points above the locus. This concept of distance has evolved to become Delta E, which continues to be used today.
Robertson's method.
Before the advent of powerful personal computers, it was common to estimate the correlated color temperature by way of interpolation from look-up tables and charts. The most famous such method is Robertson's, who took advantage of the relatively even spacing of the mired scale (see above) to calculate the CCT Tc using linear interpolation of the isotherm's mired values:
where formula_6 and formula_7 are the color temperatures of the look-up isotherms and "i" is chosen such that formula_8. (Furthermore, the test chromaticity lies between the only two adjacent lines for which formula_9.)
If the isotherms are tight enough, one can assume formula_10, leading to
The distance of the test point to the "i"-th isotherm is given by
where formula_13 is the chromaticity coordinate of the "i"-th isotherm on the Planckian locus and "mi" is the isotherm's slope. Since it is perpendicular to the locus, it follows that formula_14 where "li" is the slope of the locus at formula_13.
Precautions.
Although the CCT can be calculated for any chromaticity coordinate, the result is meaningful only if the light sources are nearly white. The CIE recommends that "The concept of correlated color temperature should not be used if the chromaticity of the test source differs more than [formula_16] from the Planckian radiator."
Beyond a certain value of formula_17, a chromaticity co-ordinate may be equidistant to two points on the locus, causing ambiguity in the CCT.
Approximation.
If a narrow range of color temperatures is considered—those encapsulating daylight being the most practical case—one can approximate the Planckian locus in order to calculate the CCT in terms of chromaticity coordinates. Following Kelly's observation that the isotherms intersect in the purple region near ("x" = 0.325, "y" = 0.154), McCamy proposed this cubic approximation:
where "n" = ("x" − "xe")/("y" − "ye") is the inverse slope line, and ("xe" = 0.3320, "ye" = 0.1858) is the "epicenter"; quite close to the intersection point mentioned by Kelly. The maximum absolute error for color temperatures ranging from 2856 K (illuminant A) to 6504 K (D65) is under 2 K.
A more recent proposal, using exponential terms, considerably extends the applicable range by adding a second epicenter for high color temperatures:
where "n" is as before and the other constants are defined below:
The inverse calculation, from color temperature to corresponding chromaticity coordinates, is discussed in Planckian locus.
Color rendering index.
The CIE color rendering index (CRI) is a method to determine how well a light source's illumination of eight sample patches compares to the illumination provided by a reference source. Cited together, the CRI and CCT give a numerical estimate of what reference (ideal) light source best approximates a particular artificial light, and what the difference is.
Spectral power distribution.
Light sources and illuminants may be characterized by their spectral power distribution (SPD). The relative SPD curves provided by many manufacturers may have been produced using 10 nm increments or more on their spectroradiometer. The result is what would seem to be a smoother ("fuller spectrum") power distribution than the lamp actually has. Owing to their spiky distribution, much finer increments are advisable for taking measurements of fluorescent lights, and this requires more expensive equipment.
Color temperature in astronomy.
In astronomy, the color temperature is defined by the local slope of the SPD at a given wavelength, or, in practice, a wavelength range. Given, for example, the color magnitudes "B" and "V" which are calibrated to be equal for an A0V star (e.g., Vega), the stellar color temperature formula_18 is given by the temperature for which the color index formula_19 of a blackbody radiator fits the stellar one. Besides the formula_19, other color indices can used as well. The color temperature (as well as the correlated color temperature defined above) may differ largely from the effective temperature given by the radiative flux of the stellar surface. For example, the color temperature of an A0V star is about 15,000 K compared to an effective temperature of about 9500 K.

</doc>
<doc id="7165" url="http://en.wikipedia.org/wiki?curid=7165" title="Cartoon">
Cartoon

A cartoon is a form of two-dimensional illustrated visual art. While the specific definition has changed over time, modern usage refers to a typically non-realistic or semi-realistic drawing or painting intended for satire, caricature, or humor, or to the artistic style of such works. An artist who creates cartoons is called a cartoonist.
The term originated in the Middle Ages and first described a preparatory drawing for a piece of art, such as a painting, fresco, tapestry, or stained glass window. In the 19th century, it came to refer to humorous illustrations in magazines and newspapers, and in the early 20th century and onward it referred to comic strips and animated films.
Fine art.
A cartoon (from the Italian "cartone" and Dutch word "karton", meaning strong, heavy paper or pasteboard) is a full-size drawing made on sturdy paper as a study or "modello" for a painting, stained glass or tapestry. Cartoons were typically used in the production of frescoes, to accurately link the component parts of the composition when painted on damp plaster over a series of days ("giornate").
Such cartoons often have pinpricks along the outlines of the design; a bag of soot was then patted or "pounced" over the cartoon, held against the wall to leave black dots on the plaster ("pouncing"). Cartoons by painters, such as the Raphael Cartoons in London and examples by Leonardo da Vinci, are highly prized in their own right. Tapestry cartoons, usually coloured, were followed by eye by the weavers on the loom.
Print media.
In modern print media, a cartoon is a piece of art, usually humorous in intent. This usage dates from 1843 when "Punch" magazine applied the term to satirical drawings in its pages, particularly sketches by John Leech. The first of these parodied the preparatory cartoons for grand historical frescoes in the then-new Palace of Westminster. The original title for these drawings was "Mr Punch's face is the letter Q" and the new title "cartoon" was intended to be ironic, a reference to the self-aggrandizing posturing of Westminster politicians.
Modern single-panel gag cartoons, found in magazines, generally consist of a single drawing with a typeset caption positioned beneath or (much less often) a speech balloon. Newspaper syndicates have also distributed single-panel gag cartoons by Mel Calman, Bill Holman, Gary Larson, George Lichty, Fred Neher and others. Many consider "New Yorker" cartoonist Peter Arno the father of the modern gag cartoon (as did Arno himself). The roster of magazine gag cartoonists includes Charles Addams, Charles Barsotti and Chon Day.
Bill Hoest, Jerry Marcus and Virgil Partch began as a magazine gag cartoonists and moved on to do syndicated comic strips. Noteworthy in the area of newspaper cartoon illustration is Richard Thompson, who illustrated numerous feature articles in "The Washington Post" before creating his "Cul de Sac" comic strip. Sports sections of newspapers usually featured cartoons, sometimes including syndicated features such as Chester "Chet" Brown's "All in Sport".
Editorial cartoons are found almost exclusively in news publications and news websites. Although they also employ humor, they are more serious in tone, commonly using irony or satire. The art usually acts as a visual metaphor to illustrate a point of view on current social and/or political topics. Editorial cartoons often include speech balloons and, sometimes, multiple panels. Editorial cartoonists of note include Herblock, David Low, Jeff MacNelly, Mike Peters and Gerald Scarfe.
Comic strips, also known as "cartoon strips" in the United Kingdom, are found daily in newspapers worldwide, and are usually a short series of cartoon illustrations in sequence. In the United States they are not as commonly called "cartoons" themselves, but rather "comics" or "funnies". Nonetheless, the creators of comic strips—as well as comic books and graphic novels—are usually referred to as "cartoonists". Although humor is the most prevalent subject matter, adventure and drama are also represented in this medium. Noteworthy cartoonists of humor strips include Scott Adams, Steve Bell, Charles Schulz, E. C. Segar, Mort Walker and Bill Watterson.
Political cartoons.
By the mid 19th century, major political newspapers in many countries featured cartoons commenting on the politics of the day. Thomas Nast in New York City brought realistic German drawing techniques to enliven American cartooning. his 160 cartoons relentlessly pursued the criminal characteristic of the Tweed machine in New York City, and help bring it down. Indeed, Tweed was arrested in Spain, when police identified him from Nast's cartoons. Sir John Tenniel was the toast of London.
Political cartoons can be humorous or satirical, sometimes with piercing effect. The target may complain, but he can seldom fight back. Lawsuits have been very rare. the first successful lawsuit against the cartoonist in over a century in Britain came in 1921 when 
J.H. Thomas, the leader of the National Union of Railwaymen (NUR), initiated libel proceedings against the magazine of the British Communist Party. Thomas claimed defamation in the form of cartoons and words depicting the events of "Black Friday"—when he allegedly betrayed the locked-out Miners' Federation. To Thomas, the framing of his image by the far left threatened to grievously degrade his character In the popular imagination. Soviet inspired Communism was a new element in European politics, and cartoonists unrestrained by tradition tested the boundaries of libel law. Thomas won his lawsuit, and restore his reputation.
Scientific cartoons.
Also in the world of science, mathematics and technology cartoons have found their place. One well-known cartoonist in the USA is Sidney Harris. Many of Gary Larson's cartoons had a scientific flavor.
Books.
Books with cartoons are usually reprints of newspaper cartoons. On some occasions, new gag cartoons have been created for book publication, as was the case with "Think Small", a 1967 promotional book distributed as a giveaway by Volkswagen dealers. Bill Hoest and other cartoonists of that decade drew cartoons showing Volkswagens, and these were published along with humorous automotive essays by such humorists as H. Allen Smith, Roger Price and Jean Shepherd. The book's design juxtaposed each cartoon alongside a photograph of the cartoon's creator.
Animation.
Because of the stylistic similarities between comic strips and early animated movies, "cartoon" came to refer to animation, and the word "cartoon" is currently used to refer to both animated cartoons and gag cartoons. While "animation" designates any style of illustrated images seen in rapid succession to give the impression of movement, the word "cartoon" is most often used in reference to TV programs and short films for children featuring anthropomorphized animals, superheroes, the adventures of child protagonists and related genres.
At the end of the 1980s, the word "cartoon" was shortened, and the word "toon" came into usage with the live action/animated feature "Who Framed Roger Rabbit" (1988), followed two years later by the TV series "Tiny Toon Adventures" (1990).

</doc>
<doc id="7167" url="http://en.wikipedia.org/wiki?curid=7167" title="Chief Minister of the Northern Territory">
Chief Minister of the Northern Territory

The Chief Minister of the Northern Territory is the head of government of the Northern Territory. He is the equivalent of a Premier in a state. 
He is formally appointed by the Administrator, who in normal circumstances will appoint the head of whatever party holds the majority of seats in the Legislative Assembly of the Northern Territory. In times of constitutional crisis, the Administrator can appoint someone else as Chief Minister. However, this has never occurred.
The incumbent Chief Minister of the Northern Territory is Adam Giles, representing the Country Liberal Party.

</doc>
<doc id="7172" url="http://en.wikipedia.org/wiki?curid=7172" title="Chemotherapy">
Chemotherapy

Chemotherapy (often abbreviated to chemo and sometimes CTX or CTx) is a category of cancer treatment that uses chemical substances, especially one or more anti-cancer drugs (chemotherapeutic agents) that are given as part of a standardized chemotherapy regimen. Chemotherapy may be given with a curative intent, or it may aim to prolong life or to reduce symptoms. Along with hormonal therapy and targeted therapy, it is one of the major categories of medical oncology (pharmacotherapy for cancer). These modalities are often used in conjunction with other cancer treatments, such as radiation therapy, surgery, and/or hyperthermia therapy. Some chemotherapy drugs are also used to treat other conditions, including AL amyloidosis, ankylosing spondylitis, multiple sclerosis, Crohn's disease, psoriasis, psoriatic arthritis, systemic lupus erythematosus, rheumatoid arthritis, and scleroderma.
Traditional chemotherapeutic agents act by killing cells that divide rapidly, one of the main properties of most cancer cells. This means that chemotherapy also harms cells that divide rapidly under normal circumstances: cells in the bone marrow, digestive tract, and hair follicles. This results in the most common side-effects of chemotherapy: myelosuppression (decreased production of blood cells, hence also immunosuppression), mucositis (inflammation of the lining of the digestive tract), and alopecia (hair loss).
Some newer anticancer drugs (for example, various monoclonal antibodies) are not indiscriminately cytotoxic, but rather target proteins that are abnormally expressed in cancer cells and that are essential for their growth. Such treatments are often referred to as targeted therapy (as distinct from classic chemotherapy) and are often used alongside traditional chemotherapeutic agents in antineoplastic treatment regimens.
Chemotherapy may use one drug at a time (single-agent chemotherapy) or several drugs at once (combination chemotherapy or polychemotherapy). The combination of chemotherapy and radiotherapy is chemoradiotherapy. Chemotherapy using drugs that convert to cytotoxic activity only upon light exposure is called photochemotherapy or photodynamic therapy.
History.
The first use of drugs to treat cancer was in the early 20th century, although it was not originally intended for that purpose. Mustard gas was used as a chemical warfare agent during World War I and was discovered to be a potent suppressor of hematopoiesis (blood production). A similar family of compounds known as nitrogen mustards were studied further during World War II at Yale University. It was reasoned that an agent that damaged the rapidly growing white blood cells might have a similar effect on cancer. Therefore, in December 1942, several patients with advanced lymphomas (cancers of the lymphatic system and lymph nodes) were given the drug by vein, rather than by breathing the irritating gas. Their improvement, although temporary, was remarkable. Concurrently, during a military operation in World War II, following a German air raid on the Italian harbour of Bari, several hundred people were accidentally exposed to mustard gas, which had been transported there by the Allied forces to prepare for possible retaliation in the event of German use of chemical warfare. The survivors were later found to have very low white blood cell counts. After WWII was over and the reports declassified, the experiences converged and led researchers to look for other substances that might have similar effects against cancer. The first chemotherapy drug to be developed from this line of research was mustine. Since then, many other drugs have been developed to treat cancer, and drug development has exploded into a multibillion-dollar industry, although the principles and limitations of chemotherapy discovered by the early researchers still apply.
The term "chemotherapy".
The word "chemotherapy" without a modifier usually refers to cancer treatment, but its historical meaning was broader. The term was coined in the early 1900s by Paul Ehrlich as meaning any use of chemicals to treat any disease ("chemo-" + "-therapy"), such as the use of antibiotics ("antibacterial chemotherapy"). Ehrlich was not optimistic that effective chemotherapy drugs would be found for the treatment of cancer. The first modern chemotherapeutic agent was arsphenamine, an arsenic compound discovered in 1909 and used to treat syphilis. This was later followed by sulfonamides (sulfa drugs) and penicillin. In today's usage, the sense "any treatment of disease with drugs" is often expressed with the word "pharmacotherapy".
General mode of action in cancer.
Cancer is the uncontrolled growth of cells coupled with malignant behaviour: invasion and metastasis (among other features). It is caused by the interaction between genetic susceptibility and environmental factors. These factors lead to accumulations of genetic mutations in oncogenes (genes that promote cancer) and tumor suppressor genes (genes that help to prevent cancer), which gives cancer cells their malignant characteristics, such as uncontrolled growth.
In the broad sense, most chemotherapeutic drugs work by impairing mitosis (cell division), effectively targeting fast-dividing cells. As these drugs cause damage to cells, they are termed "cytotoxic". They prevent mitosis by various mechanisms including damaging DNA and inhibition of the cellular machinery involved in cell division. One theory as to why these drugs kill cancer cells is that they induce a programmed form of cell death known as apoptosis.
As chemotherapy affects cell division, tumors with high growth rates (such as acute myelogenous leukemia and the aggressive lymphomas, including Hodgkin's disease) are more sensitive to chemotherapy, as a larger proportion of the targeted cells are undergoing cell division at any time. Malignancies with slower growth rates, such as indolent lymphomas, tend to respond to chemotherapy much more modestly. Heterogeneic tumours may also display varying sensitivities to chemotherapy agents, depending on the subclonal populations within the tumor.
Types.
Alkylating agents.
Alkylating agents are the oldest group of chemotherapeutics in use today. Originally derived from mustard gas used in World War I, there are now many types of alkylating agents in use. They are so named because of their ability to alkylate many molecules, including proteins, RNA and DNA. This ability to bind covalently to DNA via their alkyl group is the primary cause for their anti-cancer effects. DNA is made of two strands and the molecules may either bind twice to one strand of DNA (intrastrand crosslink) or may bind once to both strands (interstrand crosslink). If the cell tries to replicate crosslinked DNA during cell division, or tries to repair it, the DNA strands can break. This leads to a form of programmed cell death called apoptosis. Alkylating agents will work at any point in the cell cycle and thus are known as cell cycle-independent drugs. For this reason the effect on the cell is dose dependent; the fraction of cells that die is directly proportional to the dose of drug.
The subtypes of alkylating agents are the nitrogen mustards, nitrosoureas, tetrazines, aziridines, cisplatins and derivatives, and non-classical alkylating agents. Nitrogen mustards include mechlorethamine, cyclophosphamide, melphalan, chlorambucil, ifosfamide and busulfan. Nitrosoureas include N-Nitroso-N-methylurea (MNU), carmustine (BCNU), lomustine (CCNU) and semustine (MeCCNU), fotemustine and streptozotocin. Tetrazines include dacarbazine, mitozolomide and temozolomide. Aziridines include thiotepa, mytomycin and diaziquone (AZQ). Cisplatin and derivatives include cisplatin, carboplatin and oxaliplatin. They impair cell function by forming covalent bonds with the amino, carboxyl, sulfhydryl, and phosphate groups in biologically important molecules. Non-classical alkylating agents include procarbazine and hexamethylmelamine.
Anti-metabolites.
Anti-metabolites are a group of molecules that impede DNA and RNA synthesis. Many of them have a similar structure to the building blocks of DNA and RNA. The building blocks are nucleotides; a molecule comprising a nucleobase, a sugar and a phosphate group. The nucleobases are divided into purines (guanine and adenine) and pyrimidines (cytosine, thymine and uracil). Anti-metabolites resemble either nucleobases or nucleosides (a nucleotide without the phosphate group), but have altered chemical groups. These drugs exert their effect by either blocking the enzymes required for DNA synthesis or becoming incorporated into DNA or RNA. By inhibiting the enzymes involved in DNA synthesis, they prevent mitosis because the DNA cannot duplicate itself. Also, after misincorperation of the molecules into DNA, DNA damage can occur and programmed cell death (apoptosis) is induced. Unlike alkylating agents, anti-metabolites are cell cycle dependent. This means that they only work during a specific part of the cell cycle, in this case S-phase (the DNA synthesis phase). For this reason, at a certain dose, the effect plateaus and proportionally no more cell death occurs with increased doses. Subtypes of the anti-metabolites are the anti-folates, fluoropyrimidines, deoxynucleoside analogues and thiopurines.
The anti-folates include methotrexate and pemetrexed. Methotrexate inhibits dihydrofolate reductase (DHFR), an enzyme that regenerates tetrahydrofolate from dihydrofolate. When the enzyme is inhibited by methotrexate, the cellular levels of folate coenzymes diminish. These are required for thymidylate and purine production, which are both essential for DNA synthesis and cell division. Pemetrexed is another anti-metabolite that affects purine and pyrimidine production, and therefore also inhibits DNA synthesis. It primarily inhibits the enzyme thymidylate synthase, but also has effects on DHFR, aminoimidazole carboxamide ribonucleotide formyltransferase and glycinamide ribonucleotide formyltransferase. The fluoropyrimidines include fluorouracil and capecitabine. Fluorouracil is a nucleobase analogue that is metabolised in cells to form at least two active products; 5-fluourouridine monophosphate (FUMP) and 5-fluoro-2'-deoxyuridine 5'-phosphate (fdUMP). FUMP becomes incorporated into RNA and fdUMP inhibits the enzyme thymidylate synthase; both of which lead to cell death. Capecitabine is a prodrug of 5-fluorouracil that is broken down in cells to produce the active drug. The deoxynucleoside analogues include cytarabine, gemcitabine, decitabine, Vidaza, fludarabine, nelarabine, cladribine, clofarabine and pentostatin. The thiopurines include thioguanine and mercaptopurine.
Anti-microtubule agents.
Anti-microtubule agents are plant-derived chemicals that block cell division by preventing microtubule function. Microtubules are an important cellular structure composed of two proteins; α-tubulin and β-tubulin. They are hollow rod shaped structures that are required for cell division, among other cellular functions. Microtubules are dynamic structures, which means that they are permanently in a state of assembly and disassembly. Vinca alkaloids and taxanes are the two main groups of anti-microtubule agents, and although both of these groups of drugs cause microtubule disfunction, their mechanisms of action are completely opposite. The vinca alkaloids prevent the formation of the microtubules, whereas the taxanes prevent the microtubule disassembly. By doing so, they prevent the cancer cells from completing mitosis. Following this, cell cycle arrest occurs, which induces programmed cell death (apoptosis). Also, these drugs can affect blood vessel growth; an essential process that tumours utilise in order to grow and metastasise.
Vinca alkaloids are derived from the Madagascar periwinkle, "Catharanthus roseus" (formerly known as "Vinca rosea"). They bind to specific sites on tubulin, inhibiting the assembly of tubulin into microtubules. The original vinca alkaloids are completely natural chemicals that include vincristine and vinblastine. Following the success of these drugs, semi-synthetic vinca alkaloids were produced: vinorelbine, vindesine, and vinflunine. These drugs are cell cycle-specific. They bind to the tubulin molecules in S-phase and prevent proper microtubule formation required for M-phase.
Taxanes are natural and semi-synthetic drugs. The first drug of their class, paclitaxel, was originally extracted from the Pacific Yew tree, "Taxus brevifolia". Now this drug and another in this class, docetaxel, are produced semi-synthetically from a chemical found in the bark of another Yew tree; "Taxus baccata". These drugs promote microtubule stability, preventing their disassembly. Paclitaxel prevents the cell cycle at the boundary of G2-M, whereas docetaxel exerts its effect during S-phase. Taxanes present difficulties in formulation as medicines because they are poorly soluble in water.
Podophyllotoxin is an anti-neoplastic lignan obtained primarily from the American Mayapple ("Podophyllum peltatum") and Himalayan Mayapple ("Podophyllum hexandrum" or "Podophyllum emodi"). It has anti-microtubule activity, and its mechanism is similar to that of vinca alkaloids in that they bind to tubulin, inhibiting microtubule formation. Podophyllotoxin is used to produce two other drugs with different mechanisms of action: etoposide and teniposide.
Topoisomerase inhibitors.
Topoisomerase inhibitors are drugs that affect the activity of two enzymes: topoisomerase I and topoisomerase II. When the DNA double-strand helix is unwound, during DNA replication or transcription, for example, the adjacent unopened DNA winds tighter (supercoils), like opening the middle of a twisted rope. The stress caused by this effect is in part aided by the topoisomerase enzymes. They produce single- or double-strand breaks into DNA, reducing the tension in the DNA strand. This allows the normal unwinding of DNA to occur during replication or transcription. Inhibition of topoisomerase I or II interferes with both of these processes.
Two topoisomerase I inhibitors, irinotecan and topotecan, are semi-synthetically derived from camptothecin, which is obtained from the Chinese ornamental tree "Camptotheca acuminata". Drugs that target topoisomerase II can be divided into two groups. The topoisomerase II poisons cause increased levels enzymes bound to DNA. This prevents DNA replication and transcription, causes DNA strand breaks, and leads to programmed cell death (apoptosis). These agents include etoposide, doxorubicin, mitoxantrone and teniposide. The second group, catalytic inhibitors, are drugs that block the activity of topoisomerase II, and therefore prevent DNA synthesis and translation because the DNA cannot unwind properly. This group includes novobiocin, merbarone, and aclarubicin, which also have other significant mechanisms of action.
Cytotoxic antibiotics.
The cytotoxic antibiotics are a varied group of drugs that have various mechanisms of action. The group includes the anthracyclines and other drugs including actinomycin, bleomycin, plicamycin, and mitomycin. Doxorubicin and daunorubicin were the first two anthracyclines, and were obtained from the bacterium "Streptomyces peucetius". Derivatives of these compounds include epirubicin and idarubicin. Other clinically used drugs in the anthracyline group are pirarubicin, aclarubicin, and mitoxantrone. The mechanisms of anthracyclines include DNA intercalation (molecules insert between the two strands of DNA), generation of highly reactive free radicals that damage intercellular molecules and topoisomerase inhibition. Actinomycin is a complex molecule that intercalates DNA and prevents RNA synthesis. Bleomycin, a glycopeptide isolated from "Streptomyces verticillus", also intercalates DNA, but produces free radicals that damage DNA. This occurs when bleomycin binds to a metal ion, becomes chemically reduced and reacts with oxygen. Mitomycin is a cytotoxic antibiotic with the ability to alkylate DNA.
Treatment strategies.
There are a number of strategies in the administration of chemotherapeutic drugs used today. Chemotherapy may be given with a curative intent or it may aim to prolong life or to palliate symptoms.
All chemotherapy regimens require that the patient be capable of undergoing the treatment. Performance status is often used as a measure to determine whether a patient can receive chemotherapy, or whether dose reduction is required. Because only a fraction of the cells in a tumor die with each treatment (fractional kill), repeated doses must be administered to continue to reduce the size of the tumor. Current chemotherapy regimens apply drug treatment in cycles, with the frequency and duration of treatments limited by toxicity to the patient.
Dosage.
Dosage of chemotherapy can be difficult: If the dose is too low, it will be ineffective against the tumor, whereas, at excessive doses, the toxicity (side-effects) will be intolerable to the patient. The standard method of determining chemotherapy dosage is based on calculated body surface area (BSA). The BSA is usually calculated with a mathematical formula or a nomogram, using a patient's weight and height, rather than by direct measurement of body mass. This formula was originally derived in a 1916 study and attempted to translate medicinal doses established with laboratory animals to equivalent doses for humans. The study only included 9 human subjects. When chemotherapy was introduced in the 1950s, the BSA formula was adopted as the official standard for chemotherapy dosing for lack of a better option.
Recently, the validity of this method in calculating uniform doses has been questioned. The reason for this is that the formula only takes into account the individual's weight and height. Drug absorption and clearance are influenced by multiple factors, including age, gender, metabolism, disease state, organ function, drug-to-drug interactions, genetics, and obesity, which has a major impact on the actual concentration of the drug in the patient's bloodstream. As a result, there is high variability in the systemic chemotherapy drug concentration among patients dosed by BSA, and this variability has been demonstrated to be more than 10-fold for many drugs. In other words, if two patients receive the same dose of a given drug based on BSA, the concentration of that drug in the bloodstream of one patient may be 10 times higher or lower compared to that of the other patient. This variability is typical with many chemotherapy drugs dosed by BSA, and, as shown below, was demonstrated in a study of 14 common chemotherapy drugs.
The result of this pharmacokinetic variability among patients is that many patients do not receive the right dose to achieve optimal treatment effectiveness with minimized toxic side effects. Some patients are overdosed while others are underdosed. For example, in a randomized clinical trial, investigators found 85% of metastatic colorectal cancer patients treated with 5-fluorouracil (5-FU) did not receive the optimal therapeutic dose when dosed by the BSA standard—68% were underdosed and 17% were overdosed.
There has been recent controversy over the use of BSA to calculate chemotherapy doses for obese patients. Because of their higher BSA, clinicians often arbitrarily reduce the dose prescribed by the BSA formula for fear of overdosing. In many cases, this can result in sub-optimal treatment.
Several clinical studies have demonstrated that when chemotherapy dosing is individualized to achieve optimal systemic drug exposure, treatment outcomes are improved and toxic side effects are reduced. In the 5-FU clinical study cited above, patients whose dose was adjusted to achieve a pre-determined target exposure realized an 84% improvement in treatment response rate and a six month improvement in overall survival (OS) compared with those dosed by BSA.
In the same study, investigators compared the incidence of common 5-FU-associated grade 3/4 toxicities between the dose-adjusted patients and the BSA-dosed patients. The incidence of debilitating grades of diarrhea was reduced from 18% in the BSA-dosed group to 4% in the dose-adjusted group of patients and serious hematologic side effects were eliminated. Because of the reduced toxicity, dose-adjusted patients were able to be treated for longer periods of time. BSA-dosed patients were treated for a total of 680 months while dose-adjusted patients were treated for a total of 791 months. Completing the course of treatment is an important factor in achieving better treatment outcomes.
Similar results were found in a study involving colorectal cancer patients treated with the popular FOLFOX regimen. The incidence of serious diarrhea was reduced from 12% in the BSA-dosed group of patients to 1.7% in the dose-adjusted group, and the incidence of severe mucositis was reduced from 15% to 0.8%.
The FOLFOX study also demonstrated an improvement in treatment outcomes. Positive response increased from 46% in the BSA-dosed patients to 70% in the dose-adjusted group. Median progression free survival (PFS) and overall survival (OS) both improved by six months in the dose adjusted group.
One approach that can help clinicians individualize chemotherapy dosing is to measure the drug levels in blood plasma over time and adjust dose according to a formula or algorithm to achieve optimal exposure. With an established target exposure for optimized treatment effectiveness with minimized toxicities, dosing can be personalized to achieve target exposure and optimal results for each patient. Such an algorithm was used in the clinical trials cited above and resulted in significantly improved treatment outcomes.
Oncologists are already individualizing dosing of some cancer drugs based on exposure. Carboplatin and busulfan dosing rely upon results from blood tests to calculate the optimal dose for each patient. Simple blood tests are also available for dose optimization of methotrexate, 5-FU, paclitaxel, and docetaxel.
Delivery.
Most chemotherapy is delivered intravenously, although a number of agents can be administered orally (e.g., melphalan, busulfan, capecitabine).
There are many intravenous methods of drug delivery, known as vascular access devices. These include the winged infusion device, peripheral cannula, midline catheter, peripherally inserted central catheter (PICC), central venous catheter and implantable port. The devices have different applications regarding duration of chemotherapy treatment, method of delivery and types of chemotherapeutic agent.
Depending on the patient, the cancer, the stage of cancer, the type of chemotherapy, and the dosage, intravenous chemotherapy may be given on either an inpatient or an outpatient basis. For continuous, frequent or prolonged intravenous chemotherapy administration, various systems may be surgically inserted into the vasculature to maintain access. Commonly used systems are the Hickman line, the Port-a-Cath, and the PICC line. These have a lower infection risk, are much less prone to phlebitis or extravasation, and eliminate the need for repeated insertion of peripheral cannulae.
Isolated limb perfusion (often used in melanoma), or isolated infusion of chemotherapy into the liver or the lung have been used to treat some tumors. The main purpose of these approaches is to deliver a very high dose of chemotherapy to tumor sites without causing overwhelming systemic damage. These approaches can help control solitary or limited metastases, but they are by definition not systemic, and, therefore, do not treat distributed metastases or micrometastases.
Topical chemotherapies, such as 5-fluorouracil, are used to treat some cases of non-melanoma skin cancer.
If the cancer has central nervous system involvement, or with meningeal disease, intrathecal chemotherapy may be administered.
Adverse effects.
Chemotherapeutic techniques have a range of side-effects that depend on the type of medications used. The most common medications affect mainly the fast-dividing cells of the body, such as blood cells and the cells lining the mouth, stomach, and intestines. Chemotherapy related toxicities can occur acutely after administration, within hours or days, or chronically, from weeks to years.
Immunosuppression and myelosuppression.
Virtually all chemotherapeutic regimens can cause depression of the immune system, often by paralysing the bone marrow and leading to a decrease of white blood cells, red blood cells, and platelets.
Anemia and thrombocytopenia, when they occur, are improved with blood transfusion. Neutropenia (a decrease of the neutrophil granulocyte count below 0.5 x 109/litre) can be improved with synthetic G-CSF (granulocyte-colony-stimulating factor, e.g., filgrastim, lenograstim).
In very severe myelosuppression, which occurs in some regimens, almost all the bone marrow stem cells (cells that produce white and red blood cells) are destroyed, meaning "allogenic" or "autologous" bone marrow cell transplants are necessary. (In autologous BMTs, cells are removed from the patient before the treatment, multiplied and then re-injected afterward; in "allogenic" BMTs, the source is a donor.) However, some patients still develop diseases because of this interference with bone marrow.
Although patients are encouraged to wash their hands, avoid sick people, and take other infection-reducing steps, about 85% of infections are due to naturally occurring microorganisms in the patient's own gastrointestinal tract (including oral cavity) and skin. This may manifest as systemic infections, such as sepsis, or as localized outbreaks, such as Herpes simplex, shingles, or other members of the Herpesviridea. Sometimes, chemotherapy treatments are postponed because the immune system is suppressed to a critically low level.
In Japan, the government has approved the use of some medicinal mushrooms like "Trametes versicolor", to counteract depression of the immune system in patients undergoing chemotherapy.
Typhlitis.
Due to immune system suppression, typhlitis is a "life-threatening gastrointestinal complication of chemotherapy." Typhlitis is an intestinal infection which may manifest itself through symptoms including nausea, vomiting, diarrhea, a distended abdomen, fever, chills, or abdominal pain and tenderness.
Typhlitis is a medical emergency. It has a very poor prognosis and is often fatal unless promptly recognized and aggressively treated. Successful treatment hinges on early diagnosis provided by a high index of suspicion and the use of CT scanning, nonoperative treatment for uncomplicated cases, and sometimes elective right hemicolectomy to prevent recurrence.
Gastrointestinal distress.
Nausea, vomiting, anorexia, diarrhoea, abdominal cramps, and constipation are common side-effects of chemotherapeutic medications that kill fast-dividing cells. Malnutrition and dehydration can result when the patient does not eat or drink enough, or when the patient vomits frequently, because of gastrointestinal damage. This can result in rapid weight loss, or occasionally in weight gain, if the patient eats too much in an effort to allay nausea or heartburn. Weight gain can also be caused by some steroid medications. These side-effects can frequently be reduced or eliminated with antiemetic drugs. Self-care measures, such as eating frequent small meals and drinking clear liquids or ginger tea, are often recommended. In general, this is a temporary effect, and frequently resolves within a week of finishing treatment. However, a high index of suspicion is appropriate, since diarrhea and bloating are also symptoms of typhlitis, a very serious and potentially life-threatening medical emergency that requires immediate treatment.
Anemia.
Anemia in cancer patients can be a combined outcome caused by myelosuppressive chemotherapy, and possible cancer-related causes such as bleeding, blood cell destruction (hemolysis), hereditary disease, kidney disfunction, nutritional 
deficiencies and/or anemia of chronic disease. Treatments to mitigate anemia include hormones to boost blood production (erythropoietin), iron supplements, and blood transfusions. Myelosuppressive therapy can cause a tendency to bleed easily, leading to anemia. Medications that kill rapidly dividing cells or blood cells can reduce the number of platelets in the blood, which can result in bruises and bleeding. Extremely low platelet counts may be temporarily boosted through platelet transfusions and new drugs to increase platelet counts during chemotherapy are being developed. Sometimes, chemotherapy treatments are postponed to allow platelet counts to recover.
Fatigue.
Fatigue may be a consequence of the cancer or its treatment, and can last for months to years after treatment. One physiological cause of fatigue is anemia, which can be caused by chemotherapy, surgery, radiotherapy, primary and metastatic disease and/or nutritional depletion. Anaerobic exercise has been found to be beneficial in reducing fatigue in people with solid tumours.
Nausea and vomiting.
Nausea and vomiting are two of the most feared cancer treatment-related side-effects for cancer patients and their families. In 1983, Coates et al. found that patients receiving chemotherapy ranked nausea and vomiting as the first and second most severe side-effects, respectively. Up to 20% of patients receiving highly emetogenic agents in this era postponed, or even refused, potentially curative treatments. Chemotherapy-induced nausea and vomiting (CINV) are common with many treatments and some forms of cancer. Since the 1990s, several novel classes of antiemetics have been developed and commercialized, becoming a nearly universal standard in chemotherapy regimens, and helping to successfully manage these symptoms in a large portion of patients. Effective mediation of these unpleasant and sometimes-crippling symptoms results in increased quality of life for the patient and more efficient treatment cycles, due to less stoppage of treatment due to better tolerance by the patient, and due to better overall health of the patient.
Hair loss.
Hair loss (Alopecia) can be caused by chemotherapy that kills rapidly dividing cells; other medications may cause hair to thin. These are most often temporary effects: hair usually starts to regrow a few weeks after the last treatment, and sometimes can change colour, texture, thickness and style. Sometimes hair has a tendency to curl after regrowth, resulting in "chemo curls." Severe hair loss occurs most often with drugs such as doxorubicin, daunorubicin, paclitaxel, docetaxel, cyclophosphamide, ifosfamide and etoposide. Permanent thinning or hair loss can result from some standard chemotherapy regimens.
Chemotherapy induced hair loss occurs by a non-androgenic mechanism, and can manifests as alopecia totalis, telogen effluvium, or less often alopecia areata. It is usually associated with systemic treatment due to the high mitotic rate of hair follicles, and more reversible than androgenic hair loss, although permanent cases can occur. Chemotherapy induces hair loss in women more often than men.
Scalp cooling offers a means of preventing both permanent and temporary hair loss, however concerns for this method have been raised.
Secondary neoplasm.
Development of secondary neoplasia after successful chemotherapy and/or radiotherapy treatment can occur. The most common secondary neoplasm is secondary acute myeloid leukemia, which develops primarily after treatment with alkylating agents or topoisomerase inhibitors. Survivors of childhood cancer are more than 13 times as likely to get a secondary neoplasm during the 30 years after treatment than the general population. Not all of this increase can be attributed to chemotherapy.
Infertility.
Some types of chemotherapy are gonadotoxic and may cause infertility. Chemotherapies with high risk include procarbazine and other alkylating drugs such as cyclophosphamide, ifosfamide, busulfan, melphalan, chlorambucil, and chlormethine. Drugs with medium risk include doxorubicin and platinum analogs such as cisplatin and carboplatin. On the other hand, therapies with low risk of gonadotoxicity include plant derivatives such as vincristine and vinblastine, antibiotics such as bleomycin and dactinomycin, and antimetabolites such as methotrexate, mercaptopurine, and 5-fluorouracil.
Female infertility by chemotherapy appears to be secondary to premature ovarian failure by loss of primordial follicles. This loss is not necessarily a direct effect of the chemotherapeutic agents, but could be due to an increased rate of growth initiation to replace damaged developing follicles.
Patients may choose between several methods of fertility preservation prior to chemotherapy, including cryopreservation of semen, ovarian tissue, oocytes, or embryos. As more than half of cancer patients are elderly, this adverse effect is only relevant for a minority of patients. A study in France between 1999 and 2011 came to the result that embryo freezing before administration of gonadotoxic agents to females caused a delay of treatment in 34% of cases, and a live birth in 27% of surviving cases who wanted to become pregnant, with the follow-up time varying between 1 and 13 years.
Potential protective or attenuating agents include GnRH analogs, where several studies have shown a protective effect "in vivo" in humans, but some studies show no such effect. Sphingosine-1-phosphate (S1P) has shown similar effect, but its mechanism of inhibiting the sphingomyelin apoptotic pathway may also interfere with the apoptosis action of chemotherapy drugs.
In chemotherapy as a conditioning regimen in hematopoietic stem cell transplantation, a study of patients conditioned with cyclophosphamide alone for severe aplastic anemia came to the result that ovarian recovery occurred in all women younger than 26 years at time of transplantation, but only in five of 16 women older than 26 years.
Teratogenicity.
Chemotherapy is potentially teratogenic during pregnancy, especially during the first trimester, to the extent that abortion usually is recommended if pregnancy in this period is found during chemotherapy. Second- and third-trimester exposure does not usually increase the teratogenic risk and adverse effects on cognitive development, but it may increase the risk of various complications of pregnancy and fetal myelosuppression.
In males previously having undergone chemotherapy or radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy. The use of assisted reproductive technologies and micromanipulation techniques might increase this risk. In females previously having undergone chemotherapy, miscarriage and congenital malformations are not increased in subsequent conceptions. However, when in vitro fertilization and embryo cryopreservationis practised between or shortly after treatment, possible genetic risks to the growing oocytes exist, and hence it has been recommended that the babies be screened.
Peripheral neuropathy.
Between 30 and 40 percent of patients undergoing chemotherapy experience chemotherapy-induced peripheral neuropathy (CIPN), a progressive, enduring, and often irreversible condition, causing pain, tingling, numbness and sensitivity to cold, beginning in the hands and feet and sometimes progressing to the arms and legs. Chemotherapy drugs associated with CIPN include thalidomide, epothilones, vinca alkaloids, taxanes, proteasome inhibitors, and the platinum-based drugs. Whether CIPN arises, and to what degree, is determined by the choice of drug, duration of use, the total amount consumed and whether the patient already has peripheral neuropathy. Though the symptoms are mainly sensory, in some cases motor nerves and the autonomic nervous system are affected. CIPN often follows the first chemotherapy dose and increases in severity as treatment continues, but this progression usually levels off at completion of treatment. The platinum-based drugs are the exception; with these drugs, sensation may continue to deteriorate for several months after the end of treatment. Some CIPN appears to be irreversible. Pain can often be managed with drug or other treatment but the numbness is usually resistant to treatment.
Cognitive impairment.
Some patients report fatigue or non-specific neurocognitive problems, such as an inability to concentrate; this is sometimes called post-chemotherapy cognitive impairment, referred to as "chemo brain" by patients' groups.
Tumor lysis syndrome.
In particularly large tumors and cancers with high white cell counts, such as lymphomas, teratomas, and some leukemias, some patients develop tumor lysis syndrome. The rapid breakdown of cancer cells causes the release of chemicals from the inside of the cells. Following this, high levels of uric acid, potassium and phosphate are found in the blood. High levels of phosphate induce secondary hypoparathyroidism, resulting in low levels of calcium in the blood. This causes kidney damage and the high levels of potassium can cause cardiac arrhythmia. Although prophylaxis is available and is often initiated in patients with large tumors, this is a dangerous side-effect that can lead to death if left untreated.
Organ damage.
Cardiotoxicity (heart damage) is especially prominent with the use of anthracycline drugs (doxorubicin, epirubicin, idarubicin, and liposomal doxorubicin). The cause of this is most likely due to the production of free radicals in the cell and subsequent DNA damage. Other chemotherapeutic agents that cause cardiotoxicity, but at a lower incidence, are cyclophosphamide, docetaxel and clofarabine.
Hepatotoxicity (liver damage) can be caused by many cytotoxic drugs. The susceptibility of an individual to liver damage can be altered by other factors such as the cancer itself, viral hepatitis, immunosuppression and nutritional deficiency. The liver damage can consist of damage to liver cells, hepatic sinusoidal syndrome (obstruction of the veins in the liver), cholestasis (where bile does not flow from the liver to the intestine) and liver fibrosis.
Nephrotoxicity (kidney damage) can be caused by tumor lysis syndrome and also due direct effects of drug clearance by the kidneys. Different drugs will affect different parts of the kidney and the toxicity may be asymptomatic (only seen on blood or urine tests) or may cause acute renal failure.
Ototoxicity (damage to the inner ear) is a common side effect of platinum based drugs that can produce symptoms such as dizziness and vertigo.
Other side-effects.
Less common side-effects include red skin (erythema), dry skin, damaged fingernails, a dry mouth (xerostomia), water retention, and sexual impotence. Some medications can trigger allergic or pseudoallergic reactions.
Specific chemotherapeutic agents are associated with organ-specific toxicities, including cardiovascular disease(e.g., doxorubicin), interstitial lung disease (e.g., bleomycin) and occasionally secondary neoplasm (e.g., MOPP therapy for Hodgkin's disease).
Limitations.
Chemotherapy does not always work, and even when it is useful, it may not completely destroy the cancer. Patients frequently fail to understand its limitations. In one study of patients who had been newly diagnosed with incurable, stage 4 cancer, more than two-thirds of patients with lung cancer and more than four-fifths of patients with colorectal cancer still believed that chemotherapy was likely to cure their cancer.
The blood brain barrier poses a difficult obstacle to pass to deliver chemotherapy to the brain. This is because the brain has an extensive system in place to protect it from harmful chemicals. Drug transporters can pump out drugs from the brain and brain's blood vessel cells into the cerebrospinal fluid and blood circulation. These transporters pump out most chemotherapy drugs, which reduces their efficacy for treatment of brain tumors. Only small lipophilic alkylating agents such as lomustine or temozolomide are able to cross this blood brain barrier.
Blood vessels in tumors are very different from those seen in normal tissues. As a tumor grows, tumor cells furthest away from the blood vessels become low in oxygen (hypoxic). To counteract this they then signal for new blood vessels to grow. The newly formed tumor vasculature is poorly formed and does not deliver an adequate blood supply to all areas of the tumor. This leads to issues with drug delivery because many drugs will be delivered to the tumor by the circulatory system.
Efficacy.
The efficacy of chemotherapy depends on the type of cancer and the stage. The overall effectiveness ranges from being curative for some cancers, such as some leukemias, to being ineffective, such as in some brain tumors, to being needless in others, like most non-melanoma skin cancers.
Even when it is impossible for chemotherapy to provide a permanent cure, chemotherapy may be useful to reduce symptoms like pain or to reduce the size of an inoperable tumor in the hope that surgery will be possible in the future.
Resistance.
Resistance is a major cause of treatment failure in chemotherapeutic drugs. There are a few possible causes of resistance in cancer, one of which is the presence of small pumps on the surface of cancer cells that actively move chemotherapy from inside the cell to the outside. Cancer cells produce high amounts of these pumps, known as p-glycoprotein, in order to protect themselves from chemotherapeutics. Research on p-glycoprotein and other such chemotherapy efflux pumps is currently ongoing. Medications to inhibit the function of p-glycoprotein are undergoing investigation, but due to toxicities and interactions with anti-cancer drugs their development has been difficult. Another mechanism of resistance is gene amplification, a process in which multiple copies of a gene are produced by cancer cells. This overcomes the effect of drugs that reduce the expression of genes involved in replication. With more copies of the gene, the drug can not prevent all expression of the gene and therefore the cell can restore its proliferative ability. Cancer cells can also cause defects in the cellular pathways of apoptosis (programmed cell death). As most chemotherapy drugs kill cancer cells in this manner, defective apoptosis allows survival of these cells, making them resistant. Many chemotherapy drugs also cause DNA damage, which can be repaired by enzymes in the cell that carry out DNA repair. Upregulation of these genes can overcome the DNA damage and prevent the induction of apoptosis. Mutations in genes that produce drug target proteins, such as tubulin, can occur which prevent the drugs from binding to the protein, leading to resistance to these types of drugs.
Cytotoxics and targeted therapies.
Targeted therapies are a relatively new class of cancer drugs that can overcome many of the issues seen with the use of cytotoxics. They are divided into two groups: small molecule and antibodies. The massive toxicity seen with the use of cytotoxics is due to the lack of cell specificity of the drugs. They will kill any rapidly dividing cell, tumor or normal. Targeted therapies are designed to affect cellular proteins or processes that are utilised by the cancer cells. This allows a high dose to cancer tissues with a relatively low dose to other tissues. As different proteins are utilised by different cancer types, the targeted therapy drugs are used on a cancer type specific, or even on a patient specific basis. Although the side effects are often less severe than that seen of cytotoxic chemotherapeutics, life-threatening effects can occur. Initially, the targeted therapeutics were supposed to be solely selective for one protein. Now it is clear that there is often a range of protein targets that the drug can bind. An example target for targeted therapy is the protein produced by the Philadelphia chromosome, a genetic lesion found commonly in chronic myelomonocytic leukemia. This fusion protein has enzyme activity that can be inhibited by imatinib, a small molecule drug.
Newer and experimental approaches.
Targeted therapies.
Specially targeted delivery vehicles aim to increase effective levels of chemotherapy for tumor cells while reducing effective levels for other cells. This should result in an increased tumor kill and/or reduced toxicity.
Antibody-drug conjugates.
Antibody-drug conjugates (ADCs) comprise an antibody, drug and a linker between them. The antibody will be targeted at a preferentially expressed protein in the tumour cells (known as a tumor antigen) or on cells that the tumor can utilise, such as blood vessel endothelial cells. They bind to the tumor antigen and are internalised, where the linker releases the drug into the cell. These specially targeted delivery vehicles vary in their stability, selectivity, and choice of target, but, in essence, they all aim to increase the maximum effective dose that can be delivered to the tumor cells. Reduced systemic toxicity means that they can also be used in sicker patients, and that they can carry new chemotherapeutic agents that would have been far too toxic to deliver via traditional systemic approaches.
The first approved drug of this type was gemtuzumab ozogamicin (Mylotarg), released by Wyeth (now Pfizer). The drug was approved to treat acute myeloid leukemia, but has now been withdrawn from the market because the drug did not meet efficacy targets in further clinical trials. Two other drugs, trastuzumab emtansine and brentuximab vedotin, are both in late clinical trials, and the latter has been granted accelerated approval for the treatment of refractory Hodgkins lymphoma and systemic anaplastic large cell lymphoma.
Nanoparticles.
Nanoparticles are 1-1000 nanometer (nm) sized particles that can promote tumor selectivity and aid in delivering low-solubility drugs. Nanoparticles can be targeted passively or actively. Passive targeting exploits the difference between tumor blood vessels and normal blood vessels. Blood vessels in tumors are "leaky" because they have gaps from 200-2000 nm, which allow nanoparticles to escape into the tumor. Active targeting uses biological molecules (antibodies, proteins, DNA and receptor ligands) to preferentially target the nanoparticles to the tumor cells. There are many types of nanoparticle delivery systems, such as silica, polymers, liposomes and magnetic particles. Nanoparticles made of magnetic material can also be used to concentrate agents at tumor sites using an externally applied magnetic field. They have emerged as a useful vehicle for poorly soluble agents such as paclitaxel.
Electrochemotherapy.
Electrochemotherapy is the combined treatment in which injection of a chemotherapeutic drug is followed by application of high-voltage electric pulses locally to the tumor. The treatment enables the chemotherapeutic drugs, which otherwise cannot or hardly go through the membrane of cells (such as bleomycin and cisplatin), to enter the cancer cells. Hence, greater effectiveness of antitumor treatment is achieved.
Clinical electrochemotherapy has been successfully used for treatment of cutaneous and subcutaneous tumors irrespective of their histological origin. The method has been reported as safe, simple and highly effective in all reports on clinical use of electrochemotherapy. According to the ESOPE project (European Standard Operating Procedures of Electrochemotherapy), the Standard Operating Procedures (SOP) for electrochemotherapy were prepared, based on the experience of the leading European cancer centres on electrochemotherapy. Recently, new electrochemotherapy modalities have been developed for treatment of internal tumors using surgical procedures, endoscopic routes or percutaneous approaches to gain access to the treatment area.
Hyperthermia therapy.
Hyperthermia therapy is heat treatment for cancer that can be a powerful tool when used in combination with chemotherapy (thermochemotherapy) or radiation for the control of a variety of cancers. The heat can be applied locally to the tumor site, which will dilate blood vessels to the tumor, allowing more chemotherapeutic medication to enter the tumor. Additionally, the bi-lipid layer of the tumor cell membrane will become more porous, further allowing more of the chemotherapeutic medicine to enter the tumor cell.
Hyperthermia has also been shown to help prevent or reverse "chemo-resistance." Chemotherapy resistance sometimes develops overtime as the tumors adapt and can overcome the toxicity of the chemo medication. “Overcoming chemoresistance has been extensively studied within the past, especially using CDDP-resistant cells. In regard to the potential benefit that drug-resistant cells can be recruited for effective therapy by combining chemotherapy with hyperthermia, it was important to show that chemoresistance against several anticancer drugs (e.g. mitomycin C, anthracyclines, BCNU, melphalan) including CDDP could be reversed at least partially by the addition of heat.
Other uses.
Some chemotherapy drugs are used in diseases other than cancer, such as in autoimmune disorders, and noncancerous plasma cell dyscrasia. In some cases they are often used at lower doses, which means that the side effects are minimized. Methotrexate is used in the treatment of rheumatoid arthritis (RA), psoriasis, ankylosing spondylitis and multiple sclerosis. The anti-inflammatory response seen in RA is thought to be due to increases in adenosine, which causes immunosuppression; effects on immuno-regulatory cyclooxygenase-2 enzyme pathways; reduction in pro-inflammatory cytokines; and anti-proliferative properties. Although methotrexate is used to treat both multiple sclerosis and ankylosing spondylitis, its efficacy in these diseases is still uncertain. Cyclophosphamide is sometimes used to treat lupus nephritis, a common symptom of systemic lupus erythematosus. Dexamethasone along with either bortezomib or melphalan is commonly used as a treatment for AL amyloidosis. Recently, bortezomid in combination with cyclophosphamide and dexamethasone has also shown promise as a treatment for AL amyloidosis. Other drugs used to treat myeloma such as lenalidomide have shown promise in treating AL amyloidosis.
Chemotherapy drugs are also used in conditioning regimens prior to bone marow transplant (hematopoietic stem cell transplant). Conditioning regimens are used to suppress the recipient's immune system in order to allow a transplant to engraft. Cyclophosphamide is a common cytotoxic drug used in this manner, and is often used in conjunction with total body irradiation. Chemotherapeutic drugs may be used at high doses to permanently remove the recipient's bone marrow cells (myeloablative conditioning) or at lower doses that will prevent permanent bone marrow loss (non-myeloablative and reduced intensity conditioning).
Occupational precautions.
Healthcare workers exposed to anti-neoplastic agents take precautions to keep their exposure to a minimum. There is a limitation in cytotoxics dissolution in Australia and the United States to 20 dissolutions per pharmacist/nurse, since pharmacists who prepare these drugs or nurses who may prepare or administer them are the two occupational groups with the highest potential exposure to antineoplastic agents. In addition, physicians and operating room personnel may also be exposed through the treatment of patients. Hospital staff, such as shipping and receiving personnel, custodial workers, laundry workers, and waste handlers, all have potential exposure to these drugs during the course of their work. The increased use of antineoplastic agents in veterinary oncology also puts these workers at risk for exposure to these drugs. Routes of entry into the users body are skin absorption, inhalation and ingestion. The long term effects of exposure include chromosomal abnormalities and infertility.
In other animals.
Chemotherapy is used in veterinary medicine similar to how it is used in human medicine.
Sales.
List of the top 10 best-selling cancer drugs of 2013:

</doc>
<doc id="7174" url="http://en.wikipedia.org/wiki?curid=7174" title="Chinese historiography">
Chinese historiography

Chinese historiography refers to the study of methods and assumptions made in studying Chinese history.
History of Chinese historians.
Recording of Chinese history dates back to the Shang Dynasty, although the oldest surviving histories—those compiled in the "Classic of History"—seem to date back only to the rise of Zhou at the earliest. The "Spring and Autumn Annals", the official chronicle of the State of Lu covering the period from 722 to 481 BC, is among the earliest surviving Chinese historical texts to be arranged as an annal. The compilation of both is traditionally ascribed to Confucius. The "Zuo Zhuan", attributed to Zuo Qiuming in the 5th century BC, is the earliest Chinese work of narrative history and covers the period from 722 to 468 BC. The anonymous "Zhan Guo Ce" was a renowned ancient Chinese historical work compiling sporadic materials on the Warring States between the 3rd and 1st centuries BC.
The first systematic Chinese historical text, the "Records of the Grand Historian", was written by Sima Qian and his father. The book covers the period from the time of the Yellow Emperor until the author's own life in a naturalistic and comprehensive manner. Due to this highly praised and highly copied work, Sima Qian is often regarded as the father of Chinese historiography. The Twenty-Four Histories it spawned, the official compilations of the histories of those dynasties considered legitimate by the imperial Chinese historians, all copied his format. Typically, the rulers initiating a new dynasty would employ scholars to compile a final history from the annals and records of the previous one.
The "Shitong" was the first Chinese work "about" historiography. It was compiled by Liu Zhiji between AD 708 and 710. The book describes the general pattern of the official dynastic histories with regard to structure, method, arrangement, sequence, caption, and commentary back to the Warring States era.
The "Comprehensive Mirror to Aid in Government" was a pioneering reference work in Chinese historiography. Emperor Yingzong of Song ordered Sima Guang and other scholars to begin compiling this universal history of China in 1065, and they presented it to his successor Shenzong in 1084. It contains 294 volumes and about 3 million characters and narrates the history of China from 403 BC to the beginning of the Song in AD 959 chronologically. This broke a nearly-thousand-year tradition dating back to Sima Qian of employing annals for imperial reigns but biographies or treatises for other topics. The more consistent style of the "Comprehensive Mirror" was not followed by later official histories. In the mid 13th century, Ouyang Xiu was heavily influenced by the work of Xue Juzheng. This led to the creation of the "New History of the Five Dynasties", which covered five dynasties in over 70 chapters.
Toward the end of the Qing dynasty, scholars looked to Japan and the West for models. Although deeply learned in the traditions he now saw as "traditional," Liang Qichao began in the late 1890s to publish extensive and influential studies and polemics which converted his young readers to a new history which he saw as scientific in its methods. This next generation became professional historians, trained and teaching in universities. They included Hu Shi, Gu Jiegang, and Tsiang Tingfu, who were PhDs from Columbia University, the center of the American "New History," and Chen Yinke, who prepared his investigations into medieval Chinese history both in Europe and the United States. Other historians, such as Qian Mu, who was trained largely through independent study, were more conservative but still innovative in their response to world trends. In the 1920s, wide ranging scholars such as Guo Moruo adopted and adapted Marxism in order to portray China as a nation among nations rather than an exotic and isolated history. Following years saw historians such as Wu Han master both Western theories, including Marxism, and Chinese learning.
Key organizing concepts.
Dynastic cycle.
Like the three ages of the Greek poet Hesiod, the oldest Chinese historiography viewed mankind as living in a fallen age of depravity, cut off from the virtues of the past. The example of the sage kings Yao and Shun was particularly revered by Confucius and his disciples in their political theory.
Unlike Hesiod's system, however, the Duke of Zhou's development of the "Mandate of Heaven" as a rationale for dethroning the supposedly divine Zi clan led subsequent historians to see man's fall as a cyclical pattern. On this view, a new dynasty is founded by a morally upright founder. On a long enough time scale, his successors cannot keep themselves from becoming morally corrupt and dissolute. This immorality removes the dynasty's divine favor and is reflected in natural disasters (particularly floods), rebellions, and foreign invasions. Eventually, the dynasty becomes so weak as to allow its replacement by a new one, whose founder is able to rectify many of society's problems and begin the cycle anew. Over time, many felt a full correction was not possible and thus the golden age of Yao and Shun could not be reattained.
This teleological theory implies that there can be only one rightful sovereign under heaven at a time; thus, despite the fact that Chinese history has had many contentious and long periods of disunity, great effort was made in official histories to find and establish "legitimate" precursors whose fall allowed the new dynasty to carry on its mandate. Similarly, regardless of the particular merits of individual emperors, founders would be portrayed in more laudatory terms and the last ruler of a dynasty would always be castigated as depraved and unworthy even when that was not the case. Such a narrative was employed even after the fall of the empire by those compiling the history of the Qing and by those who justified the attempted restorations of the imperial system by Yuan Shikai and Zhang Xun.
Republican.
The schools of thought on the 1911 Revolution have evolved and developed from the early years of the Republic. The Marxist view saw the events of 1911 as a "Bourgeois Revolution". In the 1920s, the Nationalist Party issued a theory of three political stages based on Sun Yatsen's writings:
The most obvious criticism is the near-identical nature of "political tutelage" and of a "constitutional democracy" consisting only of one-party rule until the 1990s. Against this, Chen Shui-bian proposed his own four-stage theory.
Multiculturalism.
Nationalist and Communist China have both upheld the view that Chinese history should include all the ethnic groups of the lands held by the Qing Empire (Zhonghua Minzu), not just the history of the Han Chinese (China Proper). On this view (which rejected the earlier Han chauvinism of the Tongmenghui), China is permitted to include its internal and external tributaries as parts of its state and is viewed as a coherent multi-ethnic nation from time immemorial, existing as a legal entity even in periods of political disunity.
The advantage of this theory is to show the contributions of non-Han to Chinese history. It allows once "foreign" dynasties like the Mongol Yuan and the Manchu Qing as well as the Khitan Liao and Jurchen Jin dynasties to be considered fully Chinese, somewhat reducing the alienation of ethnic minorities living in China. This school also avoids "Han-centered" analyses. For example, it denies Yue Fei, a "Han Chinese" who fought for "China" against the Jurchens, a place as a "hero of China".
Support for the inclusion of ethnic minorities' history in China's own history varies in accordance with separatist and nationalist politics. For example, the 14th Dalai Lama, long insistent on Tibet's history being separate from that of China, conceded in 2005 that Tibet "is a part" of China's "5,000-year history" as part of a new proposal for Tibetan autonomy. Korean nationalists, however, have virulently reacted against China's application to UNESCO for recognition of Goguryeo tombs in Chinese territory: the absolute independence of Goguryeo is a central aspect of Korean identity, because according to Korean legend, it was comparatively independent of China and Japan, in contrast to subordinate states like the Joseon Dynasty and the Korean Empire. The legacy of Genghis Khan has been contested between China, Mongolia, and Russia, all three states having significant numbers of ethnic Mongols within their borders and holding territory that was conquered by the Khan.
The Chinese tradition since the 3rd-century Jin Dynasty that emperors of an incoming dynasty sponsor the writing of an official history for the immediately-preceding one has been cited in favor of an ethnically-inclusive interpretation of history. The compilation of official histories usually involved monumental intellectual labor. The Yuan and Qing Dynasties, which might be thought "foreign", faithfully carried out this practice, writing the official Chinese-language histories of the Han-ruled Song and Ming Dynasties, respectively. Had the two "non-Han" imperial families not thought of themselves as continuing the "Mandate of Heaven" of the Middle Kingdom, it would be hard to explain why they retained the costly tradition. Thus, every non-Han dynasty saw itself as the legitimate holder of the "Mandate of Heaven", which legitimized the dynastic cycle regardless of social or ethnic background.
Recent Western scholars have reacted against the ethnic-inclusiveness narrative in PRC-sponsored history by writing revisionist histories of China that feature, according to James A. Millward, "a degree of 'partisanship' for the indigenous underdogs of frontier history". Scholarly interest in writing about Chinese minorities from non-Chinese perspectives is growing.
Marxism.
Most Chinese history that is published in the People's Republic of China is based on a Marxist interpretation of history. These theories were first applied in the 1920s by Chinese scholars such as Guo Moruo, and became orthodoxy in academic study after 1949. The Marxist view of history is that history is governed by universal laws and that according to these laws, a society moves through a series of stages, with the transition between stages being driven by class struggle. These stages are:
The official historical view within the People's Republic of China associates each of these stages with a particular era in Chinese history, as well as making some subdivisions.
Because of the strength of the Communist Party of China and the importance of the Marxist interpretation of history in legitimizing its rule, it was for many years difficult for historians within the PRC to actively argue in favor of non-Marxist and anti-Marxist interpretations of history. However, this political restriction is less confining than it may first appear in that the Marxist historical framework is surprisingly flexible, and it is a rather simple matter to modify an alternative historical theory to use language that at least does not challenge the Marxist interpretation of history.
Partly because of the interest of Mao Zedong, historians in the 1950s took a special interest in the role of peasant rebellions in Chinese history and compiled documentary histories to examine them.
There are several problems associated with imposing Marx's European-based framework on Chinese history. First, slavery existed throughout China's history but was never the primary mode of production. While the Zhou and earlier dynasties may be labelled as feudal, later dynasties were much more centralized than Marx analyzed their European counterparts as being. To account for the discrepancy, Chinese Marxists invented the term "bureaucratic feudalism". The placement of the Tang as the beginning of the bureaucratic phase rests largely on the replacement of patronage networks with the imperial examination. Some world-systems analysts, such as Janet Abu-Lughod, claim analysis of Kondratiev waves shows that capitalism first arose in Song dynasty China; however, its widespread trade was subsequently disrupted and then curtailed.
There was a gradual relaxation of Marxist interpretation after the death of Mao in 1976, which was accelerated after the Tian'anmen Square protest and other revolutions in 1989, which damaged Marxism's ideological legitimacy in the eyes of Chinese academics.
Modernization.
This view of Chinese history sees Chinese society as a "traditional" society needing to become "modern", usually with the implicit assumption that Western society is the definition of modern society. Such a view was common among British and French scholars during the 19th and early 20th centuries but is now typically dismissed as eurocentrism or even racism, since such a view permits an implicit justification for breaking the society from its static past and bringing it into the modern world under European direction.
There are a number of other criticisms of this view. One centers on the slippery definition of "traditional society", which can become simply a catch-all for any non-Western society and treats all such societies similarly. To use an analogy, animals might be classified into "fish" and "non-fish", but the latter category is not a particularly helpful one.
By the mid-20th century, it was increasingly clear to historians that the notion of "changeless China" was untenable. A new concept, popularized by John Fairbank, was the notion of "change within tradition", which argued that China did change in the pre-modern period but that this change existed within certain cultural traditions. This notion has also been subject to the criticism that to say "China has not changed fundamentally" is tautological, since it requires that one look for things that have not changed and then define those as fundamental. This is far too broad, leading to conclusions like "England has not changed in the past thousand years because it has maintained its fundamental traditions of fishing, sheep-rearing, and monarchy".
Nonetheless, studies seeing China's interaction with Europe as the driving force behind its recent history are still common. Such studies, e.g., may consider the First Opium War as the starting point for China's modern period. Examples include the works of H.B. Morse, who wrote chronicles of China's international relations such as "Trade and Relations of the Chinese Empire".
Several of Fairbank's students argued in the 1950s that Confucianism was incompatible with modernity. Joseph Levenson and Mary C. Wright, and Albert Feuerwerker argued in effect that traditional Chinese values were a barrier to modernity and would have to be dismantled before China could make progress. Wright concluded, “The failure of the T’ung-chih ["Tongzhi"] Restoration demonstrated with a rare clarity that even in the most favorable circumstances there is no way in which an effective modern state can be grafted onto a Confucian society. Yet in the decades that followed, the political ideas that had been tested and, for all their grandeur, found wanting, were never given a decent burial.” 
In a different view of modernization, the Japanese historian Naito Torajiro argued that China reached "modernity" during its mid-Imperial period, centuries before Europe. He believed that the reform of the civil service into a meritocratic system and the disappearance of the ancient Chinese nobility from the bureaucracy constituted a modern society. The problem associated with this approach is the subjective use of "modernity". Chinese nobility had been in decline since the Qin dynasty and, while the exams were largely meritocratic, performance required time and resources that meant examinees were still typically from the gentry. Moreover, expertise in the Confucian classics did not guarantee competent meritocrats when it came to managing public works or preparing a budget. Confucian hostility to commerce maintained merchants at the bottom of the four occupations, itself an archaism maintained by devotion to classic texts. The social goal continued to be to invest in land and enter the gentry, ideas more similar to those of the physiocrats than those of Adam Smith.
Hydraulic despotism.
With ideas derived from Marx and Max Weber, Karl August Wittfogel argued that bureaucracy arose to manage irrigation systems. Despotism was needed to force the people into building canals, dikes, and waterways to increase agriculture. Yu the Great, one of China's legendary founders, is mostly known for his control of the floods of the Yellow River. The hydraulic empire produces wealth from its stability; while dynasties may change, the structure remains intact until destroyed by modern powers. Whereas in Europe agriculture always depended on rainfall and did not necessitate irrigation, in the Orient natural conditions were such that the bulk of the land could not be cultivated without large-scale irrigation works. As only a centralized administration could organize the building and maintenance of large-scale systems of irrigation, the need for such systems made bureaucratic despotism inevitable in Oriental lands.
Critics of Wittfogel's theory point out that water management was given the high status China accorded to officials concerned with taxes, rituals, or fighting off bandits. The theory also has a strong orientalist bent, regarding all Asian states as generally the same while finding reasons for European polities to escape the pattern.
Convergence.
Convergence theory, including Hu Shih and Ray Huang's involution theory, holds that the past 150 years have been a period in which Chinese and Western civilization have been in the process of converging into a world civilization. Such a view is heavily influenced by modernization theory but, in China's case, it is also strongly influenced by indigenous sources such as the notion of "Shijie Datong" or "Great Unity". It has tended to be less popular among more recent historians, as postmodern Western historians discount overarching narratives and nationalist Chinese historians feel similarly about narratives failing to account for some special or unique characteristics of Chinese culture.
Anti-Imperialism.
Closely related are colonial and anti-imperialist narratives. These often merge or are part of Marxist critiques from within China or the former Soviet Union or postmodern critiques such as Edward Said's "Orientalism", which fault traditional scholarship for trying to fit West, South, and East Asia's histories into European categories unsuited to them. With regard to China particularly, T.F. Tsiang and John Fairbank used newly opened archives in the 1930s to write modern history from a Chinese point of view. Fairbank and Teng Ssu-yu then edited the influential "China's Response to the West" (1953), although this was attacked for ascribing change in China to outside forces. In the 1980s, Paul Cohen continued to call for a more "China-Centered history of China".
One example of a blind spot which is provided by this viewpoint is the influence of central Asian policies on interactions with Europe in the Qing dynasty.
Postmodernism.
Postmodern interpretations of Chinese history tend to reject narrative history and instead focus on a small subset of Chinese history, particularly the daily lives of ordinary people in particular locations or settings.
Recent trends.
The late 20th century and early 21st century have seen a large number of studies of Chinese history that seek to challenge traditional paradigms. The field is rapidly evolving, with much new scholarship. Much of this new scholarship is based on the realization that there is much about Chinese history that is unknown or controversial. To give one such controversy, it is an active topic of discussion whether the typical Chinese peasant in 1900 was seeing his life improve or decline. In addition to the realization that there are major gaps in our knowledge of Chinese history is the equal realization that there are tremendous quantities of primary source material that have not yet been analyzed.
Recent Western scholarship of China has been heavily influenced by postmodernism, seeking to question modernist narratives regarding China's backwardness and lack of development. The desire to challenge the preconception that 19th-century China was weak, for instance, has led to scholarly interest in Qing expansion into Central Asia, where they were much more successful for a time than elsewhere. In fact, postmodern scholarship largely rejects grand narratives altogether, preferring to publish empirical studies on the socioeconomics and political or cultural dynamics of smaller communities within China.
Current scholars also attempt to assess source material more critically. For example, for a long period it was assumed that Imperial China had no system of civil law because the law codes did not have explicit provisions for civil lawsuits. However, more recent studies which use the records of civil magistrates suggest that China did in fact have a well-developed system of civil law, in which provisions of the criminal code were interpreted to allow civil causes of action. Another example has been statements made by intellectuals of the mid-Qing dynasty which were hostile toward commerce. Recent studies using sources such as magistrates' diaries and genealogical records show that these statements should not be taken at face value, without also recognizing the powerful unwritten influence merchants exerted on government policies. The division between the world of the merchant and that of the official was far more porous than traditionally believed. In fact, the growing consensus is that the anti-merchant statements of the mid-Qing record a growth of merchants' power and influence rather than the opposite.
Finally, current scholars are assessing new and previously overlooked documentary and historical evidence. Examples include masses of governmental and family archives, economic records such as the census tax-rolls, price records, and land surveys. In addition, there are large numbers of cultural artifacts such as vernacular novels, how-to books, and children's books which are in the process of being analyzed for clues about day-to-day life.
In contrast, Chinese historical scholarship remains largely nationalist and modernist or even traditionalist. The legacies of the modernist school (such as Lo Hsiang-lin) and the traditionalist school (such as Chien Mu) remain strong in Chinese circles. The more modernist works focus on imperial systems in China and employ scientific method to analyze epochs of Chinese dynasties from geographical, genealogical, and cultural artifacts: for example, using Carbon-14 dating and geographical records to correlate climates with cycles of calm and calamity in Chinese history. The traditionalist school of scholarship resorts to official imperial records and colloquial historical works and analyzes the different dynasties' rises and falls using a Confucian philosophy, albeit modified by an institutional administration perspective.
From the beginning of Communist rule in 1949 until the 1980s, the focus of Chinese historical scholarship was largely on peasant life interpreted via the officially-sanctioned Marxist theory of class struggle. From the time of Deng Xiaoping on, there has been a drift towards a Marxist-inspired nationalist perspective, and consideration of China's contemporary international status has became of paramount importance in historical studies. The current focus tends to be on specifics of civilization in ancient China, and the general paradigm of how China has responded to the dual challenges of interactions with the outside world and modernization in the post-1700 era. Long abandoned as a research focus among most Western scholars due to postmodernism's influence, this remains the primary interest for most historians inside China.

</doc>
<doc id="7175" url="http://en.wikipedia.org/wiki?curid=7175" title="Communist Party of China">
Communist Party of China

The Communist Party of China (CPC) is the founding and ruling political party of the People's Republic of China (PRC). The CPC is the sole governing party of China, although it coexists alongside 8 other legal parties that make up the United Front. It was founded in 1921, chiefly by Chen Duxiu and Li Dazhao. The party grew quickly, and by 1949 the CPC had defeated the Kuomintang (KMT) in a 10-year civil war, thus leading to the establishment of the People's Republic of China. With a membership of 86.7 million, it is the largest political party in the world.
The CPC is organized on the basis of democratic centralism, a principle conceived by Russian Marxist theoretician Vladimir Lenin which entails democratic and open discussion on policy on the condition of unity in upholding the agreed upon policies. The highest body of the CPC is the National Congress, convened every fifth year. When the National Congress is not in session, the Central Committee is the highest body, but since the body meets normally only once a year, most duties and responsibilities are vested in the Politburo and its Standing Committee. The party's leader holds the offices of General Secretary (responsible for civilian party duties), Chairman of the Central Military Commission (CMC) (responsible for military affairs) and state president (a largely ceremonial position). Through these posts the party leader is the country's paramount leader. The current party leader is Xi Jinping, elected at the 18th National Congress (held in 2012).
The CPC is still committed to communist thought. According to the party constitution the CPC adheres to Marxism–Leninism, Mao Zedong Thought, socialism with Chinese characteristics, Deng Xiaoping Theory, Three Represents and the Scientific Outlook on Development. The official explanation for China's economic reforms is that the country is in the primary stage of socialism, a developmental stage similar to the capitalist mode of production. The planned economy established under Mao Zedong was replaced by the socialist market economy, the current economic system, on the basis that "Practice is the Sole Criterion for the Truth" (i.e. the planned economy was deemed inefficient).
Since the collapse of Eastern European communist regimes in 1989–1990 and the dissolution of the Soviet Union in 1991, the CPC has emphasized its party-to-party relations with the ruling parties of the remaining socialist states. While the CPC still maintains party-to-party relations with non-ruling communist parties around the world, it has since the 1980s established relations with several non-communist parties, most notably with ruling parties of one-party states (whatever their ideology), dominant parties in democratic systems (whatever their ideology), and social democratic parties.
History.
Founding and early history (1921–27).
The CPC has its origins in the May Fourth Movement of 1919, during which radical ideologies like anarchism and communism gained traction among Chinese intellectuals. Li Dazhao was the first leading Chinese intellectual who publicly supported Leninism and world revolution. In contrast to Chen Duxiu, Li did not renounce participation in the affairs of the Republic of China. Both of them regarded the October Revolution in Russia as groundbreaking, believing it to herald a new era for oppressed countries everywhere. The CPC was modeled on Vladimir Lenin's theory of a vanguard party. Study circles were, according to Cai Hesen, "the rudiments [of our party]". Several study circles were established during the New Culture Movement, but "by 1920 skepticism about their suitability as vehicles for reform had become widespread."
The founding National Congress of the CPC was held on 23–31 July 1921. While it was originally planned to be held in Shanghai French Concession, police offers interrupted the meeting on 3 July. Because of that, the congress was moved to a tourist boat on South Lake in Jiaxing, Zhenjiang province. Only 12 delegates attended the congress, with neither Li nor Chen being able to attend. Chen sent a personal representative to attend the congress. The resolutions of the congress called for the establishment of a communist party (as a branch of the Communist International) and elected Chen as its leader.
The communists dominated the left wing of the KMT, a party organized on Leninist lines, struggling for power with the party's right wing. When KMT leader Sun Yat-sen died in May 1925, he was succeeded by a rightist, Chiang Kai-shek, who initiated moves to marginalize the position of the communists. Fresh from the success of the Northern Expedition to overthrow the warlords, Chiang Kai-shek turned on the communists, who by now numbered in the tens of thousands across China. Ignoring the orders of the Wuhan-based KMT government, he marched on Shanghai, a city controlled by communist militias. Although the communists welcomed Chiang's arrival, he turned on them, massacring 5000 with the aid of the Green Gang. Chiang's army then marched on Wuhan, but was prevented from taking the city by CPC General Ye Ting and his troops. Chiang's allies also attacked communists; in Beijing, 19 leading communists were killed by Zhang Zuolin, while in Changsha, He Jian's forces machine gunned hundreds of peasant militiamen. That May, tens of thousands of communists and their sympathisers were killed by nationalists, with the CPC losing approximately of its members.
The CPC continued supporting the Wuhan KMT government, but on 15 July 1927 the Wuhan government expelled all communists from the KMT. The CPC reacted by founding the Workers' and Peasants' Red Army of China, better known as the "Red Army", to battle the KMT. A battalion led by General Zhu De was ordered to take the city of Nanchang on 1 August 1927 in what became known as the Nanchang Uprising; initially successful, they were forced into retreat after five days, marching south to Shantou, and from there being driven into the wilderness of Fujian. Mao Zedong was appointed commander-in-chief of the Red Army, and led four regiments against Changsha in the Autumn Harvest Uprising, hoping to spark peasant uprisings across Hunan. His plan was to attack the KMT-held city from three directions on 9 September, but the Fourth Regiment deserted to the KMT cause, attacking the Third Regiment. Mao's army made it to Changsha, but could not take it; by 15 September, he accepted defeat, with 1000 survivors marching east to the Jinggang Mountains of Jiangxi.
Chinese Civil War and World War II (1927–49).
The near-destruction of the CPC's urban organizational apparatus, led to institutional changes within the party. The party adopted democratic centralism, a way to organize revolutionary parties, and established a Politburo (functioned as the standing committee of the Central Committee). The result was increased centralization of power within the party . At every-level of the party this was duplicated, with standing committees now in effective control. After Chen Duxiu's dismissal, Li Lisan was able to assume "de facto" control of the party organization by 1929–30. Li Lisan's leadership was a failure, and by the end of it the CPC was on the brink of destruction. The Comintern became involved, and by late-1930 he had been taken away his powers. By 1935 Mao had become the party's formal leader, with Zhou Enlai and Zhang Wentian, the formal head of the party, serving as his informal deputies. The conflict with the KMT led to the reorganization of the Red Army, with power now centralized in the leadership through the creation of CPC political departments charged with supervising the army.
The Second Sino-Japanese War caused a pause in the conflict between the CPC and the KMT. The Second United Front was established between the CPC and the KMT to tackle the invasion. While the front formally existed until 1945, all collaboration between the two parties had ended by 1940. Despite their formal alliance, the CPC used the opportunity to expand and carve out independent bases of operations to prepare for the coming war with the KMT. In 1939 the KMT began to restrict CPC expansion within China. This led to frequent clashes between CPC and KMT forces. It did not take long before the situation were deescalated, since none of the parties considered a civil war an option at this time. Despite this, by 1943 the CPC was again actively expanding its territory at the expense of the KMT.
From 1945 until 1949, the war had been reduced to two parties; the CPC and the KMT. This period lasted through four stages; the first was from August 1945 (when the Japanese surrendered) to June 1946 (when the peace talks between the CPC and the KMT ended). By 1945, the KMT three-times more soldiers under its command then the CPC, and because of it, it looked early on like it was winning. With the cooperation of the Americans and the Japanese, the KMT was able to retake major parts of the country. Around the same time, the CPC launched an invasion of Manchuria, where they were given assistance by the Soviet Union. However, KMT rule over the reconquested territories would prove unpopular because of endemic corruption within the party. However, the main failure was that the KMT, with 2 millions more troops than the CPC, failed to reconquer the rural territories which made up the CPC's stronghold. The second stage, lasting from July 1946 to June 1947, saw KMT extend its control over major cities, such as Yanan (the CPC headquarter for much of the war). The KMT's successes were hollow, the CPC had tactically withdrawn from the cities, and instead attacked KMT authorities by instigating protests amongst students and intellectuals in the cities (the KMT responded to these events with heavy-handed repression). In the meantime, the KMT was struggling with factional infighting and Chiang Kai-shek's autocratic control over the party, which weakened the KMT's ability to respond to attacks. The third stage, lasting from July 1947 to August 1948, saw a limited counteroffensive by the CPC. The objective was to clear "Central China, strengthening North China, and recovering Northeast China." This policy, coupled with desertions from the KMT military force (by spring 1948 KMT military had lost an estimated 2 million troops, having 1 million troops left) and the increasing unpopularity of KMT rule. The result was that the CPC was able to cut off KMT garrisons in Manchuria and retake several lost territories. The last stage, lasting from September 1948 to December 1949, saw the communist take the initiative and the collapse of KMT rule in mainland China. On 1 October 1949, Mao declared the establishment of the PRC, which signified the end of the Chinese Revolution (as it is officially described by the CPC).
Ruling party (1949–present).
The Chinese Revolution, directed by Mao Zedong and the CPC, led to the establishment of the (PRC) in 1949. The PRC was founded on Marxist–Leninist principles, or more precisely, the sinification of Marxism–Leninism (officially known as Mao Zedong Thought or Maoism). During the 1960s and 1970s, the CPC experienced a significant ideological separation from the Communist Party of the Soviet Union. By that time, Mao had begun saying that the "continued revolution under the dictatorship of the proletariat" stipulated that class enemies continued to exist even though the socialist revolution seemed to be complete, leading to the Cultural Revolution.
Following Mao's death in 1976, a power struggle between CPC General Secretary Hua Guofeng and Deng Xiaoping erupted. Deng won the struggle, and became the "paramount leader". Deng, alongside Chen Yun and Li Xiannian, spearheaded the Reform and opening policy, and introduced the ideological concept of socialism with Chinese characteristics. In reversing some of Mao's "extreme-leftist" policies, Deng argued that a socialist state could use the market economy without itself being capitalist. While asserting the political power of the Party, the change in policy generated significant economic growth. The new ideology, however, was contested on both sides of the spectrum, by Maoists as well as by those supporting political liberalization. With other social factors, the conflicts culminated in the 1989 Tiananmen Square Protests. Deng's vision on economics prevailed, and by the early 1990s the concept a socialist market economy had been introduced. In 1997, Deng's beliefs (Deng Xiaoping Theory), were embedded in the CPC constitution.
CPC General Secretary Jiang Zemin succeeded Deng as “paramount leader” in the 1990s, and continued most of his policies. As part of Jiang Zemin's nominal legacy, the CPC ratified the Three Represents for the 2003 revision of the Party constitution, as a "guiding ideology" to encourage the Party to represent "advanced productive forces, the progressive course of China's culture, and the fundamental interests of the people." The theory has legitimized the entry of private business owners and bourgeois elements into the party. Hu Jintao, Jiang Zemin's successor as paramount leader, took office in 2002. Unlike Mao, Deng and Jiang Zemin, Hu laid emphasis on collective leadership and opposed one-man dominance of the political system. The insistence on focusing on economic growth has led to a wide range of serious social problems. To address these, Hu introduced two main ideological concepts: the Scientific Outlook on Development and Harmonious Socialist Society. Hu resigned from his post as CPC General Secretary and Chairman of the CMC at the 18th National Congress held in 2012, and was succeeded in both posts by Xi Jinping.
Governance.
Collective leadership.
Currently, in a bid to curtail the powers of the individuals, collective leadership, the idea that decisions will be taken through consensus, has become the ideal in the CPC. The concept has its origins back to Vladimir Lenin and the Russian Bolshevik Party. At the level of the central party leadership this means that, for instance, all members of the Politburo Standing Committee are of equal standing (each member having only one vote). A member of the Politburo Standing Committee often represents a sector; during Mao's reign, he controlled the People's Liberation Army, Kang Sheng the security apparatus and Zhou Enlai the State Council and the Ministry of Foreign Affairs. This counts as informal power. Despite this, in a paradoxical relation, members of a body are ranked hierarchically (despite the fact that members are in theory equal to each others). In spite of this, the CPC is led by an informal leader principle, each collective leadership is led by a core, that is a paramount leader; a person who holds the offices of CPC General Secretary, CMC chairman and President of the PRC. Before Jiang Zemin's tenure as paramount leader, the party core and collective leadership were indistinguishable. In practice, the core was not responsible to the collective leadership. However, by the time of Jiang, the party had begun propagating a responsibility system, referring to it in official pronouncements to the "core of the collective leadership".
Democratic centralism.
The CPC's organizational principle is democratic centralism, which is based on two principles; democracy (synonymous in official discourse with "socialist democracy" and "inner-party democracy") and centralism. This has been the guiding organizational principle of the party since the 5th National Congress, held in 1927. In the words of the party constitution, "The Party is an integral body organized under its program and constitution and on the basis of democratic centralism". Mao once quipped that democratic centralism was "at once democratic and centralized, with the two seeming opposites of democracy and centralization united in a definite form." Mao claimed that the superiority of democratic centralism laid in its internal contradictions, between democracy and centralism, and freedom and discipline. Currently, the CPC is claiming that "democracy is the lifeline of the Party, the lifeline of socialism". But for democracy to be implemented, and functioning properly, there needs to be centralization. Democracy in any form, the CPC claims, needs centralism, since without centralism there will be no order. According to Mao, democratic centralism "is centralized on the basis of democracy and democratic under centralized guidance. This is the only system that can give full expression to democracy with full powers vested in the people’s congresses at all levels and, at the same time, guarantee centralized administration with the governments at each level exercising centralized management of all the affairs entrusted to them by the people’s congresses at the corresponding level and safeguarding whatever is essential to the democratic life of the people".
Multi-party Cooperation System.
The Multi-party Cooperation and Political Consultation System is led by the CPC in cooperation and consultation with the 8 parties which make up the United Front. Consultation takes place under the leadership of the CPC, with mass organizations, the United Front parties, and "representatives from all walks of life". These consultations contribute, at least in theory, to the formation of the country's basic policy in the fields of political, economic, cultural and social affairs. The CPC's relationship with other parties is based on the principle of "long-term coexistence and mutual supervision, treating each other with full sincerity and sharing weal or woe." This process is institutionalized in the Chinese People's Political Consultative Conference (CPPCC). All the parties in the United Front support China's road to socialism, and hold steadfast to the leadership of the CPC. Despite all this, the CPPCC is a body without any real power. While discussions do take place, they are all supervised by the CPC.
Organization.
National Congress.
The National Congress is the party's supreme organ, and is held every fifth year (in the past there were long intervals between congresses, but since the 9th National Congress in 1969, congresses have been held regularly). According to the party's constitution, a congress may not be postponed except "under extraordinary circumstances". A congress may be held before the given date if the Central Committee so decides, or if "one third of the party organizations at the provincial level so request". Under Mao, the delegates to congresses were appointed; however, since 1982 the congress delegates have been elected, due to the decision that there must be more candidates than seats. At the 15th National Congress in 1997, for instance, several princelings (the sons or daughters of powerful CPC officials) failed to be elected to the 15th Central Committee; among them were Chen Yuan, Wang Jun and Bo Xilai. The elections are carried out through secret ballots. Despite this, certain seats are not subject to elections; instead, the outgoing Central Committee "recommends" certain choices to the party electorate. These figures are mostly high-ranking members of the party leadership or special guests. For instance, at the 15th National Congress, 60 seats were given to members who had joined the CPC before 1927, and some were given to the outgoing members of the 15th Central Commission for Discipline Inspection (CCDI) and the 15th Central Committee.
The party constitution gives the National Congress six responsibilities: (1) electing the party's executive and legislative branches, represented by the Central Committee; (2) electing the judicial branch, represented by the CCDI; (3) to examining the report of the outgoing Central Committee; (4) examining the report of the outgoing CCDI; (5) discussing and enacting party policies; and (6) revising the party's constitution. However, the delegates rarely discuss issues in length at the National Congresses; most discussion takes place before the congress, in the preparation period.
Constitution.
According to the CPC-published book "Concise History of the Communist Party of China", the party's first constitution was adopted at the 1st National Congress. Since then several constitutions have been written, such as the second constitution, adopted at the 7th National Congress. The constitution regulates party life, and the CCDI is responsible for supervising the party to ensure that it is followed. The constitution currently in force was adopted at the 12th National Congress. It has many affinities with the state constitution, and they are generally amended either at party congresses or shortly thereafter. The preamble of the state constitution is largely copied from the "General Program" (the preamble) of the party constitution.
Central Committee.
The Central Committee is empowered by the party constitution to enact policies in the periods between party congresses. A Central Committee is "de jure" elected by a party Congress, but in reality its membership is chosen by the central party leadership. The authority of the Central Committee has increased in recent years, with the leaders rarely, if ever, going against Central Committee, which often occurred during the early years of the People's Republic. The Central Committee is required to meet at least once every year; however, in the early years of the People's Republic there were several years when it did not convene at all; 1951–53, 1960, 1963–65, 1967, 1971, 1974 and 1976.
While the Central Committee is the highest organ in the periods between party congresses, few resolutions cite its name. Instead, the majority of party resolutions refer to the "Communist Party Centre", an indirect way of protecting the powers of, and resolutions produced by, the Politburo, the Politburo Standing Committee and the General Secretary. This method shields the central party leadership from lower-level bodies, reducing accountability, as lower levels can never be sure which body produced which resolution. In contrast to the Central Committee of the Communist Party of Vietnam (CPV), the CPC Central Committee does not have the power to remove general secretaries or other leading officials, despite the fact that the party constitution grants it those rights. When the CPV dismissed its General Secretary Do Muoi, it convened a special session of its Central Committee, and when it chose its new general secretary, it convened another Central Committee plenum. In contrast, in China, when the CPC dismissed Hu Yaobang (in 1987) and Zhao Ziyang in 1989, the Politburo, not the Central Committee, convened a special session. Not only did the meeting itself break constitutional practices, since the CPC constitution clearly states that a Central Committee session must be called, but the meeting included several party veterans who were neither formal members of the Politburo nor of the Central Committee. In short, the CPC Central Committee, in contrast to the CPV Central Committee, is responsible to the higher bodies of the party (the Politburo and the Politburo Standing Committee), while in Vietnam the higher bodies are accountable to the Central Committee.
Central Commission for Discipline Inspection.
The Central Commission for Discipline Inspection (CCDI) is responsible for monitoring and punishing CPC cadres who abuse power, are corrupt or in general commit wrongdoing. CCDI organs exist at every level of the party hierarchy. The CCDI is the successor to the Control Commission, abolished in 1968 at the height of the Cultural Revolution. Although the CCDI was originally designed to restore party morale and discipline, it has taken over many of the functions of the former Control Commission. The CCDI is elected by the National Congress, held every fifth year.
Bodies of the Central Committee.
Party leader.
At the party's founding in 1921, Chen Duxiu was elected as the party leader, holding the position of Secretary of the Central Bureau. As the party expanded, the title changed several times over the next 3 years, until in 1925 the position of General Secretary was introduced. The term General Secretary continued in general use until 1943, when Mao Zedong was elected as Chairman of the Politburo. In 1945, Mao was elected Chairman of the CPC Central Committee, the title he held for the rest of his life. The office of General Secretary was revived in 1956 at the 8th National Congress, but it functioned as a lesser office, responsible to the office of the CPC Chairman. At a party meeting in 1959, Mao explained the relationship between the CPC Chairman and the CPC General Secretary as follows: "As Chairman, I am the commander; as General Secretary, Deng Xiaoping is deputy commander."
The office of Chairman of the Communist Party was abolished in 1982, and replaced with that of General Secretary of the CPC Central Committee. According to the party constitution, the General Secretary must be a member of the Politburo Standing Committee (PSC), and is responsible for convening meetings of the PSC and the Politburo, while also presiding over the work of the Secretariat.
The party's leader holds the offices of General Secretary (responsible for civilian party duties), Chairman of the CMC (responsible for military affairs) and President of the PRC (a largely ceremonial position). Through these posts the party leader is the country's paramount leader.
Politburo.
The Politburo of the Central Committee "exercises the functions and powers of the Central Committee when a plenum is not in session". It is formally elected at the first plenary meeting of each newly elected Central Committee. In reality, however, Politburo membership is decided by the central party leadership. During his rule, Mao controlled the composition of the Politburo himself. The Politburo was "de facto" the highest organ of power until the 8th National Congress, when the PSC was established. The powers given to the PSC came at the expense of the Politburo. The Politburo meets at least once a month. The CPC General Secretary is responsible for convening the Politburo.
From 2003 onwards, the Politburo has been delivering a work report to every Central Committee plenum, further cementing the Politburo's status as accountable to the Central Committee. Also, from the 16th National Congress onwards, the CPC has been reporting on meetings of the Politburo, the PSC and its study sessions. However, the reports do not contain all the information discussed at the meetings; the end of the reports usually notes that "other matters" were also discussed at the meeting.
In the Politburo, decisions are reached through consensus, not votes. In certain cases, straw votes are used to see how many members support or oppose a certain case (these straw votes do not necessarily affect the ultimate decision). Every member has the right to participate in the collective discussion. It is the CPC General Secretary who convenes the Politburo and sets the agenda for the meeting. Each Politburo member is told of the agenda beforehand, and is given materials by the General Secretary on the subject so as to be prepared for the discussions. The first person to speak at the meeting is the member who proposed the agenda. After that, those who know about the subject, or whose work is directly related to it, may speak. Then those who doubt or oppose the agenda speak. Lastly, the General Secretary speaks, and he usually supports the agenda, as he supported discussing it in the first place. When the General Secretary is finished speaking, he calls for a vote. If the vote is unanimous or nearly so, it may be accepted; if the vote is nearly unanimous, but members who directly work in the area discussed oppose it, the issue will be postponed. When the Politburo enacts a decision without all the members' agreement, the other members usually try to convince their opponents. In many ways, the CPC Politburo's decision-making process remains very similar to that of the Politburo of the Communist Party of the Soviet Union after Nikita Khrushchev's removal.
Politburo Standing Committee.
The Politburo Standing Committee (PSC) is the highest organ of the Communist Party when neither the Politburo, the Central Committee and the National Congress are in session. It convenes at least once a week. It was established at the 8th National Congress, in 1958, to take over the policy-making role formerly assumed by the Secretariat. The PSC is "the primary decision-making body, though there is growing evidence of its being made more responsive to the collective agreements of the entire Politburo." Despite formal rules stating that a PSC member must serve a term in the Politburo before advancing to the PSC, this rule has been breached twice, first in 1992 when Hu Jintao was appointed to PSC, and again in 2007 when Xi Jinping and Li Keqiang were appointed to it. In reality, however, the PSC is not accountable to the Central Committee and has never been.
Secretariat.
The Secretariat of the Central Committee is headed by the General Secretary and is responsible for supervising the central party organizations: departments, commissions, newspapers, etc. It is also responsible for implementing the decisions of the Politburo and the Politburo Standing Committee. The Secretariat was abolished in 1966 and its formal functions taken over by the Central Office of Management, but it was reestablished in 1980. To be appointed to the Secretariat, a person has to be nominated by the Politburo Standing Committee; the nomination must be approved by the Central Committee.
Central Military Commission.
The Central Military Commission is elected by the Central Committee, and is responsible for the PLA. The position of CMC Chairman is one of the most powerful in China, and the CMC Chairman must concurrently serve as CPC General Secretary. Unlike the collective leadership ideal of other party organs, the CMC Chairman acts as commander-in-chief with the right to appoint or dismiss top military officers as he pleases. The CMC Chairman can deploy troops, controls the country's nuclear weapons, and allocates the budget. The promotion or transfer of officers above the divisional level must be validated by the CMC Chairman's signature.
In theory, the CMC Chairman is under the responsibility of the Central Committee, but in practice, he reports only to the paramount leader. This is in many ways due to Mao, who did not want other Politburo members to involve themselves in military affairs. As he put it, "the Politburo's realm is state affairs, the CMC's is military". This state of things has continued until today. The CMC has controlled the PLA through three organs since 1937: the General Staff Department, the General Political Department and the General Logistics Department. A fourth organ, the General Armaments Department, was established in 1998.
National Security Commission.
The Central National Security Commission (CNSC) was established at the 3rd Plenary Session of the 18th Central Committee (held in 2013). Its purpose is to "co-ordinate security strategies across various departments, including intelligence, the military, foreign affairs and the police in order to cope with growing challenges to stability at home and abroad." The idea of establishing a CNSC was first mentioned in the 1980s, but was muted "by vested interests that stand to lose power in a reshuffle". Currently little is known of the body outside of the CPC, but it is generally believed to have strengthened the party's control over the People's Liberation Army (PLA), the Chinese armed forces. On 24 January 2014 Xi Jinping, the current CPC General Secretary, was appointed CNSC Chairman, while Li Keqiang, the Premier of the State Council, and Zhang Dejiang, the Chairman of the Standing Committee of the NPC (head of parliament), were appointed CNSC deputy chairmen.
Subordinate organs.
There are several organs under the auspices of the Central Committee. The following are the most important:
Lower-level organizations.
Party committees exist at the level of provinces; autonomous regions; municipalities directly under the central government; ; autonomous prefectures; counties (including banners); autonomous counties; ; and municipal districts. These committees are elected by party congresses (at their own level). Local party congresses are supposed to be held every fifth year, but under extraordinary circumstances they may be held earlier or postponed. However that decision must be approved by the next higher level of the local party committee. The number of delegates and the procedures for their election are decided by the local party committee, but must also have the approval of the next higher party committee.
A local party congress has many of the same duties as the National Congress, and it is responsible for examining the report of the local Party Committee at the corresponding level; examining the report of the local Commission for Discipline Inspection at the corresponding level; discussing and adopting resolutions on major issues in the given area; and electing the local Party Committee and the local Commission for Discipline Inspection at the corresponding level. Party committees of "a province, autonomous region, municipality directly under the central government, city divided into districts, or autonomous prefecture [are] elected for a term of five years", and include full and alternate members. The party committees "of a county (banner), autonomous county, city not divided into districts, or municipal district [are] elected for a term of five years", but full and alternate members "must have a Party standing of three years or more." If a local Party Congress is held before or after the given date, the term of the members of the Party Committee shall be correspondingly shortened or lengthened.
A local Party Committee is responsible to the Party Committee at the next higher level. The number of full and alternate members at the local Party Committee is decided by the Party Committee at the next higher level. Vacancies in a Party Committee shall be filled by an alternate members according to the order of precedence, which is decided by the number of votes an alternate member got during his or hers election. A Party Committee must convene for at least two plenary meetings a year. During its tenure, a Party Committee shall "carry out the directives of the next higher Party organizations and the resolutions of the Party congresses at the corresponding levels." The local Standing Committee (analogous to the Central Politburo) is elected at the first plenum of the corresponding Party Committee after the local party congress. A Standing Committee is responsible to the Party Committee at the corresponding level and the Party Committee at the next higher level. A Standing Committee exercises the duties and responsibilities of the corresponding Party Committee when it is not in session.
Members.
Probationary period, rights and duties.
To join the party an applicant must be 18 years of age, and must spend a year as a probationary member. In contrast to the past, when emphasis was placed on the applicants' ideological criteria, the current CPC stresses technical and educational qualifications. However, applicants and members are expected to be both "red and expert". To become a probationary member, two current CPC members must recommend the applicant to the local party leadership. The recommending members must acquaint themselves with the applicants, and be aware of the "applicant's ideology, character, personnel records and work performance" while teaching them about the party's program and constitution, as well as the duties and responsibilities of members. To this end, the recommending members must write a report to the local party leadership, reporting their opinion that the applicant is either qualified or unqualified for membership. To become a probationary member, the applicant must take an admission oath before the party flag. The relevant CPC organization is responsible for observing and educating probationary members. Probationary members have duties similar to those of full members, with the exception that they may not vote in party elections nor stand for election.
Before 1949, joining the CPC was a matter of personal commitment to the communist cause. After 1949, people joined to gain good government jobs or access to universities, which were then limited to CPC members. Many joined the CPC through the Communist Youth League. Under Jiang Zemin, private entrepreneurs were allowed become party members. According to Article 3 of the CPC constitution, a member must "conscientiously study Marxism–Leninism, Mao Zedong Thought, Deng Xiaoping Theory and the important thoughts of Three Represents, study the Scientific Outlook on Development, study the Party's line, principles, policies and resolutions, acquire essential knowledge concerning the Party, obtain general, scientific, legal and professional knowledge and work diligently to enhance their ability to serve the people." A member, in short, must follow orders, be disciplined, uphold unity, serve the Party and the people, and promote the socialist way of life. Members enjoy the privilege of attending Party meetings, reading relevant Party documents, receiving Party education, participating in Party discussions through the Party's newspapers and journals, making suggestions and proposal, making "well-grounded criticism of any Party organization or member at Party meetings" (even of the central party leadership), voting and standing for election, and of opposing and criticizing Party resolutions ("provided that they resolutely carry out the resolution or policy while it is in force"); and they have the ability "to put forward any request, appeal, or complaint to higher Party organizations, even up to the Central Committee, and ask the organizations concerned for a responsible reply." No party organization, including the CPC central leadership, can deprive a member of these rights.
Composition of the party.
As of the 18th National Congress, farmers, herdsmen and fishermen make up 31 percent of the party membership; 9 percent are workers. The second largest membership group, "Managing, professional and technical staff in enterprises and public institutions", makes up 23 percent of CPC membership. Retirees make up 18 percent, "Party and government staff" make up 8 percent, "others" make up another 8 percent, and students are 3 percent of CPC membership. Men make-up 77 percent of CPC membership, while woman make up 23 percent. The CPC currently has 86.7 million members, making it the largest political party in the world.
Communist Youth League.
The Communist Youth League (CYL) is the CPC's youth wing, and the largest mass organization for youth in China. According to the CPC's constitution the CYL is a "mass organization of advanced young people under the leadership of the Communist Party of China; it functions as a party school where a large number of young people learn about socialism with Chinese characteristics and about communism through practice; it is the Party's assistant and reserve force." To join, an applicant has to be between the ages of 14 and 28. It controls and supervises Young Pioneers, a youth organization for children below the age of 14. The organizational structure of CYL is an exact copy of the CPC's; the highest body is the National Congress, followed by the Central Committee, Politburo and the Politburo Standing Committee. However, the Central Committee (and all central organs) of the CYL work under the guidance of the CPC central leadership. Therefore, in a peculiar situation, CYL bodies are both responsible to higher bodies within CYL and the CPC, a distinct organization. As of the 17th National Congress (held in 2013), CYL has 89 million members.
Symbols.
According to the Article 53 of the CPC constitution, "the Party emblem and flag are the symbol and sign of the Communist Party of China." At the beginning of its history, the CPC did not have a single official standard for the flag, but instead allowed individual party committees to copy the flag of the Communist Party of the Soviet Union. On 28 April 1942, the Central Politburo decreed the establishment of a sole official flag. "The flag of the CPC has the length-to-width proportion of 3:2 with a hammer and sickle in the upper-left corner, and with no five-pointed star. The Political Bureau authorizes the General Office to custom-make a number of standard flags and distribute them to all major organs". According to "People's Daily", "The standard party flag is 120 centimeters (cm) in length and 80 cm in width. In the center of the upper-left corner (a quarter of the length and width to the border) is a yellow hammer-and-sickle 30 cm in diameter. The flag sleeve (pole hem) is in white and 6.5 cm in width. The dimension of the pole hem is not included in the measure of the flag. The red color symbolizes revolution; the hammer-and-sickle are tools of workers and peasants, meaning that the Communist Party of China represents the interests of the masses and the people; the yellow color signifies brightness." In total the flag has five dimensions, the sizes are "no. 1: 388 cm in length and 192 cm in width; no. 2: 240 cm in length and 160 cm in width; no. 3: 192 cm in length and 128 cm in width; no. 4: 144 cm in length and 96 cm in width; no. 5: 96 cm in length and 64 cm in width." On 21 September 1966, the CPC General Office issued "Regulations on the Production and Use of the CPC Flag and Emblem", which stated that the emblem and flag were the official symbols and signs of the party.
Ideology.
It has been argued in recent years, mainly by foreign commentators, that the CPC does not have an ideology, and that the party organization is pragmatic and interested only in what works. This simplistic view is wrong in many ways, since official statements make it very clear the party does have a coherent worldview. For instance, Hu Jintao stated in 2012 that the Western world is "threatening to divide us" and that "the international culture of the West is strong while we are weak ... Ideological and cultural fields are our main targets". The CPC puts a great deal of effort into the party schools and into crafting its ideological message. Before the "Practice Is the Sole Criterion for the Truth" campaign, the relationship between ideology and decision-making was a deductive one, meaning that policy-making was derived from ideological knowledge. Under Deng this relationship was turned upside down, with decision-making justifying ideology and not the other way around. Lastly, Chinese policy-makers believe that one of the reasons for the dissolution of the Soviet Union was its stagnant state ideology. They therefore believe that their party ideology must be dynamic to safeguard the party's rule, unlike the Soviet Union's communist party, whose ideology they believe became "rigid, unimaginative, ossified, and disconnected from reality."
Marxism–Leninism and Mao Zedong Thought.
Marxism–Leninism was the first official ideology of the Communist Party of China, and is a combination of classical Marxism (the works of Karl Marx and Friedrich Engels) and Leninism (the thoughts of Vladimir Lenin). According to the CPC, "Marxism–Leninism reveals the universal laws governing the development of history of human society." To the CPC, Marxism–Leninism provides a vision of the contradictions in capitalist society and of the inevitability of a future socialist and communist societies. Marx and Engels first created the theory behind Marxist party building; Lenin developed it in practice before, during and after the Russian Revolution of 1917. Lenin's biggest achievement came in party-building, through concepts such as the vanguard party of the working class and democratic centralism. According to the "People's Daily", Mao Zedong Thought]] "is Marxism–Leninism applied and developed in China".
Mao Zedong Thought was conceived not only by Mao Zedong, but by leading party officials. According to Xinhua, Mao Zedong Thought is "an integration of the universal truth of Marxism–Leninism with the practice of the Chinese revolution." Currently, the CPC interprets the essence of Mao Zedong Thought as "Seeking truth from facts": "we [CPC] must proceed from reality and put theory into practice in everything. In other words, we must integrate the universal theory of Marxism–Leninism with China's specific conditions."
While analysts generally agree that the CPC has rejected orthodox Marxism–Leninism and Mao Zedong Thought (or at least basic thoughts within orthodox thinking), the CPC itself disagrees. Some Western commentators also talk about a "crisis of ideology" within the party; they believe that the CPC has rejected communism. Wang Xuedong, the Director of the Institute of World Socialism, said in response, "We know there are those abroad who think we have a 'crisis of ideology,' but we do not agree." According to Jiang Zemin, the CPC "must never discard Marxism–Leninism and Mao Zedong Thought.” He said that “if we did, we would lose our foundation.” He further noted that Marxism in general "like any science, needs to change as time and circumstances advance." Certain groups argue that Jiang Zemin ended the CPC's formal commitment to Marxism with the introduction of the ideological theory, the Three Represents. However, party theorist Leng Rong disagrees, claiming that "President Jiang rid the Party of the ideological obstacles to different kinds of ownership [...] He did not give up Marxism or socialism. He strengthened the Party by providing a modern understanding of Marxism and socialism—which is why we talk about a ‘socialist market economy’ with Chinese characteristics." Marxism in its core is, according to Jiang Zemin, methodology and the goal of a future, classless society, not analyses of class and of the contradictions between different classes.
Karl Marx argued that society went through different stages of development, and believed that the capitalist mode of production was the third stage. The stages were: ancient, based mostly on slavery; feudal; capitalist; socialist; and the communist mode of production. The attainment of true "communism" is described as the CPC's and China's "ultimate goal". While the CPC claims that China is in the primary stage of socialism, party theorists argue that the current development stage "looks a lot like capitalism". Alternately, certain party theorists argue that “capitalism is the early or first stage of communism.” In official pronouncements, the primary stage of socialism is predicted to last about 100 years, after which China will reach another developmental stage. Some have dismissed the concept of a primary stage of socialism as intellectual cynicism. According to Robert Lawrence Kuhn, a China analyst, "When I first heard this rationale, I thought it more comic than clever—a wry caricature of hack propagandists leaked by intellectual cynics. But the 100-year horizon comes from serious political theorists".
Rationale for reforms.
While Westerners have argued that the reforms introduced by the CPC under Deng were a rejection of the party's Marxist heritage and ideology, the CPC does not view it as such. The rationale behind the reforms was that the productive forces of China lagged behind the advanced culture and ideology developed by the party-state. In 1986, to end this deficiency, the party came to the conclusion that the main contradiction in Chinese society was that between the backward productive forces and the advanced culture and ideology of China. By doing this, they deemphasized class struggle, and contradicted both Mao and Karl Marx, who both considered that class struggle was the main focus of the communist movement. According to this logic, thwarting the CPC's goal of advancing productive forces was synonymous with class struggle. The classical goal of class struggle was declared by Deng to have been achieved in 1976. While Mao had also emphasized the need to develop productive forces, under Deng it became paramount.
Party theoretician and former Politburo member Hu Qiaomu in his thesis "Observe economic laws, speed up the Four Modernizations", published in 1978, argued that economic laws were objective, on par with natural laws. He insisted that economic laws were no more negotiable "than the law of gravity". Hu's conclusion was that the Party was responsible for the socialist economy's acting on these economic laws. He believed that only an economy based on the individual would satisfy these laws, since "such an economy would be in accord with the productive forces". The CPC followed his line, and at the 12th National Congress, the party constitution was amended, stating that the private economy was a "needed complement to the socialist economy." This sentiment was echoed by Xue Muqiao; "practice shows that socialism is not necessarily based on a unified public ownership by the whole society."
The official communiqué of the 3rd plenum of the 11th Central Committee included the words: "integrate the universal principles of Marxism–Leninism–Mao Zedong Thought with the concrete practice of socialist modernization and develop it under the new historical conditions." With the words "new historical conditions", the CPC had in fact made it possible to view the old, Maoist ideology as obsolete (or at least certain tenants). To know if a policy was obsolete or not, the party had to "seek truth from facts" and follow the slogan "practice is the sole criterion of the truth". At the 6th plenum of the 11th Central Committee, the "Resolution on Certain Questions in the History of Our Party Since the Founding of the People's Republic of China" was adopted. The resolution separated Mao the person from Maoism, claiming that Mao had contravened Maoism during his rule. While the document criticized Mao, it clearly stated that he was a "proletarian revolutionary" (i.e. not all of his views were wrong), and that without Mao there would have been no new China. Su Shaozi, a party theoretician and the head of the Institute of Marxism–Leninism–Mao Zedong Thought, argued that the CPC needed to reassess the New Economic Policy introduced by Vladimir Lenin and ended by Stalin, as well as Stalin's industrialization policies and the prominent role he gave to class struggle. Su concluded that the "exploiting classes in China had been eliminated". Dong Fureng, a Deputy Director at the Institute of Economics, agreed with the reformist discourse, first by criticizing Marx and Friedrich Engels' view that a socialist society had to abolish private property, and secondly, accusing both Marx and Engels for being vague on what kind of ownership of the means of production was necessary in socialist society. While both Su and Dong agreed that it was the collectivization of agriculture and the establishment of People's Communes which had ended rural exploitation, neither of them sought a return to collectivized agriculture.
Creation of a "Socialist market economy".
The term "socialism with Chinese characteristics" was added to the General Program of the party's constitution at the 12th National Congress, without a definition of the term. At the 13th National Congress, held in 1987, Zhao Ziyang, the CPC General Secretary, claimed that socialism with Chinese characteristics was the "integration of the fundamental tenets of Marxism with the modernization drive in China" and was "scientific socialism rooted in the realities of present-day China." By this time the CPC believed that China was in the primary stage of socialism, and therefore needed market relations so as to develop into a socialist society. Two years earlier, Su had tried to internationalize the term "primary stage of socialism" by claiming that socialism contained three different production phases. China was currently in the first phase, while the Soviet Union and the remaining Eastern Bloc countries were in the second phase. Because China was in the primary stage of socialism, Zhao argued that in "[China] for a long time to come, we shall develop various sectors of the economy, always ensuring the dominant position of the public sector." Further, some individuals should be allowed to become rich "before the objective of common prosperity [pure communism] is achieved." Lastly, during the primary stage of socialism, planning would no longer be the primary means of organization of the economy. Upon hearing this remark, Chen Yun, a conservative and the second-most powerful politician in China, walked out of the meeting.
Both Chen Yun and Deng supported the formation of a private market. At the 8th National Congress, Chen first proposed an economy where the socialist sector would be dominant, with the private economy in a secondary role. He believed that by following the "Ten Major Relationships", an article by Mao on how to proceed with socialist construction, the CPC could remain on the socialist road while also supporting private property. Chen Yun conceived of the bird-cage theory, where the bird represents the free market and the cage represents a central plan. Chen proposed that a balance should be found between "setting the bird free" and choking the bird with a central plan that was too restrictive.
Between the time of the 13th National Congress and the Tiananmen Square incident and the ensuing crackdown, the line between right and left within the CPC became clearer. The rift became visible in the run-up to the 7th plenum of the 13th National Congress (in 1990), when problems arose concerning China's 8th Five-Year Plan. The draft for the 8th Five-Year Plan, supervised by Premier Li Peng and Deputy Premier Yao Yilin, openly endorsed Chen Yun's economic view that planning should be primary, coupled with slow, balanced growth. Li went further and directly contradicted Deng, stating, "Reform and opening up should not be taken as the guiding principle; instead, sustained, steady, and coordinated development should be taken as the guiding principle." Because of this stance, Deng rejected the Draft for the 8th Five-Year Plan, claiming that the 1990s was the "best time" for continuing with reform and opening up. Li and Yao even went so far as to try to annul two key resolutions passed by the 13th National Congress: the theory of socialist political civilization, and the resolution that central planning and markets were equals. Deng rejected the idea of reopening discussions on these subjects, and restated that reforms were essential for the CPC's future. Not accepting Deng's stance, party theorist Deng Liqun, along with others, began promoting "Chen Yun Thought". After a discussion with General Wang Zhen, a supporter of Chen Yun, Deng stated he would propose the abolishment of the Central Advisory Commission (CAC). Chen Yun retaliated by naming Bo Yibo to succeed him as CAC chairman. Indeed, when the 7th plenum of the 13th Central Committee did in fact convene, nothing notable took place, with both sides trying not to widen the ideological gap even further. The resolution of the 7th plenum did contain a great deal of ideological language ("firmly follow the road of socialism with Chinese characteristics"), but no clear formulation of new policy was uttered. 
Chen Yun's thoughts and policies dominated CPC discourse from 1989 until Deng's Southern Tour in 1992. Deng began campaigning for his reformist policies in 1991, managing to get reformist articles printed in the "People's Daily" and "Liberation Army" during this period. The articles criticized those communists who believed that central planning and market economics were polar opposites, instead repeating the Dengist mantra that planning and markets were only two different ways in which to regulate economic activity. By that time, the party had begun preparing for the 14th National Congress. Deng threatened to withdraw his support for Jiang Zemin's reelection as CPC General Secretary if Jiang did not accept reformist policies. However, at the 8th plenum of the 13th Central Committee, in 1991, the conservatives still held the upper hand within the party leadership.
To reassert his economic agenda, in the spring of 1992, Deng made his famous southern tour of China, visiting Guangzhou, Shenzhen, and Zhuhai, and spending the New Year in Shanghai. He used his travels to reassert his economic policy ideas after his retirement from office. On the tour, Deng made many speeches and generated large local support for his reformist platform. He stressed the importance of economic reform in China, and criticized those who were against further reform and opening up. The tour proved that amongst the party's grassroots organizations, support for reform and opening up was firm. Because of this, more and more leading members of the central party leadership converted to Deng's position, amongst them Jiang Zemin. In his speech "Deeply Understand and Implement Comrade Deng Xiaoping's Important Spirit, Make Economic Construction, Reform and Opening Go Faster and Better" to the Central Party School, Jiang said it did not matter if a certain mechanism was capitalist or socialist, the key question was whether it worked. Jiang's speech is notable since it introduced the term socialist market economy, which replaced Chen Yun's "planned socialist market economy". In a later Politburo meeting, members voted unanimously, in old communist fashion, to continue with reform and opening up. Knowing that he had lost, Chen Yun gave in, and claimed that because of new conditions, the old techniques of the planned economy were outdated.
At the 14th National Congress, the thought of Deng Xiaoping was officially dubbed Deng Xiaoping Theory, and elevated to the same level as Mao Zedong Thought. The concepts of "socialism with Chinese characteristics" and "the primary stage of socialism" were credited to him. At the congress, Jiang reiterated Deng's view that it was unnecessary to ask if something was socialist or capitalist, since the important factor was whether it worked. Several capitalist techniques were introduced, while science and technology were to be the primary productive force.
Three Represents.
The term ″Three Represents″ was first used in 2000 by Jiang Zemin in a trip to Guangdong province. From then until its inclusion in the party's constitution at the 16th National Congress, the Three Represents became a constant theme for Jiang Zemin. In his speech at the anniversary of the founding of the PRC, Jiang Zemin said that "we [the CPC] must always represent the development trend of China's advanced productive forces, the orientation of China's advanced culture, and the fundamental interests of the overwhelming majority of the people in China." By this time, Jiang and the CPC had reached the conclusion that attaining the communist mode of production, as formulated by earlier communists, was more complex than had been realized, and that it was useless to try to force a change in the mode of production, as it had to develop naturally, by following the economic laws of history. While segments within the CPC criticized the Three Represents as being un-Marxist and a betrayal of basic Marxist values, supporters viewed it as a further development of socialism with Chinese characteristics. The theory is most notable for allowing capitalists, officially referred to as the "new social strata", to join the party on the grounds that they engaged in "honest labor and work" and through their labour contributed "to build[ing] socialism with Chinese characteristics." Jiang contended that capitalists should be able to join the Party on the grounds that;
"It is not advisable to judge a person’s political orientation simply by whether he or she owns property or how much property he or she owns [...] Rather, we should judge him or her mainly by his or her political awareness, moral integrity and performance, by how he or she has acquired the property, how it has been disposed of and used, and by his or her actual contribution to the cause of building socialism with Chinese characteristics."
Scientific Outlook on Development.
The 3rd plenum of the 16th Central Committee conceived and formulated the ideology of Scientific Outlook on Development. This concept is generally considered to be Hu Jintao's contribution to the official ideological discourse. It is considered a continuation and creative development of ideologies advanced by previous CPC leaders. To apply the Scientific Outlook on Development on China, the CPC must adhere to building a Harmonious Socialist Society. According to Hu Jintao, the concept is a sub-ideology of socialism with Chinese characteristics. It is a further adaptation of Marxism to the specific conditions of China, and a concept open to change.
Views on capitalism.
The CPC do not believe that they have abandoned Marxism. The party views the world as organized into two opposing camps; socialist and capitalist. They insist that socialism, on the basis of historical materialism, will eventually triumph over capitalism. In recent years, when the party has been asked to explain the capitalist globalization occurring, the party has returned to the writings of Karl Marx. Marx wrote that capitalists, in their search for profit, would travel the world in a bid to establish new international markets – hence, it's generally assumed that Marx forecasted globalization. His writings on the subject is used to justify the CPC's market reforms, since nations, according to Marx, have little choice in the matter of joining or not. Opting not to take part in capitalist globalization means losing out in the fields of economic development, technological development, foreign investment and world trade. This view is strengthened by the economic failures of the Soviet Union and of China under Mao.
Despite admitting that globalization developed through the capitalist system, the party's leaders and theorist argue that globalization is not intrinsically capitalist. The reason being that if globalization was purely capitalist, it would exclude an alternate socialist form of modernity. Globalization, as with the market economy, therefore does not have one specific class character (either socialist or capitalist) according to the party. The instance that globalization is not fixed in nature, comes from Deng's insistence that China can pursue socialist modernization by incorporating elements of capitalism. Because of this there is considerable optimism within the CPC that despite the current capitalist dominance of globalization, globalization can be turned into a vehicle supporting socialism. This event will occur through capitalism's own contradictions. These contradictions are, according to party theorist Yue Yi from the Chinese Academy of Social Sciences, "that between private ownership of the means of production and socialised production. This contradiction has manifested itself globally as the following contradictions; the contradiction between planned and regulated national economies and the unplanned and unregulated world economy; the contradiction between well-organized and scientifically managed Transnational Corporations (TNCs) and a blindly expanding and chaotic world market; the contradiction between the unlimited increase of productive capacity and the limited world market; and the contradiction between sovereign states and TNCs." It was these contradictions, argue Yue Yi, that led to the dot-com bubble of the 1990s, that has caused unbalanced development and polarization, and widened the gap between rich and poor. These contradictions will lead to the inevitable demise of capitalism and the resultant dominance of socialism.
Economics.
Deng Xiaoping, the leading figure in the reform era, did not believe that the fundamental difference between the capitalist mode of production and the socialist mode of production was central planning versus free markets. He said, "A planned economy is not the definition of socialism, because there is planning under capitalism; the market economy happens under socialism, too. Planning and market forces are both ways of controlling economic activity". Jiang Zemin supported Deng's thinking, and stated in a party gathering that it did not matter if a certain mechanism was capitalist or socialist, because the only thing that mattered was whether it worked. It was at this gathering that Jiang Zemin introduced the term socialist market economy, which replaced Chen Yun's "planned socialist market economy". In his report to the 14th National Congress Jiang Zemin told the delegates that the socialist state would "let market forces play a basic role in resource allocation." At the 15th National Congress, the party line was changed to "make market forces further play their role in resource allocation"; this line continued until the 3rd Plenary Session of the 18th Central Committee, when it was amended to "let market forces play a "decisive" role in resource allocation." Despite this, the 3rd Plenary Session of the 18th Central Committee upheld the creed "Maintain the dominance of the public sector and strengthen the economic vitality of the State-owned economy."
Stance on religion.
The CPC, as an officially atheist institution, prohibits party members from belonging to a religion. Although religion is banned for party members, personal beliefs are not held accountable. During Mao's rule, religious movements were oppressed, and religious organizations were forbidden to have contact with foreigners. All religious organizations were state-owned and not independent. Relations with foreign religious institutions were worsened when in 1947, and again in 1949, the Vatican forbade any Catholic to support a communist party. On questions of religion, Deng was more open than Mao, but the issue was left unresolved during his leadership. According to Ye Xiaowen, the former Director of the State Administration for Religious Affairs, "In its infancy, the socialist movement was critical of religion. In Marx’s eyes, theology had become a bastion protecting the feudal ruling class in Germany. Therefore the political revolution had to start by criticizing religion. It was from this perspective that Marx said ‘religion is the opium of the people’." It was because of Marx's writings that the CPC initiated anti-religious policies under Mao and Deng. The Marxist view that religion would decline as modern society emerged was proven false (they believed) with the rise of Falun Gong.
The popularity of Falun Gong, and its subsequent banning by state authorities, led to the convening of a three-day National Work Conference for Religious Affairs in 1999, the highest-level gathering on religious affairs in the party's history. Jiang Zemin, who had subscribed to the classical Marxist view that religion would wither away, was forced to change his mind when he learnt that religion in China was in fact growing, not decreasing. In his concluding speech to the National Work Conference, Jiang asked the participants to find a way to make "socialism and religion adapt to each other". He added that "asking religions to adapt to socialism doesn’t mean we want religious believers to give up their faith". Jiang ordered Ye Xiaowen to study the classical Marxist works in depth to find an excuse to liberalize the CPC's policy towards religion. It was discovered that Friedrich Engels had written that religion would survive as long as problems existed. With this rationale, religious organizations were given more autonomy.
Party-to-party relations.
Communist parties.
The CPC continues to have relations with non-ruling communist and workers' parties and attends international communist conferences, most notably the International Meeting of Communist and Workers' Parties. Delegates of foreign communist parties still visit China; in 2013, for instance, the General Secretary of the Portuguese Communist Party (PCP), Jeronimo de Sousa, personally met with Liu Qibao, a member of the Central Politburo. In another instance, Pierre Laurent, the National Secretary of the French Communist Party (FCP), met with Liu Yunshan, a Politburo Standing Committee member. While the CPC retains contact with major parties such as the PCP, FCP, the Communist Party of the Russian Federation, the Communist Party of Bohemia and Moravia, the Communist Party of Brazil, the Communist Party of Nepal (Unified Marxist−Leninist) and the Communist Party of Spain, the party also retains relations with minor communist and workers' parties, such as the Communist Party of Australia, the Workers Party of Bangladesh, the Communist Party of Bangladesh (Marxist–Leninist) (Barua), the Communist Party of Sri Lanka, the Workers' Party of Belgium, the Hungarian Workers' Party, the Dominican Workers' Party and the Party for the Transformation of Honduras, for instance. In recent years, noting the self-reform of the European social democratic movement in the 1980s and 1990s, the CPC "has noted the increased marginalization of West European communist parties."
Ruling parties of socialist states.
The CPC has retained close relations with the remaining socialist states still espousing communism: Cuba, Laos, North Korea and Vietnam and their respective ruling parties. It spends a fair amount of time analyzing the situation in the remaining socialist states, trying to reach conclusions as to why these states survived when so many did not, following the collapse of the Eastern European socialist states in 1989 and the dissolution of the Soviet Union in 1991. In general, the analyses of the remaining socialist states and their chances of survival have been positive, and the CPC believes that the socialist movement will be revitalized sometime in the future.
The ruling party which the CPC is most interested in is the Communist Party of Vietnam (CPV). In general the CPV is considered a model example of socialist development in the post-Soviet era. Chinese analysts on Vietnam believe that the introduction of the Doi Moi reform policy at the 6th CPV National Congress is the key reason for Vietnam's current success.
While the CPC is probably the organization with most access to North Korea, writing about North Korea is tightly circumscribed. The few reports accessible to the general public are those about North Korean economic reforms. While Chinese analysts of North Korea tend to speak positively of North Korea in public, in official discussions they show much disdain for North Korea's economic system, the cult of personality which pervades society, the Kim family, the idea of hereditary succession in a socialist state, the security state, the use of scarce resources on the Korean People's Army and the general impoverishment of the North Korean people. There are those analysts who compare the current situation of North Korea with that of China during the Cultural Revolution. Over the years, the CPC has tried to persuade the Workers' Party of Korea (or WPK, North Korea's ruling party) to introduce economic reforms by showing them key economic infrastructure in China. For instance, in 2006 the CPC invited the WPK General Secretary Kim Jong-il to Guandong province to showcase the success economic reforms have brought China. In general, the CPC considers the WPK and North Korea to be negative examples of a communist ruling party and socialist state.
There is a considerable degree of interest in Cuba within the CPC. Fidel Castro, the former First Secretary of the Communist Party of Cuba (PCC), is greatly admired, and books have been written focusing on the successes of the Cuban Revolution. Communication between the CPC and the PCC has increased considerably since the 1990s, hardly a month going by without a diplomatic exchange. At the 4th Plenary Session of the 16th Central Committee, which discussed the possibility of the CPC learning from other ruling parties, praise was heaped on the PCC. When Wu Guanzheng, a Central Politburo member, met with Fidel Castro in 2007, he gave him a personal letter written by Hu Jintao: "Facts have shown that China and Cuba are trustworthy good friends, good comrades, and good brothers who treat each other with sincerity. The two countries' friendship has withstood the test of a changeable international situation, and the friendship has been further strengthened and consolidated."
Non-communist parties.
Since the decline and fall of communism in Eastern Europe, the CPC has begun establishing party-to-party relations with non-communist parties. These relations are sought so that the CPC can learn from them. For instance, the CPC has been eager to understand how the People's Action Party of Singapore (PAP) maintains its total domination over Singaporean politics through its "low-key presence, but total control." According to the CPC's own analysis of Singapore, the PAP's dominance can be explained by its "well-developed social network, which controls constituencies effectively by extending its tentacles deeply into society through branches of government and party-controlled groups." While the CPC accepts that Singapore is a democracy, they view it as a guided democracy led by the PAP. Other differences are, according to the CPC, "that it is not a political party based on the working class—instead it is a political party of the elite ... It is also a political party of the parliamentary system, not a revolutionary party." Other parties the CPC studies and maintains strong party-to-party relations with are the United Malays National Organisation, which has ruled Malaysia democratically since 1957, and the Liberal Democratic Party in Japan, which dominated Japanese politics from 1955 to 2009. The KMT is another case entirely, where party-to-party relations are retained so as to strengthen the probability of the reunification of Taiwan with mainland China. However, several studies have been written on the KMT's loss of power in 2000, after having ruled Taiwan since 1949 (the KMT officially ruled China, then called the Republic of China, from 1928 to 1949). In general, one-party states or dominant-party states are of special interest to the party, and party-to-party relations are formed so that the CPC can study them. For instance, the longevity of the Syrian Regional Branch of the Arab Socialist Ba'ath Party is attributed to the personalization of power in the al-Assad family, the strong presidential system, the inheritance of power, which passed from Hafez al-Assad to his son Bashar al-Assad, and the role given to the Syrian military in politics.
In recent years, the CPC has been especially interested in Latin America, as shown by the increasing number of delegates sent to and received from these countries. Of special fascination for the CPC is the 71-year-long rule of the Institutional Revolutionary Party (PRI) in Mexico. While the CPC attributed the PRI's long reign in power to the strong presidential system, tapping into the machismo culture of the country, its nationalist posture, its close identification with the rural populace and the implementation of nationalization alongside the marketization of the economy, the CPC concluded that the PRI failed because of the lack of inner-party democracy, its pursuit of social democracy, its rigid party structures that could not be reformed, its political corruption, the pressure of globalization, and American interference in Mexican politics. While the CPC was slow to recognize the Pink tide in Latin America, it has strengthened party-to-party relations with several socialist and anti-American political parties over the years. There may have been some irritation over Hugo Chavez's anti-capitalist and anti-American rhetoric on the CPC's part. Despite this, in 2013 the CPC reached an agreement with the United Socialist Party of Venezuela (PSUV), the party founded by Chavez, for the CPC to educate PSUV cadres in political and social fields. By 2008, the CPC claimed to have established relations with 99 political parties in 29 Latin American countries.
European social democracy has been of great interest to the CPC since the early 1980s. With the exception of a short period in which the CPC forged party-to-party relations with far-right parties during the 1970s in an effort to halt "Soviet expansionism", the CPC's relations with European social democratic parties were its first serious efforts to establish cordial party-to-party relations with non-communist parties. The CPC credits the European social democrats with creating a "capitalism with a human face". Before the 1980s, the CPC had a highly negative and dismissive view of social democracy, a view dating back to the Second International and the Leninist and Stalinist view on the social democratic movement. By the 1980s that view had changed, and the CPC concluded that it could actually learn something from the social democratic movement. CPC delegates were sent all over Europe to observe. It should be noted that by the 1980s most European social democratic parties were facing electoral decline, and were in a period of self-reform. The CPC followed this with great interest, laying most weight on reform efforts within the British Labour Party and the Social Democratic Party of Germany. The CPC concluded that both parties were reelected because they modernized, replacing traditional state socialist tenets with new ones supporting privatization, shedding the belief in big government, conceiving a new view of the welfare state, changing negative views of the market, and moving from their traditional support base of trade unions to entrepreneurs, younger members and students.

</doc>
<doc id="7176" url="http://en.wikipedia.org/wiki?curid=7176" title="Cryogenics">
Cryogenics

In physics, cryogenics is the study of the production and behaviour of materials at very low temperatures (below −150 °C, −238 °F or 123 K). A person who studies elements that have been subjected to extremely cold temperatures is called a cryogenicist. Rather than the relative temperature scales of Celsius and Fahrenheit, cryogenicists use the absolute temperature scales. These are Kelvin (SI units) or Rankine scale (Imperial and US units).
The term cryogenics is often mistakenly used in fiction and popular culture to refer to the very different cryonics.
Etymology.
The word "cryogenics" stems from Greek and means "the production of freezing cold"; however, the term is used today as a synonym for the low-temperature state. It is not well-defined at what point on the temperature scale refrigeration ends and cryogenics begins, but most scientists assume it starts at or below . The National Institute of Standards and Technology at Boulder, Colorado has chosen to consider the field of cryogenics as that involving temperatures below . This is a logical dividing line, since the normal boiling points of the so-called permanent gases (such as helium, hydrogen, neon, nitrogen, oxygen, and normal air) lie below −180 °C while the Freon refrigerants, hydrogen sulfide, and other common refrigerants have boiling points above −180 °C.
Industrial applications.
Liquefied gases, such as liquid nitrogen and liquid helium, are used in many cryogenic applications. Liquid nitrogen is the most commonly used element in cryogenics and is legally purchasable around the world. Liquid helium is also commonly used and allows for the lowest attainable temperatures to be reached.
These liquids may be stored in Dewar flasks, which are double-walled containers with a high vacuum between the walls to reduce heat transfer into the liquid. Typical laboratory Dewar flasks are spherical, made of glass and protected in a metal outer container. Dewar flasks for extremely cold liquids such as liquid helium have another double-walled container filled with liquid nitrogen. Dewar flasks are named after their inventor, James Dewar, the man who first liquefied hydrogen. "Thermos" ® bottles are smaller vacuum flasks fitted in a protective casing.
Cryogenic barcode labels are used to mark dewar flasks containing these liquids, and will not frost over down to -195 degrees Celsius. 
Cryogenic transfer pumps are the pumps used on LNG piers to transfer liquefied natural gas from LNG carriers to LNG storage tanks, as are cryogenic valves.
Cryogenic processing.
The field of cryogenics advanced during World War II when scientists found that metals frozen to low temperatures showed more resistance to wear. Based on this theory of cryogenic hardening, the commercial cryogenic processing industry was founded in 1966 by Ed Busch. With a background in the heat treating industry, Busch founded a company in Detroit called CryoTech in 1966 which merged with 300 Below in 1999 to become the world's largest and oldest commercial cryogenic processing company. Busch originally experimented with the possibility of increasing the life of metal tools to anywhere between 200%-400% of the original life expectancy using cryogenic tempering instead of heat treating. This evolved in the late 1990s into the treatment of other parts.
Cryogens, such as liquid nitrogen, are further used for specialty chilling and freezing applications. Some chemical reactions, like those used to produce the active ingredients for the popular statin drugs, must occur at low temperatures of approximately . Special cryogenic chemical reactors are used to remove reaction heat and provide a low temperature environment. The freezing of foods and biotechnology products, like vaccines, requires nitrogen in blast freezing or immersion freezing systems. Certain soft or elastic materials become hard and brittle at very low temperatures, which makes cryogenic milling (cryomilling) an option for some materials that cannot easily be milled at higher temperatures.
Cryogenic processing is not a substitute for heat treatment, but rather an extension of the heating - quenching - tempering cycle. Normally, when an item is quenched, the final temperature is ambient. The only reason for this is that most heat treaters do not have cooling equipment. There is nothing metallurgically significant about ambient temperature. The cryogenic process continues this action from ambient temperature down to .
In most instances the cryogenic cycle is followed by a heat tempering procedure. As all alloys do not have the same chemical constituents, the tempering procedure varies according to the material's chemical composition, thermal history and/or a tool's particular service application.
The entire process takes 3–4 days.
Fuels.
Another use of cryogenics is cryogenic fuels for rockets with liquid hydrogen as the most widely used example. Liquid oxygen (LOX) is even more widely used but as an oxidizer, not a fuel. NASA's workhorse space shuttle used cryogenic hydrogen/oxygen propellant as its primary means of getting into orbit. LOX is also widely used with RP-1 kerosene, a non-cryogenic hydrocarbon, such as in the rockets built for the Soviet space program by Sergei Korolev.
Russian aircraft manufacturer Tupolev developed a version of its popular design Tu-154 with a cryogenic fuel system, known as the Tu-155. The plane uses a fuel referred to as liquefied natural gas or LNG, and made its first flight in 1989.
Other applications.
Some applications of cryogenics:
Production.
Cryogenic cooling of devices and material is usually achieved via the use of liquid nitrogen, liquid helium, or a cryocompressor (which uses high pressure helium lines). Newer devices such as pulse cryocoolers and Stirling cryocoolers have been devised. The most recent development in cryogenics is the use of magnets as regenerators as well as refrigerators. These devices work on the principle known as the magnetocaloric effect.
Detectors.
Cryogenic temperatures, usually well below 77 K (−196 °C) are required to operate cryogenic detectors.

</doc>
<doc id="7179" url="http://en.wikipedia.org/wiki?curid=7179" title="Cary Elwes">
Cary Elwes

Ivan Simon Cary Elwes (; born 26 October 1962), known professionally as Cary Elwes, is an English actor and voice actor.
He acted in off-Broadway plays during college and moved from England to the United States in the early 1980s. He is known for his roles as Westley in "The Princess Bride", Arthur Holmwood in Francis Ford Coppola's "Bram Stoker's Dracula", Robin Hood in "", Garrett in "Quest for Camelot", and Dr. Lawrence Gordon in "Saw" and "Saw 3D: The Final Chapter". 
He appeared in box office hits such as "Days of Thunder", "Hot Shots!", "Twister", "Liar, Liar" and "New Year's Eve". He has had recurring roles in television series such as "The X-Files" playing Brad Follmer and "Psych" playing Pierre Despereaux.
Early life.
Elwes was born in Westminster, London, England. He is the youngest of three boys born to portrait-painter Dominic Elwes and interior designer and socialite Tessa Kennedy. His brothers are Damian Elwes, an artist, and Cassian Elwes, a producer and agent. He was the stepson of Elliott Kastner, an American film producer. Elwes' paternal grandfather was painter Simon Elwes, whose father was the diplomat and tenor Gervase Elwes (1866–1921). His other great-grandfathers were diplomat Rennell Rodd, 1st Baron Rennell, Sir John Macfarlane Kennedy, and Croatian industrialist Ivan Rikard Ivanović. Elwes' ancestry is English, Irish, Scottish, Croatian Jewish, and Serbian (the latter two through his maternal grandmother, Daška McLean, born Daška Marija Ivanović-Banac; her brother, Elwes' great-uncle, was diplomat Vane Ivanović). One of Elwes' ancestors is John Elwes, who is alleged in some sources to have been the inspiration for Ebenezer Scrooge in "A Christmas Carol" (1843) (Elwes played five roles in the 2009 film adaptation of the novel).
He was brought up as a Roman Catholic and was an altar boy at Westminster Cathedral, although he did not attend denominational schools, unlike most of the men on his father's side of the family, including his own father. Elwes attended Harrow School in London and then the London Academy of Music and Dramatic Art. He discussed some of this background in an interview while he was filming the 2005 CBS television film, "Pope John Paul II", in which Elwes played the young priest Karol Wojtyła, many years before he was elected pontiff in 1978.
His parents divorced when he was four years old, and in 1975 when Elwes was thirteen his father committed suicide. In 1981, he returned to the United States to study acting at Sarah Lawrence College in Bronxville, New York. While living in New York, Elwes studied acting at both the Actors Studio and the Lee Strasberg Theatre and Film Institute.
Elwes worked as a production assistant on the films "Octopussy" and "Superman", where he worked for a week assigned to Marlon Brando. When Elwes introduced himself, Brando told him he was lying and that his (Elwes') name was actually Rocky.
Career.
Film.
Elwes made his acting debut in 1984 with Marek Kanievska's film "Another Country". He played James Harcourt, a young and sentimental homosexual student from an English boarding school. He went on to play Guilford Dudley in the British film "Lady Jane", co-starring Helena Bonham Carter. He was cast as a stable boy turned swashbuckler Westley in Rob Reiner's fantasy-comedy "The Princess Bride", based on the novel of the same name by William Goldman. It was a modest box office success, but received critical acclaim, earning a score of 96% on the review aggregation website Rotten Tomatoes. Since being released on home video and television it has become a cult classic. In an interview around the film's DVD release in 2001, Elwes said, "The studio didn't know how to sell itas an adventure, fantasy, comedy or love story, it had to rely on word of mouth". He also acknowledged the film's cult following saying, "Many people tell me they have it in their video collection, it's a family film but also a cult film in a way, being passed down to other generations".
He continued working steadily, varying between dramatic roles, as in "The Bride" (1985) with Sting and Jennifer Beals, to the Academy Award-winning "Glory" (1989), and comedic roles, as in "Hot Shots!" (1991). In 1993, he starred as Robin Hood in Mel Brooks's comedy, "". Elwes also appeared in such films as Francis Coppola's adaptation of "Bram Stoker's Dracula", "The Crush", "Rudyard Kipling's The Jungle Book", "Twister", "Liar Liar", and "Kiss the Girls". In 1999, he portrayed famed theatre and film producer John Houseman for Tim Robbins in his ensemble film based on Orson Welles's musical, Cradle Will Rock. And months after that he traveled to Luxembourg to work with John Malkovich and Willem Dafoe in the Academy Award Nominated "Shadow of the Vampire". In 2001, he traveled to Berlin to work on Peter Bogdanovich’s "The Cat’s Meow" portraying the doomed movie mogul Thomas Ince. 
In 2004, he starred in the horror–thriller "Saw" which, at a budget of a little over $1 million, grossed over $100 million worldwide. The same year he appeared in "Ella Enchanted", this time as the villain not the hero. He made an uncredited appearance as Sam Green, the man who introduced Andy Warhol to Edie Sedgwick, in the 2006 film "Factory Girl". In 2007, he appeared in Garry Marshall's "Georgia Rule" opposite Jane Fonda. Two years later he appeared in Robert Zemeckis’s motion capture adaptation of Charles Dickens' "A Christmas Carol" portraying five roles. That same year he was chosen by Steven Spielberg to appear in his motion capture adaptation of Belgian artist Hergé's popular comic strip "The Adventures of Tintin".
Elwes returned to the "Saw" franchise in "Saw 3D" (2010), the seventh and final film in the series, as Dr. Lawrence Gordon. Elwes was set to portray George Harrison in Zemeckis's 3D motion capture re-telling of "Yellow Submarine"; however in May 2011, Disney withdrew from the project. 
In 2011, Elwes was chosen by Ivan Reitman to star alongside Natalie Portman in his comedy "No Strings Attached". That same year, he and Garry Marshall teamed up together again when he joined the ensemble of romantic comedy New Year’s Eve opposite Robert de Niro.
In 2012, Elwes appeared in the independent drama "The Citizen". In 2013 Elwes joined Selena Gomez for the comedy ensemble, "Behaving Badly" directed by Tim Garrick. This year he has completed four movies: John Herzfeld's comedy "Reach Me" opposite Sylvester Stallone; the ensemble drama, "Sugar Mountain" directed by Richard Gray; and the dramas "H8RZ" directed by Derrick Borte and "The Greens Are Gone" directed by Peer Pedersen opposite Catherine Keener.
On 13 December 2012, "The Hollwood Reporter" announced that Elwes will make his directorial debut with an independent film about the life of Kit Lambert, manager of the iconic rock group The Who, working with a script by Pat Gilbert, a former editor of the British music magazine "Mojo". The film is based on interviews and recordings with Lambert made by journalist Jon Lindsay, and is being produced by Orian Williams.
Television.
In 1996, Elwes made his first television appearance as David Lookner on the sitcom "Seinfeld". In 1998, he played astronaut Michael Collins in the Golden Globe Award-winning HBO miniseries "From the Earth To the Moon". The following year Elwes was nominated for a Golden Satellite Award for Best Performance by an Actor in a Mini-Series or Motion Picture Made for Television for his portrayal of Colonel James Burton in The Pentagon Wars directed by Richard Benjamin. In 1999, he guest starred as Dr. John York in an episode of the television series "The Outer Limits". Shortly afterward he received another Golden Satellite Award nomination for his work on the ensemble NBC Television movie "Uprising" opposite Jon Voigt directed by Jon Avnet. Elwes had a recurring role in the final season (from 2001 to 2002) of "The X-Files" as FBI Assistant Director Brad Follmer. In 2004, he received great praise with his portrayal of serial killer Ted Bundy in the A&E Network film "The Riverman", which became one of the highest rated original movies in the network’s history and garnered a prestigious BANFF Rockie Award nomination. The following year, Elwes played the young Pope John Paul II in the CBS television film "Pope John Paul II" opposite Jon Voigt again. The TV film was highly successful not only in North America, but also in Europe, where it broke box office records in the late pope's native Poland and became the first film ever to break $1 million (GBP588,200) in three days. 
In 2007, he made a guest appearance on the " episode " as a Mafia lawyer. In 2009, he played the role of Pierre Despereaux, an international art thief, in the fourth season premiere of "Psych". In 2010, he returned to "Psych", reprising his role in the second half of the fifth season, and again in the show's sixth season. In March 2011, Elwes was selected to appear as Henry Detmer in the pilot episode of NBC's "Wonder Woman". However, the show was never picked up for a series. Elwes recently completed work on the USA Network TV movie, , produced by Gale Anne Hurd. In 2014, Elwes began playing Hugh Ashmeade, the CIA Director, in the second season of the series, "Granite Flats".
Voiceovers.
Elwes' voice-over work includes the narrator in James Patterson's audio book "The Jester", as well as characters in film and television animations such as "Quest for Camelot", "Pinky and The Brain", "Batman Beyond", and the English versions of the Studio Ghibli films "Porco Rosso", "Whisper of the Heart" and "The Cat Returns". For the 2004 video game "The Bard's Tale", he served as scriptwriter, improviser, and voice actor of the main character The Bard. In 2009, Elwes performed the English voice over for Indian film, "Delhi Safari". The following year Elwes portrayed the part of Gremlin Gus in Disney's video game, "".
Theatre.
In 2003, he performed in Bob Balaban-directed Off-Broadway dramatic stage play "The Exonerated" in New York during its 18–23 March, week run.
Literature.
In October 2014, Touchstone (Simon & Schuster) will publish Elwes’s memoir of the making of The Princess Bride, entitled "As You Wish: Inconceivable Tales from the Making of The Princess Bride". The book is filled with never-before-told stories, exclusive photographs, and interviews with costars Robin Wright, Wallace Shawn, Billy Crystal, Christopher Guest, and Mandy Patinkin, as well as author and screenwriter William Goldman, producer Norman Lear, and director Rob Reiner.
Personal life.
Elwes met still photographer Lisa Marie Kurbikoff in 1991 at a Malibu chili cook-off and became engaged in 1997. They married in 2000 and have one child Dominique Elwes (born April 24 2007) 
In August 2005, Elwes filed a lawsuit against Evolution Entertainment, his management firm and producer of "Saw". He said he was promised a minimum of one percent of the producers' net profits of the film and did not receive the full amount. The case eventually was settled out of court. In 2010, he went on to reprise his role in "Saw 3D".

</doc>
<doc id="7180" url="http://en.wikipedia.org/wiki?curid=7180" title="Chris Sarandon">
Chris Sarandon

Christopher "Chris" Sarandon, Jr., pronounced sa-RAN-dan (born July 24, 1942) is an American actor who is best known for playing Prince Humperdinck in the film "The Princess Bride", the vampire Jerry Dandrige in "Fright Night" and Detective Mike Norris in the first entry of the "Child's Play" series, and for providing the speaking voice of Jack Skellington from "The Nightmare Before Christmas". He was nominated for an Academy Award for Best Supporting Actor for his performance as Leon in "Dog Day Afternoon".
Early life.
Sarandon was born and raised in Beckley, West Virginia, the son of restaurateurs Cliffie (née Cardullias) and Christopher "Chris" Sarandon, Sr. His mother is of Greek descent, and his father, whose surname was originally "Sarondonethes", was born in Istanbul, Turkey, of Greek descent.
He graduated from Woodrow Wilson High School in Beckley. He went on to West Virginia University and earned his degree in performing arts. He earned his master's degree in theater from The Catholic University of America (CUA) in Washington, D.C.. While attending the university, he fell in love with Susan Tomalin, a classmate whom he married in 1967. After graduating, they both pursued acting careers, with Susan taking his surname as her professional name and performing under the name Susan Sarandon. They would later divorce in 1979.
Career.
After graduation, he toured with numerous improv companies and became heavily involved in regional theatre, making his professional debut in "The Rose Tattoo" in 1965. In 1968, Sarandon moved to New York, where he landed his first television role as Dr. Tom Halverson on "The Guiding Light" (1973 – 1974). He appeared in the primetime TV movies "The Satan Murders" (1974) and "Thursday's Game" before landing the role of Al Pacino's transsexual wife in "Dog Day Afternoon" (1975), a performance which earned him nominations for Best New Male Star of the Year at the Golden Globes and the Academy Award for Best Supporting Actor.
Sarandon appeared in "The Rothschilds" and "The Two Gentlemen of Verona" on Broadway, as well making regular appearances at numerous Shakespeare and George Bernard Shaw festivals in the United States and Canada. He also appeared in a series of television roles, some of which (such as "A Tale of Two Cities" in 1980) mirrored his affinity for the classics. He also took roles in horror films opposite the late Margaux Hemingway in the thriller "Lipstick" (1976) and as a demon in the shocker "The Sentinel" (1977).
To avoid being type cast as villainous characters, Sarandon took on various roles in the years to come, portraying the title role in the made-for-television movie "The Day Christ Died" (1980). He received accolades for his portrayal of Sydney Carton in a made for television version of "A Tale of Two Cities" (1980), co-starred with Dennis Hopper in "The Osterman Weekend" (1983), which was based on the Robert Ludlum novel of the same name, and co-starred with Goldie Hawn in "Protocol" (1984). These were followed by another mainstream success as the vampire-next-door in the teen horror film "Fright Night" (1985).
He is best known in the film industry for his role as Prince Humperdinck in Rob Reiner's 1987 film "The Princess Bride", though he also has had supporting parts in some other successful films such as the original "Child's Play" (1988). In 1992, he played Joseph Curwen/Charles Dexter Ward in "The Resurrected". He also provided the voice of Jack Skellington, the main character in Tim Burton's animated Disney film "The Nightmare Before Christmas" (1993), and has since reprised the role in many other subsequent productions, including the Disney/Square video games "Kingdom Hearts" and "Kingdom Hearts II" and the Capcom sequel to the original film, "". Sarandon also reprised his role as Jack Skellington for "Halloween Screams" and the Haunted Mansion Holiday, a three-month overlay of the Haunted Mansion at Disneyland, where Jack and his friends take over the Mansion in an attempt to take over Christmas, much as his character did in the film.
Sarandon would later find work on television again with a recurring role as Dr. Burke on NBC's long-running medical drama "ER".
In 1991 he performed on Broadway in the short-lived musical "Nick & Nora" (based on the "Thin Man" film) with Joanna Gleason, the daughter of Monty Hall. Sarandon married Gleason in 1994. They have appeared together in a number of films, including "Edie & Pen" (1996), "American Perfekt" (1997) and "Let the Devil Wear Black" (1999). In the 2000s he made guest appearances in quite a few TV series, notably as the Necromancer demon, Armand, in "Charmed", and as superior court judge Barry Krumble in six episodes of "Judging Amy".
He returned to Broadway in 2006 playing Signor Naccarelli in the six-time Tony award-winning Broadway musical "The Light in the Piazza" at Lincoln Center. Most recently he appeared in "Cyrano de Bergerac" as Antoine de Guiche, alongside Kevin Kline, Jennifer Garner and Daniel Sunjata. He is on the Advisory Board for the Greenbrier Valley Theatre in Lewisburg, West Virginia.

</doc>
<doc id="7182" url="http://en.wikipedia.org/wiki?curid=7182" title="Christopher Guest">
Christopher Guest

Christopher Haden-Guest, 5th Baron Haden-Guest (born February 5, 1948), better known as Christopher Guest, is an English-American screenwriter, composer, musician, director, actor, and comedian who holds dual British and American citizenship. He is most widely known in Hollywood for having written, directed and starred in several improvisational "mockumentary" films featuring an ensemble cast. This series of films began with "This Is Spinal Tap", and continued with "Waiting for Guffman", "Best In Show", "A Mighty Wind", and "For Your Consideration". Guest also had a featured role as the evil six-fingered Count Rugen in the film "The Princess Bride".
He holds a hereditary British peerage as the 5th Baron Haden-Guest, and has publicly expressed a desire to see the House of Lords reformed as a democratically elected chamber. Though he was initially active in the Lords, his career there was cut short by the House of Lords Act 1999. When using his title, he is normally styled as Lord Haden-Guest. Guest is married to the actress and author Jamie Lee Curtis.
Early years.
Guest was born in New York City, the son of Peter Haden-Guest, a British United Nations diplomat who later became the 4th Baron Haden-Guest, and his second wife, Jean Pauline Hindes, a former vice president of casting at CBS. Guest's paternal grandfather, Leslie, Baron Haden-Guest, was a Labour Party politician who was a convert to Judaism, and Guest's paternal grandmother's father was Colonel Albert Goldsmid, a British officer who founded the Jewish Lads' and Girls' Brigade and the Maccabaeans. Guest's maternal grandparents were Jewish immigrants from Russia. Both of Guest's parents had become atheists, and Guest had no religious upbringing. Nearly a decade before he was born, his uncle, David Guest, a lecturer and Communist Party member, was killed in the Spanish Civil War fighting in the International Brigades.
Guest spent parts of his childhood in his father's native United Kingdom. He attended The High School of Music & Art (New York City), studying classical music (clarinet) and the Stockbridge School in Interlaken, MA. He later took up the mandolin, became interested in country music, and played guitar with Arlo Guthrie a fellow student at Stockbridge School. Guest later began performing with bluegrass bands until he took up rock and roll.
Guest studied acting at New York University's Graduate Acting Program at the Tisch School of the Arts, graduating in 1971.
Career.
1970s.
Guest began his career in theatre during the early 1970s with one of his earliest professional performances being the role of Norman in Michael Weller's "Moonchildren" for the play's American premiere at the Arena Stage in Washington D.C. in November 1971. Guest continued with the production when it moved to Broadway in 1972. The following year he began making contributions to "The National Lampoon Radio Hour" for a variety of National Lampoon audio recordings. He both performed comic characters (Flash Bazbo—Space Explorer, Mr. Rogers, music critic Roger de Swans, and sleazy record company rep Ron Fields) and wrote, arranged and performed numerous musical parodies (of Bob Dylan, James Taylor and others). He was featured alongside Chevy Chase and John Belushi in the Off-Broadway revue "National Lampoon's Lemmings". Two of his earliest film roles were small parts as uniformed police officers in the 1972 film "The Hot Rock" and 1974's "Death Wish".
Guest played a small role in the 1977 "All In the Family" episode "", where in a flashback sequence Mike and Gloria recall their first blind date, set up by Michael's college buddy Jim (Guest), who dated Gloria's girlfriend Debbie (Priscilla Lopez).
1980s.
Guest's biggest role of the first two decades of his career is likely that of Nigel Tufnel in the 1984 mockumentary film "This Is Spinal Tap". Guest made his first appearance as Tufnel on the 1978 sketch comedy program "The TV Show".
Along with Martin Short, Billy Crystal and Harry Shearer, Guest was hired as a one-year only cast member for the 1984–85 season on NBC's "Saturday Night Live". Recurring characters on SNL played by Guest include Frankie, of Willie and Frankie (coworkers who recount in detail physically painful situations in which they have found themselves, remarking laconically "I hate when that happens"); Herb Minkman, a shady novelty toymaker with a brother named Al (played by Crystal); Rajeev Vindaloo, an eccentric foreign man in the same vein as Andy Kaufman's Latka character from "Taxi"; and Señor Cosa, a Spanish ventriloquist often seen on the recurring spoof of "The Joe Franklin Show". He also experimented behind the camera with pre-filmed sketches, notably directing a documentary-style short starring Shearer and Short as synchronized swimmers. In another short film from SNL, Guest and Crystal appear as retired Negro league baseball players, "The Rooster and the King".
He appeared as Count Rugen in "The Princess Bride". He had a cameo role as the first customer, a smarmy pedestrian, in the 1986 musical remake of "The Little Shop of Horrors", that also featured his SNL co-star, Steve Martin. As a co-writer and director, Guest made the Hollywood satire "The Big Picture".
Upon his father succeeding to the family peerage in 1987, he was henceforth known as "The Hon. Christopher Haden-Guest" in full. This was his official style and name until he inherited the barony in 1996.
1990–present.
The experience of having made "Spinal Tap" would directly inform the second phase of his career. Starting in 1996, Guest began writing, directing and acting in his own series of substantially improvised films. Many of them would come to be definitive examples of what came to be known as "mockumentaries".
His frequent writing partner is Eugene Levy. Together, Levy, Guest and a small band of other actors have formed a loose repertory group, which appear across several films. These include Catherine O'Hara, Michael McKean, Parker Posey, Bob Balaban, Jane Lynch, John Michael Higgins, Harry Shearer, Jennifer Coolidge, Ed Begley, Jr. and Fred Willard. Guest and Levy write backgrounds for each of the characters and notecards for each specific scene, outlining the plot, and then leave it up to the actors to improvise the dialogue, which is supposed to result in a much more natural conversation than scripted dialogue would. Notably, everyone who appears in these movies receives the same fee, and the same portion of profits.
Despite making a number of mockumentaries, Guest dislikes the term. He maintains that his intention is not to mock anyone, but to explore insular, perhaps obscure communities through his method of filmmaking.
He had a guest voice-over role in the animated comedy series "SpongeBob SquarePants" as SpongeBob's cousin, Stanley.
Guest appeared as Dr. Stone in "A Few Good Men" (1992), as Lord Cromer in "Mrs Henderson Presents" (2005) and in the 2009 comedy "The Invention of Lying".
He is also currently a member of the musical group The Beyman Bros, which he formed with his childhood friend David Nichtern and Spinal Tap's current keyboardist C. J. Vanston. Their debut album "Memories of Summer as a Child" was released on January 20, 2009.
In 2010, the United States Census Bureau paid $2.5 million to have a television commercial directed by Guest shown during television coverage of Super Bowl XLIV.
Guest holds an honorary doctorate from and is a member of the board of trustees for Berklee College of Music in Boston.
He is currently the writer and producer of the HBO series, "Family Tree," a lighthearted story in the mockumentary style he made famous in "Spinal Tap", in which the main character, Tom Chadwick, inherits a box of curios from his great aunt, spurring interest in his ancestry.
Family.
Peerage and heirs.
Guest became the 5th Baron Haden-Guest, of Great Saling, in the County of Essex, when his father died in 1996. He succeeded upon the ineligibility of his older half-brother, Anthony Haden-Guest, who was born prior to the marriage of his parents. According to an article in "The Guardian", Guest attended the House of Lords regularly until the House of Lords Act 1999 barred most hereditary peers from their seats. In the article Guest remarked:
Personal life.
Guest married actress Jamie Lee Curtis in 1984 at the home of their mutual friend, Rob Reiner. They have two adopted children: Anne (born 1986) and Thomas (born 1996). Because Guest's children are adopted, they cannot inherit the family barony under the terms of the letters patent that created it, though a 2004 Royal Warrant addressing the style of a peer's adopted children states that they can use courtesy titles. The current heir presumptive to the barony is Guest's younger brother, the actor The Hon. Nicholas Haden-Guest.
Off-stage demeanor.
As reported by Louis B. Hobson, "On film, Guest is a hilariously droll comedian. In person he is serious and almost dour." He quotes Guest as saying "People want me to be funny all the time. They think I'm being funny no matter what I say or do and that's not the case. I rarely joke unless I'm in front of a camera. It's not what I am in real life. It's what I do for a living."
External links.
 

</doc>
<doc id="7183" url="http://en.wikipedia.org/wiki?curid=7183" title="Carol Kane">
Carol Kane

Carolyn Laurie "Carol" Kane (born June 18, 1952) is an American stage, screen, and television actress. She became known in the 1970s in films such as "Hester Street" (for which she received an Academy Award nomination) and "Annie Hall". She appeared on the television series "Taxi" in the early 1980s, as the wife of Latka, the character played by Andy Kaufman, winning two Emmy Awards for her work. She has played the character of Madame Morrible in the musical "Wicked", both in regional productions and on Broadway from 2005 to 2014.
Early life.
Kane was born in Cleveland, Ohio, the daughter of Joy, a jazz singer, teacher, dancer, and pianist, and Michael Kane, an architect who worked for the World Bank. Her family is Jewish, and her grandparents emigrated from Russia. Her parents divorced when she was 12 years old. She attended the Cherry Lawn School, a progressive boarding school in Darien, Connecticut, until 1965. She attended the Professional Children's School, in New York City, and made her professional theatre debut in a 1966 production of "The Prime of Miss Jean Brodie", starring Tammy Grimes.
Career.
Kane is perhaps best known for her portrayal of Simka Dahblitz-Gravas, wife of Latka Gravas (Andy Kaufman), on the American television series "Taxi", from 1981 to 1983, and also for her role as Allison Portchnik in Woody Allen's "Annie Hall". Kane earned two Emmy Awards for her work in the series and was nominated for an Academy Award for Best Actress for her role in the film "Hester Street". She also appeared in "Dog Day Afternoon" (1975), "The Princess Bride" (1987) and "Scrooged" (1988), in which "Variety" called her "unquestionably [the] pic's comic highlight."
Kane was a regular on the 1986 NBC series "All Is Forgiven", a regular on the 1990–91 NBC series "American Dreamer", guest-starred on a 1994 episode of "Seinfeld" and had a supporting role in the short-lived 1996–97 sitcom "Pearl", which starred Rhea Perlman. In 1998, she played Mother Duck on the cartoon movie "The First Snow of Winter". She also appeared in the NBC television live action production of "The Year Without a Santa Claus" in December 2006.
In January 2009, Kane appeared in the TV series "Two and a Half Men" as the mother of Alan Harper's receptionist.
She starred in the off-Broadway play "Love, Loss, and What I Wore" in February 2010.
In March 2010, Kane appeared in the TV series "Ugly Betty" as Justin Suarez's acting teacher.
Kane made her West End debut in January 2011 in a major revival of Lillian Hellman's drama "The Children's Hour", at London's Comedy Theatre. She stars alongside Keira Knightley, Elisabeth Moss and Ellen Burstyn.
In May 2012, Kane is appearing on Broadway as Betty Chumley in a revival of the play "Harvey".
"Wicked".
Kane is known for her portrayal of the evil headmistress Madame Morrible in the Broadway musical "Wicked", whom she played in various productions from 2005 to 2009.
Kane made her "Wicked" debut on the 1st National Tour, playing the role from March 9 through December 19, 2005. She then reprised the role in the Broadway production from January 10 through November 12, 2006. She again played the role for the Los Angeles production which began performances on February 7, 2007. She left the production on December 30, 2007, and later returned from August 26, 2008 until the production closed on January 11, 2009.
She then transferred with the L.A. company, to play the role once again, in the San Francisco production which began performances January 27, 2009. She ended her limited engagement on March 22, 2009. Kane returned to the Broadway company of "Wicked" from July 1, 2013 through February 22, 2014.

</doc>
<doc id="7184" url="http://en.wikipedia.org/wiki?curid=7184" title="C*-algebra">
C*-algebra

C∗-algebras (pronounced "C-star") are an important area of research in functional analysis, a branch of mathematics. A C*-algebra is a complex algebra "A" of continuous linear operators on a complex Hilbert space with two additional properties:
It is generally believed that C*-algebras were first considered primarily for their use in quantum mechanics to model algebras of physical observables. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. Subsequently John von Neumann attempted to establish a general framework for these algebras which culminated in a series of papers on rings of operators. These papers considered a special class of C*-algebras which are now known as von Neumann algebras.
Around 1943, the work of Israel Gelfand and Mark Naimark yielded an abstract characterisation of C*-algebras making no reference to operators on a Hilbert space.
C*-algebras are now an important tool in the theory of unitary representations of locally compact groups, and are also used in algebraic formulations of quantum mechanics. Another active area of research is the program to obtain classification, or to determine the extent of which classification is possible, for separable simple nuclear C*-algebras.
Abstract characterization.
We begin with the abstract characterization of C*-algebras given in the 1943 paper by Gelfand and Naimark.
A C*-algebra, "A", is a Banach algebra over the field of complex numbers, together with a map * : "A" → "A". One writes "x*" for the image of an element "x" of "A". The map * has the following properties:
Remark. The first three identities say that "A" is a *-algebra. The last identity is called the C* identity and is equivalent to:
formula_6
which is sometimes called the B*-identity. For history behind the names C*- and B*-algebras, see the section below.
The C*-identity is a very strong requirement. For instance, together with the spectral radius formula, it implies that the C*-norm is uniquely determined by the algebraic structure:
A bounded linear map, π : "A" → "B", between C*-algebras "A" and "B" is called a *-homomorphism if
In the case of C*-algebras, any *-homomorphism π between C*-algebras is non-expansive, i.e. bounded with norm ≤ 1. Furthermore, an injective *-homomorphism between C*-algebras is isometric. These are consequences of the C*-identity.
A bijective *-homomorphism π is called a C*-isomorphism, in which case "A" and "B" are said to be isomorphic.
Some history: B*-algebras and C*-algebras.
The term B*-algebra was introduced by C. E. Rickart in 1946 to describe Banach *-algebras that satisfy the condition:
This condition automatically implies that the *-involution is isometric, that is, ||"x"|| = ||"x*"||. Hence ||"xx*"|| = ||"x"|| ||"x*"||, and therefore, a B*-algebra is also a C*-algebra. Conversely, the C*-condition implies the B*-condition. This is nontrivial, and can be proved without using the condition ||"x"|| = ||"x*"||. For these reasons, the term B*-algebra is rarely used in current terminology, and has been replaced by the term 'C*-algebra'.
The term C*-algebra was introduced by I. E. Segal in 1947 to describe norm-closed subalgebras of "B"("H"), namely, the space of bounded operators on some Hilbert space "H". 'C' stood for 'closed'.
Structure of C*-algebras.
C*-algebras have a large number of properties that are technically convenient. Some of these properties can be established by using the continuous functional calculus or by reduction to commutative C*-algebras. In the latter case, we can use the fact that the structure of these is completely determined by the Gelfand isomorphism.
Self-adjoint elements.
Self-adjoint elements are those of the form "x"="x"*. The set of elements of a C*-algebra "A" of the form "x*x" forms a closed convex cone. This cone is identical to the elements of the form "xx*". Elements of this cone are called "non-negative" (or sometimes "positive", even though this terminology conflicts with its use for elements of R.)
The set of self-adjoint elements of a C*-algebra "A" naturally has the structure of a partially ordered vector space; the ordering is usually denoted ≥. In this ordering, a self-adjoint element "x" of "A" satisfies "x" ≥ 0 if and only if the spectrum of "x" is non-negative, if and only if "x" = "s*s" for some "s". Two self-adjoint elements "x" and "y" of "A" satisfy "x" ≥ "y" if "x"−"y" ≥ 0.
This partially ordered subspace allows the definition of a positive linear functional on a C*-algebra, which in turn is used to define the states of a C*-algebra, which in turn can be used to construct the spectrum of a C*-algebra using the GNS construction.
Quotients and approximate identities.
Any C*-algebra "A" has an approximate identity. In fact, there is a directed family {"e"λ}λ∈I of self-adjoint elements of "A" such that
Using approximate identities, one can show that the algebraic quotient of a C*-algebra by a closed proper two-sided ideal, with the natural norm, is a C*-algebra.
Similarly, a closed two-sided ideal of a C*-algebra is itself a C*-algebra.
Examples.
Finite-dimensional C*-algebras.
The algebra M("n", C) of "n" × "n" matrices over C becomes a C*-algebra if we consider matrices as operators on the Euclidean space, C"n", and use the operator norm ||.|| on matrices. The involution is given by the conjugate transpose. More generally, one can consider finite direct sums of matrix algebras. In fact, all C*-algebras that are finite dimensional as vector spaces are of this form, up to isomorphism. The self-adjoint requirement means finite-dimensional C*-algebras are semisimple, from which fact one can deduce the following theorem of Artin–Wedderburn type:
Theorem. A finite-dimensional C*-algebra, "A", is canonically isomorphic to a finite direct sum
where min "A" is the set of minimal nonzero self-adjoint central projections of "A".
Each C*-algebra, "Ae", is isomorphic (in a noncanonical way) to the full matrix algebra M(dim("e"), C). The finite family indexed on min "A" given by {dim("e")}"e" is called the "dimension vector" of "A". This vector uniquely determines the isomorphism class of a finite-dimensional C*-algebra. In the language of K-theory, this vector is the positive cone of the "K"0 group of "A".
An immediate generalization of finite dimensional C*-algebras are the approximately finite dimensional C*-algebras.
C*-algebras of operators.
The prototypical example of a C*-algebra is the algebra "B(H)" of bounded (equivalently continuous) linear operators defined on a complex Hilbert space "H"; here "x*" denotes the adjoint operator of the operator "x" : "H" → "H". In fact, every C*-algebra, "A", is *-isomorphic to a norm-closed adjoint closed subalgebra of "B"("H") for a suitable Hilbert space, "H"; this is the content of the Gelfand–Naimark theorem.
C*-algebras of compact operators.
Let "H" be a separable infinite-dimensional Hilbert space. The algebra "K"("H") of compact operators on "H" is a norm closed subalgebra of "B"("H"). It is also closed under involution; hence it is a C*-algebra.
Concrete C*-algebras of compact operators admit a characterization similar to Wedderburn's theorem for finite dimensional C*-algebras:
Theorem. If "A" is a C*-subalgebra of "K"("H"), then there exists Hilbert spaces {"Hi"}"i"∈"I" such that 
where the (C*-)direct sum consists of elements ("Ti") of the Cartesian product Π "K"("Hi") with ||"Ti"|| → 0.
Though "K"("H") does not have an identity element, a sequential approximate identity for "K"("H") can be developed. To be specific, "H" is isomorphic to the space of square summable sequences "l"2; we may assume that "H" = "l"2. For each natural number "n" let "Hn" be the subspace of sequences of "l"2 which vanish for indices "k" ≤ "n" and let "en" be the orthogonal projection onto "Hn". The sequence {"en"}"n" is an approximate identity for "K"("H").
"K"("H") is a two-sided closed ideal of "B"("H"). For separable Hilbert spaces, it is the unique ideal. The quotient of "B"("H") by "K"("H") is the Calkin algebra.
Commutative C*-algebras.
Let "X" be a locally compact Hausdorff space. The space C0("X") of complex-valued continuous functions on "X" that "vanish at infinity" (defined in the article on local compactness) form a commutative C*-algebra C0("X") under pointwise multiplication and addition. The involution is pointwise conjugation. C0("X") has a multiplicative unit element if and only if "X" is compact. As does any C*-algebra, C0("X") has an approximate identity. In the case of C0("X") this is immediate: consider the directed set of compact subsets of "X", and for each compact "K" let "fK" be a function of compact support which is identically 1 on "K". Such functions exist by the Tietze extension theorem which applies to locally compact Hausdorff spaces. "{fK}K" is an approximate identity.
The Gelfand representation states that every commutative C*-algebra is *-isomorphic to the algebra C0("X"), where "X" is the space of characters equipped with the weak* topology. Furthermore if C0("X") is isomorphic to C0("Y") as C*-algebras, it follows that "X" and "Y" are homeomorphic. This characterization is one of the motivations for the noncommutative topology and noncommutative geometry programs.
C*-enveloping algebra.
Given a Banach *-algebra "A" with an approximate identity, there is a unique (up to C*-isomorphism) C*-algebra E("A") and *-morphism π from "A" into E("A") which is universal, that is, every other continuous *-morphism factors uniquely through π. The algebra E("A") is called the C*-enveloping algebra of the Banach *-algebra "A".
Of particular importance is the C*-algebra of a locally compact group "G". This is defined as the enveloping C*-algebra of the group algebra of "G". The C*-algebra of "G" provides context for general harmonic analysis of "G" in the case "G" is non-abelian. In particular, the dual of a locally compact group is defined to be the primitive ideal space of the group C*-algebra. See spectrum of a C*-algebra.
von Neumann algebras.
von Neumann algebras, known as W* algebras before the 1960s, are a special kind of C*-algebra. They are required to be closed in the weak operator topology, which is weaker than the norm topology.
The Sherman–Takeda theorem implies that any C*-algebra has a universal enveloping W*-algebra, such that any homomorphism to a W*-algebra factors through it.
Type for C*-algebras.
A C*-algebra "A" is of type I if and only if for all non-degenerate representations π of "A" the von Neumann algebra π("A")′′ (that is, the bicommutant of π("A")) is a type I von Neumann algebra. In fact it is sufficient to consider only factor representations, i.e. representations π for which π("A")′′ is a factor.
A locally compact group is said to be of type I if and only if its group C*-algebra is type I.
However, if a C*-algebra has non-type I representations, then by results of James Glimm it also has representations of type II and type III. Thus for C*-algebras and locally compact groups, it is only meaningful to speak of type I and non type I properties.
C*-algebras and quantum field theory.
In quantum mechanics, one typically describes a physical system with a C*-algebra "A" with unit element; the self-adjoint elements of "A" (elements "x" with "x*" = "x") are thought of as the "observables", the measurable quantities, of the system. A "state" of the system is defined as a positive functional on "A" (a C-linear map φ : "A" → C with φ("u*u") ≥ 0 for all "u" ∈ "A") such that φ(1) = 1. The expected value of the observable "x", if the system is in state φ, is then φ("x").
This C*-algebra approach is used in the Haag-Kastler axiomatization of local quantum field theory, where every open set of Minkowski spacetime is associated with a C*-algebra.

</doc>
<doc id="7185" url="http://en.wikipedia.org/wiki?curid=7185" title="London Borough of Croydon">
London Borough of Croydon

The London Borough of Croydon () is a London borough in South London, England and is part of Outer London. It covers an area of and is the largest London borough by population. It is the southernmost borough of London. At its centre is the historic town of Croydon from which the borough takes its name. Croydon is mentioned in the Domesday Book, and from a small market town has expanded into one of the most populous areas on the fringe of London. Croydon is the civic centre of the borough. The borough is now one of London's leading business, financial and cultural centres, and its influence in entertainment and the arts contribute to its status as a major metropolitan centre.
Formed in 1965 from the Coulsdon and Purley Urban District and the County Borough of Croydon, the local authority Croydon London Borough Council, is now part of the local government association for Greater London, London Councils. The borough has a long history which is based mainly around the economy of the area. The economic strength of Croydon dates back mainly to Croydon Airport which was a major factor in the development of Croydon as a business centre. Once London's main airport for all international flights to and from the capital, it was closed on 30 September 1959 due to the lack of expansion space needed for an airport to serve the growing city. It is now a Grade II listed building and tourist attraction. Croydon Council and its predecessor Croydon Corporation unsuccessfully applied for city status in 1954, 2000, 2002 and 2012. The area is currently going through a large regeneration project called Croydon Vision 2020 which is predicted to attract more businesses and tourists to the area as well as backing Croydon's bid to become "London's Third City". Since 2003 Croydon has been certified as a Fairtrade borough by the Fairtrade Foundation. It was the first London Borough to have Fairtrade status which is awarded on certain criteria.
The area is one of the hearts of culture in London and the South East of England. Institutions such as the major arts and entertainment centre Fairfield Halls add to the vibrancy of the borough. However, its famous fringe theatre "the Warehouse Theatre" was put under administration in 2012 when the council withdrew its funding and the building itself was demolished in 2013. The Croydon Clocktower was opened by Queen Elizabeth II in 1994 as an arts venue featuring a library, the independent David Lean Cinema cinema (closed by the council in 2011 after 16 years of operating, but now partially reopened on a part-time and volunteers basis) and museum. From 2000 to 2010, Croydon staged an annual summer festival celebrating the area's black and Indian cultural diversity, with audiences reaching over 50,000 people. An internet radio station, Croydon Radio, is run by local people for the area. The borough is also home to its own local TV station, "Croydon TV". Premier League football club Crystal Palace F.C. play at Selhurst Park in South Norwood, a stadium they have been based in since 1924. Other landmarks in the borough include Addington Palace, an 18th-century mansion which became the official second residence of six archbishops, Shirley Windmill, one of the few surviving large windmills in Greater London built in the 1800s, and the BRIT School, a creative arts institute run by the BRIT Trust which has produced artists such as Adele, Amy Winehouse and Leona Lewis.
History.
The London Borough of Croydon was formed in 1965 from the Coulsdon and Purley Urban District and the County Borough of Croydon. The name Croydon comes from Crogdene or Croindone, named by the Saxons in the 8th century when they settled here, although the area had been inhabited since prehistoric times. It is thought to derive from the Anglo-Saxon "croeas deanas", meaning "the valley of the crocuses", indicating that, like Saffron Walden in Essex, it was a centre for the collection of saffron.
By the time of the Norman invasion Croydon had a church, a mill and around 365 inhabitants as recorded in the Domesday Book. The Archbishop of Canterbury, Archbishop Lanfranc lived at Croydon Palace which still stands. Visitors included Thomas Beckett (another Archbishop), and royal figures such as Henry VIII of England and Elizabeth I.
Croydon carried on through the ages as a prosperous market town, they produced charcoal, tanned leather, and ventured into brewing. Croydon was served by the Surrey Iron Railway, the first public railway (horse drawn) in the world, in 1803, and by the London to Brighton rail link in the mid-19th century, helping it to become the largest town in what was then Surrey.
In the 20th century Croydon became known for industries such as metal working, car manufacture and its aerodrome, Croydon Airport. Starting out during World War I as an airfield for protection against Zeppelins, an adjacent airfield was combined, and the new aerodrome opened on 29 March 1920. It became the largest in London, and was the main terminal for international air freight into the capital. It developed into one of the great airports of the world during the 1920s and 1930s, and welcomed the world's pioneer aviators in its heyday. British Airways Ltd used the airport for a short period after redirecting from Northolt Aerodrome, and Croydon was the operating base for Imperial Airways. It was partly due to the airport that Croydon suffered heavy bomb damage during World War II. As aviation technology progressed, however, and aircraft became larger and more numerous, it was recognized in 1952 that the airport would be too small to cope with the ever-increasing volume of air traffic. The last scheduled flight departed on 30 September 1959. It was superseded as the main airport by both London Heathrow and London Gatwick Airport (see below). The air terminal, now known as Airport House, has been restored, and has a hotel and museum in it.
In the late 1950s and through the 1960s the council commercialized the centre of Croydon with massive development of office blocks and the Whitgift Centre which was formerly the biggest in town shopping centre in Europe. The centre was officially opened in October 1970 by the Duchess of Kent. The original Whitgift School there had moved to Haling Park, South Croydon in the 1930s; the replacement school on the site, Whitgift Middle School, now the Trinity School of John Whitgift, moved to Shirley Park in the 1960s when the buildings were demolished.
The borough council unsuccessfully applied for city status in 1965, 2000 and again in 2002. If it had been successful it would have been the third local authority in Greater London to hold that status along with the "City of London" and the "City of Westminster". At present the London Borough of Croydon is the second most populous Local government district of England without city status, Kirklees being the first. Croydon's applications were refused as it was felt not to have an identity separate from the rest of Greater London. In 1965 it was described as "...now just part of the London conurbation and almost indistinguishable from many of the other Greater London boroughs" and in 2000 as having "no particular identity of its own".
Croydon is currently going through a vigorous regeneration plan, called Croydon Vision 2020. This will change the urban planning of central Croydon completely. Its main aims are to make Croydon "London's Third City" and the hub of retail, business, culture and living in South London and South East England. The plan was showcased in a series of events called Croydon Expo. It was aimed at business and residents in the London Borough of Croydon to demonstrate the £3.5bn development projects the Council wishes to see in Croydon in the next ten years.
There have also been exhibitions for regional districts of Croydon, including Waddon, South Norwood and Woodside, Purley, New Addington and Coulsdon. Examples of upcoming architecture featured in the expo can easily be found to the centre of the borough in the form of the Croydon Gateway site and the Cherry Orchard Road Towers.
Governance.
Politics of Croydon Council.
Croydon Council is governed by 70 councillors elected in 24 wards. From 1994 to 2006 Labour Party councillors controlled the council.
At the 2010 Croydon local elections seats lost previously in Addiscombe, South Norwood and Upper Norwood were retaken by Labour Party councillors; in New Addington the Conservative party gained a councillor. The composition of the council after the 2010 elections is Conservatives 37, Labour 33.
Mike Fisher, Conservative group leader since May 2005, was named as Council Leader following the Conservative victory. Croydon is a cabinet-style council, and the Leader heads a ten-person cabinet, its members responsible for areas such as education or planning. There is a Shadow Cabinet drawn from the principal opposition party. A backbench cross-party scrutiny and overview committee is in place to hold the executive cabinet to account.
At the 2006 local elections Conservative councillors regained control in gaining 12 councillors, taking ten seats from Labour in Addiscombe, Waddon, South Norwood and Upper Norwood and ousting the single Liberal Democrat councillor in Coulsdon. Between the 2006 and 2010 elections, a by-election in February 2007 saw a large swing to Labour from the Conservatives. Whereas 6% Conservative to Labour swings were produced in the two previous by-elections to 2006, won by a councillor from the incumbent party (in both cases the party of a councillor who had died).
Crossover has occurred in political affiliation, during 2002–06 one Conservative councillor defected to Labour, went back to the Conservatives and spent some time as an independent. In March 2008, the Labour councillor Mike Mogul joined the Conservatives while a Conservative councillor became an independent.
Councillor Jonathan Driver, who became Mayor in 2008, died unexpectedly at the close of the year, causing a by-election in highly marginal Waddon which was successfully held by the Conservatives.
From February 2005 until May 2006 the Leader of Croydon Council was Labour Co-operative Councillor Tony Newman, succeeding Hugh Malyan.
Westminster Representation.
The borough is covered by three parliamentary constituencies for the Westminster Parliament, these are Croydon North, Croydon Central and Croydon South. There are 24 wards which represent Croydon Council.
Civic history.
For much of its history, Croydon Council was controlled by the Conservative Party or conservative-leaning independents. Former Croydon councillors include former MPs Andrew Pelling, Vivian Bendall, David Congdon, Geraint Davies and Reg Prentice, London Assembly member Valerie Shawcross, Lord Bowness, John Donaldson, Baron Donaldson of Lymington (Master of the Rolls) and H.T. Muggeridge, MP and father of Malcolm Muggeridge. The first Mayor of the newly created County Borough was Jabez Balfour, later a disgraced Member of Parliament. Former Conservative Director of Campaigning, Gavin Barwell, was a Croydon councillor between 1998 and 2010 and since 2010 is the MP for Croydon Central.
Some 10,000 people work directly or indirectly for the council, in its main offices at Bernard Weatherill House or in its schools, care homes, housing offices or work depots. The council is generally well-regarded, having made important improvements in education and social services. However, there have been concerns over benefits, leisure services and waste collection. Although the council has one of London's lower rates of council tax, there are claims that it is too high and that resources are wasted.
The Mayor of Croydon for 2010-11 is Councillor Avril Slipper. The Leader is Cllr Mike Fisher and the Deputy Leaders are Cllr Tim Pollard and Cllr Dudley Mead. The Chief Executive since 7 July 2007 has been Jon Rouse.
Government buildings.
Croydon Town Hall on Katharine Street in Central Croydon houses the committee rooms, the mayor's and other councillors' offices, electoral services and the arts and heritage services.
The present Town Hall is Croydon's third. The first town hall is thought to have been built in either 1566 or 1609. The second was built in 1808 to serve the growing town but was demolished after the present town hall was erected in 1895.
The present town hall was designed by local architect Charles Henman and was officially opened by the Prince and Princess of Wales on 19 May 1896. It was constructed in red brick, sourced from Wrotham in Kent, with Portland stone dressings and green Westmoreland slates for the roof. It also housed the court and most central council employees.
Parts, including the former court rooms, have been converted into the Museum of Croydon and exhibition galleries. The original public library was converted into the David Lean Cinema, part of the Croydon Clocktower. The Braithwaite Hall is used for events and performances. The town hall was renovated in the mid-1990s and the imposing central staircase, long closed to the public and kept for councillors only, was re-opened in 1994. The civic complex, meanwhile, was substantially added to, with buildings across Mint Walk and the 19-floor Taberner House to house the rapidly expanding corporation's employees.
Ruskin House is the headquarters of Croydon's Labour, Trade Union and Co-operative movements and is itself a co-operative with shareholders from organisations across the three movements. In the 19th century, Croydon was a bustling commercial centre of London. It was said that, at the turn of the 20th century, approximately £10,000 was spent in Croydon's taverns and inns every week. For the early labour movement, then, it was natural to meet in the town's public houses, in this environment. However, the temperance movement was equally strong, and Georgina King Lewis, a keen member of the Croydon United Temperance Council, took it upon herself to establish a dry centre for the labour movement. The first Ruskin House was highly successful, and there has been two more since. The current house was officially opened in 1967 by the then Labour Prime Minister, Harold Wilson. Today, Ruskin House continues to serve as the headquarters of the Trade Union, Labour and Co-operative movements in Croydon, hosting a range of meetings and being the base for several labour movement groups. Office tenants include the headquarters of the Communist Party of Britain and Croydon Labour Party. Geraint Davies, the MP for Croydon Central, had offices in the building, until he was defeated by Andrew Pelling and is now the Labour representative standing for Swansea West in Wales.
Taberner House was built between 1964 and 1967, designed by architect H. Thornley, with Allan Holt and Hugh Lea as borough engineers. Although the council had needed extra space since the 1920s, it was only with the imminent creation of the London Borough of Croydon that action was taken. The building, being demolished in 2014, was in classic 1960s style, praised at the time but subsequently much derided. It has its elegant upper slab block narrowing towards both ends, a formal device which has been compared to the famous Pirelli Tower in Milan. It was named after Ernest Taberner OBE, Town Clerk from 1937 to 1963. Until September 2013, Taberner House housed most of the council's central employees and was the main location for the public to access information and services, particularly with respect to housing.
In September 2013, Council staff moved into Bernard Weatherill House in Fell Road, (named after the former Speaker of the House and Member of Parliament for Croydon North-East). Staff from the Met Police, NHS, Jobcentre Plus, Croydon Credit Union, Citizens Advice Bureau as well as 75 services from the council all moved to the new building.
Geography and climate.
The borough is in the deep south of London, with the M25 orbital motorway stretching to the south of it, between Croydon and Tandridge. In the north and east of Croydon the authority mainly borders the London Borough of Bromley and in the north west the boroughs of Lambeth and Southwark. The boroughs of Sutton and Merton are located directly to the west. It is at the head of the River Wandle, just to the north of a significant gap in the North Downs. It lies south of Central London, and the earliest settlement may have been a Roman staging post on the London-Portslade road, although conclusive evidence has not yet been found. The main town centre houses a great variety of well-known stores on North End and two shopping centres. It was pedestrianised in 1989 to attract people back to the town centre. Another shopping centre called Park Place was due to open in 2012 but has since been scrapped.
Cityscape.
The CR postcode area covers most of the south and centre of the borough while the SE and SW postcodes cover the northern parts, including Crystal Palace, Upper Norwood, South Norwood, Selhurst, Thornton Heath, Norbury and Pollards Hill.
Districts in the London Borough of Croydon include Addington, a small village to the east of Croydon which until 2000 was poorly linked to the rest of the borough as it was without any railway or light rail stations with only a few patchy bus services to rely on. Addiscombe is a district just northeast of the centre of Croydon, and is popular with commuters to central London due to its close proximity to the busy East Croydon station. Ashburton, to the northeast of Croydon, is mostly home to residential houses and flats, being named after Ashburton House, one of the three big houses in the Addiscombe area. Broad Green is a small district, centred on a large green with many homes and local shops in West Croydon. Coombe is an area, just east of Croydon, which has barely been urbanised and has retained its collection of large houses fairly intact. Coulsdon, southwest of Central Croydon, which has retained a good mix of traditional high street shops as well as a large number of restaurants for its size. Croydon is the principal area of the borough, Crystal Palace is an area north of Croydon, which is shared with the London Boroughs of Lambeth, Southwark, Lewisham and Bromley. Fairfield, just northeast of Croydon, holds the Fairfield Halls and the village of Forestdale, to the east of Croydon's main area, commenced work in the late 1960s and completed in the mid-70s to create a larger town on what was previously open ground. Hamsey Green is a place on the plateau of the North Downs, south of Croydon. Kenley, again south of the centre, lie within the London Green Belt and features a landscape dominated by green space. New Addington, to the east, is a large local authority estate surrounded by open countryside and golf courses. Norbury, to the northwest, is a suburb with a large ethnic population. Norwood New Town is a part of the Norwood triangle, to the north of Croydon. Monks Orchard is a small district made up of large houses and open space in the northeast of the borough. Pollards Hill is a residential district with houses on roads, which are lined with pollarded lime trees, stretching to Norbury. Purley, to the south, is a main town whose name derives from "pirlea", which means 'Peartree lea'. Sanderstead, to the south, is a village mainly on high ground at the edge of suburban development in Greater London. Selhurst is a town, to the north of Croydon, which holds the nationally known school, The BRIT School. Selsdon is a suburb which was developed during the inter-war period in the 1920s and 1930s, and is remarkable for its many Art Deco houses, to the southeast of Croydon Centre. Shirley, is to the east of Croydon, and holds Shirley Windmill. South Croydon, to the south of Croydon, is a locality which holds local landmarks such as The Swan and Sugarloaf public house and independent Whitgift School part of the Whitgift Foundation. South Norwood, to the north, is in common with West Norwood and Upper Norwood, named after a contraction of Great North Wood and has a population of around 14,590. Thornton Heath is a town, to the northwest of Croydon, which holds Croydon's principal hospital Mayday. Upper Norwood is, west to Croydon, on a mainly elevated area of the borough. Waddon is a residential area, mainly based on the Purley Way retail area, to the west of the borough. Woodside is located to the northeast of the borough, with streets based on Woodside Green, a small sized area of green land. And finally Whyteleafe is a town, right to the edge of Croydon with some areas in the Surrey district of Tandridge.
Croydon is a gateway to the south from central London, and therefore has a number of major roads running through it. Purley Way on the A23 road was built to by-pass Croydon town centre on which the A23 once did, is one of the busiest roads in the borough, and has been the site of several major retail developments including one of only 18 IKEA stores in the country, built on the site of the former power station. It carries on to Brighton Road which is the main route running towards the south from Croydon to Purley and continues on the A23. The centre of Croydon is very congested, and the urban planning has since become out of date and quite inadequate, due to the expansion of Croydon's main shopping area and office blocks. Wellesley Road, is a dual carriageway that cuts through the centre of the town, and makes it hard to interchange between the civic centre's two railway stations. Croydon Vision 2020 includes a plan for a more pedestrian-friendly replacement. It has also been named as one of the worst roads for cyclists in the area. Construction of the Croydon Underpass beneath the junction of George Street and Wellesley Road/Park Lane during the early Sixties started, with the main aim to prevent traffic congestion on Park Lane, situated above the underpass. The Croydon Flyover on the other hand is situated near the underpass and next to Taberner House. It mainly leads traffic on to Duppas Hill, towards Purley Way with the intention for easy links with Sutton and Kingston upon Thames further afield. The major junction on the flyover is for Old Town, which is also a large three-lane road.
Topography and climate.
Croydon covers an area of 86.52 km2, the 256th largest district in England. Croydon's physical features consist of many hills and rivers that are spread out across the borough and into the North Downs, Surrey and the rest of South London. Addington Hills is a major hilly area to the south of London and is recognised as a significant obstacle to the growth of London from its origins as a port on the north side of the river, to a large circular city. The Great North Wood is a former natural oak forest that covered the Sydenham Ridge and the southern reaches of the River Effra and its tributaries. The most notable tree, called Vicar's Oak, marked the boundary of four ancient parishes; Lambeth, Camberwell, Croydon and Bromley. John Aubrey referred to this "ancient remarkable tree" in the past tense as early as 1718, but according to JB Wilson, the Vicar's Oak survived until 1825. The River Wandle is also a major tributary of the River Thames, where it stretches to Wandsworth and Putney for from its main source in Waddon.
Croydon has a temperate climate in common with most areas of Great Britain, it is similar to that of Greenwich in Inner London: its Köppen climate classification is "Cfb". Its mean annual temperature of 9.6 °C is similar to that experienced throughout the Weald, and slightly cooler than nearby areas such as the Sussex coast and central London. Rainfall is considerably below England's average (1971–2000) level of 838 mm, and every month is drier overall than the England average.
The nearest weather station is at .
Architecture.
The skyline of Croydon has significantly changed over the past 50 years. High rise buildings, mainly office blocks, now dominate the skyline. The most notable of these buildings include Croydon Council's headquarters Taberner House, which has been compared to the famous Pirelli Tower of Milan, and the Nestlé Tower, the former UK headquarters of Nestlé.
In recent years, the development of tall buildings, such as the approved Croydon Vocational Tower and Wellesley Square, has been encouraged in the London Plan, which will lead to the erection of new skyscrapers over the next few years as London goes through a high-rise boom.
No. 1 Croydon, formerly the NLA Tower, Britain's 88th tallest tower, close to East Croydon station, is an example of 1970s architecture. The tower was originally nicknamed the "Threepenny bit building", as it resembles a stack of pre-decimalisation Threepence coins, which were 12-sided. Since decimalisation it has gained the alternative nickname "50 pence building", based on the more familiar 50 pence coin.
Lunar House is another high-rise building. Like other government office buildings on Wellesley Road, such as Apollo House, the name of the building was inspired by the US moon landings (In the Croydon suburb of New Addington there is a public house, built during the same period, called "The Man on the Moon").
A new generation of buildings are being considered by the council as part of Croydon Vision 2020, so that the borough doesn't lose its title of having the "largest office space in the south east", excluding central London. Projects such as Wellesley Square, which will be a mix of residential and retail with an eye-catching colour design and 100 George Street a proposed modern office block are incorporated in this vision.
Notable events that have happened to Croydon's skyline include the Millennium project to create the largest single urban lighting project ever. It was created for the buildings of Croydon to illuminate them for the third millennium. Not only did this project give new lighting to the buildings, but it provided an opportunity to project onto them images and words, mixing art and poetry with coloured light, and also displaying public information after dark. Apart from increasing night time activity in Croydon and thereby reducing the fear of crime, it helped to promote the sustainable use of older buildings by displaying them in a more positive way.
Demography.
According to the 2001 census, Croydon has a population of around 269,100. In 2005 this was recorded to have risen up to 342,700, making Croydon the ninth most populous local authority in England out of 354 boroughs. 159,111 were males, with 171,476 females. In 2001 the number of people per hectare in Croydon was 38.21, in London 45.62, and in England 3.77. The mean age of the residents of Croydon was 33.75 and 233,748 out of 330,587 residents described their health as 'good'.
White is the majority ethnicity with over 72%, compared to 92% in England as a whole. Black was the second-largest ethnicity, over 13%; 11.3% is South Asian.
The most common householder type were owner occupied with only a small percentage rented. Many new housing schemes and developments are currently taking place in Croydon, such as The Exchange and Bridge House, IYLO, Wellesley Square (now known as Saffron Square) and Altitude 25. In 2006, The Metropolitan Police recorded a 10% fall in the number of crimes committed in Croydon, better than the rate which crime in London as a whole is falling. Croydon has had the highest fall in the number of cases of violence against the person in South London, and is one of the top 10 safest local authorities in London. According to "Your Croydon" (a local community magazine) this is due to a stronger partnership struck between Croydon Council and the police. In 2007, overall crime figures across the borough saw decrease of 5%, with the number of incidents decreasing from 32,506 in 2006 to 30,862 in 2007. However, in the year ending April 2012, The Metropolitan Police recorded the highest rates for murder and rape throughout London in Croydon, accounting for almost 10% of all murders, and 7% of all rapes. Croydon has five police stations. Croydon police station is on Park Lane in the centre of the town near the Fairfield Halls; South Norwood police station is a newly refurbished building just off the High Street; Norbury police station is on London Road; Kenley station is on Godstone Road; and New Addington police station is on Addington Village road.
Population change.
The table shows details on the population change since 1901, including the percentage change since the last available census data. Although the London Borough of Croydon has existed as a London borough since 1963, figures have been generated by combining data from the towns, villages, and civil parishes that would later be constituent parts of the authority.
Economy.
The main employment sectors of the Borough is retail and enterprise which is mainly based in Central Croydon. Major employers are well-known companies, who hold stores or offices in the town. Purley Way is a major employer of people, looking for jobs as sales assistants, sales consultants and store managerial jobs. IKEA Croydon, when it was built in 1992, brought many non-skilled jobs to Croydon. The store, which is a total size of 23,000 m2, took over the former site of Croydon Power station, which had led to the unemployment of many skilled workers. In May 2006, the extension of the IKEA made it the fifth biggest employer in Croydon, and includes the extension of the showroom, market hall and self-serve areas. Other big employers around Purley include the large Tesco Extra store in the town centre, along with other stores in Purley Way including Sainsbury's, B&Q, Comet, Vue and Toys "R" Us. Croydon town centre is also a major retail centre, and home to many high street and department stores as well as designer boutiques. The main town centre shopping areas are on the North End precinct, in the Whitgift Centre, Centrale and St George's Walk. Department stores in Croydon town centre include House of Fraser, Marks and Spencer, Allders, Debenhams and T.K. Maxx. Croydon's main market is Surrey Street Market, which has a royal charter dating back to 1276. Shopping areas outside the city centre include the Valley Park retail complex, Croydon Colonnades, Croydon Fiveways, and the Waddon Goods Park.
In research from 2010 on retail footprint, Croydon came out as 29th in terms of retail expenditure at £770 million. This puts it 6th in the Greater London area, falling behind Kingston upon Thames and Westfield London. In 2005, Croydon came 21st, second in London behind the West End, with £909 million, whilst Kingston was 24th with £864 million. In a 2004 survey on the top retail destinations, Croydon was 27th.
In 2007, Croydon leapt up the annual business growth league table, with a 14% rise in new firms trading in the borough after 125 new companies started up, increasing the number from 900 to 1,025, enabling the town, which has also won the Enterprising Britain Award and "the most enterprising borough in London" award, to jump from 31 to 14 in the table.
Tramlink created many jobs when it opened in 2000, not only drivers but engineers as well. Many of the people involved came from Croydon, which was the original hub of the system. Retail stores inside both Centrale and the Whitgift Centre as well as on North End employee people regularly and create many jobs, especially at Christmas. As well as the new building of Park Place, which will create yet more jobs, so will the regeneration of Croydon, called Croydon Vision 2020, highlighted in the Croydon Expo which includes the Croydon Gateway, Wellesley Square, Central One plus much more.
Croydon is a major office area in the south east of England, being the largest outside of central London. Many powerful companies based in Europe and worldwide have European or British headquarters in the town. American International Group (AIG) have offices in No. 1 Croydon, formerly the NLA Tower, shared with Liberata, Pegasus and the Institute of Public Finance. AIG is the sixth-largest company in the world according to the 2007 Forbes Global 2000 list. The Swiss company Nestlé has its UK headquarters in the Nestlé Tower, on the site of the formerly proposed Park Place shopping centre. Real Digital International has developed a purpose built factory on Purley Way equipped with "the most sophisticated production equipment and technical solutions". ntl:Telewest, now Virgin Media, have offices at Communications House, from the Telewest side when it was known as Croydon Cable. The Home Office UK Border Agency has its headquarters in Lunar House in Central Croydon. In 1981, Superdrug opened a distribution centre and office complex at Beddington Lane. The head office of international engineering and management consultant Mott MacDonald is located in Mott MacDonald House on Sydenham Road, one of four offices they occupy in the town centre. BT has large offices in Prospect East in Central Croydon. The Royal Bank of Scotland also has large offices in Purley, south of Croydon. Direct Line also has an office opposite Taberner House. Other companies with offices in Croydon include Lloyds TSB, Merrill Lynch and Balfour Beatty. Ann Summers used to have its headquarters in the borough but has moved to the Wapses Lodge Roundabout in Tandridge.
Landmarks.
There are a large number of attractions and places of interest all across the borough of Croydon, ranging from historic sites in the north and south to modern towers in the centre.
Croydon Airport was once London's main airport, but closed on 30 September 1959 due to the expansion of London and the need of more room at the airport which was impossible to provide, so Heathrow International Airport took over as London's main airport. It is now disused and is a tourist attraction.
The Croydon Clocktower arts venue was opened by Elizabeth II in 1994. It includes the Braithwaite Hall (the former reference library - named after the Rev. Braithwaite who donated it to the town) for live events, David Lean Cinema (built in memory of David Lean), the Museum of Croydon and Croydon Central Library. The Museum of Croydon (formerly known as Croydon Lifetimes Museum) highlights Croydon in the past and the present and currently features high-profile exhibitions including the Riesco Collection, The Art of Dr Seuss and the Whatever the Weather gallery. Shirley Windmill is a working windmill and one of the few surviving large windmills in Surrey, built in 1854. It is Grade II listed and received a £218,100 grant from the Heritage Lottery Fund. Addington Palace is an 18th-century mansion in Addington which was originally built as Addington Place in the 16th century. The palace became the official second residence of six archbishops, five of whom are buried in St Mary's Church and churchyard nearby.
North End is the main pedestrianised shopping road in Croydon, having Centrale to one side and the Whitgift Centre to the other. The Warehouse Theatre is a popular theatre for mostly young performers and is due to get a face-lift on the Croydon Gateway site.
The Nestlé Tower was the UK headquarters of Nestlé and is one of the tallest towers in England, which is due to be re-fitted during the Park Place development. The Fairfield Halls is a well known concert hall and exhibition centre, opened in 1962. It is frequently used for BBC recordings and was formerly the home of ITV's World of Sport. It includes the Ashcroft Theatre and the Arnhem Gallery.
Croydon Palace was the summer residence of the Archbishop of Canterbury for over 500 years and included regular visitors such as Henry III and Queen Elizabeth I. It is thought to have been built around 960. Croydon Cemetery is a large cemetery and crematorium west of Croydon and is most famous for the gravestone of Derek Bentley, who was wrongly hanged in 1953. Mitcham Common is an area of common land partly shared with the boroughs of Sutton and Merton. Almost 500,000 years ago, Mitcham Common formed part of the river bed of the River Thames.
The BRIT School is a performing Arts & Technology school, owned by the BRIT Trust (known for the BRIT Awards Music Ceremony). Famous former students include Kellie Shirley, Amy Winehouse, Leona Lewis, Kate Nash, Dane Bowers, Katie Melua and Lyndon David-Hall. Grants is an entertainment venue in the centre of Croydon which includes a Vue cinema and the Tiger Tiger nightclub. Taberner House houses the main offices of Croydon Council, and was built between 1964 and 1967. It has been compared to the Pirelli Tower in Milan.
Surrey Street Market has a Royal Charter dating back to 1276 linking it to the Archbishop of Canterbury. The market is regularly used as a location for TV, film and advertising. Beanos, a collectors' record store that has been in Croydon for over three decades, was once the largest second-hand record shop in Europe. The Parish Church of St John the Baptist is a large church dating from the 15th century. It was largely destroyed by fire in 1867 and rebuilt by Sir George Gilbert Scott. It is the burial place of six Archbishops of Canterbury with monuments to Archbishops Sheldon and Whitgift. BedZED, Beddington Zero Energy Development, is just outside the borough, in the neighbouring London Borough of Sutton.
Transport.
There are two main interchanges for all public transport modes (national and local rail, tram, and local buses) at West Croydon and East Croydon station.
National and international travel.
Croydon is linked into the national motorway network via the M23 and M25 orbital motorway. The M25 skirts the south of the borough, linking Croydon with other parts of London and the surrounding counties; the M23 branches from the M25 close to Coulsdon, linking the town with the south coast, Crawley, Reigate, and London Gatwick Airport. The A23 connects the borough with the motorways. The A23 is the major trunk road through Croydon, linking it with central London, East Sussex, Horsham, and Littlehaven. The old London to Brighton road, passes through the west of the borough on Purley Way, bypassing the commercial centre of Croydon which it once did.
The Brighton Main Line railway route south from Croydon links the town to Sussex, Surrey, and Kent and to central London to the north: providing direct services to Hastings, Southampton, Brighton, Portsmouth, Gatwick Airport, Bedford and Luton. The main station for all these services is East Croydon station in the centre of the town centre. East Croydon station is the largest and busiest station in Croydon, third busiest in London, excluding Travelcard Zone 1. The station at West Croydon serves all trains travelling west except the fastest. There are also more regional stations scattered around the borough. Passenger rail services through Croydon are provided by Southern and Thameslink. A pilot scheme launched by the Strategic Rail Authority, Transport for London and three train operators is designed to encourage more passengers to travel off-peak. In full partnership with the South London Boroughs which includes Croydon, SWELTRAC, SELTRANS and the transport users group, the scheme promotes the advantages of off-peak travel following improvements to safety, travel connections and upgrading of station facilities. The Thameslink Programme (formerly known as Thameslink 2000), is a £3.5 billion major project to expand the Thameslink network from 51 to 172 stations spreading northwards to Bedford, Peterborough, Cambridge and King's Lynn and southwards to Guildford, Eastbourne, Horsham, Hove to Littlehampton, East Grinstead, Ashford and Dartford. The project includes the lengthening of platforms, station remodelling, new railway infrastructure (e.g. viaduct) and additional rolling stock. When implemented, Thameslink services would call at other stations in the borough including Purley and Norwood Junction.
The closest international airport to Croydon is London Gatwick Airport, which is located from the town centre. Gatwick airport opened on August 1930 as an aerodrome and is a major international operational base for British Airways, EasyJet and Virgin Atlantic. It currently handles around 35 million passengers a year, making it London's second largest airport, and the second busiest airport in the United Kingdom after Heathrow. London Heathrow Airport, London City Airport and London Luton Airport all lie within a two hours' drive of Croydon. Gatwick and Luton Airports are connected to Croydon by frequent direct trains, while Heathrow is accessible by the route X26 bus.
Local travel.
The A23 and A22 roads are the major trunk roads through Croydon. These both run north-south, connecting to each other in Purley. The A22 connects Croydon, its starting point, to East Grinstead, Tunbridge Wells, Uckfield, and Eastbourne. Other major roads generally radiate spoke-like from the city centre. Wellesley Road is an urban dual carriageway which cuts through the middle of the central business district. It was constructed in the 1960s as part of a planned ring road for Croydon and includes an underpass, which allows traffic to avoid going into the town centre.
The hilly topography of Croydon and the lack of underground services in that part of South London is a reason for the extensive suburban and inter-urban railway network. Croydon is in the commuter belt to London as part of suburbia. There are several busy local rail routes running along the borough's towns, connecting it with London Bridge and London Victoria. These local routes mainly run on the Brighton Main Line and Sutton & Mole Valley Lines. As well as the main stations of East Croydon and West Croydon, there are several suburban stations at Norwood Junction, Purley, Coulsdon South and Kenley and more.
The light rail system "Tramlink" (Operated by Tramtrack Croydon, a wholly owned subsidiary of Transport for London), opened in 2000, serves the borough and surrounding areas. Its network consists of three lines, from Elmers End to West Croydon, from Beckenham to West Croydon, and from New Addington to Wimbledon, with all three lines running via the Croydon loop on which it is centred on. It has been highly successful, environmentally-friendly and a reliable light rail system carrying around 22 million passengers a year. It is also the only tram system in London but there is another light rail system in the Docklands. It serves Mitcham, Woodside, Addiscombe and the Purley Way retail and industrial area amongst others. An extension to Crystal Palace is currently being developed by Transport for London with the support of the council and the South London Partnership. This would improve public transport access to Upper Norwood and Crystal Palace Park and help to stimulate regeneration across the wider area. The extension could be in service by 2013. Other possible extensions include Sutton, a new park and ride close to the M25, Coulsdon, Purley, Kingston upon Thames, Tolworth, Tooting, Brixton for an interchange with the proposed Cross River Tram, Bromley and Lewisham for an interchange with the Docklands Light Railway.
A sizeable bus infrastructure which is part of the London Buses network operates from a main hub at West Croydon station. The bus station at West Croydon is undergoing a major re-development to make it more modern and future-proof. There are also plans to create a new bus terminal at Park Place if the shopping centre is built. Addington Interchange is a regional bus terminal in Addington Village which has an interchange between route three and bus services in the remote area. Arriva London, part of Arriva, is one of the largest bus operators to serve Croydon along with Metrobus, Selkent, and National Express London. Unlike other places in the country, London's transport infrastructure is regulated and therefore is not subject to price wars between different companies with TfL setting a standard price for bus services which is currently set at 90p with an Oyster card. Services include buses to central London, Purley Way, Bromley, Lewisham and a number of other civic centres in the south. London Buses route X26, the longest route in London, provides services to Heathrow Airport via Richmond and Sutton.
Although hilly, Croydon is compact and has few major trunk roads running through it. It is on one of the Connect2 schemes which are part of the National Cycle Network route running around Croydon. The North Downs, an area of outstanding natural beauty popular with both on- and off-road cyclists, is so close to Croydon that part of the park lies within the borough boundary, and there are routes into the park almost from the civic centre.
Following the extension of the East London Line in 2010, two stations in Croydon are now served by London Overground services; Norwood Junction and West Croydon. Currently Croydon is one of only five London Boroughs not to have at least one London Underground station within its boundaries, and the closest tube station is in Morden.
Travel to work.
In March 2011, the main forms of transport that residents used to travel to work were: driving a car or van, 24.2% of all residents aged 16–74; train, 9.5%; bus, minibus or coach, 9.5%; on foot, 5.1%; underground, metro, light rail, tram, 4.3%; work mainly at or from home, 2.9%; passenger in a car or van, 1.5%.
Public services.
Home Office policing in Croydon is provided by the Metropolitan Police. The force's Croydon arm have their head offices for policing on Park Lane next to the Fairfield Halls and Croydon College in central Croydon. Public transport is co-ordinated by Transport for London. Statutory emergency fire and rescue service is provided by the London Fire Brigade, which has five stations in Croydon.
Health services.
NHS Croydon - Croydon Primary Care Trust is the body responsible for public health and for planning and funding health services in the borough. Croydon has 227 GPs in 64 practices, 156 dentists in 51 practices, 166 pharmacists and 70 optometrists in 28 practices.
The Mayday University Hospital, built on a site in Thornton Heath at the west of Croydon's boundaries with Merton, is a large NHS hospital administrated by Mayday Healthcare NHS Trust. Former names of the hospital include the Croydon Union Infirmary from 1885 to 1923 and the Mayday Road Hospital from 1923 to around 1930. It is a District General Hospital with a 24-hour accident and emergency department. NHS Direct has a regional centre based at the hospital. The NHS Trust also provides services at Purley War Memorial Hospital, in Purley. Croydon General Hospital was on London Road but services transferred to Mayday, as the size of this hospital was insufficient to cope with the growing population of the borough. Sickle Cell and Thalassaemia Centre and the Emergency Minor Treatment Centre are other smaller hospitals operated by the Mayday in the borough. Cane Hill was a psychiatric hospital in Coulsdon.
Waste management.
Waste management is co-ordinated by the local authority. Unlike other waste disposal authorities in Greater London, Croydon's rubbish is collected independently and isn't part of a waste authority unit. Locally produced inert waste for disposal is sent to landfill in the south of Croydon. There have recently been calls by the ODPM to bring waste management powers to the Greater London Authority, giving it a waste function. The Mayor of London has made repeated attempts to bring the different waste authorities together, to form a single waste authority in London. This has faced significant opposition from existing authorities. However, it has had significant support from all other sectors and the surrounding regions managing most of London's waste. Croydon has the joint best recycling rate in London, at 36%, but the refuse collectors have been criticised for their rushed performance lacking quality. Croydon's Distribution Network Operator for electricity is EDF Energy Networks; there are no power stations in the borough. Thames Water manages Croydon's drinking and waste water; water supplies being sourced from several local reservoirs, including Beckton and King George VI. Before 1971, Croydon Corporation was responsible for water treatment in the borough.
London Fire Brigade.
The borough of Croydon is 86.52 kmsq, populating approximately 340,000 people. There are five fire stations within the borough; Addington (two pumping appliances), Croydon (two pumping appliances, incident response unit, fire rescue unit and a USAR appliance), Norbury (two pumping appliances), Purley (one pumping appliance) and Woodside (one pumping appliance). Purley has the largest station ground, but dealt with the fewest incidents during 2006/07.
The fire stations, as part of the Community Fire Safety scheme, visited 49 schools in 2006/2007.
Education.
The borough compared with the other London boroughs has the highest amount of schools in it, with 26% of its population under 20 years old. They include primary schools (95), secondary schools (21) and four further education establishments. Croydon College has its main building in Central Croydon, it is a high rise building. John Ruskin College is one of the other colleges in the borough, located in Addington and Coulsdon College in Coulsdon. South Norwood has been the home of Spurgeon's College, a world-famous Baptist theological college, since 1923; Spurgeon's is located on South Norwood Hill and currently has some 1000 students. The London Borough of Croydon is the local education authority for the borough.
Overall, Croydon was ranked 77th out of the all the Local Education Authorities in the UK, up from 92nd in 2007. In 2007, the Croydon LEA was ranked 81st out of 149 in the country – and 21st in Greater London – based on the percentage of pupils attaining at least 5 A*–C grades at GCSE including maths and English (37.8% compared with the national average of 46.7%). The most successful public sector schools in 2010 were Harris City Academy Crystal Palace and Coloma Convent Girls' School. The percentage of pupils achieving 5 A*-C GCSEs including maths and English was above the national average in 2010.
Libraries.
The borough of Croydon has 14 libraries, a joint library and a mobile library. Many of the libraries were built a long time ago and therefore have become outdated, so the council started updating a few including Ashburton Library which moved from its former spot into the state-of-the-art Ashburton Learning Village complex which is on the former site of the old 'A Block' of Ashburton Community School which is now situated inside the centre. The library is now on one floor. This format was planned to be rolled out across all of the council's libraries but what was seen as costing too much.
South Norwood Library, New Addington Library, Shirley Library, Selsdon Library, Sanderstead Library, Purley Library, Coulsdon Library and Bradmore Green Library are examples of older council libraries. The main library is Croydon Central Library which holds many references, newspaper archives and a tourist information point (one of three in southeast London). Upper Norwood Library is a joint library with the London Borough of Lambeth. This means that both councils fund the library and its resources, but even though Lambeth have nearly doubled their funding for the library in the past several years Croydon has kept it the same, doubting the future of the library.
Religion.
The predominant religion of the borough is Christianity. According to the United Kingdom Census 2001, the borough has over 215,124 Christians, mainly Protestants. This is the largest religious following in the borough followed by Islam with 17,642 Muslims resident. This is a small portion of the more than 600,000 Muslims in London as a whole.
48,615 Croydon residents stated that they are atheist or non-religious in the 2001 Census.
There are more than 35 churches in the borough, with Croydon Minster being the main one. This church was founded in Saxon times, since there is a record of "a priest of Croydon" in 960, although the first record of a church building is in the Domesday Book (1086). In its final medieval form, the church was mainly a Perpendicular-style structure, but this was severely damaged by fire in 1867, following which only the tower, south porch and outer walls remained. Under the direction of Sir George Gilbert Scott the church was rebuilt, incorporating the remains and essentially following the design of the medieval building, and was reconsecrated in 1870. It still contains several important monuments and fittings saved from the old church.
Croydon has strong religious links, from a royal charter for Surrey Street Market dating back to 1276, to Croydon Palace which was the summer residence of the Archbishop of Canterbury for over 500 years, with visitors such as Henry III and Queen Elizabeth I. The Bishop of Croydon is a position as a suffragan Bishop in the Anglican Diocese of Southwark. The latest bishop was Rt Rev Nicholas (Nick) Baines.
Sport and leisure.
The borough has been criticized in the past for not having enough leisure facilities, maintaining the position of Croydon as a three star borough. At the moment only three leisure centres are open for public use and two of these are expected to be closed down in the near future, with plans for only one of them to be re-built. Thornton Heath's ageing sports centre was recently demolished and replaced by a newer more modern leisure centre. South Norwood Leisure Centre was closed down in early 2006 so that it could be demolished and re-designed from scratch like Thornton Heath, at an estimated cost of around £10 million.
In May 2006 the Conservative Party took control of Croydon Council and decided a refurbishment would be more economical than rebuilding, this decision caused some controversy.
Purley Pool is to close soon, but a new "super-pool" is planned in Coulsdon. The ageing New Addington Leisure Centre is also set to close but is to be re-built. A new leisure centre is also going to be built on the A23, southern end of Purley Way in Waddon.
Sport Croydon, is the commercial arm for leisure in the borough. Fusion currently provides leisure services for the council, a contract previously held by Parkwood Leisure.
Football teams include Crystal Palace F.C., which play at Selhurst Park, and in the Premier League. Coulsdon Town F.C. are a team that currently play in the Surrey Elite League, Division One. AFC Croydon Athletic, whose nickname is The Rams, is a football club who play at Croydon Sports Arena along with Croydon F.C., both in the Combined Counties League and Holmesdale, who were founded in South Norwood but currently playing on Oakley Road in Bromley, currently in the Southern Counties East Football League. Non-football teams that play in Croydon are Streatham-Croydon RFC, a historic rugby union club in Thornton Heath who play at Frant Road, as well as South London Storm Rugby League Club, based at Streatham's ground, who compete in the Rugby League Conference. Another rugby union club that play in Croydon is Croydon RFC, who play at Addington Road. The London Olympians are an American Football team that play in Division 1 South in the British American Football League. The Croydon Pirates are one of the most successful teams in the British Baseball Federation, though their ground is actually just located outside the borough in Sutton.
Croydon Amphibians SC plays in the Division 2 British Water Polo League. The team won the National League Division 2 in 2008.
Croydon has over 120 parks and open spaces, ranging from the Selsdon Wood Nature Reserve to many recreation grounds and sports fields scattered throughout the Borough.
Culture.
Croydon aims to become one of the hearts of culture in London and the South East of England.
This has been proved with the dedication the council has shown to projects such as the proposed Croydon Arena.
It has cut funding to the Warehouse Theatre.
In 2005, Croydon Council drew up a "Public Art Strategy", with a vision intended to be accessible and to enhance people's enjoyment of their surroundings. The public art strategy delivered a new event called "Croydon's Summer Festival" hosted in Lloyd Park. The festival consists of two days of events. The first is called "Croydon's World Party" which is a free one-day event with three stages featuring world, jazz and dance music from the UK and internationally. The final days event is the "Croydon Mela", a day of music with a mix of traditional Asian culture and east-meets-western club beats across four stages as well as dozens of food stalls and a funfair. It has attracted crowds of over 50,000 people. The strategy also created a creative industries hub in Old Town, ensured that public art is included in developments such as College Green and Ruskin Square and investigated the possibility of gallery space in the Cultural Quarter.
Fairfield Halls, Arnhem Gallery and the Ashcroft Theatre show productions that are held throughout the year such as drama, ballet, opera and pantomimes and can be converted to show films. It also contains the Arnhem Gallery civic hall and an art gallery. Other cultural activities, including shopping and exhibitions, are Surrey Street Market which is mainly a meat and vegetables market near the main shopping environment of Croydon. The market has a Royal Charter dating back to 1276. Airport House is a newly refurbished conference and exhibition centre inside part of Croydon Airport. The Whitgift Centre, the current main shopping centre in the borough is also one of the largest in-town shopping centres in the whole of Europe. Centrale, a new shopping centre that houses many more familiar names, as well as Croydon's House of Fraser. North End, the main shopping street, which holds both centres. Park Place, a shopping centre that is planned to be built in Central Croydon by Minerva. Croydon Arena is a proposed construction for the Ruskin Square site which if built would feature more commercial exhibitions and sporting events next to East Croydon station.
Media.
There are three local newspapers which operate within the borough. The Croydon Advertiser began life in 1869, and was in 2005 the third-best selling paid-for weekly newspaper in London. The Advertiser is Croydon's major paid-for weekly paper and is on sale every Friday in five geographical editions: Croydon; Sutton & Epsom; Coulsdon & Purley; New Addington; and Caterham. The paper converted from a broadsheet to a compact (tabloid) format on 31 March 2006. It was bought by Northcliffe Media which is part of the Daily Mail and General Trust group on 6 July 2007. The Croydon Post is a free newspaper available across the borough and is operated by the Advertiser group. The circulation of the newspaper was in 2008 more than the main title published by the Advertiser Group.
The Croydon Guardian is another local weekly paper, which is paid for at newsagents but free at Croydon Council libraries and via deliveries. It is one of the best circulated local newspapers in London and once had the highest circulation in Croydon with around one thousand more copies distributed than The Post.
The borough is served by the London regional versions of BBC and ITV coverage, from either the Crystal Palace or Croydon transmitters.
Croydon Television is owned by Croydon broadcasting corporation. Broadcasting from studios in Croydon, the CBC is fully independent. It does not receive any government or local authority grants or funding and is supported by donations, sponsorship and by commercial advertising.
Capital Radio and Classic Gold Digital 1521 serve the borough. Local BBC radio is provided by BBC London 94.9. Other stations include Kiss 100 and Magic 105.4 FM from Bauer Radio, Capital Xtra, Heart 106.2 and Smooth Radio from Global Radio and Absolute Radio from TIML Radio Limited. In 2012, Croydon Radio, an internet radio station, began serving the area.
Twinning.
The London Borough of Croydon is twinned with the municipality of Arnhem which is located in the east of the Netherlands. The city of Arnhem is one of the 10 largest cities in the Netherlands. They have been twinned since 1946 after both towns had suffered extensive bomb damage during the recently ended war. There is also a Guyanese link supported by the council.
Investment in the tobacco industry.
In September 2009 it was revealed that Croydon Council had around £20m of its pension fund for employees invested in shares in Imperial Tobacco and British American Tobacco. Members of the opposition Labour group on the council, who had banned such shareholdings when in control, described this as "dealing in death" and inconsistent with the council's tobacco control strategy.

</doc>
<doc id="7188" url="http://en.wikipedia.org/wiki?curid=7188" title="Carme (moon)">
Carme (moon)

Carme ( ; ) is a retrograde irregular satellite of Jupiter. It was discovered by Seth Barnes Nicholson at Mount Wilson Observatory in California in July 1938. It is named after the mythological Carme, mother by Zeus of Britomartis, a Cretan goddess.
Carme did not receive its present name until 1975; before then, it was simply known as . It was sometimes called "Pan" between 1955 and 1975 (Pan is now the name of a satellite of Saturn).
It gives its name to the Carme group, made up of irregular retrograde moons orbiting Jupiter at a distance ranging between 23 and 24 Gm and at an inclination of about 165°. Its orbital elements are as of January 2000. They are continuously changing due to solar and planetary perturbations.

</doc>
<doc id="7193" url="http://en.wikipedia.org/wiki?curid=7193" title="Commutator">
Commutator

In mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.
Group theory.
The commutator of two elements, "g" and "h", of a group "G", is the element
It is equal to the group's identity if and only if "g" and "h" commute (i.e., if and only if "gh" = "hg"). The subgroup of "G" generated by all commutators is called the "derived group" or the "commutator subgroup" of "G". Note that one must consider the subgroup generated by the set of commutators because in general the set of commutators is not closed under the group operation. Commutators are used to define nilpotent and solvable groups.
The above definition of the commutator is used by some group theorists, as well as throughout this article. However, many other group theorists define the commutator as
Identities (group theory).
Commutator identities are an important tool in group theory. The expression "ax" denotes the conjugate of "a" by "x", defined as "x"−1"a x".
Identity 5 is also known as the "[[Hall–Witt identity]]". It is a group-theoretic analogue of the [[Jacobi identity]] for the ring-theoretic commutator (see next section).
N.B. The above definition of the conjugate of "a" by "x" is used by some group theorists. Many other group theorists define the conjugate of "a" by "x" as "xax−1". This is often written formula_9. Similar identities hold for these conventions.
A wide range of identities are used that are true modulo certain subgroups. These can be particularly useful in the study of [[solvable group]]s and [[nilpotent group]]s. For instance, in any group second powers behave well
If the [[derived subgroup]] is central, then
Ring theory.
The commutator of two elements "a" and "b" of a [[ring (algebra)|ring]] or an [[associative algebra]] is defined by
It is zero if and only if "a" and "b" commute. In [[linear algebra]], if two endomorphisms of a space are represented by commuting matrices with respect to one basis, then they are so represented with respect to every basis.
By using the commutator as a [[Lie algebra|Lie bracket]], every associative algebra can be turned into a [[Lie algebra]].
The anticommutator of two elements "a" and "b" of a ring or an associative algebra is defined by
Sometimes the brackets [ ]+ are also used. The anticommutator is used less often than the commutator, but can be used for example to define [[Clifford algebra]]s, [[Jordan algebra]]s and is utilised to derive the [[Dirac equation]] in particle physics.
In physics, this is an important overarching principle in [[quantum mechanics]]. The commutator of two operators acting on a [[Hilbert space]] is a central concept in [[quantum mechanics]], since it quantifies how well the two [[observable]]s described by these operators can be measured simultaneously. The [[uncertainty principle]] is ultimately a [[theorem]] about such commutators, by virtue of the [[Uncertainty relation|Robertson–Schrödinger relation]]. 
In [[phase space]], equivalent commutators of function [[Moyal product|star-products]] are called [[Moyal bracket]]s, and are completely isomorphic to the Hilbert-space commutator structures mentioned.
Identities (ring theory).
The commutator has the following properties:
"Lie-algebra relations:"
The second relation is called [[anticommutativity]], while the third is the [[Jacobi identity]].
"Additional relations:"
If "A" is a fixed element of a ring ℜ, the second additional relation can also be interpreted as a [[Leibniz rule]] for the map formula_28 given by "B" ↦ ["A","B"]. In other words, the map "DA" defines a [[derivation (abstract algebra)|derivation]] on the ring ℜ.
The following identity ("Hadamard Lemma") involving nested commutators, underlying the [[Baker–Campbell–Hausdorff formula#The Hadamard lemma|Campbell–Baker–Hausdorff expansion]] of log (exp "A" exp "B"), is also useful:
Use of the same expansion expresses the above Lie group commutator in terms of a series of nested Lie bracket (algebra) commutators,
These identities differ slightly for the anticommutator (defined above)
Graded rings and algebras.
When dealing with [[graded algebra]]s, the commutator is usually replaced by the graded commutator, defined in homogeneous components as formula_32
Derivations.
Especially if one deals with multiple commutators, another notation turns out to be useful involving the [[Adjoint representation of a Lie algebra|adjoint representation]]:
Then formula_34 is a [[derivation (abstract algebra)|derivation]] and formula_35 is linear, "i.e.", formula_36 and formula_37, and a [[Lie algebra]] homomorphism, "i.e.", formula_38, but it is not always an algebra homomorphism, "i.e." the identity formula_39 does not hold in general.
Examples:
External links.
[[Category:Abstract algebra]]
[[Category:Group theory]]
[[Category:Binary operations]]
[[Category:Mathematical identities]]

</doc>
<doc id="7196" url="http://en.wikipedia.org/wiki?curid=7196" title="Cairn">
Cairn

A cairn is a man-made pile (or stack) of stones. The word "cairn" comes from the (plural ). 
Cairns are used as trail markers in many parts of the world, in uplands, on moorland, on mountaintops, near waterways and on sea cliffs, and also in barren desert and tundra areas. They vary in size from small stone markers to entire artificial hills, and in complexity from loose, conical rock piles to delicately balanced sculptures and elaborate feats of megalithic engineering. Cairns may be painted or otherwise decorated, whether for increased visibility or for religious reasons. An ancient example is the inuksuk (plural inuksuit), used by the Inuit, Inupiat, Kalaallit, Yupik, and other peoples of the Arctic region of North America. These structures are found from Alaska to Greenland. This region, above the Arctic Circle, is dominated by the tundra biome and has areas with few natural landmarks.
In modern times, cairns are often erected as landmarks, a use they have had since ancient times, but since prehistory they have also been built for a variety of other reasons, such as burial monuments, and for defence and hunting, as well as ceremonial, astronomical, and other purposes.
Modern cairns.
Today, cairns are built for many purposes. The most common use in North America and Northern Europe is to mark mountain bike and hiking trails and other cross-country trail blazing, especially in mountain regions at or above the tree line. For example, the extensive trail network maintained by the DNT, the Norwegian Trekking Association, extensively uses cairns in conjunction with T-painted rock faces to mark trails. Other examples of these can be seen in the lava fields of Volcanoes National Park to mark several hikes. Placed at regular intervals, a series of cairns can be used to indicate a path across stony or barren terrain, even across glaciers. Such cairns are often placed at junctions or in places where the trail direction is not obvious. They may also be used to indicate an obscured danger such as a sudden drop, or a noteworthy point such as the summit of a mountain. Most trail cairns are small, usually being a foot or less in height. However, they may be built taller so as to protrude through a layer of snow. Hikers passing by often add a stone, as a small bit of maintenance to counteract the erosive effects of severe weather. North American trail marks are sometimes called "ducks" or "duckies", because they sometimes have a "beak" pointing in the direction of the route. The expression "two rocks do not make a duck" reminds hikers that just one rock resting upon another could be the result of accident or nature rather than intentional trail marking.
The building of cairns for recreational purposes along trails, to mark one's personal passage through the area, can result in an overabundance of rock piles. This distracts from cairns used as genuine navigational guides, and also conflicts with the Leave No Trace ethic. This ethic of outdoor practice advocates for leaving the outdoors undisturbed and in its natural condition.
Coastal cairns, or "sea marks", are also common in the northern latitudes, especially in the island-strewn waters of Scandinavia and eastern Canada. Often indicated on navigation charts, they may be painted white or lit as beacons for greater visibility offshore.
Modern cairns may also be erected for historical or memorial commemoration or simply for decorative or artistic reasons. One example is a series of many cairns marking British soldiers' mass graves at the site of the Battle of Isandlwana, South Africa. Another is the Matthew Flinders Cairn on the side of Arthur's Seat, a small mountain on the shores of Port Phillip Bay, Australia. A large cairn, commonly referred to as "the igloo" by the locals, was built atop a hill next to the I-476 highway in Radnor, Pennsylvania and is a part of a series of large rock sculptures initiated in 1988 to symbolize the township's Welsh heritage and to beautify the visual imagery along the highway. Some are merely places where farmers have collected stones removed from a field. These can be seen in the Catskill Mountains, North America where there is a strong Scottish heritage, and may also represent places where livestock were lost. In locales exhibiting fantastic rock formations, such as the Grand Canyon, tourists often construct simple cairns in reverence of the larger counterparts. By contrast, cairns may have a strong aesthetic purpose, for example in the art of Andy Goldsworthy.
History.
Europe.
The building of cairns for various purposes goes back into prehistory in Eurasia, ranging in size from small rock sculptures to substantial man-made hills of stone (some built on top of larger, natural hills). The latter are often relatively massive Bronze Age or earlier structures which, like kistvaens and dolmens, frequently contain burials; they are comparable to tumuli (kurgans), but of stone construction instead of earthworks. "Cairn" originally could more broadly refer to various types of hills and natural stone piles, but today is used exclusively of artificial ones.
The word "cairn" derives from Scots ' (with the same meaning), in turn from Scottish Gaelic ', which is essentially the same as the corresponding words in other native Celtic languages of Britain Ireland and Brittany, including Welsh ' (and '), Breton ', Irish ', and Cornish ' or '. Cornwall (') itself may actually be named after the cairns that dot its landscape, such as Cornwall's highest point, Brown Willy Summit Cairn, a 5 m (16 ft) high and 24 m (79 ft) diameter mound atop Brown Willy hill in Bodmin Moor, an area with many ancient cairns. Burial cairns and other megaliths are the subject of a variety of legends and folklore throughout Britain and Ireland. In Scotland, it is traditional to carry a stone up from the bottom of a hill to place on a cairn at its top. In such a fashion, cairns would grow ever larger. An old Scottish Gaelic blessing is ', "I'll put a stone on your cairn". In Highland folklore it is believed that the Highland Clans, before they fought in a battle, each man would place a stone in a pile. Those who survived the battle returned and removed a stone from the pile. The stones that remained were built into a cairn to honour the dead. Cairns in the region were also put to vital practical use. For example, Dún Aonghasa, an all-stone Iron Age Irish hill fort on Inishmore in the Aran Islands, is still surrounded by small cairns and strategically placed jutting rocks, used collectively as an alternative to defensive earthworks because of the karst landscape's lack of soil.
In Scandinavia, cairns have been used for centuries as trail and sea marks, among other purposes. In Iceland, cairns were often used as markers along the numerous single-file roads or paths that crisscrossed the island; many of these ancient cairns are still standing, although the paths have disappeared. In Norse Greenland, cairns were used as a hunting implement, a game-driving "lane", used to direct reindeer towards a game jump.
In the mythology of ancient Greece, cairns were associated with Hermes, the god of overland travel. According to one legend, Hermes was put on trial by Hera for slaying her favorite servant, the monster Argus. All of the other gods acted as a jury, and as a way of declaring their verdict they were given pebbles, and told to throw them at whichever person they deemed to be in the right, Hermes or Hera. Hermes argued so skillfully that he ended up buried under a heap of pebbles, and this was the first cairn.
In Croatia, in areas of ancient Dalmatia, such as Herzegovina and the Krajina, they are known as "gromila".
In Portugal a cairn is called '. In a legend the stones, moledros, are enchanted soldiers, and if one stone is taken from the pile and put under a pillow in the morning a soldier will appear for a brief moment, then will change back to a stone and magically return to the pile. The cairns that mark the place where someone died or cover the graves alongside the roads where in the past people were buried are called '. The same name given to the stones was given the dead whose identity was unknown. The ' or ' are in the Galician legends, spirits of the night. The word "Fes" or "Fieis" is thought to mean fairy, the same root as fate (Fado) that can take the same meaning as proto-Celtic *bato- meaning death.
Cairns are also common on the Mediterranean island of Corsica.,
North and Northeast Africa.
Cairns ("taalo") are a common feature at El Ayo, Haylan, Qa’ableh, Qombo'ul and Heis, among other places. Northern Somalia in general is home to a lot of such historical settlements and archaeological sites wherein are found numerous ancient ruins and buildings, many of obscure origins. However, many of these old structures have yet to be properly explored, a process which would help shed further light on local history and facilitate their preservation for posterity.
Since Neolithic times, the climate of North Africa has become drier. A reminder of the desertification of the area is provided by megalithic remains, which occur in a great variety of forms and in vast numbers in presently arid and uninhabitable wastelands: cairns ("kerkour"), dolmens and circles like Stonehenge, underground cells excavated in rock, barrows topped with huge slabs, and step pyramid-like mounds.
Asia and the Pacific.
Starting in the Bronze Age, burial cists were sometimes interred into cairns, which would be situated in conspicuous positions, often on the skyline above the village of the deceased. The stones may have been thought to deter grave robbers and scavengers. A more sinister explanation is that they were to stop the dead from rising. There remains a Jewish tradition of placing small stones on a person's grave as a token of respect, though this is generally to relate the longevity of stone to the eternal nature of the soul and is not usually done in a cairn fashion. Stupas in India and Tibet probably started out in a similar fashion, although they now generally contain the ashes of a Buddhist saint or lama.
A traditional and often decorated, heap-formed cairn called an "ovoo" is made in Mongolia. It primarily serves religious purposes, and finds use in both Tengriist and Buddhist ceremonies.
In Hawaii, cairns are called by the Hawaiian word .
In South Korea cairns are quite prevalent, often found along roadsides and trails, up on mountain peaks, and adjacent to Buddhist temples. Hikers frequently add stones to existing cairns trying to get just one more on top of the pile, to bring good luck. This tradition has its roots in the worship of San-shin, or Mountain Spirit, so often still revered in Korean culture.
The Americas.
Throughout what today are the continental United States and Canada, cairns still mark indigenous peoples' game-driving "lanes" leading to buffalo jumps, some of which may date to 12,000 years ago.
Natives of arctic North America (i.e. northern Canada, Alaska and indigenous Greenland) have built carefully constructed cairns and stone sculptures, called by names such as , as landmarks and directional markers since before contact with Europeans. They are iconic of the region (an "" even features on the flag of the Canadian far-northeastern territory, Nunavut), and are increasingly used as a symbol of Canadian national identity.
In North America, cairns are often petroforms in the shapes of turtles or other animals.
Cairns have been used since pre-Columbian times throughout Latin America to mark trails. Even today, in the Andes of South America, the Quechuan peoples use cairns as religious shrines to the indigenous Inca goddess Pachamama. This is often part of a synchretic form of Roman Catholicism.
The building of cairns in the American Southwest has proliferated in the last decade and has caused much controversy regarding the conflict between providing navigational trail markers and protecting the remaining natural condition of the landscape. In many cases the greatest numbers of cairns appear along trails which are themselves well trodden and unmistakable paths, frequently with official signage and not needing further markers. In some areas park rangers and land managers must routinely disassemble recreational cairns which have become eyesores or misleading markers. There is evidence that New Age spiritualism is a motivation for the construction of personal cairns throughout the landscape.
Cairns and anthropomorphism.
Although the practice is not common in English, cairns are sometimes referred to by their anthropomorphic qualities. In German and Dutch, a cairn is known as and respectively, meaning literally "stone man". A form of the Inuit is also meant to represent a human figure, and is called an ' ("imitation of a person"). In Italy, especially the Italian Alps, a cairn is an ', or a "small man".
Sea cairns.
Coastal cairns called sea marks are also common in the northern latitudes, and are placed along shores and on islands and islets. Usually painted white for improved offshore visibility, they serve as navigation aids. In Scandinavia they are called ' in Swedish and ' in Finnish, and are indicated in navigation charts and maintained as part of the nautical marking system. Inversely, they are used on land as sea cliff warnings in rugged and hilly terrain in the foggy Faroe Islands. In the Canadian Maritimes, cairns have been used as beacons like small lighthouses to guide boats, as depicted in the novel "The Shipping News".

</doc>
<doc id="7198" url="http://en.wikipedia.org/wiki?curid=7198" title="Characteristic subgroup">
Characteristic subgroup

In mathematics, particularly in the area of abstract algebra known as group theory, a characteristic subgroup is a subgroup that is invariant under all automorphisms of the parent group. Because conjugation is an automorphism, every characteristic subgroup is normal, though not every normal subgroup is characteristic. Examples of characteristic subgroups include the commutator subgroup and the center of a group.
Definitions.
A characteristic subgroup of a group "G" is a subgroup "H" that is invariant under each automorphism of "G". That is,
for every automorphism "φ" of "G" (where "φ"("H") denotes the image of "H" under "φ").
The statement “"H" is a characteristic subgroup of "G"” is written
Characteristic vs. normal.
If "G" is a group, and "g" is a fixed element of "G", then the conjugation map
is an automorphism of "G" (known as an inner automorphism). A subgroup of "G" that is invariant under all inner automorphisms is called normal. Since a characteristic subgroup is invariant under all automorphisms, every characteristic subgroup is normal.
Not every normal subgroup is characteristic. Here are several examples:
Note: If "H" is the unique subgroup of a group "G", then "H" is characteristic in "G".
Comparison to other subgroup properties.
Distinguished subgroups.
A related concept is that of a distinguished subgroup (also called strictly characteristic subgroup). In this case the subgroup "H" is invariant under the applications of surjective endomorphisms. For a finite group this is the same, because surjectivity implies injectivity, but not for an infinite group: a surjective endomorphism is not necessarily an automorphism.
Fully invariant subgroups.
For an even stronger constraint, a fully characteristic subgroup (also called a fully invariant subgroup) "H" of a group "G" is a group remaining invariant under every endomorphism of "G"; in other words, if "f" : "G" → "G" is any homomorphism, then "f"("H") is a subgroup of "H".
Verbal subgroups.
An even stronger constraint is verbal subgroup, which is the image of a fully invariant subgroup of a free group under a homomorphism.
Containments.
Every subgroup that is fully characteristic is certainly distinguished and therefore characteristic; but a characteristic or even distinguished subgroup need not be fully characteristic.
The center of a group is always a distinguished subgroup, but it is not always fully characteristic. The finite group of order 12, Sym(3) × Z/2Z has a homomorphism taking ("π", "y") to ( (1,2)"y", "0") which takes the center 1 × Z/2Z into a subgroup of Sym(3) × 1, which meets the center only in the identity.
The relationship amongst these subgroup properties can be expressed as:
Examples.
Finite example.
Consider the group "G" = S3 × Z2 (the group of order 12 which is the direct product of the symmetric group of order 6 and a cyclic group of order 2). The center of "G" is its second factor Z2. Note that the first factor S3 contains subgroups isomorphic to Z2, for instance {identity,(12)}; let "f": Z2 → S3 be the morphism mapping Z2 onto the indicated subgroup. Then the composition of the projection of "G" onto its second factor Z2, followed by "f", followed by the inclusion of S3 into "G" as its first factor, provides an endomorphism of "G" under which the image of the center Z2 is not contained in the center, so here the center is not a fully characteristic subgroup of "G".
Cyclic groups.
Every subgroup of a cyclic group is characteristic.
Subgroup functors.
The derived subgroup (or commutator subgroup) of a group is a verbal subgroup. The torsion subgroup of an abelian group is a fully invariant subgroup.
Topological groups.
The identity component of a topological group is always a characteristic subgroup.
Transitivity.
The property of being characteristic or fully characteristic is transitive; if "H" is a (fully) characteristic subgroup of "K", and "K" is a (fully) characteristic subgroup of "G", then "H" is a (fully) characteristic subgroup of "G".
Moreover, while it is not true that every normal subgroup of a normal subgroup is normal, it is true that every characteristic subgroup of a normal subgroup is normal. Similarly, while it is not true that every distinguished subgroup of a distinguished subgroup is distinguished, it is true that every fully characteristic subgroup of a distinguished subgroup is distinguished.
Map on Aut and End.
If formula_6, then every automorphism of "G" induces an automorphism of the quotient group "G/H", which yields a map formula_7.
If "H" is fully characteristic in "G", then analogously, every endomorphism of "G" induces an endomorphism of "G/H", which yields a map
formula_8.

</doc>
<doc id="7199" url="http://en.wikipedia.org/wiki?curid=7199" title="List of cat breeds">
List of cat breeds

The following list of cat breeds includes domestic cat breeds and domestic/wild hybrids. The list includes established breeds recognized by various cat registries, new and experimental breeds, distinct domestic populations not being actively developed and lapsed breeds.
Inconsistency in breed classification among registries means that an individual animal may be considered different breeds by different registries. For example, The International Cat Association's Himalayan is considered a colorpoint version of the Persian by the Cat Fanciers' Association while the CFA's Javanese is considered a color variation of the Balinese by TICA.
The domestic shorthair and domestic longhair are not breeds but terms used in cat fancy to describe cats that do not belong to a particular breed.

</doc>
<doc id="7200" url="http://en.wikipedia.org/wiki?curid=7200" title="Class action">
Class action

A class action, class suit, or representative action is a lawsuit where a group of people in similar circumstances (a class) sues another party, usually consumers suing a large business. It is pleonastic to refer to a class action as a "class action suit". Collective lawsuits originated in the United States and are still predominantly a U.S. phenomenon. But in several European countries with civil law, contrary to Anglo-American common law, changes have been made in recent years to allow consumer organizations to bring claims on behalf of consumers.
U.S. federal class actions.
In the United States federal courts, class actions are governed by Federal Rules of Civil Procedure Rule 23 and 28 U.S.C.A. § 1332(d).
Class actions may be brought in federal court if the claim arises under federal law or if the claim falls under 28 USCA § 1332(d). Under § 1332(d) (2) the federal district courts have original jurisdiction over any civil action where the amount in controversy exceeds $5,000,000 and
Nationwide plaintiff classes are possible, but such suits must have a commonality of issues across state lines. This may be difficult if the civil law in the various states lack significant commonalities. Large class actions brought in federal court frequently are consolidated for pre-trial purposes through the device of multidistrict litigation (MDL). It is also possible to bring class actions under state law, and in some cases the court may extend its jurisdiction to all the members of the class, including out of state (or even internationally) as the key element is the jurisdiction that the court has over the defendant.
Typically, federal courts are thought to be more favorable for defendants, and state courts more favorable for plaintiffs. Many class actions are filed initially in state court. The defendant will frequently try to remove the case to federal court. The Class Action Fairness Act of 2005 increases defendants' ability to remove state cases to federal court by giving federal courts original jurisdiction for all class actions with damages exceeding $5,000,000 exclusive of interest and costs. It should be noted, however, that the Class Action Fairness Act contains carve-outs for "inter alia", shareholder class actions covered by the Private Securities Litigation Reform Act of 1995 and those concerning internal corporate governance issues (the latter typically being brought as shareholder derivative actions in the state courts of Delaware, the state of incorporation of most large corporations).
The procedure for filing a class action is to file suit with one or several named plaintiffs on behalf of a proposed class. The proposed class must consist of a group of individuals or business entities that have suffered a common injury or injuries. Typically these cases result from an action on the part of a business or a particular product defect or policy that applied to all proposed class members in a typical manner. After the complaint is filed, the plaintiff must file a motion to have the class certified. In some cases class certification may require discovery in order to determine its size and if the proposed class meets the standard for class certification.
Upon the motion to certify the class, the defendants may object to whether the issues are appropriately handled as a class action, to whether the named plaintiffs are sufficiently representative of the class, and to their relationship with the law firm or firms handling the case. The court will also examine the ability of the firm to prosecute the claim for the plaintiffs and their resources for dealing with class actions.
Due process requires in most cases that notice describing the class action be sent, published, or broadcast to class members. As part of this notice procedure, there may have to be several notices, first a notice giving class members the opportunity to opt out of the class, i.e. if individuals wish to proceed with their own litigation they are entitled to do so, only to the extent that they give timely notice to the class counsel or the court that they are opting out. Second, if there is a settlement proposal, the court will usually direct the class counsel to send a settlement notice to all the members of the certified class, informing them of the details of the proposed settlement.
In federal civil procedure law, which has also been accepted by approximately 35 states (through adoption of state civil procedure rules similar to the federal rules), the class action must have certain definite characteristics (often referred to by the acronym CANT):
State class actions.
Since 1938, many states have adopted rules similar to the FRCP. However, some states, like California, have civil procedure systems, which deviate significantly from the federal rules; the California Codes provide for four separate types of class actions. As a result, there are two separate treatises devoted solely to the complex topic of California class actions. Some states, such as Virginia, do not provide for any class actions, while others, such as New York, limit the types of claims that may be brought as class actions. In 2013, several class action lawsuits have been filed under both the federal Fair Debt Collection Practices Act and similar Michigan state laws against some of Michigan's largest medical providers and collection agencies, which may have a dramatic impact on the collection of unpaid medical bills and the collection industry as a whole.
History.
The ancestor of the class action was what modern observers call "group litigation", which appears to have been quite common in medieval England from about 1200 onward. These lawsuits involved groups of people either suing or being sued in actions at common law. These groups were usually based on existing societal structures like villages, towns, parishes, and guilds. What is striking about these early cases is that unlike modern courts, the medieval English courts "never" questioned the right of the actual plaintiffs to sue on behalf of a group or a few representatives to defend an entire group.
As UCLA law professor Stephen Yeazell has pointed out, the most likely reason is that the abysmally poor transportation, communications, and administrative apparatus of medieval times made it impossible for the English sovereign to directly manage the entire country in terms of individuals; it was easier to structure society by imposing obligations upon groups which were enforced by the sporadic use of force. In turn, lawyers and judges who operated the king's system of justice in a society strictly organized into groups would "not" question the right of a group to sue or be sued because to do so would bring into question the entire group-oriented society in which they operated.
From 1400 to 1700, group litigation gradually switched from being the norm in England to the exception. The development of the concept of the corporation led to the wealthy supporters of the corporate form becoming suspicious of all unincorporated legal entities, which in turn led to the modern concept of the unincorporated or voluntary association. The tumultuous history of the Wars of the Roses and then the Star Chamber resulted in periods during which the common law courts were frequently paralyzed, and out of the confusion the Court of Chancery emerged with exclusive jurisdiction over group litigation.
Chancery cases on group litigation after 1700 were a totally incoherent mess, which Yeazell has explained by pointing to the trends towards fragmentation and individualism in English society during that period; the resulting societal pressures ultimately led to the Reform Act 1832. The problem which confounded Chancery was the shift from representation based on the presumed or implied "consent" of a group to representation based on a common "interest", such as holding shares of a corporation. Group litigation has struggled with the tension between consent and interest ever since.
By 1850, Parliament had enacted several statutes on a case-by-case basis to deal with issues regularly faced by certain types of organizations, like joint-stock companies, and with the impetus for most types of group litigation removed, it went into a steep decline in English jurisprudence from which it never recovered. It was further weakened by the fact that equity pleading in general was falling into disfavor, which culminated in the Judicature Acts of 1874 and 1875. Group litigation was essentially dead in England after 1850.
Group litigation survived in the United States only thanks to the influence of Supreme Court Associate Justice Joseph Story, who imported it in a rather mangled form into U.S. law through summary discussions in his two equity treatises as well as his famous opinion in "West v. Randall" (1820). Although Story was highly intelligent and familiar with all the relevant English precedents, he merely summarized them in a confused fashion because he "could not conceive of a modern function or a coherent theory for representative litigation: why?" Like most Americans then and since, Story took individualism for granted; on that basis, he simply could not comprehend a rule that allowed a court to bind someone who had never been a party to litigation purportedly conducted on his behalf.
The oldest predecessor to the class action rule was Equity Rule 48, promulgated in 1833, which allowed for representative suits in situations where there were too many individual parties (which now forms the first requirement for class action litigation, numerosity). However, this rule did not allow such suits to bind similarly situated absent parties, which rendered the rule almost entirely useless and was a direct reflection of Story's inability to understand the old English Chancery precedents. Story's confusion was apparently typical of 19th-century American lawyers, as even the legendary Christopher Columbus Langdell also could not understand the old cases.
Within ten years, the Supreme Court interpreted Rule 48 in such a way so that it could apply to absent parties under certain circumstances, but only by ignoring the plain meaning of the rule. In the early 20th century, Equity Rule 48 was replaced with Equity Rule 38 as part of a major restructuring of the Equity Rules, and when federal courts merged their legal and equitable procedural systems in 1938, Equity Rule 38 became Rule 23 of the Federal Rules of Civil Procedure.
A major revision of the FRCP in 1966 radically transformed Rule 23, made the opt-out class action the standard option, and gave birth to the modern class action. Entire treatises have been written since to summarize the huge mass of law that sprang up from the 1966 revision of Rule 23. Just as medieval group litigation bound all members of the group regardless of whether they all actually appeared in court, the modern class action binds "all" members of the class, with the exception of those who appear and object.
The Advisory Committee that drafted the new Rule 23 in the mid-1960s was influenced by two major developments. First was the suggestion of Harry Kalven, Jr. and Maurice Rosenfeld in 1941 that class action litigation by individual shareholders on behalf of all shareholders of a company could effectively supplement direct government regulation of securities markets and other similar markets. The second development was the rise of the African-American civil rights movement, environmentalism, and consumerism. As expected, the groups behind these movements, as well as many others in the 1960s, 1970s, and 1980s, all turned to class actions as a means for achieving their goals. For example, a 1978 environmental law treatise reprinted the "entire" text of Rule 23 and mentioned "class actions" 14 times in its index.
Of course, businesses targeted by class actions for inflicting massive aggregate harm have sought ways to avoid class actions altogether. In the 1990s, the U.S. Supreme Court issued a number of decisions which strengthened the "federal policy favoring arbitration". In response, lawyers have added provisions to consumer contracts of adhesion called "collective action waivers". In 1999 the National Arbitration Forum began advocating that such contracts should be drafted so as to force consumers to waive the right to a class action completely, and such provisions have become very popular among businesses. As of November 2007, the legal validity of contracts of adhesion with class action waivers is unclear, and courts have rendered mixed and sometimes contradictory opinions.
In the 2011 court case "AT&T Mobility v. Concepcion", the U.S. Supreme Court ruled in a 5–4 decision that the Federal Arbitration Act of 1925 preempts state laws that prohibit contracts from disallowing class action lawsuits, which will make it more difficult for consumers to file class action lawsuits. The dissent pointed to a saving clause in the federal act which allowed states to determine how a contract or its clauses may be revoked.
In two major cases in the 21st century, the United States Supreme Court ruled 5-4 against certification of class actions due to differences in each individual members' circumstances; first in the 2011 case Wal-Mart v. Dukes and later in the 2013 case Comcast Corp. v. Behrend.
Advantages.
Class actions may offer a number of advantages because they aggregate a large number of individualized claims into one representational lawsuit.
First, aggregation can increase the efficiency of the legal process, and lower the costs of litigation. In cases with common questions of law and fact, aggregation of claims into a class action may avoid the necessity of repeating "days of the same witnesses, exhibits and issues from trial to trial." "Jenkins v. Raymark Indus. Inc.", 782 F.2d 468, 473 (5th Cir. 1986) (granting certification of a class action involving asbestos).
Second, a class action may overcome "the problem that small recoveries do not provide the incentive for any individual to bring a solo action prosecuting his or her rights." "Amchem Prods., Inc. v. Windsor", 521 U.S. 591, 617 (1997) (quoting "Mace v. Van Ru Credit Corp.", 109 F.3d 388, 344 (7th Cir. 1997)). "A class action solves this problem by aggregating the relatively paltry potential recoveries into something worth someone’s (usually an attorney’s) labor." "Amchem Prods., Inc.", 521 U.S. at 617 (quoting "Mace", 109 F.3d at 344). In other words, a class action ensures that a defendant who engages in widespread harmbut does so minimally against each individual plaintiffmust compensate those individuals for their injuries. For example, thousands of shareholders of a public company may have losses too small to justify separate lawsuits, but a class action can be brought efficiently on behalf of all shareholders. Perhaps even more important than compensation is that class treatment of claims may be the only way to impose the costs of wrongdoing on the wrongdoer, thus deterring future wrongdoing.
Third, class action cases may be brought to purposely change behavior of a class of which the defendant is a member. Landeros v. Flood was a landmark case used to purposefully change the behavior of doctors, and encourage them to report suspected child abuse. Otherwise, they would face the threat of civil action for damages in tort proximately flowing from the failure to report the suspected injuries. Previously, many physicians had remained reluctant to report cases of apparent child abuse, despite existing law that required it.
Fourth, in "limited fund" cases, a class action ensures that all plaintiffs receive relief and that early-filing plaintiffs do not raid the fund ("i.e.", the defendant) of all its assets before other plaintiffs may be compensated. "See" "Ortiz v. Fibreboard Corp.", 527 U.S. 815 (1999). A class action in such a situation centralizes all claims into one venue where a court can equitably divide the assets amongst all the plaintiffs if they win the case.
Finally, a class action avoids the situation where different court rulings could create "incompatible standards" of conduct for the defendant to follow. See Fed. R. Civ. P. 23(b)(1)(A). For example, a court might certify a case for class treatment where a number of individual bond-holders sue to determine whether they may convert their bonds to common stock. Refusing to litigate the case in one trial could result in different outcomes and inconsistent standards of conduct for the defendant corporation. Thus, courts will generally allow a class action in such a situation. "See, e.g., Van Gemert v. Boeing Co.", 259 F. Supp. 125 (S.D.N.Y. 1966).
Whether a class action is superior to individual litigation depends on the case and is determined by the judge's ruling on a motion for class certification. The Advisory Committee Note to Rule 23, for example, states that mass torts are ordinarily "not appropriate" for class treatment. Class treatment may not improve the efficiency of a mass tort because the claims frequently involve individualized issues of law and fact that will have to be re-tried on an individual basis. "See Castano v. Am. Tobacco Co.", 84 F.3d 734 (5th Cir. 1996) (rejecting nationwide class action against tobacco companies). Mass torts also involve high individual damage awards; thus, the absence of class treatment will not impede the ability of individual claimants to seek justice. "See id." Other cases, however, may be more conducive to class treatment.
The preamble to the Class Action Fairness Act of 2005, passed by the United States Congress, found:
Class-action lawsuits are an important and valuable part of the legal system when they permit the fair and efficient resolution of legitimate claims of numerous parties by allowing the claims to be aggregated into a single action against a defendant that has allegedly caused harm. 
Criticisms.
There are several criticisms of class actions. The preamble to the Class Action Fairness Act stated that some abusive class actions harmed class members with legitimate claims and defendants that have acted responsibly, adversely affected interstate commerce, and undermined public respect for the country's judicial system.
Class members often receive little or no benefit from class actions. Examples cited for this include large fees for the attorneys, while leaving class members with coupons or other awards of little or no value; unjustified awards are made to certain plaintiffs at the expense of other class members; and confusing notices are published that prevent class members from being able to fully understand and effectively exercise their rights.
For example, in the United States, class lawsuits sometimes bind all class members with a low settlement. These "coupon settlements" (which usually allow the plaintiffs to receive a small benefit such as a small check or a coupon for future services or products with the defendant company) are a way for a defendant to forestall major liability by precluding a large number of people from litigating their claims separately, to recover reasonable compensation for the damages. However, existing law requires judicial approval of all class action settlements, and in most cases class members are given a chance to opt out of class settlement, though class members, despite opt-out notices, may be unaware of their right to opt out because they did not receive the notice, did not read it, or did not understand it.
The Class Action Fairness Act of 2005 addresses these concerns. Coupon settlements may be scrutinized by an independent expert before judicial approval in order to ensure that the settlement will be of value to the class members (28 U.S.C.A. 1712(d)). Further, if the action provides for settlement in coupons, "the portion of any attorney’s fee award to class counsel that is attributable to the award of the coupons shall be based on the value to class members of the coupons that are redeemed." 28 U.S.C.A. 1712(a).
Ethics.
Class action cases present significant ethical challenges. Defendants can hold reverse auctions and any of several parties can engage in collusive settlement discussions. Subclasses may have interests that diverge greatly from the class, but may be treated the same. Proposed settlements could offer some groups (such as former customers) much greater benefits than others. In one paper presented at an ABA conference on class actions in 2007, authors commented that "competing cases can also provide opportunities for collusive settlement discussions and reverse auctions by defendants anxious to resolve their new exposure at the most economic cost." 
Defendant class action.
Although normally plaintiffs are the class, defendant class actions are also possible. For example, in 2005, the Roman Catholic Archdiocese of Portland in Oregon was sued as part of the Catholic priest sex-abuse scandal. All parishioners of the Archdiocese's churches were cited as a defendant class. This was done to include their assets (local churches) in any settlement. Where both the plaintiffs and the defendants have been organized into court-approved classes, the action is called a bilateral class action.
Mass actions.
In a class action, the plaintiff seeks court approval to litigate on behalf of a group of similarly-situated persons. Not every plaintiff looks for, or could obtain, such approval. As a procedural alternative, plaintiff's counsel may attempt to sign up every similarly-situated person that counsel can find as a client. Plaintiff's counsel can then join the claims of all of these persons in one complaint, a so-called "mass action," hoping to have the same efficiencies and economic leverage as if a class had been certified.
Because mass actions operate outside the detailed procedures laid out for class actions, they can pose special difficulties for both plaintiffs, defendants, and the court. For example, settlement of class actions follows a predictable path of negotiation with class counsel and representatives, court scrutiny, and notice. There may not be a way to uniformly settle all of the many claims brought via a mass action. Some states permit plaintiff's counsel to settle for all the mass action plaintiffs according to a majority vote, for example. Other states, such as New Jersey, require each plaintiff to approve the settlement of that plaintiff's own individual claims.
Austria.
The Austrian Code of Civil Procedure (Zivilprozessordnung – ZPO) does not provide for a special proceeding for complex class action litigation. However, Austrian consumer organizations (Verein für Konsumenteninformation/VKI and the Federal Chamber of Labour/Bundesarbeitskammer) have, in recent years, brought claims on behalf of hundreds or even thousands of consumers. In these cases the individual consumers assigned their claims to one entity, who has then brought an ordinary (two party) lawsuit over the assigned claims. The monetary benefits were redistributed among the class. This technique, soon labelled as “class action Austrian style”, allows for a significant reduction of overall costs. The Austrian Supreme Court, in a recent judgment, has confirmed the legal admissibility of these lawsuits under the condition that all claims are essentially based on the same grounds.
The Austrian Parliament has unanimously requested the Austrian Federal Minister for Justice to examine the possibility of new legislation providing for a cost-effective and appropriate way to deal with mass claims. Together with the Austrian Ministry for Social Security, Generations and Consumer Protection, the Justice Ministry opened the discussion with a conference held in Vienna in June, 2005. With the aid of a group of experts from many fields, the Justice Ministry began drafting the new law in September, 2005. With the individual positions varying greatly, a political consensus could not be reached.
Canada.
Provincial laws in Canada allow class actions. All provinces permit plaintiff classes and some permit defendant classes. Quebec was the first province to enact class proceedings legislation in 1978. Ontario was next with the Class Proceedings Act, 1992. As of 2008, 9 of 10 provinces have enacted comprehensive class actions legislation. In Prince Edward Island, where no comprehensive legislation exists, following the decision of the Supreme Court of Canada in "Western Canadian Shopping Centres Inc. v. Dutton", [2001] 2 S.C.R. 534, class actions may be advanced under a local rule of court. The Federal Court of Canada permits class actions under Part V.1. of the Federal Courts Rules.
Legislation in Saskatchewan, Manitoba, Ontario, and Nova Scotia expressly or by judicial opinion have been read to allow for what are informally known as national "opt-out" class actions, whereby residents of other provinces may be included in the class definition and potentially be bound by the court's judgment on common issues unless they opt out in a prescribed manner and time. Court rulings have determined that this permits a court in one province to include residents of other provinces in the class action on an "opt-out" basis.
Recent judicial opinions have indicated that provincial legislative national opt-out powers should not be exercised to interfere with the ability of another province to certify a parallel class action for residents of other provinces. The first court to certify will generally exclude residents of provinces whose courts have certified a parallel class action. However, in the Vioxx litigation, two provincial courts recently certified overlapping class actions whereby Canadian residents are class members in two class actions in two provinces. Both decisions are under appeal.
The largest class action suit to date in Canada was settled in 2005 after Nora Bernard initiated efforts that led to an estimated 79,000 survivors of Canada's residential school system suing the Canadian government. The settlement amounted to upwards of $5 billion.
France.
Under French law, an association can represent the collective interests of consumers; however, each claimant must be individually named in the lawsuit. On January 4, 2005, President Chirac urged changes that would provide greater consumer protection. A draft bill was proposed in April 2006. Under the proposals the court will be able to decide whether to allow an action brought by an association on behalf of consumers (which must comprise at least two individuals) for goods purchased under a standard contract. After such an action is brought, the association would be entitled to identify additional consumers for a one-month period. The court would determine the damages that must be awarded to the consumers who have opted in to the proceedings, with damages limited to 2,000 Euros; contingent fees for attorneys would be barred. The president of the French Supreme Court recently declared that "class actions are inescapable." Nevertheless, the bill was withdrawn in January 2007 at the request of Minister of Health Xavier Bertrand.
Following the change of majority in France in 2012, the new government is once more talking of introducing class actions into French law. The project of "loi Hamon" of May 2013 aims to limit the class action to consumer and competition disputes.
Germany.
On November 1, 2005, Germany enacted the “Act on Model Case Proceedings in Disputes under Capital Markets Law (Capital Markets Model Case Act)” allowing sample proceedings to be brought before the courts in litigation arising from mass capital markets transactions. It does not apply to any other civil law proceeding. It is not like class actions in the United Statesit only applies to parties who have already filed suit and does not allow a claim to be brought in the name of an unknown group of claimants. The effects of the new law will be monitored over the next five years. It contains a ‘sunset clause’, and it will automatically cease to have effect on November 1, 2010, unless the legislature decides to prolong the law, or extend it to other mass civil case proceedings.
Italy.
Italy has class action legislation. Consumer associations can file claims on behalf of groups of consumers to obtain judicial orders against corporations that cause injury or damage to consumers. These types of claims are increasing and Italian courts have recently allowed them against banks that continue to apply compound interest on retail clients’ current account overdrafts. The introduction of class actions is on the new government’s agenda. On November 19, 2007, the Senato della Repubblica passed a class action law in Finanziara 2008, a financial document for the economy management of the government. Now (from 10 December 2007), in order of Italian legislation system, the law is before the House and has to be passed also by the Camera dei Deputati, the second house of Italian Parliament, to become an effective law. In 2004, the Italian parliament considered the introduction of a type of class action, specifically in the area of consumers’ law. To date, no such law has been enacted, but scholars demonstrated that class actions ("azioni rappresentative") do not contrast with Italian principles of civil procedure. Class action is regulated by art. 140 bis of the Italian consumers' code and will be in force from 1 July 2009.
India.
Decisions of the Indian Supreme Court in the 1980s loosened strict "locus standi" requirements to permit the filing of suits on behalf of rights deprived sections of society by public minded individuals or bodies. Although not strictly "class action litigation" as it is understood in American law, Public Interest Litigation arose out of the wide powers of judicial review granted to the Supreme Court of India and the various High Courts under and Article 226 of the Constitution of India, respectively. The sort of remedies sought from courts in Public Interest Litigation go beyond mere award of damages to all affected groups and have sometimes (controversially) gone on to include Court monitoring of the implementation of legislation and even the framing of guidelines in the absence of Parliamentary legislation.
However, this innovative jurisprudence did not help the victims of the Bhopal Gas Tragedy, who were unable to fully prosecute a class action litigation (as understood in the American sense) against Union Carbide due to procedural rules that would make such litigation impossible to conclude and unwieldy to carry out. Instead, the Government of India exercised its right of parens patriae to appropriate all the claims of the victims and proceeded to litigate on their behalf, first in the New York courts and later, in the Indian courts. Ultimately, the matter was settled between the Union of India and Union Carbide (in a settlement overseen by the Supreme Court of India) for a sum of as a complete settlement of all claims of all victims for all time to come.
Public Interest Litigation has now broadened in scope to cover larger and larger groups of citizens who may be affected by Government inaction. Recent examples of this trend include the conversion of all public transport in the city of Delhi from Diesel engines to CNG engines on the basis of the orders of the Delhi High Court; the monitoring of forest use by the High Courts and the Supreme Court to ensure that there is no unjustified loss of forest cover; and the directions mandating the disclosure of assets of electoral candidates for the Houses of Parliament and State Assembly.
Of late, the Supreme Court has observed that the PIL has tended to become a means to gain publicity or obtain relief contrary to constitutionally valid legislation and policy. Observers point out that many High Courts and certain Supreme Court judges are reluctant to entertain PILs, even those filed by well-known Non-governmental organizations (NGO) and activists, citing concerns of Separation of powers and the importance of democratic law making.
Netherlands.
Dutch law allows associations ("verenigingen") and foundations ("stichtingen") to bring a so-called collective action on behalf of other persons, provided they can represent the interests of such persons according to their by-laws ("statuten") (section 3:305a Dutch Civil Code). All types of actions are permitted, excluding a claim for monetary damages. Most class actions over the past decade have been in the field of securities fraud and financial services. The acting association or foundation may come to a collective settlement with the defendant. The settlement may also include – and usually primarily consists of – monetary compensation of damages. Such settlement can be declared binding for all injured parties by the Amsterdam Court of Appeal (section 7:907 Dutch Civil Code). The injured parties have an opt-out right during the opt-out period set by the Court, usually 3 to 6 months. Interestingly, settlements involving injured parties from outside The Netherlands can also be declared binding by the Court. Notably since US courts are reluctant to take up class actions brought on behalf of injured parties not residing in the US who have suffered damages due to acts or omissions committed outside the US, it may be interesting to combine a US class action and a Dutch collective action to be able to come to a settlement that covers plaintiffs worldwide. An example of this is the Royal Dutch Shell Oil Reserves Settlement that was declared binding upon both US and non-US plaintiffs.
Spain.
Spanish law allows nominated consumer associations to take action to protect the interests of consumers. A number of groups already have the power to bring collective or class actions: certain consumer associations, bodies legally constituted to defend the ‘collective interest’ and groups of injured parties.
Recent changes to Spanish civil procedure rules include the introduction of a quasi-class action right for certain consumer associations to claim damages on behalf of unidentified classes of consumers. The rules require consumer associations to represent an adequate number of affected parties who have suffered the same harm. Also any judgment made by the Spanish court will list the individual beneficiaries or, if that is not possible, conditions that need to be fulfilled for a party to benefit from a judgment.
Switzerland.
Swiss law does not allow for any form of class action. When the government proposed a new federal code of civil procedure in 2006, replacing the cantonal codes of civil procedure, it rejected the introduction of class actions, arguing that
United Kingdom.
The Civil Procedure Rules of the courts of England and Wales came into force in 1999 and have provided for representative actions in limited circumstances (under Part 19.6 ). These have not been much used, with only two reported cases at the court of first instance in the first ten years after the Civil Procedure Rules took effect

</doc>
<doc id="7201" url="http://en.wikipedia.org/wiki?curid=7201" title="Contempt of court">
Contempt of court

Contempt of court, often referred to simply as "contempt", is the offense of being disobedient to or disrespectful towards a court of law and its officers in the form of behavior that opposes or defies authority, justice, and dignity of the court. It manifests itself in willful disregard of or disrespect for the authority of a court of law, which is often behavior that is illegal because it does not obey or respect the rules of a law court. 
As explained in the People's Law Dictionary by Gerald and Kathleen Hill, "there are essentially two types of contempt: (1) being rude, disrespectful to the judge or other attorneys or causing a disturbance in the courtroom, particularly after being warned by the judge; (2) willful failure to obey an order of the court." Contempt proceedings are especially used to enforce equitable remedies, such as injunctions.
When a court decides that an action constitutes contempt of court, it can issue a court order that in the context of a court trial or hearing declares a person or organization to have disobeyed or been disrespectful of the court's authority, called "found" or "held in contempt"; this is the judge's strongest power to impose sanctions for acts that disrupt the court's normal process.
A finding of being in contempt of court may result from a failure to obey a lawful order of a court, showing disrespect for the judge, disruption of the proceedings through poor behaviour, or publication of material deemed likely to jeopardize a fair trial. A judge may impose sanctions such as a fine or jail for someone found guilty of contempt of court. Judges in common law systems usually have more extensive power to declare someone in contempt than judges in civil law systems. The client or person must be proven to be guilty before he/she will be punished.
In use today.
Contempt of court is essentially seen as a form of disturbance that may impede the functionality of the court. The judge may impose fines and/or jail time upon any person committing contempt of court. The person is usually let out upon his agreement to fulfill the wishes of the court. Civil contempt can involve acts of omission. The judge will make use of warnings in most situations that may lead to a person being charged with contempt. It is relatively rare that a person is charged for contempt without first receiving at least one warning from the judge. Constructive contempt, also called consequential contempt is when a person fails to fulfill the will of the court as it applies to outside obligations of the person. In most cases, constructive contempt is considered to be in the realm of civil contempt because of its passive nature.
Indirect contempt is something that is associated with civil and constructive contempt and involves a failure to follow court orders. Criminal contempt includes anything that could be called a disturbance such as repeatedly talking out of turn, bringing forth previously banned evidence, or harassment of any other party in the courtroom. Direct contempt is an unacceptable act in the presence of the judge (in facie curiae), and generally begins with a warning, and may be accompanied by an immediate imposition of punishment. Yawning in some cases can be considered contempt of court.
Contempt of court has a significant impact on journalism in the form of restrictions on court reporting which are set out in statute in the UK.
Australia.
In Australia a judge may impose a fine or jail.
Canada.
Criminal offences are found within the Criminal Code of Canada or other federal/provincial laws, with the exception that contempt of court is the only remaining common law offence in Canada.
Contempt of Court includes the following behaviours:
Canadian Federal courts.
"This section applies only to Federal Court of Appeal and Federal Court."
Under Federal Court Rules, Rules 466, and Rule 467 a person who is accused of Contempt needs to be first served with a contempt order and then appear in court to answer the charges. Convictions can only be made when proof beyond a reasonable doubt is achieved.
If it is a matter of urgency or the contempt was done in front of a judge, that person can be punished immediately. Punishment can range from the person being imprisoned for a period of less than five years or until the person complies with the order or fine.
Tax Court of Canada.
Under Tax Court of Canada Rules of "Tax Court of Canada Act", a person who is found to be in contempt may be imprisoned for a period of less than two years or fined. Similar procedures for serving an order first is also used at the Tax Court.
Provincial courts.
Different procedures exist for different provincial courts. For example, in British Columbia, Justice of Peace can only issue summon to the offender for Contempt, for which will be dealt with by a judge, even if the offence was done at the face of the Justice.
Hong Kong.
Judges from the Court of Final Appeal, High Court, District Courts along with members from the various tribunals and Coroner's Court all have the power to impose immediate punishments for contempt in the face of the court, derived from legislation or through common law:
The use of insulting or threatening language in the magistrates' courts or against a magistrate is in breach of section 99 of the Magistrates Ordinance (Cap 227) which states the magistrate can 'summarily sentence the offender to a fine at level 3 and to imprisonment for 6 months.'
In addition, certain appeal boards are given the statutory authority for contempt by them (i.e., Residential Care Home, Hotel and Guesthouse Accommodation, Air Pollution Control, etc.). For contempt in front of these boards, the chairperson will certify the act of contempt to the Court of First Instance who will then proceed with a hearing and determine the punishment.
India.
In India contempt of court is of two types:
1. Civil Contempt
Under Section 2(b) of the Contempt of Courts Act of 1971, civil contempt has been defined as wilful disobedience to any judgment, decree, direction, order, writ or other process of a court or wilful breach of an undertaking given to a court.
2. Criminal Contempt
Under Section 2(c) of the Contempt of Courts Act of 1971, criminal contempt has been defined as the publication (whether by words, spoken or written, or by signs, or by visible representation, or otherwise) of any matter or the doing of any other act whatsoever which:
(i) Scandalises or tends to scandalise, or lowers or tends to lower the authority of, any court, or
(ii) Prejudices, or interferes or tends to interfere with the due course of any judicial proceeding, or
(iii) Interferes or tends to interfere with, or obstructs or tends to obstruct, the administration of justice in any other manner. 
(a) 'High Court' means the high court for a state or a union territory and includes the court of the judicial commissioner in any union territory.
England.
In English law (a common law jurisdiction) the law on contempt is partly set out in case law, and partly specified in the Contempt of Court Act 1981. Contempt may be a criminal or civil offence. The maximum sentence for criminal contempt is two years.
Disorderly, contemptuous, or insolent behaviour toward the judge or magistrates while holding the court, tending to interrupt the due course of a trial or other judicial proceeding, may be prosecuted as "direct" contempt. The term "direct" means that the court itself cites the person in contempt by describing the behaviour observed on the record. Direct contempt is distinctly different from indirect contempt, wherein another individual may file papers alleging contempt against a person who has willfully violated a lawful court order.
Criminal contempt of court.
The Crown Court is a superior court of record under the Senior Courts Act 1981 and accordingly has power to punish for contempt of its own motion. The Divisional Court has stated that this power applies in three circumstances:
Where it is necessary to act quickly the judge (even the trial judge) may act to sentence for contempt.
Where it is not necessary to be so urgent, or where indirect contempt has taken place the Attorney General can intervene and the Crown Prosecution Service will institute criminal proceedings on his behalf before a Divisional Court of the Queen's Bench Division of the High Court of Justice of England and Wales.
Magistrates' courts are not superior courts of record, but nonetheless have powers granted under the Contempt of Court Act 1981. They may detain any person who insults the court or otherwise disrupts its proceedings until the end of the sitting. Upon the contempt being either admitted or proved the judge or JP may imprison the offender for a maximum of one month, fine them up to GBP £2,500, or do both.
It is contempt of court to bring an audio recording device or picture-taking device of any sort into an English court without the consent of the court.
It is not contempt of court (under section 10 of the Act) for a journalist to refuse to disclose his sources, unless the court has considered the evidence available and determined that the information is "necessary in the interests of justice or national security or for the prevention of disorder or crime".
Strict liability contempt.
Under the Contempt of Court Act 1981 it is criminal contempt of court to publish anything which creates a real risk that the course of justice in proceedings may be seriously impaired. It only applies where proceedings are active, and the Attorney General has issued guidance as to when he believes this to be the case, and there is also statutory guidance. The clause prevents the newspapers and media from publishing material that is too extreme or sensationalist about a criminal case until the trial is over and the jury has given its verdict.
Section 2 of the Act limits the common law presumption that conduct may be treated as contempt regardless of intention: now only cases where there is a substantial risk of serious prejudice to a trial are affected.
Civil contempt.
In civil proceedings there are two main ways in which contempt is committed:
United States.
Under the United States jurisprudence, acts of contempt are divided into direct or indirect and civil or criminal. Direct contempt occurs in the presence of a judge; civil contempt is "coercive and remedial" as opposed to punitive. In the United States, relevant statutes include and Federal Rule of Criminal Procedure 42.
Contempt of court in a civil suit is generally not considered to be a criminal offense, with the party benefiting from the order also holding responsibility for the enforcement of the order. However, some cases of civil contempt have been perceived as intending to harm the reputation of the plaintiff, or to a lesser degree, the judge or the court.
Sanctions for contempt may be criminal or civil. If a person is to be punished criminally, then the contempt must be proven beyond a reasonable doubt, but once the charge is proven, then punishment (such as a fine or, in more serious cases, imprisonment) is imposed unconditionally. The civil sanction for contempt (which is typically incarceration in the custody of the sheriff or similar court officer) is limited in its imposition for so long as the disobedience to the court's order continues: once the party complies with the court's order, the sanction is lifted. The imposed party is said to "hold the keys" to his or her own cell, thus conventional due process is not required. The burden of proof for civil contempt, however, is a preponderance of the evidence, and theoretically punitive sanctions (punishment) can only be imposed after due process but the due process is unpublished. 
In civil contempt cases there is no principle of proportionality. In "Chadwick v. Janecka" (3d Cir. 2002), a U.S. court of appeals held that H. Beatty Chadwick could be held indefinitely under federal law, for his failure to produce US$ 2.5 million as state court ordered in a civil trial. Chadwick had been imprisoned for nine years at that time and continued to be held in prison until 2009, when a state court set him free after 14 years, making his imprisonment the longest on a contempt charge to date.
Civil contempt is only appropriate when the imposed party has the power to comply with the underlying order. Controversial contempt rulings have periodically arisen from cases involving asset protection trusts, where the court has ordered a settlor of an asset protection trust to repatriate assets so that the assets may be made available to a creditor. A court cannot maintain an order of contempt where the imposed party does not have the ability to comply with the underlying order. This claim when made by the imposed party is known as the "impossibility defense".
Contempt of court is considered a prerogative of the court, and "the requirement of a jury does not apply to 'contempts committed in disobedience of any lawful writ, process, order, rule, decree, or command entered in any suit or action brought or prosecuted in the name of, or on behalf of, the United States'".
The United States Marshals Service is the agency component that first holds all federal prisoners. It uses the Prisoner Population Management System /Prisoner Tracking System. The only types of records that are disclosed as being in the system are those of "federal prisoners who are in custody pending criminal proceedings." The records of "alleged civil contempors" are not listed in the Federal Register as being in the system leading to a potential claim for damages under The Privacy Act, .
News media in the United States.
In the United States, because of the broad protections granted by the First Amendment, with extremely limited exceptions, unless the media outlet is a party to the case, a media outlet cannot be found in contempt of court for reporting about a case because a court cannot order the media in general not to report on a case or forbid it from reporting facts discovered publicly. Newspapers cannot be closed because of their content.
Criticism.
There have been criticisms over the practice of trying contempt from the bench. In particular, Supreme Court Justice Hugo Black wrote in a dissent, "It is high time, in my judgment, to wipe out root and branch the judge-invented and judge-maintained notion that judges can try criminal contempt cases without a jury."

</doc>
<doc id="7202" url="http://en.wikipedia.org/wiki?curid=7202" title="Corroborating evidence">
Corroborating evidence

Corroborating evidence (in "corroboration") is evidence that tends to support a proposition that is already supported by some initial evidence, therefore confirming the proposition. For example, W, a witness, testifies that she saw X drive his automobile into a green car. Meanwhile Y, another witness, testifies that when he examined X's car, later that day, he noticed green paint on its fender. Or there can be corroborating evidence related to a certain source, such as what makes an author think a certain way due to the evidence that was supplied by witnesses or objects.
For more information on this type of reasoning, see: Casuistry.
Another type of corroborating evidence comes from using the Baconian method, i.e. the method of agreement, method of difference, and method of concomitant variations.
These methods are followed in experimental design. They were codified by Francis Bacon, and developed further by John Stuart Mill and consist of controlling several variables, in turn, to establish which variables are causally connected. These principles are widely used intuitively in various kinds of proofs, demonstrations and investigations, in addition to being fundamental to experimental design.
In law, corroboration refers to the requirement in some jurisdictions, such as in Scotland, that any evidence adduced be backed up by at least one other source (see Corroboration in Scots law).
Corroboration is not needed in certain instances. For example, there are certain statutory exceptions. In the Education (Scotland) Act, it is only necessary to produce a register as proof of lack of attendance. No further evidence is needed.
England and Wales.
Perjury
See section 13 of the Perjury Act 1911.
Speeding offences
See section 89(2) of the Road Traffic Regulation Act 1984.
Sexual offences
See section 32 of the Criminal Justice and Public Order Act 1994.
Confessions by mentally handicapped persons
See section 77 of the Police and Criminal Evidence Act 1984.
Evidence of children
See section 34 of the Criminal Justice Act 1988.
Evidence of accomplices
See section 32 of the Criminal Justice and Public Order Act 1994.
References.
Plutchik, Robert (1983) "Foundations of Experimental Research" Harper's Experimental Psychology Series.

</doc>
<doc id="7203" url="http://en.wikipedia.org/wiki?curid=7203" title="Cross-examination">
Cross-examination

In law, cross-examination is the interrogation of a witness called by one's opponent. It is preceded by direct examination (in the United Kingdom, Australia, Canada, South Africa, India and Pakistan known as examination-in-chief) and may be followed by a redirect (re-examination in England, Scotland, Australia, Canada, South Africa, India, Hong Kong, and Pakistan).
Variations by jurisdiction.
In the United States federal courts, a cross-examining attorney is typically not permitted to ask questions that do not pertain to the testimony offered during direct examination, but most state courts do permit a lawyer to cross-examine a witness on matters not raised during direct examination. Similarly, courts in England, Australia, and Canada allow a cross-examiner to exceed the scope of direct examination.
Since a witness called by the opposing party is presumed to be hostile, cross-examination does permit leading questions. A witness called by the direct examiner, on the other hand, may only be treated as hostile by that examiner after being permitted to do so by the judge, at the request of that examiner and as a result of the witness being openly antagonistic and/or prejudiced against the opposing party.
The main purposes of cross-examination are to elicit favorable facts from the witness, or to impeach the credibility of the testifying witness to lessen the weight of unfavorable testimony. Cross-examination frequently produces critical evidence in trials, especially if a witness contradicts previous testimony. The advocate Edward Marshall-Hall built his career on cross-examination that often involved histrionic outbursts designed to sway jurors. Most experienced and skilled cross-examiners however, refrain from caustic or abrasive cross-examination so as to avoid alienating jurors. John Mortimer, Queen's Counsel, observed that "cross-examination" was not the art of examining crossly. Indeed the good cross-examiner gets a witness to assert to a series of linked propositions culminating in one that undermines that witnesses' evidence rather than pursuing an antagonistic approach.
The art of cross-examination.
Cross-examination is considered an essential component of a jury trial because of the impact it has on the opinions of the judge and jury. Few lawyers practice trial law or complex litigation and typically refer such cases to those who have the time, resources and experience to handle a complex trial and the commitment involved to complete a trial successfully. Few attorneys get the practice necessary to develop the techniques needed to do an effective job cross-examining a witness. 
It is sometimes referred to as an art form, because of the need for an attorney to know precisely how to elicit the testimony from the opposing witness that will help, not hinder, their client's case. Typically a cross-examiner must not only be effective at getting the witness to reveal the truth, but in most cases to reveal confusion as to the facts such as time, dates, people, places, wording etc. More often than not a cross-examiner will also attempt to undermine the credibility of a witness if he or she will not be perceived to be a bully (such as discrediting a very elderly person or young child). 
The cross-examiner often needs to discredit a potentially biased or damaging witness in the eyes of the jury without appearing to be doing so in an unfair way. Typically the cross-examiner must appear friendly, talk softly and sincerely to relax the guarded witness. Or on other occasions they may start by being more confrontational, unsettling an already disturbed witness. They typically begin repeating similar basic questions in a variety of different ways to get different responses, which will then be used against the witness as misstatements of fact later when the attorney wants to make their point. If it is too obvious the questions are too clearly repetitive and making the witness nervous, the other attorney may accuse the cross examiner of badgering the witness. There is a fine line between badgering and getting the witness to restate facts differently that is typically pursued. 
The less the witness says, and the slower the witness speaks, the more control they can maintain under the pressure of a crafty opponent. The key for a witness is to understand the facts that they believe to be the case and not add additional thoughts to those facts, lest they be used to undermine the testimony. Sticking to the brief known facts is key for the witness, making it difficult for the cross-examiner to make the witness appear confused, biased or deceitful. The cross examiner will assume the witness has been told that and begin asking supporting questions about where the witness was, what time it was, what the witness saw, what they said, and sooner or later upon asking again the witness may use a different word that will give the cross-examiner a chance to ask the question again doubtfully and pointedly implying contradiction. The witness will try typically to explain and clarify, which sometimes reveals weakness in the witness's statements of fact. Other times the witness is just being truthful but undermined for the purpose of casting doubt to the jury and or judge.
There is a measure of drama that cross-examination adds to any trial because of the challenging of the statements made by a witness. In the 1903 book titled "The Art of Cross-Examination" by Francis L. Wellmann much effort is devoted to highlighting components of cross-examination and the impact on trials of the past century. An example of an inflammatory way a question will be asked by a cross-examiner to a witness he was trying to undermine would be "What is your recollection toDAY?" implying it was stated differently yesterday. Simply the accent of syllables can leave a bewildered jury believing they must put their guard up with a witness–or in some cases the cross-examiner if they are not careful. The book freely uses accenting in its dialogue to give the reader such insight as to how cross-examiners rattle witnesses to obtain their desired effect for the jury.
In most common-law countries, cross-examiners are expected to follow the well-established rule in "Browne v. Dunn".
Affecting the outcome of jury trials.
Cross-examination is a key component in a trial and the topic is given substantial attention during courses on Trial Advocacy. The opinions by a jury or judge are often changed during cross examination if doubt is cast on the witness. In other times a credible witness affirms the belief in their original statements or in some cases enhances the judge's or jury's belief. Though the closing argument is often considered the deciding moment of a trial, effective cross-examination wins trials. 
Attorneys anticipate hostile witness' responses during pretrial planning, and often attempt to shape the witnesses' perception of the questions to draw out information helpful to the attorney's case. Typically during an attorney's closing argument he will repeat any admissions made by witnesses that favor their case. Indeed, in the United States, cross-examination is seen as a core part of the entire adversarial system of justice, in that it "is the principal means by which the believability of a witness and the truth of his testimony are tested." Another key component affecting a trial outcome is the jury selection, in which attorneys will attempt to include jurors from whom they feel they can get a favorable response or at the least unbiased fair decision. So while there are many factors affecting the outcome of a trial, the cross-examination of a witness will often have an impact on an open minded unbiased jury searching for the certainty of facts upon which to base their decision.

</doc>
<doc id="7206" url="http://en.wikipedia.org/wiki?curid=7206" title="Christiania">
Christiania

Christiania may refer to:

</doc>
<doc id="7207" url="http://en.wikipedia.org/wiki?curid=7207" title="Charles d'Abancour">
Charles d'Abancour

Charles Xavier Joseph de Franque Ville d'Abancour (4 July 1758 – 9 September 1792) was a French statesman, minister to Louis XVI.
Biography.
D'Abancourt was born in Douai, and was the nephew of Charles Alexandre de Calonne. He was Louis XVI's last minister of war (July 1792), and organised the defence of the Tuileries Palace during the 10th of August attack. Ordered by the Legislative Assembly to send away the Swiss Guards, he refused, and was arrested for treason to the nation and sent to Orléans to be tried.
At the end of August the Assembly ordered Abancourt and the other prisoners at Orléans to be transferred to Paris with an escort commanded by Claude Fournier "l'Americain". At Versailles, they learned of the September Massacres in Paris. Abancourt and his fellow-prisoners were murdered in cold blood in massacres on 9 September 1792 at Versailles, and Fournier was unjustly charged with complicity in the crime.
References.
 

</doc>
<doc id="7211" url="http://en.wikipedia.org/wiki?curid=7211" title="Curtiss P-40 Warhawk">
Curtiss P-40 Warhawk

The Curtiss P-40 Warhawk was an American single-engined, single-seat, all-metal fighter and ground-attack aircraft that first flew in 1938. The P-40 design was a modification of the previous Curtiss P-36 Hawk which reduced development time and enabled a rapid entry into production and operational service. The Warhawk was used by most Allied powers during World War II, and remained in frontline service until the end of the war. It was the third most-produced American fighter, after the P-51 and P-47; by November 1944, when production of the P-40 ceased, 13,738 had been built, all at Curtiss-Wright Corporation's main production facilities at Buffalo, New York.
P-40 Warhawk was the name the United States Army Air Corps adopted for all models, making it the official name in the United States for all P-40s. The British Commonwealth and Soviet air forces used the name Tomahawk for models equivalent to the P-40B and P-40C, and the name Kittyhawk for models equivalent to the P-40D and all later variants.
P-40s first saw combat with the British Commonwealth squadrons of the Desert Air Force in the Middle East and North African campaigns, during June 1941. No. 112 Squadron Royal Air Force, was among the first to operate Tomahawks in North Africa and the unit was the first Allied military aviation unit to feature the "shark mouth" logo, copying similar markings on some Luftwaffe Messerschmitt Bf 110 twin-engine fighters. 
The P-40's lack of a two-stage supercharger made it inferior to Luftwaffe fighters such as the Messerschmitt Bf 109 or the Focke-Wulf Fw 190 in high-altitude combat and it was rarely used in operations in Northwest Europe. Between 1941 and 1944, the P-40 played a critical role with Allied air forces in three major theaters: North Africa, the Southwest Pacific and China. It also had a significant role in the Middle East, Southeast Asia, Eastern Europe, Alaska and Italy. The P-40's performance at high altitudes was not as important in those theaters, where it served as an air superiority fighter, bomber escort and fighter bomber. Although it gained a postwar reputation as a mediocre design, suitable only for close air support, recent research including scrutiny of the records of individual Allied squadrons, indicates that the P-40 performed surprisingly well as an air superiority fighter, at times suffering severe losses but also taking a very heavy toll of enemy aircraft, especially when flown against the lightweight and maneuverable Japanese fighters like the Oscar and Zero in the manner recommended in 1941 by General Claire Chennault, the AVG's commander in southern China. The P-40 offered the additional advantage of low cost, which kept it in production as a ground-attack aircraft long after it was obsolete as a fighter. In 2008, 29 P-40s were airworthy.
Design and development.
Origins.
On 14 October 1938, Curtiss test pilot Edward Elliott flew the prototype XP-40, on its first flight in Buffalo. The XP-40 was the 10th production Curtiss P-36 Hawk, with its Pratt & Whitney R-1830 (Twin Wasp) 14-cylinder air-cooled radial engine replaced at the direction of Chief Engineer Don R. Berlin by a liquid-cooled, supercharged Allison V-1710 V-12 engine. The first prototype placed the glycol coolant radiator in an underbelly position on the fighter, just aft of the wing's trailing edge. USAAC Fighter Projects Officer Lieutenant Benjamin S. Kelsey flew this prototype some 300 miles in 57 minutes, approximately . Hiding his disappointment, he told reporters that future versions would likely go faster. Kelsey was interested in the Allison engine because it was sturdy and dependable, and it had a smooth, predictable power curve. The V-12 engine offered as much power as a radial engine but had a smaller frontal area and allowed a more streamlined cowl than an aircraft with radial engines, promising a theoretical 5% increase in top speed.
Curtiss engineers worked to improve the XP-40's speed by moving the radiator forward in steps. Seeing little gain, Kelsey ordered the aircraft to be evaluated in a NACA wind tunnel to identify solutions for better aerodynamic qualities. From 28 March to 11 April 1939, the prototype was studied by NACA. Based on the data obtained, Curtiss moved the glycol coolant radiator forward to the chin; its new air scoop also accommodated the oil cooler air intake. Other improvements to the landing gear doors and the exhaust manifold combined to give performance that was satisfactory to the USAAC. Without beneficial tail winds, Kelsey flew the XP-40 from Wright Field back to Curtiss's plant in Buffalo at an average speed of . Further tests in December 1939 proved the fighter could reach .
An unusual production feature was a special truck rig to speed delivery at the main Curtiss plant in Buffalo, New York. The rig moved the newly built P-40s in two main components, the main wing and the fuselage, the eight miles from the plant to the airport where the two units were mated for flight and delivery.
Performance characteristics.
The P-40 was originally conceived as a ground support aircraft and was very agile at low and medium altitudes but suffered due to lack of power at higher altitudes. At medium and high speeds it was one of the tightest turning early monoplane designs of the war due to its great structural strength. At lower speeds it was out turned by the lightweight fighters A6M Zero and Nakajima Ki-43 "Oscar" which did not possess the structural strength of the P-40 for high speed hard turns. The American Volunteer Group Commander Claire Chennault advised against prolonged dog fighting with the Japanese fighters due to the resulting airspeed reduction which favored the lightweight Japanese designs low speed turning superiority.
Allison V-1710 engines produced about at sea level and at : not powerful by the standards of the time and the early P-40 variants' top speeds were unimpressive. Also, the single-stage, single-speed supercharger meant that the P-40 could not compete with contemporary designs as a high-altitude fighter. Later versions, with Allisons or more powerful 1,400 hp Packard Merlin engines were more capable. Climb performance was fair to poor, depending on the subtype. Dive acceleration was good and dive speed was excellent. The highest-scoring P-40 ace, Clive Caldwell (RAAF), who claimed 22 of his 28½ kills in the type, said that the P-40 had "almost no vices", although "it was a little difficult to control in terminal velocity". Caldwell added that the P-40 was "faster downhill than almost any other aeroplane with a propeller."
The P-40 tolerated harsh conditions in the widest possible variety of climates. It was a semi-modular design and thus easy to maintain in the field. It lacked innovations of the time, such as boosted ailerons or automatic leading edge slats, but it had a strong structure including a five-spar wing, which enabled P-40s to survive some midair collisions: both accidental impacts and intentional ramming attacks against enemy aircraft were occasionally recorded as victories by the Desert Air Force and Soviet Air Forces. Caldwell said P-40s "would take a tremendous amount of punishment, violent aerobatics as well as enemy action." Operational range was good by early war standards, and was almost double that of the Supermarine Spitfire or Messerschmitt Bf 109, although it was inferior to the Mitsubishi A6M Zero, Nakajima Ki-43 and Lockheed P-38 Lightning.
Caldwell found the P-40C Tomahawk's armament of two .50 in (12.7 mm) Browning AN/M2 "light-barrel" dorsal nose-mount synchronized machine guns and two .303 Browning machine guns in each wing to be inadequate. This was rectified with the P-40D (Kittyhawk I) which abandoned the synchronized gun mounts and instead had two .50 in (12.7 mm) guns in each wing, although Caldwell still preferred the earlier Tomahawk in other respects. The D had armor around the engine and the cockpit, which enabled it to withstand considerable damage. This was one of the characteristics that allowed Allied pilots in Asia and the Pacific to attack Japanese fighters head on, rather than try to out-turn and out-climb their opponents. Late-model P-40s were regarded as well armored. Visibility was adequate, although hampered by an overly complex windscreen frame, and completely blocked to the rear in early models due to the raised turtledeck. Poor ground visibility and the relatively narrow landing gear track led to many losses due to accidents on the ground.
Operational history.
In April 1939, the U.S. Army Air Corps, witnessing the new sleek, high speed, in-line-engined fighters of the European air forces, placed the largest, single fighter order it had ever made: 524 P-40s.
French Air Force.
An early order came from the French "Armée de l'Air", which was already operating P-36s. The "Armée de l'Air" initially ordered 100 (later the order was increased to 230) as the Hawk 81A-1 but the French military had been defeated before the aircraft had left the factory, consequently, the aircraft were diverted to British and Commonwealth service (as the Tomahawk I), in some cases complete with metric flight instruments.
In late 1942, as French forces in North Africa split from the Vichy government to side with the Allies, U.S. forces transferred P-40Fs from 33rd FG to the "GC II/5", a squadron that was historically associated with the Lafayette Escadrille. GC II/5 used its P-40Fs and Ls in combat in Tunisia and, later, for patrol duty off the Mediterranean coast until mid-1944 when they were replaced by Republic P-47D Thunderbolts.
British Commonwealth units in Mediterranean and European theatres.
Deployment.
In all, 18 Royal Air Force (RAF) squadrons, as well as four Royal Canadian Air Force (RCAF), three South African Air Force (SAAF), and two Royal Australian Air Force (RAAF) squadrons serving with RAF formations, used P-40s.
The first units to convert were Hawker Hurricane squadrons of the Desert Air Force (DAF), in early 1941. The first Tomahawks delivered came without armor, bulletproof windscreens or self-sealing fuel tanks. These were installed in subsequent shipments. Pilots used to British-designed fighters sometimes found it difficult to adapt to the P-40's rear-folding landing gear, which was more prone to collapse than the lateral-folding landing gear found on the Hawker Hurricane or Supermarine Spitfire. In contrast to the "three-point landing" commonly employed with British types, P-40 pilots were obliged to use a "wheels landing": a longer, low angle approach that touched down on the main wheels first.
Testing showed the aircraft did not have adequate performance for use in Northwest Europe in high-altitude combat due to the effective service ceiling limitation. Spitfires used in the theater operated at heights around , while the P-40's Allison engine, with its single-stage, low altitude rated supercharger, worked best at or lower. When the Tomahawk was used by Allied units based in the UK from February 1941, this limitation relegated the Tomahawk to low-level reconnaissance with RAF Army Cooperation Command
and only one squadron, No. 403 Squadron RCAF was used in the fighter role. Subsequently, the British Air Ministry deemed the P-40 completely unsuitable for the theater. UK P-40 squadrons from mid-1942 re-equipped with aircraft such as Mustangs.
The Tomahawk was superseded in North Africa by the more powerful Kittyhawk ("D"-mark onwards) types from early 1942, though some Tomahawks remained in service until 1943. Kittyhawks included many major improvements, and were the DAF's air superiority fighter for the critical first few months of 1942, until "tropicalised" Spitfires were available. An interesting discovery in 2012 was the virtually intact remains of a Kittyhawk that ran out of fuel in the Egyptian Sahara in June 1942.
DAF units received nearly 330 Packard V-1650 Merlin-powered P-40Fs, called Kittyhawk IIs, most of which went to the USAAF, and the majority of the 700 "lightweight" L models, also powered by the Packard Merlin, in which the armament was reduced to four .50 in (12.7 mm) Brownings (Kittyhawk IIA). The DAF also received some 21 of the later P-40K and the majority of the 600 P-40Ms built; these were known as Kittyhawk IIIs. The "lightweight" P-40Ns (Kittyhawk IV) arrived from early 1943 and were used mostly in the fighter-bomber role. 
From July 1942 until mid-1943, elements of the U.S. 57th Fighter Group (57th FG) were attached to DAF P-40 units. The British government also donated 23 P-40s to the Soviet Union.
Combat performance.
Tomahawks and Kittyhawks bore the brunt of "Luftwaffe" and "Regia Aeronautica" fighter attacks during the North African campaign. The P-40s were considered superior to the Hurricane, which they replaced as the primary fighter of the Desert Air Force. 
The P-40 initially proved quite effective against Axis aircraft and contributed to a slight shift of momentum in the Allied favor. The gradual replacement of Hurricanes by the Tomahawks and Kittyhawks led to the "Luftwaffe" accelerating retirement of the Bf 109E and introducing the newer Bf 109F; these were to be flown by the veteran pilots of elite "Luftwaffe" units, such as "Jagdgeschwader" 27 (JG27), in North Africa.
The P-40 was generally considered roughly equal or slightly superior to the Bf 109 at low altitude, but inferior at high altitude, particularly against the Bf 109F. Most air combat in North Africa took place well below , thus negating much of the Bf 109's superiority. The P-40 usually had an edge over the Bf 109 in horizontal maneuverability, dive speed and structural strength, was roughly equal in firepower, but was slightly inferior in speed and outclassed in rate of climb and operational ceiling.
The P-40 was generally superior to early Italian fighter types, such as the Fiat G.50 and the Macchi C.200. Its performance against the Macchi C.202 "Folgore" elicited varying opinions. Some observers consider the Macchi C.202 superior. Clive Caldwell, who scored victories against them in his P-40, felt that the "Folgore" was superior to both the P-40 and the Bf 109 except that its armament of only two or four machine guns was inadequate. Other observers considered the two equally matched, or favored the "Folgore" in aerobatic performance, such as turning radius. Aviation historian Walter J. Boyne wrote that over Africa, the P-40 and the "Folgore" were "equivalent".
Against its lack of high-altitude performance, the P-40 was considered to be a stable gun platform, and its rugged construction meant that it was able to operate from rough front line airstrips with a good rate of serviceability.
The earliest victory claims by P-40 pilots include Vichy French aircraft, during the 1941 Syria-Lebanon campaign, against Dewoitine D.520s, a type often considered to be the best French fighter used during World War II. The P-40 was deadly against Axis bombers in the theater, as well as against the Bf 110 twin-engine fighter.
In June 1941, Caldwell, who was serving at the time with No. 250 Squadron RAF in Egypt, and flying as F/O Jack Hamlyn's wingman, recorded in his log book that he was involved in the first air combat victory for the P-40. This was a CANT Z.1007 bomber on 6 June. The claim was not officially recognized, as the crash of the CANT was not witnessed. The first official victory occurred on 8 June, when Hamlyn and Flt Sgt Tom Paxton destroyed a CANT Z.1007 from "211a Squadriglia" of the "Regia Aeronautica", over Alexandria. Several days later, the Tomahawk was in action over Syria with No. 3 Squadron RAAF, which claimed 19 aerial victories over Vichy French aircraft during June and July 1941, for the loss of one P-40 (as well as one lost to ground fire).
Some DAF units initially failed to use the P-40's strengths and/or utilised outdated defensive tactics such as the Lufbery circle. However, the superior climb rate of the Bf 109 enabled fast, swooping attacks, neutralizing the advantages offered by conventional defensive tactics. Various new formations were tried by Tomahawk units in 1941-42, including: "fluid pairs" (similar to the German "rotte"); one or two "weavers" at the back of a squadron in formation, and whole squadrons bobbing and weaving in loose formations. Werner Schröer, who was credited with destroying 114 Allied aircraft in only 197 combat missions, referred to the latter formation as "bunches of grapes", because he found them so easy to pick off. The leading German "expert" in North Africa, Hans-Joachim Marseille, claimed as many as 101 P-40s during his career.
From 26 May 1942, all Kittyhawk units operated primarily as fighter-bomber units, giving rise to the nickname "Kittybomber". As a result of this change in role, and because DAF P-40 squadrons were frequently used in bomber escort and close air support missions, they suffered relatively high attrition rates; many Desert Air Force P-40 pilots were caught flying low and slow by marauding Bf 109s.
Caldwell believed that Operational Training Units did not properly prepare pilots for air combat in the P-40, and as a commander, stressed the importance of training novice pilots properly.
Nevertheless, competent pilots who took advantage of the P-40's strengths were effective against the best of the "Luftwaffe" and "Regia Aeronautica". At least 46 British Commonwealth pilots achieved ace status flying the P-40. For example, on one occasion in August 1941, Caldwell was attacked by two Bf 109s, one of them piloted by German ace Werner Schröer. Although Caldwell was wounded three times, and his Tomahawk was hit by more than 100 bullets and five 20 mm cannon shells, Caldwell shot down Schröer's wingman and returned to base. Some sources also claim that in December 1941, Caldwell killed a prominent German "Experte", Erbo von Kageneck (69 kills), while flying a P-40. Caldwell's victories in North Africa included 10 Bf 109s and two Macchi C.202s. Billy Drake of 112 Squadron was the leading British P-40 ace with 13 victories. James "Stocky" Edwards (RCAF), who achieved 12 kills in the P-40 in North Africa, shot down German ace Otto Schulz (51 kills) while flying a Kittyhawk with No. 260 Squadron RAF. Caldwell, Drake, Edwards and Nicky Barr were among at least a dozen pilots who achieved ace status twice over while flying the P-40. A total of 46 British Commonwealth pilots became aces in P-40s, including seven double aces.
Chinese Air Force.
Flying Tigers (American Volunteer Group).
The Flying Tigers, known officially as the 1st American Volunteer Group (AVG), were a unit of the Chinese Air Force, recruited from U.S. aviators. From late 1941, the P-40B was used by the Flying Tigers. They were divided into three pursuit squadrons, the "Adam & Eves", the "Panda Bears" and the "Hell's Angels".
Compared to opposing Japanese fighters, the P-40B's strengths were that it was sturdy, well armed, faster in a dive and possessed an excellent rate of roll. While the P-40s could not match the maneuverability of the Japanese Army air arm's Nakajima Ki-27s and Ki-43s, nor the much more famous Zero naval fighter in a slow speed turning dogfight, at higher speeds the P-40s were more than a match. AVG leader Claire Chennault trained his pilots to use the P-40's particular performance advantages. The P-40 had a higher dive speed than any Japanese fighter aircraft of the early war years, for example, and could be used to exploit so-called "boom-and-zoom" tactics. The AVG was highly successful, and its feats were widely published, to boost sagging public morale at home, by an active cadre of international journalists. According to their official records, in just 6 1/2 months, the Flying Tigers destroyed 115 enemy aircraft for the loss of just four of their own in air-to-air combat.
4th Air Group.
China received 27 P-40E in early 1943. These were assigned to squadrons of the 4th Air Group.
United States Army Air Forces.
A total of 15 entire USAAF pursuit/fighter groups (FG), along with other pursuit/fighter squadrons and a few tactical reconnaissance (TR) units, operated the P-40 during 1941–45.
As was also the case with the Bell P-39 Airacobra, many USAAF officers considered the P-40 inadequate, and it was gradually replaced by the Lockheed P-38 Lightning, the Republic P-47 Thunderbolt and the North American P-51 Mustang. However, the bulk of the fighter operations by the USAAF in 1942–43 were borne by the P-40 and the P-39. In the Pacific, these two fighters, along with the U.S. Navy's Grumman F4F Wildcat, contributed more than any other U.S. types to breaking Japanese air power during this critical period.
Pacific theaters.
The P-40 was the main USAAF fighter aircraft in the South West Pacific and Pacific Ocean theaters during 1941–42.
In the first major battles, at Pearl Harbor and in the Philippines, USAAF P-40 squadrons suffered crippling losses on the ground and in the air to Japanese fighters such as the Ki-43 Oscar and A6M Zero. During the attack on Pearl Harbor, a few P-40s managed to shoot down several Japanese planes, most notably by George Welch and Kenneth Taylor.
However, in the Dutch East Indies campaign, the 17th Pursuit Squadron (Provisional), formed from USAAF pilots evacuated from the Philippines, claimed 49 Japanese aircraft destroyed, for the loss of 17 P-40s The seaplane tender USS "Langley" was sunk by Japanese planes while delivering P-40s to Tjilatjap, Java. In the Solomon Islands and New Guinea Campaigns, as well as the air defence of Australia, improved tactics and training allowed the USAAF to more effectively utilize the strengths of the P-40.
Due to aircraft fatigue, scarcity of spare parts and replacement problems, the US Fifth Air Force and Royal Australian Air Force created a joint P-40 management and replacement pool on 30 July 1942 and many P-40s went back and forth between the air forces.
The 49th Fighter Group was in action in the Pacific from the beginning of the war. Robert DeHaven scored 10 kills (of 14 overall) in the P-40 with the 49th FG. He compared the P-40 favorably with the P-38:
The 8th, 15th, 18th, 24th, 49th, 343rd and 347th PGs/FGs, flew P-40s in the Pacific theaters between 1941 and 1945, with most units converting to P-38s during 1943-44. In 1945, the 71st Reconnaissance Group employed them as armed forward air controllers during ground operations in the Philippines until it received delivery of P-51s. They claimed 655 aerial victories.
Contrary to conventional wisdom, with sufficient altitude the P-40 could actually turn with the A6M and other Japanese fighters, using a combination of a nose-down vertical turn with a bank turn, a technique known as a low yo-yo. Robert DeHaven describes how this tactic was used in the 49th Fighter group:
China Burma India Theater.
USAAF and Chinese P-40 pilots performed well in this theater, scoring high kill ratios against Japanese types such as the Ki-43, Nakajima Ki-44 "Tojo" and the Zero. The P-40 remained in use in the China Burma India Theater (CBI) until 1944, and was reportedly preferred over the P-51 Mustang by some US pilots flying in China.
The American Volunteer Group (Flying Tigers) was integrated into the USAAF as the 23rd Fighter Group in June 1942. The unit continued to fly newer model P-40s until the end of the war, racking up a high kill-to-loss ratio.
Units arriving in the CBI after the AVG in the 10th and 14th Air Forces continued to perform well with the P-40, claiming 973 kills in the theater, or 64.8 percent of all enemy aircraft shot down. Aviation historian Carl Molesworth stated that "...the P-40 simply dominated the skies over Burma and China. They were able to establish air superiority over free China, northern Burma and the Assam valley of India in 1942, and they never relinquished it."
In addition to the 23rd FG, the 3rd, 5th, 51st and 80th FGs, along with the 10th TRS, operated the P-40 in the CBI. In addition to its role as a fighter aircraft, CBI P-40 pilots used the aircraft very effectively as a fighter-bomber. The 80th Fighter Group in particular used its so-called "B-40" (P-40s carrying 1,000-pound high explosive bombs) to destroy Japanese-held bridges and kill bridge repair crews, sometimes demolishing their target with a single bomb. At least 40 U.S. pilots reached ace status while flying the P-40 in the CBI.
Europe and Mediterranean theaters.
On 14 August 1942, the first confirmed victory by a USAAF unit over a German aircraft in World War II was achieved by a P-40C pilot. 2nd Lt Joseph D. Shaffer, of the 33rd Fighter Squadron, intercepted a Focke-Wulf Fw 200C-3 maritime patrol plane that overflew his base at Reykjavík, Iceland. Shaffer damaged the Fw 200, which was finished off by a P-38F.
Warhawks were used extensively in the Mediterranean Theater of Operations (MTO) by USAAF units, including the 33rd, 57th, 58th, 79th, 324th and 325th Fighter Groups.
While the P-40 suffered heavy loses in the MTO, many USAAF P-40 units achieved high kill-to-loss ratios against Axis aircraft. For example, the 324th FG scored better than a 2:1 ratio in the MTO. In all, 23 U.S. pilots became aces in the MTO while flying the P-40, most of them during the first half of 1943. As in the Pacific, success in combat depended in part on experience and effective tactics.
Individual pilots from the 57th FG were the first USAAF P-40 pilots to see action in the MTO, while attached to Desert Air Force Kittyhawk squadrons, from July 1942. The 57th was also the main unit involved in the "Palm Sunday Massacre", on 18 April 1943. De-coded Ultra signals had given away a plan for a large formation of German Junkers Ju 52 transports to cross the Mediterranean, escorted by German and Italian fighters. Between 1630 and 1830 hours, all wings of the Group were engaged in an intensive effort against the enemy air transports. Of the four Kittyhawk Wings, three had left the patrol area before a convoy of a 100+ enemy transports were sighted by 57 Group, who tallied 74 aircraft destroyed. 57 Group was last in the area, and intercepted the Ju 52s escorted by large numbers of Bf 109s, Bf 110s and Macchi C.202s. In all, they claimed 58 Ju 52s, 14 Bf 109s and two Bf 110s destroyed with a number of others probably destroyed and damaged. Between 20–40 of the E/A landed on the beaches around Cap Bon to avoid being shot down. Six Allied fighters were lost, five of them P-40s.
On 22 April, in Operation Flax, a similar force of P-40s attacked a formation of 14 Messerschmitt Me 323 "Gigant" ("Giant") six-engine transports, covered by seven Bf 109s from II./JG 27. All the transports were shot down, for a loss of three P-40s destroyed. The 57th FG was equipped with the Curtiss fighter until early 1944, during which time they were credited with at least 140 air-to-air kills.
In early 1943, 75 P-40Ls were transported on the aircraft carrier . On 23 February, during Operation Torch, the pilots of the 58th FG flew these P-40s off "Ranger" to land at newly captured Vichy French airfield, Cazas, near Casablanca, in French Morocco. The aircraft resupplied the 33rd FG and the pilots were reassigned.
The 325th FG (known as the "Checkertail Clan") flew P-40s in the MTO. The 325th was credited with at least 133 air-to-air kills in April–October 1943, of which 95 were Bf 109s and 26 were Macchi C.202s, for the loss of 17 P-40s in combat. An anecdote concerning the 325th FG, indicates what could happen if Bf 109 pilots made the mistake of trying to out-turn the P-40. 325th FG historian Carol Cathcart wrote: "on 30 July, 20 P-40s of the 317th [Fighter Squadron] ... took off on a fighter sweep ... over Sardinia. As they turned to fly south over the west part of the island, they were attacked near Sassari... The attacking force consisted of 25 to 30 Bf 109s and Macchi C.202s... In the brief, intense battle that occurred ... [the 317th claimed] 21 enemy aircraft." Cathcart states that Lt. Robert Sederberg who assisted a comrade being attacked by five Bf 109s, destroyed at least one German aircraft, and may have shot down as many as five. Sederberg was shot down in the dogfight and became a prisoner of war.
A famous African-American unit, the 99th FS, better known as the "Tuskegee Airmen" or "Redtails", flew P-40s in stateside training and for their initial eight months in the MTO. On 9 June 1943, they became the first African-American fighter pilots to engage enemy aircraft, over Pantelleria, Italy. A single Focke Wulf Fw 190 was reported damaged by Lieutenant Willie Ashley Jr. On 2 July the squadron claimed its first verified kill; a Fw 190 destroyed by Captain Charles Hall. The 99th continued to score with P-40s until February 1944, when they were assigned P-39s and P-51 Mustangs.
The much-lightened P-40L was most heavily used in the MTO, primarily by U.S. pilots. Many US pilots stripped down their P-40s even further to improve performance, often removing two or more of the wing guns from the P-40F/L.
Royal Australian Air Force.
The Kittyhawk was the main fighter used by the RAAF in World War II, in greater numbers than the Spitfire. Two RAAF squadrons serving with the Desert Air Force, No. 3 and No. 450 Squadrons, were the first Australian units to be assigned P-40s. Other RAAF pilots served with RAF or SAAF P-40 squadrons in the theater.
Many RAAF pilots achieved high scores in the P-40. At least five reached "double ace" status: Clive Caldwell, Nicky Barr, John Waddy, Bob Whittle (11 kills each) and Bobby Gibbes (10 kills) in the Middle East, North African and/or New Guinea campaigns. In all, 18 RAAF pilots became aces while flying P-40s.
Nicky Barr, like many Australian pilots, considered the P-40 a reliable mount: "The Kittyhawk became, to me, a friend. It was quite capable of getting you out of trouble more often than not. It was a real warhorse."
At the same time as the heaviest fighting in North Africa, the Pacific War was also in its early stages, and RAAF units in Australia were completely lacking in suitable fighter aircraft. Spitfire production was being absorbed by the war in Europe; P-38s were trialled, but were difficult to obtain; Mustangs had not yet reached squadrons anywhere, and Australia's tiny and inexperienced aircraft industry was geared towards larger aircraft. USAAF P-40s and their pilots originally intended for the U.S. Far East Air Force in the Philippines, but diverted to Australia as a result of Japanese naval activity were the first suitable fighter aircraft to arrive in substantial numbers. By mid-1942, the RAAF was able to obtain some USAAF replacement shipments; the P-40 was given the RAAF designation A-29.
RAAF Kittyhawks played a crucial role in the South West Pacific theater. They fought on the front line as fighters during the critical early years of the Pacific War, and the durability and bomb-carrying abilities (1,000 lb/454 kg) of the P-40 also made it ideal for the ground attack role. For example, 75, and 76 Squadrons played a critical role during the Battle of Milne Bay, fending off Japanese aircraft and providing effective close air support for the Australian infantry, negating the initial Japanese advantage in light tanks and sea power.
The RAAF units that most used Kittyhawks in the South West Pacific were: 75, 76, 77, 78, 80, 82, 84 and 86 Squadrons. These squadrons saw action mostly in the New Guinea and Borneo campaigns.
Late in 1945, RAAF fighter squadrons in the South West Pacific began converting to P-51Ds. However, Kittyhawks were in use with the RAAF until the end of the war, in Borneo. In all, the RAAF acquired 841 Kittyhawks (not counting the British-ordered examples used in North Africa), including 163 P-40E, 42 P-40K, 90 P-40 M and 553 P-40N models. In addition, the RAAF ordered 67 Kittyhawks for use by No. 120 (Netherlands East Indies) Squadron (a joint Australian-Dutch unit in the South West Pacific). The P-40 was retired by the RAAF in 1947.
Royal Canadian Air Force.
A total of 13 Royal Canadian Air Force units operated the P-40 in the North West European or Alaskan theaters.
In mid-May 1940, Canadian and US officers watched comparative tests of a XP-40 and a Spitfire, at RCAF Uplands, Ottawa. While the Spitfire was considered to have performed better, it was not available for use in Canada and the P-40 was ordered to meet home air defense requirements. In all, eight Home War Establishment Squadrons were equipped with the Kittyhawk: 72 Kittyhawk I, 12 Kittyhawk Ia, 15 Kittyhawk III and 35 Kittyhawk IV aircraft, for a total of 134 aircraft. These aircraft were mostly diverted from RAF Lend-Lease orders for service in Canada. The P-40 Kittyhawks were obtained in lieu of 144 P-39 Airacobras originally allocated to Canada but reassigned to the RAF.
However, before any home units received the P-40, three RCAF Article XV squadrons operated Tomahawk aircraft from bases in the United Kingdom. No. 403 Squadron RCAF, a fighter unit, used the Tomahawk Mk II briefly before converting to Spitfires. Two Army Co-operation (close air support) squadrons: 400 and 414 Sqns trained with Tomahawks, before converting to Mustang Mk. I aircraft and a fighter/reconnaissance role. Of these, only No. 400 Squadron used Tomahawks operationally, conducting a number of armed sweeps over France in the late 1941. RCAF pilots also flew Tomahawks or Kittyhawks with other British Commonwealth units based in North Africa, the Mediterranean, South East Asia and (in at least one case) the South West Pacific.
In 1942, the Imperial Japanese Navy occupied two islands, Attu and Kiska, in the Aleutians, off Alaska. RCAF home defense P-40 squadrons saw combat over the Aleutians, assisting the USAAF. The RCAF initially sent 111 Squadron, flying the Kittyhawk I, to the US base on Adak island. During the drawn-out campaign, 12 Canadian Kittyhawks operated on a rotational basis from a new, more advanced base on Amchitka, southeast of Kiska. 14 and 111 Sqns took "turn-about" at the base. During a major attack on Japanese positions at Kiska on 25 September 1942, Squadron Leader Ken Boomer shot down a Nakajima A6M2-N ("Rufe") seaplane. The RCAF also purchased 12 P-40Ks directly from the USAAF while in the Aleutians. After the Japanese threat diminished, these two RCAF squadrons returned to Canada and eventually transferred to England without their Kittyhawks.
In January 1943, a further Article XV unit, 430 Squadron was formed at RAF Hartford Bridge, England and trained on obsolete Tomahawk IIA. The squadron converted to the Mustang I before commencing operations in mid-1943.
In early 1945 pilots from No. 133 Squadron RCAF, operating the P-40N out of RCAF Patricia Bay, (Victoria, BC), intercepted and destroyed two Japanese balloon-bombs, which were designed to cause wildfires on the North American mainland. On 21 February, Pilot Officer E. E. Maxwell shot down a balloon, which landed on Sumas Mountain in Washington State. On 10 March, Pilot Officer J. 0. Patten destroyed a balloon near Saltspring Island, BC. The last interception took place on 20 April 1945 when Pilot Officer P.V. Brodeur from 135 Squadron out of Abbotsford, British Columbia shot down a balloon over Vedder Mountain.
The RCAF units that operated P-40s were, in order of conversion: 
Royal New Zealand Air Force.
Some Royal New Zealand Air Force (RNZAF) pilots and New Zealanders in other air forces flew British P-40s while serving with DAF squadrons in North Africa and Italy, including the ace Jerry Westenra.
A total of 301 P-40s were allocated to the RNZAF under Lend-Lease, for use in the Pacific Theater, although four of these were lost in transit. The aircraft equipped 14 Squadron, 15 Squadron, 16 Squadron, 17 Squadron, 18 Squadron, 19 Squadron and 20 Squadron.
RNZAF P-40 squadrons were successful in air combat against the Japanese between 1942 and 1944. Their pilots claimed 100 aerial victories in P-40s, whilst losing 20 aircraft in combat Geoff Fisken, the highest scoring British Commonwealth ace in the Pacific, flew P-40s with 15 Squadron, although half of his victories were claimed with the Brewster Buffalo.
The overwhelming majority of RNZAF P-40 victories were scored against Japanese fighters, mostly Zeroes. Other victories included Aichi D3A "Val" dive bombers. The only confirmed twin engine claim, a Ki-21 "Sally" (misidentified as a G4M "Betty") fell to Fisken in July 1943.
From late 1943 and 1944, RNZAF P-40s were increasingly used against ground targets, including the innovative use of naval depth charges as improvised high-capacity bombs. The last front line RNZAF P-40s were replaced by Vought F4U Corsairs in 1944. The P-40s were relegated to use as advanced pilot trainers.
The remaining RNZAF P-40s, excluding the 20 shot down and 154 written off, were mostly scrapped at Rukuhia in 1948.
Soviet Union.
The Soviet "Voenno-Vozdushnye Sily" (VVS; "Military Air Forces") and "Morskaya Aviatsiya" (MA; "Naval Air Service") also referred to P-40s as "Tomahawks" and "Kittyhawks". In fact, the Curtiss P-40 Tomahawk / Kittyhawk was the first Allied fighter supplied to the USSR under the Lend-Lease agreement.
The Soviet units received 247 P-40B/Cs (equivalent to the Tomahawk IIA/B in RAF service) and 2,178 P-40E, -K, -L, and -N models between 1941 and 1944. The Tomahawks were shipped from Great Britain and directly from the US, many of them arriving incomplete, lacking machine guns and even the lower half of the engine cowling. In late September 1941, the first 48 P-40s were assembled and checked in USSR. Test flights showed some manufacturing defects: generator and oil pump gears and generator shafts failed repeatedly, which led to emergency landings. The test report indicated that the Tomahawk was inferior to Soviet "M-105P-powered production fighters in speed and rate of climb. However, it had good short field performance, horizontal manoeuvrability, range and endurance."
Nevertheless, Tomahawks and Kittyhawks were used against the Germans. The 126th IAP fighting on the Western and Kalinin fronts were the first unit to receive the P-40. The regiment entered action on 12 October 1941. By 15 November 1941, that unit had shot down 17 German aircraft. However, Lt (SG) Smirnov noted that the P-40 armament was sufficient for strafing enemy lines but rather ineffective in aerial combat. Another pilot, S.G. Ridnyy (Hero of Soviet Union), remarked that he had to shoot half the ammunition at 50–100 meters (164–339 ft) to shoot down an enemy aircraft.
In January 1942, some 198 aircraft sorties were flown (334 flying hours) and 11 aerial engagements were conducted, in which five Bf 109s, one Ju 88, and one He 111 were downed. These statistics reveal a surprising fact: it turns out that the Tomahawk was fully capable of successful air combat with a Bf 109. The reports of pilots about the circumstances of the engagements confirm this fact. On 18 January 1942, Lieutenants S. V. Levin and I. P. Levsha (in pair) fought an engagement with seven Bf 109s and shot down two of them without loss. On 22 January, a flight of three aircraft led by Lieutenant E. E. Lozov engaged 13 enemy aircraft and shot down two Bf 109Es, again without loss. Altogether, in January, two Tomahawks were lost; one downed by German antiaircraft artillery and one lost to Messerschmitts.
The Soviets stripped down their P-40s significantly for combat, in many cases removing the wing guns altogether in P-40B/C types, for example. Soviet Air Force reports state that they liked the range and fuel capacity of the P-40, which were superior to most of the Soviet fighters, though they still preferred the P-39. Soviet pilot Nikolai G. Golodnikov recalled: "The cockpit was vast and high. At first it felt unpleasant to sit waist-high in glass, as the edge of the fuselage was almost at waist level. But the bullet-proof glass and armoured seat were strong and visibility was good. The radio was also good. It was powerful, reliable, but only on HF (high frequency). The American radios did not have hand microphones but throat microphones. These were good throat mikes: small, light and comfortable." The biggest complaint of some Soviet airmen was its poor climb rate and problems with maintenance, especially with burning out the engines. VVS pilots usually flew the P-40 at War Emergency Power settings while in combat, bringing the acceleration and speed performance closer to that of their German rivals, but could burn out engines in a matter of weeks. They also had difficulty with the more demanding requirements for fuel quality and oil purity of the Allison engines. A fair number of burnt out P-40s were re-engined with Soviet Klimov engines but these performed relatively poorly and were relegated to rear area use.
The P-40 saw the most front line use in Soviet hands in 1942 and early 1943. Deliveries over the Alaska-Siberia ALSIB ferry route began in October 1942. It was used in the northern sectors and played a significant role in the defense of Leningrad. The most numerically important types were P-40B/C, P-40E and P-40K/M. By the time the better P-40F and N types became available, production of superior Soviet fighters had increased sufficiently so that the P-40 was replaced in most Soviet Air Force units by the Lavochkin La-5 and various later Yakovlev types. In spring 1943, Lt D.I. Koval of the 45th IAP gained ace status on the North-Caucasian front, shooting down six German aircraft flying a P-40. Some Soviet P-40 squadrons had good combat records. They provided close air support as well as air-to-air capability while Soviet pilots became aces on the P-40, not as many as on the P-39 Airacobra, which was the most numerous Lend Lease fighter used by the Soviet Union. However Soviet commanders considered the Kittyhawk to significantly outclass the Hurricane, although it was "not in the same league as the Yak-1".
Japan.
The Japanese Army captured some P-40s and later operated a number in Burma. The Japanese appear to have had as many as 10 flyable P-40Es. For a brief period in 1943, a few of them were actually used operationally by 2 "Hiko Chutai", 50 "Hiko Sentai" (2nd Air Squadron, 50th Air Regiment) in the defense of Rangoon. Testimony of this is given by Yasuhiko Kuroe, a member of the 64 "Hiko Sentai". In his memoirs, he says one Japanese-operated P-40 was shot down in error by a friendly Mitsubishi Ki-21 "Sally" over Rangoon.
Other nations.
The P-40 was used by over two dozen countries during and after the war. The P-40 was used by Brazil, Egypt, Finland and Turkey. The last P-40s in military service, used by the Brazilian Air Force (FAB), were retired in 1960.
In the air war over Finland, several Soviet P-40s were shot down or had to crash-land due to other reasons. The Finns, short of good aircraft, collected these and managed to repair one P-40M, P-40M-10-CU 43-5925, "white 23", which received Finnish Air Force serial number KH-51 (KH denoting "Kittyhawk", as the British designation of this type was Kittyhawk III). This aircraft was attached to an operational squadron HLeLv 32 of the Finnish Air Force, but lack of spares kept it on the ground, with the exception of a few evaluation flights.
Variants and development stages.
This new liquid-cooled engine fighter had a radiator mounted under the rear fuselage
but the prototype XP-40 was later modified and the radiator was moved forward under the engine.
Survivors.
On 11 May 2012, a crashed P-40 was found in the Sahara desert in phenomenal condition. The pilot went missing in 1942. Plans are being made to move the aircraft to a British museum.
Of the 13,738 P-40s built, only 28 P-40s remain airworthy, with three of them being converted to dual-controls/dual-seat configuration. Approximately 13 aircraft are on static display and another 36 airframes are under restoration for either display or flight.

</doc>
<doc id="7212" url="http://en.wikipedia.org/wiki?curid=7212" title="Creed">
Creed

A creed (also "confession", "symbol", or "statement of faith") is a statement of the shared beliefs of a religious community in the form of a fixed formula summarizing core tenets.
One of the most widely used creeds in Christianity is the Nicene Creed, first formulated in AD 325 at the First Council of Nicaea. It was based on Christian understanding of the Canonical Gospels, the letters of the New Testament and to a lesser extent the Old Testament. Affirmation of this creed, which describes the Trinity, is generally taken as a fundamental test of orthodoxy for most Christian denominations. The Apostles' Creed is also broadly accepted. Some Christian denominations and other groups have rejected the authority of those creeds.
Muslims declare the "shahada", or testimony: "I bear witness that there is no god but (the One) God "(Allah)", and I bear witness that Muhammad is God's messenger."
Whether Judaism is creedal has been a point of some controversy. Although some say Judaism is noncreedal in nature, others say it recognizes a single creed, the "Shema Yisrael", which begins: "Hear, O Israel: the LORD our God, the LORD is one."
Terminology.
The word "creed" is particularly used for a concise statement which is recited as part of liturgy. Term is anglicized from Latin "credo" "I believe", the "incipit" of the Latin texts of the Apostles' Creed and the Nicene Creed. 
A creed is sometimes referred to as a "symbol" in a specialized meaning of that word (which was first introduced to Late Middle English in this sense), after Latin "symbolum" "creed" (as in "Symbolum Apostolorum" = "Apostles' Creed"), after Greek "symbolon" "token, watchword"
Some longer statements of faith in the Protestant tradition are instead called "confessions of faith", or simply "confession" (as in e.g. Helvetic Confession).
Within Evangelicalism, the terms "doctrinal statement" or "doctrinal basis" tend to be preferred. Doctrinal statements may include positions on lectionary and translations of the Bible, particularly in fundamentalist churches of the King James Only movement.
The term "creed" is sometimes extended to comparable concepts in non-Christian theologies; thus the Islamic concept of "ʿaqīdah" (literally "bond, tie") is often rendered as "creed".
Christian creeds.
Several creeds have originated in Christianity.
Christian confessions of faith.
Protestant denominations are usually associated with confessions of faith, which are similar to creeds but usually longer.
Christians without creeds.
Some Christian denominations, and particularly those descending from the Radical Reformation, do not profess a creed. The Quakers, also known as the Religious Society of Friends, believe that they have no need for creedal formulations of faith. The Church of the Brethren also espouses no creed, referring to the New Testament, as their "rule of faith and practice." Jehovah's Witnesses contrast "memorizing or repeating creeds" with acting to "do what Jesus said". Unitarian Universalists, who practice probably the most liberal of all religions, do not share a creed.
Many evangelical Protestants similarly reject creeds as definitive statements of faith, even while agreeing with some creeds' substance. The Baptists have been non-creedal "in that they have not sought to establish binding authoritative confessions of faith on one another". While many Baptists are not opposed to the ancient creeds, they regard them as "not so final that they cannot be revised and re-expressed. At best, creeds have a penultimacy about them and, of themselves, could never be the basis of Christian fellowship". Moreover, Baptist "confessions of faith" have often had a clause such as this from the First London (Particular) Baptist Confession (Revised edition, 1646):
Similar reservations about the use of creeds can be found in the Restoration Movement and its descendants, the Christian Church (Disciples of Christ), the Churches of Christ, and the Christian churches and churches of Christ.
Some religious leaders in traditional creedal Churches have also come to question the utility of creeds. Bishop John Shelby Spong, retired Episcopal Bishop of Newark, has written that dogmas and creeds were merely "a stage in our development" and "part of our religious childhood." In his book, "Sins of the Scripture", Spong claims that "Jesus seemed to understand that no one can finally fit the holy God into his or her creeds or doctrines. That is idolatry."
Latter Day Saints.
Within the sects of the Latter Day Saint movement, the "Articles of Faith" are a list composed by Joseph Smith as part of an 1842 letter sent to "Long" John Wentworth, editor of the "Chicago Democrat". It is canonized with the "Pearl of Great Price", part of the standard works of The Church of Jesus Christ of Latter-day Saints.
Creedal works include:
Jewish creed.
Whether Judaism is creedal in character has generated some controversy. Rabbi Milton Steinberg wrote that "By its nature Judaism is averse to formal creeds which of necessity limit and restrain thought" and asserted in his book "Basic Judaism" (1947) that "Judaism has never arrived at a creed." The 1976 Centenary Platform of the Central Conference of American Rabbis, an organization of Reform rabbis, agrees that "Judaism emphasizes action rather than creed as the primary expression of a religious life."
Others, however, characterize the Shema Yisrael as a creedal statement in strict monotheism embodied in a single prayer: "Hear O Israel, the Lord is our God, the Lord is One" (; transliterated "Shema Yisrael Adonai Eloheinu Adonai Echad").
A notable statement of Jewish principles of faith was drawn up by Maimonides as his 13 Principles of Faith.
Islamic creed.
In Islamic theology, the term most closely corresponding to "creed" is "ʿaqīdah" ()
The first such creed was written as "a short answer to the pressing heresies of the time" is known as "Fiqh Akbar" and ascribed to Abū Ḥanīfa. Two well known creeds were the "Fiqh Akbar II" "representative" of the al-Ash'ari, and "Fiqh Akbar III", "representative" of the Ash-Shafi'i. Al-Ghazali also had a ʿAqīdah.
"Iman" () in Islamic theology denotes a believer's religious faith . Its most simple definition is the belief in the six articles of faith, known as "arkān al-īmān".
At an early time in the history of Islam, the article regarding "belief in prophets" became predominantly focussed on the veneration of Muhammad. The shahada, the two-part statement that "There is no god but God; Muhammad is the messenger of God" is often popularly called "the Islamic creed" and its utterance is one of the "five pillars" of Sunni Islam.

</doc>
<doc id="7213" url="http://en.wikipedia.org/wiki?curid=7213" title="Claudius Aelianus">
Claudius Aelianus

Claudius Aelianus (; c. 175 – c. 235 CE), often seen as just Aelian (), born at Praeneste, was a Roman author and teacher of rhetoric who flourished under Septimius Severus and probably outlived Elagabalus, who died in 222. He spoke Greek so perfectly that he was called "honey-tongued" ("meliglossos"); Roman-born, he preferred Greek authors, and wrote in a slightly archaizing Greek himself.
His two chief works are valuable for the numerous quotations from the works of earlier authors, which are otherwise lost, and for the surprising lore, which offers unexpected glimpses into the Greco-Roman world-view.
"De Natura Animalium".
"On the Nature of Animals" ("On the Characteristics of Animals" is an alternative title; Greek: Περὶ Ζῴων Ἰδιότητος; usually cited, though, by its Latin title: "De Natura Animalium"), is a curious collection, in 17 books, of brief stories of natural history, sometimes selected with an eye to conveying allegorical moral lessons, sometimes because they are just so astonishing:
The Loeb Classical Library introduction characterizes the book as
Aelian's anecdotes on animals rarely depend on direct observation: they are almost entirely taken from written sources, often Pliny the Elder, but also other authors and works now lost, to whom he is thus a valuable witness. He is more attentive to marine life than might be expected, though, and this seems to reflect first-hand personal interest; he often quotes "fishermen". At times he strikes the modern reader as thoroughly credulous, but at others he specifically states that he is merely reporting what is told by others, and even that he does not believe them. Aelian's work is one of the sources of medieval natural history and of the bestiaries of the Middle Ages; in some ways an allegory of the moral world, an Emblem Book.
The text as it has come down to us is badly mangled and garbled and replete with later interpolations. Conrad Gessner (or Gesner), the Swiss scientist and natural historian of the Renaissance, made a Latin translation of Aelian's work, to give it a wider European audience. An English translation by A. F. Scholfield has been published in the Loeb Classical Library, 3 vols. (19[ ]-59).
"Varia Historia".
"Various History" (Ποικίλη Ἱστορία) — for the most part preserved only in an abridged form — is Aelian's other well-known work, a miscellany of anecdotes and biographical sketches, lists, pithy maxims, and descriptions of natural wonders and strange local customs, in 14 books, with many surprises for the cultural historian and the mythographer, anecdotes about the famous Greek philosophers, poets, historians, and playwrights and myths instructively retold. The emphasis is on "various" moralizing tales about heroes and rulers, athletes and wise men; reports about food and drink, different styles in dress or lovers, local habits in giving gifts or entertainments, or in religious beliefs and death customs; and comments on Greek painting. Aelian gives an account of fly fishing, using lures of red wool and feathers, of lacquerwork, serpent worship — Essentially the "Various History" is a Classical "magazine" in the original senses of that word. He is not perfectly trustworthy in details, and his agenda was heavily influenced by Stoic opinions, perhaps so that his readers will not feel guilty, but Jane Ellen Harrison found survivals of archaic rites mentioned by Aelian very illuminating in her "Prolegomena to the Study of Greek Religion" (1903, 1922).
The first printing was in 1545. The standard modern text is Mervin R. Dilts's, of 1974.
Two English translations of the "Various History," by Fleming (1576) and Stanley (1665) made Aelian's miscellany available to English readers, but after 1665 no English translation appeared, until three English translations appeared almost simultaneously: James G. DeVoto, "Claudius Aelianus: Ποιϰίλης Ἱοτορίας ("Varia Historia")" Chicago, 1995; Diane Ostrom Johnson, "An English Translation of Claudius Aelianus' "Varia Historia"", 1997; and N. G. Wilson, "Aelian: Historical Miscellany" in the Loeb Classical Library.
Other works.
Considerable fragments of two other works, "On Providence" and "Divine Manifestations", are preserved in the early medieval encyclopedia, the "Suda." Twenty "letters from a farmer" after the manner of Alciphron are also attributed to him. The letters are invented compositions to a fictitious correspondent, which are a device for vignettes of agricultural and rural life, set in Attica, though mellifluous Aelian once boasted that he had never been outside Italy, never been aboard a ship (which is at variance, though, with his own statement, "de Natura Animalium" XI.40, that he had seen the bull Serapis with his own eyes). Thus conclusions about actual agriculture in the "Letters" are as likely to evoke Latium as Attica. The fragments have been edited in 1998 by D. Domingo-Foraste, but are not available in English. The "Letters" are available in the Loeb Classical Library, translated by Allen Rogers Benner and Francis H. Fobes (1949).

</doc>
<doc id="7214" url="http://en.wikipedia.org/wiki?curid=7214" title="Callisto (mythology)">
Callisto (mythology)

In Greek mythology, Callisto or Kallisto (; ) was a nymph of Lycaon. Transformed into a bear and set among the stars, she was the bear-mother of the Arcadians, through her son Arcas.
The fourth Galilean moon of Jupiter is named after Callisto.
Origin of the myth.
The name "Kalliste" (Καλλίστη), "most beautiful", may be recognized as an epithet of the goddess herself, though none of the inscriptions at Athens that record priests of "Artemis Kalliste" (Άρτεμις Καλλίστη), dates before the third century BCE. Artemis Kalliste was worshipped in Athens in a shrine which lay outside the Dipylon gate, by the side of the road to the Academy. W. S. Ferguson suggested that Artemis Soteira and Artemis Kalliste were joined in a common cult administered by a single priest. The bearlike character of Artemis herself was a feature of the Brauronia.
The myth in "Catasterismi" may be derived from the fact that a set of constellations appear close together in the sky, in and near the Zodiac sign of Libra, namely Ursa Minor, Ursa Major, Boötes, and Virgo. The constellation Boötes, was explicitly identified in the Hesiodic "Astronomia" (Αστρονομία) as Arcas, the "Bear-warden" ("Arktophylax"; Αρκτοφύλαξ):
"He is Arkas the son of Kallisto and Zeus, and he lived in the country about Lykaion. After Zeus had seduced Kallisto, Lykaon, pretending not to know of the matter, entertained Zeus, as Hesiod says, and set before him on the table the babe [Arkas] which he had cut up."
Aeschylus' tragedy "Callisto" is lost.
According to Julien d'Huy who used phylogenetical and statistical tools, the story could be a recent transformation of a Palaeolithical myth
Myth.
As a follower of Artemis, Callisto, who Hesiod said was the daughter of Lycaon, king of Arcadia, took a vow to remain a virgin, as did all the nymphs of Artemis. But to have her, Zeus disguised himself, Ovid says, as Artemis (Diana) herself, in order to lure her into his embrace. Callisto was then turned into a bear, as Hesiod had told it:
"...but afterwards, when she was already with child, was seen bathing and so discovered. Upon this, the goddess was enraged and changed her into a beast. Thus she became a bear and gave birth to a son called Arcas."
Either Artemis "slew Kallisto with a shot of her silver bow," perhaps urged by the wrath of Juno (Hera) or later Arcas, the eponym of Arcadia, nearly killed his bear-mother, when she had wandered into the forbidden precinct of Zeus. In every case, Zeus placed them both in the sky as the constellations Ursa Major, called "Arktos" (αρκτος), the "Bear", by Greeks, and Ursa Minor.
According to Ovid, it was Jupiter (Zeus) who took the form of Diana (Artemis) so that he might evade his wife Juno’s detection, forcing himself upon Callisto while she was separated from Diana and the other nymphs. Her pregnant condition was discovered some months later while bathing with Diana and her fellow nymphs. Upon this, Diana was enraged and expelled Callisto from the group, and subsequently she gave birth to Arcas. Juno then took the opportunity to avenge her wounded pride and transformed the nymph into a bear. Sixteen years later Callisto, still a bear, encountered her son Arcas hunting in the forest. Just as Arcas was about to kill his own mother with his javelin, Jupiter averted the tragedy by placing mother and son amongst the stars as Ursa Major and Minor, respectively. Juno, enraged that her attempt at revenge had been frustrated, appealed to Oceanus that the two might never meet his waters, thus providing a poetic explanation for their circumpolar positions.
The stars of Ursa Major were all circumpolar in Athens of 400 BCE, and all but the stars in the Great Bear's left foot were circumpolar in Ovid's Rome, in the first century CE. Now, however, due to the precession of the equinoxes, the feet of the Great Bear constellation do sink below the horizon from Rome and especially from Athens; however, Ursa Minor (Arcas) does remain completely above the horizon, even from latitudes as far south as Honolulu and Hong Kong.

</doc>
<doc id="7218" url="http://en.wikipedia.org/wiki?curid=7218" title="Cookie">
Cookie

In the United States and Canada a cookie is a small, flat, baked treat, usually containing flour, eggs, sugar, and either butter or cooking oil, and often including ingredients such as raisins, oats, or chocolate chips. Most other English-speaking countries would call it a biscuit.
Terminology.
In most English-speaking countries outside North America, including the United Kingdom, the most common word for this type of treat is biscuit and the term cookie is often used to describe only certain types of biscuits. However, in many regions both terms are used, such as the American-inspired Maryland Cookies, while in others the two words have different meanings. 
In North American English a biscuit is a kind of quick bread similar to a scone.
In Scotland the term cookie is sometimes used to describe a plain bun.
Etymology.
Its American name derives from the Dutch word "koekje" or more precisely its informal, dialect variant "koekie" which means "little cake," and arrived in American English through the Dutch in North America.
According to the Scottish National Dictionary, its Scottish name derives from the diminutive form (+ suffix "-ie") of the word "cook", giving the Middle Scots "cookie", "cooky" or "cu(c)kie". It also gives an alternative etymology, from the Dutch word "koekje", the diminutive of "koek", a cake. There was much trade and cultural contact across the North Sea between the Low Countries and Scotland during the Middle Ages, which can also be seen in the history of curling and, perhaps, golf.
Description.
Cookies are most commonly baked until crisp or just long enough that they remain soft, but some kinds of cookies are not baked at all. Cookies are made in a wide variety of styles, using an array of ingredients including sugars, spices, chocolate, butter, peanut butter, nuts, or dried fruits. The softness of the cookie may depend on how long it is baked.
A general theory of cookies may be formulated this way. Despite its descent from cakes and other sweetened breads, the cookie in almost all its forms has abandoned water as a medium for cohesion. Water in cakes serves to make the base (in the case of cakes called "batter") as thin as possible, which allows the bubbles – responsible for a cake's fluffiness – to better form. In the cookie, the agent of cohesion has become some form of oil. Oils, whether they be in the form of butter, egg yolks, vegetable oils, or lard, are much more viscous than water and evaporate freely at a much higher temperature than water. Thus a cake made with butter or eggs instead of water is far denser after removal from the oven.
Oils in baked cakes do not behave as soda tends to in the finished result. Rather than evaporating and thickening the mixture, they remain, saturating the bubbles of escaped gases from what little water there might have been in the eggs, if added, and the carbon dioxide released by heating the baking powder. This saturation produces the most texturally attractive feature of the cookie, and indeed all fried foods: crispness saturated with a moisture (namely oil) that does not sink into it.
History.
Cookie-like hard wafers have existed for as long as baking is documented, in part because they deal with travel very well, but they were usually not sweet enough to be considered cookies by modern standards.
Cookies appear to have their origins in 7th century AD Persia, shortly after the use of sugar became relatively common in the region. They spread to Europe through the Muslim conquest of Spain. By the 14th century, they were common in all levels of society throughout Europe, from royal cuisine to street vendors.
With global travel becoming widespread at that time, cookies made a natural travel companion, a modernized equivalent of the travel cakes used throughout history. One of the most popular early cookies, which traveled especially well and became known on every continent by similar names, was the jumble, a relatively hard cookie made largely from nuts, sweetener, and water.
Cookies came to America in the early English settlement (the 17th century), although the name "koekje" arrived with the Dutch. This became Anglicized to "cookie" or cooky. Among the popular early American cookies were the macaroon, gingerbread cookies, and of course jumbles of various types.
The most common modern cookie, given its style by the creaming of butter and sugar, was not common until the 18th century.
Classification of cookies.
Cookies are broadly classified according to how they are formed, including at least these categories:
Cookies also may be decorated with an icing, especially chocolate, and closely resemble a type of confectionery.
Biscuits (cookies) in the United Kingdom.
A basic biscuit (cookie) recipe includes flour, shortening (often lard), baking powder or soda, milk (buttermilk or sweet milk), and sugar. Common savory variations involve substituting sugar with an ingredient such as cheese or other dairy products. Shortbread is a popular biscuit in the United Kingdom.

</doc>
<doc id="7220" url="http://en.wikipedia.org/wiki?curid=7220" title="Common Gateway Interface">
Common Gateway Interface

Common Gateway Interface (CGI) is a standard method used to generate dynamic content on Web pages and Web applications. CGI, when implemented on a Web server, provides an interface between the Web server and programs that generate the Web content. These programs are known as "CGI scripts" or simply "CGIs"; they are usually written in a scripting language, but can be written in any programming language.
History.
In 1993, The NCSA team wrote the specification for calling command line executables on the www-talk mailing list. ; however, NCSA no longer hosts this. The other Web server developers adopted it, and it has been a standard for Web servers ever since. A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined. This work resulted in RFC 3875, which specified CGI Version 1.1. Specifically mentioned in the RFC are the following contributors:
Overview.
Each Web server runs HTTP server software, which responds to requests from Web browsers. Generally the HTTP server has a directory (folder), which is designated as a document collection — files that can be sent to Web browsers connected to this server. For example, if the Web server has the domain name codice_1, and its document collection is stored at codice_2 in the local file system, then the Web server will respond to a request for codice_3 by sending out the (pre-written) file at codice_4.
CGI extends this system by allowing the owner of the Web server to designate a directory within the document collection as containing executable scripts (or binary files) instead of pre-written pages; this is known as a CGI directory. For example, codice_5 could be designated as a CGI directory on the Web server. If a Web browser requests a URL that points to a file within the CGI directory (e.g., codice_6), then, instead of simply sending that file (codice_7) to the Web browser, the HTTP server runs the specified script and passes the output of the script to the Web browser. That is, anything that the script sends to standard output is passed to the Web client instead of being shown on-screen in a terminal window.
The CGI system also allows the Web browser to send information to the script via the URL or an HTTP POST request. If a slash and additional directory name(s) are appended to the URL immediately after the name of the script, then that path is stored in the codice_8 environment variable before the script is called. If parameters are passed to the script via an HTTP POST or GET request (e.g., a question mark appended to the URL, followed by param=value pairs), then those parameters are stored in the codice_9 environment variable before the script is called. The script can then read these environment variables and adapt to the Web browser's request.
Syntax.
The following CGI program shows all the environment variables passed by the Web server:
If a Web browser issues a request for the environment variables at codice_10, a 64-bit Microsoft Windows web server running cygwin returns the following information:
From the environment, it can be seen that the Web browser is Firefox running on a Windows 7 PC, the Web server is Apache running on a system that emulates Unix, and the CGI script is named codice_11.
The program could then generate any content, write that to standard output, and the Web server will transmit it to the browser.
The following are environment variables passed to CGI programs:
The program returns the result to the Web server in the form of standard output, beginning with a header and a blank line.
The header is encoded in the same way as an HTTP header and must include the MIME type of the document returned. The headers, supplemented by the Web server, are generally forwarded with the response back to the user.
Deployment.
A Web server that supports CGI can be configured to interpret a URL that it serves as a reference to a CGI script. A common convention is to have a codice_36 directory at the base of the directory tree and treat all executable files within this directory (and no other, for security) as CGI scripts. Another popular convention is to use filename extensions; for instance, if CGI scripts are consistently given the extension codice_37, the web server can be configured to interpret all such files as CGI scripts. While convenient, and required by many prepackaged scripts,it opens the server to attack if a remote user can upload executable code with the proper extension.
In the case of HTTP PUT or POSTs, the user-submitted data are provided to the program via the standard input. The Web server creates a subset of the environment variables passed to it and adds details pertinent to the HTTP environment.
Uses.
An example of a CGI program is one implementing a Wiki. The user agent requests the name of an entry; the Web server executes the CGI; the CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result. The web server receives the input from the CGI and transmits it to the user agent. If the "Edit this page" link is clicked, the CGI populates an HTML codice_38 or other editing control with the page's contents, and saves it back to the server when the user submits the form.
Alternatives.
Calling a command generally means the invocation of a newly created process on the server. Starting the process can consume much more time and memory than the actual work of generating the output, especially when the program still needs to be interpreted or compiled.
If the command is called often, the resulting workload can quickly overwhelm the server.
The overhead involved in interpretation may be reduced by using compiled CGI programs, such as those in C/C++, rather than using Perl or other interpreted languages. The overhead involved in process creation can be reduced by techniques such as FastCGI that "prefork" interpreter processes, or by running the application code entirely within the server, using extension modules such as mod_php.
Several approaches can be adopted for remedying this:
The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these tradeoffs need to be analyzed to determine the best implementation for a given task and time budget.

</doc>
<doc id="7222" url="http://en.wikipedia.org/wiki?curid=7222" title="Choctaw">
Choctaw

The Choctaw (alternatively spelled Chahta, Chactas, Tchakta, Chocktaw, and Chactaw) are Native American people originally from the Southeastern United States (modern-day Mississippi, Florida, Alabama, and Louisiana). The Choctaw language belongs to the Muskogean linguistic group. 
The Choctaw are descendants of the peoples of the Hopewell and Mississippian cultures, who lived throughout the east of the Mississippi River valley and its tributaries. About 1,700 years ago, the Hopewell people built Nanih Waiya, a great earthwork mound, which is still considered sacred by the Choctaw. The early Spanish explorers of the mid-16th century encountered Mississippian-culture villages and chiefs. The anthropologist John Swanton suggested that the Choctaw derived their name from an early leader. Henry Halbert, a historian, suggests that their name is derived from the Choctaw phrase "Hacha hatak" (river people).
The Choctaw coalesced as a people in the 17th century, and developed three distinct political and geographical divisions: eastern, western and southern, which sometimes created differing alliances with nearby European powers. These included the French, based on the Gulf Coast and in Louisiana, the English of the Southeast, and the Spanish of Florida and Louisiana during the colonial era. During the American Revolution, most Choctaw supported the Thirteen Colonies' bid for independence from the British Crown. They never went to war against the United States prior to Indian Removal.
In the 19th century, the Choctaw became known as one of the "Five Civilized Tribes" because they adopted numerous practices of their United States neighbors. The Choctaw and the United States (US) agreed to nine treaties and, by the last three, the US gained vast land cessions and deracinated most Choctaw west of the Mississippi River to Indian Territory. They were the first Native Americans forced under the Indian Removal Act. The Choctaw were exiled because the U.S. wanted to expand territory available for settlement by European Americans, to save the tribe from extinction, and to acquire their natural resources. The Choctaw negotiated the largest area and most desirable lands in Indian Territory. Their early government had three districts, each with its own chief, who together with the town chiefs sat on the National Council. They appointed a Choctaw Delegate to represent them with the US government in Washington, DC.
By the 1831 Treaty of Dancing Rabbit Creek, those Choctaw who chose to stay in the newly formed state of Mississippi were one of the first major non-European ethnic groups to become U.S. citizens. (Article 8 in the 1817 treaty with the Cherokee stated Cherokees may wish to become citizen of the United States.) During the American Civil War, the Choctaw in both Oklahoma and Mississippi mostly sided with the Confederate States of America. The Confederacy suggested it would support a state under Indian control if it won the war. In a new treaty after the war, the US required them to emancipate their slaves and offer them full citizenship; they have become known as Choctaw Freedmen. After the Civil War, the Mississippi Choctaw fell into obscurity for some time.
The Choctaw in Oklahoma struggled to build a nation, transferring the Choctaw Academy there and opening one for girls in the 1840s. In the aftermath of the Dawes Act, the US dissolved tribal governments and appointed chiefs. During World War I, Choctaw soldiers served in the U.S. military as the first Native American codetalkers, using the Choctaw language. After the Indian Reorganization Act of 1934, the Choctaw reconstituted their government, and the Choctaw Nation had kept their culture alive despite years of pressure for assimilation. The third largest federally recognized tribe, since the mid-twentieth century, they have created new institutions, such as a tribal college, housing authority, and justice system. Today the Choctaw Nation of Oklahoma and the Mississippi Band of Choctaw Indians are the two federally recognized Choctaw tribes; Mississippi recognizes another band, and smaller Choctaw groups are located in Alabama (MOWA), Louisiana (Jena), and Texas.
History.
Paleo-Indian period.
Many thousands of years ago groups classified by anthropologists as Paleo-Indians lived in what today is referred to as the American South. These groups were hunter-gatherers who hunted a wide range of animals, including a variety of megafauna, which became extinct following the end of the Pleistocene age. The 19th-century historian Horatio Cushman noted that Choctaw oral history accounts suggested their ancestors had known of mammoths in the Tombigbee River area; this suggests that the Choctaw ancestors had been in the Mississippi area for at least 4,000–8,000 years. Cushman wrote: "the ancient Choctaw through their tradition (said) 'they saw the mighty beasts of the forests, whose tread shook the earth." Scholars believe that Paleo-Indians were specialized, highly mobile foragers who hunted late Pleistocene fauna such as bison, mastodons, caribou, and mammoths. Direct evidence in the Southeast is meager, but archaeological discoveries in related areas support this hypothesis.
Woodland culture.
Later cultures became more complex. Moundbuilding cultures included the Woodland period people who first built Nanih Waiya. Scholars believe the mound was contemporary with such earthworks as Igomar Mound in Mississippi and Pinson Mounds in Tennessee. Based on dating of surface artifacts, the Nanih Waiya mound was likely constructed and first occupied by indigenous peoples about 0-300 CE, in the Middle Woodland period.
The original site was bounded on three sides by an earthwork circular enclosure, about ten feet high and encompassing a square mile. Occupation of Nanih Waiya and several smaller nearby mounds likely continued through 700 CE, the Late Woodland Period. The smaller mounds may also have been built by later cultures. As they have been lost to cultivation since the late 19th century and the area has not been excavated, theories have been speculation.
Mississippian culture.
The Mississippian culture was a Native American culture that flourished in what is now the Midwestern, Eastern, and Southeastern United States from 800 to 1500 CE. The Mississippian culture developed in the lower Mississippi river valley and its tributaries, including the Ohio River. In present-day Mississippi, Moundville, Plaquemine,
When the Spanish made their first forays inland in the 16th century from the shores of the Gulf of Mexico, they encountered some chiefdoms of the Mississippians, but others were already in decline, or had disappeared. The Mississippian culture are the peoples encountered by other early Spanish explorers, beginning on April 2, 1513, with Juan Ponce de León's Florida landing and the 1526 Lucas Vázquez de Ayllón expedition in South Carolina and Georgia region. A Spanish expedition in the later 16th century, in what is now western North Carolina, encountered people of the Mississippian culture at Joara and settlements further west. The Spanish built a fort at Joara and left a garrison there, as well as five other forts. The following year all the Spanish garrisons were killed and the forts destroyed by the Native Americans, who ended Spanish colonization attempts in the interior.
17th century emergence of Choctaw.
The contemporary historian Patricia Galloway argues from fragmentary archaeological and cartographic evidence that the Choctaw did not exist as a unified people before the 17th century. Only then did various southeastern peoples, remnants of Moundville, Plaquemine, and other Mississippian cultures, coalesce to form a self-consciously Choctaw people. The historical homeland of the Choctaw, or of the peoples from whom the Choctaw nation arose, included the area of "Nanih Waiya", an earthwork mound in present-day Winston County, Mississippi, which they considered sacred ground. Their homeland was bounded by the Tombigbee River to the east, the Pearl River on the north and west, and "the Leaf-Pascagoula system" to the South. This area was mostly uninhabited during the Mississippian -culture period.
While Nanih Waiya mound continued to be a ceremonial center and object of veneration, scholars believe Native Americans traveled to it during the Mississippian culture period. From the 17th century on, the Choctaw occupied this area and revered this site as the center of their origin stories. These included stories of migration to this site from west of the great river (believed to refer to the Mississippi River.)
In "Histoire de La Louisiane" (Paris, 1758), French explorer Antoine-Simon Le Page du Pratz recounted that "...when I asked them from whence the Chat-kas came, to express the suddenness of their appearance they replied that they had come out from under the earth." American scholars took this as intended to explain the Choctaws' immediate appearance, and not a literal creation account. It was perhaps the first European writing that included part of the Choctaw origin story.
Early 19th century and contemporary Choctaw storytellers describe that the Choctaw people emerged from either Nanih Waiya mound or cave. A companion story describes their migration journey from the west, beyond the Mississippi River, when they were directed by their leader's use of a sacred red pole.
Contact era.
After the castaway Cabeza de Vaca of the ill-fated Narváez expedition returned to Spain, he described to the Court that the New World was the "richest country in the world." It commissioned the Spaniard Hernando de Soto to lead the first expedition into the interior of the North American continent. De Soto, convinced of the "riches", wanted Cabeza de Vaca to accompany him on the expedition. Cabeza de Vaca declined because of a payment dispute. From 1540–1543, Hernando de Soto traveled through present-day Florida and Georgia, and then into the Alabama and Mississippi areas that would later be inhabited by the Choctaw.
De Soto had the best-equipped militia at the time. As the brutalities of the de Soto expedition through the Southeast became known, ancestors to the Choctaw rose in defense. The Battle of Mabila, an ambush arranged by Chief Tuskaloosa, was a turning point for the de Soto venture. The battle "broke the back" of the campaign, and they never fully recovered.
The archaeological record for the period between 1567 and 1699 is not complete or well-studied. It appears that some Mississippian settlements were abandoned well before the 17th century. Similarities in pottery coloring and burials suggest the following scenario for the emergence of the distinctive Choctaw society.
According to Patricia Galloway, the Choctaw region of Mississippi, generally located between the Yazoo basin to the north and the Natchez bluffs to the south, was slowly occupied by Burial Urn people from the Bottle Creek Indian Mounds area in the Mobile, Alabama delta, along with remnants of people from the Moundville chiefdom (near present-day Tuscaloosa, Alabama), which had collapsed some years before. Facing severe depopulation, they fled westward, where they combined with the Plaquemines and a group of “prairie people” living near the area. When this occurred is not clear. In the space of several generations, they created a new society which became known as Choctaw (albeit with a strong Mississippian background).
Other scholars note the Choctaw oral history recounting their long migration from west of the Mississippi River.
French colonization (1682).
In 1682 La Salle was the first French explorer to venture into the southeast along the Mississippi River. His expedition did not meet with the Choctaw; it established a post along the Arkansas River. The post signaled to the English that the French were serious at colonization in the South. The Choctaw allied with French colonists as a defense against the English, who had been taking Choctaws as captives for the Indian slave trade.
The first direct recorded contact between the Choctaw and the French was with Pierre Le Moyne d'Iberville in 1699; indirect contact had likely occurred between the Choctaw and British settlers through other tribes, including the Creek and Chickasaw. The Choctaw, along with other tribes, had formed a relationship with New France, French Louisiana. Illegal fur trading may have led to further unofficial contact. 
As the historian Greg O'Brien has noted, the Choctaw developed three distinct political and geographic regions, which during the colonial period sometimes had differing alliances with trading partners among the French, Spanish and English. They also expressed differences during and after the American Revolutionary War. Their divisions were roughly eastern, western (near present-day Vicksburg, Mississippi) and southern (Six Towns). Each division was headed by a principal chief, and subordinate chiefs led each of the towns within the area. All the chiefs would meet on a National Council, but the society was highly decentralized for some time.
The French were the main trading partners of the Choctaw before the Seven Years' War, and the British had established some trading. After Great Britain defeated France, it ceded its territory east of the Mississippi River. From 1763 to 1781, Britain was the Choctaw main trading partner. With Spanish forces based in New Orleans in 1766, when they took over French territory west of the Mississippi, the Choctaw sometimes traded with them to the west. Spain declared war against Great Britain during the American Revolution in 1779.
United States relations.
During the American Revolution, the Choctaw divided over whether to support Britain or Spain. Some Choctaw warriors from the western and eastern divisions supported the British in the defense of Mobile and Pensacola. Chief Franchimastabé led a Choctaw war party with British forces against American rebels in Natchez. The Americans had left by the time Franchimastabé arrived, but the Choctaw occupied Natchez for weeks and convinced residents to remain loyal to Britain.
Other Choctaw companies joined Washington's army during the war, and served the entire duration. Bob Ferguson, a Southeastern Indian historian, noted, "[In] 1775 the American Revolution began a period of new alignments for the Choctaws and other southern Indians. Choctaw scouts served under Washington, Morgan, Wayne and Sullivan."
Over a thousand Choctaw fought for Britain, largely against Spain's campaigns along the Gulf Coast. At the same time, a significant number of Choctaw aided Spain.
Ferguson wrote that with the end of the Revolution, "'Franchimastabe', Choctaw head chief, went to Savannah, Georgia to secure American trade." In the next few years, some Choctaw scouts served in Ohio with U.S. General Anthony Wayne in the Northwest Indian War.
George Washington (first U.S. President) and Henry Knox (first U.S. Secretary of War) proposed the cultural transformation of Native Americans. Washington believed that Native Americans were equals but that their society was inferior to that of the European Americans. He formulated a policy to encourage the "civilizing" process, and Thomas Jefferson continued it. The historian Robert Remini wrote, "[T]hey presumed that once the Indians adopted the practice of private property, built homes, farmed, educated their children, and embraced Christianity, these Native Americans would win acceptance from white Americans."
Washington's six-point plan included impartial justice toward Indians; regulated buying of Indian lands; promotion of commerce; promotion of experiments to civilize or improve Indian society; presidential authority to give presents; and punishing those who violated Indian rights. The government appointed agents, such as Benjamin Hawkins, to live among the Indians and to teach them through example and instruction, how to live like whites. While living among the Choctaw for nearly 30 years, Hawkins married Lavinia Downs, a Choctaw woman. As the people had a matrilineal system of property and hereditary leadership, their children were born into the mother's clan and gained their status from her people. In the late eighteenth and early nineteenth century, a number of Scots-Irish traders lived among the Choctaw and married high-status women. Choctaw chiefs saw these as strategic alliances to build stronger relationships with the Americans in a changing environment that influenced ideas of capital and property. The children of such marriages were Choctaw, first and foremost. Some of the sons were educated in Anglo-American schools and became important interpreters and negotiators for Choctaw-US relations.
Hopewell council and treaty (1786).
Starting in October 1785, "Taboca", a Choctaw prophet/chief, led over 125 Choctaws to the Keowee, near Seneca Old Town, now known as Hopewell, South Carolina. After two months of travel, they met with U.S. representatives Benjamin Hawkins, Andrew Pickens, and Joseph Martin. In high Choctaw ceremonial symbolism, they named, adopted, smoked, and performed dances, revealing the complex and serious nature of Choctaw diplomacy. One such dance was the eagle tail dance. The Choctaw explained that the Bald Eagle, who has direct contact with the upper world of the sun, is a symbol of peace. Choctaw women painted in white would adopt and name commissioners as kin. Smoking sealed agreements between peoples and the shared pipes sanctified peace between the two nations.
After the rituals, the Choctaw asked John Woods to live with them to improve communication with the U.S. In exchange they allowed Taboca to visit the United States Congress. On January 3, 1786, the Treaty of Hopewell was signed. Article 11 stated, "[T]he hatchet shall be forever buried, and the peace given by the United States of America, and friendship re-established between the said states on the one part, and all the Choctaw nation on the other part, shall be universal; and the contracting parties shall use their utmost endeavors to maintain the peace given as aforesaid, and friendship re-established."
The treaty required Choctaws to return escaped slaves to colonists, to turn over any Choctaw convicted of crimes by the U.S., establish borderlines between the U.S. and Choctaw Nation, and the return any property captured from colonists during the Revolutionary War.
After the Revolutionary War, the Choctaw were reluctant to ally themselves with countries hostile to the United States. John R. Swanton wrote, "the Choctaw were never at war with the Americans. A few were induced by "Tecumseh" (a Shawnee leader who sought support from various Native American tribes) to ally themselves with the hostile Creeks [in the early 19th century], but the Nation as a whole was kept out of anti-American alliances by the influence of "Apushmataha", greatest of all Choctaw chiefs."
War of 1812.
Early in 1811, the Shawnee leader "Tecumseh" gathered Indian tribes in an alliance to try to expel U.S. settlers from the Northwest area south of the Great Lakes. Tecumseh met the Choctaws to persuade them to join the alliance. "Pushmataha", considered by historians to be the greatest Choctaw leader, countered Tecumseh's influence. As chief for the Six Towns (southern) district, Pushmataha strongly resisted such a plan, arguing that the Choctaw and their neighbors the Chickasaw had always lived in peace with European Americans, had learned valuable skills and technologies, and had received honest treatment and fair trade. The joint Choctaw-Chickasaw council voted against alliance with Tecumseh. On Tecumseh's departure, Pushmataha accused him of tyranny over his own Shawnee and other tribes. Pushmataha warned Tecumeseh that he would fight against those who fought the United States.
On the eve of the War of 1812, Governor William C. C. Claiborne of Louisiana sent interpreter Simon Favre to give a talk to the Choctaws, urging them to stay out of this "white man's war." Ultimately, however, the Choctaw did become involved, and with the outbreak of the war, Pushmataha led the Choctaws in alliance with the U.S., arguing in favor of opposing the Creek Red Sticks' alliance with Britain after the massacre at Fort Mims. Pushmataha arrived at St. Stephens, Alabama in mid-1813 with an offer of alliance and recruitment. He was escorted to Mobile to speak with General Flournoy, then commanding the district. Flournoy initially declined Pushmataha's offer and offended the chief. However, Flournoy's staff quickly convinced him to reverse his decision. A courier with a message accepting the offer of alliance caught up with Pushmataha at St. Stephens.
Returning to Choctaw territory, Pushmataha raised a company of 125 Choctaw warriors with a rousing speech and was commissioned (as either a lieutenant colonel or a brigadier general) in the United States Army at St. Stephens. After observing that the officers and their wives would promenade along the Alabama River, Pushmataha summoned his own wife to St. Stephens to accompany him.
He joined the U.S. Army under General Ferdinand Claiborne in mid-November, and some 125 Choctaw warriors took part in an attack on Creek forces at Kantachi (near present day Econochaca, Alabama) on 23 December 1813. With this victory, Choctaw began to volunteer in greater numbers from the other two districts of the tribe. By February 1814, a larger band of Choctaws under Pushmataha had joined General Andrew Jackson's force for the sweeping of the Creek territories near Pensacola, Florida. Many Choctaw departed from Jackson's main force after the final defeat of the Creek at the Battle of Horseshoe Bend. By the Battle of New Orleans, only a few Choctaw remained with the army; they were the only Native American tribe represented in the battle.
Doak's Stand (1820).
In October 1820, Andrew Jackson and Thomas Hinds were sent as commissioners representing the United States, to conduct a treaty that would require the Choctaw to surrender to the United States a portion of their country located in present day Mississippi. They met with chiefs, mingos (leaders), and headsmen such as Colonel Silas Dinsmore and Chief Pushmataha at Doak's Stand on the Natchez Trace.
The convention began on October 10 with a talk by "Sharp Knife", the nickname of Jackson, to more than 500 Choctaws. Pushmataha accused Jackson of deceiving them about the quality of land west of the Mississippi. Pushmataha responded to Jackson's retort with "I know the country well ... The grass is everywhere very short ... There are but few beavers, and the honey and fruit are rare things." Jackson resorted to threats, which pressured the Choctaws to sign the Doak's Stand treaty. Pushmataha would continue to argue with Jackson about the conditions of the treaty. Pushmataha assertively stated "that no alteration shall be made in the boundaries of the portion of our territory that will remain, until the Choctaw people are sufficiently progressed in the arts of civilization to become citizens of the States, owning land and homes of their own, on an equal footing with the white people." Jackson responded with "That ... is a magnificent rangement and we consent to it, [American Citizenship], readily." Historian Anna Lewis stated that "Apuckshunubbee", a Choctaw district chief, was blackmailed by Jackson to sign the treaty. On October 18, the Treaty of Doak's Stand was signed.
Article 4 of the Treaty of Doak's Stand prepared Choctaws to become U.S. citizens when he or she became "civilized." This article would later influence Article 14 in the Treaty of Dancing Rabbit Creek.
Negotiations with the US government (1820s).
Apuckshunubbee, Pushmataha, and Mosholatubbee, the principal chiefs of the three divisions of Choctaw, led a delegation to Washington City (the 19th century name for Washington, D.C.) to discuss the problems of European Americans' squatting on Choctaw lands. They sought either expulsion of the settlers or financial compensation for the loss of their lands. The group also included Talking Warrior, Red Fort, Nittahkachee, who was later Principal Chief; Col. Robert Cole and David Folsom, both Choctaw of mixed-race ancestry; Captain Daniel McCurtain, and Major John Pitchlynn, the U.S. interpreter, who had been raised by the Choctaw after having been orphaned when young and married a Choctaw woman. Apuckshunubbee died in Maysville, Kentucky of an accident during the trip before the party reached Washington.
Pushmataha met with President James Monroe and gave a speech to Secretary of War John C. Calhoun, reminding him of the longstanding alliances between the United States and the Choctaws. He said, "[I] can say and tell the truth that no Choctaw ever drew his bow against the United States ... My nation has given of their country until it is very small. We are in trouble." On January 20, 1825, Pushmataha and other chiefs signed the Treaty of Washington City, by which the Choctaw ceded more territory to the United States.
Pushmataha died in Washington of a respiratory disease described as croup, before the delegation returned to the Choctaw Nation. He was given full U.S. military burial honors at the Congressional Cemetery in Washington, D.C.
The deaths of these two strong division leaders was a major loss to the Choctaw Nation, but younger leaders were arising who were educated in European-American schools and led adaptation of the culture. Threatened with European-American encroachment, the Choctaw continued to adapt and take on some technology, housing styles, and accepted missionaries to the Choctaw Nation, in the hopes of being accepted by the Mississippi and national government. In 1825 the National Council approved the founding of the Choctaw Academy for education of its young men, urged by Peter Pitchlynn, a young leader and future chief. The school was established in Blue Spring, Scott County, Kentucky; it was operated there until 1842, when the staff and students were transferred to the Choctaw Nation, Indian Territory. There they founded the Spencer Academy in 1844.
With the election of Andrew Jackson as president in 1828, many of the Choctaw realized that removal was inevitable. They continued to adopt useful European practices but faced Jackson's and settlers' unrelenting pressure.
1830 election and treaty.
In March 1830 the division chiefs resigned, and the National Council elected Greenwood LeFlore, chief of the western division, as Principal Chief of the nation to negotiate with the US government on their behalf, the first time such a position had been authorized. Believing removal was inevitable and hoping to preserve rights for Choctaw in Indian Territory and Mississippi, LeFlore drafted a treaty and sent it to Washington, DC. There was considerable turmoil in the Choctaw Nation among people who thought he would and could resist removal, but the chiefs had agreed they could not undertake armed resistance.
Treaty of Dancing Rabbit Creek (1830).
At Andrew Jackson's request, the United States Congress opened what became a fierce debate on an Indian Removal Bill. In the end, the bill passed, but the vote was very close. The Senate passed the measure 28 to 19, while in the House it narrowly passed, 102 to 97. Jackson signed the legislation into law June 30, 1830, and turned his focus onto the Choctaw in Mississippi Territory.
On August 25, 1830, the Choctaw were supposed to meet with Andrew Jackson in Franklin, Tennessee, but Greenwood Leflore, a district Choctaw chief, informed Secretary of War John H. Eaton that his warriors were fiercely opposed to attending. President Jackson was angered. Journalist Len Green writes "although angered by the Choctaw refusal to meet him in Tennessee, Jackson felt from LeFlore's words that he might have a foot in the door and dispatched Secretary of War Eaton and John Coffee to meet with the Choctaws in their nation." Jackson appointed Eaton and General John Coffee as commissioners to represent him to meet the Choctaws at the Dancing Rabbit Creek near present-day Noxubee County, Mississippi.
The commissioners met with the chiefs and headmen on September 15, 1830, at Dancing Rabbit Creek. In a carnival-like atmosphere, they tried to explain the policy of removal to an audience of 6,000 men, women, and children. The Choctaws faced migration or submitting to U.S. law as citizens. The treaty required them to cede their remaining traditional homeland to the United States; however, a provision in the treaty made removal more acceptable.
On September 27, 1830, the Treaty of Dancing Rabbit Creek was signed. It represented one of the largest transfers of land that was signed between the U.S. Government and Native Americans without being instigated by warfare. By the treaty, the Choctaw signed away their remaining traditional homelands, opening them up for European-American settlement. Article 14 allowed for some Choctaw to stay in Mississippi, and nearly 1,300 Choctaws chose to do so. They were one of the first major non-European ethnic group to become U.S. citizens. Article 22 sought to put a Choctaw representative in the U.S. House of Representatives. The Choctaw at this crucial time split into two distinct groups: the Choctaw Nation of Oklahoma and the Mississippi Band of Choctaw Indians. The nation retained its autonomy, but the tribe in Mississippi submitted to state and federal laws.
Removal era.
After ceding nearly , the Choctaw emigrated in three stages: the first in the fall of 1831, the second in 1832 and the last in 1833. Nearly 15,000 Choctaws made the move to what would be called Indian Territory and then later Oklahoma. About 2,500 died along the Trail of Tears. The Treaty of Dancing Rabbit Creek was ratified by the U.S. Senate on February 25, 1831, and the President was anxious to make it a model of removal. Principal Chief George W. Harkins wrote a farewell letter to the American people before the removals began. It was widely published
Alexis de Tocqueville, noted French political thinker and historian, witnessed the Choctaw removals while in Memphis, Tennessee in 1831:
Approximately 4,000–6,000 Choctaw remained in Mississippi in 1831 after the initial removal efforts. The U.S. agent William Ward, who was responsible for Choctaw registration in Mississippi under article XIV, strongly opposed their treaty rights. Although estimates suggested 5000 Choctaw remained in Mississippi, only 143 family heads (for a total of 276 adult persons) received lands under the provisions of Article 14. For the next ten years, the Choctaws in Mississippi were objects of increasing legal conflict, racism, harassment, and intimidation. The Choctaws described their situation in 1849: "we have had our habitations torn down and burned, our fences destroyed, cattle turned into our fields and we ourselves have been scourged, manacled, fettered and otherwise personally abused, until by such treatment some of our best men have died." Joseph B. Cobb, who moved to Mississippi from Georgia, described the Choctaw as having "no nobility or virtue at all, and in some respect he found blacks, especially native Africans, more interesting and admirable, the red man's superior in every way. The Choctaw and Chickasaw, the tribes he knew best, were beneath contempt, that is, even worse than black slaves." Removal continued throughout the 19th and 20th centuries. In 1846 1,000 Choctaw removed, and in 1903, another 300 Mississippi Choctaw were persuaded to move to the Nation in Oklahoma. By 1930 only 1,665 remained in Mississippi.
Pre-Civil War (1840).
In the 1840s, the Choctaw chief Greenwood LeFlore stayed in Mississippi after the signing of Treaty of Dancing Rabbit Creek and became an American citizen, a successful businessman, and a state politician. He was elected as a Mississippi representative and senator, was a fixture of Mississippi high society, and a personal friend of Jefferson Davis. He represented his county in the state house for two terms and served as a state senator for one term. Some of the elite used Latin language, an indulgence used by some politicians. LeFlore, in defense of his heritage, spoke in the Choctaw language and asked the Senate floor which was better understood, Latin or Choctaw.
Midway through the Great Irish Famine (1845–1849), a group of Choctaw collected $710 (although many articles say the original amount was $170 after a misprint in Angi Debo's "The Rise and Fall of the Choctaw Nation") and sent it to help starving Irish men, women, and children. 
"It had been just 16 years since the Choctaw people had experienced the Trail of Tears, and they had faced starvation ... It was an amazing gesture. By today's standards, it might be a million dollars" according to Judy Allen, editor of the Choctaw Nation of Oklahoma's newspaper, "Bishinik", based at the Oklahoma Choctaw tribal headquarters in Durant, Oklahoma. To mark the 150th anniversary, eight Irish people retraced the Trail of Tears. In the late 20th century, Irish President Mary Robinson extolled the donation in a public commemoration.
For the Choctaw who remained in or returned to Mississippi after 1855, the situation deteriorated. Many lost their lands and money to unscrupulous whites. The state of Mississippi refused the Choctaw any participation in government. Their limited understanding of the English language caused them to live in isolated groups. In addition, they were prohibited from attending any of the few institutions of higher learning, as the European Americans considered them free people of color and excluded from the segregated white institutions. The state had no public schools prior to those established during the Reconstruction era.
American Civil War (1861).
At the beginning of the American Civil War, Albert Pike was appointed as Confederate envoy to Native Americans. In this capacity he negotiated several treaties, including the Treaty with Choctaws and Chickasaws in July 1861. The treaty covered sixty-four terms, covering many subjects, such as Choctaw and Chickasaw nation sovereignty, Confederate States of America citizenship possibilities, and an entitled delegate in the House of Representatives of the Confederate States of America.
Some Choctaw identified with the Southern cause and a few owned slaves. In addition, they well remembered and resented the Indian removals from thirty years earlier, and the poor services they received from the federal government. The main reason the Choctaw Nation agreed to sign the treaty was for protection from regional tribes. Soon Confederate battalions were formed in Indian Territory and later in Mississippi in support of the southern cause.
The Confederacy wanted to recruit Indians east of the Mississippi River in 1862, so they opened up a recruiting camp in Mobile, Alabama. The Confederacy advertised in the "Mobile Advertiser and Register" for recruits:
J. W. Pierce and S. G. Spann organized the Choctaw Confederates in Mississippi between 1862-63. After a Confederate troop train wreck, referred to as the Chunky Creek Train Wreck of 1863, near Hickory, Mississippi, the new Choctaw Battalion led rescue and recovery efforts. Led by Jack Amos and Elder Williams, the Indians rushed to the scene, stripped, and plunged into the flooded creek. Many of the passengers were rescued due to their heroic acts." The noted Choctaw historian Clara Kidwell writes, "in an act of heroism in Mississippi, Choctaws rescued twenty-three survivors and retrieved ninety bodies when a Confederate troop train plunged off a bridge and fell into the Chunky River."
Major S. G. Spann, Commander of Dabney H. Maury Camp of Meridian, Mississippi, wrote about the deeds of the Choctaw years after the Civil War had ended. After the Union captured Mississippi Choctaws in Ponchatoula, Louisiana, they were taken to New York, where several died in a Union prison. Spann describes the incident, "[Maj. J.W. Pearce] established two camps—a recruiting camp in Newton County and a drill camp at Tangipahoa—just beyond the State boundary line in Louisiana in the fall of 1862. New Orleans at that time was in the hands of the Federal Gen. B.F. Butler. Without notice a reconnoitering party of the enemy raided the camp, and captured over two dozen Indians and several non-commissioned white officers and carried them to New Orleans. All the officers and several of the Indians escaped and returned to the Newton County camp; but all the balance of the captured Indians were carried to New York, and were daily paraded in the public parks as curiosities for the sport of sight-seers."
Choctaw Under Reconstruction (1865).
Mississippi Choctaw.
From about 1865 to 1918, Mississippi Choctaws were largely ignored by governmental, health, and educational services and fell into obscurity. In the aftermath of the Civil War, their issues were pushed aside in the struggle between defeated Confederates, freedmen and Union sympathizers. Records about the Mississippi Choctaw during this period are non-existent. They had no legal recourse, and were often bullied and intimidated by local whites, who tried to re-establish white supremacy. They chose to live in isolation and practiced their culture as they had for generations.
Following the Reconstruction era and conservative Democrats' regaining political power in the late 1870s, white state legislators passed laws establishing Jim Crow laws and legal segregation by race. In addition, they effectively disfranchised freedmen and Native Americans by the new Mississippi constitution of 1890, which changed rules regarding voter registration and elections to discriminate against both groups. The white legislators effectively divided society into two groups: white and "colored," into which they classified Mississippi Choctaw and other Native Americans. They subjected the Choctaw to racial segregation and exclusion from public facilities along with freedmen and their descendants. The Choctaw were non-white, landless, and had minimal legal protection.
At the turn of the 20th century, only 1,253 Choctaw Indians remained in Mississippi. "The beginning of the 20th century found Mississippi Choctaws struggling to overcome poverty, discrimination, and lack of opportunity." 
After a US Congressional investigation discovered their poor living conditions, in 1918 the US Bureau of Indian Affairs (BIA) established the Choctaw Agency. Under segregation, few schools were open to Choctaw children, whom the white southerners classified as non-whites. The Choctaw agency was based in Philadelphia, Mississippi, a center of several Indian communities. It set up elementary schools and worked to address the poor health conditions of the Choctaw, building a hospital in Philadelphia for tribal members. Dr. Frank McKinley was the first superintendent. Prior to McKinley's arrival, the Choctaw had grouped themselves in six communities.
Because the state remained dependent on agriculture, despite the declining price of cotton, most landless men earned a living by becoming sharecroppers. The women created and sold traditional hand-woven baskets. Choctaw sharecropping declined following World War II as major planters had adopted mechanization, which reduced the need for labor.
Choctaw Nation.
The Confederacy’s loss was also the Choctaw Nation’s loss. Prior to removal, the Choctaws had interacted with Africans in their native homeland of Mississippi, and the wealthiest had bought slaves. The Choctaw who developed larger plantations adopted chattel slavery, as practiced by European Americans, to gain sufficient labor. During the antebellum period, enslaved African Americans had more formal legal protection under United States law than did the Choctaw. Moshulatubbee, the chief of the western region, held slaves, as did many of the Europeans who married into the Choctaw nation. The Choctaw took slaves with them to Indian Territory during removal, and descendants purchased others there. They kept slavery until 1866. After the Civil War, they were required by treaty with the United States to emancipate the slaves within their Nation and, for those who chose to stay, offer them full citizenship and rights. Former slaves of the Choctaw Nation were called the Choctaw Freedmen. After considerable debate, the Choctaw Nation granted Choctaw Freedmen citizenship in 1885. In post-war treaties, the US government also acquired land in the western part of the territory and access rights for railroads to be built across Indian Territory. Choctaw chief, Allen Wright, suggested "Oklahoma" (red man, a "portmanteau" of the Choctaw words "okla" "man" and "humma" "red") as the name of a territory created from Indian Territory in 1890.
The improved transportation afforded by the railroads increased the pressure on the Choctaw Nation. It drew large-scale mining and timber operations, which added to tribal receipts. But, the railroads and industries also attracted European-American settlers, including new immigrants to the United States.
With the goal of assimilating the Native Americans, the Curtis Act of 1898, sponsored by a Native American who believed that was the way for his people to do better, ended tribal governments. In addition, it proposed the end of communal, tribal lands. Continuing the struggle over land and assimilation, the US proposed the end to the tribal lands held in common, and allotment of lands to tribal members in severalty (individually). The US declared land in excess of the registered households needs to be "surplus" to the tribe, and took it for sale to new European-American settlers. In addition, individual ownership meant that Native Americans could sell their individual plots. This would also enable new settlers to buy land from those Native Americans who wished to sell. The US government set up the Dawes Commission to manage the land allotment policy; it registered members of the tribe and made allocations of lands.
Beginning in 1894, the Dawes Commission was established to register Choctaw and other families of the Indian Territory, so that the former tribal lands could be properly distributed among them. The final list included 18,981 citizens of the Choctaw Nation, 1,639 Mississippi Choctaw, and 5,994 former slaves (and descendants of former slaves), most held by Choctaws in the Indian/Oklahoma Territory. (At the same time, the Dawes Commission registered members of the other Five Civilized Tribes for the same purpose. The Dawes Rolls have become important records for proving tribal membership.) Following completion of the land allotments, the US proposed to end tribal governments of the Five Civilized Tribes and admit the two territories jointly as a state.
Territory transition to Oklahoma statehood (1889).
The establishment of Oklahoma Territory following the Civil War was a required land cession by the Five Civilized Tribes, who had supported the Confederacy. The government used its railroad access to the Oklahoma Territory to stimulate development there. The Indian Appropriations Bill of 1889 included an amendment by Illinois Representative William McKendree Springer, that authorized President Benjamin Harrison to open the two million acres (8,000 km²) of Oklahoma Territory for settlement, resulting in the Land Run of 1889. The Choctaw Nation was overwhelmed with new settlers and could not regulate their activities. In the late 19th century, Choctaws suffered almost daily from violent crimes, murders, thefts and assaults from whites and from other Choctaws. Intense factionalism divided the traditionalistic "Nationalists" and pro-assimilation "Progressives," who fought for control.
In 1905, delegates of the Five Civilized Tribes met at the Sequoyah Convention to write a constitution for an Indian-controlled state. They wanted to have Indian Territory admitted as the State of Sequoyah. Although they took a thoroughly developed proposal to Washington, DC, seeking approval, eastern states' representatives opposed it, not wanting to have two western states created in the area, as the Republicans feared that both would be Democrat-dominated, as the territories had a southern tradition of settlement. President Theodore Roosevelt, a Republican, ruled that the Oklahoma and Indian territories had to be jointly admitted as one state, Oklahoma. To achieve this, tribal governments had to end and all residents accept state government. Many of the leading Native American representatives from the Sequoyah Convention participated in the new state convention. Its constitution was based on many elements of the one developed for the State of Sequoyah.
In 1906 the U.S. dissolved the governments of the Five Civilized Tribes. This action was part of continuing negotiations by Native Americans and European Americans over the best proposals for the future. The Choctaw Nation continued to protect resources not stipulated in treaty or law. On November 16, 1907, Oklahoma was admitted to the union as the 46th state.
World War I (1918).
In the closing days of World War I, a group of Choctaws serving in the U.S. Army used their native language as the basis for secret communication among Americans, as Germans could not understand it. They are now called the Choctaw Code Talkers. The Choctaws were the Native American innovators who served as code talkers. Captain Lawrence, a company commander, overheard Solomon Louis and Mitchell Bobb conversing in the Choctaw language. He learned there were eight Choctaw men in the battalion.
Fourteen Choctaw Indian men in the Army's 36th Division trained to use their language for military communications. Their communications, which could not be understood by Germans, helped the American Expeditionary Force win several key battles in the Meuse-Argonne Campaign in France, during the last big German offensive of the war. Within 24 hours after the US Army starting using the Choctaw speakers, they turned the tide of battle by controlling their communications. In less than 72 hours, the Germans were retreating and the Allies were on full attack. The 14 Choctaw Code Talkers were Albert Billy, Mitchell Bobb, Victor Brown, Ben Caterby, James Edwards, Tobias Frazer, Ben Hampton, Solomon Louis, Pete Maytubby, Jeff Nelson, Joseph Oklahombi, Robert Taylor, Calvin Wilson, and Captain Walter Veach.
More than 70 years passed before the contributions of the Choctaw Code talkers were fully recognized. On November 3, 1989, in recognition of the important role the Choctaw Code Talkers played during World War I, the French government presented the "Chevalier de L'Ordre National du Mérite" (the Knight of the National Order of Merit) to the Choctaws Code Talkers. 
The US Army again used Choctaw speakers for coded language during World War II.
Reorganization (1934).
During the Great Depression and the Roosevelt Administration, officials began numerous initiatives to alleviate some of the social and economic conditions in the South. The 1933 "Special Narrative Report" described the dismal state of welfare of Mississippi Choctaws, whose population by 1930 had declined to 1,665 people. John Collier, the US Commissioner for Indian Affairs (now BIA), had worked for a decade on Indian affairs and been developing ideas to change federal policy. He used the report as instrumental support to re-organize the Mississippi Choctaw as the Mississippi Band of Choctaw Indians. This enabled them to establish their own tribal government, and gain a beneficial relationship with the federal government.
In 1934, President Franklin Roosevelt signed into law the Indian Reorganization Act. This law proved critical for survival of the Mississippi Choctaw. Baxter York, Emmett York, and Joe Chitto worked on gaining recognition for the Choctaw. They realized that the only way to gain recognition was to adopt a constitution. A rival organization, the Mississippi Choctaw Indian Federation, opposed tribal recognition because of fears of dominance by the Bureau of Indian Affairs (BIA). They disbanded after leaders of the opposition were moved to another jurisdiction. The first Mississippi Band of Choctaw Indians tribal council members were Baxter and Emmett York with Joe Chitto as the first chairperson.
With the tribe's adoption of government, in 1944 the Secretary of the Interior declared that would be held in trust for the Choctaw of Mississippi. Lands in Neshoba and surrounding counties were set aside as a federal Indian reservation. Eight communities were included in the reservation land: Bogue Chitto, Bogue Homa, Conehatta, Crystal Ridge, Pearl River, Red Water, Tucker, and Standing Pine.
Under the Indian Reorganization Act, the Mississippi Choctaws re-organized on April 20, 1945 as the Mississippi Band of Choctaw Indians. This gave them some independence from the Democrat-dominated state government, which continued with enforcement of racial segregation and discrimination.
World War II (1941).
World War II was a significant turning point for Choctaws and Native Americans in general. Although the Treaty of Dancing Rabbit Creek stated Mississippi Choctaws had U.S. citizenship, they had become associated with "colored people" as non-white in a state that had imposed racial segregation under Jim Crow laws. State services for Native Americans were non-existent. The state was poor and still dependent on agriculture. In its system of segregation, services for minorities were consistently underfunded. The state constitution and voter registration rules dating from the turn of the 20th century kept most Native Americans from voting, making them ineligible to serve on juries or to be candidates for local or state offices. They were without political representation.
A Mississippi Choctaw veteran stated, "Indians were not supposed to go in the military back then ... the military was mainly for whites. My category was white instead of Indian. I don't know why they did that. Even though Indians weren't citizens of this country, couldn't register to vote, didn't have a draft card or anything, they took us anyway."
Van Barfoot, a Choctaw from Mississippi, who was a sergeant and later a second lieutenant in the U.S. Army, 157th Infantry, 45th Infantry Division, received the Medal of Honor. Barfoot was commissioned a second lieutenant after he destroyed two German machine gun nests, took 17 prisoners, and disabled an enemy tank.
Post-Reorganization (1946).
After World War II, pressure in Congress mounted to reduce Washington's authority on Native American lands and liquidate the government's responsibilities to them. In 1953 the House of Representatives passed Resolution 108, proposing an end to federal services for 13 tribes deemed ready to handle their own affairs. The same year, Public Law 280 transferred jurisdiction over tribal lands to state and local governments in five states. Within a decade Congress terminated federal services to more than sixty groups despite intense opposition by Indians. Congress settled on a policy to terminate tribes as quickly as possible. Out of concern for the isolation of many Native Americans in rural areas, the federal government created relocation programs to cities to try to expand their employment opportunities. Indian policy experts hoped to expedite assimilation of Native Americans to the larger American society, which was becoming urban. In 1959, the Choctaw Termination Act was passed. Unless repealed by the federal government, the Choctaw Nation of Oklahoma would effectively be terminated as a sovereign nation as of August 25, 1970.
President John F. Kennedy halted further termination in 1961 and decided against implementing additional terminations. He did enact some of the last terminations in process, such as with the Ponca. Both presidents Lyndon Johnson and Richard Nixon repudiated termination of the federal government's relationship with Native American tribes.
Mississippi Choctaw Self-Determination Era.
The Choctaw people continued to struggle economically due to bigotry, cultural isolation, and lack of jobs. The Choctaw, who for 150 years had been neither white nor black, were "left where they had always been"—in poverty. Will Campbell, a Baptist minister and Civil Rights activist, witnessed the destitution of the Choctaw. He would later write, "the thing I remember the most ... was the depressing sight of the Choctaws, their shanties along the country roads, grown men lounging on the dirt streets of their villages in demeaning idleness, sometimes drinking from a common bottle, sharing a roll-your-own cigarette, their half-clad children a picture of hurting that would never end." With reorganization and establishment of tribal government, however, over the next decades they took control of "schools, health care facilities, legal and judicial systems, and social service programs."
The Choctaws witnessed the social forces that brought Freedom Summer and its after effects to their ancient homeland. The Civil Rights Era produced significant social change for the Choctaw in Mississippi, as their civil rights were enhanced. Prior to the Civil Rights Act of 1964, most jobs were given to whites, then blacks. Donna Ladd wrote that a Choctaw, now in her 40s, remembers "as a little girl, she thought that a 'white only' sign in a local store meant she could only order white, or vanilla, ice cream. It was a small story, but one that shows how a third race can easily get left out of the attempts for understanding." 
On June 21, 1964 James Chaney, Andrew Goodman, and Michael Schwerner (renowned civil rights workers) disappeared; their remains were later found in a newly constructed dam. A crucial turning point in the FBI investigation came when the charred remains of the murdered Mississippi civil rights workers' station wagon was found on a Mississippi Choctaw reservation. Two Choctaw women, who were in the back seat of a deputy's patrol car, said they witnessed the meeting
of two conspirators who expressed their desire to "beat-up" the boys. The end of legalized racial segregation permitted the Choctaws to participate in public institutions and facilities that had been reserved exclusively for white patrons.
Phillip Martin, who had served in the U. S. Army in Europe during World War II, returned to visit his former Neshoba County, Mississippi home. After seeing the poverty of his people, he decided to stay to help. Martin served as chairperson in various Choctaw committees up until 1977.
Martin was elected as Chief of the Mississippi Band of Choctaw Indians. He served a total of 30 years, being re-elected until 2007. Martin died in Jackson, Mississippi, on February 4, 2010. He was eulogized as a visionary leader, who had lifted his people out of poverty with businesses and casinos built on tribal land.
Changes after the civil rights era were reflected in sports and other venues. In 1981, the African-American athlete, Marcus Dupree, played his final high school football game at Warriors Stadium of the Choctaw Indian Reservation's tribal high school. He finished with 5,284 rushing yards on 8.3 yards per carry; there was considerable media coverage to witness the record-breaking event that was praised by many Mississippians.
Choctaws today.
In the social changes around the Civil Rights Era, between 1965 and 1982 Native Americans renewed their commitments to the value of their ancient heritage. Working to celebrate their own strengths and exercise appropriate rights; they dramatically reversed the trend toward abandonment of Indian culture and tradition. During the 1960s, Community Action programs connected with Native Americans were based on citizen participation. In the 1970s, the Choctaws repudiated the extremes of Indian activism. The Oklahoma Choctaw sought a local grassroots solution to reclaim their cultural identity and sovereignty as a nation. The Mississippi Choctaw would lay the foundations of business ventures. Policy continued toward the ideology of Self-Determination.
Soon after this, Congress passed the landmark Indian Self-Determination and Education Assistance Act of 1975, completing a 15-year period of federal policy reform with regard to American Indian tribes. The legislation included means by which tribes could negotiate contracts with the BIA to manage more of their own education and social service programs. In addition, it provided direct grants to help tribes develop plans for assuming responsibility. It also provided for Indian parents' involvement on school boards.
Beginning in 1979 the tribal council worked on a variety of economic development initiatives, first geared toward attracting industry to the reservation. They had many people available to work, natural resources and no taxes. Industries have included automotive parts, greeting cards, direct mail and printing, and plastic-molding. The Mississippi Band of Choctaw Indians is one of the state's largest employers, running 19 businesses and employing 7,800 people.
Starting with New Hampshire in 1963, numerous state governments began to operate lotteries and other gambling to raise money for government services. In 1987 the Supreme Court of the United States ruled that federally recognized tribes could operate gaming facilities on reservation land free from state regulation. In 1988 the U.S. Congress enacted the Indian Gaming Regulatory Act (IGRA). It set the terms for Native American tribes to operate casinos.
After years of waiting under the Ray Mabus administration, Mississippi Governor Kirk Fordice in 1992 gave permission for the Mississippi Band of Choctaw Indians to develop Class III gaming. The Mississippi Band of Choctaw Indians (MBCI) has one of the largest casino resorts in the nation; it is located in Philadelphia, Mississippi. The Silver Star Casino opened its doors in 1994. The Golden Moon Casino opened in 2002. The casinos are collectively known as the Pearl River Resort. 
The Choctaw Nation of Oklahoma has its own gaming operations: the Choctaw Casino Resort and Choctaw Casino Bingo, popular gaming destinations in Durant. Near the Oklahoma-Texas border, they serve residents of Southern Oklahoma and North Texas. The largest regional population base from which they draw is the Dallas-Fort Worth Metroplex.
After nearly two hundred years, the Choctaw have retaken control of the ancient site of Nanih Waiya. For years protected as a Mississippi state park, "Nanih Waiya" was returned to the Choctaw in 2006, under Mississippi Legislature State Bill 2803.
Jack Abramoff and Indian casino lobbying.
In the second half of the 1990s, Abramoff was employed by Preston Gates Ellis & Rouvelas Meeds LLP, the lobbying arm of Preston Gates & Ellis LLP law firm based in Seattle, Washington. In 1995, Abramoff began representing Native American tribes with gambling interests, starting with the Mississippi Band of Choctaw Indians.
The Choctaw originally had lobbied the federal government directly, but beginning in 1994, they found that many of the congressional members who had responded to their issues had either retired or were defeated in the "Republican Revolution" of the 1994 elections. Nell Rogers, the tribe's specialist on legislative affairs, had a friend who was familiar with the work of Abramoff and his father as Republican activists. The tribe contacted Preston Gates, and soon after hired the firm and Abramoff.
Abramoff succeeded in gaining defeat of a Congressional bill to use the unrelated business income tax (UBIT) to tax Native American casinos, sponsored by Reps. Bill Archer (R-TX) and Ernest Istook (R-OK). Since the matter involved taxation, Abramoff enlisted help from his college Republican acquaintance Grover Norquist, and his Americans for Tax Reform (ATR). The bill was eventually defeated in 1996 in the Senate, due in part to grassroots work by ATR, for which the Choctaw paid $60,000.
According to "Washington Business Forward", a lobbying trade magazine, Senator [Tom DeLay] was a major factor in those victories. The fight strengthened Abramoff's alliance with him.
Purporting to represent Native Americans before Congress and state governments in this new field, Jack Abramoff and Michael Scanlon used fraudulent means to gain profits of $15 million in total payments from the Mississippi Band of Choctaw Indians. Congressional hearings were held on the issue and charges were brought against Abramoff and Scanlon. In an e-mail sent January 29, 2002, Abramoff wrote to Scanlon, "I have to meet with the monkeys from the Choctaw tribal council."
On January 3, 2006, Abramoff pled guilty to three felony counts — conspiracy, fraud, and tax evasion — involving charges stemming principally from his lobbying activities in Washington on behalf of Native American tribes. In addition, Abramoff and other defendants must make restitution of at least $25 million that was defrauded from clients, most notably the Native American tribes.
MOWA Choctaw.
For generations Choctaw lived in Mobile and Washington counties in southwestern Alabama. These Indians traced their lineage to two groups of Choctaw who settled the area in the early 19th century. The first group had been allies of the Red Sticks during the Creek War of 1813-14 and hid in the remote swamps after their defeat.The second group arrived in the 1830s, when they hid rather than be removed to the western Indian Territories. These two groups merged and lived in relative isolation from white contact for several decades.
Over the years, the MOWA (for Mobile and Washington) faced discrimination, persecution, and debt peonage because of their racial status, which the federal government considered undocumented and ambiguous. Some had intermarried with African Americans or European Americans, and been classified socially by the state as black or white in the binary racial culture of the South, rather than being allowed to preserve their identity as Choctaw. In the 21st century, the MOWA Choctaw were recognized as a Native American tribe by the state of Alabama, but they are still seeking federal recognition.
Choctaw in the 2010 Census.
The 2010 Census found Choctaw living in every state of the Union. The states with the largest Choctaw populations were:
Culture.
The Choctaw people are believed to have coalesced in the 17th century, perhaps from peoples from Alabama and the Plaquemine culture. Their culture continued to evolve in the Southeast. The Choctaw practiced Head flattening as a ritual adornment for its people, but the practice eventually fell out of favor. Some of their communities had extensive trade and interaction with Europeans, including people from Spain, France, and England greatly shaped it as well. After the United States was formed and its settlers began to move into the Southeast, the Choctaw were among the Five Civilized Tribes, who adopted some of their ways. They transitioned to yeoman farming methods, and accepted European Americans and African Americans into their society. In mid-summer the Mississippi Band of Choctaw Indians celebrate their traditional culture during the Choctaw Indian Fair with ball games, dancing, cooking and entertainment.
Clans.
Within the Choctaws were two distinct moieties: "Imoklashas" (elders) and "Inhulalatas" (youth). Each moiety had several clans or "Iskas"; it is estimated there were about 12 Iskas altogether. The people had a matrilineal kinship system, with children born into the clan or iska of the mother and taking their social status from it. In this system, their maternal uncles had important roles. Identity was established first by moiety and iska; so a Choctaw identified first as Imoklasha or Inhulata, and second as Choctaw. Children belonged to the Iska of their mother. The following were some major districts:
By the early 1930s, the anthropologist John Swanton wrote of the Choctaw: "[T]here are only the faintest traces of groups with truly totemic designations, the animal and plant names which occur seeming not to have had a totemic connotation."
Swanton wrote, "Adam Hodgson ... told ... that there were tribes or families among the Indians, somewhat similar to the Scottish clans; such as, the Panther family, the Bird family, Raccoon Family, the Wolf family." The following are possible totemic clan designations:
Games.
Choctaw stickball, the oldest field sport in North America, was also known as the "little brother of war" because of its roughness and substitution for war. When disputes arose between Choctaw communities, stickball provided a civil way to settle issues. The stickball games would involve as few as twenty or as many as 300 players. The goal posts could be from a few hundred feet apart to a few miles. Goal posts were sometimes located within each opposing team's village. A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject. The Mississippi Band of Choctaw Indians continue to practice the sport.
Chunkey was a game using a stone-shaped disk that was about 1–2 inches in length. Players would throw the disk down a corridor so that it could roll past the players at great speed. As the disk rolled down the corridor, players would throw wooden shafts at it. The object of the game was to strike the disk or prevent your opponents from hitting it.
Other games included using corn, cane, and moccasins. The corn game used five to seven kernels of corn. One side was blackened and the other side white. Players won points based on each color. One point was awarded for the black side and 5-7 points for the white side. There were usually only two players.
Language.
The Choctaw language is a member of the Muskogean family and was well known among the frontiersmen, such as Andrew Jackson and William Henry Harrison, of the early 19th century. The language is closely related to Chickasaw, and some linguists consider the two dialects a single language. The Choctaw language is the essence of tribal culture, tradition, and identity. Many Choctaw adults learned to speak the language before speaking English. The language is a part of daily life on the Mississippi Choctaw reservation. The following table is an example of Choctaw text and its translation:
Religion.
The Choctaw believed in a good spirit and an evil spirit. They may have been sun, or "Hushtahli", worshippers. The historian Swanton wrote, 
"[T]he Choctaws anciently regarded the sun as a deity ... the sun was ascribed the power of life and death. He was represented as looking down upon the earth, and as long as he kept his flaming eye fixed on any one, the person was safe ... fire, as the most striking representation of the sun, was considered as possessing intelligence, and as acting in concert with the sun ... [having] constant intercourse with the sun ..."
The word "nanpisa" (the one who sees) expressed the reverence the Choctaw had for the sun.
Choctaw prophets were known to have addressed the sun. Swanton wrote, "an old Choctaw informed Wright that before the arrival of the missionaries, they had no conception of prayer. He added, "I have indeed heard it asserted by some, that anciently their hopaii, or prophets, on some occasions were accustomed to address the sun ..."
Traditional clothing.
The colorful dresses worn by today's Choctaw are made by hand. They are based on designs of their ancestors, who adapted 19th-century European-American styles to their needs. Today many Choctaw wear such traditional clothing mainly for special events. Choctaw elders, especially the women, dress in their traditional garb every day. Choctaw dresses are trimmed by full diamond, half diamond or circle, and crosses that represent stickball sticks.
Treaties.
Land was the most valuable asset, which the Native Americans held in collective stewardship. The United States systematically obtained Choctaw land for conventional European-American settlement through treaties, legislation, and threats of warfare. Although the Choctaw made treaties with Great Britain, France, Spain, and the Confederate States of America; the nation signed only nine treaties with the United States. Some treaties which the US made with other nations, such as the Treaty of San Lorenzo, indirectly affected the Choctaw.
Reservations.
Reservations can be found in Alabama-(MOWA Band of Choctaw Indians), Louisiana-(Jena Band of Choctaw Indians; United Houma Nation; Choctaw-Apache Tribe of Ebarb; Bayou Lacombe Choctaw; Clifton Choctaw), Texas-(Mount Tabor Indian Community), Mississippi-(Mississippi Band of Choctaw Indians), and Oklahoma-(Choctaw Nation of Oklahoma). Other population centers include California, Oregon, Dallas, Houston and Chicago.
Remove reference to Oklahoma reservation, as the Oklahoma Choctaws do not reside on a reservation.

</doc>
<doc id="7224" url="http://en.wikipedia.org/wiki?curid=7224" title="Calypso">
Calypso

Calypso (mythology) is the name of one of the Nereids (sea nymphs) in Greek mythology, She is the daughter of Atlas - one of the twelve Titans. The word may also refer to:

</doc>
<doc id="7225" url="http://en.wikipedia.org/wiki?curid=7225" title="Chemical affinity">
Chemical affinity

History.
Early Theories.
The idea of "affinity" is extremely old. Many attempts have been made at identifying its origins. The majority of such attempts, however, except in a general manner, end in futility since "affinities" lie at the basis of all magic, thereby pre-dating science. Physical chemistry, however, was one of the first branches of science to study and formulate a "theory of affinity". The name "affinitas" was first used in the sense of chemical relation by German philosopher Albertus Magnus near the year 1250. Later, those as Robert Boyle, John Mayow, Johann Glauber, Isaac Newton, and Georg Stahl put forward ideas on elective affinity in attempts to explain how heat is evolved during combustion reactions.
The term "affinity" has been used figuratively since c. 1600 in discussions of structural relationships in chemistry, philology, etc., and reference to "natural attraction" is from 1616. "Chemical affinity", historically, has referred to the "force" that causes chemical reactions. as well as, more generally, and earlier, the ″tendency to combine″ of any pair of substances. The broad definition, used generally throughout history, is that chemical affinity is that whereby substances enter into or resist decomposition.
The modern term chemical affinity is a somewhat modified variation of its eighteenth-century precursor "elective affinity" or elective attractions, a term that was used by the 18th century chemistry lecturer William Cullen. Whether Cullen coined the phrase is not clear, but his usage seems to predate most others, although it rapidly became widespread across Europe, and was used in particular by the Swedish chemist Torbern Olof Bergman throughout his book "De attractionibus electivis" (1775). Affinity theories were used in one way or another by most chemists from around the middle of the 18th century into the 19th century to explain and organise the different combinations into which substances could enter and from which they could be retrieved. Antoine Lavoisier, in his famed 1789 "Traité Élémentaire de Chimie (Elements of Chemistry)", refers to Bergman’s work and discusses the concept of elective affinities or attractions.
According to chemistry historian Henry Leicester, the influential 1923 textbook "Thermodynamics and the Free Energy of Chemical Reactions" by Gilbert N. Lewis and Merle Randall led to the replacement of the term "affinity" by the term "free energy" in much of the English-speaking world.
According to Prigogine, the term was introduced and developed by Théophile de Donder.
Goethe used the concept in his novel Elective Affinities, (1809)
Visual Representations.
The affinity concept was very closely linked to the visual representation of substances on a table. The first-ever "affinity table", which was based on displacement reactions, was published in 1718 by the French chemist Étienne François Geoffroy. Geoffroy's name is best known in connection with these tables of "affinities" ("tables des rapports"), which were first presented to the French Academy of Sciences in 1718 and 1720, as shown below:
During the 18th century many versions of the table were proposed with leading chemists like Torbern Bergman in Sweden and Joseph Black in Scotland adapting it to accommodate new chemical discoveries. All the tables were essentially lists, prepared by collating observations on the actions of substances one upon another, showing the varying degrees of affinity exhibited by analogous bodies for different reagents.
Crucially, the table was the central graphic tool used to teach chemistry to students and its visual arrangement was often combined with other kinds diagrams. Joseph Black, for example, used the table in combination with chiastic and circlet diagrams to visualise the core principles of chemical affinity. Affinity tables were used throughout Europe until the early 19th century when they were displaced by affinity concepts introduced by Claude Berthollet.
Modern conceptions.
In chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.
In modern terms, we relate affinity to the phenomenon whereby certain atoms or molecules have the tendency to aggregate or bond. For example, in the 1919 book "Chemistry of Human Life" physician George W. Carey states that, "Health depends on a proper amount of iron phosphate Fe3(PO4)2 in the blood, for the molecules of this salt have chemical affinity for oxygen and carry it to all parts of the organism." In this antiquated context, chemical affinity is sometimes found synonymous with the term "magnetic attraction". Many writings, up until about 1925, also refer to a "law of chemical affinity".
Ilya Prigogine summarized the concept of affinity, saying, "All chemical reactions drive the system to a state of equilibrium in which the "affinities" of the reactions vanish."
Thermodynamics.
The present IUPAC definition is that affinity "A" is the negative partial derivative of Gibbs free energy "G" with respect to extent of reaction "ξ" at constant pressure and temperature. That is,
It follows that affinity is positive for spontaneous reactions.
In 1923, the Belgian mathematician and physicist Théophile de Donder derived a relation between affinity and the Gibbs free energy of a chemical reaction. Through a series of derivations, de Donder showed that if we consider a mixture of chemical species with the possibility of chemical reaction, it can be proven that the following relation holds:
With the writings of Théophile de Donder as precedent, Ilya Prigogine and Defay in "Chemical Thermodynamics" (1954) defined chemical affinity as the rate of change of the uncompensated heat of reaction "Q"' as the reaction progress variable or reaction extent "ξ" grows infinitesimally:
This definition is useful for quantifying the factors responsible both for the state of equilibrium systems (where {{nowrap|1="A" = 0}}), and for changes of state of non-equilibrium systems (where "A" ≠ 0").

</doc>
<doc id="7227" url="http://en.wikipedia.org/wiki?curid=7227" title="Comet Hale–Bopp">
Comet Hale–Bopp

Comet Hale–Bopp (formally designated C/1995 O1) was perhaps the most widely observed comet of the 20th century and one of the brightest seen for many decades. It was visible to the naked eye for a record 18 months, twice as long as the previous record holder, the Great Comet of 1811.
Hale–Bopp was discovered on July 23, 1995, at a great distance from the Sun, raising expectations that the comet would brighten considerably by the time it passed close to Earth. Although predicting the brightness of comets with any degree of accuracy is very difficult, Hale–Bopp met or exceeded most predictions when it passed perihelion on April 1, 1997. The comet was dubbed the Great Comet of 1997.
Discovery.
The comet was discovered independently on July 23, 1995 by two observers, Alan Hale and Thomas Bopp, both in the United States.
Hale had spent many hundreds of hours searching for comets without success, and was tracking known comets from his driveway in New Mexico when he chanced upon Hale–Bopp just after midnight. The comet had an apparent magnitude of 10.5 and lay near the globular cluster M70 in the constellation of Sagittarius. Hale first established that there was no other deep-sky object near M70, and then consulted a directory of known comets, finding that none were known to be in this area of the sky. Once he had established that the object was moving relative to the background stars, he emailed the Central Bureau for Astronomical Telegrams, the clearing house for astronomical discoveries.
Bopp did not own a telescope. He was out with friends near Stanfield, Arizona observing star clusters and galaxies when he chanced across the comet while at the eyepiece of his friend's telescope. He realized he might have spotted something new when, like Hale, he checked his star maps to determine if any other deep-sky objects were known to be near M70, and found that there were none. He alerted the Central Bureau for Astronomical Telegrams through a Western Union telegram. Brian G. Marsden, who had run the bureau since 1968, laughed, "Nobody sends telegrams anymore. I mean, by the time that telegram got here, Alan Hale had already e-mailed us three times with updated coordinates."
The following morning, it was confirmed that this was a new comet, and it was given the designation C/1995 O1. The discovery was announced in International Astronomical Union circular 6187.
The comet may have been observed by ancient Egyptians during the reign of pharaoh Pepi I (2332–2283 BC). In Pepi's pyramid in Saqqara is a text referring to an "nhh-star" as a companion of the pharaoh in the heavens, where "" is the hieroglyph for long hair.
Early observation.
Hale–Bopp's orbital position was calculated as 7.2 astronomical units (AU) from the Sun, placing it between Jupiter and Saturn and by far the greatest distance from Earth at which a comet had been discovered by amateurs. Most comets at this distance are extremely faint, and show no discernible activity, but Hale–Bopp already had an observable coma. An image taken at the Anglo-Australian Telescope in 1993 was found to show the then-unnoticed comet some 13 AU from the Sun, a distance at which most comets are essentially unobservable. (Halley's Comet was more than 100 times fainter at the same distance from the Sun.) Analysis indicated later that its comet nucleus was 60±20 kilometres in diameter, approximately six times the size of Halley.
Its great distance and surprising activity indicated that comet Hale–Bopp might become very bright indeed when it reached perihelion in 1997. However, comet scientists were wary – comets can be extremely unpredictable, and many have large outbursts at great distance only to diminish in brightness later. Comet Kohoutek in 1973 had been touted as a 'comet of the century' and turned out to be unspectacular.
Perihelion.
Hale–Bopp became visible to the naked eye in May 1996, and although its rate of brightening slowed considerably during the latter half of that year, scientists were still cautiously optimistic that it would become very bright. It was too closely aligned with the Sun to be observable during December 1996, but when it reappeared in January 1997 it was already bright enough to be seen by anyone who looked for it, even from large cities with light-polluted skies.
The Internet was a growing phenomenon at the time, and numerous websites that tracked the comet's progress and provided daily images from around the world became extremely popular. The Internet played a large role in encouraging the unprecedented public interest in comet Hale–Bopp.
As the comet approached the Sun, it continued to brighten, shining at 2nd magnitude in February, and showing a growing pair of tails, the blue gas tail pointing straight away from the Sun and the yellowish dust tail curving away along its orbit. On March 9, a solar eclipse in China, Mongolia and eastern Siberia allowed observers there to see the comet in the daytime. Hale–Bopp had its closest approach to Earth on March 22, 1997 at a distance of 1.315 AU.
As it passed perihelion on April 1, 1997 the comet developed into a spectacular sight. It shone brighter than any star in the sky except Sirius, and its dust tail stretched 40–45 degrees across the sky. The comet was visible well before the sky got fully dark each night, and while many great comets are very close to the Sun as they pass perihelion, comet Hale–Bopp was visible all night to northern hemisphere observers.
After perihelion.
After its perihelion passage, the comet moved into the southern celestial hemisphere. The comet was much less impressive to southern hemisphere observers than it had been in the northern hemisphere, but southerners were able to see the comet gradually fade from view during the second half of 1997. The last naked-eye observations were reported in December 1997, which meant that the comet had remained visible without aid for 569 days, or about 18 and a half months. The previous record had been set by the Great Comet of 1811, which was visible to the naked eye for about 9 months.
The comet continued to fade as it receded, but is still being tracked by astronomers. In October 2007, 10 years after the perihelion and at distance of 25.7 AU from Sun, the comet was still active as indicated by the detection of the CO-driven coma. Herschel Space Observatory images taken in 2010 suggest comet Hale–Bopp is covered in a fresh frost layer. Hale–Bopp was again detected in December 2010 when it was 30.7AU from the Sun, and again on 2012 Aug 7 when it was 33.2AU from the Sun. Astronomers expect that the comet will remain observable with large telescopes until perhaps 2020, by which time it will be nearing 30th magnitude. By this time it will become very difficult to distinguish the comet from the large numbers of distant galaxies of similar brightness.
Orbital changes.
The comet likely made its last perihelion 4,200 years ago. Its orbit is almost perpendicular to the plane of the ecliptic, which ensures that close approaches to planets are rare. However, in April 1996 the comet passed within 0.77 AU of Jupiter, close enough for its orbit to be affected by the planet's gravity. The comet's orbit was shortened considerably to a period of roughly 2,533 years, and it will next return to the inner Solar System around the year 4385. Its greatest distance from the Sun (aphelion) will be about 370 AU, reduced from about 525 AU.
Over many orbits, the cumulative effect of gravitational perturbations on comets with high orbital inclinations and small perihelion distances is generally to reduce the perihelion distance to very small values. Hale–Bopp has about a 15% chance of eventually becoming a sungrazing comet through this process.
It has been calculated that the previous visit by Hale–Bopp occurred in July 2215 BC. The comet may have presented a similar sight to people then, as the estimated closest approach to Earth was 1.4 AU, but no records of it have survived. Hale–Bopp may have had a near collision with Jupiter in early June 2215 BC, which probably caused a dramatic change in its orbit, and 2215 BC may have been its first passage through the inner Solar System.
The estimated probability of impacting Earth in future passages through the inner Solar System is remote, about 2.5×10−9 per orbit. However, given that the comet nucleus is around 60 km in diameter, the consequences of such an impact would be apocalyptic. A calculation given by Weissman conservatively estimates the diameter at 35 km; an estimated density of 0.6 g/cm3 then gives a cometary mass of 1.3×1019 g. An impact velocity of 52.5 km/s yields an impact energy of 1.9×1032 ergs, or 4.4×109 megatons, about 44 times the estimated energy of the K-T impact event.
Scientific results.
Comet Hale–Bopp was observed intensively by astronomers during its perihelion passage, and several important advances in cometary science resulted from these observations. The dust production rate of the comet was very high (up to 2.0 kg/s), which may have made the inner coma optically thick. Based on the properties of the dust grains—high temperature, high albedo and strong 10 μm silicate emission feature—the astronomers concluded the dust grains are smaller than observed in any other comet.
Hale–Bopp showed the highest ever linear polarization detected for any comet. Such polarization is the result of solar radiation getting scattered by the dust particles in the coma of the comet and depends on the nature of the grains. It further confirms that the dust grains in the coma of comet Hale–Bopp were smaller than inferred in any other comet.
Sodium tail.
One of the most remarkable discoveries was that the comet had a third type of tail. In addition to the well-known gas and dust tails, Hale–Bopp also exhibited a faint sodium tail, only visible with powerful instruments with dedicated filters. Sodium emission had been previously observed in other comets, but had not been shown to come from a tail. Hale–Bopp's sodium tail consisted of neutral atoms (not ions), and extended to some 50 million kilometres in length.
The source of the sodium appeared to be the inner coma, although not necessarily the nucleus. There are several possible mechanisms for generating a source of sodium atoms, including collisions between dust grains surrounding the nucleus, and 'sputtering' of sodium from dust grains by ultraviolet light. It is not yet established which mechanism is primarily responsible for creating Hale–Bopp's sodium tail, and the narrow and diffuse components of the tail may have different origins.
While the comet's dust tail roughly followed the path of the comet's orbit and the gas tail pointed almost directly away from the Sun, the sodium tail appeared to lie between the two. This implies that the sodium atoms are driven away from the comet's head by radiation pressure.
Deuterium abundance.
The abundance of deuterium in comet Hale–Bopp in the form of heavy water was found to be about twice that of Earth's oceans. If Hale–Bopp's deuterium abundance is typical of all comets, this implies that although cometary impacts are thought to be the source of a significant amount of the water on Earth, they cannot be the only source.
Deuterium was also detected in many other hydrogen compounds in the comet. The ratio of deuterium to normal hydrogen was found to vary from compound to compound, which astronomers believe suggests that cometary ices were formed in interstellar clouds, rather than in the solar nebula. Theoretical modelling of ice formation in interstellar clouds suggests that comet Hale–Bopp formed at temperatures of around 25–45 Kelvin.
Organics.
Spectroscopic observations of Hale–Bopp revealed the presence of many organic chemicals, several of which had never been detected in comets before. These complex molecules may exist within the cometary nucleus, or might be synthesised by reactions in the comet.
Detection of argon.
Hale–Bopp was the first comet where the noble gas argon was detected. Noble gases are chemically inert and highly volatile, and since different noble elements have different sublimation temperatures, they can be used for probing the temperature histories of the cometary ices. Krypton has a sublimation temperature of 16–20 K and was found to be depleted more than 25 times relative to the solar abundance, while argon with its higher sublimation temperature was enriched relative to the solar abundance. Together these observations indicate that the interior of Hale–Bopp has always been colder than 35–40 K, but has at some point been warmer than 20 K. Unless the solar nebula was much colder and richer in argon than generally believed, this suggests that the comet formed beyond Neptune in the Kuiper belt region and then migrated outward to the Oort cloud.
Rotation.
Comet Hale–Bopp's activity and outgassing were not spread uniformly over its nucleus, but instead came from several specific jets. Observations of the material streaming away from these jets allowed astronomers to measure the rotation period of the comet, which was found to be about 11 hours 46 minutes.
Binary nucleus question.
In 1997 a paper was published that hypothesised the existence of a binary nucleus to fully explain the observed pattern of comet Hale–Bopp's dust emission observed in October 1995. The paper was based on theoretical analysis, and did not claim an observational detection of the proposed satellite nucleus, but estimated that it would have a diameter of about 30 km, with the main nucleus being about 70 km across, and would orbit in about three days at a distance of about 180 km. This analysis was confirmed by observations in 1996 using Wide-Field Planetary Camera 2 of the Hubble Space Telescope which had taken images of the comet that revealed the satellite.
Although observations using adaptive optics in late 1997 and early 1998 showed a double peak in the brightness of the nucleus, controversy still exists over whether such observations can only be explained by a binary nucleus. The discovery of the satellite was not confirmed by other observations. Also, while comets have been observed to break up before, no case has previously been found of a stable binary nucleus. Given the very small mass of this comet, the orbit of the binary nucleus would be easily disrupted by the gravity of the Sun and planets.
UFO claims.
In November 1996 amateur astronomer Chuck Shramek of Houston, Texas took a CCD image of the comet, which showed a fuzzy, slightly elongated object nearby. When his computer sky-viewing program did not identify the star, Shramek called the Art Bell radio program Coast to Coast AM to announce that he had discovered a "Saturn-like object" following Hale–Bopp. UFO enthusiasts, such as remote viewing proponent Courtney Brown, soon concluded that there was an alien spacecraft following the comet.
Several astronomers, including Alan Hale, claimed the object was simply an 8.5-magnitude star, SAO141894, which did not appear on Shramek's computer program because the user preferences were set incorrectly. Later, Art Bell even claimed to have obtained an image of the object from an anonymous astrophysicist who was about to confirm its discovery. However, astronomers Olivier Hainaut and David J. Tholen of the University of Hawaii stated that the alleged photo was an altered copy of one of their own comet images.
A few months later, in March 1997, the cult Heaven's Gate committed mass suicide with the intention of teleporting to a spaceship they believed was flying behind the comet.
Nancy Lieder, a self-proclaimed contactee who claims to receive messages from aliens through an implant in her brain, stated that Hale–Bopp was a fiction designed to distract the population from the coming arrival of "Nibiru" or "Planet X", a giant planet whose close passage would disrupt the Earth's rotation, causing global cataclysm. Although Lieder's original date for the apocalypse, May 2003, passed without incident, predictions of the imminent arrival of Nibiru continued by various conspiracy websites, most of whom tied it to the 2012 phenomenon.
Legacy.
Its lengthy period of visibility and extensive coverage in the media meant that Hale–Bopp was probably the most-observed comet in history, making a far greater impact on the general public than the return of Halley's Comet in 1986, and certainly seen by a greater number of people than witnessed any of Halley's previous appearances. For instance, 69% of Americans had seen Hale–Bopp by April 9, 1997.
Hale–Bopp was a record-breaking comet—the farthest comet from the Sun discovered by amateurs, with the largest well-measured cometary nucleus known after 95P/Chiron, and it was visible to the naked eye for twice as long as the previous record-holder. It was also brighter than magnitude 0 for eight weeks, longer than any other recorded comet.

</doc>
<doc id="7230" url="http://en.wikipedia.org/wiki?curid=7230" title="Conspiracy">
Conspiracy

Conspiracy or conspirator may refer to: 

</doc>
<doc id="7232" url="http://en.wikipedia.org/wiki?curid=7232" title="Cholistan Desert">
Cholistan Desert

The Cholistan Desert (, Punjabi:), also locally known as Rohi (), sprawls thirty kilometers from Bahawalpur, Punjab, Pakistan and covers an area of . It adjoins the Thar Desert, extending over to Sindh and into India.
The word Cholistan is derived from the Turkic word "chol", which means "desert". Cholistan thus means Land of the Desert. The people of Cholistan lead a semi-nomadic life, moving from one place to another in search of water and fodder for their animals. The dry bed of the Hakra River runs through the area, along which many settlements of the Indus Valley Civilization have been found.
The desert also has an annual Jeep rally, known as Cholistan Desert Jeep Rally. It is the biggest motor sports event in Pakistan.
Culture and traditions.
Local dialect.
The language of Cholistan also reflects a number of features for its historical and geographical background. Cholistan is a pretty diffused society as many people in search of rain travel larger distances and many travel to central parts of Punjab for work when there is shortage of water. So due to the diffused nature of these people language in Cholistan is a mixture of many Dialects of the Punjab. Many communities do speak their peculiar local dialects but general Punjabi language is easily understood and can be spoken owing to the migration trends in search for work in these regions.
Arts and crafts.
In a harsh and barren land where rainfall is very sparse and unreliable, Cholistanis rely mainly on their livestock of sheep, goats, and camel. However in cold nights of winter they huddle indoor and engage themselves in various arts and crafts such as textiles, weaving, leatherwork, and pottery.
Local crafts.
As mentioned above, the Indus Valley has always been occupied by the wandering nomadic tribes, who are fond of isolated areas, as such areas allow them to lead life free of foreign intrusion, enabling them to establish their own individual and unique cultures. Cholistan till the era of Mughal rule had also been isolated from outside influence. During the rule of Mughal Emperor Akbar, it became a proper productive unit. The entire area was ruled by a host of kings who securely guarded their frontiers. The rulers were the great patrons of art, and the various crafts underwent a simultaneous and parallel development, influencing each other. Mesons, stone carvers, artisans, artists, and designers started rebuilding the old cities and new sites, and with that flourished new courts, paintings, weaving, and pottery. The fields of architecture, sculpture, terra cotta, and pottery developed greatly in this phase.
Livestock.
The backbone of Cholistan economy is cattle breeding. It has the major importance for satisfying the area's major needs for cottage industry as well as milk meat and fat. Because of the nomadic way of life the main wealth of the people are their cattle that are bred for sale, milked or shorn for their wool. Moreover, isolated as they were, they had to depend upon themselves for all their needs like food, clothing, and all the items of daily use. So all their crafts initially stemmed from necessity but later on they started exporting their goods to the other places as well. The estimated number of livestock in the desert areas is 1.6 million.
Cotton and wool products.
Cholistan produces very superior type of carpet wool as compared to that produced in other parts of Pakistan. From this wool they knit beautiful carpets, rugs and other woolen items. This includes blankets, which is also a local necessity for the desert is not just a land of dust and heat, but winter nights here are very cold, usually below freezing points. Khes and pattu are also manufactured with wool or cotton. Khes is a form of blanket with a field of black white and pattu has a white ground base. Cholistanis now sell the wool for it brings maximum profit.
Textiles.
It may be mentioned that cotton textiles have always been a hallmark of craft of Indus valley civilization. Various kinds of khaddar-cloth are made for local consumption, and fine khaddar bedclothes and coarse lungies are woven here. A beautiful cloth called Sufi is also woven of silk and cotton, or with cotton wrap and silk wool. Gargas are made with numerous patterns and color, having complicated embroidery, mirror, and patchwork. Ajrak is another specialty of Cholistan. It is a special and delicate printing technique on both sides of the cloth in indigo blue and red patterns covering the base cloth. Cotton turbans and shawls are also made here. Chunri is another form of dopattas, having innumerable colors and patterns like dots, squares, and circles on it.
Camel products.
Camels are highly valued by the desert dwellers. Camels are not only useful for transportation and loading purposes, but its skin and wool are also quite worthwhile. Camel wool is spun and woven into beautiful woolen blankets known as falsies and into stylish and durable rugs. The camel's leather is also utilized in making kuppies, goblets, and expensive lampshades.
Leatherwork.
Leatherwork is another important local cottage industry due to the large number of livestock here. Other than the products mentioned above, Khusa (shoes) is a specialty of this area. Cholistani khusas are very famous for the quality of workmanship, variety, and richness of designs especially when stitched and embroidered with golden or brightly colored threads.
Jewelry.
The Cholistanis are fond of jewelry, especially gold jewelry. The chief ornaments made and worn by them are "Nath" (nose gay), "Katmala" (necklace) "Kangan" (bracelet), and "Pazeb" (anklets). Gold and silver bangles are also a product of Cholistan. The locals similarly work in enamel, producing enamel buttons, earrings, bangles, and rings.
Love for colors.
The great desert though considered to be colorless and drab, is not wholly devoid of color. Its green portion plays the role of "color belt" especially after rains when vegetation growth is at its peak. Adding to that the locals always wear brightly colored clothes mostly consisting of brilliant reds, blazing oranges shocking pinks, and startling yellows and greens. Even the cloth trappings of their bullocks and camels are richly colored and highly textured.
Forests in Cholistan.
There is a rain forest in Cholistan named "Dodhla Forest"
Terra cotta.
The Indus Civilization was the earliest center of ceramics, and thus the pottery of Cholistan has a long history. Local soil is very fine, thus most suitable for making pottery. The fineness of the earth can be observed on the Kacha houses which are actually plastered with mud but look like white cemented. The chief Cholistani ceramic articles are their surahies, piyalas, and glasses, remarkable for their lightness and fine finishing.
In the early times only the art of pottery and terracotta developed, but from the seventh century onwards, a large number of temples and images were also built on account of the intensified religious passions and the accumulation of wealth in cities. The building activity reached to such an extent that some cities actually became city temples. In fact the area particularly came to be known for its forts, villas, palaces, havelis, gateways, fortifications, and city walls.

</doc>
<doc id="7233" url="http://en.wikipedia.org/wiki?curid=7233" title="Causantín mac Cináeda">
Causantín mac Cináeda

Causantín or Constantín mac Cináeda (in Modern Gaelic, "Còiseam mac Choinnich"; died 877) was a king of the Picts. He is often known as Constantine I, in reference to his place in modern lists of kings of Scots, though contemporary sources described Causantín only as a Pictish king. A son of Cináed mac Ailpín ("Kenneth MacAlpin"), he succeeded his uncle Domnall mac Ailpín as Pictish king following the latter's death on 13 April 862. It is likely that Causantín's (Constantine I) reign witnessed increased activity by Vikings, based in Ireland, in Northumbria and in northern Britain. He died fighting one such invasion.
Sources.
Very few records of ninth century events in northern Britain survive. The main local source from the period is the "Chronicle of the Kings of Alba", a list of kings from Cináed mac Ailpín (died 858) to Cináed mac Maíl Coluim (died 995). The list survives in the Poppleton Manuscript, a thirteenth-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added from the tenth century onwards. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Cináed mac Ailpín may date from the end of the tenth century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain. The Pictish king-lists originally ended with this Causantín, who was reckoned the seventieth and last king of the Picts.
For narrative history the principal sources are the "Anglo-Saxon Chronicle" and the Irish annals. While Scandinavian sagas describe events in 9th century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. If the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts—the modern regions of north-west England and all of northern and western Scotland—are non-existent, and archaeology and toponymy are of primary importance.
Languages and names.
Writing a century before Causantín was born, Bede recorded five languages in Britain. Latin, the common language of the church, Old English, the language of the Angles and Saxons, Irish, spoken on the western coasts of Britain and in Ireland, Brythonic, ancestor of the Welsh language, spoken in large parts of western Britain, and Pictish, spoken in northern Britain. By the ninth century a sixth language, Old Norse, had arrived with the Vikings.
Amlaíb and Ímar.
Viking activity in northern Britain appears to have reached a peak during Causantín's reign. Viking armies were led by a small group of men who may have been kinsmen. Among those noted by the Irish annals, the "Chronicle of the Kings of Alba" and the "Anglo-Saxon Chronicle" are Ívarr—Ímar in Irish sources—who was active from East Anglia to Ireland, Halfdán—Albdann in Irish, Healfdene in Old English— and Amlaíb or Óláfr. As well as these leaders, various others related to them appear in the surviving record.
Viking activity in Britain increased in 865 when the Great Heathen Army, probably a part of the forces which had been active in Francia, landed in East Anglia. The following year, having obtained tribute from the East Anglian King Edmund, the Great Army moved north, seizing York, chief city of the Northumbrians. The Great Army defeated an attack on York by the two rivals for the Northumbrian throne, Osberht and Ælla, who had put aside their differences in the face of a common enemy. Both would-be kings were killed in the failed assault, probably on 21 March 867. Following this, the leaders of the Great Army are said to have installed one Ecgberht as king of the Northumbrians. Their next target was Mercia where King Burgred, aided by his brother-in-law King Æthelred of Wessex, drove them off.
While the kingdoms of East Anglia, Mercia and Northumbria were under attack, other Viking armies were active in the far north. Amlaíb and Auisle (Ásl or Auðgísl), said to be his brother, brought an army to Fortriu and obtained tribute and hostages in 866. Historians disagree as to whether the army returned to Ireland in 866, 867 or even in 869. Late sources of uncertain reliability state that Auisle was killed by Amlaíb in 867 in a dispute over Amlaíb's wife, the daughter of Cináed. It is unclear whether, if accurate, this woman should be identified as a daughter of Cináed mac Ailpín, and thus Causantín's sister, or as a daughter of Cináed mac Conaing, king of Brega. While Amlaíb and Auisle were in north Britain, the "Annals of Ulster" record that Áed Findliath, High King of Ireland, took advantage of their absence to destroy the longphorts along the northern coasts of Ireland. Áed Findliath was married to Causantín's sister Máel Muire. She later married Áed's successor Flann Sinna. Her death is recorded in 913.
In 870, Amlaíb and Ívarr attacked Dumbarton Rock, where the River Leven meets the River Clyde, the chief place of the kingdom of Alt Clut, south-western neighbour of Pictland. The siege lasted four months before the fortress fell to the Vikings who returned to Ireland with many prisoners, "Angles, Britons and Picts", in 871. Archaeological evidence suggests that Dumbarton Rock was largely abandoned and that Govan replaced it as the chief place of the kingdom of Strathclyde, as Alt Clut was later known. King Artgal of Alt Clut did not long survive these events, being killed "at the instigation" of Causantín son of Cináed two years later. Artgal's son and successor Run was married to a sister of Causantín.
Amlaíb disappears from Irish annals after his return to Ireland in 871. According to the "Chronicle of the Kings of Alba" he was killed by Causantín either in 871 or 872 when he returned to Pictland to collect further tribute. His ally Ívarr died in 873.
Last days of the Pictish kingdom.
In 875, the "Chronicle" and the "Annals of Ulster" again report a Viking army in Pictland. A battle, fought near Dollar, was a heavy defeat for the Picts; the "Annals of Ulster" say that "a great slaughter of the Picts resulted". In 877, shortly after building a new church for the Culdees at St Andrews, Causantín was captured and executed (or perhaps killed in battle) after defending against Viking raiders. Although there is agreement on the time and general manner of his death, it is not clear where this happened. Some believe he was beheaded on a Fife beach, following a battle at Fife Ness, near Crail. William Forbes Skene reads the "Chronicle" as placing Causantín's death at Inverdovat (by Newport-on-Tay), which appears to match the Prophecy of Berchán. The account in the "Chronicle of Melrose" names the place as the "Black Cave," and John of Fordun calls it the "Black Den". Causantín was buried on Iona.
Aftermath.
Causantín's son Domnall and his descendants represented the main line of the kings of Alba and later Scotland.
References.
 

</doc>
<doc id="7234" url="http://en.wikipedia.org/wiki?curid=7234" title="Constantine II (emperor)">
Constantine II (emperor)

Constantine II () (January/February 316 – 340) was Roman Emperor from 337 to 340. Son of Constantine the Great and co-emperor alongside his brothers, his attempt to exert his perceived rights of primogeniture led to his death in a failed invasion of Italy in 340.
Career.
The eldest son of Constantine the Great and Fausta, after the death of his half-brother Crispus, Constantine II was born in Arles in February 316 and raised as a Christian. On 1 March 317, he was made Caesar; at the age of seven, in 323, he took part in his father's campaign against the Sarmatians. At age ten he became commander of Gaul, following the death of Crispus. An inscription dating to 330 records the title of "Alamannicus", so it is probable that his generals won a victory over the Alamanni. His military career continued when Constantine I made him field commander during the 332 campaign against the Goths.
Following the death of his father in 337, Constantine II initially became emperor jointly with his brothers Constantius II and Constans, with the Empire divided between them and their cousins, the "Caesars" Dalmatius and Hannibalianus. This arrangement barely survived Constantine I’s death, as his sons arranged the slaughter of most of the rest of the family by the army. As a result, the three brothers gathered together in Pannonia and there, on 9 September 337, divided the Roman world between themselves. Constantine, proclaimed "Augustus" by the troops received Gaul, Britannia and Hispania.
He was soon involved in the struggle between factions rupturing the unity of the Christian Church. The Western portion of the Empire, under the influence of the Popes in Rome, favored Catholicism over Arianism, and through their intercession they convinced Constantine to free Athanasius, allowing him to return to Alexandria. This action aggravated Constantius II, who was a committed supporter of Arianism.
Constantine was initially the guardian of his younger brother Constans, whose portion of the empire was Italia, Africa and Illyricum. Constantine soon complained that he had not received the amount of territory that was his due as the eldest son. Annoyed that Constans had received Thrace and Macedonia after the death of Dalmatius, Constantine demanded that Constans hand over the African provinces, to which he agreed in order to maintain a fragile peace. Soon, however, they began quarreling over which parts of the African provinces belonged to Carthage, and thus Constantine, and which belonged to Italy, and therefore Constans.
Further complications arose when Constans came of age and Constantine, who had grown accustomed to dominating his younger brother, would not relinquish the guardianship. In 340 Constantine marched into Italy at the head of his troops. Constans, at that time in Dacia, detached and sent a select and disciplined body of his Illyrian troops, stating that he would follow them in person with the remainder of his forces. Constantine was engaged in military operations and was killed in an ambush outside Aquileia. Constans took control of his deceased brother's realm.

</doc>
<doc id="7235" url="http://en.wikipedia.org/wiki?curid=7235" title="Constantine II of Scotland">
Constantine II of Scotland

Constantine, son of Áed (Medieval Gaelic: "Constantín mac Áeda"; Modern Gaelic: "Còiseam mac Aoidh", known in most modern regnal lists as Constantine II; before 879 – 952) was an early King of Scotland, known then by the Gaelic name "Alba". The Kingdom of Alba, a name which first appears in Constantine's lifetime, was in northern Great Britain. The core of the kingdom was formed by the lands around the River Tay. Its southern limit was the River Forth, northwards it extended towards the Moray Firth and perhaps to Caithness, while its western limits are uncertain. Constantine's grandfather Kenneth I of Scotland (Cináed mac Ailpín, died 858) was the first of the family recorded as a king, but as king of the Picts. This change of title, from king of the Picts to king of Alba, is part of a broader transformation of Pictland and the origins of the Kingdom of Alba are traced to Constantine's lifetime.
His reign, like those of his predecessors, was dominated by the actions of Viking rulers in the British Isles, particularly the Uí Ímair ("the grandsons of Ímar", or Ivar the Boneless). During Constantine's reign the rulers of the southern kingdoms of Wessex and Mercia, later the Kingdom of England, extended their authority northwards into the disputed kingdoms of Northumbria. At first allied with the southern rulers against the Vikings, Constantine in time came into conflict with them. King Æthelstan was successful in securing Constantine's submission in 927 and 934, but the two again fought when Constantine, allied with the Strathclyde Britons and the Viking king of Dublin, invaded Æthelstan's kingdom in 937, only to be defeated at the great battle of Brunanburh. In 943 Constantine abdicated the throne and retired to the Céli Dé (Culdee) monastery of St Andrews where he died in 952. He was succeeded by his predecessor's son Malcolm I (Máel Coluim mac Domnaill).
Constantine's reign of 43 years, exceeded in Scotland only by that of King William the Lion before the Union of the Crowns in 1603, is believed to have played a defining part in the gaelicisation of Pictland, in which his patronage of the Irish Céli Dé monastic reformers was a significant factor. During his reign the words "Scots" and "Scotland" () are first used to mean part of what is now Scotland. The earliest evidence for the ecclesiastical and administrative institutions which would last until the Davidian Revolution also appears at this time.
Sources.
Compared to neighbouring Ireland and Anglo-Saxon England, few records of 9th- and 10th-century events in Scotland survive. The main local source from the period is the "Chronicle of the Kings of Alba", a list of kings from Kenneth MacAlpin (died 858) to Kenneth II (Cináed mac Maíl Coluim, died 995). The list survives in the Poppleton Manuscript, a 13th-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added in the 10th and 12th centuries. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Kenneth MacAlpin may date from the end of the 10th century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain.
For narrative history the principal sources are the "Anglo-Saxon Chronicle" and the Irish annals. The evidence from charters created in the Kingdom of England provides occasional insight into events in northern Britain. While Scandinavian sagas describe events in 10th-century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. Mainland European sources rarely concern themselves with affairs in Britain, and even less commonly with events in northern Britain, but the life of Saint Cathróe of Metz, a work of hagiography written in Germany at the end of the 10th century, provides plausible details of the saint's early life in north Britain.
While the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts—the modern regions of north-west England and all of northern and western Scotland—are non-existent, and archaeology and toponymy are of primary importance.
Pictland from Constantín mac Fergusa to Constantine I.
The dominant kingdom in eastern Scotland before the Viking Age was the northern Pictish kingdom of Fortriu on the shores of the Moray Firth. By the 9th century, the Gaels of Dál Riata (Dalriada) were subject to the kings of Fortriu of the family of Constantín mac Fergusa (Constantine son of Fergus). Constantín's family dominated Fortriu after 789 and perhaps, if Constantín was a kinsman of Óengus I of the Picts (Óengus son of Fergus), from around 730. The dominance of Fortriu came to an end in 839 with a defeat by Viking armies reported by the "Annals of Ulster" in which King Uen of Fortriu and his brother Bran, Constantín's nephews, together with the king of Dál Riata, Áed mac Boanta, "and others almost innumerable" were killed. These deaths led to a period of instability lasting a decade as several families attempted to establish their dominance in Pictland. By around 848 Kenneth MacAlpin had emerged as the winner.
Later national myth made Kenneth MacAlpin the creator of the kingdom of Scotland, the founding of which was dated from 843, the year in which he was said to have destroyed the Picts and inaugurated a new era. The historical record for 9th century Scotland is meagre, but the Irish annals and the 10th-century "Chronicle of the Kings of Alba" agree that Kenneth was a Pictish king, and call him "king of the Picts" at his death. The same style is used of Kenneth's brother Donald I (Domnall mac Ailpín) and sons Constantine I (Constantín mac Cináeda) and Áed (Áed mac Cináeda).
The kingdom ruled by Kenneth's descendants—older works used the name House of Alpin to describe them but descent from Kenneth was the defining factor, Irish sources referring to "Clann Cináeda meic Ailpín" ("the Clan of Kenneth MacAlpin")—lay to the south of the previously dominant kingdom of Fortriu, centred in the lands around the River Tay. The extent of Kenneth's nameless kingdom is uncertain, but it certainly extended from the Firth of Forth in the south to the Mounth in the north. Whether it extended beyond the mountainous spine of north Britain—Druim Alban—is unclear. The core of the kingdom was similar to the old counties of Mearns, Forfar, Perth, Fife, and Kinross. Among the chief ecclesiastical centres named in the records are Dunkeld, probably seat of the bishop of the kingdom, and "Cell Rígmonaid" (modern St Andrews).
Kenneth's son Constantine died in 876, probably killed fighting against a Viking army which had come north from Northumbria in 874. According to the king lists, he was counted the 70th and last king of the Picts in later times.
Britain and Ireland at the end of the 9th century.
In 899 Alfred the Great, king of Wessex, died leaving his son Edward the Elder as ruler of Britain south of the River Thames and his daughter Æthelflæd and son-in-law Æthelred ruling the western, English part of Mercia. The situation in the Danish kingdoms of eastern Britain is less clear. King Eohric was probably ruling in East Anglia, but no dates can reliably be assigned to the successors of Guthfrith of York in Northumbria. It is known that Guthfrith was succeeded by Sigurd and Cnut, although whether these men ruled jointly or one after the other is uncertain. Northumbria may have been divided by this time between the Viking kings in York and the local rulers, perhaps represented by Eadulf, based at Bamburgh who controlled the lands from the River Tyne or River Tees to the Forth in the north.
In Ireland, Flann Sinna, married to Constantine's aunt Máel Muire, was dominant. The years around 900 represented a period of weakness among the Vikings and Norse-Gaels of Dublin. They are reported to have been divided between two rival leaders. In 894 one group left Dublin, perhaps settling on the Irish Sea coast of Britain between the River Mersey and the Firth of Clyde. The remaining Dubliners were expelled in 902 by Flann Sinna's son-in-law Cerball mac Muirecáin, and soon afterwards appeared in western and northern Britain.
To the south-west of Constantine's lands lay the kingdom of Strathclyde. This extended north into the Lennox, east to the River Forth, and south into the Southern Uplands. In 900 it was probably ruled by King Dyfnwal.
The situation of the Gaelic kingdoms of Dál Riata in western Scotland is uncertain. No kings are known by name after Áed mac Boanta. The Frankish "Annales Bertiniani" may record the conquest of the Inner Hebrides, the seaward part of Dál Riata, by Northmen in 849. In addition to these, the arrival of new groups of Vikings from northern and western Europe was still commonplace. Whether there were Viking or Norse-Gael kingdoms in the Western Isles or the Northern Isles at this time is debated.
Early life.
Áed, Constantine's father, succeeded Constantine's uncle and namesake Constantine I in 876 but was killed in 878. Áed's short reign is glossed as being of no importance by most king lists. Although the date of his birth is nowhere recorded, Constantine II cannot have been born any later than the year after his father's death, that is 879. His name may suggest that he was born rather earlier, during the reign of his uncle Constantine I.
After Áed's death there is a two decade gap until the death of Donald II (Domnall mac Constantín) in 900 during which nothing is reported in the Irish annals. The entry for the reign between Áed and Donald II is corrupt in the "Chronicle of the Kings of Alba", and in this case the "Chronicle" is at variance with every other king list. According to the "Chronicle", Áed was followed by Eochaid, a grandson of Kenneth MacAlpin, who is somehow connected with Giric, but all other lists say that Giric ruled after Áed and make great claims for him. Giric is not known to have been a kinsman of Kenneth's, although it has been suggested that he was related to him by marriage. The major changes in Pictland which began at about this time have been associated by Alex Woolf and Archie Duncan with Giric's reign.
Woolf suggests that Constantine and his cousin Donald may have passed Giric's reign in exile in Ireland where their aunt Máel Muire was wife of two successive High Kings of Ireland, Áed Findliath and Flann Sinna. Giric died in 889. If he had been in exile, Constantine may have returned to Pictland where his cousin Donald II became king. Donald's reputation is suggested by the epithet "dasachtach", a word used of violent madmen and mad bulls, attached to him in the 11th-century writings of Flann Mainistrech, echoed by the his description in the "Prophecy of Berchan" as "the rough one who will think relics and psalms of little worth". Wars with the Viking kings in Britain and Ireland continued during Donald's reign and he was probably killed fighting yet more Vikings at Dunnottar in the Mearns in 900. Constantine succeeded him as king.
Vikings and bishops.
The earliest event recorded in the "Chronicle of the Kings of Alba" in Constantine's reign is an attack by Vikings and the plundering of Dunkeld "and all Albania" in his third year. This is the first use of the word Albania, the Latin form of the Old Irish "Alba", in the "Chronicle" which until then describes the lands ruled by the descendants of Cináed as Pictavia.
These Northmen may have been some of those who were driven out of Dublin in 902, but could also have been the same group who had defeated Domnall in 900. The "Chronicle" states that the Northmen were killed in "Srath Erenn", which is confirmed by the "Annals of Ulster" which records the death of Ímar grandson of Ímar and many others at the hands of the men of Fortriu in 904. This Ímar was the first of the Uí Ímair, that is the grandsons of Ímar, to be reported; three more grandsons of Ímar appear later in Constantín's reign. The "Fragmentary Annals of Ireland" contain an account of the battle, and this attributes the defeat of the Norsemen to the intercession of Saint Columba following fasting and prayer. An entry in the "Chronicon Scotorum" under the year 904 may possibly contain a corrupted reference to this battle.
The next event reported by the "Chronicle of the Kings of Alba" is dated to 906. This records that:King Constantine and Bishop Cellach met at the "Hill of Belief" near the royal city of Scone and pledged themselves that the laws and disciplines of the faith, and the laws of churches and gospels, should be kept "pariter cum Scottis". The meaning of this entry, and its significance, have been the subject of debate.
The phrase "pariter cum Scottis" in the Latin text of the "Chronicle" has been translated in several ways. William Forbes Skene and Alan Orr Anderson proposed that it should be read as "in conformity with the customs of the Gaels", relating it to the claims in the king lists that Giric liberated the church from secular oppression and adopted Irish customs. It has been read as "together with the Gaels", suggesting either public participation or the presence of Gaels from the western coasts as well as the people of the east coast. Finally, it is suggested that it was the ceremony which followed "the custom of the Gaels" and not the agreements.
The idea that this gathering agreed to uphold Irish laws governing the church has suggested that it was an important step in the gaelicisation of the lands east of Druim Alban. Others have proposed that the ceremony in some way endorsed Constantine's kingship, prefiguring later royal inaugurations at Scone. Alternatively, if Bishop Cellach was appointed by Giric, it may be that the gathering was intended to heal a rift between king and church.
Return of the Uí Ímair.
Following the events at Scone, there is little of substance reported for a decade. A story in the "Fragmentary Annals of Ireland", perhaps referring to events some time after 911, claims that Queen Æthelflæd, who ruled in Mercia, allied with the Irish and northern rulers against the Norsemen on the Irish sea coasts of Northumbria. The "Annals of Ulster" record the defeat of an Irish fleet from the kingdom of Ulaid by Vikings "on the coast of England" at about this time.
In this period the "Chronicle of the Kings of Alba" reports the death of Cormac mac Cuilennáin, king of Munster, in the eighth year of Constantine's reign. This is followed by an undated entry which was formerly read as "In his time Domnall [i.e. Dyfnwal], king of the [Strathclyde] Britons died, and Domnall son of Áed was elected". This was thought to record the election of a brother of Constantine named Domnall to the kingship of the Britons of Strathclyde and was seen as early evidence of the domination of Strathclyde by the kings of Alba. The entry in question is now read as "...Dynfwal... and Domnall son Áed king of Ailech died", this Domnall being a son of Áed Findliath who died on 915. Finally, the deaths of Flann Sinna and Niall Glúndub are recorded.
There are more reports of Viking fleets in the Irish Sea from 914 onwards. By 916 fleets under Sihtric Cáech and Ragnall, said to be grandsons of Ímar (that is, they belonged to the same Uí Ímair kindred as the Ímar who was killed in 904), were very active in Ireland. Sihtric inflicted a heavy defeat on the armies of Leinster and retook Dublin in 917. The following year Ragnall appears to have returned across the Irish sea intent on establishing himself as king at York. The only precisely dated event in the summer of 918 is the death of Queen Æthelflæd on 918 at Tamworth, Staffordshire. Æthelflæd had been negotiating with the Northumbrians to obtain their submission, but her death put an end to this and her successor, her brother Edward the Elder, was occupied with securing control of Mercia.
The northern part of Northumbria, and perhaps the whole kingdom, had probably been ruled by Ealdred son of Eadulf since 913. Faced with Ragnall's invasion, Ealdred came north seeking assistance from Constantine. The two advanced south to face Ragnall, and this led to a battle somewhere on the banks of the River Tyne, probably at Corbridge where Dere Street crosses the river. The Second Battle of Corbridge appears to have been indecisive; the "Chronicle of the Kings of Alba" is alone in giving Constantine the victory.
The report of the battle in the "Annals of Ulster" says that none of the kings or mormaers among the men of Alba were killed. This is the first surviving use of the word mormaer; other than the knowledge that Constantine's kingdom had its own bishop or bishops and royal villas, this is the only hint to the institutions of the kingdom.
After Corbridge, Ragnall enjoyed only a short respite. In the south, Alfred's son Edward had rapidly secured control of Mercia and had a burh constructed at Bakewell in the Peak District from which his armies could easily strike north. An army from Dublin led by Ragnall's kinsman Sihtric struck at north-western Mercia in 919, but in 920 or 921 Edward met with Ragnall and other kings. The "Anglo-Saxon Chronicle" states that these king "chose Edward as father and lord". Among the other kings present were Constantine, Ealdred son of Eadwulf, and the king of Strathclyde, either Dyfnwal II or, more probably, Owen I. Here, again, a new term appears in the record, the "Anglo-Saxon Chronicle" for the first time using the word "scottas", from which Scots derives, to describe the inhabitants of Constantine's kingdom in its report of these events.
Edward died in 924. His realms appear to have been divided with the West Saxons recognising Ælfweard while the Mercians chose Æthelstan who had been raised at Æthelflæd's court. Ælfweard died within weeks of his father and Æthelstan was inaugurated as king of all of Edward's lands in 925.
Æthelstan.
By 926 Sihtric had evidently acknowledged Æthelstan as over-king, adopting Christianity and marrying a sister of Æthelstan at Tamworth. Within the year he may have abandoned his new faith and repudiated his wife, but before Æthelstan and he could fight, Sihtric died suddenly in 927. His kinsman, perhaps brother, Gofraid, who had remained as his deputy in Dublin, came from Ireland to take power in York, but failed. Æthelstan moved quickly, seizing much of Northumbria. In less than a decade, the kingdom of the English had become by far the greatest power in Britain and Ireland, perhaps stretching as far north as the Firth of Forth.
John of Worcester's chronicle suggests that Æthelstan faced opposition from Constantine, from Owain of Strathclyde, and from the Welsh kings. William of Malmesbury writes that Gofraid, together with Sihtric's young son Olaf Cuaran fled north and received refuge from Constantine, which led to war with Æthelstan. A meeting at Eamont Bridge on 927 was sealed by an agreement that Constantine, Owen of Strathclyde, Hywel Dda, and Ealdred would "renounce all idolatry": that is, they would not ally with the Viking kings. William states that Æthelstan stood godfather to a son of Constantine, probably Indulf (Ildulb mac Constantín), during the conference.
Æthelstan followed up his advances in the north by securing the recognition of the Welsh kings. For the next seven years, the record of events in the north is blank. Æthelstan's court was attended by the Welsh kings, but not by Constantine or Owen of Strathclyde. This absence of record means that Æthelstan's reasons for marching north against Constantine in 934 are unclear.
Æthelstan's campaign is reported by in brief by the "Anglo-Saxon Chronicle", and later chroniclers such as John of Worcester, William of Malmesbury, Henry of Huntingdon, and Symeon of Durham add detail to that bald account. Æthelstan's army began gathering at Winchester by 934, and reached Nottingham by . He was accompanied by many leaders, including the Welsh kings Hywel Dda, Idwal Foel, and Morgan ab Owain. From Mercia the army went north, stopping at Chester-le-Street, before resuming the march accompanied by a fleet of ships. Owen of Strathclyde was defeated and Symeon states that the army went as far north as Dunnottar and Fortriu, while the fleet is said to have raided Caithness, by which a much larger area, including Sutherland, is probably intended. It is unlikely that Constantine's personal authority extended so far north, and while the attacks may have been directed at his allies, they may also have been simple looting expeditions.
The "Annals of Clonmacnoise" state that "the Scottish men compelled [Æthelstan] to return without any great victory", while Henry of Huntingdon claims that the English faced no opposition. A negotiated settlement may have ended matters: according to John of Worcester, a son of Constantine was given as a hostage to Æthelstan and Constantin himself accompanied the English king on his return south. He witnessed a charter with Æthelstan at Buckingham on 934 in which he is described as "subregulus", that is a king acknowledging Æthelstan's overlordship. The following year, Constantine was again in England at Æthelstan's court, this time at Cirencester where he appears as a witness, appearing as the first of several subject kings, followed by Owen of Strathclyde and Hywel Dda, who subscribed to the diploma. At Christmas of 935, Owen of Strathclyde was once more at Æthelstan's court along with the Welsh kings, but Constantine was not. His return to England less than two years later would be in very different circumstances.
Brunanburh and after.
Following his disappearance from Æthelstan's court after 935, there is no further report of Constantine until 937. In that year, together with Owen of Strathclyde and Olaf Guthfrithson of Dublin, Constantine invaded England. The resulting battle of Brunanburh—"Dún Brunde"—is reported in the "Annals of Ulster" as follows:a great battle, lamentable and terrible was cruelly fought... in which fell uncounted thousands of the Northmen.  ...And on the other side, a multitude of Saxons fell; but Æthelstan, the king of the Saxons, obtained a great victory. The battle was remembered in England a generation later as "the Great Battle". When reporting the battle, the "Anglo-Saxon Chronicle" abandons its usual terse style in favour of a heroic poem vaunting the great victory. In this the "hoary" Constantine, by now around 60 years of age, is said to have lost a son in the battle, a claim which the "Chronicle of the Kings of Alba" confirms. The "Annals of Clonmacnoise" give his name as Cellach. For all its fame, the site of the battle is uncertain and several sites have been advanced, with Bromborough on the Wirral the most favoured location.
Brunanburh, for all that it had been a famous and bloody battle, settled nothing. On 939 Æthelstan, the "pillar of the dignity of the western world" in the words of the "Annals of Ulster", died at Malmesbury. He was succeeded by his brother Edmund, then aged 18. Æthelstan's empire, seemingly made safe by the victory of Brunanburh, collapsed in little more than a year from his death when Amlaíb returned from Ireland and seized Northumbria and the Mercian Danelaw. Edmund spent the remainder of Constantín's reign rebuilding the empire.
For Constantine's last years as king there is only the meagre record of the "Chronicle of the Kings of Alba". The death of Æthelstan is reported, as are two others. The first of these, in 938, is that of Dubacan, mormaer of Angus or son of the mormaer. Unlike the report of 918, on this occasion the title mormaer is attached to a geographical area, but it is unknown whether the Angus of 938 was in any way similar to the later mormaerdom or earldom. The second death, entered with that of Æthelstan, is that of Eochaid mac Ailpín, who may, from his name, have been a kinsman of Constantín.
Abdication and posterity.
By the early 940s Constantine was an old man, perhaps more than 70 years of age. The kingdom of Alba was too new to be said to have a customary rule of succession, but Pictish and Irish precedents favoured an adult successor descended from Kenneth MacAlpin. Constantine's surviving son Indulf, probably baptised in 927, would have been too young to be a serious candidate for the kingship in the early 940s, and the obvious heir was Constantine's nephew, Malcolm I. As Malcolm was born no later than 901, by the 940s he was no longer a young man, and may have been impatient. Willingly or not—the 11th-century "Prophecy of Berchán", a verse history in the form of a supposed prophecy, states that it was not a voluntary decision—Constantine abdicated in 943 and entered a monastery, leaving the kingdom to Malcolm.
Although his retirement may have been involuntary, the "Life" of Cathróe of Metz and the "Prophecy of Berchán" portray Constantine as a devout king. The monastery which Constantine retired to, and where he is said to have been abbot, was probably that of St Andrews. This had been refounded in his reign and given to the reforming Céli Dé (Culdee) movement. The Céli Dé were subsequently to be entrusted with many monasteries throughout the kingdom of Alba until replaced in the 12th century by new orders imported from France.
Seven years later the "Chronicle of the Kings of Alba" says:[Malcolm I] plundered the English as far as the river Tees, and he seized a multitude of people and many herds of cattle: and the Scots called this the raid of Albidosorum, that is, Nainndisi. But others say that Constantine made this raid, asking of the king, Malcolm, that the kingship should be given to him for a week's time, so that he could visit the English. In fact, it was Malcolm who made the raid, but Constantine incited him, as I have said. Woolf suggests that the association of Constantine with the raid is a late addition, one derived from a now-lost saga or poem.
Constantine's death in 952 is recorded by the Irish annals, who enter it among ecclesiastics. His son Indulf would become king on Malcolm's death. The last of Constantine's certain descendants to be king in Alba was a great-grandson, Constantine III (Constantín mac Cuiléin). Another son had died at Brunanburh, and, according to John of Worcester, Amlaíb mac Gofraid was married to a daughter of Constantine. It is possible that Constantine had other children, but like the name of his wife, or wives, this has not been recorded.
The form of kingdom which appeared in Constantine's reign continued in much the same way until the Davidian Revolution in the 12th century. As with his ecclesiastical reforms, his political legacy was the creation of a new form of Scottish kingship that lasted for two centuries after his death.

</doc>
<doc id="7236" url="http://en.wikipedia.org/wiki?curid=7236" title="Constantine the Great">
Constantine the Great

Constantine the Great (; Greek: Κωνσταντῖνος ὁ Μέγας; 27 February c. 272 – 22 May 337), also known as Constantine I or Saint Constantine, was Roman Emperor from 306 to 337. Constantine was the son of Flavius Valerius Constantius, a Roman army officer, and his consort Helena. His father became "Caesar", the deputy emperor in the west in 293. Constantine was sent east, where he rose through the ranks to become a military tribune under the emperors Diocletian and Galerius. In 305, Constantius was raised to the rank of "Augustus", senior western emperor, and Constantine was recalled west to campaign under his father in Britannia. Acclaimed as emperor by the army at "Eburacum" (York) after his father's death in 306, Constantine emerged victorious in a series of civil wars against the emperors Maxentius and Licinius to become sole ruler of both west and east by 324.
As emperor, Constantine enacted many administrative, financial, social, and military reforms to strengthen the empire. The government was restructured and civil and military authority separated. A new gold coin, the solidus, was introduced to combat inflation. It would become the standard for Byzantine and European currencies for more than a thousand years. The first Roman emperor to claim conversion to Christianity, Constantine played an influential role in the proclamation of the Edict of Milan, which decreed tolerance for Christianity in the empire. He called the First Council of Nicaea in 325, at which the Nicene Creed was professed by Christians. In military matters, the Roman army was reorganised to consist of mobile field units and garrison soldiers capable of countering internal threats and barbarian invasions. Constantine pursued successful campaigns against the tribes on the Roman frontiers—the Franks, the Alamanni, the Goths, and the Sarmatians—even resettling territories abandoned by his predecessors during the turmoil of the previous century.
The age of Constantine marked a distinct epoch in the history of the Roman Empire. He built a new imperial residence at Byzantium and named it Constantinople after himself (the laudatory epithet of 'New Rome' came later, and was never an official title). It would later be the capital of what is now known as the Byzantine Empire for over one thousand years. Because of this, he is thought of as the founder of the Byzantine Empire. His more immediate political legacy was that, in leaving the empire to his sons, he replaced Diocletian's tetrarchy with the principle of dynastic succession. His reputation flourished during the lifetime of his children and centuries after his reign. The medieval church upheld him as a paragon of virtue while secular rulers invoked him as a prototype, a point of reference, and the symbol of imperial legitimacy and identity. Beginning with the renaissance, there were more critical appraisals of his reign due to the rediscovery of anti-Constantinian sources. Critics portrayed him as a despotic tyrant. Trends in modern and recent scholarship attempted to balance the extremes of previous scholarship.
Constantine—as the first Christian emperor—is a significant figure in the history of Christianity. The Church of the Holy Sepulchre, built on his orders at the purported site of Jesus' tomb in Jerusalem, became the holiest place in Christendom. The Papacy claimed temporal power through Constantine. He is venerated as a saint by Eastern Orthodox Christians, Byzantine Catholics, and Anglicans. The Eastern churches hold his memory in particular esteem, regarding Constantine as "isapostolos" or "equal-to-an-apostle".
Sources.
Constantine was a ruler of major historical importance, and he has always been a controversial figure. The fluctuations in Constantine's reputation reflect the nature of the ancient sources for his reign. These are abundant and detailed, but have been strongly influenced by the official propaganda of the period, and are often one-sided. There are no surviving histories or biographies dealing with Constantine's life and rule. The nearest replacement is Eusebius of Caesarea's "Vita Constantini", a work that is a mixture of eulogy and hagiography. Written between 335 and circa 339, the "Vita" extols Constantine's moral and religious virtues. The "Vita" creates a contentiously positive image of Constantine, and modern historians have frequently challenged its reliability. The fullest secular life of Constantine is the anonymous "Origo Constantini". A work of uncertain date, the "Origo" focuses on military and political events, to the neglect of cultural and religious matters.
Lactantius' "De Mortibus Persecutorum", a political Christian pamphlet on the reigns of Diocletian and the Tetrarchy, provides valuable but tendentious detail on Constantine's predecessors and early life. The ecclesiastical histories of Socrates, Sozomen, and Theodoret describe the ecclesiastic disputes of Constantine's later reign. Written during the reign of Theodosius II (408–50), a century after Constantine's reign, these ecclesiastic historians obscure the events and theologies of the Constantinian period through misdirection, misrepresentation and deliberate obscurity. The contemporary writings of the orthodox Christian Athanasius and the ecclesiastical history of the Arian Philostorgius also survive, though their biases are no less firm.
The epitomes of Aurelius Victor ("De Caesaribus"), Eutropius ("Breviarium"), Festus ("Breviarium"), and the anonymous author of the "Epitome de Caesaribus" offer compressed secular political and military histories of the period. Although not Christian, the epitomes paint a favorable image of Constantine, but omit reference to Constantine's religious policies. The "Panegyrici Latini", a collection of panegyrics from the late third and early fourth centuries, provide valuable information on the politics and ideology of the tetrarchic period and the early life of Constantine. Contemporary architecture, such as the Arch of Constantine in Rome and palaces in Gamzigrad and Córdoba, epigraphic remains, and the coinage of the era complement the literary sources.
Early life.
Flavius Valerius Constantinus, as he was originally named, was born in the city of Naissus, (today Niš, Serbia) part of the Dardania province of Moesia, on 27 February of an uncertain year, probably near 272. His father was Flavius Constantius, a native of Dardania province of Moesia (later Dacia Ripensis). Constantius was a tolerant and politically skilled man. Constantine probably spent little time with his father. Constantius was an officer in the Roman army, part of the Emperor Aurelian's imperial bodyguard. Constantius advanced through the ranks, earning the governorship of Dalmatia from Emperor Diocletian, another of Aurelian's companions from Illyricum, in 284 or 285. Constantine's mother was Helena, a Bithynian/Thracian woman of low social standing. It is uncertain whether she was legally married to Constantius or merely his concubine.
In July 285, Diocletian declared Maximian, another colleague from Illyricum, his co-emperor. Each emperor would have his own court, his own military and administrative faculties, and each would rule with a separate praetorian prefect as chief lieutenant. Maximian ruled in the West, from his capitals at Mediolanum (Milan, Italy) or Augusta Treverorum (Trier, Germany), while Diocletian ruled in the East, from Nicomedia (İzmit, Turkey). The division was merely pragmatic: the Empire was called "indivisible" in official panegyric, and both emperors could move freely throughout the Empire. In 288, Maximian appointed Constantius to serve as his praetorian prefect in Gaul. Constantius left Helena to marry Maximian's stepdaughter Theodora in 288 or 289.
Diocletian divided the Empire again in 293, appointing two Caesars (junior emperors) to rule over further subdivisions of East and West. Each would be subordinate to their respective Augustus (senior emperor) but would act with supreme authority in his assigned lands. This system would later be called the Tetrarchy. Diocletian's first appointee for the office of Caesar was Constantius; his second was Galerius, a native of Felix Romuliana. According to Lactantius, Galerius was a brutal, animalistic man. Although he shared the paganism of Rome's aristocracy, he seemed to them an alien figure, a semi-barbarian. On 1 March, Constantius was promoted to the office of Caesar, and dispatched to Gaul to fight the rebels Carausius and Allectus. In spite of meritocratic overtones, the Tetrarchy retained vestiges of hereditary privilege, and Constantine became the prime candidate for future appointment as Caesar as soon as his father took the position. Constantine went to the court of Diocletian, where he lived as his father's heir presumptive.
In the East.
Constantine received a formal education at Diocletian's court, where he learned Latin literature, Greek, and philosophy. The cultural environment in Nicomedia was open, fluid and socially mobile, and Constantine could mix with intellectuals both pagan and Christian. He may have attended the lectures of Lactantius, a Christian scholar of Latin in the city. Because Diocletian did not completely trust Constantius—none of the Tetrarchs fully trusted their colleagues—Constantine was held as something of a hostage, a tool to ensure Constantius's best behavior. Constantine was nonetheless a prominent member of the court: he fought for Diocletian and Galerius in Asia, and served in a variety of tribunates; he campaigned against barbarians on the Danube in 296, and fought the Persians under Diocletian in Syria (297) and under Galerius in Mesopotamia (298–99). By late 305, he had become a tribune of the first order, a "tribunus ordinis primi".
Constantine had returned to Nicomedia from the eastern front by the spring of 303, in time to witness the beginnings of Diocletian's "Great Persecution", the most severe persecution of Christians in Roman history. In late 302, Diocletian and Galerius sent a messenger to the oracle of Apollo at Didyma with an inquiry about Christians. Constantine could recall his presence at the palace when the messenger returned, when Diocletian accepted his court's demands for universal persecution. On 23 February 303, Diocletian ordered the destruction of Nicomedia's new church, condemned its scriptures to the flames, and had its treasures seized. In the months that followed, churches and scriptures were destroyed, Christians were deprived of official ranks, and priests were imprisoned.
It is unlikely that Constantine played any role in the persecution. In his later writings he would attempt to present himself as an opponent of Diocletian's "sanguinary edicts" against the "worshippers of God", but nothing indicates that he opposed it effectively at the time. Although no contemporary Christian challenged Constantine for his inaction during the persecutions, it remained a political liability throughout his life.
On 1 May 305, Diocletian, as a result of a debilitating sickness taken in the winter of 304–5, announced his resignation. In a parallel ceremony in Milan, Maximian did the same. Lactantius states that Galerius manipulated the weakened Diocletian into resigning, and forced him to accept Galerius' allies in the imperial succession. According to Lactantius, the crowd listening to Diocletian's resignation speech believed, until the very last moment, that Diocletian would choose Constantine and Maxentius (Maximian's son) as his successors. It was not to be: Constantius and Galerius were promoted to Augusti, while Severus and Maximin were appointed their Caesars respectively. Constantine and Maxentius were ignored.
Some of the ancient sources detail plots that Galerius made on Constantine's life in the months following Diocletian's abdication. They assert that Galerius assigned Constantine to lead an advance unit in a cavalry charge through a swamp on the middle Danube, made him enter into single combat with a lion, and attempted to kill him in hunts and wars. Constantine always emerged victorious: the lion emerged from the contest in a poorer condition than Constantine; Constantine returned to Nicomedia from the Danube with a Sarmatian captive to drop at Galerius' feet. It is uncertain how much these tales can be trusted.
In the West.
Constantine recognized the implicit danger in remaining at Galerius's court, where he was held as a virtual hostage. His career depended on being rescued by his father in the west. Constantius was quick to intervene. In the late spring or early summer of 305, Constantius requested leave for his son to help him campaign in Britain. After a long evening of drinking, Galerius granted the request. Constantine's later propaganda describes how he fled the court in the night, before Galerius could change his mind. He rode from post-house to post-house at high speed, hamstringing every horse in his wake. By the time Galerius awoke the following morning, Constantine had fled too far to be caught. Constantine joined his father in Gaul, at Bononia (Boulogne) before the summer of 305.
From Bononia they crossed the Channel to Britain and made their way to Eboracum (York), capital of the province of Britannia Secunda and home to a large military base. Constantine was able to spend a year in northern Britain at his father's side, campaigning against the Picts beyond Hadrian's Wall in the summer and autumn. Constantius's campaign, like that of Septimius Severus before it, probably advanced far into the north without achieving great success. Constantius had become severely sick over the course of his reign, and died on 25 July 306 in Eboracum (York). Before dying, he declared his support for raising Constantine to the rank of full Augustus. The Alamannic king Chrocus, a barbarian taken into service under Constantius, then proclaimed Constantine as Augustus. The troops loyal to Constantius' memory followed him in acclamation. Gaul and Britain quickly accepted his rule; Iberia, which had been in his father's domain for less than a year, rejected it.
Constantine sent Galerius an official notice of Constantius's death and his own acclamation. Along with the notice, he included a portrait of himself in the robes of an Augustus. The portrait was wreathed in bay. He requested recognition as heir to his father's throne, and passed off responsibility for his unlawful ascension on his army, claiming they had "forced it upon him". Galerius was put into a fury by the message; he almost set the portrait on fire. His advisers calmed him, and argued that outright denial of Constantine's claims would mean certain war. Galerius was compelled to compromise: he granted Constantine the title "Caesar" rather than "Augustus" (the latter office went to Severus instead). Wishing to make it clear that he alone gave Constantine legitimacy, Galerius personally sent Constantine the emperor's traditional purple robes. Constantine accepted the decision, knowing that it would remove doubts as to his legitimacy.
Early rule.
Constantine's share of the Empire consisted of Britain, Gaul, and Spain. He therefore commanded one of the largest Roman armies, stationed along the important Rhine frontier. After his promotion to emperor, Constantine remained in Britain, driving back the tribes of the Picts and secured his control in the northwestern dioceses. He completed the reconstruction of military bases begun under his father's rule, and ordered the repair of the region's roadways. He soon left for Augusta Treverorum (Trier) in Gaul, the Tetrarchic capital of the northwestern Roman Empire. The Franks, after learning of Constantine's acclamation, invaded Gaul across the lower Rhine over the winter of 306–7. Constantine drove them back beyond the Rhine and captured two of their kings, Ascaric and Merogaisus. The kings and their soldiers were fed to the beasts of Trier's amphitheater in the "adventus" (arrival) celebrations that followed.
Constantine began a major expansion of Trier. He strengthened the circuit wall around the city with military towers and fortified gates, and began building a palace complex in the northeastern part of the city. To the south of his palace, he ordered the construction of a large formal audience hall, and a massive imperial bathhouse. Constantine sponsored many building projects across Gaul during his tenure as emperor of the West, especially in Augustodunum (Autun) and Arelate (Arles). According to Lactantius, Constantine followed his father in following a tolerant policy towards Christianity. Although not yet a Christian, he probably judged it a more sensible policy than open persecution, and a way to distinguish himself from the "great persecutor", Galerius. Constantine decreed a formal end to persecution, and returned to Christians all they had lost during the persecutions.
Because Constantine was still largely untried and had a hint of illegitimacy about him, he relied on his father's reputation in his early propaganda: the earliest panegyrics to Constantine give as much coverage to his father's deeds as to those of Constantine himself. Constantine's military skill and building projects soon gave the panegyrist the opportunity to comment favorably on the similarities between father and son, and Eusebius remarked that Constantine was a "renewal, as it were, in his own person, of his father's life and reign". Constantinian coinage, sculpture and oratory also shows a new tendency for disdain towards the "barbarians" beyond the frontiers. After Constantine's victory over the Alemanni, he minted a coin issue depicting weeping and begging Alemannic tribesmen—"The Alemanni conquered"—beneath the phrase "Romans' rejoicing". There was little sympathy for these enemies. As his panegyrist declared: "It is a stupid clemency that spares the conquered foe."
Maxentius' rebellion.
Following Galerius' recognition of Constantine as caesar, Constantine's portrait was brought to Rome, as was customary. Maxentius mocked the portrait's subject as the son of a harlot, and lamented his own powerlessness. Maxentius, envious of Constantine's authority, seized the title of emperor on 28 October 306. Galerius refused to recognize him, but failed to unseat him. Galerius sent Severus against Maxentius, but during the campaign, Severus' armies, previously under command of Maxentius' father Maximian, defected, and Severus was seized and imprisoned. Maximian, brought out of retirement by his son's rebellion, left for Gaul to confer with Constantine in late 307. He offered to marry his daughter Fausta to Constantine, and elevate him to Augustan rank. In return, Constantine would reaffirm the old family alliance between Maximian and Constantius, and offer support to Maxentius' cause in Italy. Constantine accepted, and married Fausta in Trier in late summer 307. Constantine now gave Maxentius his meagre support, offering Maxentius political recognition.
Constantine remained aloof from the Italian conflict, however. Over the spring and summer of 307, he had left Gaul for Britain to avoid any involvement in the Italian turmoil; now, instead of giving Maxentius military aid, he sent his troops against Germanic tribes along the Rhine. In 308, he raided the territory of the Bructeri, and made a bridge across the Rhine at Colonia Agrippinensium (Cologne). In 310, he marched to the northern Rhine and fought the Franks. When not campaigning, he toured his lands advertising his benevolence, and supporting the economy and the arts. His refusal to participate in the war increased his popularity among his people, and strengthened his power base in the West. Maximian returned to Rome in the winter of 307–8, but soon fell out with his son. In early 308, after a failed attempt to usurp Maxentius' title, Maximian returned to Constantine's court.
On 11 November 308, Galerius called a general council at the military city of Carnuntum (Petronell-Carnuntum, Austria) to resolve the instability in the western provinces. In attendance were Diocletian, briefly returned from retirement, Galerius, and Maximian. Maximian was forced to abdicate again and Constantine was again demoted to Caesar. Licinius, one of Galerius' old military companions, was appointed Augustus of the west. The new system did not last long: Constantine refused to accept the demotion, and continued to style himself as Augustus on his coinage, even as other members of the Tetrarchy referred to him as a Caesar on theirs. Maximinus Daia was frustrated that he had been passed over for promotion while the newcomer Licinius had been raised to the office of Augustus, and demanded that Galerius promote him. Galerius offered to call both Maximinus and Constantine "sons of the Augusti", but neither accepted the new title. By the spring of 310, Galerius was referring to both men as Augusti.
Maximian's rebellion.
In 310, a dispossessed Maximian rebelled against Constantine while Constantine was away campaigning against the Franks. Maximian had been sent south to Arles with a contingent of Constantine's army, in preparation for any attacks by Maxentius in southern Gaul. He announced that Constantine was dead, and took up the imperial purple. In spite of a large donative pledge to any who would support him as emperor, most of Constantine's army remained loyal to their emperor, and Maximian was soon compelled to leave. Constantine soon heard of the rebellion, abandoned his campaign against the Franks, and marched his army up the Rhine. At Cabillunum (Chalon-sur-Saône), he moved his troops onto waiting boats to row down the slow waters of the Saône to the quicker waters of the Rhone. He disembarked at Lugdunum (Lyon). Maximian fled to Massilia (Marseille), a town better able to withstand a long siege than Arles. It made little difference, however, as loyal citizens opened the rear gates to Constantine. Maximian was captured and reproved for his crimes. Constantine granted some clemency, but strongly encouraged his suicide. In July 310, Maximian hanged himself.
In spite of the earlier rupture in their relations, Maxentius was eager to present himself as his father's devoted son after his death. He began minting coins with his father's deified image, proclaiming his desire to avenge Maximian's death. Constantine initially presented the suicide as an unfortunate family tragedy. By 311, however, he was spreading another version. According to this, after Constantine had pardoned him, Maximian planned to murder Constantine in his sleep. Fausta learned of the plot and warned Constantine, who put a eunuch in his own place in bed. Maximian was apprehended when he killed the eunuch and was offered suicide, which he accepted. Along with using propaganda, Constantine instituted a "damnatio memoriae" on Maximian, destroying all inscriptions referring to him and eliminating any public work bearing his image.
The death of Maximian required a shift in Constantine's public image. He could no longer rely on his connection to the elder emperor Maximian, and needed a new source of legitimacy. In a speech delivered in Gaul on 25 July 310, the anonymous orator reveals a previously unknown dynastic connection to Claudius II, a third-century emperor famed for defeating the Goths and restoring order to the empire. Breaking away from tetrarchic models, the speech emphasizes Constantine's ancestral prerogative to rule, rather than principles of imperial equality. The new ideology expressed in the speech made Galerius and Maximian irrelevant to Constantine's right to rule. Indeed, the orator emphasizes ancestry to the exclusion of all other factors: "No chance agreement of men, nor some unexpected consequence of favor, made you emperor," the orator declares to Constantine.
The oration also moves away from the religious ideology of the Tetrarchy, with its focus on twin dynasties of Jupiter and Hercules. Instead, the orator proclaims that Constantine experienced a divine vision of Apollo and Victory granting him laurel wreaths of health and a long reign. In the likeness of Apollo Constantine recognized himself as the saving figure to whom would be granted "rule of the whole world", as the poet Virgil had once foretold. The oration's religious shift is paralleled by a similar shift in Constantine's coinage. In his early reign, the coinage of Constantine advertised Mars as his patron. From 310 on, Mars was replaced by Sol Invictus, a god conventionally identified with Apollo. There is little reason to believe that either the dynastic connection or the divine vision are anything other than fiction, but their proclamation strengthened Constantine's claims to legitimacy and increased his popularity among the citizens of Gaul.
Civil wars.
War against Maxentius.
By the middle of 310, Galerius had become too ill to involve himself in imperial politics. His final act survives: a letter to the provincials posted in Nicomedia on 30 April 311, proclaiming an end to the persecutions, and the resumption of religious toleration. He died soon after the edict's proclamation, destroying what little remained of the tetrarchy. Maximinus mobilized against Licinius, and seized Asia Minor. A hasty peace was signed on a boat in the middle of the Bosphorus. While Constantine toured Britain and Gaul, Maxentius prepared for war. He fortified northern Italy, and strengthened his support in the Christian community by allowing it to elect a new Bishop of Rome, Eusebius.
Maxentius' rule was nevertheless insecure. His early support dissolved in the wake of heightened tax rates and depressed trade; riots broke out in Rome and Carthage; and Domitius Alexander was able to briefly usurp his authority in Africa. By 312, he was a man barely tolerated, not one actively supported, even among Christian Italians. In the summer of 311, Maxentius mobilized against Constantine while Licinius was occupied with affairs in the East. He declared war on Constantine, vowing to avenge his father's "murder". To prevent Maxentius from forming an alliance against him with Licinius, Constantine forged his own alliance with Licinius over the winter of 311–12, and offered him his sister Constantia in marriage. Maximin considered Constantine's arrangement with Licinius an affront to his authority. In response, he sent ambassadors to Rome, offering political recognition to Maxentius in exchange for a military support. Maxentius accepted. According to Eusebius, inter-regional travel became impossible, and there was military buildup everywhere. There was "not a place where people were not expecting the onset of hostilities every day".
Constantine's advisers and generals cautioned against preemptive attack on Maxentius; even his soothsayers recommended against it, stating that the sacrifices had produced unfavorable omens. Constantine, with a spirit that left a deep impression on his followers, inspiring some to believe that he had some form of supernatural guidance, ignored all these cautions. Early in the spring of 312, Constantine crossed the Cottian Alps with a quarter of his army, a force numbering about 40,000. The first town his army encountered was Segusium (Susa, Italy), a heavily fortified town that shut its gates to him. Constantine ordered his men to set fire to its gates and scale its walls. He took the town quickly. Constantine ordered his troops not to loot the town, and advanced with them into northern Italy.
At the approach to the west of the important city of Augusta Taurinorum (Turin, Italy), Constantine met a large force of heavily armed Maxentian cavalry. In the ensuing battle Constantine's army encircled Maxentius' cavalry, flanked them with his own cavalry, and dismounted them with blows from his soldiers' iron-tipped clubs. Constantine's armies emerged victorious. Turin refused to give refuge to Maxentius' retreating forces, opening its gates to Constantine instead. Other cities of the north Italian plain sent Constantine embassies of congratulation for his victory. He moved on to Milan, where he was met with open gates and jubilant rejoicing. Constantine rested his army in Milan until mid-summer 312, when he moved on to Brixia (Brescia).
Brescia's army was easily dispersed, and Constantine quickly advanced to Verona, where a large Maxentian force was camped. Ruricius Pompeianus, general of the Veronese forces and Maxentius' praetorian prefect, was in a strong defensive position, since the town was surrounded on three sides by the Adige. Constantine sent a small force north of the town in an attempt to cross the river unnoticed. Ruricius sent a large detachment to counter Constantine's expeditionary force, but was defeated. Constantine's forces successfully surrounded the town and laid siege. Ruricius gave Constantine the slip and returned with a larger force to oppose Constantine. Constantine refused to let up on the siege, and sent only a small force to oppose him. In the desperately fought encounter that followed, Ruricius was killed and his army destroyed. Verona surrendered soon afterwards, followed by Aquileia, Mutina (Modena), and Ravenna. The road to Rome was now wide open to Constantine.
Maxentius prepared for the same type of war he had waged against Severus and Galerius: he sat in Rome and prepared for a siege. He still controlled Rome's praetorian guards, was well-stocked with African grain, and was surrounded on all sides by the seemingly impregnable Aurelian Walls. He ordered all bridges across the Tiber cut, reportedly on the counsel of the gods, and left the rest of central Italy undefended; Constantine secured that region's support without challenge. Constantine progressed slowly along the "Via Flaminia", allowing the weakness of Maxentius to draw his regime further into turmoil. Maxentius' support continued to weaken: at chariot races on 27 October, the crowd openly taunted Maxentius, shouting that Constantine was invincible. Maxentius, no longer certain that he would emerge from a siege victorious, built a temporary boat bridge across the Tiber in preparation for a field battle against Constantine. On 28 October 312, the sixth anniversary of his reign, he approached the keepers of the Sibylline Books for guidance. The keepers prophesied that, on that very day, "the enemy of the Romans" would die. Maxentius advanced north to meet Constantine in battle.
Constantine's army adopts the Christian cross.
Maxentius organized his forces—still twice the size of Constantine's—in long lines facing the battle plain, with their backs to the river. Constantine's army arrived at the field bearing unfamiliar symbols on either its standards or its soldiers' shields. According to Lactantius, Constantine was visited by a dream the night before the battle, wherein he was advised "to mark the heavenly sign of God on the shields of his soldiers ... by means of a slanted letter X with the top of its head bent round, he marked Christ on their shields." Eusebius describes another version, where, while marching at midday, "he saw with his own eyes in the heavens a trophy of the cross arising from the light of the sun, carrying the message, "In Hoc Signo Vinces" or "with this sign, you will conquer"; in Eusebius's account, Constantine had a dream the following night, in which Christ appeared with the same heavenly sign, and told him to make a standard, the "labarum", for his army in that form. Eusebius is vague about when and where these events took place, but it enters his narrative before the war against Maxentius begins. Eusebius describes the sign as Chi (Χ) traversed by Rho (Ρ): ☧, a symbol representing the first two letters of the Greek spelling of the word "Christos" or Christ. In 315 a medallion was issued at Ticinum showing Constantine wearing a helmet emblazoned with the "Chi Rho", and coins issued at Siscia in 317/18 repeat the image. The figure was otherwise rare, however, and is uncommon in imperial iconography and propaganda before the 320s.
Constantine deployed his own forces along the whole length of Maxentius' line. He ordered his cavalry to charge, and they broke Maxentius' cavalry. He then sent his infantry against Maxentius' infantry, pushing many into the Tiber where they were slaughtered and drowned. The battle was brief: Maxentius' troops were broken before the first charge. Maxentius' horse guards and praetorians initially held their position, but broke under the force of a Constantinian cavalry charge; they also broke ranks and fled to the river. Maxentius rode with them, and attempted to cross the bridge of boats, but he was pushed by the mass of his fleeing soldiers into the Tiber, and drowned.
In Rome.
Constantine entered Rome on 29 October. He staged a grand "adventus" in the city, and was met with popular jubilation. Maxentius' body was fished out of the Tiber and decapitated. His head was paraded through the streets for all to see. After the ceremonies, Maxentius' disembodied head was sent to Carthage; at this Carthage would offer no further resistance. Unlike his predecessors, Constantine neglected to make the trip to the Capitoline Hill and perform customary sacrifices at the Temple of Jupiter. He did, however, choose to honor the Senatorial Curia with a visit, where he promised to restore its ancestral privileges and give it a secure role in his reformed government: there would be no revenge against Maxentius' supporters. In response, the Senate decreed him "title of the first name", which meant his name would be listed first in all official documents, and acclaimed him as "the greatest Augustus". He issued decrees returning property lost under Maxentius, recalling political exiles, and releasing Maxentius' imprisoned opponents.
An extensive propaganda campaign followed, during which Maxentius' image was systematically purged from all public places. Maxentius was written up as a "tyrant", and set against an idealized image of the "liberator", Constantine. Eusebius, in his later works, is the best representative of this strand of Constantinian propaganda. Maxentius' rescripts were declared invalid, and the honors Maxentius had granted to leaders of the Senate were invalidated. Constantine also attempted to remove Maxentius' influence on Rome's urban landscape. All structures built by Maxentius were re-dedicated to Constantine, including the Temple of Romulus and the Basilica of Maxentius. At the focal point of the basilica, a stone statue of Constantine holding the Christian "labarum" in its hand was erected. Its inscription bore the message the statue had already made clear: By this sign Constantine had freed Rome from the yoke of the tyrant.
Where he did not overwrite Maxentius' achievements, Constantine upstaged them: the Circus Maximus was redeveloped so that its total seating capacity was twenty-five times larger than that of Maxentius' racing complex on the Via Appia. Maxentius' strongest supporters in the military were neutralized when the Praetorian Guard and Imperial Horse Guard ("equites singulares") were disbanded. Their tombstones were ground up and put to use in a basilica on the Via Labicana. On 9 November 312, barely two weeks after Constantine captured the city, the former base of the Imperial Horse Guard was chosen for redevelopment into the Lateran Basilica. The Legio II Parthica was removed from Alba (Albano Laziale), and the remainder of Maxentius' armies were sent to do frontier duty on the Rhine.
Wars against Licinius.
In the following years, Constantine gradually consolidated his military superiority over his rivals in the crumbling Tetrarchy. In 313, he met Licinius in Milan to secure their alliance by the marriage of Licinius and Constantine's half-sister Constantia. During this meeting, the emperors agreed on the so-called Edict of Milan,
officially granting full tolerance to Christianity and all religions in the Empire.
The document had special benefits for Christians, legalizing their religion and granting them restoration for all property seized during Diocletian's persecution. It repudiates past methods of religious coercion and used only general terms to refer to the divine sphere—"Divinity" and "Supreme Divinity", "summa divinitas".
The conference was cut short, however, when news reached Licinius that his rival Maximin had crossed the Bosporus and invaded European territory. Licinius departed and eventually defeated Maximin, gaining control over the entire eastern half of the Roman Empire. Relations between the two remaining emperors deteriorated, as Constantine suffered an assassination attempt at the hands of a character that Licinius wanted elevated to the rank of Caesar; Licinius, for his part had Constantine's statues in Emona destroyed.
In either 314 or 316 the two Augusti fought against one another at the Battle of Cibalae, with Constantine being victorious. They clashed again at the Battle of Mardia in 317, and agreed to a settlement in which Constantine's sons Crispus and Constantine II, and Licinius' son Licinianus were made "caesars".
After this arrangement, Constantine ruled the dioceses of Pannonia and Macedonia and took residence at Sirmium, from whence he could wage war on the Goths and Sarmatians in 322, and on the Goths in 323.
In the year 320, Licinius allegedly reneged on the religious freedom promised by the Edict of Milan in 313 and began to oppress Christians anew,
generally without bloodshed, but resorting to confiscations and sacking of Christian office-holders. Although this characterization of Licinius as anti-Christian is somewhat doubtful, the fact is that he seems to have been far less open in his support of Christianity than Constantine. Therefore Licinius was prone to see the Church as a force more loyal to Constantine than to the Imperial system in general - the explanation offered by the Church historian Sozomen.
This dubious arrangement eventually became a challenge to Constantine in the West, climaxing in the great civil war of 324. Licinius, aided by Goth mercenaries, represented the past and the ancient Pagan faiths. Constantine and his Franks marched under the standard of the "labarum", and both sides saw the battle in religious terms. Outnumbered, but fired by their zeal, Constantine's army emerged victorious in the Battle of Adrianople. Licinius fled across the Bosphorus and appointed Martius Martinianus, the commander of his bodyguard, as Caesar, but Constantine next won the Battle of the Hellespont, and finally the Battle of Chrysopolis on 18 September 324.
Licinius and Martinianus surrendered to Constantine at Nicomedia on the promise their lives would be spared: they were sent to live as private citizens in Thessalonica and Cappadocia respectively, but in 325 Constantine accused Licinius of plotting against him and had them both arrested and hanged; Licinius's son (the son of Constantine's half-sister) was also killed.
Thus Constantine became the sole emperor of the Roman Empire.
Later rule.
Foundation of Constantinople.
Licinius' defeat came to represent the defeat of a rival center of Pagan and Greek-speaking political activity in the East, as opposed to the Christian and Latin-speaking Rome, and it was proposed that a new Eastern capital should represent the integration of the East into the Roman Empire as a whole, as a center of learning, prosperity, and cultural preservation for the whole of the Eastern Roman Empire. Among the various locations proposed for this alternative capital, Constantine appears to have toyed earlier with Serdica (present-day Sofia), as he was reported saying that "Serdica is my Rome". Sirmium and Thessalonica were also considered. Eventually, however, Constantine decided to work on the Greek city of Byzantium, which offered the advantage of having already been extensively rebuilt on Roman patterns of urbanism, during the preceding century, by Septimius Severus and Caracalla, who had already acknowledged its strategic importance. The city was thus founded in 324, dedicated on 11 May 330 and renamed "Constantinopolis" ("Constantine's City" or Constantinople in English). Special commemorative coins were issued in 330 to honor the event. The new city was protected by the relics of the True Cross, the Rod of Moses and other holy relics, though a cameo now at the Hermitage Museum also represented Constantine crowned by the tyche of the new city. The figures of old gods were either replaced or assimilated into a framework of Christian symbolism. Constantine built the new Church of the Holy Apostles on the site of a temple to Aphrodite. Generations later there was the story that a divine vision led Constantine to this spot, and an angel no one else could see, led him on a circuit of the new walls. The capital would often be compared to the 'old' Rome as "Nova Roma Constantinopolitana", the "New Rome of Constantinople".
Religious policy.
Constantine was the first emperor to stop Christian persecutions and to legalise Christianity along with all the other cults in the Roman Empire.
In February 313, Constantine met with Licinius in Milan, where they developed the Edict of Milan. The edict stated that Christians should be allowed to follow the faith without oppression. This removed penalties for professing Christianity, under which many had been martyred previously, and returned confiscated Church property. The edict protected from religious persecution not only Christians but all religions, allowing anyone to worship whichever deity they chose. A similar edict had been issued in 311 by Galerius, then senior emperor of the Tetrarchy; Galerius' edict granted Christians the right to practise their religion but did not restore any property to them. The Edict of Milan included several clauses which stated that all confiscated churches would be returned as well as other provisions for previously persecuted Christians.
Scholars debate whether Constantine adopted his mother St. Helena's Christianity in his youth, or whether he adopted it gradually over the course of his life. Constantine would retain the title of "pontifex maximus" until his death, a title emperors bore as heads of the pagan priesthood, as would his Christian successors on to Gratian ("r". 375–83). According to Christian writers, Constantine was over 40 when he finally declared himself a Christian, writing to Christians to make clear that he believed he owed his successes to the protection of the Christian High God alone. Throughout his rule, Constantine supported the Church financially, built basilicas, granted privileges to clergy (e.g. exemption from certain taxes), promoted Christians to high office, and returned property confiscated during the Diocletianic persecution. His most famous building projects include the Church of the Holy Sepulchre, and Old Saint Peter's Basilica.
However, Constantine certainly did not patronize Christianity alone. After gaining victory in the Battle of the Milvian Bridge (312), a triumphal arch—the Arch of Constantine—was built (315) to celebrate his triumph. The arch is decorated with images of the goddess Victoria. At the time of its dedication, sacrifices to gods like Apollo, Diana, and Hercules were made. Absent from the Arch are any depictions of Christian symbolism. However, as the Arch was commissioned by the Senate, the absence of Christian symbols may reflect the role of the Curia at the time as a pagan redoubt.
Later in 321, Constantine instructed that Christians and non-Christians should be united in observing the venerable day of the sun, referring to the sun-worship that Aurelian had established as an official cult. Furthermore, and long after his oft alleged conversion to Christianity, Constantine's coinage continued to carry the symbols of the sun. Even after the pagan gods had disappeared from the coinage, Christian symbols appeared only as Constantine's "personal" attributes: the chi rho between his hands or on his labarum, but never on the coin itself. Even when Constantine dedicated the new capital of Constantinople, which became the seat of Byzantine Christianity for a millennium, he did so wearing the Apollonian sun-rayed Diadem; no Christian symbols were present at this dedication.
The reign of Constantine established a precedent for the position of the emperor as having great influence and ultimate regulatory authority within the religious discussions involving the early Christian councils of that time, e.g., most notably the dispute over Arianism, and the nature of God. Constantine himself disliked the risks to societal stability that religious disputes and controversies brought with them, preferring where possible to establish an orthodoxy. One way in which Constantine used his influence over the early Church councils was to seek to establish a consensus over the oft debated and argued issue over the nature of God.
Most notably, from 313 to 316 bishops in North Africa struggled with other Christian bishops who had been ordained by Donatus in opposition to Caecilian. The African bishops could not come to terms and the Donatists asked Constantine to act as a judge in the dispute. Three regional Church councils and another trial before Constantine all ruled against Donatus and the Donatism movement in North Africa. In 317 Constantine issued an edict to confiscate Donatist church property and to send Donatist clergy into exile. More significantly, in 325 he summoned the Council of Nicaea, effectively the first Ecumenical Council (unless the Council of Jerusalem is so classified). The Council of Nicaea is most known for its dealing with Arianism and for instituting the Nicene Creed.
Constantine enforced the prohibition of the First Council of Nicaea against celebrating the Lord's Supper on the day before the Jewish Passover (14 "Nisan") (see Quartodecimanism and Easter controversy). This marked a definite break of Christianity from the Judaic tradition. From then on the Roman Julian Calendar, a solar calendar, was given precedence over the lunar Hebrew Calendar among the Christian churches of the Roman Empire.
Constantine made new laws regarding the Jews. They were forbidden to own Christian slaves or to circumcise their slaves.
Administrative reforms.
Beginning in the mid-3rd century the emperors began to favor members of the equestrian order over senators, who had had a monopoly on the most important offices of state. Senators were stripped of the command of legions and most provincial governorships (as it was felt that they lacked the specialized military upbringing needed in an age of acute defense needs), such posts being given to equestrians by Diocletian and his colleagues—following a practice enforced piecemeal by their predecessors. The emperors however, still needed the talents and the help of the very rich, who were relied on to maintain social order and cohesion by means of a web of powerful influence and contacts at all levels. Exclusion of the old senatorial aristocracy threatened this arrangement.
In 326, Constantine reversed this pro-equestrian trend, raising many administrative positions to senatorial rank and thus opening these offices to the old aristocracy, and at the same time elevating the rank of already existing equestrians office-holders to senator, degrading the equestrian order —at least as a bureaucratic rank —in the process, so that by the end of the 4th century the title of "perfectissimus" was granted only to mid-low officials.
By the new Constantinian arrangement, one could become a senator, either by being elected praetor or (in most cases) by fulfilling a function of senatorial rank: from then on, holding of actual power and social status were melded together into a joint imperial hierarchy. At the same time, Constantine gained with this the support of the old nobility, as the Senate was allowed itself to elect praetors and quaestors, in place of the usual practice of the emperors directly creating new magistrates ("adlectio"). In one inscription in honor of city prefect (336–37) Ceionius Rufus Albinus, it was written that Constantine had restored the Senate "the "auctoritas" it had lost at Caesar's time".
The Senate as a body remained devoid of any significant power; nevertheless, the senators, who had been marginalized as potential holders of imperial functions during the 3rd century, could now dispute such positions alongside more upstart bureaucrats. Some modern historians see in those administrative reforms an attempt by Constantine at reintegrating the senatorial order into the imperial administrative elite to counter the possibility of alienating pagan senators from a Christianized imperial rule; however, such an interpretation remains conjectural, given the fact that we do not have the precise numbers about pre-Constantine conversions to Christianity in the old senatorial milieu—some historians suggesting that early conversions among the old aristocracy were more numerous than previously supposed.
Constantine's reforms had to do only with the civilian administration: the military chiefs, who since the Crisis of the Third Century had risen from the ranks, remained outside the senate, in which they were included only by Constantine's children.
Monetary reforms.
After the runaway inflation of the third century, associated with the production of fiat money to pay for public expenses, Diocletian had tried unsuccessfully to reestablish trustworthy minting of silver and billon coins. The failure of the various Diocletianic attempts at the restoration of a functioning silver coin resided in the fact that the silver currency was overvalued in terms of its actual metal content, and therefore could only circulate at much discounted rates. Minting of the Diocletianic "pure" silver "argenteus" ceased, therefore, soon after 305, while the billon currency continued to be used until the 360s. From the early 300s on, Constantine forsook any attempts at restoring the silver currency, preferring instead to concentrate on minting large quantities of good standard gold pieces—the solidus, 72 of which made a pound of gold. New (and highly debased) silver pieces would continue to be issued during Constantine's later reign and after his death, in a continuous process of retariffing, until this billon minting eventually ceased, "de jure", in 367, with the silver piece being "de facto" continued by various denominations of bronze coins, the most important being the "centenionalis". These bronze pieces continued to be devalued, assuring the possibility of keeping fiduciary minting alongside a gold standard. The anonymous author of the possibly contemporary treatise on military affairs "De Rebus Bellicis" held that, as a consequence of this monetary policy, the rift between classes widened: the rich benefited from the stability in purchasing power of the gold piece, while the poor had to cope with ever-degrading bronze pieces. Later emperors like Julian the Apostate tried to present themselves as advocates of the "humiles" by insisting on trustworthy mintings of the bronze currency.
Constantine's monetary policy were closely associated with his religious ones, in that increased minting was associated with measures of confiscation—taken since 331 and closed in 336—of all gold, silver and bronze statues from pagan temples, who were declared as imperial property and, as such, as monetary assets. Two imperial commissioners for each province had the task of getting hold of the statues and having them melded for immediate minting—with the exception of a number of bronze statues who were used as public monuments for the beautification of the new capital in Constantinople.
Executions of Crispus and Fausta.
On some date between 15 May and 17 June 326, Constantine had his eldest son Crispus, by Minervina, seized and put to death by "cold poison" at Pola (Pula, Croatia). In July, Constantine had his wife, the Empress Fausta, killed at the behest of his mother, Helena. Fausta was left to die in an over-heated bath. Their names were wiped from the face of many inscriptions, references to their lives in the literary record were erased, and the memory of both was condemned. Eusebius, for example, edited praise of Crispus out of later copies of his "Historia Ecclesiastica", and his "Vita Constantini" contains no mention of Fausta or Crispus at all. Few ancient sources are willing to discuss possible motives for the events; those few that do, offer unconvincing rationales, are of later provenance, and are generally unreliable. At the time of the executions, it was commonly believed that the Empress Fausta was either in an illicit relationship with Crispus, or was spreading rumors to that effect. A popular myth arose, modified to allude to Hippolytus–Phaedra legend, with the suggestion that Constantine killed Crispus and Fausta for their immoralities. One source, the largely fictional "Passion of Artemius", probably penned in the eighth century by John of Damascus, makes the legendary connection explicit. As an interpretation of the executions, the myth rests on only "the slimmest of evidence": sources that allude to the relationship between Crispus and Fausta are late and unreliable, and the modern suggestion that Constantine's "godly" edicts of 326 and the irregularities of Crispus are somehow connected rests on no evidence at all.
Although Constantine created his apparent heirs "Caesars", following a pattern established by Diocletian, he gave his creations a hereditary character, alien to the tetrarchic system: Constantine's Caesars were to be kept in the hope of ascending to Empire, and entirely subordinated to their Augustus, as long as he was alive. Therefore, an alternative explanation for the execution of Crispus was, perhaps, Constantine's desire to keep a firm grip on his prospective heirs, this — and Fausta's desire for having her sons inheriting instead of their half-brother — being reason enough for killing Crispus; the subsequent execution of Fausta, however, was probably meant as a reminder to her children that Constantine would not hesitate in "killing his own relatives when he felt this was necessary".
Later campaigns.
Constantine considered Constantinople as his capital and permanent residence. He lived there for a good portion of his later life. He rebuilt Trajan's bridge across the Danube, in hopes of reconquering Dacia, a province that had been abandoned under Aurelian. In the late winter of 332, Constantine campaigned with the Sarmatians against the Goths. The weather and lack of food cost the Goths dearly: reportedly, nearly one hundred thousand died before they submitted to Rome. In 334, after Sarmatian commoners had overthrown their leaders, Constantine led a campaign against the tribe. He won a victory in the war and extended his control over the region, as remains of camps and fortifications in the region indicate. Constantine resettled some Sarmatian exiles as farmers in Illyrian and Roman districts, and conscripted the rest into the army. Constantine took the title "Dacicus maximus" in 336.
In the last years of his life Constantine made plans for a campaign against Persia. In a letter written to the king of Persia, Shapur, Constantine had asserted his patronage over Persia's Christian subjects and urged Shapur to treat them well. The letter is undatable. In response to border raids, Constantine sent Constantius to guard the eastern frontier in 335. In 336, prince Narseh invaded Armenia (a Christian kingdom since 301) and installed a Persian client on the throne. Constantine then resolved to campaign against Persia himself. He treated the war as a Christian crusade, calling for bishops to accompany the army and commissioning a tent in the shape of a church to follow him everywhere. Constantine planned to be baptized in the Jordan River before crossing into Persia. Persian diplomats came to Constantinople over the winter of 336–7, seeking peace, but Constantine turned them away. The campaign was called off however, when Constantine fell sick in the spring of 337.
Sickness and death.
Constantine had known death would soon come. Within the Church of the Holy Apostles, Constantine had secretly prepared a final resting-place for himself. It came sooner than he had expected. Soon after the Feast of Easter 337, Constantine fell seriously ill. He left Constantinople for the hot baths near his mother's city of Helenopolis (Altinova), on the southern shores of the Gulf of İzmit. There, in a church his mother built in honor of Lucian the Apostle, he prayed, and there he realized that he was dying. Seeking purification, he became a catechumen, and attempted a return to Constantinople, making it only as far as a suburb of Nicomedia. He summoned the bishops, and told them of his hope to be baptized in the River Jordan, where Christ was written to have been baptized. He requested the baptism right away, promising to live a more Christian life should he live through his illness. The bishops, Eusebius records, "performed the sacred ceremonies according to custom". He chose the Arianizing bishop Eusebius of Nicomedia, bishop of the city where he lay dying, as his baptizer. In postponing his baptism, he followed one custom at the time which postponed baptism until after infancy. It has been thought that Constantine put off baptism as long as he did so as to be absolved from as much of his sin as possible. Constantine died soon after at a suburban villa called Achyron, on the last day of the fifty-day festival of Pentecost directly following Pascha (or Easter), on 22 May 337.
Although Constantine's death follows the conclusion of the Persian campaign in Eusebius's account, most other sources report his death as occurring in its middle. Emperor Julian (a nephew of Constantine), writing in the mid-350s, observes that the Sassanians escaped punishment for their ill-deeds, because Constantine died "in the middle of his preparations for war". Similar accounts are given in the "Origo Constantini", an anonymous document composed while Constantine was still living, and which has Constantine dying in Nicomedia; the "Historiae abbreviatae" of Sextus Aurelius Victor, written in 361, which has Constantine dying at an estate near Nicomedia called Achyrona while marching against the Persians; and the "Breviarium" of Eutropius, a handbook compiled in 369 for the Emperor Valens, which has Constantine dying in a nameless state villa in Nicomedia. From these and other accounts, some have concluded that Eusebius's "Vita" was edited to defend Constantine's reputation against what Eusebius saw as a less congenial version of the campaign.
Following his death, his body was transferred to Constantinople and buried in the Church of the Holy Apostles there. He was succeeded by his three sons born of Fausta, Constantine II, Constantius II and Constans. A number of relatives were killed by followers of Constantius, notably Constantine's nephews Dalmatius (who held the rank of Caesar) and Hannibalianus, presumably to eliminate possible contenders to an already complicated succession. He also had two daughters, Constantina and Helena, wife of Emperor Julian.
Legacy.
Although he earned his honorific of "The Great" ("Μέγας") from Christian historians long after he had died, he could have claimed the title on his military achievements and victories alone. Besides reuniting the Empire under one emperor, Constantine won major victories over the Franks and Alamanni in 306–8, the Franks again in 313–14, the Goths in 332 and the Sarmatians in 334. By 336, Constantine had reoccupied most of the long-lost province of Dacia, which Aurelian had been forced to abandon in 271. At the time of his death, he was planning a great expedition to end raids on the eastern provinces from the Persian Empire. Serving for a total of almost 31 years (combining his years as co-ruler and sole ruler), he was also the longest serving emperor since Augustus and the second longest serving emperor in Roman history.
In the cultural sphere Constantine contributed to the revival of the clean shaven face fashion of the Roman emperors from Augustus to Trajan, which was originally introduced among the Romans by Scipio Africanus. This new Roman imperial fashion lasted until the reign of Phocas.
The Byzantine Empire considered Constantine its founder and the Holy Roman Empire reckoned him among the venerable figures of its tradition. In the later Byzantine state, it had become a great honor for an emperor to be hailed as a "new Constantine". Ten emperors, including the last emperor of Byzantium, carried the name. Monumental Constantinian forms were used at the court of Charlemagne to suggest that he was Constantine's successor and equal. Constantine acquired a mythic role as a warrior against "heathens". The motif of the Romanesque equestrian, the mounted figure in the posture of a triumphant Roman emperor, became a visual metaphor in statuary in praise of local benefactors. The name "Constantine" itself enjoyed renewed popularity in western France in the eleventh and twelfth centuries. Most Eastern Christian churches consider Constantine a saint (Άγιος Κωνσταντίνος, Saint Constantine). In the Byzantine Church he was called "isapostolos" (Ισαπόστολος Κωνσταντίνος)—an equal of the Apostles. Niš airport is named Constantine the Great in honor of his birth in Naissus.
Historiography.
During his life and those of his sons, Constantine was presented as a paragon of virtue. Pagans such as Praxagoras of Athens and Libanius showered him with praise. When the last of his sons died in 361, however, his nephew (and son-in-law) Julian the Apostate wrote the satire "Symposium, or the Saturnalia", which denigrated Constantine, calling him inferior to the great pagan emperors, and given over to luxury and greed. Following Julian, Eunapius began—and Zosimus continued—a historiographic tradition that blamed Constantine for weakening the Empire through his indulgence to the Christians.
In medieval times, when the Roman Catholic Church was dominant, Catholic historians presented Constantine as an ideal ruler, the standard against which any king or emperor could be measured. The Renaissance rediscovery of anti-Constantinian sources prompted a re-evaluation of Constantine's career. The German humanist Johann Löwenklau, discoverer of Zosimus' writings, published a Latin translation thereof in 1576. In its preface, he argued that Zosimus' picture of Constantine was superior to that offered by Eusebius and the Church historians, and damned Constantine as a tyrant. Cardinal Caesar Baronius, a man of the Counter-Reformation, criticized Zosimus, favoring Eusebius' account of the Constantinian era. Baronius' "Life of Constantine" (1588) presents Constantine as the model of a Christian prince. For his "History of the Decline and Fall of the Roman Empire" (1776–89), Edward Gibbon, aiming to unite the two extremes of Constantinian scholarship, offered a portrait of Constantine built on the contrasted narratives of Eusebius and Zosimus. In a form that parallels his account of the empire's decline, Gibbon presents a noble war hero corrupted by Christian influences, who transforms into an Oriental despot in his old age: "a hero ... degenerating into a cruel and dissolute monarch".
Modern interpretations of Constantine's rule begin with Jacob Burckhardt's "The Age of Constantine the Great" (1853, rev. 1880). Burckhardt's Constantine is a scheming secularist, a politician who manipulates all parties in a quest to secure his own power. Henri Grégoire, writing in the 1930s, followed Burckhardt's evaluation of Constantine. For Grégoire, Constantine developed an interest in Christianity only after witnessing its political usefulness. Grégoire was skeptical of the authenticity of Eusebius' "Vita", and postulated a pseudo-Eusebius to assume responsibility for the vision and conversion narratives of that work. Otto Seeck, in "Geschichte des Untergangs der antiken Welt" (1920–23), and André Piganiol, in "L'empereur Constantin" (1932), wrote against this historiographic tradition. Seeck presented Constantine as a sincere war hero, whose ambiguities were the product of his own naïve inconsistency. Piganiol's Constantine is a philosophical monotheist, a child of his era's religious syncretism. Related histories by A.H.M. Jones ("Constantine and the Conversion of Europe" (1949)) and Ramsay MacMullen ("Constantine" (1969)) gave portraits of a less visionary, and more impulsive, Constantine.
These later accounts were more willing to present Constantine as a genuine convert to Christianity. Beginning with Norman H. Baynes' "Constantine the Great and the Christian Church" (1929) and reinforced by Andreas Alföldi's "The Conversion of Constantine and Pagan Rome" (1948), a historiographic tradition developed which presented Constantine as a committed Christian. T. D. Barnes's seminal "Constantine and Eusebius" (1981) represents the culmination of this trend. Barnes' Constantine experienced a radical conversion, which drove him on a personal crusade to convert his empire. Charles Matson Odahl's recent "Constantine and the Christian Empire" (2004) takes much the same tack. In spite of Barnes' work, arguments over the strength and depth of Constantine's religious conversion continue. Certain themes in this school reached new extremes in T.G. Elliott's "The Christianity of Constantine the Great" (1996), which presented Constantine as a committed Christian from early childhood. A similar view of Constantine is held in Paul Veyne's recent (2007) work, "Quand notre monde est devenu chrétien", which does not speculate on the origins of Constantine's Christian motivation, but presents him, in his role as Emperor, as a religious revolutionary who fervently believed himself meant "to play a providential role in the millenary economy of the salvation of humanity".
Donation of Constantine.
Latin Rite Catholics considered it inappropriate that Constantine was baptized only on his death-bed and by an unorthodox bishop, as it undermined the authority of the Papacy. Hence, by the early fourth century, a legend had emerged that Pope Sylvester I (314–35) had cured the pagan emperor from leprosy. According to this legend, Constantine was soon baptized, and began the construction of a church in the Lateran Palace. In the eighth century, most likely during the pontificate of Stephen II (752–7), a document called the Donation of Constantine first appeared, in which the freshly converted Constantine hands the temporal rule over "the city of Rome and all the provinces, districts, and cities of Italy and the Western regions" to Sylvester and his successors. In the High Middle Ages, this document was used and accepted as the basis for the Pope's temporal power, though it was denounced as a forgery by Emperor Otto III and lamented as the root of papal worldliness by the poet Dante Alighieri. The 15th century philologist Lorenzo Valla proved the document was indeed a forgery.
Geoffrey of Monmouth's "Historia".
Because of his fame and his being proclaimed Emperor in the territory of Roman Britain, later Britons regarded Constantine as a king of their own people. In the 12th century Henry of Huntingdon included a passage in his "Historia Anglorum" that Constantine's mother Helena was a Briton, the daughter of King Cole of Colchester. Geoffrey of Monmouth expanded this story in his highly fictionalized "Historia Regum Britanniae", an account of the supposed Kings of Britain from their Trojan origins to the Anglo-Saxon invasion. According to Geoffrey, Cole was King of the Britons when Constantius, here a senator, came to Britain. Afraid of the Romans, Cole submitted to Roman law so long as he retained his kingship. However, he died only a month later, and Constantius took the throne himself, marrying Cole's daughter Helena. They had their son Constantine, who succeeded his father as King of Britain before becoming Roman Emperor.
Historically, this series of events is extremely improbable. Constantius had already left Helena by the time he left for Britain. Additionally, no earlier source mentions that Helena was born in Britain, let alone that she was a princess. Henry's source for the story is unknown, though it may have been a lost hagiography of Helena.
Citations.
Essays from "The Cambridge Companion to the Age of Constantine" are marked with a "(CC)".

</doc>
<doc id="7237" url="http://en.wikipedia.org/wiki?curid=7237" title="Common Language Infrastructure">
Common Language Infrastructure

The Common Language Infrastructure (CLI) is an open specification developed by Microsoft and standardized by ISO and ECMA that describes the executable code and runtime environment. The specification defines an environment that allows multiple high-level languages to be used on different computer platforms without being rewritten for specific architectures. The .NET Framework and the free and open source Mono and Portable.NET are implementations of the CLI.
Overview.
Among other things, the CLI specification describes the following four aspects:
All compatible languages compile to Common Intermediate Language (CIL), which is an intermediate language that is abstracted from the platform hardware. When the code is executed, the platform-specific VES will compile the CIL to the machine language according to the specific hardware and operating system.
Standardization and licensing.
In August 2000, Microsoft, Hewlett-Packard, Intel, and others worked to standardize CLI. By December 2001, it was ratified by the ECMA, with ISO standardization following in April 2003.
Microsoft and its partners hold patents for CLI. ECMA and ISO require that all patents essential to implementation be made available under "reasonable and non-discriminatory (RAND) terms." It is common for RAND licensing to require some royalty payment, which could be a cause for concern with Mono. As of January 2013, neither Microsoft nor its partners have identified any patents essential to CLI implementations subject to RAND terms.
As of July 2009, Microsoft added C# and CLI to the list of specifications that the Microsoft Community Promise applies to, so anyone can safely implement specified editions of the standards without fearing a patent lawsuit from Microsoft. To implement the CLI standard requires conformance to one of the supported and defined profiles of the standard, the minimum of which is the kernel profile. The kernel profile is actually a very small set of types to support in comparison to the well known core library of default .NET installations. However, the conformance clause of the CLI allows for extending the supported profile by adding new methods and types to classes, as well as deriving from new namespaces. But it does not allow for adding new members to interfaces. This means that the features of the CLI can be used and extended, as long as the conforming profile implementation does not change the behavior of a program intended to run on that profile, while allowing for unspecified behavior from programs written specifically for that implementation.
In 2012, ECMA and ISO published the new edition of the CLI standard, which is not covered by the Community Promise.

</doc>
<doc id="7239" url="http://en.wikipedia.org/wiki?curid=7239" title="Cricket World Cup">
Cricket World Cup

The ICC Cricket World Cup is the international championship of One Day International (ODI) cricket. The event is organised by the sport's governing body, the International Cricket Council (ICC), with preliminary qualification rounds leading up to a finals tournament which is held every four years. The tournament is one of the world's most viewed sporting events after FIFA World Cup and Summer Olympics is considered the "flagship event of the international cricket calendar" by the ICC.
The first World Cup was organised in England in June 1975, with the first ODI cricket match having been played only four years prior. However, a separate Women's Cricket World Cup had been held two years before the first men's tournament, and a tournament involving multiple international teams had been held as early as 1912, when a triangular tournament of Test matches was played between Australia, England and South Africa. Each of the first three World Cups were held in England. From the 1987 tournament onwards, hosting has been shared between countries under an unofficial rotation system, with fourteen ICC members having hosted at least one match in the tournament. Hosting of a single edition is often shared between neighbouring countries—the tournament's most recent edition in 2011 was shared between Bangladesh, India, and Sri Lanka.
The finals of the World Cup are contested by all ten full members of the ICC (that is, Test-playing teams) and a number of teams (four at the 2011 tournament) made up from associate and affiliate members of the ICC, selected via the World Cricket League and a later qualifying tournament. A total of 19 teams have competed in the ten editions of the tournament, with 14 competing in the 2011 tournament. Australia has won the tournament four times, with the West Indies, India (twice each), Pakistan and Sri Lanka (once each) also having won the tournament. The best performance by a non-full-member team came when Kenya made the semi-finals of the 2003 tournament.
History.
Before the first Cricket World Cup.
The first ever international cricket match was played between Canada and the United States, on the 24 and 25 September 1844. However, the first credited Test match was played in 1877 between Australia and England, and the two teams competed regularly for The Ashes in subsequent years. South Africa was admitted to Test status in 1889. Representative cricket teams were selected to tour each other, resulting in bilateral competition. Cricket was also included as an Olympic sport at the 1900 Paris Games, where Great Britain defeated France to win the gold medal. This was the only appearance of cricket at the Summer Olympics.
The first multilateral competition at international level was the 1912 Triangular Tournament, a Test cricket tournament played in England between all three Test-playing nations at the time: England, Australia and South Africa. The event was not a success: the summer was exceptionally wet, making play difficult on damp uncovered pitches, and attendances were poor, attributed to a "surfeit of cricket". In subsequent years, international Test cricket has generally been organised as bilateral series: a multilateral Test tournament was not organised again until the quadrangular Asian Test Championship in 1999.
The number of nations playing Test cricket increased gradually over the years, with the addition of West Indies in 1928, New Zealand in 1930, India in 1932, and Pakistan in 1952, but international cricket continued to be played as bilateral Test matches over three, four or five days.
In the early 1960s, English county cricket teams began playing a shortened version of cricket which only lasted for one day. Starting in 1962 with a four-team knockout competition known as the Midlands Knock-Out Cup, and continuing with the inaugural Gillette Cup in 1963, one-day cricket grew in popularity in England. A national Sunday League was formed in 1969. The first One-Day International event was played on the fifth day of a rain-aborted Test match between England and Australia at Melbourne in 1971, to fill the time available and as compensation for the frustrated crowd. It was a forty over match with eight balls per over.
In the late 1970s, Kerry Packer established the rival World Series Cricket (WSC) competition. It introduced many of the features of One Day International cricket that are now commonplace, including coloured uniforms, matches played at night under floodlights with a white ball and dark sight screens, and, for television broadcasts, multiple camera angles, effects microphones to capture sounds from the players on the pitch, and on-screen graphics. The first of the matches with coloured uniforms was the WSC Australians in wattle gold versus WSC West Indians in coral pink, played at VFL Park in Melbourne on 17 January 1979. The success and popularity of the domestic one-day competitions in England and other parts of the world, as well as the early One-Day Internationals, prompted the ICC to consider organising a Cricket World Cup.
Prudential World Cups (1975–1983).
The inaugural Cricket World Cup was hosted in 1975 by England, the only nation able to put forward the resources to stage an event of such magnitude at that time. The 1975 tournament started on 7 June. The first three events were held in England and officially known as the Prudential Cup after the sponsors Prudential plc. The matches consisted of 60 six-ball overs per team, played during the daytime in traditional form, with the players wearing cricket whites and using red cricket balls.
Eight teams participated in the first tournament: Australia, England, India, New Zealand, Pakistan, and the West Indies (the six Test nations at the time), together with Sri Lanka and a composite team from East Africa. One notable omission was South Africa, who were banned from international cricket due to apartheid. The tournament was won by the West Indies, who defeated Australia by 17 runs in the final at Lord's. Glenn Turner was the top run getter with 333 runs and Gary Gilmour head the bowling charts with 11 wickets.
The 1979 World Cup saw the introduction of the ICC Trophy competition to select non-Test playing teams for the World Cup, with Sri Lanka and Canada qualifying. The West Indies won a second consecutive World Cup tournament, defeating the hosts, England, by 92 runs in the final At a meeting which followed the World Cup, the International Cricket Conference agreed to make the competition a quadrennial event. Gordon Greenidge was the top run getter with 253 runs and Mike Hendrick was the top wicket taker with 10 wickets.
The 1983 event was hosted by England for a third consecutive time. By this time, Sri Lanka had become a Test-playing nation, and Zimbabwe qualified through the ICC Trophy. A fielding circle was introduced, away from the stumps. Four fieldsmen needed to be inside it at all times. In this tournament teams faced each other twice, before moving into the knock-outs. India, an outsider quoted at 66–1 to win by bookmakers before the competition began, were crowned champions after upsetting the West Indies by 43 runs in the final. Roger Binny was the top wicket taker with 18 wickets and David Gower with 384 runs was the highest run getter.
1987–1996.
The 1987 tournament, named the Reliance World Cup after their Indian sponsors, was held in India and Pakistan, the first time that the competition was held outside England. All the test sides along with Zimbabwe who qualified again through the ICC Trophy competed. The games were reduced from 60 to 50 overs per innings, the current standard, because of the shorter daylight hours in the Indian subcontinent compared with England's summer. Australia won the championship by defeating England by 7 runs in the final, the closest margin in World Cup final history.
The 1992 World Cup, held in Australia and New Zealand, introduced many changes to the game, such as coloured clothing, white balls, day/night matches, and an alteration to the fielding restrictions. All the test nations competed including the South African cricket team who participated in the event for the first time, following the fall of the apartheid regime and the end of the international sports boycott. Pakistan overcame a dismal start to emerge as champioins, defeating England by 22 runs in the final and therefore winning the first white-ball world cup.
The 1996 championship was held in the Indian subcontinent for a second time, Pakistan and India with the inclusion of Sri Lanka as host for some of its group stage matches. In the semi-final, Sri Lanka, heading towards a crushing victory over India at Eden Gardens after their hosts lost eight wickets while scoring 120 runs in pursuit of 254, were awarded victory by default after crowd unrest broke out in protest against the Indian performance. Sri Lanka went on to win their maiden championship by defeating Australia by seven wickets in the final held in Lahore.
Australian treble (1999-2007).
In 1999 the event was hosted by England, with some matches also being held in Scotland, Ireland, Wales and the Netherlands. The nine full members contested the World Cup along with three associate members: Kenya, and for the first time, Bangladesh and Scotland who qualified through the ICC Trophy. Australia qualified for the semi-finals after reaching their target in their Super 6 match against South Africa off the final over of the match. They then proceeded to the final with a tied match in the semi-final (also against South Africa) where a mix-up between South African batsmen Lance Klusener and Allan Donald saw Donald drop his bat and stranded mid-pitch to be run out. In the final, Australia dismissed Pakistan for 132 and then reached the target in less than 20 overs, with eight wickets in hand.
South Africa, Zimbabwe and Kenya hosted the 2003 World Cup. The number of teams participating in the event increased from twelve to fourteen. Kenya's victories over Sri Lanka and Zimbabwe, among others – and a forfeit by the New Zealand team, which refused to play in Kenya because of security concerns – enabled Kenya to reach the semi-finals, the best result by an associate. In the final, Australia made 359 runs for the loss of two wickets, the largest ever total in a final, defeating India by 125 runs.
In 2007 the tournament was hosted by the West Indies and expanded to sixteen teams. Bangladesh progressed to the second round for the first time after defeating India. Ireland making their World Cup debut tied with Zimbabwe and defeated Pakistan to progress to the second round, where they went on to defeat Bangladesh to get promoted to the main ODI table. Following their defeat to Ireland, the Pakistani coach Bob Woolmer was found dead in his hotel room. Jamaican police had initially launched a murder investigation into Woolmer's death but later confirmed that he died of heart failure. Australia defeated Sri Lanka in the final by 53 runs (D/L), in farcical light conditions, extending their undefeated run in the World Cup to 29 matches and winning three straight World Cups.
2011.
The 2011 Cricket World Cup was jointly hosted by India, Sri Lanka and for the first time Bangladesh. Pakistan were stripped of their hosting rights following the terrorist attack on the Sri Lankan cricket team, with the games originally scheduled for Pakistan redistributed to the other host countries. Fourteen teams participated in the tournament consisting of the 10 full members and four Associate teams who qualified via the 2009 Cricket World Cup Qualifier. India won their second World Cup title by beating Sri Lanka by 6 wickets in the final in Mumbai, thus becoming the first country to win the Cricket World Cup final on home soil. India's Yuvraj Singh was named as the man of the tournament.
Format.
Qualification.
The Test-playing nations qualify automatically for the World Cup main event, while the other teams have to qualify through a series of preliminary qualifying tournaments. The One Day International playing nations automatically enter the final qualification tournament, the World Cup Qualifier, along with other nations who have qualified through separate competitions.
Qualifying tournaments were introduced for the second World Cup, where two of the eight places in the finals were awarded to the leading teams in the ICC Trophy. The number of teams selected through the ICC Trophy has varied throughout the years; currently, six teams are selected for the Cricket World Cup. The World Cricket League (administered by the International Cricket Council) is the qualification system provided to allow the Associate and Affiliate members of the ICC more opportunities to qualify. The name "ICC Trophy" has been changed to "ICC World Cup Qualifier".
Under the current qualifying process, the World Cricket League, all 91 Associate and Affiliate members of the ICC are able to qualify for the World Cup. Associate and Affiliate members must play between two and five stages in the ICC World Cricket League to qualify for the World Cup finals, depending on the Division in which they start the qualifying process.
Process summary in chronological order:
Tournament.
The format of the Cricket World Cup has changed greatly over the course of its history. Each of the first four tournaments was played by eight teams, divided into two groups of four. There, competition comprised two stages, a group stage and a knock-out stage. The four teams in each group played each other in the round-robin group stage, with the top two teams in each group progressing to the semi-finals. The winners of the semi-finals played against each other in the final. With the return of South Africa in 1992 after the ending of the apartheid boycott, nine teams played each other once in the group phase, and the top four teams progressed to the semi-finals. The tournament was further expanded in 1996, with two groups of six teams. The top four teams from each group progressed to quarter-finals and semi-finals.
A new format was used for the 1999 and 2003 World Cups. The teams were split into two pools, with the top three teams in each pool advancing to the "Super 6". The "Super 6" teams played the three other teams that advanced from the other group. As they advanced, the teams carried their points forward from previous matches against other teams advancing alongside them, giving them an incentive to perform well in the group stages. The top four teams from the "Super 6" stage progressed to the semi-finals, with winners playing in the final.
The last format used in the 2007 World Cup, features 16 teams allocated into four groups of four. Within each group, the teams play each other in a round-robin format. Teams earn points for wins and half-points for ties. The top two teams from each group move forward to the "Super 8" round. The "Super 8" teams play the other six teams that progressed from the different groups. Teams earned points in the same way as the group stage, but carrying their points forward from previous matches against the other teams who qualified from the same group to the "Super 8" stage. The top four teams from the "Super 8" round advance to the semi-finals, and the winners of the semi-finals play in the final.
The current format used in the 2011 World Cup features 2 groups of 7 teams, each playing in a round-robin format. The top four teams from each group proceed to the knock out stage consisting of quarter-finals, semi-finals and then the final.
Trophy.
The ICC Cricket World Cup Trophy is presented to the winners of the World Cup finals. The current trophy was created for the 1999 championships, and was the first permanent prize in the tournament's history; prior to this, different trophies were made for each World Cup. The trophy was designed and produced in London by a team of craftsmen from Garrard & Co over a period of two months.
The current trophy is made from silver and gild, and features a golden globe held up by three silver columns. The columns, shaped as stumps and bails, represent the three fundamental aspects of cricket: batting, bowling and fielding, while the globe characterises a cricket ball. It stands 60 cm high and weighs approximately 11 kilograms. The names of the previous winners are engraved on the base of the trophy, with space for a total of twenty inscriptions.
The original trophy is kept by the ICC. A replica, which differs only in the inscriptions, is permanently awarded to the winning team.
Media coverage.
The tournament is the world's third largest (with only the FIFA World Cup and the Summer Olympics exceeding it), being televised in over 200 countries to over 2.2 billion television viewers. Television rights, mainly for the 2011 and 2015 World Cup, were sold for over US$1.1 billion, and sponsorship rights were sold for a further US$500 million. The 2003 Cricket World Cup matches were attended by 626,845 people, while the 2007 Cricket World Cup sold more than 672,000 tickets.
Successive World Cup tournaments have generated increasing media attention as One-Day International cricket has become more established. The 2003 World Cup in South Africa was the first to sport a mascot, "Dazzler" the zebra. An orange mongooses known as "Mello" was the mascot for the 2007 Cricket World Cup. "Stumpy", a blue elephant was the mascot for the 2011 World Cup.
Selection of hosts.
The International Cricket Council's executive committee votes for the hosts of the tournament after examining the bids made by the nations keen to hold a Cricket World Cup.
England hosted the first three competitions. The ICC decided that England should host the first tournament because it was ready to devote the resources required to organising the inaugural event. India volunteered to host the third Cricket World Cup, but most ICC members preferred England as the longer period of daylight in England in June meant that a match could be completed in one day. The 1987 Cricket World Cup was held in Pakistan and India, the first hosted outside England.
Many of the tournaments have been jointly hosted by nations from the same geographical region, such as South Asia in 1987, 1996 and 2011, Australasia in 1992, Southern Africa in 2003 and West Indies in 2007.
Performances by teams.
Twenty nations have qualified for the finals of the Cricket World Cup at least once (excluding qualification tournaments). Seven teams have competed in every finals tournament, five of which have won the title. The West Indies won the first two tournaments, Australia has won four, India has won two, while Pakistan and Sri Lanka have each won once. The West Indies (1975 and 1979) and Australia (1999, 2003 and 2007) are the only nations to have won consecutive titles. Australia has played in 6 of the 10 final matches (1975, 1987, 1996, 1999, 2003, 2007). England has yet to win the World Cup, but has been runners-up three times (1979, 1987, 1992). The best result by a non-Test playing nation is the semi-final appearance by Kenya in the 2003 tournament; while the best result by a non-Test playing team on their debut is the Super 8 (second round) by Ireland in 2007.
Sri Lanka, who co-hosted the 1996 Cricket World Cup, was the first host to win the tournament, though the final was held in Pakistan. India won the 2011 as host and was the first team to win in a final played in their own country. England is the only other host to have made the final, in 1979. Other countries which have achieved or equalled their best World Cup results while co-hosting the tournament are New Zealand, semi-finalists in 1992; Zimbabwe, reaching the Super Six in 2003; and Kenya, semi-finalists in 2003. In 1987, co-hosts India and Pakistan both reached the semi-finals, but were eliminated by Australia and England respectively. Australia in 1992, England in 1999, South Africa in 2003, and Bangladesh in 2011 have been the host teams to get out in the first round.
Teams' performances.
Comprehensive teams' performances of over the past World Cups:
Prior to the 1992 World Cup, South Africa was banned due to apartheid.
Number of wins followed by Run-rate is the criteria for determining the ranks till 1987 World Cup.
Number of points followed by, head to head performance and then Net Run-rate is the criteria for determining the ranks for World Cup from 1992 onwards.
Legend
Debutant teams.
†
Overview.
The table below provides an overview of the performances of teams over past World Cups, as of the end of the 2011 tournament. Teams are sorted by best performance, then total number of wins, then total number of games, then by alphabetical order.
Awards.
Man of the tournament.
Since 1992, one player has been declared as "Man of the Tournament" at the end of the World Cup finals:
Man of the Match in the World Cup Final.
Before 1992, there was no tournament award, although Man of the Match awards have always been given for individual matches. Winning the Man of the Match in the final is logically noteworthy, as this indicates the player deemed to have played the biggest part in the World Cup final. To date the award has always gone to a member of the winning side. The Man of the Match award in the final of the competition has been awarded to:

</doc>
<doc id="7241" url="http://en.wikipedia.org/wiki?curid=7241" title="Commonwealth Heads of Government Meeting">
Commonwealth Heads of Government Meeting

The Commonwealth Heads of Government Meeting (CHOGM; or ) is a biennial summit meeting of the heads of government from all Commonwealth nations. Every two years the meeting is held in a different member state, and is chaired by that nation's respective Prime Minister or President, who becomes the Commonwealth Chairperson-in-Office. Queen Elizabeth II, who is the Head of the Commonwealth, attended every CHOGM beginning with Ottawa in 1973, although her formal participation only began in 1997. However, she was represented by the Prince of Wales at the 2013 meeting as the 88-year-old monarch is curtailing her overseas travel.
The first CHOGM was held in 1971, and there have been 23 held in total: the most recent was held in Colombo, Sri Lanka. They are held once every two years, although this pattern has twice been interrupted. They are held around the Commonwealth, rotating by invitation amongst its members.
In the past, CHOGMs have attempted to orchestrate common policies on certain contentious issues and current events, with a special focus on issues affecting member nations. CHOGMs have discussed the continuation of apartheid rule in South Africa and how to end it, military coups in Pakistan and Fiji, and allegations of electoral fraud in Zimbabwe. Sometimes the member states agree on a common idea or solution, and release a joint statement declaring their opinion. More recently, beginning at the 1997 CHOGM, the meeting has had an official 'theme', set by the host nation, on which the primary discussions have been focused.
History.
The meetings originated with the leaders of the self-governing colonies of the British Empire. The First Colonial Conference in 1887 was followed by periodic meetings, known as Imperial Conferences from 1907, of government leaders of the Empire. The development of the independence of the dominions, and the creation of a number of new dominions, as well as the invitation of Southern Rhodesia (which also attended as a "sui generis" colony), changed the nature of the meetings. As the dominion leaders asserted themselves more and more at the meetings, it became clear that the time for 'imperial' conferences was over.
From the ashes of the Second World War, seventeen Commonwealth Prime Ministers' Conferences were held between 1944 and 1969. Of these, sixteen were held in London, reflecting then-prevailing views of the Commonwealth as the continuation of the Empire and the centralisation of power in the British Commonwealth Office (the one meeting outside London, in Lagos, was an extraordinary meeting held in January 1966 to co-ordinate policies towards Rhodesia). Two supplementary meetings were also held during this period: a Commonwealth Statesmen's meeting to discuss peace terms in April 1945, and a Commonwealth Economic Conference in 1952.
The 1960s saw an overhaul of the Commonwealth. The swift expansion of the Commonwealth after decolonisation saw the newly independent countries demand the creation of the Commonwealth Secretariat, and the United Kingdom, in response, successfully founding the Commonwealth Foundation. This decentralisation of power demanded a reformulation of the meetings. Instead of the meetings always being held in London, they would rotate across the membership, subject to countries' ability to host the meetings: beginning with Singapore in 1971. They were also renamed the 'Commonwealth Heads of Government Meetings' to reflect the growing diversity of the constitutional structures in the Commonwealth.
Structure.
The core of the CHOGM are the executive sessions, which are the formal gatherings of the heads of government to do business. However, the majority of the important decisions are held not in the main meetings themselves, but at the informal 'retreats': introduced at the second CHOGM, in Ottawa, by Prime Minister of Canada Pierre Trudeau, but reminiscent of the excursions to Chequers or Dorneywood in the days of the Prime Ministers' Conferences. The rules are very strict: allowing the head of the delegation, his or her spouse, and one other person. The additional member can be of any capacity (personal, political, security, etc.), but he or she has only occasional and intermittent access to the head. It is usually at the retreat where, isolated from their advisers, the heads resolve the most intransigent issues: leading to the Gleneagles Agreement in 1977, the Lusaka Declaration in 1979, the Langkawi Declaration in 1989, the Millbrook Programme in 1995, and the Aso Rock Declaration in 2003.
The 'fringe' of civil society organisations, including the Commonwealth Family and local groups, adds a cultural dimension to the event, and brings the CHOGM a higher media profile and greater acceptance by the local population. First officially recognised at Limassol in 1993, these events, spanning a longer period than the meeting itself, have, to an extent, preserved the length of the CHOGM: but only in the cultural sphere. Other meetings, such as those of the Commonwealth Ministerial Action Group, Commonwealth Business Council, and respective foreign ministers, have also dealt with business away from the heads of government themselves.
As the scope of the CHOGM has expanded beyond the meetings of the heads of governments themselves, the CHOGMs have become progressively shorter, and their business more compacted into less time. The 1971 CHOGM lasted for nine days, and the 1977 and 1991 CHOGMs for seven days each. However, Harare's epochal CHOGM was the last to last a week; the 1993 CHOGM lasted for five days, and the contentious 1995 CHOGM for only three-and-a-half. The 2005 and subsequent conferences were held over two days.
Issues.
During the 1980s, CHOGMs were dominated by calls for the Commonwealth to impose sanctions on South Africa to pressure the country to end apartheid. The division between Britain, during the government of Margaret Thatcher which resisted the call for sanctions and African Commonwealth countries, and the rest of the Commonwealth was intense at times and led to speculation that the organisation might collapse. According to one of Margaret Thatcher's former aides, Mrs. Thatcher, very privately, used to say that CHOGM stood for "Compulsory Handouts to Greedy Mendicants."
In 2011, British Prime Minister David Cameron informed the British House of Commons that his proposals to reform the rules governing royal succession, a change which would require the approval of all Commonwealth realms, was approved at the 28–30 October CHOGM in Perth, subsequently referred to as the Perth Agreement.
Agenda.
Under the Millbrook Commonwealth Action Programme, each CHOGM is responsible for renewing the remit of the Commonwealth Ministerial Action Group, whose responsibility it is to uphold the Harare Declaration on the core political principles of the Commonwealth.
Incidents.
A bomb exploded at the Sydney Hilton Hotel, the venue for the February 1978 Commonwealth Heads of Government Regional Meeting. Twelve foreign heads of government were staying in the hotel at the time. Most delegates were evacuated by Royal Australian Air Force helicopters and the meeting was moved to Bowral, protected by 800 soldiers of the Australian Army.
As the convocation of heads of governments and permanent Commonwealth staff and experts, CHOGMs are the highest institution of action in the Commonwealth, and rare occasions on which Commonwealth leaders all come together. CHOGMs have been the venues of many of the Commonwealth's most dramatic events. Robert Mugabe announced Zimbabwe's immediate withdrawal from the Commonwealth at the 2003 CHOGM, and Nigeria's execution of Ken Saro-Wiwa and eight others on the first day of the 1995 CHOGM led to that country's suspension.
It has also been the trigger of a number of events that have shook participating countries domestically. The departure of Uganda's President Milton Obote to the 1971 CHOGM allowed Idi Amin to overthrow Obote's government. Similarly, President James Mancham's attendance of the 1977 CHOGM gave Prime Minister France-Albert René the opportunity to seize power in the Seychelles.

</doc>
<doc id="7242" url="http://en.wikipedia.org/wiki?curid=7242" title="Chinese classics">
Chinese classics

Chinese classic texts or canonical texts (Chinese:  ,  ,  "Zhōngguó gǔdiǎn diǎnjí") refers to the Chinese texts which originated before the imperial unification by the Qin dynasty in particularly the "Four Books and Five Classics" of the Neo-Confucian tradition, themselves a customary abridgment of the "Thirteen Classics". All of these pre-Qin texts were written in classical Chinese. All three canons are collectively known as the classics ( ,  , "jīng",  "warp").
Chinese classic texts may more broadly refer to texts written either in vernacular Chinese or in the classical Chinese that was current until the fall of the last imperial dynasty, the Qing, in 1912. These can include "shi" (, historical works), "zi" (, philosophical works belonging to schools of thought other than the Confucian but also including works on agriculture, medicine, mathematics, astronomy, divination, art criticism, and other miscellaneous writings) and "ji" (, literary works) as well as "jing".
In the Ming and Qing dynasties, the Four Books and Five Classics were the subject of mandatory study by those Confucian scholars who wished to take the imperial exams to become government officials. Any political discussion was full of references to this background, and one could not be one of the literati (or, in some periods, even a military officer) without having memorized them. Generally, children first memorized the Chinese characters of the "Three Character Classic" and the "Hundred Family Surnames" and then went on to memorize the other classics. The literate elite therefore shared a common culture and set of values.
Scholarship on these texts naturally divides itself into two periods, before and after the burning of the books during the fall of the Qin dynasty, when many of the original pre-Qin texts were lost.

</doc>
<doc id="7243" url="http://en.wikipedia.org/wiki?curid=7243" title="Call centre">
Call centre

A call centre or call center is a centralised office used for the purpose of receiving or transmitting a large volume of requests by telephone. An inbound call centre is operated by a company to administer incoming product support or information inquiries from consumers. Outbound call centers are operated for telemarketing, solicitation of charitable or political donations, debt collection and market research. In addition to a call centre, collective handling of letter, fax, live support software, social media and e-mail at one location is known as a contact centre.
A call centre is operated through an extensive open workspace for call centre agents, with work stations that include a computer for each agent, a telephone set/headset connected to a telecom switch, and one or more supervisor stations. It can be independently operated or networked with additional centres, often linked to a corporate computer network, including mainframes, microcomputers and LANs. Increasingly, the voice and data pathways into the centre are linked through a set of new technologies called computer telephony integration (CTI).
A contact centre, also known as customer interaction centre is a central point of any organization from which all customer contacts are managed. Through contact centres, valuable information about company are routed to appropriate people, contacts to be tracked and data to be gathered. It is generally a part of company’s customer relationship management (CRM). Today, customers contact companies by telephone, email, online chat, fax, and instant message.
Technology.
Call centre technology is subject to improvements and innovations. Some of these technologies include speech recognition software to allow computers to handle first level of customer support, text mining and natural language processing to allow better customer handling, agent training by automatic mining of best practices from past interactions, support automation and many other technologies to improve agent productivity and customer satisfaction. Automatic lead selection or lead steering is also intended to improve efficiencies, both for inbound and outbound campaigns. This allows inbound calls to be directly routed to the appropriate agent for the task, whilst minimizing wait times and long lists of irrelevant options for people calling in. For outbound calls, lead selection allows management to designate what type of leads go to which agent based on factors including skill, socioeconomic factors and past performance and percentage likelihood of closing a sale per lead.
The concept of the universal queue standardizes the processing of communications across multiple technologies such as fax, phone, and email whilst the concept of a Virtual queue provides callers with an alternative to waiting on hold when no agents are available to handle inbound call demand.
Premise-based call centre technology
Historically, call centres have been built on PBX equipment that is owned and hosted by the call centre operator. The PBX might provide functions such as automatic call distribution, interactive voice response, and skills-based routing. The call centre operator would be responsible for the maintenance of the equipment and necessary software upgrades as released by the vendor.
Virtual call centre technology
With the advent of the "software as a service" delivery model, the virtual call centre has emerged. In a virtual call centre model, the call centres operator does not own, operate or host the equipment that the call centre runs on. Instead, they subscribe to a service for a monthly or annual fee with a service provider that hosts the call centre telephony equipment in their own data centre. Such a vendor may host many call centres on their equipment. Agents connect to the vendor's equipment through traditional PSTN telephone lines, or over voice over IP. Calls to and from prospects or contacts originate from or terminate at the vendor's data centre, rather than at the call centre operator's premise. The vendor's telephony equipment then connects the calls to the call centre operator's agents.
Virtual call centre technology allows people to work from home, instead of in a traditional, centralised, call centre location, which increasingly allows people with physical or other disabilities that prevent them from leaving the house, to work. 
Cloud computing for call centres
Cloud computing for call centres extends cloud computing to software as a service, or hosted, on-demand call centres by providing application programming interfaces (APIs) on the call centre cloud computing platform that allow call centre functionality to be integrated with cloud-based Customer relationship management, such as Salesforce.com or Oracle CRM and leads management and other applications.
The APIs typically provide programmatic access to two key groups of features in the call centre platform:
Dynamics.
Calls may be "inbound" or "outbound". Inbound calls are made by consumers, for example to obtain information, report a malfunction, or ask for help. In contrast, outbound calls are made by agents to consumers, usually for sales purposes (telemarketing). One can combine inbound and outbound campaigns.
Call centre staff are often organised into a multi-tier support system for more efficient handling of calls. The first tier consists of operators, who initially answer calls and provide general information. If a caller requires more assistance, the call is forwarded to the second tier (in the appropriate department depending on the nature of the call). In some cases, there are three or more tiers of support staff. Typically the third tier of support is formed of product engineers/developers or highly skilled technical support staff for the product.
Some critics of call centres argue that the work atmosphere in such an environment is dehumanising. Others point to the low rates of pay and restrictive working practices of some employers. There has been much controversy over such things as restricting the amount of time that an employee can spend in the toilet. Call centres have also been the subject of complaints by callers who find the staff often do not have enough skill or authority to resolve problems, while the staff sometimes appear apathetic. Other research illustrates how call center workers develop ways to counter or resist this environment by integrating local cultural sensibilities or embracing a vision of a new life.
Telephone calls are easily monitored, and the close monitoring of call centre staff is widespread. This has the benefit of helping the company to plan the workload and time of its employees. However, it has also been argued that such close monitoring breaches the human right to privacy. Most call centres provide electronic reports that outline performance metrics, quarterly highlights and other information about the calls made and received.
Varieties.
Some variations of call centre models are listed below:
Call Centre Metrics.
Some vital call centre performance metrics are listed below:
It is a measure of how products and services supplied by a company meet or surpass customer expectation. C-SAT is based on customer’s experience with the support or service. The scoring for this answer is most often based on a 0 to 10 scale.
Average Handling Time (AHT) is a key measure for any contact center planning system, as it tells you how long a new item of work takes to be handled and not just the talk time.
Revenue Per Call (RPC) is usually used in sales projects which calculates the effort of a representative with respect to increasing sales. RPC can be calculated by dividing the total amount of sale by total number of calls.
First Call Resolution is properly addressing the customer’s need the first time they call, thereby eliminating the need for the customer to follow up with a second call.
Percentage of time the problem has been completely resolved from the customer point of view. This KPI is mostly used for: Operational Excellence. This keeps troubleshooting time to a minimum, which, according to industry averages, currently accounts for as much as 80 percent of total problem resolution time, and gets the problem fixed.
Net Promoter Score (NPS) measures the loyalty that exists between a provider and a consumer. NPS is based on a direct question: How likely is it that you would recommend our company/product/service to a friend or colleague? The scoring for this answer is most often based on a 0 to 10 scale.
Quality Scores is by far the most common metric used. It provides the ability to look at the overall caller experience and the conversations that agents are using on their phone calls.
A service-level agreement is an agreement between two or more parties where one is the customer and the others are service providers. The contract may involve financial penalties and the right to terminate the contract if the SLA metrics are consistently missed.
The Active and Waiting Calls measures current volume of active calls compared to the number of callers waiting to be patched through to an agent. This is a real-time status metric that should be shared with all the agents to offer them insight on their performance. Agents should be encouraged to resolve calls on a timely basis in order to get to the next caller in queue and not keep the callers on wait.
Call Abandonment measures the number of calls that are disconnected before they can be connected to one of your agents. This metric is closely related to Service Level and Customer Satisfaction. Customers do are not expected to be patient. They will hang up and possibly switch their brand loyalties.
Forecasting accuracy is better described as forecasted contact load vs. actual contact load. It is a performance metric that reflects the percent variance between the number of inbound customer contacts forecasted for a particular time period and the number of said contacts actually received by the center during that time period.
The best way to measure the satisfaction of your workforce is to look at the percentage of staff that leaves. There can be some telling information in these numbers and it is crucial to track and analyze the turnover rates in many ways.
The up-sell rate or cross-sell rate is simply the success rate of generating revenue above the original intention of the call. It is becoming an increasingly common practice, not just for pure revenue-generating call centers but for customer service centers as well.
Staff shrinkage is the percentage of time that employees are not available to handle calls. It is classified as non-productive time, and is made up of meeting and training time, breaks, paid time off, off-phone work, and general unexplained time where agents are not available to handle customer interactions.
Blockage is a measure of accessibility that indicates what percentage of customers will not be able to get in touch with the contact center at a given time due to insufficient network facilities.
A major factor determining revenue is the cost of running the organization. A common measure of operational efficiency is cost incurred for each minute of handling the call workload, commonly referred to as Cost per Call. This cost per call can be simply a labor cost per call, or it can be a fully loaded rate that includes payroll in addition to telecommunications, facilities, and other services costs.
Criticism and performance.
Criticisms of call centres generally follow a number of common themes, from both callers and call centre staff. From callers, common criticisms include:
Common criticisms from staff include:
The net-net of these concerns is that call centres as a business process exhibit levels of variability. The experience a customer gets and the results a company achieves on a given call are almost totally dependent on the quality of the agent answering that call. Call centres are beginning to address this by using agent-assisted automation to standardise the process all agents use. Anton and Phelps have provided a detailed manual on how to conduct the performance evaluation of the business, whereas others are using various scientific technologies to do the jobs. However, more popular alternatives are using personality and skill based approaches. The various challenges encountered by call operators are discussed by several authors.
Outsourced bureau contact centres.
Outsourced bureau contact centres are a model of contact centre that provide services on a "pay per use" model. The overheads of the contact centre are shared by many clients thereby supporting a very cost effective model especially for low volumes of calls.
Bureau contact centres provide an opportunity for:
Unionization.
Unions in North America have made some effort to gain members from this sector, including the Communications Workers of America and the United Steelworkers. In Australia, the National Union of Workers represents unionised workers; their activities form part of the Australian labour movement. In Europe, Uni Global Union of Switzerland is involved in assisting unionisation in this realm and in Germany Vereinte Dienstleistungsgewerkschaft represents call centre workers.
Mathematical theory.
Queueing theory is a branch of mathematics in which models of service systems have been developed. A call centre can be seen as a queueing network and results from queueing theory such as the probability an arriving customer needs to wait before starting service useful for provisioning capacity. (Erlang's C formula is such a result for an M/M/c queue and approximations exist for an M/G/k queue.) Statistical analysis of call centre data has suggested arrivals are governed by an inhomogeneous Poisson process and jobs have a log-normal service time distribution.
Call centre operations have been supported by mathematical models beyond queueing, with operations research, which considers a wide range of optimisation problems seeking to reduce waiting times while keeping server utilisation and therefore efficiency high.
Media portrayals.
Indian call centres have been the focus several documentary films, the 2004 film "Thomas L. Friedman Reporting: The Other Side of Outsourcing", the 2005 films "John and Jane", "Nalini by Day, Nancy by Night", and "1-800-India: Importing a White-Collar Economy", and the 2006 film "Bombay Calling", among others. An Indian call centre is also the subject of the 2006 film "Outsourced (film)" and a key location in the 2008 film, "Slumdog Millionaire". "BBC The Call Centre" is an often distorted although humorous view of life in a Welsh Call Centre. There are critics of call centres who argue that the working environment within call centre's are dehumanising. "The Call Centre" argues against this point, and tries to illustrate that you can have high employee engagement within a call centre.

</doc>
<doc id="7245" url="http://en.wikipedia.org/wiki?curid=7245" title="Caliph">
Caliph

Caliph or khalifa is a title used for Islamic rulers who are considered politico-religious leaders of the Islamic community of believers, and who rule in accordance with Islamic law. A state ruled by a caliph is a caliphate.
Following Muhammad's death in 632, the early leaders of the Muslim nation were called "Khalifat Rasul Allah", the political successors to the messenger of God (referring to Muhammad). A calipha is either a female caliph or the wife or widow of a caliph. There was one known instance in history that a calipha ruled a Caliphate: Sitt al-Mulk was regent of the Fatimid Caliphate from 1021 to 1023. Some caliphas, such as Zaynab an-Nafzawiyyah and Al-Khayzuran bint Atta, wielded great influence in the courts of their husbands.
The term derives from the Arabic word "" ( ), which means "successor", "steward", or "deputy".
Qur'an.
The Qur'an uses the term "khalifa" twice. First, in al-Baqara 30, it refers to God creating humanity as his "khalifa" on Earth. Second, in Sad 26, it addresses King David as God's "khalifa "and reminds him of his obligation to rule with justice.
Succession to Muhammad.
In his book "The Early Islamic Conquests" (1981), Fred Donner argues that the standard Arabian practice at the time was for the prominent men of a kinship group, or tribe, to gather after a leader's death and elect a leader from amongst themselves. There was no specified procedure for this shura or consultation. Candidates were usually, but not necessarily, from the same lineage as the deceased leader. Capable men who would lead well were preferred over an ineffectual heir.
Sunni Muslims believe and confirm that Abu Bakr was chosen by the community and that this was the proper procedure. Sunnis further argue that a caliph should ideally be chosen by election or community consensus.
Shi'a Muslims believe that Ali, the son-in-law and cousin of Muhammad, was chosen by Muhammad as his spiritual and temporal successor as the "Mawla" (the Imam and the Caliph) of all Muslims at a place called al-Gadhir Khumm. Here Mohammad called upon the around 100,000 gathered returning pilgrims to give their bayah (oath of allegiance) to Ali in his very presence and thenceforth to proclaim the good news of Ali's succession to his (Muhammad's) leadership to all Muslims they should come across.
Word usage / etymology.
The word "caliph" is derived from the Arabic word khalifa ( "/") meaning "successor", "substitute", or "lieutenant". In Matthew S. Gordon's "The Rise of Islam", caliph is said to translate to "deputy (or representative) of God." It is used in the Quran to establish Adam's role as representative of God on earth. Khalifa is also used to describe the belief that man's role, in his real nature, is as khalifa or viceroy to Allah. The word is also most commonly used for the Islamic leader of the Ummah; starting with Abu Bakr and his line of successors.
The first four Caliphs: Abu Bakr as-Siddiq, Umar ibn al-Khattab, Uthman ibn Affan, and Ali ibn Abi Talib are commonly known by Sunnis, mainly, as the Khulafā’ur-Rāshideen ("rightly guided successors") Caliphs.
It should be noted that in Indonesia, the term "caliph" traditionally has a looser meaning, and has been applied to numerous leaders. For example, the current sultan Hamengkubuwono X has 'caliph' in his title, but without serious meaning.
History.
Succession and recognition.
Sunni and Shi'a Muslims differ on the legitimacy of the reigns of the Rashidun Caliphs, the first four Caliphs. The Sunnis follow the Caliphates of all four, while the Shi'ites recognize only the Caliphate of Ali and the short Caliphate of his son Hasan. This schism occurred following the death of Muhammad.
According to Sunni beliefs, Muhammad gave no specific directions as to the choosing of his successor when he died. At this time there were two customary means of selecting a leader: having a hereditary leader for general purposes, and choosing someone with good qualities in times of crisis or opportunities for action.
While Sunni and Shia Islam differ sharply on the conduct of a caliph and the right relations between a leader and a community, they do not differ on the underlying theory of stewardship. Both abhor waste of natural resources in particular to show off or demonstrate power.
In the initial stages the latter way of choosing leadership prevailed among the leading companions of Muhammad. Abu Bakr was elected as the first caliph or successor to Muhammad, with the other companions of Muhammad giving an oath of allegiance to him. Those opposing this method thought that Ali, Muhammad's nearest relative, should have succeeded him. However the appointment of the next two caliphs varied from the election of Abu Bakr. On his deathbed, Abu Bakr appointed Umar as his successor without an election by the community of Believers. The oath, approving the appointment of Umar, was taken only by the Companions present in Medina at the time. This led to certain groups disputing the authority of Umar. Umar also altered the way his successor would be found. Before he was assassinated, Umar decided that his successor would come from a group of six. This group included Ali and Uthman, another companion of Muhammad. These six would have to establish from among themselves Umar's successor. Ultimately Uthman was chosen as Umar's successor, becoming the third Caliph. After the assassination of Uthman, Ali was elected as the fourth Caliph
Ali's caliphate and the rise of the Umayyad Dynasty.
Ali's reign as Caliph was plagued by great turmoil and internal strife. Ali was faced with multiple rebellions and insurrections. The primary one came from a misunderstanding on the part of Mu'awiyah, the governor of Damascus, marking the beginning of the end of the Caliphs. The Persians, taking advantage of this, infiltrated the two armies and attacked the other army causing chaos and internal hatred between the Companions at the Battle of Siffin. The battle lasted several months, resulting in a stalemate. In order to avoid further bloodshed, Ali agreed to negotiate with Mu'awiyah. This caused a faction of approximately 4,000 people that would be known as the Kharijites, to abandon the fight. After defeating the Kharijites at the Battle of Nahrawan, Ali would later be assassinated by the Kharijite Ibn Muljam. Ali's son Hasan was elected as the next Caliph, but handed his title to Mu'awiyah a few months later. Mu'awiyah became the fifth Caliph, establishing the Umayyad Dynasty, named after the great-grandfather of Uthman and Mu'awiyah, Umayya ibn Abd Shams.
"(All Caliphs after Mu'awiyah aren't considered true caliphs from an islamic perspective, though it was used as a title afterwards.)"
Umayyads.
Under the Umayyads (661 to 750 AD, and 929 to 1031 in the Iberian Peninsula), the Muslim empire grew rapidly. To the West, Muslim rule expanded across North Africa and into Spain. To the East, it expanded through Iran and ultimately to India. This made it one of the largest empires in the history of West Eurasia, extending its entire breadth.
However, the Umayyad dynasty was not universally supported within Islam itself. Some Muslims supported prominent early Muslims like az-Zubayr; others felt that only members of Muhammad's clan, the Banū Hashim, or his own lineage, the descendants of , should rule. There were numerous rebellions against the Umayyads, as well as splits within the Umayyad ranks (notably, the rivalry between Yaman and Qays). Eventually, supporters of the Banu Hisham and Alid claims united to bring down the Umayyads in 750. However, the "", "the Party of ", were again disappointed when the Abbasid dynasty took power, as the Abbasids were descended from Muhammad's uncle, Abbas ibn Abd al-Muttalib and not from .
Abbasids.
The Abbasids would provide an unbroken line of rulers for over five centuries (750-1258 AD). It consolidated Islamic rule and cultivated great intellectual and cultural developments in the Middle East. But by 940 the power of the caliphate under the Abbasids was waning as non-Arabs, particularly the Turkish (and later the Mamluks in Egypt in the latter half of the 13th century), gained military power, and sultans and emirs became increasingly independent. However, the caliphate endured as both a symbolic position and a unifying entity for the Islamic world.
During the period of the Abbasid dynasty, Abbasid claims to the caliphate did not go unchallenged. The Said ibn Husayn of the Fatimid dynasty, which claimed descent from Muhammad through his daughter, claimed the title of Caliph in 909, creating a separate line of caliphs in North Africa. Initially covering Morocco, Algeria, Tunisia and Libya, the Fatimid caliphs extended their rule for the next 150 years, taking Egypt and the Levant, before the Abbasid dynasty was able to turn the tide, limiting Fatimid rule to Egypt. The Fatimid dynasty finally ended in 1171. The Umayyad dynasty, which had survived and come to rule over the Muslim provinces of Spain, reclaimed the title of Caliph in 929, lasting until it was overthrown in 1031. This period of upheaval was known as the Fitna of al-Ándalus.
Fatimids.
The Fatimid Caliphate or al-Fātimiyyūn (Arabic الفاطميون) was a Berber Shi'ite dynasty that ruled over varying areas of the Maghreb, Egypt, Malta and the Levant from 5 January 909 to 1171, during the time that the (Sunni) Abbasid Caliphate ruled from Baghdad. The caliphate was ruled by the Fatimids, who established the Egyptian city of Cairo as their capital. The term Fatimite is sometimes used to refer to the citizens of this caliphate. The ruling elite of the state belonged to the Ismaili branch of Shi'ism. The leaders of the dynasty were also Shi'ite Ismaili religious tribes, hence, they had a religious significance to Ismaili Muslims. They are also part of the chain of holders of the title of Caliph, as recognized by Shi'ites majority. Therefore, this constitutes a rare period in history in which some form of Shi'ism and the title of Caliph were united to any degree. The Fatimids, however, are not recognized nor counted by the Sunnis as a caliphate.
With exceptions, the Fatimids were recorded to exercise a slight degree of tolerance towards non-Shi'ite sects of Islam, as well as towards Jews, Christians and pagans, due to the Shi'ite minority in every single land they conquered.[1]
Shadow Caliphate.
1258 saw the conquest of Baghdad and the murder of the Abassid ruler al-Musta'sim by Mongol forces under Hulagu Khan. A surviving member of the Abbasid House was installed as Caliph at Cairo under the patronage of the Mamluk Sultanate three years later. However, the authority of this line of Caliphs was heavily restrained to ceremonial and religious matters, and Muslim historians refer to it as a "shadow" of Abbasid rule.
Ottomans.
As the Ottoman Empire grew in size and strength, Ottoman rulers beginning with Mehmed II began to claim caliphal authority. Their claim was strengthened when the Ottomans defeated the Mamluks in 1517 and annexed the Arab lands. The last Abbasid ruler at Cairo, al-Mutawakkil III, became a political prisoner and was taken to Constantinople (conquered in 1453), where he was forced to surrender his authority to Selim I. However Ottoman rulers were known by the title of Sultan.
According to Barthold, the first time the title of caliph was used as a political instead of symbolic religious title by the Ottomans was in the Treaty of Küçük Kaynarca ending the Russo-Turkish War of 1768–1774. The outcome of this war was disastrous for the Ottomans. Large territories, including those with large Muslim populations such as the Crimean Peninsula, were lost to the Christian Russian Empire. However, the Ottomans under Abdulhamid I claimed a diplomatic victory, the recognition of themselves as protectors of Muslims in Russia as part of the peace treaty. This was the first time the Ottoman caliph was acknowledged as having political significance outside of Ottoman borders by a European power. As a consequence of this diplomatic victory, as the Ottoman borders were shrinking, the powers of the Ottoman caliph increased.
Around 1880 Sultan Abdulhamid II reasserted the title as a way of countering creeping European colonialism in Muslim lands. His claim was most fervently accepted by the Barelwis of British India. By the eve of the First World War, the Ottoman state, despite its weakness vis-à-vis Europe, represented the largest and most powerful independent Islamic political entity. But the sultan also enjoyed some authority beyond the borders of his shrinking empire as caliph of Muslims in Egypt, India and Central Arabia.
Abolition of the institution.
The Khilafat movement (1919–1924) was a pan-Islamic, political protest campaign launched by Muslims in British India to influence the British government and to protect the Ottoman Empire at the end of the First World War. After the Armistice of Mudros of October 1918 with the military occupation of Istanbul and Treaty of Versailles (1919), the position of the Ottomans was uncertain. The movement to protect or restore the Ottomans gained force after the Treaty of Sèvres (August 1920) which imposed the partitioning of the Ottoman Empire and gave Greece a powerful position in Anatolia, to the distress of the Turks. They called for help and the movement was the result. The movement had collapsed by late 1922.
The new ruler of Turkey, Mustafa Kemal Atatürk, wanted a secular state. On 3 March 1924, the Turkish Grand National Assembly dissolved the institution of the Caliphate.
Prior to its reestablishment, occasional demonstrations were held calling for the reestablishment of the Caliphate. Organisations which call for the re-establishment of the Caliphate include Hizb ut-Tahrir and the Muslim Brotherhood.
Ahmadiyya Caliphate.
The Ahmadiyya Muslim Community, a messianic movement in Islam, "believes" that the Ahmadiyya Caliphate established after the passing of the community's founder Mirza Ghulam Ahmad is the re-establishment of the Rashidun Caliphate (The Rightly Guided Caliphs) as prophesized by Muhammad. The current successor to Mirza Ghulam Ahmad is Khalifatul Masih V, Mirza Masroor Ahmad residing in London, England. 
The Ahmadiyya community's Caliph (Khalifa) has overall authority in all religious and organizational matters. According to Ahmadis, it is not essential for a Muslim Caliph to be the head of a state but instead the spiritual and religious significance of the Caliph should be emphasized. The Ahmadiyya believe their community is about 10 to 20 million strong in 200 countries and territories of the world.
Titular uses.
Secular offices.
In Morocco, the Sherifian Monarch awarded the title "Khalifa" or "Chaliphe", here meaning 'Viceroy', to royal princes (styled Mulay), including future Sultans, who represented the crown in a part of the sultanate:
Sokoto Caliphate.
The leaders of Sokoto Caliphate have used the title Caliph (Amir al-Mu'minin) from 1804.
The Islamic State (ISIS).
On 29 June 2014, the Islamic State of Iraq and the Levant declared the territories under its control (on the Iraq-Syria border) to be a new Caliphate, naming Abu Bakr al-Baghdadi as the Caliph. 
Other uses.
Khalifa can have a definition, be a first name, or family or tribe name. Like many titles, Khalifa also occurs in many names.
It is the family name of the Al Khalifa dynasty, rulers of the peninsular Arab nation of Bahrain, who are descended from the Bani Utub tribe.
Authority of the successor.
The question of who should succeed Muhammad was not the only issue that faced the early Muslims; they also had to clarify the extent of the leader's powers. Muhammad, during his lifetime, was not only the Muslim political leader, but the Islamic prophet. All law and spiritual practice proceeded from Muhammad. Nobody claimed that his successor would be a prophet; succession referred to political authority. The uncertainty centered on the extent of that authority. Muhammad's revelations from God were soon written down (the Qur'an), which was a supreme authority, limiting what any leader could legitimately command, though few "Caliphs" actually abided by it.
However, some later caliphs did believe that they had authority to rule in matters not specified in the Quran. They believed themselves to be temporal and spiritual leaders in issues not commanded in the Quran, and insisted that implicit obedience to the caliph in all things not contradicting the Quran, was the hallmark of the good Muslim. Patricia Crone and Martin Hinds, in their book "God's Caliph", outline the evidence for an early, expansive view of the caliph's importance and authority. They argue that this view of the caliph was eventually nullified (in Sunni Islam, at least) by the rising power of the ulema, or Islamic lawyers, judges, scholars, and religious specialists. The ulema insisted on their right to determine what was legal and orthodox. The proper Muslim leader, in the ulema's opinion, was the leader who enforced the rulings of the ulema, rather than making rulings of his own, unless he himself was qualified in Islamic law. Conflict between caliph and ulema, akin to a modern judiciary, was a recurring theme in early Islamic history, and ended in the victory of the ulema. The caliph was henceforth limited to temporal rule only. He would be considered a righteous caliph if he were guided by the ulema. Crone and Hinds argue that Shi'a Muslims, with their expansive view of the powers of the imamate, have preserved some of the beliefs of the early Umayyad dynasty which ironically, they despise. Crone and Hinds' thesis is not accepted by the scholars, however, who have evidence that the reason behind this is that the Rashidun and Umayyad leaders were Ulema themselves.
Most Mainstream Muslims (non-Shi'ites) believe that the caliph has always been a merely temporal ruler, and that the ulema has always been responsible for enforcing orthodox, Islamic law (shari'a). As for the special role ascribed to the first four caliphs, Islamic tradition holds that they were followers the Qur'an and the way or sunnah of Muhammad in all things and, for this reason, calls them the Rashidun, the Rightly Guided Caliphs. They also believe that the Khilafah ended towards the end of Ali's reign and not with the fall of the Ottomans. Every Muslim leader after Ali and Muawiyah, they say, were not legitimate due to they not being accepted by all the Muslims. Caliphs are not a significant part of mainstream Islam as opposed to the Shi'ites.
Al-Ghazali on the desired character traits for administration.
Al Ghazali's "Nasihat al-Muluk" or "Advice for Kings", part of the Nasîhatnâme genre, gave ten different ethics of royal administration:
Single Caliph for the Muslim world.
According to the Sahih Muslim hadith, Muhammad said:
According to the "Sīrat Rasūl Allāh" of Ibn Isḥaq, Abu-Bakr, Muhammad's closest friend, said:
Umar bin Al-Khattab another disciple of Muhammad is reported by the same source to have said: "There is no way for two (leaders) together at any one time"
Imam Al-Nawawi a 12th-century authority of the Sunni Shafi'i madhhab said: "It is forbidden to give an oath to two caliphs or more, even in different parts of the world and even if they are far apart"
Notable Caliphs.
Several Arabic surnames found throughout the Middle East are derived from the word "khalifa". These include: Khalif, Khalifa, Khillif, Kalif, Kalaf, Khalaf, and Kaylif. The usage of this title as a surname is comparable to the existence of surnames such as King, Duke, and Noble in the English language.
Dynasties.
The more important dynasties include:
Note on the overlap of Umayyad and Abbasid Caliphates: After the massacre of the Umayyad clan by the Abbassids, one lone prince escaped and fled to North Africa, which remained loyal to the Umayyads. This was Abd-ar-rahman I. From there, he proceeded to Spain, where he overthrew and united the provinces conquered by previous Umayyad Caliphs (in 712 and 712). From 756 to 929, this Umayyad domain in Spain was an independent emirate, until Abd-ar-rahman III reclaimed the title of Caliph for his dynasty. The Umayyad Emirs of Spain are not listed in the summary below because they did not claim the caliphate until 929. For a full listing of all the Umayyad rulers in Spain see the Umayyad article.
Claims to the caliphate.
Many local rulers in Islamic countries have claimed to be caliphs. Most claims were ignored outside their limited domains. In many cases, these claims were made by rebels against established authorities and ended when the rebellion was crushed. Notable claimants include:

</doc>
<doc id="7246" url="http://en.wikipedia.org/wiki?curid=7246" title="Charles Messier">
Charles Messier

Charles Messier (26 June 1730 – 12 April 1817) was a French astronomer most notable for publishing an astronomical catalogue consisting of nebulae and star clusters that came to be known as the 110 "Messier objects". The purpose of the catalogue was to help astronomical observers, in particular comet hunters such as himself, distinguish between permanent and transient visually diffuse objects in the sky.
Biography.
Messier was born in Badonviller in the Lorraine region of France, being the tenth of twelve children of Françoise B. Grandblaise and Nicolas Messier, a Court usher. Six of his brothers and sisters died while young and in 1741, his father died. Charles' interest in astronomy was stimulated by the appearance of the spectacular, great six-tailed comet in 1744 and by an annular solar eclipse visible from his hometown on 25 July 1748.
In 1751 he entered the employ of Joseph Nicolas Delisle, the astronomer of the French Navy, who instructed him to keep careful records of his observations. Messier's first documented observation was that of the Mercury transit of 6 May 1753.
In 1764, he was made a fellow of the Royal Society, in 1769, he was elected a foreign member of the Royal Swedish Academy of Sciences, and on 30 June 1770, he was elected to the French Academy of Sciences.
Messier discovered 13 comets :
Messier is buried in Père Lachaise Cemetery, Paris, in Section 11. The grave is fairly plain and faintly inscribed, and while it is not on most maps of the cemetery, it can be found near the grave of Frédéric Chopin, slightly to the west and directly north, and behind the small mausoleum of the jeweller Abraham-Louis Breguet.
The Messier catalogue.
Messier's occupation as a comet hunter led him to continually come across fixed diffuse objects in the night sky which could be mistaken for comets (they are known today to be galaxies, nebulae, and star clusters). He compiled a list of them, in collaboration with his friend and assistant Pierre Méchain (who may have found at least 20 of the objects), to avoid wasting time sorting them out from the comets they were looking for. 
Messier did his observing with 100 mm (four inch) refracting telescope from Hôtel de Cluny (now the Musée national du Moyen Âge), in Paris, France. The list he compiled contains only objects found in the sky area he could observe: from the north celestial pole to a celestial latitude of about −35.7° and are not organized scientifically by object type, or even by location. The first version of Messier's catalogue contained 45 objects and was published in 1774 in the journal of the French Academy of Sciences in Paris. In addition to his own discoveries, this version included objects previously observed by other astronomers, with only 17 of the 45 objects being Messier’s. By 1780 the catalog had increased to 80 objects.
The final version of the catalogue was published in 1781, in "Connoissance des Temps for 1784". The final list of Messier objects had grown to 103. On several occasions between 1921 and 1966, astronomers and historians discovered evidence of another seven objects that were observed either by Messier or by Méchain, shortly after the final version was published. These seven objects, M104 through M110, are accepted by astronomers as "official" Messier objects.
The objects' Messier designations, from M1 to M110, still are in use by professional and amateur astronomers today and their relative brightness makes them popular objects in the amateur astronomical community.
Legacy.
The crater Messier on the Moon and the asteroid 7359 Messier were named in his honor.

</doc>
<doc id="7247" url="http://en.wikipedia.org/wiki?curid=7247" title="Cemetery H culture">
Cemetery H culture

The Cemetery H culture developed out of the northern part of the Indus Valley Civilization around 1700 BCE, in and around western Punjab region located in present-day India and Pakistan. It was named after a cemetery found in "area H" at Harappa.
The Cemetery H culture is part of the Punjab Phase, one of three cultural phases that developed in the Localization Era of the Indus Valley Tradition. It is considered to be part of the Late Harappan phase.
Remains of the culture have been dated from about 1900 BCE until about 1300 BCE. Together with the Gandhara grave culture and the Ochre Coloured Pottery culture, it is considered by some scholars a nucleus of Vedic civilization. 
Cremation in India is first attested in the Cemetery H culture, considered the formative stage of Vedic civilization. The Rigveda contains a reference to the emerging practice, in RV 10.15.14, where the forefathers "both cremated ("agnidagdhá-") and uncremated ("ánagnidagdha-")" are invoked.
The distinguishing features of this culture include:
The Cemetery H culture also "shows clear biological affinities" with the earlier population of Harappa.
The archaeologist Kenoyer noted that this culture "may only reflect a change in the focus of settlement organization from that which was the pattern of the earlier Harappan phase and not cultural discontinuity, urban decay, invading aliens, or site abandonment, all of which have been suggested in the past."

</doc>
<doc id="7248" url="http://en.wikipedia.org/wiki?curid=7248" title="Corrado Gini">
Corrado Gini

Corrado Gini (May 23, 1884 – March 13, 1965) was an Italian statistician, demographer and sociologist who developed the Gini coefficient, a measure of the income inequality in a society. Gini was also a leading fascist theorist and ideologue who wrote "The Scientific Basis of Fascism" in 1927. Gini was a proponent of organicism and applied it to nations.
Career.
Gini was born on May 23, 1884, in Motta di Livenza, near Treviso, into an old landed family. He entered the Faculty of Law at the University of Bologna, where in addition to law he studied mathematics, economics, and biology.
Gini's scientific work ran in two directions: towards the social sciences and towards statistics. His interests ranged well beyond the formal aspects of statistics—to the laws that govern biological and social phenomena.
His first published work was, "Il sesso dal punto di vista statistico" (1908). This work is a thorough review of the natal sex ratio, looking at past theories and at how new hypothesis fit the statistical data. In particular, it presents evidence that the tendency to produce one or the other sex of child is, to some extent, heritable.
In 1910, he acceded to the Chair of Statistics in the University of Cagliari and then at Padua in 1913.
He founded the statistical journal "Metron" in 1920, directing it until his death; it only accepted articles with practical applications.
He became a professor at the Sapienza University of Rome in 1925. At the University, he founded a lecture course on sociology, maintaining it until his retirement. He also set up the School of Statistics in 1928, and, in 1936, the Faculty of Statistical, Demographic and Actuarial Sciences.
In 1929, Gini founded the Italian Committee for the Study of Population Problems ("Comitato italiano per lo studio dei problemi della popolazione) " which, two years later, organised the first Population Congress in Rome.
In 1926, he was appointed President of the Central Institute of Statistics in Rome. This he organised as a single centre for Italian statistical services. He resigned in 1932 in protest at interference in his work by the fascist state.
Milestones during the rest of his career include:
Italian Unionist Movement.
On October 12, 1944, Gini joined with the Calabrian activist Santi Paladino, and fellow-statistician Ugo Damiani to found the Italian Unionist Movement, for which the emblem was the Stars and Stripes, the Italian flag and a world map. According to the three men, the Government of the United States should annex all free and democratic nations worldwide, thereby transforming itself into a world government, and allowing Washington DC to maintain Earth in a perpetual condition of peace. The party existed up to 1948 but had little success and its aims were not supported by the United States.
Organicism and nations.
Gini was a proponent of organicism and saw nations as organic in nature. Gini shared the view held by Oswald Spengler that populations go through a cycle of birth, growth, and decay. Gini claimed that nations at a primitive level have a high birth rate, but, as they evolve, the upper classes birth rate drops while the lower class birth rate, while higher, will inevitably deplete as their stronger members emigrate, die in war, or enter into the upper classes. If a nation continues on this path without resistance, Gini claimed the nation would enter a final decadent stage where the nation would degenerate as noted by decreasing birth rate, decreasing cultural output, and the lack of imperial conquest. At this point, the decadent nation with its aging population can be overrun by a more youthful and vigorous nation. Gini's organicist theories of nations and natality are believed to have influenced policies of Italian Fascism.
Honours.
The following honorary degrees were conferred upon him:

</doc>
<doc id="7249" url="http://en.wikipedia.org/wiki?curid=7249" title="Crankshaft">
Crankshaft

The crankshaft, sometimes abbreviated to "crank", is responsible for conversion between reciprocating motion and rotational motion. In a reciprocating engine, it translates reciprocating linear piston motion into rotational motion, whereas in a reciprocating compressor, it converts the rotational motion into reciprocating motion. In order to do the conversion between two motions, the crankshaft has "crank throws" or "crankpins", additional bearing surfaces whose axis is offset from that of the crank, to which the "big ends" of the connecting rods from each cylinder attach.
It is typically connected to a flywheel to reduce the pulsation characteristic of the four-stroke cycle, and sometimes a torsional or vibrational damper at the opposite end, to reduce the torsional vibrations often caused along the length of the crankshaft by the cylinders farthest from the output end acting on the torsional elasticity of the metal.
History.
Western World.
Classical Antiquity.
A Roman iron crankshaft of yet unknown purpose dating to the 2nd century AD was excavated in Augusta Raurica, Switzerland. The 82.5 cm long piece has fitted to one end a 15 cm long bronze handle, the other handle being lost.
The earliest evidence, anywhere in the world, for a crank and connecting rod in a machine appears in the late Roman Hierapolis sawmill from the 3rd century AD and two Roman stone sawmills at Gerasa, Roman Syria, and Ephesus, Asia Minor (both 6th century AD). On the pediment of the Hierapolis mill, a waterwheel fed by a mill race is shown transmitting power through a gear train to two frame saws, which cut rectangular blocks by way of some kind of connecting rods and, through mechanical necessity, cranks. The accompanying inscription is in Greek.
The crank and connecting rod mechanisms of the other two archaeologically attested sawmills worked without a gear train. In ancient literature, we find a reference to the workings of water-powered marble saws close to Trier, now Germany, by the late 4th century poet Ausonius; about the same time, these mill types seem also to be indicated by the Christian saint Gregory of Nyssa from Anatolia, demonstrating a diversified use of water-power in many parts of the Roman Empire. The three finds push back the date of the invention of the crank and connecting rod back by a full millennium; for the first time, all essential components of the much later steam engine were assembled by one technological culture:
Middle Ages.
The Italian physician Guido da Vigevano (c. 1280−1349), planning for a new crusade, made illustrations for a paddle boat and war carriages that were propelled by manually turned compound cranks and gear wheels (center of image). The Luttrell Psalter, dating to around 1340, describes a grindstone rotated by two cranks, one at each end of its axle; the geared hand-mill, operated either with one or two cranks, appeared later in the 15th century;
Renaissance.
The first depictions of the compound crank in the carpenter's brace appear between 1420 and 1430 in various northern European artwork. The rapid adoption of the compound crank can be traced in the works of the Anonymous of the Hussite Wars, an unknown German engineer writing on the state of the military technology of his day: first, the connecting-rod, applied to cranks, reappeared, second, double compound cranks also began to be equipped with connecting-rods and third, the flywheel was employed for these cranks to get them over the 'dead-spot'. 
In Renaissance Italy, the earliest evidence of a compound crank and connecting-rod is found in the sketch books of Taccola, but the device is still mechanically misunderstood. A sound grasp of the crank motion involved is demonstrated a little later by Pisanello, who painted a piston-pump driven by a water-wheel and operated by two simple cranks and two connecting-rods.
One of the drawings of the Anonymous of the Hussite Wars shows a boat with a pair of paddle-wheels at each end turned by men operating compound cranks (see above). The concept was much improved by the Italian Roberto Valturio in 1463, who devised a boat with five sets, where the parallel cranks are all joined to a single power source by one connecting-rod, an idea also taken up by his compatriot Francesco di Giorgio. 
Crankshafts were also described by Konrad Kyeser (d. 1405), Leonardo da Vinci (1452–1519) and a Dutch "farmer" by the name Cornelis Corneliszoon van Uitgeest in 1592. His wind-powered sawmill used a crankshaft to convert a windmill's circular motion into a back-and-forward motion powering the saw. Corneliszoon was granted a patent for his crankshaft in 1597.
From the 16th century onwards, evidence of cranks and connecting rods integrated into machine design becomes abundant in the technological treatises of the period: Agostino Ramelli's "The Diverse and Artifactitious Machines" of 1588 alone depicts eighteen examples, a number that rises in the "Theatrum Machinarum Novum" by Georg Andreas Böckler to 45 different machines, one third of the total.
Middle and Far East.
Al-Jazari (1136–1206) described a crank and connecting rod system in a rotating machine in two of his water-raising machines. His twin-cylinder pump incorporated a crankshaft, but the device was unnecessarily complex indicating that he still did not fully understand the concept of power conversion. In China, the potential of the crank of converting circular motion into reciprocal one never seems to have been fully realized, and the crank was typically absent from such machines until the turn of the 20th century.
Internal combustion engines.
Large engines are usually multicylinder to reduce pulsations from individual firing strokes, with more than one piston attached to a complex crankshaft. Many small engines, such as those found in mopeds or garden machinery, are single cylinder and use only a single piston, simplifying crankshaft design. This engine can also be built with no riveted seam.
Bearings.
The crankshaft has a linear axis about which it rotates, typically with several bearing journals riding on replaceable bearings (the main bearings) held in the engine block. As the crankshaft undergoes a great deal of sideways load from each cylinder in a multicylinder engine, it must be supported by several such bearings, not just one at each end. This was a factor in the rise of V8 engines, with their shorter crankshafts, in preference to straight-8 engines. The long crankshafts of the latter suffered from an unacceptable amount of flex when engine designers began using higher compression ratios and higher rotational speeds. High performance engines often have more main bearings than their lower performance cousins for this reason.
Piston stroke.
The distance the axis of the crank throws from the axis of the crankshaft determines the piston stroke measurement, and thus engine displacement. A common way to increase the low-speed torque of an engine is to increase the stroke, sometimes known as "shaft-stroking." This also increases the reciprocating vibration, however, limiting the high speed capability of the engine. In compensation, it improves the low speed operation of the engine, as the longer intake stroke through smaller valve(s) results in greater turbulence and mixing of the intake charge. Most modern high speed production engines are classified as "over square" or short-stroke, wherein the stroke is less than the diameter of the cylinder bore. As such, finding the proper balance between shaft-stroking speed and length leads to better results.
Engine configuration.
The configuration and number of pistons in relation to each other and the crank leads to straight, V or flat engines. The same basic engine block can be used with different crankshafts, however, to alter the firing order; for instance, the 90° V6 engine configuration, in older days sometimes derived by using six cylinders of a V8 engine with what is basically a shortened version of the V8 crankshaft, produces an engine with an inherent pulsation in the power flow due to the "missing" two cylinders. The same engine, however, can be made to provide evenly spaced power pulses by using a crankshaft with an individual crank throw for each cylinder, spaced so that the pistons are actually phased 120° apart, as in the GM 3800 engine. While production V8 engines use four crank throws spaced 90° apart, high-performance V8 engines often use a "flat" crankshaft with throws spaced 180° apart. The difference can be heard as the flat-plane crankshafts result in the engine having a smoother, higher-pitched sound than cross-plane (for example, IRL IndyCar Series compared to NASCAR Sprint Cup Series, or a Ferrari 355 compared to a Chevrolet Corvette). See the main article on crossplane crankshafts.
Engine balance.
For some engines it is necessary to provide counterweights for the reciprocating mass of each piston and connecting rod to improve engine balance. These are typically cast as part of the crankshaft but, occasionally, are bolt-on pieces. While counter weights add a considerable amount of weight to the crankshaft, it provides a smoother running engine and allows higher RPM levels to be reached.
Rotary aircraft engines.
Some early aircraft engines were a rotary engine design, where the crankshaft was fixed to the airframe and instead the cylinders rotated with the propeller.
Radial engines.
The radial engine is a reciprocating type internal combustion engine configuration in which the cylinders point outward from a central crankshaft like the spokes of a wheel. It resembles a stylized star when viewed from the front, and is called a "star engine" (German Sternmotor, French Moteur en étoile) in some languages. The radial configuration was very commonly used in aircraft engines before turbine engines became predominant.
Construction.
Crankshafts can be monolithic (made in a single piece) or assembled from several pieces. Monolithic crankshafts are most common, but some smaller and larger engines use assembled crankshafts.
Forging and casting.
Crankshafts can be forged from a steel bar usually through roll forging or cast in ductile steel. Today more and more manufacturers tend to favor the use of forged crankshafts due to their lighter weight, more compact dimensions and better inherent damping. With forged crankshafts, vanadium microalloyed steels are mostly used as these steels can be air cooled after reaching high strengths without additional heat treatment, with exception to the surface hardening of the bearing surfaces. The low alloy content also makes the material cheaper than high alloy steels. Carbon steels are also used, but these require additional heat treatment to reach the desired properties. Iron crankshafts are today mostly found in cheaper production engines (such as those found in the Ford Focus diesel engines) where the loads are lower. Some engines also use cast iron crankshafts for low output versions while the more expensive high output version use forged steel.
Machining.
Crankshafts can also be machined out of a billet, often a bar of high quality vacuum remelted steel. Though the fiber flow (local inhomogeneities of the material's chemical composition generated during casting) doesn’t follow the shape of the crankshaft (which is undesirable), this is usually not a problem since higher quality steels, which normally are difficult to forge, can be used. These crankshafts tend to be very expensive due to the large amount of material that must be removed with lathes and milling machines, the high material cost, and the additional heat treatment required. However, since no expensive tooling is needed, this production method allows small production runs without high costs.
In an effort to reduce costs, used crankshafts may also be machined. A good core may often be easily reconditioned by a crankshaft grinding process. Severely damaged crankshafts may also be repaired with a welding operation, prior to grinding, that utilizes a submerged arc welding machine. To accommodate the smaller journal diameters a ground crankshaft has, and possibly an over-sized thrust dimension, undersize engine bearings are used to allow for precise clearances during operation.
Fatigue strength.
The fatigue strength of crankshafts is usually increased by using a radius at the ends of each main and crankpin bearing. The radius itself reduces the stress in these critical areas, but since the radius in most cases is rolled, this also leaves some compressive residual stress in the surface, which prevents cracks from forming.
Hardening.
Most production crankshafts use induction hardened bearing surfaces, since that method gives good results with low costs. It also allows the crankshaft to be reground without re-hardening. But high performance crankshafts, billet crankshafts in particular, tend to use nitridization instead. Nitridization is slower and thereby more costly, and in addition it puts certain demands on the alloying metals in the steel to be able to create stable nitrides. The advantage of nitridization is that it can be done at low temperatures, it produces a very hard surface, and the process leaves some compressive residual stress in the surface, which is good for fatigue properties. The low temperature during treatment is advantageous in that it doesn’t have any negative effects on the steel, such as annealing. With crankshafts that operate on roller bearings, the use of carburization tends to be favored due to the high Hertzian contact stresses in such an application. Like nitriding, carburization also leaves some compressive residual stresses in the surface.
Counterweights.
Some expensive, high performance crankshafts also use heavy-metal counterweights to make the crankshaft more compact. The heavy-metal used is most often a tungsten alloy but depleted uranium has also been used. A cheaper option is to use lead, but compared with tungsten its density is much lower.
Stress on crankshafts.
The shaft is subjected to various forces but generally needs to be analysed in two positions.
Firstly, failure may occur at the position of maximum bending; this may be at the centre of the crank or at either end. In such a condition the failure is due to bending and the pressure in the cylinder is maximal. Second, the crank may fail due to twisting, so the conrod needs to be checked for shear at the position of maximal twisting. The pressure at this position is the maximal pressure, but only a fraction of maximal pressure."

</doc>
<doc id="7251" url="http://en.wikipedia.org/wiki?curid=7251" title="Central nervous system">
Central nervous system

The central nervous system (CNS) is the part of the nervous system consisting of the brain and spinal cord. The peripheral nervous system (or PNS), is composed of nerves leading to and from the CNS, often through junctions known as ganglia.
The central nervous system is so named because it integrates information it receives from, and coordinates and influences the activity of, all parts of the bodies of bilaterally symmetric animals — that is, all multicellular animals except sponges and radially symmetric animals such as jellyfish, and it contains the majority of the nervous system. Arguably, many consider the retina and the optic nerve (2nd cranial nerve), as well as the olfactory nerves (1st) and olfactory epithelium as parts of the CNS, synapsing directly on brain tissue without intermediate ganglia. Following this classification the olfactory epithelium is the only central nervous tissue in direct contact with the environment, which opens up for therapeutic treatments.
 The CNS is contained within the dorsal cavity, with the brain in the cranial cavity and the spinal cord in the spinal cavity. In vertebrates, the brain is protected by the skull, while the spinal cord is protected by the vertebrae, both enclosed in the meninges.
Structure.
The central nervous system consists of the two major structures: the brain and spinal cord.
The brain is encased in the skull, and protected by the cranium. The spinal cord is continuous with the brain and lies caudaly to the brain, and is protected by the vertebra. The spinal cord reaches from the base of the skull, continues through or starting below the foramen magnum, and terminates roughly level with the first or second lumbar vertebra, occupying the upper sections of the vertebral canal.
White and gray matter.
Microscopically, there are differences between the neurons and tissue of the central nervous system and the peripheral nervous system. The central nervous system is divided in white and gray matter. This can also be seen macroscopically on brain tissue. The white matter constitutes of axons and oligodendrocytes, while the gray matter chiefly constitutes of neurons. Both tissues include a number of glial cells (although the white matter contains more), which are often referred to as supporting cells of the central nervous system. Different forms of glial cells have different functions, some acting almost as scaffolding for neuroblasts to climb during neurogenesis such as bergmann glia, while others such as microglia are a specialized form of macrophage, involved in the immune system of the brain as well as the clearance of various metabolites from the brain tissue. Astrocytes may be involved with both clearance of metabolites as well as transport of fuel and various beneficial substances to neurons from the capillaries of the brain. Upon CNS injury astrocytes will proliferate, causing gliosis, a form of neuronal scar tissue, lacking in functional neurons.
The brain (cerebrum as well as midbrain and hindbrain) consists of a cortex, composed of neuron-bodies constituting gray matter, while internally there is more white matter that form tracts and commissures. Apart from cortical gray matter there is also subcortical gray making up a large number of different nuclei.
Spinal cord.
From and to the spinal cord are projections of the peripheral nervous system in the form of spinal nerves (sometimes segmental nerves). The nerves connect the spinal cord with skin, joints, muscles etc. and allow for the transmission of efferent motor as well as afferent sensory signals and stimuli. This allows for voluntary and involuntary motions of muscles, as well as the perception of senses.
All in all 31 spinal nerves project from the brain stem, some forming plexa as they branch out, such as the brachial plexa, sacral plexa etc. Each spinal nerve will carry both sensory and motor signals, but the nerves synapse at different regions of the spinal cord, either from the periphery to sensory relay neurons that relay the information to the CNS or from the CNS to motor neurons, which relay the information out.
The spinal cord relays information up to the brain through spinal tracts through the "final common pathway" to the thalamus and ultimately to the cortex. Not all information is relayed to the cortex, and does not reach our immediate consciousness, but is instead transmitted only to the thalamus which sorts and adapts accordingly. This in turn may explain why we are not constantly aware of all aspects of our surroundings.
Cranial nerves.
Apart from the spinal cord, there are also peripheral nerves of the PNS that synapse through intermediaries or ganglia directly on the CNS. These 12 nerves exist in the head and neck region and are called cranial nerves. Cranial nerves bring information to the CNS to and from the face, as well as to certain muscles (such as the trapezius muscle, which is innervated by accessory nerves as well as certain cervical spinal nerves).
Two pairs of cranial nerves; the olfactory nerves and the optic nerves are often considered structures of the central nervous system. This is because they do not synapse first on peripheral ganglia, but directly on central nervous neurons. The olfactory epithelium is significant in that it consists of central nervous tissue expressed in direct contact to the environment, allowing for administration of certain pharmaceuticals and drugs.
Brain.
Rostrally to the spinal cord lies the brain.
The brain makes up the largest portion of the central nervous system, and is often the main structure referred to when speaking of the nervous system. The brain is the major functional unit of the central nervous system. While the spinal cord has certain processing ability such as that of spinal locomotion and can process reflexes, the brain is the major processing unit of the nervous system.
Brain stem.
The brain stem consists of the medulla, the pons and the midbrain. The medulla can be referred to as an extension of the spinal cord, and its organization and functional properties are similar to those of the spinal cord. The tracts passing from the spinal cord to the brain pass through here.
Regulatory functions of the medulla nuclei include control of the blood pressure and breathing. Other nuclei are involved in balance, taste, hearing and control of muscles of the face and neck.
The next structure rostral to the medulla is the pons, which lies on the ventral anterior side of the brain stem. Nuclei in the pons include pontine nuclei which work with the cerebellum and transmit information between the cerebellum and the cerebral cortex.
In the dorsal posterior pons lie nuclei that have to do with breathing, sleep and taste.
The midbrain (or mesencephalon) is situated above and rostral to the pons, and includes nuclei linking distinct parts of the motor system, among others the cerebellum, the basal ganglia and both cerebral hemispheres. Additionally parts of the visual and auditory systems are located in the mid brain, including control of automatic eye movements.
The brain stem at large provides entry and exit to the brain for a number of pathways for motor and autonomic control of the face and neck through cranial nerves, and autonomic control of the organs is mediated by the tenth cranial (vagus) nerve. A large portion of the brain stem is involved in such autonomic control of the body. Such functions may engage the heart, blood vessels, pupillae, among others.
The brain stem also hold the reticular formation, a group of nuclei involved in both arousal and alertness.
Cerebellum.
The cerebellum lies posteriorly or dorsally and rostrally to the pons. The cerebellum is composed of several dividing fissures and lobes. Its function includes posture and coordination of movements of eyes, limbs as well as that of the head. Further it is involved in motion that has been learned and perfected though practice, and will adapt to new learned movements.
Despite its previous classification as a motor structure, the cerebellum also displays connections to areas of the cerebral cortex involved in language as well as cognitive functions. These connections have been recently shown through anatomical studies, such as fMRI and PET.
The body of the cerebellum holds more neurons than any other structure of the brain including that of the larger cerebrum (or cerebral hemispheres), but is also more extensively understood than other structures of the brain, and includes fewer types of different neurons. It handles and processes sensory stimuli, motor information as well as balance information from the vestibular organ.
Diencephalon.
The two structures of the diecephalon worth noting are the thalamus and the hypothalamus. The thalamus acts as a linkage between incoming pathways from the peripheral nervous system as well as the optical nerve (though it does not receive input from the olfactory nerve) to the cerebral hemispheres. Previously it was considered only a "relay station", but it is engaged in the sorting of information that will reach cerebral hemispheres (neocortex).
Apart from its function of sorting information from the periphery, the thalamus also connects the cerebellum and basal ganglia with the cerebrum. In common with the aforementioned reticular system the thalamus is involved in wakefullness and consciousness, such as though the SCN.
The hypothalamus engages in functions of a number of primitive emotions or feelings such as hunger, thirst and maternal bonding. This is regulated partly through control of secretion of hormones from the pituitary gland. Additionally the hypothalamus plays a role in motivation and many other behaviors of the individual.
Cerebrum.
The cerebrum of cerebral hemispheres make up the largest portion of the human brain. Various structures combine forming the cerebral hemispheres, among others, the cortex, basal ganglia, amygdala and hippocampus. The hemispheres together control a large portion of the functions of the human brain such as emotion, memory, perception and motor functions. Apart from this the cerebral hemispheres stand for the cognitive capabilities of the brain.
Connecting each of the hemispheres is the corpus callosum as well as several additional commissures.
One of the most important parts of the cerebral hemispheres is the cortex, made up of gray matter covering the surface of the brain. Functionally, the cerebral cortex is involved in planning and carrying out of everyday tasks.
The hippocampus is involved in storage of memories, the amygdala plays a role in perception and communication of emotion, while the basal ganglia play a major role in the coordination of voluntary movement.
Difference from the peripheral nervous system.
This differentiates the central nervous system from the peripheral nervous system, which consists of neurons, axons and Schwann cells. Schwann cells and oligodendrocytes have similar functions in the central and peripheral nervous system respectively. Both act to add myelin sheaths to the axons, which acts as a form of insulation allowing for better and faster proliferation of electrical signals along the nerves. Axons in the central nervous system are often very short (barely a few millimeters) and do not need the same degree of isolation as peripheral nerves do. Some peripheral nerves can be over 1m in length, such as the nerves to the big toe. To ensure signals move at sufficient speed, myelination is needed.
The way in which the Schwann cells and oligodendrocytes myelinate nerves differ. A Schwann cell usually myelinates a single axon, completely surrounding it. Sometimes they may myelinate many axons, especially when in areas of short axons. Oligodendrocytes usually myelinate several axons. They do this by sending out thin projections of their cell membrane which envelop and enclose the axon.
Development.
During early development of the vertebrate embryo, a longitudinal groove on the neural plate gradually deepens and the ridges on either side of the groove (the neural folds) become elevated, and ultimately meet, transforming the groove into a closed tube, the ectodermal wall of which forms the rudiment of the nervous system. This tube initially differentiates into three vesicles (pockets): the prosencephalon at the front, the mesencephalon, and, between the mesencephalon and the spinal cord, the rhombencephalon. (By six weeks in the human embryo) the prosencephalon then divides further into the telencephalon and diencephalon; and the rhombencephalon divides into the metencephalon and myelencephalon.
As a vertebrate grows, these vesicles differentiate further still. The telencephalon differentiates into, among other things, the striatum, the hippocampus and the neocortex, and its cavity becomes the first and second ventricles. Diencephalon elaborations include the subthalamus, hypothalamus, thalamus and epithalamus, and its cavity forms the third ventricle. The tectum, pretectum, cerebral peduncle and other structures develop out of the mesencephalon, and its cavity grows into the mesencephalic duct (cerebral aqueduct). The metencephalon becomes, among other things, the pons and the cerebellum, the myelencephalon forms the medulla oblongata, and their cavities develop into the fourth ventricle.
Evolution.
Planarians, members of the phylum Platyhelminthes (flatworms), have the simplest, clearly defined delineation of a nervous system into a central nervous system (CNS) and a peripheral nervous system (PNS).
Their primitive brains, consisting of two fused anterior ganglia, and longitudinal nerve cords form the CNS; the laterally projecting nerves form the PNS. A molecular study found that more than 95% of the 116 genes involved in the nervous system of planarians, which includes genes related to the CNS, also exist in humans. Like planarians, vertebrates have a distinct CNS and PNS, though more complex than those of planarians.
The CNS of chordates differs from that of other animals in being placed dorsally in the body, above the gut and notochord/spine. The basic pattern of the CNS is highly conserved throughout the different species of vertebrates and during evolution. The major trend that can be observed is towards a progressive telencephalisation: the telencephalon of reptiles is only an appendix to the large olfactory bulb, while in mammals it makes up most of the volume of the CNS. In the human brain, the telencephalon covers most of the diencephalon and the mesencephalon. Indeed, the allometric study of brain size among different species shows a striking continuity from rats to whales, and allows us to complete the knowledge about the evolution of the CNS obtained through cranial endocasts.
Mammals – which appear in the fossil record after the first fishes, amphibians, and reptiles – are the only vertebrates to possess the evolutionarily recent, outermost part of the cerebral cortex known as the neocortex.
The neocortex of monotremes (the duck-billed platypus and several species of spiny anteaters) and of marsupials (such as kangaroos, koalas, opossums, wombats, and Tasmanian devils) lack the convolutions – gyri and sulci – found in the neocortex of most placental mammals (eutherians).
Within placental mammals, the size and complexity of the neocortex increased over time. The area of the neocortex of mice is only about 1/100 that of monkeys, and that of monkeys is only about 1/10 that of humans. In addition, rats lack convolutions in their neocortex (possibly also because rats are small mammals), whereas cats have a moderate degree of convolutions, and humans have quite extensive convolutions. Extreme convolution of the neocortex is found in dolphins, possibly related to their complex echolocation.
Clinical significance.
Diseases.
There are many central nervous system diseases and conditions, including infections of the central nervous system such as encephalitis and poliomyelitis, early-onset neurological disorders including ADHD and autism, late-onset neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and essential tremor, autoimmune and inflammatory diseases such as multiple sclerosis and acute disseminated encephalomyelitis, genetic disorders such as Krabbe's disease and Huntington's disease, as well as amyotrophic lateral sclerosis and adrenoleukodystrophy. Lastly, cancers of the central nervous system can cause severe illness and, when malignant, can have very high mortality rates.
Specialty professional organizations recommend that neurological imaging of the brain be done only to answer a specific clinical question and not as routine screening.

</doc>
