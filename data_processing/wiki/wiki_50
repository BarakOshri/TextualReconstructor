<doc id="7252" url="http://en.wikipedia.org/wiki?curid=7252" title="Cell cycle">
Cell cycle

The cell cycle, or cell-division cycle, is the series of events that take place in a cell leading to its division and duplication (replication) that produces two daughter cells. In cells without a nucleus (prokaryotic), the cell cycle occurs via a process termed binary fission. In cells with a nucleus (eukaryotes), the cell cycle can be divided in three periods: interphase—during which the cell grows, accumulating nutrients needed for mitosis preparing it for cell division and duplicating its DNA—and the mitotic (M) phase, during which the cell splits itself into two distinct cells, often called "daughter cells" and the final phase, cytokinesis, where the new cell is completely divided. The cell-division cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed.
After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of cell division.
G0 phase.
The word "post-mitotic" is sometimes used to refer to both quiescent and senescent cells. Nonproliferative cells in multicellular eukaryotes generally enter the quiescent G0 state from G1 and may remain quiescent for long periods of time, possibly indefinitely (as is often the case for neurons). This is very common for cells that are fully differentiated. Cellular senescence occurs in response to DNA damage or degradation that would make a cell's progeny nonviable; it is often a biochemical reaction; division of such a cell could, for example, become cancerous. Some cells enter the G0 phase semi-permanentally e.g., some liver, kidney, stomach cells. Many cells do not enter G0 and continue to divide throughout an organism's life, e.g. epithelial cells.
Interphase.
Before a cell can enter cell division, it needs to take in nutrients. All of the preparations are done during interphase. Interphase is a series of changes that takes place in a newly formed cell and its nucleus, before it becomes capable of division again. It is also called preparatory phase or intermitosis. Previously it was called resting stage because there is no apparent activity related to cell division.Typically interphase lasts for at least 90% of the total time required for the cell cycle.
Interphase proceeds in three stages, G1, S, and G2, preceded by the previous cycle of mitosis and cytokinesis. The most significant event is the replication of genetic material (DNA) in S phase.
G1 Phase.
The first phase within interphase, from the end of the previous M phase until the beginning of DNA synthesis, is called G1 (G indicating "gap"). It is also called the growth phase. During this phase the biosynthetic activities of the cell, which are considerably slowed down during M phase, resume at a high rate. This phase is marked by the use of 20 amino acids to form millions of proteins and later on enzymes that are required in S phase, mainly those needed for DNA replication. Duration of G1 is highly variable, even among different cells of the same species. It is under the control of the p53 gene. We can say that in this phase, cell increases its supply of proteins, increases the number of organelles (such as mitochondria, ribosomes), and grows in size.
S Phase.
The ensuing S phase starts when DNA replication commences; when it is complete, all of the chromosomes have been replicated, i.e., each chromosome has two (sister) chromatids. Thus, during this phase, the amount of DNA in the cell has effectively doubled, though the ploidy of the cell remains the same. During this phase, synthesis is completed as quickly as possible due to the exposed base pairs being sensitive to harmful external factors such as mutagens.
Mitosis (M phase, mitotic phase).
The relatively brief "M phase" consists of nuclear division (karyokinesis). It is a relatively short period of the cell cycle. M phase is complex and highly regulated. The sequence of events is divided into phases, corresponding to the completion of one set of activities and the start of the next. These phases are sequentially known as:
Mitosis is the process by which a eukaryotic cell separates the chromosomes in its cell nucleus into two identical sets in two nuclei. During the process of mitosis the pairs of chromosomes condense and attach to fibers that pull the sister chromatids to opposite sides of the cell. It is generally followed immediately by cytokinesis, which divides the nuclei, cytoplasm, organelles and cell membrane into two cells containing roughly equal shares of these cellular components. Mitosis and cytokinesis together define the mitotic (M) phase of the cell cycle - the division of the mother cell into two daughter cells, genetically identical to each other and to their parent cell. This accounts for approximately 10% of the cell cycle.
Mitosis occurs exclusively in eukaryotic cells, but occurs in different ways in different species. For example, animals undergo an "open" mitosis, where the nuclear envelope breaks down before the chromosomes separate, while fungi such as "Aspergillus nidulans" and "Saccharomyces cerevisiae" (yeast) undergo a "closed" mitosis, where chromosomes divide within an intact cell nucleus. Prokaryotic cells, which lack a nucleus, divide by a process called binary fission.
Because cytokinesis usually occurs in conjunction with mitosis, "mitosis" is often used interchangeably with "M phase". However, there are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei in a process called endoreplication. This occurs most notably among the fungi and slime moulds, but is found in various groups. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development. Errors in mitosis can either kill a cell through apoptosis or cause mutations that may lead to cancer.
Regulation of eukaryotic cell cycle.
Regulation of the cell cycle involves processes crucial to the survival of a cell, including the detection and repair of genetic damage as well as the prevention of uncontrolled cell division. The molecular events that control the cell cycle are ordered and directional; that is, each process occurs in a sequential fashion and it is impossible to "reverse" the cycle.
Role of cyclins and CDKs.
Two key classes of regulatory molecules, cyclins and cyclin-dependent kinases (CDKs), determine a cell's progress through the cell cycle. Leland H. Hartwell, R. Timothy Hunt, and Paul M. Nurse won the 2001 Nobel Prize in Physiology or Medicine for their discovery of these central molecules. Many of the genes encoding cyclins and CDKs are conserved among all eukaryotes, but in general more complex organisms have more elaborate cell cycle control systems that incorporate more individual components. Many of the relevant genes were first identified by studying yeast, especially "Saccharomyces cerevisiae"; genetic nomenclature in yeast dubs many of these genes "cdc" (for "cell division cycle") followed by an identifying number, e.g., "cdc25" or "cdc20".
Cyclins form the regulatory subunits and CDKs the catalytic subunits of an activated heterodimer; cyclins have no catalytic activity and CDKs are inactive in the absence of a partner cyclin. When activated by a bound cyclin, CDKs perform a common biochemical reaction called phosphorylation that activates or inactivates target proteins to orchestrate coordinated entry into the next phase of the cell cycle. Different cyclin-CDK combinations determine the downstream proteins targeted. CDKs are constitutively expressed in cells whereas cyclins are synthesised at specific stages of the cell cycle, in response to various molecular signals.
General mechanism of cyclin-CDK interaction.
Upon receiving a pro-mitotic extracellular signal, G1 cyclin-CDK complexes become active to prepare the cell for S phase, promoting the expression of transcription factors that in turn promote the expression of S cyclins and of enzymes required for DNA replication. The G1 cyclin-CDK complexes also promote the degradation of molecules that function as S phase inhibitors by targeting them for ubiquitination. Once a protein has been ubiquitinated, it is targeted for proteolytic degradation by the proteasome.
Active S cyclin-CDK complexes phosphorylate proteins that make up the pre-replication complexes assembled during G1 phase on DNA replication origins. The phosphorylation serves two purposes: to activate each already-assembled pre-replication complex, and to prevent new complexes from forming. This ensures that every portion of the cell's genome will be replicated once and only once. The reason for prevention of gaps in replication is fairly clear, because daughter cells that are missing all or part of crucial genes will die. However, for reasons related to gene copy number effects, possession of extra copies of certain genes is also deleterious to the daughter cells.
Mitotic cyclin-CDK complexes, which are synthesized but inactivated during S and G2 phases, promote the initiation of mitosis by stimulating downstream proteins involved in chromosome condensation and mitotic spindle assembly. A critical complex activated during this process is a ubiquitin ligase known as the anaphase-promoting complex (APC), which promotes degradation of structural proteins associated with the chromosomal kinetochore. APC also targets the mitotic cyclins for degradation, ensuring that telophase and cytokinesis can proceed.
Specific action of cyclin-CDK complexes.
Cyclin D is the first cyclin produced in the cell cycle, in response to extracellular signals (e.g. growth factors). Cyclin D binds to existing CDK4, forming the active cyclin D-CDK4 complex. Cyclin D-CDK4 complex in turn phosphorylates the retinoblastoma susceptibility protein (Rb). The hyperphosphorylated Rb dissociates from the E2F/DP1/Rb complex (which was bound to the E2F responsive genes, effectively "blocking" them from transcription), activating E2F. Activation of E2F results in transcription of various genes like cyclin E, cyclin A, DNA polymerase, thymidine kinase, etc. Cyclin E thus produced binds to CDK2, forming the cyclin E-CDK2 complex, which pushes the cell from G1 to S phase (G1/S, which initiates the G2/M transition. Cyclin B-cdc2 complex activation causes breakdown of nuclear envelope and initiation of prophase, and subsequently, its deactivation causes the cell to exit mitosis.
Inhibitors.
Two families of genes, the "cip/kip" ("CDK interacting protein/Kinase inhibitory protein") family and the INK4a/ARF ("In"hibitor of "K"inase 4/"A"lternative "R"eading "F"rame) family, prevent the progression of the cell cycle. Because these genes are instrumental in prevention of tumor formation, they are known as tumor suppressors.
The "cip/kip" family includes the genes p21, p27 and p57. They halt cell cycle in G1 phase, by binding to, and inactivating, cyclin-CDK complexes. p21 is activated by p53 (which, in turn, is triggered by DNA damage e.g. due to radiation). p27 is activated by Transforming Growth Factor of β (TGF β), a growth inhibitor.
The INK4a/ARF family includes p16INK4a, which binds to CDK4 and arrests the cell cycle in G1 phase, and p19ARF which prevents p53 degradation.
Synthetic inhibitors of Cdc25 could also be useful for the arrest of cell cycle and therefore be useful as antineoplastic and anticancer agents.
Transcriptional regulatory network.
Evidence suggests that a semi-autonomous transcriptional network acts in concert with the CDK-cyclin machinery to regulate the cell cycle. Several gene expression studies in "Saccharomyces cerevisiae" have identified approximately 800 to 1200 genes that change expression over the course of the cell cycle; they are transcribed at high levels at specific points in the cell cycle, and remain at lower levels throughout the rest of the cell cycle. While the set of identified genes differs between studies due to the computational methods and criterion used to identify them, each study indicates that a large portion of yeast genes are temporally regulated.
Many periodically expressed genes are driven by transcription factors that are also periodically expressed. One screen of single-gene knockouts identified 48 transcription factors (about 20% of all non-essential transcription factors) that show cell cycle progression defects. Genome-wide studies using high throughput technologies have identified the transcription factors that bind to the promoters of yeast genes, and correlating these findings with temporal expression patterns have allowed the identification of transcription factors that drive phase-specific gene expression. The expression profiles of these transcription factors are driven by the transcription factors that peak in the prior phase, and computational models have shown that a CDK-autonomous network of these transcription factors is sufficient to produce steady-state oscillations in gene expression).
Experimental evidence also suggests that gene expression can oscillate with the period seen in dividing wild-type cells independently of the CDK machinery. Orlando "et al." used microarrays to measure the expression of a set of 1,271 genes that they identified as periodic in both wild type cells and cells lacking all S-phase and mitotic cyclins ("clb1,2,3,4,5,6"). Of the 1,271 genes assayed, 882 continued to be expressed in the cyclin-deficient cells at the same time as in the wild type cells, despite the fact that the cyclin-deficient cells arrest at the border between G1 and S phase. However, 833 of the genes assayed changed behavior between the wild type and mutant cells, indicating that these genes are likely directly or indirectly regulated by the CDK-cyclin machinery. Some genes that continued to be expressed on time in the mutant cells were also expressed at different levels in the mutant and wild type cells. These findings suggest that while the transcriptional network may oscillate independently of the CDK-cyclin oscillator, they are coupled in a manner that requires both to ensure the proper timing of cell cycle events. Other work indicates that phosphorylation, a post-translational modification, of cell cycle transcription factors by Cdk1 may alter the localization or activity of the transcription factors in order to tightly control timing of target genes (Ubersax "et al." 2003; Sidorova "et al." 1995; White "et al." 2009).
While oscillatory transcription plays a key role in the progression of the yeast cell cycle, the CDK-cyclin machinery operates independently in the early embryonic cell cycle. Before the midblastula transition, zygotic transcription does not occur and all needed proteins, such as the B-type cyclins, are translated from maternally loaded mRNA.
DNA replication and DNA replication origin activity.
Analyses of synchronized cultures of "Saccharomyces cerevisiae" under conditions that prevent DNA replication initiation without delaying cell cycle progression showed that
origin licensing decreases the expression of genes with origins near their 3' ends, revealing that downstream origins can regulate the expression of upstream genes.
This confirms previous predictions from mathematical modeling of a global causal coordination between DNA replication origin activity and mRNA expression,
and shows that mathematical modeling of DNA microarray data can be used to correctly predict previously unknown biological modes of regulation.
Checkpoints.
Cell cycle checkpoints are used by the cell to monitor and regulate the progress of the cell cycle. Checkpoints prevent cell cycle progression at specific points, allowing verification of necessary phase processes and repair of DNA damage. The cell cannot proceed to the next phase until checkpoint requirements have been met.
Several checkpoints are designed to ensure that damaged or incomplete DNA is not passed on to daughter cells. Three main checkpoints exist: the G1/S checkpoint, the G2/M checkpoint and the metaphase (mitotic) checkpoint. G1/S transition is a rate-limiting step in the cell cycle and is also known as restriction point. An alternative model of the cell cycle response to DNA damage has also been proposed, known as the postreplication checkpoint.
p53 plays an important role in triggering the control mechanisms at both G1/S and G2/M checkpoints.
Role in tumor formation.
A disregulation of the cell cycle components may lead to tumor formation. As mentioned above, when some genes like the cell cycle inhibitors, RB, p53 etc. mutate, they may cause the cell to multiply uncontrollably, forming a tumor. Although the duration of cell cycle in tumor cells is equal to or longer than that of normal cell cycle, the proportion of cells that are in active cell division (versus quiescent cells in G0 phase) in tumors is much higher than that in normal tissue. Thus there is a net increase in cell number as the number of cells that die by apoptosis or senescence remains the same.
The cells which are actively undergoing cell cycle are targeted in cancer therapy as the DNA is relatively exposed during cell division and hence susceptible to damage by drugs or radiation. This fact is made use of in cancer treatment; by a process known as debulking, a significant mass of the tumor is removed which pushes a significant number of the remaining tumor cells from G0 to G1 phase (due to increased availability of nutrients, oxygen, growth factors etc.). Radiation or chemotherapy following the debulking procedure kills these cells which have newly entered the cell cycle.
The fastest cycling mammalian cells in culture, crypt cells in the intestinal epithelium, have a cycle time as short as 9 to 10 hours. Stem cells in resting mouse skin may have a cycle time of more than 200 hours. Most of this difference is due to the varying length of G1, the most variable phase of the cycle. M and S do not vary much.
In general, cells are most radiosensitive in late M and G2 phases and most resistant in late S.
For cells with a longer cell cycle time and a significantly long G1 phase, there is a second peak of resistance late in G1.
The pattern of resistance and sensitivity correlates with the level of sulfhydryl compounds in the cell. Sulfhydryls are natural radioprotectors and tend to be at their highest levels in S and at their lowest near mitosis.

</doc>
<doc id="7253" url="http://en.wikipedia.org/wiki?curid=7253" title="Cartesian">
Cartesian

Cartesian means of or relating to the French philosopher and discoverer René Descartes—from his Latinized name "Cartesius". It may refer to:
In mathematics:

</doc>
<doc id="7255" url="http://en.wikipedia.org/wiki?curid=7255" title="Connection (dance)">
Connection (dance)

In partner dancing, connection is a physical communication method used by a pair of dancers to facilitate synchronized dance movement, in which one dancer (the "lead") directs the movements of the other dancer (the "follower") by means of non-verbal directions conveyed through a physical connection between the dancers. It is an essential technique in many types of partner dancing and is used extensively in partner dances that feature significant physical contact between the dancers, including the Argentine Tango, Lindy Hop, Balboa, East Coast Swing, West Coast Swing, Salsa, and Modern Jive.
Other forms of communication, such as visual cues or spoken cues, sometimes aid in connecting with one's partner, but are often used in specific circumstances (e.g., practicing figures, or figures which are purposely danced without physical connection). Connection can be used to transmit power and energy as well as information and signals; some dance forms (and some dancers) primarily emphasize power or signaling, but most are probably a mixture of both.
Following and leading in a partner dance is accomplished by maintaining a physical connection called the frame that allows the leader to transmit body movement to the follower, and for the follower to suggest ideas to the leader. A frame is a stable structural combination of both bodies maintained through the dancers' arms and/or legs.
Connection occurs in both open and closed dance positions (also called "open frame" and "closed frame").
In closed position with body contact, connection is achieved by maintaining the frame. The follower moves to match the leader, maintaining the pressure between the two bodies as well as the position.
When creating frame, tension is the primary means of establishing communication. Changes in tension are made to create rhythmic variations in moves and movements, and are communicated through points of contact. In an open position or a closed position without body contact, the hands and arms alone provide the connection, which may be one of three forms: tension, compression or neutral.
In swing dances, tension and compression may be maintained for a significant period of time. In other dances, such as Latin, tension and compression may be used as indications of upcoming movement. However, in both styles, tension and compression do not signal immediate movement: the follow must be careful not to move prior to actual movement by the lead. Until then, the dancers must match pressures without moving their hands. In some styles of Lindy Hop, the tension may become quite high without initiating movement.
The general rule for open connections is that moves of the leader's hands back, forth, left or right are originated through moves of the entire body. Accordingly, for the follower, a move of the connected hand is immediately transformed into the corresponding move of the body. Tensing the muscles and locking the arm achieves this effect but is neither comfortable nor correct. Such tension eliminates the subtler communication in the connection, and eliminates free movement up and down, such as is required to initiate many turns.
Instead of just tensing the arms, connection is achieved by engaging the shoulder, upper body and torso muscles. Movement originates in the body's core. A leader leads by moving himself and maintaining frame and connection. Different forms of dance and different movements within each dance may call for differences in the connection. In some dances the separation distance between the partners remains pretty constant. In others e.g. Modern Jive moving closer together and further apart are fundamental to the dance, requiring flexion and extension of the arms, alternating compression and tension.
The connection between two partners has a different feel in every dance and with every partner. Good social dancers adapt to the conventions of the dance and the responses of their partners.

</doc>
<doc id="7257" url="http://en.wikipedia.org/wiki?curid=7257" title="Caste">
Caste

Caste is a form of social stratification characterized by endogamy, hereditary transmission of a lifestyle which often includes an occupation, ritual status in a hierarchy and customary social interaction and exclusion based on cultural notions of purity and pollution. According to Human Rights Watch and UNICEF, caste discrimination affects an estimated 250 million people worldwide.
A paradigmatic, ethnographic example is the division of Indian society into rigid social groups, with roots in India's ancient history and persisting until today. Historically, the caste system in India has consisted of thousands of endogamous groups called Jatis or Quoms (among Muslims). Independent India has witnessed caste-related violence. The Nepalese caste system resembles that of the Indian Jāti system with numerous Jāti divisions with a Varna system superimposed for a rough equivalence. Religious, historical and sociocultural factors have helped define the bounds of endogamy for Muslims in some parts of Pakistan. The Caste system in Sri Lanka is a division of society into strata, influenced by the classic Aryan Varnas of North India and the Dravida Jāti system found in South India. 
Balinese caste structure has been described in early 20th-century European literature to be based on three categories – triwangsa (thrice born) or the nobility, dwijati (twice born) in contrast to ekajati (once born) the low folks. In China during the period of Yuan Dynasty, ruler Kublai Khan enforced a "Four Class System", which was a legal caste system. The order of four classes of people was maintained by the information of the descending order were: Mongolian, Semu people, Han people (in the northern areas of China), and Southerners (people of the former Southern Song Dynasty). In Japan's history, social strata based on inherited position rather than personal merits, was rigid and highly formalized. With the unification of the three kingdoms in the 7th century and the foundation of the Goryeo dynasty in the Middle Ages, Koreans systemised its own native class system. 
Yezidi society is hierarchical. In Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Various sociologists have reported caste systems in Africa.
Etymology.
The English word "caste" derives from the Spanish and Portuguese "casta", which the "Oxford English Dictionary" quotes John Minsheu's Spanish dictionary (1599) to mean, "race, lineage, or breed". When the Spanish colonized the New World, they used the word to mean a "clan or lineage." However, it was the Portuguese who employed "casta" in the primary modern sense when they applied it to the many in-marrying hereditary Indian social groups they encountered upon their arrival in India in 1498. The use of the spelling "caste," with this latter meaning, is first attested to in English in 1613.
Caste in South Asia.
Caste system of India.
Historically, the caste system in India has consisted of thousands of endogamous groups called Jatis or Quoms (among Muslims). All the Jatis were clubbed under the "varnas" categories during the British colonial Census of 1901. The terms "varna" (theoretical classification based on occupation) and "jāti" (caste) are two distinct concepts: while varna is the idealised four-part division envisaged by the Twice-Borns, jāti (community) refers to the thousands of actual endogamous groups prevalent across the subcontinent. A jati may be divided into exogamous groups based on same gotras. The classical authors scarcely speak of anything other than the varnas; even Indologists sometimes confuse the two.
Independent India has witnessed caste-related violence. In 2005. government statistics recorded approximately 110,000 cases of reported violent acts, including rape and murder, committed against Dalits The economic significance of the caste system in India has been declining as a result of urbanization and affirmative action programs. Upon independence from the British rule, the Indian Constitution listed 1,108 castes across the country as Scheduled Castes in 1950, for affirmative action. The Scheduled Castes are sometimes called Dalit in contemporary literature. In 2001, the proportion of Dalit population was 16.2 percent of India's total population. The majority of the 15 million bonded child workers in India, are from the lowest castes.
Nepal.
The Nepalese caste system resembles that of the Indian Jāti system with numerous Jāti divisions with a Varna system superimposed for a rough equivalence. But since the culture and the society is different some of the things are different. Inscriptions attest the beginnings of a caste system during the Lichchhavi period. Jayasthiti Malla (1382–95) categorized Newars into 64 castes (Gellner 2001). A similar exercise was made during the reign of Mahindra Malla (1506–75). The Hindu social code was later set up in Gorkha by Ram Shah (1603–36).
Pakistan.
Religious, historical and sociocultural factors have helped define the bounds of endogamy for Muslims in some parts of Pakistan. There is a preference for endogamous marriages based on the clan-oriented nature of the society, which values and actively seeks similarities in social group identity based on several factors, including religious, sectarian, ethnic, and tribal/clan affiliation. Religious affiliation is itself multilayered and includes religious considerations other than being Muslim, such as sectarian identity (e.g. Shia or Sunni, etc.) and religious orientation within the sect (Isnashari, Ismaili, Ahmedi, etc.).
Both ethnic affiliation (e.g. Pathan,Sindhi, Baloch, Punjabi, etc.) and membership of specific biraderis or zaat/quoms are additional integral components of social identity. Within the bounds of endogamy defined by the above parameters, close consanguineous unions are preferred due to a congruence of key features of group- and individual-level background factors as well as affinities. McKim Marriott claims a social stratification that is hierarchical, closed, endogamous and hereditary is widely prevalent, particularly in western parts of Pakistan. Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes.
Sri Lanka.
The Caste system in Sri Lanka is a division of society into strata, influenced by the classic Aryan Varnas of North India and the Dravida Jāti system found in South India. Ancient Sri Lankan texts such as the Pujavaliya, Sadharmaratnavaliya and Yogaratnakaraya and inscriptional evidence show that the above hierarchy prevailed throughout the feudal period. The repetition of the same caste hierarchy even as recently as the 18th century, in the British/Kandyan period Kadayimpoth - Boundary books as well, indicates the continuation of the tradition right up to the end of Sri Lanka's monarchy.
Caste-like stratification outside South Asia.
Southeast Asia.
Indonesia.
Balinese caste structure has been described in early 20th-century European literature to be based on three categories – triwangsa (thrice born) or the nobility, dwijati (twice born) in contrast to ekajati (once born) the low folks. Four statuses were identified in these sociological studies, spelled a bit differently from the caste categories for India:
The Brahmana caste was further subdivided by these Dutch ethnographers into two: Siwa and Buda. The Siwa caste was subdivided into five – Kemenuh, Keniten, Mas, Manuba and Petapan. This classification was to accommodate the observed marriage between higher caste Brahmana men with lower caste women. The other castes were similarly further sub-classified by these 19th-century and early-20th-century ethnographers based on numerous criteria ranging from profession, endogamy or exogamy or polygamy, and a host of other factors in a manner similar to "castas" in Spanish colonies such as Mexico, and caste system studies in British colonies such as India.
East Asia.
China and Mongolia.
During the period of Yuan Dynasty, ruler Kublai Khan enforced a "Four Class System", which was a legal caste system. The order of four classes of people was maintained by the information of the descending order were:-
Some scholars notes that it was a kind of psychological indication that the earlier they submitted to Mongolian people, the higher social status they would have. The 'Four Class System' and its people received different treatment in political, legal, and military affairs.
Today, the Hukou system is considered by various sources as the current caste system of the China.
Japan.
In Japan's history, social strata based on inherited position rather than personal merits, was rigid and highly formalized. At the top were the Emperor and Court nobles (kuge), together with the Shogun and daimyo. Below them the population was divided into four classes in a system known as "mibunsei" (身分制). These were: samurai, peasants, craftsmen and merchants. Only the samurai class was allowed to bear arms. A samurai had a right to kill any peasants and other craftsmen and merchants whom he felt were disrespectful. Craftsmen produced products, being the third, and the last merchants were thought to be as the meanest class because they did not produce any products. The castes were further sub-divided; for example, the peasant caste were labelled as "furiuri", "tanagari", "mizunomi-byakusho" amongst others. The castes and sub-classes, as in Europe, were from the same race, religion and culture.
Howell, in his review of Japanese society notes that if a Western power had colonized Japan in the 19th century, they would have discovered and imposed a rigid four-caste hierarchy in Japan.
De Vos and Wagatsuma observe that a systematic and extensive caste system was part of the Japanese society. They also discuss how alleged caste impurity and alleged racial inferiority, concepts often quickly assumed to be slightly different, are superficial terms, two faces of identical inner psychological processes, which expressed themselves in Japan and other countries of the world.
Endogamy was common because marriage across caste lines was socially unacceptable.
Japan had its own untouchable caste, shunned and ostracized, historically referred to by the insulting term "Eta", now called "Burakumin". While modern law has officially abolished the class hierarchy, there are reports of discrimination against the Buraku or Burakumin underclasses. The Burakumin are regarded as "ostracised." The burakumin are one of the main minority groups in Japan, along with the Ainu of Hokkaidō and those of residents of Korean and Chinese descent.
Korea.
With the unification of the three kingdoms in the 7th century and the foundation of the Goryeo dynasty in the Middle Ages, Koreans systemised its own native class system. At the top were the two official classes, the Yangban that literally means "two classes." It was composed of scholars (Munban) and warriors (Muban). Within the Yangban class, the Scholars (Munban) enjoyed a significant social advantage over the warrior (Muban) class, until the Muban Rebellion in 1170 resulting in the 100 year Goryeo military regime. Muban ruled Korea under successive Warrior Leaders until the final Mongol victory in 1270. In 1392, with the foundation of Confucian Joseon dynasty, the full ascendancy of munban over muban was final.
Beneath the Yangban class were the "Jung-in" (중인-中人: literally "middle people"). They were the technicians. This class was small and specialized in fields such as medicine, accounting, translators, regional bureaucrats, etc.
Beneath the Jung-in were the "Sangmin" (상민-常民: literally 'commoner'). These were independent farmers working their own fields.
Underneath them all were the Baekjeong. The meaning today is that of butcher. They originate from the Khitan invasion of Korea in the 11th century. The defeated Khitans who had surrendered were settled in isolated communities throughout Goryeo to forestall rebellion. They were valued for their skills in hunting, herding, butchering, and making of leather, common skill sets among nomads. Over time their ethnic origin was forgotten, and they formed the bottom layer of Korean society.
Korea had a very large slave population, "nobi", ranging from a third to half of the entire population for most of the millennium between the Silla period and the Joseon Dynasty. Slavery was legally abolished in Korea in 1894 but remained extant in reality until 1930.
The opening of Korea to foreign Christian missionary activity in the late 19th century saw some improvement in the status of the "baekjeong"; However, everyone was not equal under the Christian congregation, and protests erupted when missionaries attempted to integrate them into worship services, with non-"baekjeong" finding such an attempt insensitive to traditional notions of hierarchical advantage. Also around the same time, the baekjeong began to resist the open social discrimination that existed against them. They focused on social and economic injustices affecting the baekjeong, hoping to create an egalitarian Korean society. Their efforts included attacking social discrimination by the upper class, authorities, and "commoners" and the use of degrading language against children in public schools.
With the Gabo reform of 1896, the class system of Korea was officially abolished. However, the Yangban families carried on traditional education and formal mannerisms into the 21st century. Gabo reform of 1896 required all peasants to adopt a last name. the vast majority of peasants who did not possess a last name opted to adopt the most prestigious of family names, resulting in the lopsided presence of Kim, Lee, and Park names among Koreans. These newly minted families do not possess Clan affiliations or genealogies. The Yangban families of Korea maintain their lineage through centuries old genealogy passed down from generation to generation. Most Yangban Koreans can name their family clan, one level beyond their last name, and possess a character in his first name containing a hereditary generation name, called dollimja (돌림자) or hangryeolja (항렬자) in Korean.
North Korea.
Committee for Human Rights in North Korea reported that "Every North Korean citizen is assigned a heredity-based class and socio-political rank over which the individual exercises no control but which determines all aspects of his or her life." Regarded as Songbun, Barbara Demick describes this "class structure" as an updating of the hereditary "caste system", combining Confucianism and Stalinism. She claims that a bad family background is called "tainted blood", and that by law this "tainted blood" lasts for three generations.
West Asia.
Yezidi society is hierarchical. The secular leader is a hereditary emir or prince, whereas a chief sheikh heads the religious hierarchy. The Yazidi are strictly endogamous; members of the three Yazidi castes, the murids, sheikhs and pirs, marry only within their group.
Yemen.
In Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Estimates put their number at over 3.5 million residents who are discriminated, out of a total Yemeni population of around 22 million.
Africa.
Various sociologists have reported caste systems in Africa. The specifics of the caste systems have varied in ethnically and culturally diverse Africa, however the following features are common - it has been a closed system of social stratification, the social status is inherited, the castes are hierarchical, certain castes are shunned while others are merely endogamous and exclusionary. In some cases, concepts of purity and impurity by birth have been prevalent in Africa. In other cases, such as the "Nupe" of Nigeria, the "Beni Amer" of East Africa, and the "Tira" of Sudan, the exclusionary principle has been driven by evolving social factors.
West Africa.
Among the Igbo of Nigeria - especially Enugu, Anambra, Imo, Abia, Ebonyi, Edo and Delta states of the country - Obinna finds Osu caste system has been and continues to be a major social issue. The Osu caste is determined by one's birth into a particular family irrespective of the religion practised by the individual. Once born into Osu caste, this Nigerian person is an outcast, shunned and ostracized, with limited opportunities or acceptance, regardless of his or her ability or merit. Obinna discusses how this caste system-related identity and power is deployed within government, Church and indigenous communities.
The "osu" class systems of eastern Nigeria and southern Cameroon are derived from indigenous religious beliefs and discriminate against the "Osus" people as "owned by deities" and outcasts.
The Songhai economy was based on a caste system. The most common were metalworkers, fishermen, and carpenters. Lower caste participants consisted of mostly non-farm working immigrants, who at times were provided special privileges and held high positions in society. At the top were noblemen and direct descendants of the original Songhai people, followed by freemen and traders.
In a review of social stratification systems in Africa, Richter reports that the term caste has been used by French and American scholars to many groups of West African artisans. These groups have been described as inferior, deprived of all political power, have a specific occupation, are hereditary and sometimes despised by others. Richter illustrates caste system in Cote d'lvoire, with six sub-caste categories. Unlike other parts of the world, mobility is sometimes possible within sub-castes, but not across caste lines. Farmers and artisans have been, claims Richter, distinct castes. Certain sub-castes are shunned more than others. For example, exogamy is rare for women born into families of woodcarvers.
Similarly, the Mandé societies in Gambia, Ghana, Guinea, Ivory Coast, Liberia, Senegal and Sierra Leone have social stratification systems that divide society by ethnic ties. The Mande class system regards the "jonow" slaves as inferior. Similarly, the Wolof in Senegal is divided into three main groups, the "geer" (freeborn/nobles), "jaam" (slaves and slave descendants) and the underclass "neeno". In various parts of West Africa, Fulani societies also have class divisions. Other castes include "Griots", "Forgerons", and "Cordonniers".
Tamari has described endogamous castes of over fifteen West African peoples, including the Tukulor, Songhay, Dogon, Senufo, Minianka, Moors, Manding, Soninke, Wolof, Serer, Fulani, and Tuareg. Castes appeared among the "Malinke" people no later than 14th century, and was present among the "Wolof" and "Soninke", as well as some "Songhay" and "Fulani" populations, no later than 16th century. Tamari claims that wars, such as the "Sosso-Malinke" war described in the "Sunjata" epic, led to the formation of blacksmith and bard castes among the people that ultimately became the Mali empire.
As West Africa evolved over time, sub-castes emerged that acquired secondary specializations or changed occupations. Endogamy was prevalent within a caste or among a limited number of castes, yet castes did not form demographic isolates according to Tamari. Social status according to caste was inherited by off-springs automatically; but this inheritance was paternal. That is, children of higher caste men and lower caste or slave concubines would have the caste status of the father.
Central Africa.
Ethel M. Albert in 1960 claimed that the societies in Central Africa were caste-like social stratification systems. Similarly, in 1961, Maquet notes that the society in Rwanda and Burundi can be best described as castes. The Tutsi, noted Maquet, considered themselves as superior, with the more numerous Hutu and the least numerous Twa regarded, by birth, as respectively, second and third in the hierarchy of Rwandese society. These groups were largely endogamous, exclusionary and with limited mobility. Maquet's theories have been controversial.
Horn of Africa.
In a review published in 1977, Todd reports that numerous scholars report a system of social stratification in different parts of Africa that resembles some or all aspects of caste system. Examples of such caste systems, he claims, are to be found in Ethiopia in communities such as the Gurage and Konso. He then presents the Dime of Southwestern Ethiopia, amongst whom there operates a system which Todd claims can be unequivocally labelled as caste system. The Dime have seven castes whose size varies considerably. Each broad caste level is a hierarchical order that is based on notions of purity, non-purity and impurity. It uses the concepts of defilement to limit contacts between caste categories and to preserve the purity of the upper castes. These caste categories have been exclusionary, endogamous and the social identity inherited. Alula Pankhurst has published a study of caste groups in SW Ethiopia.
Among the Kafa, there were also traditionally groups labeled as castes. "Based on research done before the Derg regime, these studies generally presume the existence of a social hierarchy similar to the caste system. At the top of this hierarchy were the Kafa, followed by occupational groups including blacksmiths (Qemmo), weavers (Shammano), bards (Shatto), potters, and tanners (Manno). In this hierarchy, the Manjo were commonly referred to as hunters, given the lowest status equal only to slaves."
The Borana Oromo of southern Ethiopia in the Horn of Africa also have a class system, wherein the Wata, an acculturated hunter-gatherer group, represent the lowest class. Though the Wata today speak the Oromo language, they have traditions of having previously spoken another language before adopting Oromo.
The traditionally nomadic Somali people are divided into clans, wherein the Rahanweyn agro-pastoral clans and the occupational clans such as the Madhiban were traditionally sometimes treated as outcasts. As Gabboye, the Madhiban along with the Yibir and Tumaal (collectively referred to as "sab") have since obtained political representation within Somalia, and their general social status has improved with the expansion of urban centers.
Europe.
France and Spain.
For centuries, through the modern times, the majority regarded Cagots of western France and northern Spain as an inferior caste, the untouchables. While they had the same skin color and religion as the majority, in the Churches, they had to use segregated doors, drink from segregated fonts, receive communion on the end of long wooden spoons. It was a closed social system. The socially isolated Cagots were endogamous, and chances of social mobility non-existent.

</doc>
<doc id="7258" url="http://en.wikipedia.org/wiki?curid=7258" title="Creation">
Creation

Creation may refer to:

</doc>
<doc id="7262" url="http://en.wikipedia.org/wiki?curid=7262" title="Coral 66">
Coral 66

CORAL (Computer On-line Real-time Applications Language) is a programming language originally developed in 1964 at the Royal Radar Establishment (RRE), Malvern, UK, as a subset of JOVIAL. Coral 66 was subsequently developed by I. F. Currie and M. Griffiths under the auspices of IECCA (Inter-Establishment Commitee for
Computer Applications). Its official definition, edited by Woodward, Wetherall and Gorman, was first published in 1970.
Overview.
Coral 66 is a general-purpose programming language based on ALGOL 60, with some features from Coral 64, JOVIAL, and FORTRAN. It includes structured record types (as in Pascal) and supports the packing of data into limited storage (also as in Pascal). Like Edinburgh IMP it allows embedded assembler, and also offers good run-time checking and diagnostics. It is specifically intended for real-time and embedded applications and for use on computers with limited processing power, including those limited to fixed point arithmetic and those without support for dynamic storage allocation.
The language was an inter-service standard for British military programming, and was also widely adopted for civil purposes in the British control and automation industry. It was used to write software for both the Ferranti and GEC computers from 1971 onwards. Implementations also exist for the Interdata 8/32, PDP-11, VAX, Alpha platforms and HP Integrity servers; for the Honeywell, and for the Computer Technology Limited (CTL, later ITL) Modular-1; as well as for SPARC running Solaris and Intel running Linux.
A variant of Coral 66 was developed during the late 1970s/early 1980s by the British GPO, in conjunction with GEC, STC and Plessey, for use on the System X digital telephone exchange control computers, known as PO-CORAL. This was later renamed BT-CORAL when British Telecom was spun off from the Post Office. Unique features of this language were the focus on real-time execution, message processing, limits on statement execution between waiting for input, and a prohibition on recursion to remove the need for a stack.
As Coral was aimed at a variety of real-time work, rather than general office DP, there was no standardised equivalent to a stdio library. IECCA recommended a primitive I/O package to accompany any compiler (in a document titled "Input/Output of Character data in Coral 66 Utility Programs"). Most implementers avoided this by producing Coral interfaces to existing Fortran and, later, C libraries.
Perhaps CORAL's most significant contribution to computing was the enforcement of quality control in commercial compilers. To have a CORAL compiler approved by IECCA, and thus allowing a compiler to be marketed as a CORAL 66 compiler, the candidate compiler had to compile and execute an official suite of 25 test programs and 6 benchmark programs. The process was part of the BS5905 approval process. This methodology was observed and adapted later by DoD for the official certification of Ada compilers.
Source code for a Coral 66 compiler (written in BCPL) has been recovered and the "Official Definition of Coral 66" document by HMSO has been scanned; the Ministry of Defence patent office has issued a licence to the Edinburgh Computer History project to allow them to put both the code and the language reference online for non-commercial use.

</doc>
<doc id="7264" url="http://en.wikipedia.org/wiki?curid=7264" title="Rhyming slang">
Rhyming slang

Rhyming slang is a form of phrase construction in the English language and is especially prevalent in dialectal English from the East End of London; hence the alternative name, Cockney rhyming slang. The construction involves replacing a common word with a rhyming phrase of two or three words and then, in almost all cases, omitting the secondary rhyming word (which is thereafter implied), in a process called hemiteleia, making the origin and meaning of the phrase elusive to listeners not in the know.
A frequently cited example involves the replacement of "stairs" with the rhyming phrase "apples and pears". Following the usual pattern of omission, "and pears" is dropped and "stairs" is referred to as "apples". Thus the spoken phrase "I'm going up the apples" means "I'm going up the stairs".
In similar fashion, "telephone" is replaced by "dog" (= 'dog-and-bone'); "wife" by "trouble" (= 'trouble-and-strife'); "eyes" by "mincers" (= 'mince pies'); "wig" by "syrup" (= 'syrup of figs') and "feet" by "plates" (= 'plates of meat'). Thus a construction of the following type could conceivably arise: "It nearly knocked me off me plates—he was wearing a syrup! So I ran up the apples, got straight on the dog to me trouble and said I couldn't believe me mincers."
In some examples the meaning is further obscured by adding a second iteration of rhyme and truncation to the original rhymed phrase. For example, the word "Aris" is often used to indicate the buttocks. This is the result of a double rhyme, starting with the original rough synonym "arse", which is rhymed with "bottle and glass", leading to "bottle". "Bottle" was then rhymed with "Aristotle" and truncated to "Aris".
The use of rhyming slang has spread beyond the purely dialectal and some examples are to be found in the mainstream British English lexicon and internationally, although many users may be unaware of the origin of those words. One example is "berk", a mild pejorative widely used across the UK and not usually considered particularly offensive, although the origin lies in a contraction of "Berkeley Hunt", as the rhyme for the significantly more offensive "cunt". Another example is to "have a butcher's" for to have a look, from "butcher's hook".
Most of the words changed by this process are nouns. A few are adjectival e.g. 'bales' (of cotton = rotten), or the adjectival phrase 'on one's tod' = 'on one's own (Tod Sloan, a famous jockey).
History.
Rhyming slang is believed to have originated in the mid-19th century in the East End of London, with several sources suggesting some time in the 1840s.
According to Partridge (1972:12), it dates from around 1840 and arose in the East End of London, however John Camden Hotten in his 1859 "Dictionary of Modern Slang, Cant and Vulgar Words" states that (English) rhyming slang originated "about twelve or fifteen years ago" (i.e. in the 1840s) with 'chaunters' and 'patterers' in the Seven Dials area of London. (The reference is to travelling salesmen of certain kinds. Chaunters sold sheet music and patterers offered cheap, tawdry goods at fairs and markets up and down the country). Hotten's "Dictionary" included a "Glossary of the Rhyming Slang", the first known such work. It included later mainstays such as "Frog and toad—the main road" and "Apples and pears—stairs" as well as many that later grew more obscure, e.g. "Battle of the Nile—a tile" (vulgar term for a hat), "Duke of York—take a walk", and "Top of Rome—home".
It remains a matter of speculation whether rhyming slang was a linguistic accident, a game, or a cryptolect developed intentionally to confuse non-locals. If deliberate, it may also have been used to maintain a sense of community. It is possible that it was used in the marketplace to allow traders to talk amongst themselves in order to facilitate collusion, without customers knowing what they were saying. Another suggestion is that it may have been used by criminals "(see thieves' cant)" to confuse the police.
Development.
At any point in history, in any location, rhyming slang can be seen to incorporate words and phrases that are relevant at that particular time and place. Many examples are based on locations in London and, in all likelihood, will be meaningless to people unfamiliar with the capital e.g. "Peckham Rye", meaning "tie" (as in necktie), which dates from the late 19th century; "Hampstead Heath", meaning "teeth" (usually as "Hampsteads"), which was first recorded in 1887 and "Barnet Fair", meaning "hair", which dates from the 1850s. (In these examples and many subsequent ones the final step of hemiteleia has been omitted to make it easier to trace the origin of the substituted words).
By the mid-20th century many rhyming slang expressions used the names of contemporary personalities, especially actors and performers: for example "Gregory Peck" meaning "neck" and also "cheque"; "Ruby Murray" meaning "curry"; "Alans", meaning "knickers" from Alan Whicker; "Max Miller" meaning "pillow" when pronounced /ˈpilə/ and "Henry Halls" for "balls (testicles)".
The use of personal names as rhymes continued into the late 20th century, for example "Tony Blairs" meaning "flares", as in trousers with a wide bottom (previously this was "Lionel Blairs" and this change illustrates the ongoing mutation of the forms of expression) and "Britney Spears", meaning "beers".
Many examples have passed into common usage. Some substitutions have become relatively widespread in England in their contracted form. "To have a butcher's", meaning to have a look, originates from "butcher's hook", an S-shaped hook used by butchers to hang up meat, and dates from the late 19th century but has existed independently in general use from around the 1930s simply as "butchers". Similarly, "use your loaf", meaning "use your head", derives from "loaf of bread" and also dates from the late 19th century but came into independent use in the 1930s. To "have a giraffe" is commonly employed for a "laugh", although technically this does not involve hemiteleia.
Rhyming slang, in keeping with the rest of the language, is at the mercy of what one might loosely refer to as "false etymology". An example occurs that involves the term "barney", which has been used to mean an altercation or fight since the late 19th century, although without a clear derivation. Thus, in the 1964 film "A Hard Day's Night", John Lennon mischievously taunts the road manager with the line “If you're gonna have a barney, can I hold your coat?". In the 2001 feature film "Ocean's Eleven" Don Cheadle uses the term "barney" and the claim is made that this rhyme is derived from Barney Rubble, ("trouble") with references to a character from the "Flintstones" cartoon show. This usage can be seen either as an abuse of history, or as a good example of the ever-changing nature of rhyming slang.
Regional and international variations.
Rhyming slang is used mainly in London in England but can to some degree be understood across the country. Some constructions, however, rely on particular regional accents for the rhymes to work. For instance, the term "Charing Cross" (a place in London), used to mean "horse" since the mid-19th century, does not work for a speaker without the lot–cloth split, common in London at that time but not nowadays. A similar example is "Joanna" meaning "piano", which is based on the pronunciation of "piano" as "pianna" . Unique formations also exist in other parts of the United Kingdom, such as in the East Midlands, where the local accent has formed "Derby Road", which rhymes with "cold".
Outside England, rhyming slang is used in many English-speaking countries in the Commonwealth of Nations, but is not in general use in the United States. (Some notable exceptions: "bread" [bread & honey = money], "blow a raspberry" [raspberry tart = fart] and "put up your dukes" [Duke of York = fork, a Cockney slang term for "fist"]). In Australian slang the term for an English person is "pommy", which has been proposed as a rhyme on "pomegranate" rhyming with "immigrant"; similarly the term "seppo/septic" [septic tank = yank] is a slang term for Americans. A more recent Australian invention is the term "reginalds" to describe underpants (referred to as "undies" in Australian slang), from "Reg Grundies" after Reg Grundy, the Australian media tycoon. In Australia and South Africa, the colloquial term "China" is derived from "mate" rhyming with "China plate" (the identical form, heard in expressions like "me old China" is also a long-established Cockney idiom).
Rhyming slang is continually evolving, and new phrases are introduced all the time. As mentioned new personalities replace old ones (as in Lionel/Tony Blairs—flares), or pop culture introduces new words—as in "I haven't a Scooby" (from Scooby Doo, the eponymous cartoon dog of the cartoon series) meaning "I haven't a clue".
Rhyming slang and taboo terms.
Rhyming slang is often used as a substitute for words regarded as taboo, often to the extent that the association with the taboo word becomes unknown over time. "" (often used to mean "foolish person") originates from the most famous of all fox hunts, the "Berkeley Hunt" meaning "cunt"; "" (often used in the context "what you said is rubbish") originates from "cobbler's awls", meaning "balls" (as in testicles); and "hampton" meaning "prick" (as in penis) originates from "Hampton Wick" (a place in London).
Lesser taboo terms include "pony and trap" for "crap" (as in defecate, but often used to denote nonsense or low quality); to blow a raspberry (rude sound of derision) from raspberry tart for "fart"; "D'Oyly Carte" for "fart"; "Jimmy Riddle" for "piddle" (as in urinate), "J. Arthur Rank" (a film mogul), "Jodrell Bank" or "ham shank" for "wank", "Bristol Cities" (contracted to 'Bristols') for "titties", etc. "Taking the Mick" or "taking the Mickey" is thought to be a rhyming slang form of "taking the piss", where "Mick" came from "Mickey Bliss".
Rhyming slang terms for Jew have included "Chelsea Blue", "Stick of Glue", "Four by Two", "Buckle my shoe", and "Front Wheel Skid", which is a more palatable form of the insulting term "Yid" (which itself comes from the Yiddish word for a Jew).
In December 2004 Joe Pasquale, winner of the fourth series of ITV's "I'm a Celebrity... Get Me Out of Here!", became well known for his frequent use of the term "Jacobs", for Jacob's Crackers, a rhyming slang term for knackers i.e. testicles.
The term Jacobs was used in a line uttered by Brick Top in the movie "Snatch":
"Listen, you fucking fringe, if I throw a dog a bone, I don't want to know if it tastes good or not. You stop me again whilst I'm walking, and I'll cut your fucking "Jacobs" off."
In popular culture.
Rhyming slang is used, then described and a number of examples suggested as dialogue in one scene of the 1967 film "To Sir With Love" starring Sidney Poitier. The English students are telling their foreign teacher that the slang is a drag and something for old people.
In Britain rhyming slang had a resurgence of popular interest beginning in the 1970s resulting from its use in a number of London-based television programmes such as "Steptoe and Son", "Mind Your Language", "The Sweeney" (the title of which is itself rhyming slang—"Sweeney Todd" for "Flying Squad", a rapid response unit of London’s Metropolitan Police), "Minder, " "Citizen Smith", "Only Fools and Horses", and "EastEnders". "Minder" could be quite uncompromising in its use of obscure forms without any clarification. Thus the non-Cockney viewer was obliged to deduce that, say, "iron" was "male homosexual" ('iron' = 'iron hoof' = 'poof'). One episode in Series 5 of "Steptoe and Son" was entitled "Any Old Iron", for the same reason, when Albert thinks that Harold is 'on the turn'. It was also featured in an episode of "The Good Life" in the first season where Tom and Barbara purchase a wood burning range from a junk trader called Sam, who litters his language with phony slang in hopes of getting higher payment. He comes up with a fake story as to the origin of Cockney Rhyming slang and is caught out rather quickly.
In "The Fall and Rise of Reginald Perrin", a comic twist was added to rhyming slang by way of spurious and fabricated examples which a young man had laboriously to explain to his father (e.g. 'dustbins' meaning 'children', as in 'dustbin lids' = 'kids'; 'Teds' being 'Ted Heath' and thus 'teeth'; and even 'Chitty Chitty' being 'Chitty Chitty Bang Bang', and thus 'rhyming slang'...).
In modern literature, Cockney rhyming slang is used frequently in the novels and short stories of Kim Newman, for instance in the short story collections "The Man from the Diogenes Club" (2006) and "Secret Files of the Diogenes Club" (2007), where it is explained at the end of each book. Also, in the novel "Moving Pictures" by Terry Pratchett, this slang is frequently used.
In popular music, Spike Jones and his City Slickers recorded "So 'Elp Me", based on rhyming slang (without the hemiteleia), in 1950. The 1967 Kinks song "Harry Rag" was based on the usage of the name Harry Wragg as rhyming slang for "fag" (i.e. a cigarette). The idiom made a brief appearance in the UK-based DJ reggae music of the 1980s in the hit "Cockney Translation" by Smiley Culture of South London; this was followed a couple of years later by Domenick and Peter Metro's "Cockney and Yardie". London-based artists such as Audio Bullys and Chas & Dave (and others from elsewhere in the UK, such as The Streets, who are from Birmingham) frequently use rhyming slang in their songs.
In movies, Cary Grant's character teaches rhyming slang to his female companion in the film "Mr. Lucky" (1943) and describes it as Australian rhyming slang. The closing song of the 1969 crime caper, "The Italian Job", ("Getta Bloomin' Move On" a.k.a. "The Self Preservation Society") contains many slang terms. In present day feature films rhyming slang is often used to lend authenticity to an East End setting. Examples include "Lock, Stock and Two Smoking Barrels" (1998) (wherein the slang is translated via subtitles in one scene); "The Limey" (1999); "Sexy Beast" (2000); "Snatch" (2000); "Ocean's Eleven" (2001); and "Austin Powers in Goldmember" (2002); "It's All Gone Pete Tong" (2004), after BBC radio disc jockey Pete Tong whose name is used in this context as rhyming slang for "wrong"; "Green Street Hooligans" (2005). In "Margin Call" (2011), Will Emerson, played by London-born actor Paul Bettany, asks a friend on the telephone, "How's the trouble and strife?" ("wife").
In Scottish Football, a number of Clubs have nicknames taken from rhyming slang. Partick Thistle are known as the "Harry Rags", which is taken from the rhyming slang of their 'official' nickname "the jags". Rangers are known as the "Teddy Bears", which comes from the rhyming slang for "the Gers" (shortened version of Ran-gers). Heart of Midlothian are known as the "Jambos", which comes from "Jam Tarts" which is the rhyming slang for "Hearts" which is the common abbreviation of the Club's name. Hibernian are also referred to as "The Cabbage" which comes from Cabbage and Ribs being the rhyming slang for Hibs.
On the TV show The League in episode "The Guest Bong", Andre repeatedly uses cockney rhyming slang and explains the pattern. One of the times he uses it, he uses "Christmas Carol" as shorthand for "barrel", which causes a misunderstanding that becomes a plot point.
British-born M.C. MF Doom released an ode to the dialect entitled "Rhymin' Slang", after settling in the UK in 2010. The track was released on his 2012 album "Keys to the Kuffs".

</doc>
<doc id="7265" url="http://en.wikipedia.org/wiki?curid=7265" title="Canchim">
Canchim

The Canchim breed is a breed of beef cattle developed in Central Brazil by crossing European Charolais cattle with Indubrazil cattle already kept in Brazil where Asian Zebu type cattle are best suited to the tropical conditions. When compared with Zebu bulls, Canchim bulls produce the same number of calves, but heavier and of superior quality. Compared to European breeds, the Canchim bull produces calves with the same weight but in larger numbers. The fast-growing progeny, from crossbred zebu cows with Canchim bulls, can be slaughtered at 18 months old from feedlots after weaning, up to 24 months old from feedlots after grazing and at 30 months from grazing on the range.
Origin.
Zebu cattle (Bos Indicus), introduced to Brazil in the last century, were extensively crossbred with herds of native cattle. The Indian breed, well known for its ability to survive in the tropics, adapted quickly to Brazil, and soon populated large areas, considerably improving Brazilian beef cattle breeding. Zebu cattle were however found to be inferior to the European breeds in growth rate and yield of meat. It became clear that the beef cattle population required genetic improvement. Simply placing European beef cattle (Bos Taurus), highly productive in temperate climates, in Central Brazil, would not produce good results, due to their inability to adapt to a tropical environment. Besides the climate, other factors such as the high occurrence of parasites, diseases and the very low nutritional value of the native forage were problems.
Formation of the breed.
The European breed used in the formation of Canchim cattle was Charolais. In 1922 the Ministry of Agriculture imported Charolais cattle to the State of Goias, where they remained till 1936, when they were transferred to São Carlos in the State of São Paulo, to the Canchim Farm of the Government Research Station, EMBRAPA. From this herd originated the dams and sires utilised in the program of crossbreeding.
The main Zebu breed which contributed to the formation to the Canchim was the Indubrazil, although Guzerá and Nelore cattle where also used. Preference was given to the Indubrasil breed, due to the ease of obtaining large herds at reasonable prices, which would have been difficult with Gir, Nelore or Guzerá.
The alternative crossbreeding programs initiated in 1940 by Dr. Antonio Teixeira Viana had the objective of obtaining first, crossbreeds 5/8 Charolais and 3/8 Zebu and second, 3/8 Charolais x 5/8 Zebu, to evaluate which of the two was the most successful. The total number of Zebu cows utilized to produce the half-breeds was 368, of which 292 were Indubrasil, 44 Guzerá and 32 Nelore. All the animals produced were reared exclusively on the range. Control of parasites was done every 15 days and the animals were weighed at birth and monthly. The females were weighed up to 30 months and the males up to 40 months.
The data collected during various years of work, permitted an evaluation of the various degrees of crossbreeding. The conclusion was that the 5/8 Charolais and 3/8 Zebu was the most suitable, presenting an excellent frame for meat, precocious, resistance to heat and parasites, and a uniform coat. The first crossbred animals, 5/8 Charolais and 3/8 Zebu, were born in 1953. Thus was born a new type of beef cattle for Central Brazil, with the name CANCHIM, derived from the name of a tree very common in the region where the breed was developed. It was not until 1971 that the Brazilian Association of Canchim Cattle Breeders (ABCCAN) was formed, and on 11 November 1972 the Herd Book was initiated. On 18 May 1983 the Ministry of Agriculture, recognized Canchim type cattle as a Breed.
New bloodlines.
The Canchim breed, being a synthetic breed, permits breeders, in the development of new crossbreeding systems, to use the breeds used to form the Canchim breed, besides the breed itself, in its development.
There are many Canchim breeders forming new blood lines. Today the Nelore breed totally dominates the Zebu breed in the formation of Canchim. American and French Charolais semen, from carefully selected bulls is also used and recommended by the ABCCAN to form new bloodlines.

</doc>
<doc id="7271" url="http://en.wikipedia.org/wiki?curid=7271" title="Communist Party of the Soviet Union">
Communist Party of the Soviet Union

The Communist Party of the Soviet Union (CPSU) was the founding and ruling political party of the Union of Soviet Socialist Republics (USSR). The CPSU was the sole governing party of the Soviet Union until 1990 when the Supreme Soviet of the Soviet Union annulled the law which granted the CPSU a monopoly over the political system. The party was founded in 1912 by the Bolsheviks, the majority faction of the Russian Social Democratic Labour Partya revolutionary group led by Vladimir Lenin which seized power in the aftermath of the October Revolution of 1917. The party was dissolved on 29 August 1991 as a result of the failed coup d'état earlier that month.
The CPSU was organized around the idea of democratic centralism, a principle conceived by Lenin that entails democratic and open discussion of policy issues and the requirement of unity in upholding agreed policies. The highest body within the CPSU was the party Congress, which convened every five years. When the Congress was not in session, the Central Committee was the highest body. Because the Central Committee met twice a year, most day-to-day duties and responsibilities were vested in the Politburo, the Secretariat, and the Orgburo (until 1952). The party leader was the head of government and held the office of either General Secretary, Premier or head of state, or some of the three offices concurrentlybut never all three at the same time. The party leader was the de facto chairman of the CPSU Politburo and the chief executive of the USSR.
The CPSU was committed to communist thought and according to its party statute the CPSU adhered to Marxism–Leninism, an ideology based on the writings of Lenin and Karl Marx, and formalized under Joseph Stalin. The party pursued state socialism, under which all industries were nationalized and a planned economy was implemented. Before central planning was enacted in 1929, Lenin had introduced a mixed economy, commonly referred to as the New Economic Policy, in the 1920s. When Mikhail Gorbachev took power, the general sentiment within the party and society was that the planned economy had failed and there was a need to introduce a market economy. Gorbachev and some of his allies envisioned the introduction of an economy similar to Lenin's New Economic Policy through a program of perestroika, or rebuilding, but the results of their reforms contributed to the fall of the entire system of government.
A number of causes contributed to CPSU's loss of control and the dissolution of the Soviet Union. Some historians have written that Gorbachev's policy of democratization was the root cause, noting that it weakened the party's control over society. Others have blamed the economic stagnation and loss of faith by the general populace in communist ideology. The Communist Party of China said that the cause of the fall originated with Joseph Stalin, criticizing him for the "bastardization of Leninism", turning Marxism into dogma, creating a one-man rule, and introducing an inefficient economic system.
History.
From Lenin to Stalin (1912–1953).
The Russian Socialist Federative Soviet Republic, the world's first constitutionally socialist state, was established in the aftermath of the October Revolution. Immediately after the Revolution, the new, Lenin-led government implemented socialist reforms, including the transfer of estates and imperial lands to workers' soviets. Lenin supported world revolution but first needed to consolidate his power at home. To focus on the civil unrest brewing in Russia, he sought immediate peace with the Central Powers and agreed to a punitive treaty that ceded much of the former Russian Empire to Germany. The treaty was voided after the Allied victory in World War I.
In 1921, Lenin proposed the New Economic Policy, a system of state capitalism that started the process of industrialization and recovery from the Civil War. On 30 December 1922, the Russian SFSR joined former territories of the Russian Empire in the Soviet Union, of which Lenin was elected leader. On 9 March 1923, Lenin suffered a stroke, which incapacitated him and effectively ended his role in government. He died on 21 January 1924 and was succeeded by Joseph Stalin.
In the 1930s, Stalin initiated the Great Purge, a period of widespread paranoia and repression that culminated in a series of show trials and the purging of nearly all original Party members. With the rise of fascism in Italy and Germany, the Party actively sought to form "collective security" alliances with western powers. Unable to do so, the USSR signed a non-aggression pact with Germany, which was broken in 1941 when Germany invaded the Soviet Union, beginning the Great Patriotic War. After the 1945 Allied victory of World War II, the Party held to a doctrine of establishing pro-Stalin governments in the post-war occupied territories and of actively seeking to expand their sphere of influence, using proxy wars and espionage and providing training and funding to promote Communist elements abroad.
Post-Stalin years (1953–1985).
After Stalin's death, Khrushchev rose to the top post by overcoming political adversaries, including Lavrentiy Beria and Georgy Malenkov, in a power struggle. In 1955, Khrushchev achieved the demotion of Malenkov and secured his own position as Soviet leader. Early in his rule and with the support of several members of the Presidium, Khrushchev initiated the Thaw, which effectively ended the Stalinist mass terror of the prior decades and reduced socio-economic oppression considerably. At the 20th Congress held in 1956, Khrushchev denounced Stalin's crimes, being careful to omit any reference to complicity by any sitting Presidium members. His economic policies, while bringing about improvements, were not enough to fix the fundamental problems of the Soviet economy. The standard of living for ordinary citizens did increase; 108 million people moved into new housing between 1956 and 1965.
Khrushchev's foreign policies led to the Sino-Soviet split, in part a consequence of his public denunciation of Stalin. Khrushchev improved relations with Josip Broz Tito's League of Communists of Yugoslavia but failed to establish the close, party-to-party relations that he wanted. While the Thaw reduced political oppression at home, it led to unintended consequences abroad, such as the Hungarian Revolution of 1956 and unrest in Poland, where the local citizenry now felt confident enough to rebel against Soviet control. Khrushchev also failed to improve Soviet relations with the West, partially because of a hawkish military stance. In the aftermath of the Cuban Missile Crisis, Khrushchev's position within the party was substantially weakened. Shortly before his eventual ousting he tried to introduce radical economic reforms championed by Evsei Liberman, a Soviet economist.
Khrushchev was ousted on 14 October 1964 in a Central Committee plenum that officially cited his inability to listen to others, his failure in consulting with the members of the Presidium, his establishment of a cult of personality, his economic mismanagement, and his anti-party reforms as the reasons he was no longer fit to remain as head of the party. He was succeeded in office by Leonid Brezhnev as First Secretary and Alexei Kosygin as Chairman of the Council of Ministers.
The Brezhnev era began with a rejection of Khrushchevism in virtually every arena except one; continued opposition to Stalinist methods of terror and political violence. Khrushchev's policies were criticized as voluntarism and the Brezhnev period saw the rise of neo-Stalinism. While Stalin was never rehabilitated during this period, the most conservative journals in the country were allowed to highlight positive features of his rule.
At the 23rd Congress held in 1966, the names of the office of First Secretary and the body of the Presidium reverted to their original names: General Secretary and Politburo, respectively. At the start of his premiership, Kosygin experimented with economic reforms similar to those championed by Malenkov, including prioritizing light industry over heavy industry to increase the production of consumer goods. Similar reforms were introduced in Hungary under the name New Economic Mechanism; however, with the rise to power of Alexander Dubček in Czechoslovakia, who called for the establishment of "socialism with a human face", all non-conformist reform attempts in the Soviet Union were stopped.
During his rule, Brezhnev supported "detente", a passive weakening of animosity with the West with the goal of improving political and economic relations. However, by the 25th Congress held in 1976, political, economic and social problems within the Soviet Union began to mount and the Brezhnev administration found itself in an increasingly difficult position. The previous year, Brezhnev's health began to deteriorate. He became addicted to painkillers and needed to take increasingly more potent medications to attend official meetings. Because of the "trust in cadres" policy implemented by his administration, the CPSU leadership evolved into a gerontocracy. At the end of Brezhnev's rule, problems continued to amount; in 1979 he consented to the Soviet intervention in Afghanistan to save the embattled communist regime there and supported the oppression of the Solidarity movement in Poland. As problems grew at home and abroad, Brezhnev was increasingly ineffective in responding to the growing criticism of the Soviet Union by Western leaders, most prominently by US Presidents Jimmy Carter and Ronald Reagan, and UK Prime Minister Margaret Thatcher. The CPSU, which had wishfully interpreted the financial crisis of the 1970s as the beginning of the end of capitalism, found its country falling far behind the West in its economic development. Brezhnev died on 10 November 1982, and was succeeded by Yuri Andropov on 12 November.
Andropov, a staunch anti-Stalinist, chaired the KGB during most of Brezhnev's reign. He had appointed several reformers to leading positions in the KGB, many of whom later became leading officials under Gorbachev. Andropov supported increased openness in the press, particularly regarding the challenges facing the Soviet Union. Andropov was in office briefly, but he appointed a number of reformers, including Yegor Ligachev, Nikolay Ryzhkov and Mikhail Gorbachev, to important positions. He also supported a crackdown on absenteeism and corruption. Andropov had intended to let Gorbachev succeed him in office, but Konstantin Chernenko and his supporters suppressed the paragraph in the letter which called for Gorbachev's elevation. Andropov died on 9 February 1984 and was succeeded by Chernenko. Throughout his short leadership, Chernenko was unable to consolidate power and effective control of the party organization remained in Gorbachev's control. Chernenko died on 10 March 1985 and was succeeded in office by Gorbachev on 11 March 1985.
Gorbachev and the CPSU's demise (1985–1991).
Gorbachev was elected CPSU General Secretary on 11 March 1985, one day after Chernenko's death. When he acceded, the Soviet Union was stagnating but was stable and may have continued largely unchanged into the 21st century if not for Gorbachev's reforms.
Gorbachev conducted a significant personnel reshuffling of the CPSU leadership, forcing old party conservatives out of office. In 1985 and early 1986, the new party leadership called for "uskoreniye" (). Gorbachev reinvigorated the party ideology by adding new concepts and updating older ones. A positive consequence of this was the allowance of "pluralism of thought" and a call for the establishment of "socialist pluralism" (literally, socialist democracy). He introduced a policy of "glasnost" (Russian: "openness", "transparency") in 1986, which led to a wave of unintended democratization. According to Russian scholar Archie Brown, the democratization of the Soviet Union brought mixed blessings to Gorbachev; it helped him to weaken his conservative opponents within the party but brought out accumulated grievances which had been oppressed during the previous decades. 
In reaction to these changes, a conservative movement gained momentum in 1987 in response to Boris Yeltsin's dismissal as First Secretary of the Moscow Communist Party. On 13 March 1988, Nina Andreyeva, a university lecturer, wrote an article titled "I Cannot Forsake My Principles". The publication was planned to occur when both Gorbachev and his protege Alexander Yakovlev were visiting foreign countries. In their place, Yegor Ligachev led the party organization and told journalists that the article was "a benchmark for what we need in our ideology today". Upon Gorbachev's return, the article was discussed at length during a Politburo meeting; it was revealed that nearly half of its members were sympathetic to the letter and opposed further reforms which could weaken the party. The meeting lasted for two days, but on 5 April, a Politburo resolution responded with a point-by-point rebuttal to Andreyeva's article.
Gorbachev convened the 19th Party Conference in June 1988. He criticized leading party conservatives Ligachev, Andrei Gromyko and Mikhail Solomentsev. In turn, conservative delegates attacked Gorbachev and the reformers. According to Brown, there had not been as much open discussion and dissent at a party meeting since the early 1920s.
Despite the deep-seated opposition for further reform, the CPSU was still hierarchical; the conservatives acceded to Gorbachev's demands because he was the CPSU General Secretary. The 19th Conference approved the establishment of the Congress of People's Deputies (CPD) and allowed for contested elections between the CPSU and independent candidates. Organized parties were not allowed. The CPD was elected in 1989; one-third of the seats were appointed by the CPSU and other public organizations to sustain the Soviet one-party state. The elections were democratic but most elected CPD members were against any more radical reform. The elections marked the highest electoral turnout in Russian history; no election before or since had a higher participation rate. An organized opposition was established within the legislature under the name Inter-Regional Group of Deputies. An unintended consequence of these reforms was the increased anti-CPSU pressure; in March 1990 at a session of the Supreme Soviet of the Soviet Union, the party was forced to relinquish its political monopoly of power, in effect turning the Soviet Union into a liberal democracy.
The CPSU's demise began in March 1990, when party elements were eclipsed in power by state bodies. From then until the Soviet Union's disestablishment, Gorbachev ruled the country through the newly created post of President of the Soviet Union. Following this, the central party apparatus played little practical role in Soviet affairs. Gorbachev had become independent from the Politburo and faced few constraints from party leaders. In the summer of 1990, the party convened the 28th Congress. A new Politburo was elected, previous incumbents except Gorbachev and Vladimir Ivashko, the CPSU Deputy General Secretary were removed. Later that year, the party began work on a new program with a working title, "Towards a Humane, Democratic Socialism". According to Brown, the program reflected Gorbachev's journey from an orthodox communist to a European social democrat. The freedoms of thought and organization, which were allowed by Gorbachev, led to a rise in nationalism in the Soviet republics, indirectly weakening the central authorities. In response to this, a referendum was held in 1991, in which most of the union republics voted to preserve the union in a different form. In reaction to this, conservative elements within the CPSU launched the August 1991 coup, which overthrew Gorbachev but failed to preserve the Soviet Union. When Gorbachev returned after the coup's collapse, he resigned from the CPSU and operations were handed over to Ivashko. The CPSU was outlawed on 29 August 1991 and Gorbachev resigned from the presidency on 25 December; the following day the Soviet Union was dissolved.
Governing style.
Democratic centralism and vanguardism.
Democratic centralism is an organizational principle conceived by Lenin. According to Soviet pronouncements, democratic centralism was distinguished from bureaucratic centralism, which referred to high-handed formulae without knowledge or discussion. In democratic centralism, decisions are taken after discussions but once the general party line has been formed, discussion on the subject must cease. No member or organizational institution may dissent on a policy after it has been agreed upon by the party's governing body; to do so would lead to expulsion from the party (formalized at the 10th Congress). Because of this stance, Lenin initiated a ban on factions, which was approved at the 10th Congress.
Lenin believed that democratic centralism safeguarded both party unity and ideological correctness. He conceived of the system after the events of 1917, when several socialist parties "deformed" themselves and actively began supporting nationalist sentiments. Lenin intended that the devotion to policy required by centralism would protect the parties from such revisionist ills and bourgeois defamation of socialism. Lenin supported the notion of a highly centralized vanguard party, in which ordinary party members elected the local party committee, the local party committee elected the regional committee, the regional committee elected the Central Committee and the Central Committee elected the Politburo, Orgburo and the Secretariat. Lenin believed that the party needed to be ruled from the centre and have at its disposal power to mobilize party members at will. This system was later introduced in communist parties abroad through the Communist International (Comintern).
A central tenet of Leninism was that of the vanguard party. The party was to represent the interests of the working class and all of those who were exploited by capitalism in general; however, it was not to become a part of that class. According to Lenin, the party's sole responsibility was to articulate and plan the long-term interests of the oppressed classes. It was not responsible for the daily grievances of those classes; that was the responsibility of the trade unions. According to Lenin, the Party and the oppressed classes could never become one because the Party was responsible for leading the oppressed classes to victory. The basic idea was that a small group of organized people could wield power disproportionate to their size with superior organizational skills. Despite this, until the end of his life, Lenin warned of the danger that the party could be taken over by bureaucrats, by a small clique, or by an individual. Toward the end of his life, he criticized the bureaucratic inertia of certain officials and admitted to problems with some of the party's control structures, which were to supervise organizational life.
Organization.
Congress.
The Congress, nominally the highest organ of the party, was convened every five years. Leading up to the October Revolution and until Stalin's consolidation of power, the Congress was the party's main decision-making body. However, after Stalin's ascension the Congresses became largely symbolic. CPSU leaders used Congresses as a propaganda and control tool. The most noteworthy Congress since the 1930s was the 20th Congress, in which Khrushchev denounced Stalin in a speech titled "The Personality Cult and its Consequences".
Despite delegates to Congresses losing their powers to criticize or remove party leadership, the Congresses functioned as a form of elite-mass communication. They were occasions for the party leadership to express the party line over the next five years to ordinary CPSU members and the general public. The information provided was general, ensuring that party leadership retained the ability to make specific policy changes as they saw fit.
The Congresses also provided the party leadership with formal legitimacy by providing a mechanism for the election of new members and the retirement of old members who had lost favour. The elections at Congresses were all predetermined and the candidates who stood for seats to the Central Committee and the Central Auditing Commission were approved beforehand by the Politburo and the Secretariat. A Congress could also provide a platform for the announcement of new ideological concepts. For instance, at the 22nd Congress, Khrushchev announced that the Soviet Union would see "communism in twenty years" a position later retracted.
Conference.
A party Conference, officially referred to as an All-Union Conference, was convened between Congresses by the Central Committee to discuss party policy and to make personnel changes within the Central Committee. 19 conferences were convened during the CPSU's existence. The 19th Congress held in 1952 removed the clause in the party's statute which stipulated that a party Conference could be convened. The clause was reinstated at the 23rd Congress, which was held in 1966.
Central Committee.
The Central Committee was a collective body elected at the annual party congress. It was mandated to meet at least twice a year to act as the party's supreme governing body. Membership of the Central Committee increased from 71 full members in 1934 to 287 in 1976. Central Committee members were elected to the seats because of the offices they held, not on their personal merit. Because of this, the Central Committee was commonly considered an indicator for Sovietologists to study the strength of the different institutions. The Politburo was elected by and reported to the Central Committee. Besides the Politburo, the Central Committee also elected the Secretariat and the General Secretarythe "de facto" leader of the Soviet Union. In 1919–1952 the Orgburo was also elected in the same manner as the Politburo and the Secretariat by the plenums of the Central Committee. In between Central Committee plenums, the Politburo and the Secretariat were legally empowered to make decisions on its behalf. The Central Committee or the Politburo and/or Secretariat on its behalf could issue nationwide decisions; decisions on behalf of the party were transmitted from the top to the bottom.
Under Lenin, the Central Committee functioned much like the Politburo did during the post-Stalin era, serving as the party's governing body. However, as the membership in the Central Committee increased, its role was eclipsed by the Politburo. Between Congresses, the Central Committee functioned as the Soviet leadership's source of legitimacy. The decline in the Central Committee's standing began in the 1920s; it was reduced to a compliant body of the Party leadership during the Great Purge. According to party rules, the Central Committee was to convene at least twice a year to discuss political mattersbut not matters relating to military policy. The body remained largely symbolic after Stalin's consolidation; leading party officials rarely attended meetings of the Central Committee.
Central Auditing Commission.
The Central Auditing Commission (CAC) was elected by the party Congresses and reported only to the party Congress. It had about as many members as the Central Committee. It was responsible for supervising the expeditious and proper handling of affairs by the central bodies of the Party; it audited the accounts of the Treasury and the enterprises of the Central Committee. It was also responsible for supervising the Central Committee apparatus, making sure that its directives were implemented and that Central Committee directives complied with the party Statute.
Statute.
The Statute, also referred to as the Rules, Charter and Constitution, were the party's by-laws and controlled life within the CPSU. The 1st Statute was adopted at the 2nd Congress of the Russian Social Democratic Labour Partythe forerunner of the CPSU. How the Statute was to be structured and organized led to a schism within the party, leading to the establishment of two competing factions; Bolsheviks (literally "majority") and Mensheviks (literally "minority"). The 1st Statute was based upon Lenin's idea of a centralized vanguard party. The 4th Congress, despite a majority of Menshevik delegates, added the concept of democratic centralism to Article 2 of the Statute. The 1st Statute lasted until 1919, when the 8th Congress adopted the 2nd Statute. It was nearly five times as long as the 1st Statute and contained 66 articles. It was amended at the 9th Congress. At the 11th Congress, the 3rd Statute was adopted with only minor amendments being made. New statutes were approved at the 17th and 18th Congresses respectively. The last party statute, which existed until the dissolution of the CPSU, was adopted at the 22nd Congress.
Central Committee apparatus.
Politburo.
The Political Bureau (Politburo), known as the Presidium from 1952 to 1966, was the highest party organ when the Congress and the Central Committee were not in session. Until the 19th Conference in 1988, the Politburo alongside the Secretariat controlled appointments and dismissals nationwide. In the post-Stalin period, the Politburo controlled the Central Committee apparatus through two channels; the General Department distributed the Politburo's orders to the Central Committee departments and through the personnel overlap which existed within the Politburo and the Secretariat. This personnel overlap gave the CPSU General Secretary a way of strengthening his position within the Politburo through the Secretariat. Kirill Mazurov, Politburo member from 1965 to 1978, accused Brezhnev of turning the Politburo into a "second echelon" of power. He accomplished this by discussing policies before Politburo meetings with Mikhail Suslov, Andrei Kirilenko, Fyodor Kulakov and Dmitriy Ustinov among others, who held seats both in the Politburo and the Secretariat. Mazurov's claim was later verified by Nikolai Ryzhkov, the Chairman of the Council of Ministers under Gorbachev. Ryzhkov said that Politburo meetings lasted only 15 minutes because the people close to Brezhnev had already decided what was to be approved.
The Politburo was abolished and replaced by a Presidium in 1952 at the 19th Congress. In the aftermath the 19th Congress and the 1st Plenum of the 19th Central Committee, Stalin ordered the creation of the Bureau of the Presidium, which acted as the standing committee of the Presidium. On 6 March 1953, one day after Stalin's death, a new and smaller Presidium was elected and the Bureau of the Presidium was abolished in a joint session with the Presidium of the Supreme Soviet and the Council of Ministers.
Until 1990, the CPSU General Secretary acted as the informal chairman of the Politburo. During the first decades of the CPSU's existence, the Politburo was officially chaired by the Chairman of the Council of People's Commissars; first by Lenin, then by Aleksey Rykov, Molotov, Stalin and Malenkov. After 1922, when Lenin was incapacitated, Lev Kamenev as Deputy Chairman of the Council of People's Commissars chaired the Politburo's meetings. This tradition lasted until Khrushchev's consolidation of power. In the first post-Stalin years, when Malenkov chaired Politburo meetings, Khrushchev as First Secretary signed all Central Committee documents into force. From 1954 until 1958, Khrushchev chaired the Politburo as First Secretary but in 1958 he dismissed and succeeded Nikolai Bulganin as Chairman of the Council of Ministers. During this period, the informal position of Second Secretarylater formalized as Deputy General Secretarywas established. The Second Secretary became responsible for chairing the Secretariat in place of the General Secretary. When the General Secretary could not chair the meetings of the Politburo, the Second Secretary would take his place. This system survived until the dissolution of the CPSU in 1991.
To be elected to the Politburo, a member had to serve in the Central Committee. The Central Committee elected the Politburo in the aftermath of a party Congress. Members of the Central Committee were given a predetermined list of candidates for the Politburo having only one candidate for each seat; for this reason the election of the Politburo was usually passed unanimously. The greater the power held by the sitting CPSU General Secretary, the higher the chance that the Politburo membership would be approved.
Secretariat.
The Secretariat headed the CPSU's central apparatus and was solely responsible for the development and implementation of party policies. It was legally empowered to take over the duties and functions of the Central Committee when it was not in plenum (did not hold a meeting). Many members of the Secretariat concurrently held a seat in the Politburo. According to a Soviet textbook on party procedures, the Secretariat's role was that of "leadership of current work, chiefly in the realm of personnel selection and in the organization of the verification of fulfillment [of party-state decisions". "Selections of personnel" () in this instance meant the maintenance of general standards and the criteria for selecting various personnel. "Verification of fulfillment" () of party and state decisions meant that the Secretariat instructed other bodies.
The powers of the Secretariat were weakened under Mikhail Gorbachev; the Central Committee Commissions took over the functions of the Secretariat in 1988. Yegor Ligachev, a Secretariat member, said that the changes completely destroyed the Secretariat's hold on power and made the body almost superfluous. Because of this, the Secretariat rarely met during the next two years. It was revitalized at the 28th Party Congress in 1990 and the Deputy General Secretary became the official Head of the Secretariat.
Orgburo.
The Organizational Bureau, or Orgburo, existed from 1919 to 1952 and was one of three leading bodies of the party when the Central Committee was not in session. It was responsible for "organizational questions, the recruitment and allocation of personnel, the coordination of activities of party, government and social organizations (e.g. trade unions and youth organizations), improvement to the party's structure, the distribution of information and reports within the party". The 19th Congress abolished the Orgburo and its duties and responsibilities were taken over by the Secretariat. At the beginning, the Orgburo held three meetings a week and reported to the Central Committee every second week. Lenin described the relation between the Politburo and the Orgburo as "the Orgburo allocates forces, while the Politburo decides policy". A decision of the Orgburo was implemented by the Secretariat. However, the Secretariat could make decisions in the Orgburo's name without consulting its members but if one Orgburo member objected to a Secretariat resolution, the resolution would not be implemented. In the 1920s, if the Central Committee could not convene the Politburo and the Orgburo would hold a joint session in its place.
Control Commission.
The Central Control Commission (CCC) functioned as the party's supreme court. The CCC was established at the 9th All-Russian Conference in September 1920, but rules organizing its procedure were not enacted before the 10th Congress. The 10th Congress formally established the CCC on all party levels and stated that it could only be elected at a party congress or a party conference. The CCC and the CCs were formally independent but had to make decisions through the party committees at their level, which led them in practice to lose their administrative independence. At first, the primary responsibility of the CCs was to respond to party complaints, focusing mostly on party complaints of factionalism and bureaucratism. At the 11th Congress, the brief of the CCs was expanded; it become responsible for overseeing party discipline. In a bid to further centralize the powers of the CCC, a Presidium of the CCC, which functioned in a similar manner to the Politburo in relation to the Central Committee, was established in 1923. At the 18th Congress, party rules regarding the CCC were changed; it was now elected by the Central Committee and was subordinate to the Central Committee.
CCC members could not concurrently be members of the Central Committee. To create an organizational link between the CCC and other central-level organs, the 9th All-Russian Conference created the joint CC–CCC plenums. The CCC was a powerful organ; the 10th Congress allowed it to expel full and candidate Central Committee members and members of their subordinate organs if two thirds of attendants at a CC–CCC plenum voted for such. At its first such session in 1921, Lenin tried to persuade the joint plenum to expel Alexander Shliapnikov from the party; instead of expelling him, Shliapnikov was given a severe reprimand.
Departments.
The leader of a department was usually given the title "head"(). In practice, the Secretariat had a major say in the running of the departments; for example, five of eleven secretaries headed their own departments in 1978. Normally, specific secretaries were given supervising duties over one or more departments. Each department established its own cellscalled sctionswhich specialized in one or more fields. During the Gorbachev era, a variety of departments made up the Central Committee apparatus. The Party Building and Cadre Work Department assigned party personnel in the nomenklatura system. The State and Legal Department supervised the armed forces, KGB, the Ministry of Internal Affairs, the trade unions, and the Procuracy. Before 1989, the Central Committee had several departments but some were abolished that year. Among these departments was the Economics Department that was responsible for the economy as a whole, one for machine building, one for the chemical industry, etc. The party abolished these departments to remove itself from the day-to-day management of the economy in favour of government bodies and a greater role for the market, as a part of the perestroika process. In their place, Gorbachev called for the creations of commissions with the same responsibilities as departments, but giving more independence from the state apparatus. This change was approved at the 19th Conference, which was held in 1988. Six commissions were established by late 1988.
"Pravda".
"Pravda" ("The Truth") was the leading newspaper in the Soviet Union. The Organizational Department of the Central Committee was the only organ empowered to dismiss "Pravda" editors. In 1905, "Pravda" began as a project by members of the Ukrainian Social Democratic Labour Party. Leon Trotsky was approached about the possibility of running the new paper because of his previous work on Ukranian newspaper "Kievan Thought". The first issue of "Pravda" was published on 3 October 1908 in Lvov, where it continued until the publication of the sixth issue in November 1909, when the operation was moved to Vienna, Austria-Hungary. During the Russian Civil War, sales of "Pravda" were curtailed by "Izvestia", the government run newspaper. At the time, the average reading figure for "Pravda" was 130,000. This Vienna-based newspaper published its last issue in 1912 and was succeeded the same year by a new newspaper dominated by the Bolsheviks, also called "Pravda", which was headquartered in St. Petersburg. The paper's main goal was to promote Marxist–Leninist philosophy and expose the lies of the bourgeoisie. In 1975, the paper reached a circulation of 10.6 million.
Higher Party School.
The Higher Party School (HPS) was the organ responsible for teaching cadres in the Soviet Union. It was the successor of the Communist Academy, which was established in 1918. The HPS was established in 1939 as the Moscow Higher Party School; it offered its students a two-year training course for becoming a Party official. It was reorganized in 1956 to that it could offer more specialized ideological training. In 1956, the school in Moscow was opened for students from socialist countries outside the USSR. The Moscow Higher Party School was the party school with the highest standing. The school itself had eleven faculties until a 1972 Central Committee resolution demanded a reorganization of the curriculum. The first regional HPS outside Moscow was established in 1946; by the early 1950s there were 70 Higher Party Schools. During the reorganization drive of 1956, Khrushchev closed 13 of them and reclassified 29 as inter-republican and inter-oblast schools.
Lower-level organization.
Republican and local organization.
The lowest organ above the primary party organization (PPO) was the district level. Every two years, the local PPO would elect delegates to the district-level party conference, which was overseen by a secretary from a higher party level. The conference elected a Party Committee and First Secretary, and re-declared the district’s commitment to the CPSU’s program. In between conferences, the "raion" party committeecommonly referred to as "raikom"was vested with ultimate authority. It convened at least six times a year to discuss party directives and to oversee the implementation of party policies in their respective districts, to oversee the implementation of party directives at the PPO-level, and to issue directives to PPOs. 75-80 percent of raikom members were full members, while the remaining 20–25 were non-voting, candidate members. Raikom members were commonly from the state sector, party sector, Komsomol or the trade unions.
Day-to-day responsibility of the raikom was handed over to a Politburo, which usually composed of 12 members. The district-level First Secretary chaired the meetings of the local Politburo and the raikom, and was the direct link between the district and the higher party echelons. The First Secretary was responsible for the smooth running of operations. The raikom was headed by the local apparatthe local agitation department or industry department. A raikom usually had no more than 4 or 5 departments, each of which was responsible for overseeing the work of the state sector but would not interfere in their work.
This system remained identical at all other levels of the CPSU hierarchy. The other levels were cities, oblasts (regions) and republics. The district level elected delegates to a conference held at least held every three years to elect the party committee. The only difference between the oblast and the district level was that the oblast had its own Secretariat and had more departments at its disposal. The oblast's party committee in turn elected delegates to the republican-level Congress, which was held every five years. The Congress then elected the Central Committee of the republic, which in turn elected a First Secretary and a Politburo. Until 1990, the Russian Soviet Federative Socialist Republic was the only republic which did not have its own republican branch, being instead represented by the CPSU Central Committee.
Primary party organizations.
The primary party organization (PPO) was the lowest level in the CPSU hierarchy. PPOs were organized cells consisting of three or more members. A PPO could exist anywhere; for example, in a factory or a student dormitory. They functioned as the party’s "eyes and ears" at the lowest level and were used to mobilize support for party policies. All CPSU members had to be a member of a local PPO. The size of a PPO varied from three people to several hundreds, depending upon its setting. In a large enterprise, a PPO usually had several hundred members. In such cases, the PPO was divided into bureaus based upon production-units. Each PPO was led by an executive committee and an executive committee secretary. Each executive committee is responsible for the PPO executive committee and its secretary. In small PPOs, members met periodically to mainly discuss party policies, ideology or practical matters. In such a case, the PPO secretary was responsible for collecting party dues, reporting to higher organs and maintaining the party records. A secretary could be elected democratically through a secret ballot, but that was not often the case; in 1979, only 88 out of the over 400,000 PPOs were elected in this fashion. The remainder were chosen by a higher party organ and ratified by the general meetings of the PPO. The PPO general meeting was responsible for electing delegates to the party conference at either the district- or town-level, depending on where the PPO was located.
Membership.
Membership of the party was not open. To become a party member, one had to be approved by various committees and one's past was closely scrutinized. As generations grew up having known nothing before the USSR, party membership became something one generally achieved after passing a series of stages. Children would join the Young Pioneers, and at the age of 14 might graduate to the Komsomol (Young Communist League). Ultimately, as an adult, if one had shown the proper adherence to party discipline - or had the right connections, one would become a member of the Communist Party itself. Membership of the party carried obligations; the Party expected Komsomol and CPSU members to pay dues and to carry out appropriate assignments and "social tasks" (общественная работа).
In 1918, Party membership was approximately 200,000. In the late 1920s under Stalin, the Party engaged in an intensive recruitment campaign (the "Lenin Levy") of new members from both the working class and rural areas. This represented an attempt to "proletarianize" the Party and an attempt by Stalin to strengthen his base by outnumbering the Old Bolsheviks and reducing their influence in the Party. In 1925, the Party had 1,025,000 members in a Soviet population of 147 million. In 1927, membership had risen to 1,200,000 and by 1933, the party had approximately 3.5 million members. However, as a result of the Great Purge of 1936-1939, Party membership fell to 1.9 million by 1939. (Nicholas DeWitt gives 2.307 million members in 1939, including candidate members, compared with 1.535 million in 1929 and 6.3 million in 1947.) In 1986, the CPSU had over 19 million membersapproximately 10% of the USSR's adult population. Over 44% of party members were classified as industrial workers and 12% as collective farmers. The CPSU had party organizations in 14 of the USSR's 15 republics. The Russian Soviet Federative Socialist Republic itself had no separate Communist Party until 1990 because the CPSU controlled affairs there directly.
Komsomol.
The All-Union Leninist Communist Youth League, commonly referred to as Komsomol, was the party's youth wing. The Komsomol acted under the direction of the CPSU Central Committee. It was responsible for indoctrinating youths in communist ideology and organizing social events. It was closely modeled on the CPSU; nominally the highest body was the Congress, followed by the Central Committee, Secretariat and the Politburo. The Komsomol participated in nationwide policy-making by appointing members to the collegiums of the Ministry of Culture, the Ministry of Higher and Specialized Secondary Education, the Ministry of Education and the State Committee for Physical Culture and Sports. The organization's newspaper was the "Komsomolskaya Pravda". The First Secretary and the Second Secretary were commonly members of the Central Committee but were never elected to the Politburo. However, at the republican level, several Komsomol first secretaries were appointed to the Politburo.
Ideology.
Marxism–Leninism.
Marxism–Leninism was the cornerstone of Soviet ideology. It explained and legitimized the CPSU's right to rule while explaining its role as a vanguard party. For instance, the ideology explained that the CPSU's policies, even if they were unpopular, were correct because the party was enlightened. It was represented as the only truth in Soviet society; the Party rejected the notion of multiple truths. Marxism–Leninism was used to justify CPSU rule and Soviet policy but it was not used as a means to an end. The relationship between ideology and decision-making was at best ambivalent; most policy decisions were made in the light of the continued, permanent development of Marxism–Leninism. Marxism–Leninism as the only truth could notby its very naturebecome outdated.
Despite having evolved over the years, Marxism–Leninism had several central tenets. The main tenet was the party's status as the sole ruling party. The 1977 Constitution referred to the party as "The leading and guiding force of Soviet society, and the nucleus of its political system, of all state and public organizations, is the Communist Party of the Soviet Union". State socialism was essential and from Stalin until Gorbachev, official discourse considered that private social and economic activity retarding the development of collective consciousness and the economy. Gorbachev supported privatization to a degree but based his policies on Lenin's and Bukharin's opinions of the New Economic Policy of the 1920s, and supported complete state ownership over the commanding heights of the economy. Unlike liberalism, Marxism–Leninism stressed the role of the individual as a member of a collective rather than the importance of the individual. Individuals only had the right to freedom of expression if it safeguarded the interests of a collective. For instance, the 1977 Constitution stated that every person had the right to express his or her opinion, but the opinion could only be expressed if it was in accordance with the "general interests of Soviet society". The quantity of rights granted to an individual was decided by the state and the state could remove these rights if it saw fit. Soviet Marxism–Leninism justified nationalism; the Soviet media portrayed every victory of the state as a victory for the communist movement as a whole. Largely, Soviet nationalism was based upon ethnic Russian nationalism. Marxism–Leninism stressed the importance of the worldwide conflict between capitalism and socialism; the Soviet press wrote about progressive and reactionary forces while claiming that socialism was on the verge of victory and that the "correlations of forces" were in the Soviet Union's favour. The ideology professed state atheism; Party members were not allowed to be religious.
Marxism–Leninism believed in the feasibility of a communist mode of production. All policies were justifiable if it contributed to the Soviet Union's achievement of that stage.
Leninism.
In Marxist philosophy, Leninism is the body of political theory for the democratic organization of a revolutionary vanguard party and the achievement of a dictatorship of the proletariat as a political prelude to the establishment of the socialist mode of production developed by Lenin. Since Karl Marx barely, if ever wrote about how the socialist mode of production would function, these tasks were left for Lenin to solve. Lenin's main contribution to Marxist thought is the concept of the vanguard party of the working class. He conceived the vanguard party as a highly-knit, centralized organization which was led by intellectuals rather than by the working class itself. The CPSU was open only to a small quantity of workers because the workers in Russia still had not developed class consciousness and needed to be educated to reach such a state. Lenin believed that the vanguard party could initiate policies in the name of the working class even if the working class did not support them. The vanguard party would know what was best for the workers because the party functionaries had attained consciousness.
Leninism was by definition authoritarian. Lenin, in light of the Marx's theory of the state (which views the state as an oppressive organ of the ruling class), had no qualms of forcing change upon the country. He viewed the dictatorship of the proletariat, rather than the dictatorship of the bourgeoisie, to be the dictatorship of the majority. The repressive powers of the state were to be used to transform the country, and to strip of the former ruling class of their wealth. Lenin believed that the transition from the capitalist mode of production to the socialist mode of production would last for a long period. In contrast to Marx, who believed that the socialist revolution would comprise and be led by the working class alone, Lenin argued that a socialist revolution did not necessarily need to be led or to comprise the working class alone. Instead, he said that a revolution needed to be led by the oppressed classes of society, which in the case of Russia was the peasant class.
Stalinism.
Stalinism, while not an ideology "per se", refers to Stalin's thoughts and policies. Stalin's introduction of the concept "Socialism in One Country" in 1924 was an important moment in Soviet ideological discourse. According to Stalin, the Soviet Union did not need a socialist world revolution to construct a socialist society. Four years later, Stalin initiated his "Second Revolution" with the introduction of state socialism and central planning. In the early 1930s, he initiated the collectivization of Soviet agriculture by de-privatizing agriculture and creating peasant cooperatives rather than making it the responsibility of the state. With the initiation of his "Second Revolution", Stalin launched the "Cult of Lenin"a cult of personality centered upon himself. The name of the city of Petrograd was changed to Leningrad, the town of Lenin's birth was renamed Ulyanov (Lenin's birth-name), the Order of Lenin became the highest state award and portraits of Lenin were hung in public squares, workplaces and elsewhere. The increasing bureaucracy which followed the introduction of a state socialist economy was at complete odds with the Marxist notion of "the withering away of the state". Stalin explained the reasoning behind it at the 16th Congress held in 1930;
We stand for the strengthening of the dictatorship of the proletariat, which represents the mightiest and most powerful authority of all forms of State that have ever existed. The highest development of the State power for the withering away of State power —this is the Marxian formula. Is this contradictory? Yes, it is contradictory. But this contradiction springs from life itself and reflects completely Marxist dialectic."
At the 1939 18th Congress, Stalin abandoned the idea that the state would wither away. In its place, he expressed confidence that the state would exist, even if the Soviet Union reached communism, as long as it was encircled by capitalism. Two key concepts were created in the latter half of his rule; the "two camp" theory and the "capitalist encirclement" theory. The threat of capitalism was used to strengthen Stalin's personal powers and Soviet propaganda began making a direct link with Stalin and stability in society, saying that the country would crumble without the leader. Stalin deviated greatly from classical Marxism on the subject of "subjective factors"; Stalin said that Party members of all ranks had to profess fanatic adherence to the Party's line and ideology, if not, those policies would fail.
Concepts.
Dictatorship of the proletariat.
Lenin, supporting Marx's theory of the state, believed democracy to be unattainable anywhere in the world before the proletariat seized power. According to Marxist theory, the state is a vehicle for oppression and is headed by a ruling class. He believed that by his time, the only viable solution was dictatorship since the war was heading into a final conflict between the "progressive forces of socialism and the degenerate forces of capitalism". The Russian Revolution was by 1917, already a failure according to its original aim, which was to act as an inspiration for a world revolution. The initial anti-statist posture and the active campaigning for direct democracy was replaced because of Russia's level of development withaccording to their own assessments dictatorship. The reasoning was Russia's lack of development, its status as the sole socialist state in the world, its encirclement by imperialist powers and its internal encirclement by the peasantry.
Marx and Lenin did not care if a bourgeois state was ruled in accordance with a republican, parliamentary or a constitutional monarchical system since this did not change the overall situation. These systems, even if they were ruled by a small clique or ruled through mass participation, were all dictatorships of the bourgeoisie who implemented policies in defence of capitalism. However, there was a difference; after the failures of the world revolutions, lenin argued that this did not necessarily have to change under the dictatorship of the proletariat. The reasoning came from practical considerations; the majority of the country's inhabitants were not communists, neither could the Party reintroduce parliamentary democracy because that was not in synchronization with its ideology and would lead to the Party losing power. He therefore concluded that the form of government has nothing do to with the nature of the dictatorship of the proletariat.
Bukharin and Trotsky agreed with Lenin; both said that the revolution had destroyed the old but had failed to create anything new. Lenin had now concluded that the dictatorship of the proletariat would not alter the relationship of power between men, but would rather "transform their productive relations so that, in the long run, the realm of necessity could be overcome and, with that, genuine social freedom realized". From 1920 to 1921, Soviet leaders and ideologists began differentiating between socialism and communism; hitherto the two terms had been used interchangeably and used to explain the same things. From then, the two terms had different meanings; Russia was in transition from capitalism to socialismreferred to interchangeably under Lenin as the dictatorship of the proletariat, socialism was the intermediate stage to communism and communism was considered the last stage of social development. By now, the party leaders believed that because of Russia's backward state, universal mass participation and true democracy could only take form in the last stage.
In early Bolshevik discourse, the term "dictatorship of the proletariat" was of little significance and the few times it was mentioned it was likened to the form of government which had existed in the Paris Commune. However, with the ensuing Russian Civil War and the social and material devastation that followed, its meaning altered from commune-type democracy to rule by iron-discipline. By now, Lenin had concluded that only a proletarian regime as oppressive as its opponents could survive in this world. The powers previously bestowed upon the Soviets were now given to the Council of People's Commissars, the central government, which was in turn to be governed by "an army of steeled revolutionary Communists [by Communists he referred to the Party]". In a letter to Gavril Myasnikov in late 1920, Lenin explained his new interpretation of the term "dictatorship of the proletariat":
"Dictatorship means nothing more nor less than authority untrammelled by any laws, absolutely unrestricted by any rules whatever, and based directly on force. The term 'dictatorship' "has no other meaning but this"."
Lenin justified these policies by claiming that all states were class states by nature and that these states were maintained through class struggle. This meant that the dictatorship of the proletariat in the Soviet Union could only be "won and maintained by the use of violence against the bourgeoisie". The main problem with this analysis is that the Party came to view anyone opposing or holding alternate views of the party as bourgeois. Its worst enemy remained the moderates, which were considered to be "the real agents of the bourgeoisie in the working class movement, the labour lieutenants of the capitalist class". The term "bourgeoisie" became synonymous with "opponent" and with people who disagreed with the Party in general. These oppressive measures led to another reinterpretation of the dictatorship of the proletariat and socialism in general; it was now defined as a purely economic system. Slogans and theoretical works about democratic mass participation and collective decision-making were now replaced with texts which supported authoritarian management. Considering the situation, the Party believed it had to use the same powers as the bourgeoisie to transform Russia; there was no alternative. Lenin began arguing that the proletariat, like the bourgeoisie, did not have a single preference for a form of government and because of that, dictatorship was acceptable to both the Party and the proletariat. In a meeting with Party officials, Lenin statedin line with his economist view of socialismthat "Industry is indispensable, democracy is not", further arguing that "we [the Party] do not promise any democracy or any freedom".
Imperialism.
The Marxist theory on imperialism was conceived by Lenin in his book, "" (published in 1917). It was written in response to the theoretical crisis within Marxist thought, which occurred due to capitalism's recovery in the 19th century. According to Lenin, imperialism was a specific stage of development of capitalism; a stage he referred to as state monopoly capitalism. The Marxist movement was split on how to solve capitalism's resurgence after the great depression of the late 19th century. Eduard Bernstein from the Social Democratic Party of Germany (SDP) considered capitalism's revitalization as proof that it was evolving into a more humane system, adding that the basic aims of socialists were not to overthrow the state but to take power through elections. Karl Kautsky, also from the SDP, held a highly dogmatic view; he said that there was no crisis within Marxist theory. Both of them denied or belittled the role of class contradictions in society after the crisis. In contrast, Lenin believed that the resurgence was the beginning of a new phase of capitalism; this stage was created because of a strengthening of class contradiction, not because of its reduction.
Lenin did not know when the imperialist stage of capitalism began; he said it would be foolish too look for a specific year, however said it began at the beginning of the 20th century (at least in Europe). Lenin believed that the economic crisis of 1900 accelerated and intensified the concentration of industry and banking, which led to the transformation of the finance capital connection to industry into the monopoly of large banks. In "Imperialism: the Highest Stage of Capitalism", Lenin wrote; "the twentieth century marks the turning point from the old capitalism to the new, from the domination of capital in general to the domination of finance capital". Lenin defines imperialism as the monopoly stage of capitalism.
Peaceful coexistence.
"Peaceful coexistence" was an ideological concept introduced under Khrushchev's rule. While the concept has been interpreted by fellow communists as proposing an end to the conflict between the systems of capitalism and socialism, Khrushchev saw it as a continuation of the conflict in every area except in the military field. The concept said that the two systems were developed "by way of diametrically opposed laws", which led to "opposite principles in foreign policy".
Peaceful coexistence was steeped in Leninist and Stalinist thought. Lenin believed that international politics were dominated by class struggle; in the 1940s Stalin stressed the growing polarization which was occurring in the capitalist and socialist systems. Khrushchev's peaceful coexistence was based on practical changes which had occurred; he accused the old "two camp" theory of neglecting the non-aligned movement and the national liberation movements. Khrushchev considered these "grey areas", in which the conflict between capitalism and socialism would be fought. He still stressed that the main contradiction in international relations were those of capitalism and socialism. The Soviet Government under Khrushchev stressed the importance of peaceful coexistence, saying that it had to form the basis of Soviet foreign policy. Failure to do, they believed, would lead to nuclear conflict. Despite this, Soviet theorists still considered peaceful coexistence to be a continuation of the class struggle between the capitalist and socialist worlds, but not based on armed conflict. Khrushchev believed that the conflict, in its current phase, was mainly economical.
The emphasis on peaceful coexistence did not mean that the Soviet Union accepted a static world with clear lines. It continued to uphold the creed that socialism was inevitable and they sincerely believed that the world had reached a stage in which the "correlations of forces" were moving towards socialism. With the establishment of socialist regimes in Eastern Europe and Asia, Soviet foreign policy planners believed that capitalism had lost its dominance as an economic system.
Socialism in One Country.
The concept of "Socialism in One Country" was conceived by Stalin in his struggle against Leon Trotsky and his concept of permanent revolution. In 1924, Trotsky published his pamphlet "Lessons of October", in which he stated that socialism in the Soviet Union would fail because of the backward state of economic development unless a world revolution began. Stalin responded to Trotsky's pamphlet with his article, "October and Comrade Trotsky's Theory of Permanent Revolution". In it, Stalin stated that he did not believe an inevitable conflict between the working class and the peasants would take place, and that "socialism in one country is completely possible and probable". Stalin held the view common among most Bolsheviks at the time; there was a possibility of real success for socialism in the Soviet Union despite the country's backwardness and international isolation. While Grigoriy Zinoviev, Lev Kamenev and Nikolai Bukharintogether with Stalinopposed Trotsky's theory of permanent revolution, their views on the way socialism could be built diverged.
According to Bukharin, Zinoviev and Kamenev supported the resolution of the 14th Conference held in 1925, which stated that "we cannot complete the building of socialism due to our technological backwardness". Despite this cynical attitude, Zinoviev and Kamenev believed that a defective form of socialism could be constructed. At the 14th Conference, Stalin reiterated his position that socialism in one country was feasible despite the capitalist blockade of the Soviet Union. After the conference, Stalin wrote "Concerning the Results of the XIV Conference of the RCP(b)", in which he stated that the peasantry would not turn against the socialist system because they had a self-interest in preserving it. Stalin said the contradictions which arose within the peasantry during the socialist transition could "be overcome by our own efforts". He concluded that the only viable threat to socialism in the Soviet Union was a military intervention.
In late 1925, Stalin received a letter from a Party official which stated that his position of "Socialism in One Country" was in contradiction with Friedrich Engels' writings on the subject. Stalin countered that Engels' writings reflected "the era of pre-monopoly capitalism, the pre-imperialist era when there were not yet the conditions of an uneven, abrupt development of the capitalist countries". From 1925, Bukharin began writing extensively on the subject and in 1926, Stalin wrote "On Questions of Leninism", which contains his best-known writings on the subject. With the publishing of "Leninism", Trotsky began countering Bukharin's and Stalin's arguments, writing that socialism in one country was only possible only in the short term, and said that without a world revolution it would be impossible to safeguard the Soviet Union from the "restoration of bourgeois relations". Zinoviev disagreed with Trotsky and Bukharin, and Stalin; he maintained Lenin's position from 1917 to 1922 and continued to say that only a defective form of socialism could be constructed in the Soviet Union without a world revolution. Bukharin began arguing for the creation of an autarkic economic model, while Trotsky said that the Soviet Union had to participate in the international division of labour to develop. In contrast to Trotsky and Bukharin, in 1938, Stalin said that a world revolution was impossible and that Engels was wrong on the matter. At the18th Congress, Stalin took the theory to its inevitable conclusion, saying that the communist mode of production could be conceived in one country. He rationalized this by saying that the state could exist in a communist society as long as the Soviet Union was encircled by capitalism. However, with the establishment of socialist regimes in Eastern Europe, Stalin said that socialism in one country was only possible in a large country like the Soviet Union and that to survive, the other states had to follow the Soviet line.
Reasons for demise.
Western view.
There were few, in any, who believed that the Soviet Union was on the verge of collapse by 1985. The economy was stable but stagnating, the political situation was calm because of twenty years of systematic repression against any threat to one-party rule and the country was in its peak of influence. The immediate causes for the Soviet Union's dissolution were the policies and thoughts of Mikhail Gorbachev, the CPSU General Secretary. His policies of "perestroika" and "glasnost" tried to revitalize the Soviet economy and the social and political culture of the country. Throughout his rule, he put more emphasis on democratizing the Soviet Union because he believed it had the lost its moral legitimacy to rule. These policies led to the collapse of the communist regimes in Eastern Europe and indirectly destabilized Gorbachev's and the CPSU's control over the Soviet Union. Archie Brown said:
The expectations of, again most notably, Lithuanians, Estonians and Latvians were enormously enhanced by what they saw happening in the 'outer empire' [Eastern Europe] and they began to believe that they could remove themselves from the 'inner empire'. In truth, a democratised Soviet Union was incompatible with denial of the Baltic states' independence for, to the extent that those Soviet republics became democratic, their opposition to remaining in a political entity whose centre was Moscow would become increasingly evident. Yet, it was not preordained that the entire Soviet Union would break up.
However, Brown said that the system did not need to collapse or to do so in the way it did. The democratization from above weakened the Party's control over the country, and put it on the defensive. Brown added that a different leader then Gorbachev would probably have oppressed the opposition and continued with economic reform. Nonetheless, Gorbachev accepted that the people sought a different road and consented to the Soviet Union's dissolution in 1991. He said that because of its peaceful collapse, the fall of Soviet communism is "one of the great success stories of 20th century politics". According to Lars T. Lih, the Soviet Union collapsed because people stopped believing in its ideology. He wrote:
"When in 1991 the Soviet Union collapsed not with a bang but a whimper, this unexpected outcome was partly the result of the previous disenchantments of the narrative of class leadership. The Soviet Union had always been based on fervent belief in this narrative in its various permutations. When the binding power of the narrative dissolved, the Soviet Union itself dissolved."
According to the Communist Party of China.
The first research into the collapse of the Soviet Union and the Eastern Bloc were very simple and did not take into account several factors. However, these examinations became more advanced by the 1990s and unlike most Western scholarship, which focuses on the role of Gorbachev and his reform efforts, the Communist Party of China (CPC) examined "core (political) life and death issues" so that it could learn from them and not make the same mistakes. Following the CPSU's demise and the Soviet Union's collapse, the CPC's analysis began examining systematic causes, unlike Western scholarship which often focuses on the immediate causes of the country's collapse. Several leading CPC officials began hailing Khrushchev's rule, saying that he was the first reformer, and that if he had continued after 1964, the Soviet Union would not have witnessed the Era of Stagnation began under Brezhnev and continued under Yuri Andropov and Konstantin Chernenko. The main economic failure was that the political leadership did not pursue any reforms to tackle the economic malaise that had taken hold, dismissing certain techniques as capitalist, and never disentangling the planned economy from socialism. Xu Zhixin from the CASS Institute of Eastern Europe, Russia, and Central Asia, argued that Soviet planners laid too much emphasis on heavy industry, which led to shortages of consumer goods. Unlike his counterparts, Xu argued that the shortages of consumer goods was not an error but "was a consciously planned feature of the system". Other CPSU failures were pursing the policy of state socialism, the high spending on the military-industrial complex, a low tax base and the subsidizing of the economy. The CPC argued that when Gorbachev came to power and introduced his economic reforms, they were "to little, too late, and too fast".
While most CPC researchers criticize the CPSU's economic policies, many have criticized what they see as "Soviet totalitarianism". They accuse Joseph Stalin of creating a system of mass terror, intimidation, annulling the democracy component of democratic centralism and emphasizing centralism, which led to the creation of an inner-party dictatorship. Other points were Russian nationalism, a lack of separation between the Party and state bureaucracies, suppression of non-Russian ethnicities, distortion of the economy through the introduction of over-centralization and the collectivization of agriculture. According to CPC researcher Xiao Guisen, Stalin's policies led to "stunted economic growth, tight surveillance of society, a lack of democracy in decision-making, an absence of the rule of law, the burden of bureaucracy, the CPSU's alienation from people's concerns, and an accumulation of ethnic tensions". Stalin's effect on ideology was also criticized; several researchers accused his policies of being "leftist", "dogmatist" and a deviation "from true Marxism–Leninism. He is criticized for initiating the "bastardization of Leninism", of deviating from true democratic centralism by establishing one-man rule and destroying all inner-party consultation, of misinterpreting Lenin's theory of imperialism and of supporting foreign revolutionary movements only when the Soviet Union could get something out of it. Yu Sui, a CPC theoretician, said that "the collapse of the Soviet Union and CPSU is a punishment for its past wrongs!" Similarly, Brezhnev, Mikhail Suslov, Alexei Kosygin and Konstantin Chernenko have been criticized for being "dogmatic, ossified, inflexible, [for having a] bureaucratic ideology and thinking", while Yuri Andropov is depicted by some of having the potential of becoming a new Khrushchev if he had not died early.
While the CPC concur with Gorbachev's assessment that the CPSU needed internal reform, they do not agree on how it was implemented, criticizing his idea of "humanistic and democratic socialism", of negating the leading role of the CPSU, of negating Marxism, of negating the analysis of class contradictions and class struggle, and of negating the "ultimate socialist goal of realizing communism". Unlike the other Soviet leaders, Gorbachev is criticized for pursuing the wrong reformist policies and for being too flexible and too rightist. The CPC Organization Department said, "What Gorbachev in fact did was not to transform the CPSU by correct principles—indeed the Soviet Communist Party "needed transformation"—but instead he, step-by-step, and ultimately, eroded the ruling party's dominance in ideological, political and organizational aspects".
The CPSU was also criticized for not taking enough care in building the primary party organization and not having inner-party democracy. Others, more radically, concur with Milovan Đilas assessment, saying that a new class was established within the central party leadership of the CPSU and that a "corrupt and privileged class" had developed because of the nomenklatura system. Other criticized the special privileges bestowed on the CPSU elite, the nomenklatura systemwhich some said had decayed continuously since Stalin's ruleand the relationship between the Soviet military and the CPSU. Unlike in China, the Soviet military was a state institution whereas in China it is a Party institution. The CPC criticizes the CPSU of pursing Soviet imperialism in its foreign policies.

</doc>
<doc id="7272" url="http://en.wikipedia.org/wiki?curid=7272" title="Christianity and homosexuality">
Christianity and homosexuality

Within Christianity there are a variety of views on the issues of sexual orientation and homosexuality. The many Christian denominations vary in their position, from condemning homosexual acts as sinful, through being divided on the issue, to seeing it as morally acceptable. Even within a denomination, individuals and groups may hold different views. Further, not all members of a denomination necessarily support their church's views on homosexuality.
In the twentieth and twenty-first centuries the extent to which the Bible mentions the subject – and whether or not it is condemned – has become the subject of debate. This debate concerns, at the very least, the proper interpretation of the Levitical code; the story of Sodom and Gomorrah; and various Pauline passages, and whether these verses condemn same-sex sexual activities.
Scriptural views.
The Bible refers to sexual practices that may be called "homosexual" in today's world, but the original language texts of the Bible do not refer explicitly to homosexuality as a sexual orientation. For example, passages in the Old Testament book Leviticus prohibit "lying with mankind as with womankind" and the story of Sodom and Gomorrah (Genesis 19) has been interpreted by some as condemning homosexual "practice".
The story of the destruction of Sodom and Gomorrah does not explicitly identify homosexual practice as the sin for which they were destroyed. Most interpreters find this story and a similar one in Judges 19 to condemn the violent rape of guests, rather than homosexual activity, but the passage has historically been interpreted within Judaism and Christianity as a punishment for homosexuality due to the interpretation that the men of Sodom wished to rape the angels who retrieved Lot.
The New Testament refers to "sexual immorality" on multiple occasions including Matthew 15:19, Mark 7:21, Acts 15:20 and 29, and many more. The definition of "sexual immorality" is disputed among scholars, but it is often included in lists along with adultery (e.g. Matthew 15:19) indicating it is much more than just adultery. Many scholars believe that everything in Leviticus referring to immoral "sexual relations" would be included in the New Testament's "sexual immorality". As such, homosexual practice would be included in all of these passages which condemn sexual immorality. The Apostle Paul's warning against unnatural sexual practices is recorded in chapter 1 of the epistle to the Romans with the passage Romans 1:26-27 in particular being most often cited as evidence of condemnation of homosexuality within the New Testament.
Other interpreters, however, maintain that the Bible does not condemn homosexual activity, arguing any of several points: (i) that the passages yield different meanings if placed in historical context, for instance the historical interpretation of Sodom's sins as being other than homosexuality; (ii) there may be questions surrounding the translation of rare or unusual words in the passages that some interpret as referring to homosexuals; (iii) both the Old Testament and New Testament contain passages that describe same-sex relationships; and/or (iv) that loving and committed relationships are not condemned in the passages. All of these assertions are disputed by other scholars, however.
Christian denominational positions.
Denominations that oppose homosexuality include the Eastern Orthodox Church, the churches of Oriental Orthodoxy and some mainline Protestant denominations, such as the Methodist churches, Reformed Church in America the American Baptist Church, as well as Conservative Evangelical organizations and churches, such as the Evangelical Alliance, the Presbyterian Church in America and the Southern Baptist Convention. Many Pentecostal churches such as the Assemblies of God, as well as Restorationist churches, like Jehovah's Witnesses and LDS Church, also take the position that homosexual activity is immoral. 
The Seventh-day Adventist Church "recognizes that every human being is valuable in the sight of God, and seeks to minister to all men and women [including homosexuals] in the spirit of Jesus," while maintaining that homosexual practice itself is "forbidden" in the Bible. "Jesus affirmed the dignity of all human beings and reached out compassionately to persons and families suffering the consequences of sin. He offered caring ministry and words of solace to struggling people, while differentiating His love for sinners from His clear teaching about sinful practices."
The Catholic Church states that "homosexual tendencies" are "objectively disordered", but does not consider them sinful per se. The Church, however, considers "homosexual acts" to be "grave sins", "intrinsically disordered", and "contrary to the natural law", and "under no circumstances can they be approved". The Eastern Orthodox churches, like the Catholic Church, condemns only homosexual acts. All Orthodox jurisdictions, such as the Orthodox Church in America, have taken the approach of welcoming people with "homosexual feelings and emotions," while encouraging them to work towards "overcoming its harmful effects in their lives," and not allowing the sacraments to people who seek to justify homosexual activity. The Catholic Church views as sinful any sexual act not related to procreation by couple joined under the Sacrament of Matrimony.
Other Christian denominations do not view monogamous same-sex relationships as sinful or immoral, and may bless such unions and consider them marriages. These include the United Church of Canada, and the United Church of Christ., all German Lutheran, reformed and united churches in EKD, all Swiss reformed churches, the Protestant Church in the Netherlands, the Church of Denmark, the Church of Sweden, the Church of Iceland and the Church of Norway. The Evangelical Lutheran Church of Finland also allows prayer for same-sex couples. In particular, the Metropolitan Community Church was founded specifically to serve the Christian LGBT community. The Global Alliance of Affirming Apostolic Pentecostals (GAAAP), traces its roots back to 1980, making it the oldest LGBT-affirming Apostolic Pentecostal denomination in existence. Another such organization is the Affirming Pentecostal Church International, currently the largest affirming Pentecostal organization, with churches in the US, UK, Central and South America, Europe and Africa.
Some denominations state opposing positions. Various churches within the Lutheran World Federation hold stances on the issue ranging from declaring homosexual acts as sin to acceptance of homosexual relationships. For example, the Lutheran Church–Missouri Synod, the Lutheran Church of Australia, and the Wisconsin Evangelical Lutheran Synod recognize homosexual behavior as intrinsically sinful and seek to minister to those who are struggling with homosexual inclinations. However, the Church of Sweden conducts same-sex marriages, while the Evangelical Lutheran Church in America opens the ministry of the church to gay pastors and other professional workers living in committed relationships. The Religious Society of Friends (Quakers) is also much like Lutheranism in regards to homosexuality. For example, the Friends United Meeting and the Evangelical Friends International believe that sexual relations are condoned only in marriage, which they define to be between a man and a woman. However, the Friends General Conference and the Friends in Great Britain approve of same-sex marriage. Most of the Anglican Communion does not approve of homosexual activity, with the exception of the Episcopal Church, which is facing a possible exclusion from international Anglican bodies over the issue.
LGBT-affirming denominations regard homosexuality as a natural occurrence. The United Church of Christ celebrates gay marriage, and some parts of the Anglican and Lutheran churches allow for the blessing of gay unions. The United Church of Canada also allows same-sex marriage, and views sexual orientation as a gift from God. Within the Anglican communion there are openly gay clergy, for example, Gene Robinson is an openly gay Bishop in the US Episcopal Church. Within the Lutheran communion there are openly gay clergy, for example, bishop Eva Brunne is an openly lesbian Bishop in the Church of Sweden. Such religious groups and denominations interpretation of scripture and doctrine leads them to accept that homosexuality is morally acceptable, and a natural occurrence. For example, in 1988 the United Church of Canada, that country's largest Protestant denomination, affirmed that "a) All persons, regardless of their sexual orientation, who profess Jesus Christ and obedience to Him, are welcome to be or become full member of the Church"; and "b) All members of the Church are eligible to be considered for the Ordered Ministry." In 2000, the Church's General Assembly further affirmed that "human sexual orientations, whether heterosexual or homosexual, are a gift from God and part of the marvelous diversity of creation."
In addition, some Christian denominations such as the Moravian Church, believe that the Holy Bible speaks negatively of homosexual acts but, as research on the matter continues, the Moravian Church seeks to establish a policy for ordination and homosexuality.
Critical views.
Most American members of the Christian Right consider homosexual acts as sinful and think they should not be accepted by society. They tend to interpret biblical verses on homosexual acts to mean that the heterosexual family was created by God and that same-sex relationships contradict God’s design for marriage and violate his will. Christians who oppose homosexual relationships sometimes contend that same-gender sexual activity is unnatural.
Christian objections to homosexual behavior are often based upon their interpretations of the Bible. Some Christians believe that the book of Leviticus contains prohibitions against homosexuality. Some Biblical scholars interpret as indicating that homosexual activity led to the destruction of the ancient cities of Sodom and Gomorrah. Other Biblical passages that some interpret as addressing the issue of homosexual behavior include , , , and .
Christian author and counselor Joe Dallas says that the Biblical passages relating to homosexual acts uniformly prohibit that behavior. Exodus International and others take the view that offers Christian believers "freedom from homosexuality," although Exodus has since ceased activities in June 2013, issuing a statement which repudiated its aims and apologized for the harm their pursuit has caused to LGBT people.
The Catechism of the Catholic Church states "men and women who have deep-seated homosexual tendencies ... must be accepted with respect, compassion, and sensitivity." Every sign of unjust discrimination in their regard should be avoided." They oppose criminal penalties against homosexuality. The Catholic Church requires those who are attracted to people of the same (or opposite) sex to practice chastity, because it teaches that sexuality should only be practiced within marriage, which includes chaste sex as permanent, procreative, heterosexual, and monogamous. The Vatican distinguishes between "deep-seated homosexual tendencies" and the "expression of a transitory problem", in relation to ordination to the priesthood; saying in a 2005 document that homosexual tendencies "must be clearly overcome at least three years before ordination to the diaconate." A 2011 report based on telephone surveys of American Catholics conducted by the Public Religion Research Institute found that 56% believe that sexual relations between two people of the same sex are not sinful.
Confessional Lutheran churches teach that it's sinful to have homosexual desires, even if they don't lead to homosexual activity. The Doctrinal statement issued by the Wisconsin Synod states that making a distinction between homosexual orientation and the act of homosexuality is confusing:
However, confessional Lutherans also warn against selective morality which harshly condemns homosexuality while treating other sins more lightly.
In opposing interpretations of the Bible that are supportive of homosexual relationships, conservative Christians have argued for the reliability of the Bible, and the meaning of texts related to homosexual acts, while often seeing what they call the diminishing of the authority of the Bible by many homosexual authors as being ideologically driven.
As an alternative to a school-sponsored Day of Silence opposing bullying of LGBT students, conservative Christians organized a Golden Rule Initiative, where they passed out cards saying "As a follower of Christ, I believe that all people are created in the image of God and therefore deserve love and respect." Others created a Day of Dialogue to oppose what they believe is the "silencing" of Christian students who make public their opposition to homosexuality.
Favorable views.
In the 20th century, theologians like Jürgen Moltmann, Hans Küng, John Robinson, Bishop David Jenkins, Don Cupitt, Bishop Jack Spong and Matthew Vines challenged traditional theological positions and understandings of the Bible; following these developments some have suggested that passages have been mistranslated or that they do not refer to what we understand as "homosexuality." Clay Witt, a minister in the Metropolitan Community Church, explains how theologians and commentators like John Shelby Spong, George Edwards and Michael England interpret injunctions against certain sexual acts as being originally intended as a means of distinguishing religious worship between Abrahamic and the surrounding pagan faiths, within which homosexual acts featured as part of idolatrous religious practices: "England argues that these prohibitions should be seen as being directed against sexual practices of fertility cult worship. As with the earlier reference from Strong’s, he notes that the word 'abomination' used here is directly related to idolatry and idolatrous practices throughout the Hebrew Testament. Edwards makes a similar suggestion, observing that 'the context of the two prohibitions in and suggest that what is opposed is not same-sex activity outside the cult, as in the modern secular sense, but within the cult identified as Canaanite'".
Some Christians believe that Biblical passages have been mistranslated or that these passages do not refer to LGBT orientation as currently understood. Liberal Christian scholars, like conservative Christian scholars, accept earlier versions of the texts that make up the Bible in Hebrew or Greek. However, within these early texts there are many terms that modern scholars have interpreted differently from previous generations of scholars. There are concerns with copying errors, forgery, and biases among the translators of later Bibles. They consider some verses such as those they say support slavery or the inferior treatment of women as not being valid today, and against the will of God present in the context of the Bible. They cite these issues when arguing for a change in theological views on sexual relationships to what they say is an earlier view. They differentiate among various sexual practices, treating rape, prostitution, or temple sex rituals as immoral and those within committed relationships as positive regardless of sexual orientation. They view certain verses, which they believe refer only to homosexual rape, as not relevant to consensual homosexual relationships.
Yale scholar John Boswell has argued that a number of Early Christians entered into homosexual relationships, and that certain Biblical figures had homosexual relationships, such as Ruth and her mother-in-law Naomi, Daniel and the court official Ashpenaz, and David and King Saul's son Jonathan. Boswell has also argued that adelphopoiesis, a rite bonding two men, was akin to a religiously sanctioned same-sex union. Having partaken in such a rite, a person was prohibited from entering into marriage or taking monastic vows, and the choreography of the service itself closely parallelled that of the marriage rite. His views have not found wide acceptance, and opponents have argued that this rite sanctified a Platonic brotherly bond, not a homosexual union. He also argued that condemnation of homosexuality began only in the 12th century. Critics of Boswell have pointed out that many earlier doctrinal sources condemn homosexuality in ethical terms without prescribing a punishment, and that Boswell's citations reflected a general trend towards harsher penalties from the 12th century onwards.
Desmond Tutu, the former Anglican Archbishop of Cape Town and a Nobel Peace Prize winner, has described homophobia as a "crime against humanity" and "every bit as unjust" as apartheid: "We struggled against apartheid in South Africa, supported by people the world over, because black people were being blamed and made to suffer for something we could do nothing about; our very skins. It is the same with sexual orientation. It is a given. ... We treat them [gays and lesbians] as pariahs and push them outside our communities. We make them doubt that they too are children of God – and this must be nearly the ultimate blasphemy. We blame them for what they are."
Modern gay Christian leader Justin R. Cannon promotes what he calls "Inclusive Orthodoxy" (not to be confused with the Eastern Orthodox Church). He explains on his ministry website: "Inclusive Orthodoxy is the belief that the Church can and must be inclusive of LGBT individuals without sacrificing the Gospel and the Apostolic teachings of the Christian faith." Cannon's ministry takes a unique approach quite distinct from modern liberal Christians, yet which still supports homosexual relations. His ministry affirms the divine inspiration of the Bible, the authority of Tradition, and says "...that there is a place within the full life and ministry of the Christian Church for lesbian, gay, bisexual, and transgender Christians, both those who are called to lifelong celibacy and those who are partnered."
Homosexual Christians and organizations.
George Barna, a conservative Christian author and researcher, conducted a survey in the United States in 2009 that found gay and lesbian people having a Christian affiliation were more numerous than had been presumed. He characterized some of his leading conclusions from the data as follows:"People who portray gay adults as godless, hedonistic, Christian bashers are not working with the facts. A substantial majority of gays cite their faith as a central facet of their life, consider themselves to be Christian, and claim to have some type of meaningful personal commitment to Jesus Christ active in their life today. The data indicate that millions of gay people are interested in faith but not in the local church and do not appear to be focused on the traditional tools and traditions that represent the comfort zone of most churched Christians. Gay adults clearly have a different way of interpreting the Bible on a number of central theological matters, such as perspectives about God. Homosexuals appreciate their faith but they do not prioritize it, and they tend to consider faith to be individual and private rather than communal." The study of 20 faith-oriented attributes revealed significant differences between the United States heterosexual and homosexual populations sampled, homosexual respondents being less likely to be "born again Christians" than heterosexual respondents (27% compared to 47%), and the degree of commitment to their faith and families also differed. Other significant contrasts were seen in regards to "liberal" versus "conservative" social positions, as well as in one’s understanding of God, with 43% of homosexual participants sharing the "orthodox, biblical" understanding of God which 71% of heterosexual participants indicate they do. Respondents were not asked to describe themselves as "born again", but as is standard in Barna studies, this classification was defined according to basic standard criteria. Barna concluded, “The data indicates that millions of gay people are interested in faith but not in the local church and do not appear to be focused on the traditional tools and traditions that represent the comfort zone of most churched Christians." And that "Gay adults clearly have a different way of interpreting the Bible on a number of central theological matters, such as perspectives about God."
Candace Chellew-Hodge, liberal Christian lesbian founder of online magazine "Whoseoever", responded to the findings:
All in all, I'm grateful for Barna even wandering into the subject of gay and lesbian religious belief. I think his study is important and can go a long way to dispelling the old "gays vs. God" dichotomy that too often gets played out in the media. However, his overall message is still harmful: Gays and lesbians are Christians – they're just not as good as straight ones.
She argued that Barna had formulated his report with undue irony and skepticism, and that he had failed to take into account the reasons for the data which enkindled his "arrière pensée." The reason why far fewer homosexuals attend church, she argued, is that there are far fewer churches who will accept them. Equally, gays and lesbians do not see the Bible as unequivocally true because they are forced by its use against them to read it more closely and with less credulity, leading them to note its myriad contradictions.
Organizations for homosexual Christians exist across a wide range of beliefs and traditions. The interdenominational Gay Christian Network has some members who affirm same-sex relationships and others who commit themselves to celibacy, groups it refers to as "Side A" and "Side B", respectively. According to founder Justin Lee, "We're just trying to get people together who experience attraction to the same sex, however they have handled that, and who love Jesus and say, OK, you are welcome here, and then let's pray together and figure out where God wants us to take it."
Some organizations cater exclusively to homosexual Christians who do not want to have gay sex, or attraction; the goals of these organizations vary. Some Christian groups focus on simply refraining from gay sex, such as Courage International and North Star. Other groups additionally encourage gay members to reduce or eliminate same-sex attractions. Exodus International and the associated Love Won Out are examples of such ministries. These groups are sometimes referred to as ex-gay organizations, though many no longer use the term. Alan Chambers, the president of Exodus, says the term incorrectly implies a complete change in sexual orientation, though the group Parents and Friends of Ex-Gays and Gays continues to use the term.
Gay Christian writer and actor Peterson Toscano argues that organizations promoting orientation change are a "ruse." An organization he co-founded, Beyond Ex-Gay, supports people who feel they have been wounded by such organizations.
Other groups support or advocate for gay Christians and their relationships. For example, in the United States, IntegrityUSA represents the interests of lesbian and gay Christians in the Episcopal Church, while United Methodists have the Reconciling Ministries Network and evangelical Christians have "Evangelicals Concerned". In 2014 the United Church of Christ filed a lawsuit challenging North Carolina’s ban on same-sex marriage, which is America’s first faith-based challenge to same-sex marriage bans; the Alliance of Baptists joined the lawsuit later that year.
In Europe, lesbian and gay evangelical Christians have a European forum. Working within the worldwide Anglican Communion on a range of discrimination issues, including those of LGBT clergy and people in the church, is "Inclusive Church". The longest standing group for lesbian and gay Christians in the UK, founded in 1976, is the non-denominational Lesbian and Gay Christian Movement; specifically aimed to meet the needs of lesbian and gay evangelicals, there is the "Evangelical Fellowship for Lesbian and Gay Christians"; specifically working within the Church of England is "Changing Attitude", which also takes an international focus in working for gay, lesbian, bisexual & transgender affirmation within the Anglican Communion. such as gay Anglicans in Nigeria.
Sociologist Richard N. Pitt argues that these organizations are only available to LGBT members of liberal denominations, as opposed to those in conservative denominations. His review of the literature on gay Christians suggests that these organizations not only represent the interests of Christians who attend their churches, but (like gay-friendly and gay-affirming churches) also give these members useful responses to homophobic and heterosexist rhetoric. His research shows that those GLBT Christians who stay at homophobic churches "kill the messenger" by attacking the minister's knowledge about homosexuality, personal morality, focus on sin instead of forgiveness, and motivations for preaching against homosexuality.

</doc>
<doc id="7273" url="http://en.wikipedia.org/wiki?curid=7273" title="Chadic languages">
Chadic languages

The Chadic languages are a branch of the Afroasiatic language family that is spoken in parts of the Sahel. They include 150 languages spoken across northern Nigeria, southern Niger, southern Chad, Central African Republic and northern Cameroon. The most widely spoken Chadic language is Hausa, a lingua franca of much of inland West Africa.
Composition.
Newman (1977) classified the languages into four groups which have been accepted in all subsequent literature. Further subbranching, however, has not been as robust; Blench (2006), for example, only accepts the A/B bifurcation of East Chadic.
Origin.
Several modern genetic studies of Chadic speaking groups in the northern Cameroon region have observed high frequencies of the Y-Chromosome Haplogroup R1b in these populations (specifically, of R1b's R-V88 variant). This paternal marker is common in parts of West Eurasia, but otherwise rare in Africa.

</doc>
<doc id="7274" url="http://en.wikipedia.org/wiki?curid=7274" title="Cushitic languages">
Cushitic languages

The Cushitic languages are a branch of the Afro-Asiatic language family spoken primarily in the Horn of Africa (Somalia, Eritrea, Djibouti, and Ethiopia), as well as the Nile Valley (Sudan and Egypt), and parts of the African Great Lakes region (Tanzania and Kenya). The branch is named after the Biblical character Cush, who was traditionally identified as an ancestor of the speakers of these specific languages as early as 947 CE (in Masudi's Arabic history "Meadows of Gold"). 
The most populous Cushitic language is Oromo (including all its variations) with about 35 million speakers, followed by Somali with about 18 million speakers, and Sidamo with about 3 million speakers. Other Cushitic languages with more than one million speakers are Afar (1.5 million) and Beja (1.2 million).
Somali, one of the official languages of Somalia, is the only Cushitic language accorded official status in any country. It is also one of the recognized national languages of Djibouti, the other being Afar.
Composition.
The Cushitic languages are usually considered to include the following branches:
Lowland and Highland East Cushitic are commonly combined with Dullay into a single branch called "East Cushitic".
Divergent languages.
The Beja language, or "North Cushitic," is sometimes listed as a separate branch of Afroasiatic. However, it is more often included within the Afro-Asiatic family's Cushitic branch.
There are also a few poorly classified languages, including Yaaku, Dahalo, Aasax, Kw'adza, Boon, the Cushitic element of Mbugu (Ma’a) and Ongota. There is a wide range of opinions as to how these languages are interrelated.
The positions of the Dullay languages and Yaaku are uncertain. These have traditionally been assigned to an "East Cushitic" branch along with Highland (Sidamic) and Lowland East Cushitic. However, Hayward believes East Cushitic may not be a valid node and that its constituents should be considered separately when attempting to work out the internal relationships of Cushitic.
The Afroasiatic identity of Ongota is also broadly questioned, as is its position within Afroasiatic among those who accept it, due to the "mixed" appearance of the language and a paucity of research and data. Harold Fleming (2006) proposes that Ongota constitutes a separate branch of Afroasiatic. Bonny Sands (2009) believes the most convincing proposal is by Savà and Tosco (2003), namely that Ongota is an East Cushitic language with a Nilo-Saharan substratum. In other words, it would appear that the Ongota people once spoke a Nilo-Saharan language but then shifted to speaking a Cushitic language while retaining some characteristics of their earlier Nilo-Saharan language.
Hetzron (1980:70ff) and Ehret (1995) have suggested that the Rift languages (South Cushitic) are a part of Lowland East Cushitic, the only one of the six groups with much internal diversity.
Cushitic was traditionally seen as also including the Omotic languages, then called "West Cushitic." However, this view has largely been abandoned, with Omotic generally agreed to be an independent branch of Afroasiatic, primarily due to the work of Harold C. Fleming (1974) and M. Lionel Bender (1975).

</doc>
<doc id="7279" url="http://en.wikipedia.org/wiki?curid=7279" title="Chapter 11, Title 11, United States Code">
Chapter 11, Title 11, United States Code

Chapter 11 is a chapter of Title 11 of the United States Bankruptcy Code, which permits reorganization under the bankruptcy laws of the United States. Chapter 11 bankruptcy is available to every business, whether organized as a corporation, partnership or sole proprietorship, and to individuals, although it is most prominently used by corporate entities. In contrast, Chapter 7 governs the process of a liquidation bankruptcy (although liquidation can go under this chapter), while Chapter 13 provides a reorganization process for the majority of private individuals.
Chapter 11 in general.
When a business is unable to service its debt or pay its creditors, the business or its creditors can file with a federal bankruptcy court for protection under either Chapter 7 or Chapter 11.
In Chapter 7, the business ceases operations, a trustee sells all of its assets, and then distributes the proceeds to its creditors. Any residual amount is returned to the owners of the company. In Chapter 11, in most instances the debtor remains in control of its business operations as a "debtor in possession", and is subject to the oversight and jurisdiction of the court.
Features of Chapter 11 reorganization.
Chapter 11 retains many of the features present in all, or most, bankruptcy proceedings in the U.S. It provides additional tools for debtors as well. Most importantly, empowers the trustee to operate the debtor's business. In chapter 11, unless a separate trustee is appointed for cause, the debtor, as debtor in possession, acts as trustee of the business.
Chapter 11 affords the debtor in possession a number of mechanisms to restructure its business. A debtor in possession can acquire financing and loans on favorable terms by giving new lenders first priority on the business's earnings. The court may also permit the debtor in possession to reject and cancel contracts. Debtors are also protected from other litigation against the business through the imposition of an automatic stay. While the automatic stay is in place, creditors are stayed from any collection attempts or activities against the debtor in possession, and most litigation against the debtor is stayed, or put on hold, until it can be resolved in bankruptcy court, or resumed in its original venue. An example of proceedings that are not necessarily stayed automatically are family law proceedings against a spouse or parent. Further, creditors may file with the court seeking relief from the automatic stay.
If the business is insolvent, its debts exceed its assets and the business is unable to pay debts as they come due, the bankruptcy restructuring may result in the company's owners being left with nothing; instead, the owners' rights and interests are ended and the company's creditors are left with ownership of the newly reorganized company.
All creditors are entitled to be heard by the court. 11 U.S.C. Sec. 1109 (b). The court is ultimately responsible for determining whether the proposed plan of reorganization complies with the bankruptcy law.
One controversy that has broken out in bankruptcy courts concerns the proper amount of disclosure that the court and other parties are entitled to receive from the members of the ad hoc creditor's committees that play a large role in many such proceedings.
Chapter 11 plan.
Chapter 11 usually results in reorganization of the debtor's business or personal assets and debts, but can also be used as a mechanism for liquidation. Debtors may "emerge" from a chapter 11 bankruptcy within a few months or within several years, depending on the size and complexity of the bankruptcy. The Bankruptcy Code accomplishes this objective through the use of a bankruptcy plan. The debtor in possession typically has the first opportunity to propose a plan during the period of exclusivity. This period allows the debtor 120 days from the date of filing for chapter 11, to propose a plan of reorganization before any other party in interest may propose a plan. If the debtor proposes a plan within the 120 day exclusivity period, a 180 day exclusivity period from the date of filing for chapter 11 is granted in order to allow the debtor to gain confirmation of the proposed plan. With some exceptions, the plan may be proposed by any party in interest. Interested creditors then vote for a plan.
Confirmation.
If the judge approves the reorganization plan and if the creditors all agree the plan can be confirmed. If at least one class of creditors votes against the plan and thus objects, the plan may nonetheless be confirmed if the requirements of cramdown are met. In order to be confirmed over their objection the plan must not discriminate against that class of creditors, and the plan must be found fair and equitable to that class.
Upon its confirmation, the plan becomes binding and identifies the treatment of debts and operations of the business for the duration of the plan.
If a plan cannot be confirmed, the court may either convert the case to a liquidation under chapter 7, or, if in the best interests of the creditors and the estate, the case may be dismissed resulting in a return to the status quo before bankruptcy. If the case is dismissed, creditors will look to non-bankruptcy law in order to satisfy their claims.
Automatic stay.
Like other forms of bankruptcy, petitions filed under chapter 11 invoke the automatic stay of . The automatic stay requires all creditors to cease collection attempts, and makes many post-petition debt collection efforts void or voidable. Under some circumstances, some creditors, otherwise the United States Trustee can request for the court converting the case into a liquidation under chapter 7, or appointing a trustee to manage the debtor's business. The court will grant a motion to convert to chapter 7 or appoint a trustee if either of these actions is in the best interest of all creditors. Sometimes a company will liquidate under chapter 11, in which the pre-existing management may be able to help get a higher price for divisions or other assets than a chapter 7 liquidation would be likely to achieve. Appointment of a trustee requires some wrongdoing or gross mismanagement on the part of existing management and is relatively rare.
Executory contracts.
Some contracts, known as executory contracts, may be rejected if canceling them would be financially favorable to the company and its creditors. Such contracts may include labor union contracts, supply or operating contracts (with both vendors and customers), and real estate leases. The standard feature of executory contracts is that each party to the contract has duties remaining under the contract. In the event of a rejection, the remaining parties to the contract become unsecured creditors of the debtor. For example, in some districts a contract for deed is an executory contract, while in others it is not.
In the new millennium airlines have fallen under intense scrutiny for what many see as abusing Chapter 11 Bankruptcy as a simple tool for escaping labor contracts, usually 30-35% of an airline's operating cost. Every major US. airline has filed for Chapter 11 since 2002. In the space of 2 years (2002 - 2004) US. Airways filed for bankruptcy twice leaving the AFL-CIO, pilot unions and other airline employees claiming the rules of Chapter 11 have helped turn the USA into a corporatocracy.
Priority.
Chapter 11 follows the same priority scheme as other bankruptcy chapters. The priority structure is defined primarily by § 507 of the Bankruptcy Code (.)
As a general rule, administrative expenses (the actual, necessary expenses of preserving the bankruptcy estate, including expenses such as employee wages, and the cost of litigating the chapter 11 case)are paid first. Secured creditors—creditors who have a security interest, or collateral, in the debtor's property—will be paid before unsecured creditors. Unsecured creditors' claims are prioritized by § 507. For instance the claims of suppliers of products or employees of a company may be paid before other unsecured creditors are paid. Each priority level must be paid in full before the next lowest priority level may receive payment.
Section 1110.
Section 1110 () generally provides a secured party with an interest in an aircraft the ability to take possession of the equipment within 60 days after a bankruptcy filing unless the airline cures all defaults. More specifically, the right of the lender to take possession of the secured equipment is not hampered by the automatic stay provisions of the U.S. Bankruptcy Code.
Stock.
If the company's stock is publicly traded, a Chapter 11 filing generally causes it to be delisted from its primary stock exchange if listed on the New York Stock Exchange, the American Stock Exchange, or the NASDAQ. On the NASDAQ the identifying fifth letter "Q" at the end of a stock symbol indicates the company is in bankruptcy (formerly the "Q" was placed in front of the pre-existing stock symbol; a celebrated example was Penn Central, whose symbol was originally "PC" and became "QPC" after the company filed Chapter 11 in 1970). Many stocks that are delisted quickly resume listing as over-the-counter (OTC) stocks. In the overwhelming majority of cases, the Chapter 11 plan, when confirmed, terminates the shares of the company, rendering shares valueless.
Individuals may file Chapter 11, but due to the complexity and expense of the proceeding, this option is rarely chosen by debtors who are eligible for Chapter 7 or Chapter 13 relief.
Rationale.
In enacting Chapter 11 of the Bankruptcy code, Congress concluded that it is sometimes the case that the value of a business is greater if sold or reorganized as a going concern than the value of the sum of its parts if the business's assets were to be sold off individually. It follows that it may be more economically efficient to allow a troubled company to continue running, cancel some of its debts, and give ownership of the newly reorganized company to the creditors whose debts were canceled. Alternatively, the business can be sold as a going concern with the net proceeds of the sale distributed to creditors ratably in accordance with statutory priorities. In this way, jobs may be saved, the (previously mismanaged) engine of profitability which is the business is maintained (presumably under better management) rather than being dismantled, and, as a proponent of a chapter 11 plan is required to demonstrate as a precursor to plan confirmation, the business's creditors end up with more money than they would in a Chapter 7 liquidation.
Considerations.
The reorganization and court process may take an inordinate amount of time, limiting the chances of a successful outcome and sufficient debtor in possession financing may be unavailable during an economic recession. A preplanned, preagreed approach between the debtor and its creditors (sometimes called a pre-packaged bankruptcy) may facilitate the desired result. A company undergoing Chapter 11 reorganization is effectively operating under the "protection" of the court until it emerges. An example is the airline industry in the United States; in 2006 over half the industry's seating capacity was on airlines that were in Chapter 11. These airlines were able to stop making debt payments, break their previously agreed upon labor union contracts, freeing up cash to expand routes or weather a price war against competitors — all with the bankruptcy court's approval.
Studies on the impact of forestalling the creditors' rights to enforce their security reach different conclusions.
Statistics.
Frequency.
Chapter 11 cases dropped by 60% from 1991 to 2003. One 2007 study found this was because businesses were turning to bankruptcy-like proceedings under state law, rather than the federal bankruptcy proceedings, including those under chapter 11. Insolvency proceedings under state law, the study stated, are currently faster, less expensive, and more private, with some states not even requiring court filings. However, a 2005 study claimed the drop may have been due to an increase in the incorrect classification of many bankruptcies as "consumer cases" rather than "business cases".
Cases involving more than US$50 million in assets are almost always handled in federal bankruptcy court, and not in bankruptcy-like state proceeding.
Largest cases.
The largest bankruptcy in history was of the US investment bank Lehman Brothers Holdings Inc., which listed $639 billion in assets as of its Chapter 11 filing in 2008. The 16 largest corporate bankruptcies as of 13 December 2011:

</doc>
<doc id="7280" url="http://en.wikipedia.org/wiki?curid=7280" title="Conjugation">
Conjugation

Conjugation or conjugate may refer to:

</doc>
<doc id="7283" url="http://en.wikipedia.org/wiki?curid=7283" title="Controversy">
Controversy

Controversy is a state of prolonged public dispute or debate, usually concerning a matter of conflicting opinion or point of view. The word was coined from the Latin "controversia", as a composite of "controversus" – "turned in an opposite direction," from "contra" – "against" – and "vertere" – to turn, or "versus" (see verse), hence, "to turn against."
The most applicable or well known controversial subjects, topics or areas are politics, religion and sex. Other areas of controversy include history and philosophy. Other minor yet prominent areas of controversy are economics, science, finances, culture, education, the military, society, celebrities, organisation, the media, age, gender, and race. Controversy in matters of theology has traditionally been particularly heated, giving rise to the phrase "odium theologicum". Controversial issues are held as potentially divisive in a given society, because they can lead to tension and ill will, as a result they are often taboo to be discussed in the light of company in many cultures.
Legal controversy.
In the theory of law, a controversy differs from a legal case; while legal cases include all suits, criminal as well as civil, a controversy is a purely civil proceeding.
For example, the Case or Controversy Clause of Article Three of the United States Constitution (, Clause 1) states that "the judicial Power shall extend ... to Controversies to which the United States shall be a Party". This clause has been deemed to impose a requirement that United States federal courts are not permitted to hear cases that do not pose an actual controversy—that is, an actual dispute between adverse parties which is capable of being resolved by the [court]. In addition to setting out the scope of the jurisdiction of the federal judiciary, it also prohibits courts from issuing advisory opinions, or from hearing cases that are either unripe, meaning that the controversy has not arisen yet, or moot, meaning that the controversy has already been resolved.
Benford's la.
Benford's law of controversy, as expressed by science fiction author Gregory Benford in 1980, states: "Passion is inversely proportional to the amount of real (true) information available." In other words, the fewer facts are known to and agreed on by the participants, the more controversy there is, and the more is known the less controversy there is. Thus, for example, controversies in physics are limited to subject-areas where experiments cannot be carried out yet, whereas Benford's Law implies that controversy is inherent to politics, where communities must frequently decide on courses of action based on insufficient information.
Psychological bases.
Controversies are frequently thought to be a result of a lack of confidence on the part of the disputants - as in Benford's law of controversy. For example, in the political controversy over anthropogenic climate change that is prevalent in the United States - it has been thought that those who are opposed to the scientific consensus did so because of a lack of evidence. A study of 1540 US adults found instead that levels of scientific literacy were correlated with the strength of opinion on climate change, but not on which side of the debate that they stood.
The puzzling phenomenon of two individuals being exposed to the same evidence and being able to reach different conclusions, has been frequently explained (particularly by Daniel Kahneman) by reference to a 'bounded rationality' - that is most judgments are made by fast acting heuristics (system 1) that work well in every day situations, but are not amenable to decision making about complex subjects such as climate change. Anchoring has been particularly identified as relevant in climate change controversies as individuals are found to be more positively inclined to believe in climate change if the outside temperature is higher, if they have been primed to think about heat, and if they are primed with higher temperatures when thinking about the future temperature increases from climate change.
In other controversies - such as that around the HPV vaccine, the same evidence seemed to license inference to radically different conclusions. Kahan et al. explained this by the cognitive biases of biased assimilation and a credibility heuristic.
Similar effects on reasoning are also seen in non-scientific controversies, for example in the gun control debate in the United States. As with other controversies, it has been suggested that exposure to empirical facts would be sufficient to resolve the debate once and for all. In computer simulations of cultural communities, beliefs were found to polarize within isolated sub-groups, based on the mistaken belief of the community's unhindered access to ground truth. Such confidence in the group to find the ground truth is explicable through the success of wisdom of the crowd based inferences, however, if there is no access to the ground truth, as there was not in this model, the method will fail.
Bayesian decision theory allows these failures of rationality to be described as part of a statistically optimized system for decision making. Experiments and computational models in multisensory integration have shown that sensory input from different senses is integrated in a statistically optimal way, in addition, it appears that the kind of inferences used to infer single sources for multiple sensory inputs uses a Bayesian inference about the causal origin of the sensory stimuli. As such, it appears neurobiologically plausible that the brain implements decision-making procedures that are close to optimal for Bayesian inference.
Brocas and Carrillo propose a model to make decisions based on noisy sensory inputs, beliefs about the state of the world are modified by Bayesian updating, and then decisions are made based on beliefs passing a threshold. They show that this model, when optimized for single-step decision making, produces belief anchoring and polarization of opinions - exactly as described in the global warming controversy context - in spite of identical evidence presented, the pre-existing beliefs (or evidence presented first) has an overwhelming effect on the beliefs formed. In addition, the preferences of the agent (the particular rewards that they value) also cause the beliefs formed to change - this explains the biased assimilation (also known as confirmation bias) shown above. This model allows the production of controversy to be seen as a consequence of a decision maker optimized for single-step decision making, rather than as a result of limited reasoning in the bounded rationality of Daniel Kahneman.

</doc>
<doc id="7284" url="http://en.wikipedia.org/wiki?curid=7284" title="Centromere">
Centromere

The centromere is the part of a chromosome that links sister chromatids. During mitosis, spindle fibers attach to the centromere via the kinetochore. Centromeres were first defined as genetic loci that direct the behavior of chromosomes. Their physical role is to act as the site of assembly of the kinetochore - a highly complex multiprotein structure that is responsible for the actual events of chromosome segregation - e.g. binding microtubules and signalling to the cell cycle machinery when all chromosomes have adopted correct attachments to the spindle, so that it is safe for cell division to proceed to completion (i.e. for cells to enter anaphase). There are broadly speaking two types of centromeres. "Point centromeres" bind to specific proteins that recognise particular DNA sequences with high efficiency. Any piece of DNA with the point centromere DNA sequence on it will typically form a centromere if present in the appropriate species. The best characterised point centromeres are those of the budding yeast, "Saccharomyces cerevisiae". "Regional centromeres" is the term coined to describe most centromeres, which typically form on regions of preferred DNA sequence, but which can form on other DNA sequences as well. The signal for formation of a regional centromere appears to be epigenetic. Most organisms, ranging from the fission yeast "Schizosaccharomyces pombe" to humans, have regional centromeres.
Regarding mitotic chromosome structure, centromeres represent a constricted region of the chromosome (often referred to as the primary constriction) where two identical sister chromatids are most closely in contact. When cells enter mitosis, the sister chromatids (which represent the two copies of each chromosomal DNA molecule resulting from DNA replication earlier in the cell cycle and packaged by histones and other proteins into chromatin) are linked all along their length by the action of the cohesin complex. It is now believed that this complex is mostly released from chromosome arms during prophase, so that by the time the chromosomes line up at the mid-plane of the mitotic spindle (also known as the metaphase plate), the last place where they are linked with one another is in the chromatin in and around the centromere.
Positions.
Each chromosome has two arms, labeled p (the shorter of the two) and q (the longer). The p arm is named for "petit" meaning 'small'; the q arm is named q simply because it follows p in the alphabet. (According to the NCBI, "q" refers to the French word "queue" meaning 'tail'.) They can be connected in either metacentric, submetacentric, acrocentric or telocentric manner.
Metacentric.
These are X-shaped chromosomes, with the centromere in the middle so that the two arms of the chromosomes are almost equal.
A chromosome is metacentric if its two arms are roughly equal in length. In a normal human karyotype, two chromosomes are considered metacentric: chromosomes 1 and 3. In some cases, a metacentric chromosome is formed by balanced translocation: the fusion of two acrocentric chromosomes to form one metacentric chromosome.
Submetacentric.
If arms' lengths are unequal, the chromosome is said to be submetacentric.
Acrocentric.
If the p (short) arm is so short that it is hard to observe, but still present, then the chromosome is acrocentric (the "acro-" in acrocentric refers to the Greek word for "peak"). The human genome includes six acrocentric chromosomes: 13, 14, 15, 21, 22, and Y.
In an acrocentric chromosome the p arm contains genetic material including repeated sequences such as nucleolar organizing regions, and can be translocated without significant harm, as in a balanced Robertsonian translocation. The domestic horse genome includes one metacentric chromosome that is homologous to two acrocentric chromosomes in the conspecific but undomesticated Przewalski's horse. This may reflect either fixation of a balanced Robertsonian translocation in domestic horses or, conversely, fixation of the fission of one metacentric chromosome into two acrocentric chromosomes in Przewalski's horses. A similar situation exists between the human and great ape genomes; in this case, because more species are extant, it is apparent that the evolutionary sequence is a reduction of two acrocentric chromosomes in the great apes to one metacentric chromosome in humans (see Karyotype#Aneuploidy).
Telocentric.
A telocentric chromosome's centromere is located at the terminal end of the chromosome. Telomeres may extend from both ends of the chromosome. For example, the standard house mouse karyotype has only telocentric chromosomes. Humans do not possess telocentric chromosomes.
Subtelocentric.
If the chromosome's centromere is located closer to its end than to its center, it may be described as subtelocentric.
Holocentric.
With holocentric chromosomes, the entire length of the chromosome acts as the centromere. Examples of this type of centromere can be found scattered throughout the plant and animal kingdoms, with the most well known example being the nematode "Caenorhabditis elegans".
Sequence.
There are two types of centromeres. In regional centromeres, DNA sequences contribute to but do not define function. Regional centromeres contain large amounts of DNA and are often packaged into heterochromatin. In most eukaryotes, the centromere's DNA sequence consists of large arrays of repetitive DNA (e.g. satellite DNA) where the sequence within individual repeat elements is similar but not identical. In humans, the primary centromeric repeat unit is called α-satellite (or alphoid), although a number of other sequence types are found in this region.
Point centromeres are smaller and more compact. DNA sequences are both necessary and sufficient to specify centromere identity and function in organisms with point centromeres. In budding yeasts, the centromere region is relatively small (about 125 bp DNA) and contains two highly conserved DNA sequences that serve as binding sites for essential kinetochore proteins.
Inheritance.
Since centromeric DNA sequence is not the key determinant of centromeric identity in metazoans, it is thought that epigenetic inheritance plays a major role in specifying the centromere. The daughter chromosomes will assemble centromeres in the same place as the parent chromosome, independent of sequence. It has been proposed that histone H3 variant CENP-A (Centromere Protein A) is the epigenetic mark of the centromere. The question arises whether there must be still some original way in which the centromere is specified, even if it is subsequently propagated epigenetically. If the centromere is inherited epigenetically from one generation to the next, the problem is pushed back to the origin of the first metazoans.
Structure.
The centromeric DNA is normally in a heterochromatin state, which is essential for the recruitment of the cohesin complex that mediates sister chromatid cohesion after DNA replication as well as coordinating sister chromatid separation during anaphase. In this chromatin, the normal histone H3 is replaced with a centromere-specific variant, CENP-A in humans. The presence of CENP-A is believed to be important for the assembly of the kinetochore on the centromere. CENP-C has been shown to localise almost exclusively to these regions of CENP-A associated chromatin. In human cells, the histones are found to be most enriched for H4K20me3 and H3K9me3 which are known heterochromatic modifications.
In the yeast "Schizosaccharomyces pombe" (and probably in other eukaryotes), the formation of centromeric heterochromatin is connected to RNAi. In nematodes such as "Caenorhabditis elegans", some plants, and the insect orders Lepidoptera and Hemiptera, chromosomes are "holocentric", indicating that there is not a primary site of microtubule attachments or a primary constriction, and a "diffuse" kinetochore assembles along the entire length of the chromosome.
Centromeric aberrations.
In rare cases in humans, neocentromeres can form at new sites on the chromosome. There are currently over 90 known human neocentromeres identified on 20 different chromosomes. The formation of a neocentromere must be coupled with the inactivation of the previous centromere, since chromosomes with two functional centromeres (Dicentric chromosome) will result in chromosome breakage during mitosis. In some unusual cases human neocentromeres have been observed to form spontaneously on fragmented chromosomes. Some of these new positions were originally euchromatic and lack alpha satellite DNA altogether.
Centromere proteins are also the autoantigenic target for some anti-nuclear antibodies, such as anti-centromere antibodies.

</doc>
<doc id="7287" url="http://en.wikipedia.org/wiki?curid=7287" title="Castello">
Castello

Castello is the Italian word for "castle". The Latin word "Castellum" means reservoir, water tank.
Castello may refer to:

</doc>
<doc id="7288" url="http://en.wikipedia.org/wiki?curid=7288" title="Common preference">
Common preference

Common preference is an "everyone wins" situation in a number of places:

</doc>
<doc id="7291" url="http://en.wikipedia.org/wiki?curid=7291" title="CuteFTP">
CuteFTP

CuteFTP is a series of FTP (file transfer) client applications Distributed and Supported by GlobalSCAPE since 1996, who later bought the rights to the software. Both a Windows-based or Mac-based interface were made for both home and professional use.
CuteFTP is used to transfer files between computers and File Transfer Protocol (FTP) servers to publish web pages, download digital images, music, multi-media files and software, and transfer files of any size or type between home and office. Since 1999, CuteFTP Pro and CuteFTP Mac Pro have also been available alongside CuteFTP Home with free trial periods.
It was originally developed by the Russian, Alex Kunadze.

</doc>
<doc id="7293" url="http://en.wikipedia.org/wiki?curid=7293" title="Commodore 64">
Commodore 64

The Commodore 64, commonly called C64, C= 64 (the "equals" sign suggesting the right half of the logo graphic on the case), occasionally CBM 64 (for Commodore Business Machines), or VIC-64, is an 8-bit home computer introduced in January 1982 by Commodore International. It is listed in the Guinness Book of World Records as the highest-selling single computer model of all time, with independent estimates placing the number sold between 10 and 17 million units.
Volume production started in early 1982, with machines being released on to the market in August at a price of (). Preceded by the Commodore VIC-20 and Commodore PET, the C64 takes its name from its of RAM, and has favorable sound and graphical specifications when compared to contemporary systems such as the Apple II. While the Apple cost around $1,200, it was sold as a complete system with disk drive and dedicated monitor—the C64's $595 price included only the computer itself. The Tandy TRS-80 Color Computer was initially priced at $399, but had only 4kB RAM and could not match the C64's graphics and sound abilities.
The C64 dominated the low-end computer market for most of the 1980s. For a substantial period (1983–1986), the C64 had between 30% and 40% share and two million units sold per year, outselling the IBM PC compatibles, Apple Inc. computers, and the Atari 8-bit family of computers. Sam Tramiel, a later Atari president and the son of Commodore's founder, said in a 1989 interview, "When I was at Commodore we were building C64s a month for a couple of years."
Part of the Commodore 64's success was because it was sold in retail stores instead of just electronics- and/or computer stores. Commodore produced many of its parts in-house to control costs, including custom IC chips from MOS Technology. It is sometimes compared to the Ford Model T automobile for its role in bringing a new technology to middle-class households via creative mass-production.
Approximately 10,000 commercial software titles were made for the Commodore 64 including development tools, office productivity applications, and games. C64 emulators allow anyone with a modern computer, or a compatible video game console, to run these programs today. The C64 is also credited with popularizing the computer demoscene and is still used today by some computer hobbyists. In 2008, 17 years after it was taken off the market, research showed that brand recognition for the model was still at 87%.
History.
In January 1981, MOS Technology, Inc., Commodore's integrated circuit design subsidiary, initiated a project to design the graphic and audio chips for a next generation video game console. Design work for the chips, named MOS Technology VIC-II (Video Integrated Circuit for graphics) and MOS Technology SID (Sound Interface Device for audio), was completed in November 1981.
Commodore then began a game console project that would use the new chips—called the "Ultimax" or alternatively the "Commodore MAX Machine", engineered by Yash Terakura from Commodore Japan. This project was eventually cancelled after just a few machines were manufactured for the Japanese market.
At the same time, Robert "Bob" Russell (system programmer and architect on the VIC-20) and Robert "Bob" Yannes (engineer of the SID) were critical of the current product line-up at Commodore, which was a continuation of the Commodore PET line aimed at business users. With the support of Al Charpentier (engineer of the VIC-II) and Charles Winterble (manager of MOS Technology), they proposed to Commodore CEO Jack Tramiel a true low-cost sequel to the VIC-20. Tramiel dictated that the machine should have of random-access memory (RAM). Although of dynamic random access memory (DRAM) cost over $100 at the time, he knew that DRAM prices were falling, and would drop to an acceptable level before full production was reached. In November, Tramiel set a deadline for the first weekend of January, to coincide with the 1982 Consumer Electronics Show (CES).
The product was code named the VIC-40 as the successor to the popular VIC-20. The team that constructed it consisted of Bob Russell, Bob Yannes and David A. Ziembicki. The design, prototypes and some sample software were finished in time for the show, after the team had worked tirelessly over both Thanksgiving and Christmas weekends.
The machine incorporated Commodore BASIC 2.0 in ROM. BASIC also served as the user interface shell and was available immediately on startup at the codice_1 prompt.
When the product was to be presented, the VIC-40 product was renamed C64 to fit the contemporary Commodore business products lineup which contained the "P128" and the "B256", both named by a letter and their respective total memory size (in KBytes).
The C64 made an impressive debut at the January 1982 Consumer Electronics Show, as recalled by Production Engineer David A. Ziembicki: "All we saw at our booth were Atari people with their mouths dropping open, saying, 'How can you do that for $595?'" The answer, as it turned out, was vertical integration; thanks to Commodore's ownership of MOS Technology's semiconductor fabrication facilities, each C64 had an estimated production cost of only $135.
Winning the market war.
The C64 faced a wide range of competing home computers at its introduction in August 1982. With a lower price and more flexible hardware, it quickly outsold many of its competitors. In the United States the greatest competitors were the Atari 8-bit 400 and 800, and the Apple II. The Atari 400 and 800 had been designed to accommodate previously stringent FCC emissions requirements and so were expensive to manufacture. The latest revision in the aging Apple II line, the Apple IIe, had higher-resolution graphics modes than the C64. Upgrade capability for the Apple II was granted by internal expansion slots, while the C64 had only a single external ROM cartridge port for bus expansion. However, the Apple used its expansion slots for interfacing to common peripherals like disk drives, printers and modems; the C64 had a variety of ports integrated into its motherboard which were used for these purposes, usually leaving the cartridge port free.
All four machines had similar standard memory configurations in the years 1982/83: 48K for the Apple II+ (upgraded within months of C64's release to 64K with the Apple IIe) and 48K for the Atari 800. At upwards of $1,200, the Apple II was about twice as expensive, while the Atari 800 cost $899. One key to the C64's success was Commodore's aggressive marketing tactics, and they were quick to exploit the relative price/performance divisions between its competitors with a series of television commercials after the C64's launch in late 1982. The company also published detailed documentation to help developers, while Atari initially kept technical information secret. At a mid-1984 conference of game developers and experts at Origins Game Fair, Dan Bunten, Sid Meier ("the computer of choice right now"), and a representative of Avalon Hill all stated that they were developing games for the 64 first as the most promising market. In April 1986 "Computer Gaming World" published a survey of ten game publishers which found that they planned to release forty-three Commodore 64 games that year, compared to nineteen for Atari and forty-eight for Apple II, and that year Alan Miller stated that Accolade developed first for the C64 because "it will sell the most on that system".
Commodore sold the C64 not only through its network of authorized dealers, but also through department stores, discount stores, toy stores and college bookstores. The C64 had a built-in RF modulator and thus could be plugged into a television set. This allowed it (like its predecessor, the VIC-20) to compete directly against video game consoles such as the Atari 2600. Like the Apple IIe, the C64 could also output baseband composite video and thus could be plugged into a specialized monitor for a sharper picture. Unlike the IIe, the C64's baseband NTSC output capability included separate luminance/chroma signal output equivalent to (and electrically compatible with) S-Video, for connection to the Commodore 1702 monitor.
Aggressive pricing of the C64 is considered to be a major catalyst in the North American video game crash of 1983. In January 1983, Commodore offered a $100 rebate in the United States on the purchase of a C64 to anyone trading in another video game console or computer. To take advantage of this rebate, some mail-order dealers and retailers offered a Timex Sinclair 1000 for as little as $10 with purchase of a C64, so the consumer could send the TS1000 to Commodore, collect the rebate, and pocket the difference; Timex Corporation departed the computer market within a year. Commodore's tactics soon led to a price war with the major home computer manufacturers. The success of the VIC-20 and C64 contributed significantly to the exit of Texas Instruments and other smaller competitors from the field. The price war with Texas Instruments was seen as a personal battle for Commodore president Jack Tramiel; TI's subsequent demise in the home computer industry in October 1983 was seen as revenge for TI's tactics in the electronic calculator market in the mid-1970s, when Commodore was almost bankrupted by TI. "Computer Gaming World" stated in January 1985 that companies such as Epyx that survived the video game crash did so because they "jumped on the Commodore bandwagon early".
In Europe, the primary competitors to the C64 were the British-built Sinclair ZX Spectrum, BBC Micro computer and the Amstrad CPC 464. In the UK, the Spectrum had been released a few months ahead of the C64, and was selling for less than half the price. The Spectrum quickly became the market leader and Commodore had an uphill struggle against the Spectrum. The C64 debuted at £399 in early 1983, while the 48K Spectrum cost £175. The C64 went on to rival the Spectrum in popularity in the latter half of the 1980s. Adjusted to the size of population the popularity of Commodore 64 was the highest in Finland where it was subsequently marketed as "the computer of the republic".
By mid-1986 Commodore had sold 3.5 million C64s, with about one million sold in 1985. Although the company reportedly attempted to discontinue the C64 more than once in favor of more expensive computers such as the 128, demand remained strong. That year Commodore introduced the 64c, a redesigned 64, which "Compute!" saw as evidence that—contrary to C64 owners' fears that the company would abandon them in favor of the Amiga and 128—"the 64 refuses to die". Its introduction also meant that Commodore raised the price of the C64 for the first time, which the magazine cited as the end of the home-computer price war. Software sales also remained strong; MicroProse, for example, in 1987 cited the Commodore and IBM PC markets as its top priorities.
By 1988, Commodore was still selling 1.5 million C64s worldwide, although Epyx CEO David Shannon Morse cautioned that "there are no new 64 buyers, or very few. It's a consistent group that's not growing ... it's going to shrink as part of our business". One computer-gaming executive stated that the Nintendo Entertainment System's enormous popularity—seven million sold that year, almost as many as the number of C64s sold in its first five years—had stopped the C64's growth, and Trip Hawkins stated that Nintendo was "the last hurrah of the 8-bit world". Although demand for the C64 dropped off in the United States by 1990, it continued to be popular in the UK and other European countries. In the end, economics, not obsolescence, sealed the C64's fate. In March 1994, at CeBIT in Hanover, Germany, Commodore announced that the C64 would be finally discontinued in 1995. Commodore stated that the C64's disk drive was more expensive to manufacture than the C64 itself. However, only one month later, in April 1994, the company filed for bankruptcy.
The C64 family.
1982: Commodore released the Commodore MAX Machine in Japan. It is called the Ultimax in the United States, and VC-10 in Germany. The MAX was intended to be a game console with limited computing capability, and was based on a very cut-down version of the hardware family later used in the C64. The MAX was discontinued months after its introduction, because of poor sales in Japan.
1983 saw Commodore attempt to compete with the Apple II's hold on the U.S. education market with the Educator 64, essentially a C64 and "greenscale" monochrome monitor in a PET case. Schools preferred the all-in-one metal construction of the PET over the standard C64's separate components, which could be easily damaged, vandalized or stolen. Schools did not prefer the Educator 64 to the wide range of software and hardware options the Apple IIe was able to offer, and it was produced in limited quantities.
In 1984, Commodore released the SX-64, a portable version of the C64. The SX-64 has the distinction of being the first "full-color" portable computer. While earlier computers using this form factor only incorporated monochrome "green screen" displays, the base SX-64 unit featured a color cathode ray tube (CRT) and an integrated 1541 floppy disk drive. The SX-64 did not have a cassette connector.
Also in 1984, Commodore released the Commodore Plus/4. It had a higher-color display, a newer implementation of Commodore BASIC (V3.5), and built-in software in what was positioned as an inexpensive business oriented system. However, it was incompatible with the C64, and the burgeoning influence of the IBM PC on the personal computer market market rendered the limited business software of the Plus/4 system of marginal value. The Plus/4 lacked hardware sprite capability and lacked a SID chip, thus under-performing in two of the areas that had made the C64 successful.
Two designers at Commodore, Fred Bowen and Bil Herd, were determined to rectify the problems of the Plus/4. They intended that the eventual successors to the C64—the Commodore 128 and 128D computers (1985)—were to build upon the C64, avoiding the Plus/4's flaws. The successors had many improvements (such as a structured BASIC with graphics and sound commands, 80-column display ability, and full CP/M compatibility). The decision to make the Commodore 128 plug compatible with the C64 was made quietly by Bowen and Herd, software and hardware designers respectively, without the knowledge or approval by the management in the post Jack Tramiel era. The designers were careful not to reveal their decision until the project was too far along to be challenged or changed and still make the impending Consumer Electronics Show (CES) show in Las Vegas. Upon learning that the C128 was designed to be compatible with the C64, Commodore's marketing department independently announced that the C128 would be 100% compatible with the C64, thereby raising the bar for C64 support. In a case of malicious compliance, the 128 design was altered to include a separate "64 mode" using a complete C64 environment to ensure total compatibility.
In 1986, Commodore released the Commodore 64c computer, which was functionally identical to the original. The exterior design was remodeled in the sleeker style of the Commodore 128. The modifications to the C64 line were more than skin deep in the 64c with new versions of the SID, VIC and I/O chips being deployed—with the core voltage reduced from 12V to 9V. In the United States, the 64c was often bundled with the third-party GEOS graphical user interface (GUI) based operating system. The Commodore 1541 disk drive received a matching face-lift resulting in the 1541c. Later a smaller, sleeker 1541-II model was introduced along with the 3.5-inch microfloppy 1581.
In 1990, the C64 was rereleased in the form of a game console, called the C64 Games System (C64GS). A simple modification to the C64C's motherboard was made to orient the cartridge connector to a vertical position. This allowed cartridges to be inserted from above. A modified ROM replaced the BASIC interpreter with a boot screen to inform the user to insert a cartridge. Designed to compete with the Nintendo Entertainment System and the Sega Master System, it suffered from very low sales compared to its rivals. It was another commercial failure for Commodore, and it was never released outside of Europe.
In 1990, an advanced successor to the C64, the Commodore 65 (also known as the "C64DX"), was prototyped, but the project was canceled by Commodore's chairman Irving Gould in 1991. The C65's specifications were very good for an 8-bit computer, bringing specs comparable to the Apple IIgs. For example, it could display 256 colors on screen, while OCS based Amigas could only display 64 in HalfBrite mode (32 colors and half-bright transformations). Although no specific reason was given for the C65's cancellation, it would have competed in the marketplace with Commodore's lower end Amigas and the Commodore CDTV.
C64 clones.
In the middle of 2004, after an absence from the marketplace of more than 10 years, PC manufacturer Tulip Computers BV (owners of the Commodore brand since 1997) announced the C64 Direct-to-TV (C64DTV), a joystick-based TV game based on the C64 with 30 games built into ROM. Designed by Jeri Ellsworth, a self-taught computer designer who had earlier designed the modern C-One C64 implementation, the C64DTV was similar in concept to other mini-consoles based on the Atari 2600 and Intellivision which had gained modest success earlier in the decade. The product was advertised on QVC in the United States for the 2004 holiday season. Some users have installed 1541 floppy disk drives, hard drives, second joysticks and keyboards to these units, which give the DTV devices nearly all of the capabilities of a full Commodore 64. The DTV hardware is also used in the mini-console/game "Hummer", sold at RadioShack mid-2005.
C64 enthusiasts still develop new hardware, including Ethernet cards, specially adapted hard disks and flash card interfaces (sd2iec).
Brand re-use.
In 1998, the C64 brand was reused for the "Web.it Internet Computer", a low-powered (even for the time) Internet-oriented, all-in-one x86 PC running Windows 3.1. Despite its "Commodore 64" nameplate, the "C64 Web.it" was not directly compatible with the original (except via included emulation software), nor did it share its appearance.
PC clones branded as C64x sold by Commodore USA, LLC, a company licensing the Commodore trademark, began shipping in June 2011. The C64x has a case resembling the original C64 computer, but- as with the "Web.it"- it is based on x86 architecture and is not compatible with the Commodore 64 on either hardware or software level.
Virtual Console.
Several Commodore 64 games were released on the Nintendo Wii's Virtual Console service in Europe and North America only. The games were removed from the service as of August 2013 for unknown reasons.
Software.
In 1982, the C64's graphics and sound capabilities were rivaled only by the Atari 8-bit family, and appeared exceptional when compared with the widely publicised Atari VCS and Apple II.
The C64 is often credited with starting the computer subculture known as the demoscene (see Commodore 64 demos). It is still being actively used in the demoscene, especially for music (its sound chip even being used in special sound cards for PCs, and the Elektron SidStation synthesizer). Unfortunately, the differences between PAL and NTSC C64s caused compatibility problems between U.S./Canadian C64s and those from most other countries. The vast majority of demos run only on PAL machines.
Even though other computers quickly caught up with it, the C64 remained a strong competitor to the later video game consoles Nintendo Entertainment System (NES) and Sega Master System, thanks in part to its by-then established software base, especially outside of North America, where it comprehensively outsold the NES.
BASIC.
As was common for home computers of the early 1980s, the C64 incorporated a ROM-based version of the BASIC programming language. There was no operating system as such. The KERNAL was accessed via BASIC commands. The disk drive had its own microprocessor, much like the Atari 800. This meant that no memory space had to be dedicated to running a disk operating system, as remained the case with earlier systems such as the Apple II.
Commodore BASIC 2.0 was used instead of the more advanced BASIC 4.0 from the PET series, since 64 users were not expected to need the disk-oriented enhancements of BASIC 4.0. The company did not expect many to buy a disk drive, and using BASIC 2.0 simplified VIC-20 owners' transition to the 64. "The choice of BASIC 2.0 instead of 4.0 was made with some soul-searching, not just at random. The typical user of a C64 is not expected to need the direct disk commands as much as other extensions and the amount of memory to be committed to BASIC were to be limited. We chose to leave expansion space for color and sound extensions instead of the disk features. As a result, you will have to handle the disk in the more cumbersome manner of the 'old days'."
The version of BASIC was limited and did not include specific commands for sound or graphics manipulation, instead required users to use the "POKE" commands to access the graphics and sound chip registers directly. To provide extended commands, including graphics and sound, Commodore produced two different cartridge-based extension to BASIC 2.0 — Simons' BASIC and Super Expander 64.
Other languages available for the C64 included Pascal, Logo, Forth, and FORTRAN. Compiled versions of BASIC such as Petspeed 2 (from Commodore) and Turbo Lightning (Ocean Software) were also available. While the first generation of C64 software may have used one of these or even the standard BASIC, after 1983 almost all professionally produced programs were written in assembly language, using a machine code monitor or an assembler. This maximised speed and minimised memory use.
Alternative operating systems.
Many third party operating systems have been developed for the C64. As well as the original GEOS, two third-party GEOS-compatible systems have been written: Wheels and GEOS megapatch. Both of these require hardware upgrades to the original C64. Several other operating systems are or have been available, including WiNGS OS, the Unix-like LUnix, operated from a command-line, and the embedded systems OS Contiki, with full GUI. Other less well known OSes include ACE, Asterix, DOS/65 and GeckOS.
A version of CP/M was released, but this required the addition of an external Z80 processor to the expansion bus, so is not considered a true C64 OS. Furthermore, the Z80 processor was underclocked to be compatible with the C64's memory bus, so performance was poor compared to other CP/M implementations. C64 CP/M and C128 CP/M both suffered a lack of software: although most commercial CP/M software could run on these systems, software media was incompatible between platforms. The low usage of CP/M on Commodores meant that software houses saw no need to invest in mastering versions for the Commodore disk format.
Networking software.
During the 1980s, the Commodore 64 was used to run many bulletin board systems using software packages such as Bizarre 64, Blue Board, C-Net, Color 64, CMBBS, C-Base, DMBBS, Image BBS, and The Deadlock Deluxe BBS Construction Kit, often with sysop-made modifications. These boards sometimes were used to distribute cracked software. As late as December 2013, there were 25 such Bulletin Board Systems in operation, reachable via the Telnet protocol.. In an attempt to address the need for a citation, a list of Commodore BBS systems currently in operation, along with others that are known to be offline is maintained by hobbyists and can be seen at http://cbbsoutpost.servebbs.com 
There were also major commercial online services, such as Compunet (UK), CompuServe (US – later bought by America Online), The Source (US) and Minitel (France) among many others. These services usually required custom software which was often bundled with a modem and included free online time as they were billed by the minute.
Quantum Link (or Q-Link) was a U.S. and Canadian online service for Commodore 64 and 128 personal computers that operated from November 5, 1985, to November 1, 1994. It was operated by Quantum Computer Services of Vienna, Virginia, which in October 1991 changed its name to America Online, and continues to operate its AOL service for the IBM PC compatible and Apple Macintosh today. Q-Link was a modified version of the PlayNET system, which Control Video Corporation (CVC, later renamed Quantum Computer Services) licensed.
Online gaming.
The first graphical character-based interactive environment was "Club Caribe". First released as "Habitat" in 1988, "Club Caribe" was introduced by LucasArts for Q-Link customers on their Commodore 64 computers. Users could interact with one another, chat and exchange items. Although the game's open world was very basic, its use of online avatars (already well-established off-line by "Ultima" and other games) and combination of chat and graphics was revolutionary. Online graphics in the late 1980s were severely restricted by the need to support modem data transfer rates as slow as 300 bits per second (bit/s). Habitat's graphics were stored locally on floppy disk, eliminating the need for network transfer. Along with these games some new gaming engines were created in the process. The game "Maniac Mansion" made by LucasArts needed its own unique engine named SCUMM to be played. The engine is somewhat of a mix of a programming language and a gaming engine. LucasArts eventually used the gaming engine for other games they released.
Hardware.
CPU and memory.
The C64 uses an 8-bit MOS Technology 6510 microprocessor. This is a close derivative of the 6502 with an added 6-bit internal I/O port that in the C64 is used for two purposes: to bank-switch the machine's read-only memory (ROM) in and out of the processor's address space, and to operate the datasette tape recorder.
The C64 has of RAM, of which are available to built-in Commodore BASIC 2.0 on startup.
There is of ROM, made up of the BASIC interpreter, the kernel, and the character ROM. As the processor could only address at a time, the ROM was mapped into memory and only of RAM were available at startup.
If a program did not use the BASIC interpreter, RAM could be mapped over the ROM locations. However, this meant the character ROM would not be available, and the RAM in its place was instead used for the character glyphs. Normally, this RAM was uninitialised, which would then result in nothing but random patterns appearing on the screen. This was solved by copying the character ROM into RAM. This had two benefits – the standard typeface could be rewritten, and character codes could be rewritten as picture elements.
Most C64 games were written in this way, using low resolution, which required much less processor time and saved memory. Furthermore, picture elements could be reused, saving even more precious memory. The same technique was used on the NES.
Graphics.
The graphics chip, VIC-II, features 16 colors, eight hardware sprites per scanline (enabling up to 112 sprites per PAL screen), scrolling capabilities, and two bitmap graphics modes. The standard text mode features 40 columns, like most Commodore PET models; the built in character encoding is not standard ASCII but PETSCII, an extended form of ASCII-1963.
Most screenshots show borders around the screen, which is a feature of the VIC-II chip. By utilising interrupts to reset various hardware registers on precise timings it was possible to place graphics within the borders and thus utilise the full screen.
There were two low-resolution and two bitmapped modes. Multicolor bitmapped mode had an addressable screen of 160 × 200 pixels, with a maximum of four colors per 4 × 8 character block. High-resolution bitmapped mode had an addressable screen of 320 × 200 pixels, with a maximum of two colors per 8 × 8 character block.
Multicolor low-resolution had a screen of 160 × 200 pixels, 40 × 25 addressable with four colors per 8 × 8 character block; high resolution "low resolution" had a screen of 320 × 200 pixels, 40 × 25 addressable with two colors per 8 × 8 character block. Most video games were multicolor low-resolution; this allowed only block-by-block character animation due to the limited addressable space. However, further innovation allowed video chips to automate sprites and vertical and horizontal scrolling pixel-by-pixel, allowing graphics to work smoothly and quickly regardless of the video mode. Some animation, like bullets, used character animation when sprites were unavailable.
Sound.
The SID chip has three channels, each with its own ADSR envelope generator, ring modulation and filter capabilities. Bob Yannes developed the SID chip and later co-founded synthesizer company Ensoniq. Yannes criticized other contemporary computer sound chips as "primitive, obviously...designed by people who knew nothing about music". Often the game music became a hit of its own among C64 users. Well-known composers and programmers of game music on the C64 are Rob Hubbard, David Whittaker, Chris Hülsbeck, Ben Daglish, Martin Galway and David Dunn among many others. Due to the chip's three channels, chords are played as arpeggios, coining the C64's characteristic lively sound. It was also possible to continuously update the master volume with sampled data to enable the playback of 4-bit digitized audio. As of 2008, it became possible to play four channel 8-bit audio samples, 2 SID channels and still use filtering.
There are two versions of the SID chip, the 6581 and the 8580. The MOS Technology 6581 was used in the original "breadbox" C64s, the early versions of the C64C and the Commodore 128. The 6581 was replaced with the MOS Technology 8580 in 1987. The 6581 sound quality is a little crisper, and many Commodore 64 fans prefer its sound. The main difference between the 6581 and the 8580 is the supply voltage. The 6581 uses a supply—the 8580, a supply. A modification can be made to use the 6581 in a C64C board (which uses the chip).
The SID chip has a distinctive sound which has retained a following of devotees to such a degree, that a number of audio enthusiasts and companies have designed SID-based products as add-ons for the C64, x86 PCs, and standalone or MIDI music devices such as the Elektron SidStation. These devices use chips taken from excess stock, or removed from used computers.
In 2007, Timbaland's extensive use of the SidStation led to the plagiarism controversy for "Block Party" and "Do It" (written for Nelly Furtado).
Hardware revisions.
Cost reduction was the driving force behind the C64's motherboard revisions. Reducing manufacturing costs was vitally important to Commodore's survival during the price war and leaner years of the 16-bit era. The C64's original (NMOS based) motherboard would go through two major redesigns, (and numerous sub-revisions) exchanging positions of the VIC-II, SID and PLA chips. Initially, a large portion of the cost was eliminated by reducing the number of discrete components, such as diodes and resistors, which enabled the use of a smaller printed circuit board.
The case is made from ABS plastic which may become brown with time. This can be reversed by using the public domain chemical mix "Retr0bright".
ICs.
The VIC-II was manufactured with 5 micrometer NMOS technology and was clocked at either (PAL) or (NTSC). Internally, the clock was divided down to generate the dot clock (about 8 MHz) and the two-phase system clocks (about 1 MHz; the exact pixel and system clock speeds are slightly different between NTSC and PAL machines). At such high clock rates, the chip generated a lot of heat, forcing MOS Technology to use a ceramic dual in-line package called a "CERDIP". The ceramic package was more expensive, but it dissipated heat more effectively than plastic.
After a redesign in 1983, the VIC-II was encased in a plastic dual in-line package, which reduced costs substantially, but it did not totally eliminate the heat problem. Without a ceramic package, the VIC-II required the use of a heat sink. To avoid extra cost, the metal RF shielding doubled as the heat sink for the VIC, although not all units shipped with this type of shielding. Most C64s in Europe shipped with a cardboard RF shield, coated with a layer of metal foil. The effectiveness of the cardboard was highly questionable, and worse still it acted as an insulator, blocking airflow which trapped heat generated by the SID, VIC, and PLA chips.
The SID was manufactured using NMOS at 7 and in some areas 6 micrometers. The prototype SID and some very early production models featured a ceramic dual in-line package, but unlike the VIC-II, these are extremely rare as the SID was encased in plastic when production started in early 1982.
Motherboard.
In 1986, Commodore released the last revision to the classic C64 motherboard. It was otherwise identical to the 1984 design, except for the two 64 kilobit × 4 bit DRAM chips that replaced the original eight 64 kilobit × 1 bit ICs.
After the release of the C64C, MOS Technology began to reconfigure the C64's chipset to use HMOS production technology. The main benefit of using HMOS was that it required less voltage to drive the IC, which consequently generates less heat. This enhanced the overall reliability of the SID and VIC-II. The new chipset was renumbered to 85xx to reflect the change to HMOS.
In 1987, Commodore released C64Cs with a highly redesigned motherboard commonly known as a "short board". The new board used the new HMOS chipset, featuring a new 64-pin PLA chip. The new "SuperPLA", as it was dubbed, integrated many discrete components and transistor–transistor logic (TTL) chips. In the last revision of the C64C motherboard, the 2114 color RAM was integrated into the SuperPLA.
Power supply.
The C64 used an external power supply, a conventional transformer with multiple tappings (as opposed to switch mode, the type now used on PC power supplies), encased in an epoxy resin gel which discouraged tampering but tended to increase the heat level during use.
This saved space within the computer's case and allowed international versions to be more easily manufactured. The 1541-II and 1581 disk drives, along with various third-party clones, also came with their own external power supply "bricks", as did most peripherals leading to a "spaghetti" of cables and the use of numerous double adapters by users. These power supplies were notorious for failing over time, usually because of overheating.
Commodore later changed the design, omitting the gel. The follow-on model, the Commodore 128, used a larger, improved power supply that included a fuse.
Specifications.
I/O ports and power supply.
The is used to supply power via a charge pump to the SID sound generator chip, provide via a rectifier to the cassette motor, a "0" pulse for every positive half wave to the time-of-day (TOD) input on the CIA chips, and directly to the user-port. Thus, as a minimum, a square wave is required. But a sine wave is preferred.
Memory map.
Note that even if I/O chips like VIC-II only uses 64 positions in the memory address space, it will occupy 1,024 addresses because some address bits are left undecoded.
Response.
"BYTE" in July 1983 stated that "the 64 retails for $595. At that price it promises to be one of the hottest contenders in the under-$1000 personal computer market". It described SID as "a true music synthesizer...the quality of the sound has to be heard to be believed", while criticizing the use of Commodore BASIC 2.0, the slow disk access ("even slower than the Atari 810 drive"), and Commodore's quality control.

</doc>
<doc id="7294" url="http://en.wikipedia.org/wiki?curid=7294" title="Cartography">
Cartography

Cartography (from Greek χάρτης "khartēs", "map"; and γράφειν "graphein", "write") is the study and practice of making maps. Combining science, aesthetics, and technique, cartography builds on the premise that reality can be modeled in ways that communicate spatial information effectively.
The fundamental problems of traditional cartography are to:
Modern cartography is largely integrated with geographic information science (GIScience) and constitutes many theoretical and practical foundations of geographic information systems.
History.
The earliest known map is a matter of some debate, both because the definition of "map" is not sharp and because some artifacts speculated to be maps might actually be something else. A wall painting, which may depict the ancient Anatolian city of Çatalhöyük (previously known as Catal Huyuk or Çatal Hüyük), has been dated to the late 7th millennium BCE. Other known maps of the ancient world include the Minoan "House of the Admiral" wall painting from c. 1600 BCE, showing a seaside community in an oblique perspective and an engraved map of the holy Babylonian city of Nippur, from the Kassite period (14th12th centuries BCE). The oldest surviving world maps are the Babylonian world maps from the 9th century BCE. One shows Babylon on the Euphrates, surrounded by a circular landmass showing Assyria, Urartu and several cities, in turn surrounded by a "bitter river" (Oceanus), with seven islands arranged around it. Another depicts Babylon as being further north from the center of the world.
The ancient Greeks and Romans created maps, beginning at latest with Anaximander in the 6th century BC. In the 2nd century AD, Ptolemy produced his treatise on cartography, Geographia. This contained Ptolemy's world map – the world then known to Western society "(Ecumene)". As early as the 8th century, Arab scholars were translating the works of the Greek geographers into Arabic.
In ancient China, geographical literature spans back to the 5th century BC. The oldest extant Chinese maps come from the State of Qin, dated back to the 4th century BC, during the Warring States period. In the book of the "Xin Yi Xiang Fa Yao", published in 1092 by the Chinese scientist Su Song, a star map on the equidistant cylindrical projection. Although this method of charting seems to have existed in China even prior to this publication and scientist, the greatest significance of the star maps by Su Song is that they represent the oldest existent star maps in printed form.
Early forms of cartography of India included the locations of the Pole star and other constellations of use. These charts may have been in use by the beginning of the Common Era for purposes of navigation.
Mappa mundi are the Medieval European maps of the world. Approximately 1,100 mappae mundi are known to have survived from the Middle Ages. Of these, some 900 are found illustrating manuscripts and the remainder exist as stand-alone documents.
The Arab geographer Muhammad al-Idrisi produced his medieval atlas "Tabula Rogeriana" in 1154. He incorporated the knowledge of Africa, the Indian Ocean and the Far East, gathered by Arab merchants and explorers with the information inherited from the classical geographers to create the most accurate map of the world up until his time. It remained the most accurate world map for the next three centuries.
In the Age of Exploration, from the 15th century to the 17th century, European cartographers both copied earlier maps (some of which had been passed down for centuries) and drew their own based on explorers' observations and new surveying techniques. The invention of the magnetic compass, telescope and sextant enabled increasing accuracy. In 1492, Martin Behaim, a German cartographer, made the oldest extant globe of the Earth.
Johannes Werner refined and promoted the Werner projection. In 1507, Martin Waldseemüller produced a globular world map and a large 12-panel world wall map ("Universalis Cosmographia") bearing the first use of the name "America". Portuguese cartographer Diego Ribero was the author of the first known planisphere with a graduated Equator (1527). Italian cartographer Battista Agnese produced at least 71 manuscript atlases of sea charts.
Due to the sheer physical difficulties inherent in cartography, map-makers frequently lifted material from earlier works without giving credit to the original cartographer. For example, one of the most famous early maps of North America is unofficially known as the "Beaver Map", published in 1715 by Herman Moll. This map is an exact reproduction of a 1698 work by Nicolas de Fer. De Fer in turn had copied images that were first printed in books by Louis Hennepin, published in 1697, and François Du Creux, in 1664. By the 18th century, map-makers started to give credit to the original engraver by printing the phrase "After [the original cartographer]" on the work.
Technological changes.
In cartography, technology has continually changed in order to meet the demands of new generations of mapmakers and map users. The first maps were manually constructed with brushes and parchment; therefore, varied in quality and were limited in distribution. The advent of magnetic devices, such as the compass and much later, magnetic storage devices, allowed for the creation of far more accurate maps and the ability to store and manipulate them digitally.
Advances in mechanical devices such as the printing press, quadrant and vernier, allowed for the mass production of maps and the ability to make accurate reproductions from more accurate data. Optical technology, such as the telescope, sextant and other devices that use telescopes, allowed for accurate surveying of land and the ability of mapmakers and navigators to find their latitude by measuring angles to the North Star at night or the sun at noon.
Advances in photochemical technology, such as the lithographic and photochemical processes, have allowed for the creation of maps that have fine details, do not distort in shape and resist moisture and wear. This also eliminated the need for engraving, which further shortened the time it takes to make and reproduce maps.
In the 20th century, Aerial photography, satellite imagery, and remote sensing provided efficient, precise methods for mapping physical features, such as coastlines, roads, buildings, watersheds, and topography. Advancements in electronic technology ushered in another revolution in cartography. Ready availability of computers and peripherals such as monitors, plotters, printers, scanners (remote and document) and analytic stereo plotters, along with computer programs for visualization, image processing, spatial analysis, and database management, democratized and greatly expanded the making of maps. The ability to superimpose spatially located variables onto existing maps created new uses for maps and new industries to explore and exploit these potentials. See also digital raster graphic.
These days most commercial-quality maps are made using software that falls into one of three main types: CAD, GIS and specialized illustration software. Spatial information can be stored in a database, from which it can be extracted on demand. These tools lead to increasingly dynamic, interactive maps that can be manipulated digitally.
With the field rugged computers, GPS and laser rangefinders, it is possible to perform mapping directly in the terrain.
Map types.
General vs. thematic cartography.
In understanding basic maps, the field of cartography can be divided into two general categories: general cartography and thematic cartography. General cartography involves those maps that are constructed for a general audience and thus contain a variety of features. General maps exhibit many reference and location systems and often are produced in a series. For example, the 1:24,000 scale topographic maps of the United States Geological Survey (USGS) are a standard as compared to the 1:50,000 scale Canadian maps. The government of the UK produces the classic 1:50,000 (replacing the older 1 inch to 1 mile) "Ordnance Survey" maps of the entire UK and with a range of correlated larger- and smaller-scale maps of great detail.
Thematic cartography involves maps of specific geographic themes, oriented toward specific audiences. A couple of examples might be a dot map showing corn production in Indiana or a shaded area map of Ohio counties, divided into numerical choropleth classes. As the volume of geographic data has exploded over the last century, thematic cartography has become increasingly useful and necessary to interpret spatial, cultural and social data.
An orienteering map combines both general and thematic cartography, designed for a very specific user community. The most prominent thematic element is shading, that indicates degrees of difficulty of travel due to vegetation. The vegetation itself is not identified, merely classified by the difficulty ("fight") that it presents.
Topographic vs. topological.
A topographic map is primarily concerned with the topographic description of a place, including (especially in the 20th and 21st centuries) the use of contour lines showing elevation. Terrain or relief can be shown in a variety of ways (see Cartographic relief depiction).
A topological map is a very general type of map, the kind you might sketch on a napkin. It often disregards scale and detail in the interest of clarity of communicating specific route or relational information. Beck's London Underground map is an iconic example. Though the most widely used map of "The Tube," it preserves little of reality: it varies scale constantly and abruptly, it straightens curved tracks, and it contorts directions. The only topography on it is the River Thames, letting the reader know whether a station is north or south of the river. That and the topology of station order and interchanges between train lines are all that is left of the geographic space. Yet those are all a typical passenger wishes to know, so the map fulfils its purpose.
Map design.
Map purpose and selection of information.
Arthur H. Robinson, an American cartographer influential in thematic cartography, stated that a map not properly designed "will be a cartographic failure." He also claimed, when considering all aspects of cartography, that "map design is perhaps the most complex." Robinson codified the mapmaker's understanding that a map must be designed foremost with consideration to the audience and its needs.
From the very beginning of mapmaking, maps "have been made for some particular purpose or set of purposes". The intent of the map should be illustrated in a manner in which the percipient acknowledges its purpose in a timely fashion. The term "percipient" refers to the person receiving information and was coined by Robinson. The principle of figure-ground refers to this notion of engaging the user by presenting a clear presentation, leaving no confusion concerning the purpose of the map. This will enhance the user's experience and keep his attention. If the user is unable to identify what is being demonstrated in a reasonable fashion, the map may be regarded as useless.
Making a meaningful map is the ultimate goal. Alan MacEachren explains that a well designed map "is convincing because it implies authenticity" (1994, pp. 9). An interesting map will no doubt engage a reader. Information richness or a map that is multivariate shows relationships within the map. Showing several variables allows comparison, which adds to the meaningfulness of the map. This also generates hypothesis and stimulates ideas and perhaps further research. In order to convey the message of the map, the creator must design it in a manner which will aid the reader in the overall understanding of its purpose. The title of a map may provide the "needed link" necessary for communicating that message, but the overall design of the map fosters the manner in which the reader interprets it (Monmonier, 1993, pp. 93).
In the 21st century it is possible to find a map of virtually anything from the inner workings of the human body to the virtual worlds of cyberspace. Therefore there are now a huge variety of different styles and types of map – for example, one area which has evolved a specific and recognisable variation are those used by public transport organisations to guide passengers, namely urban rail and metro maps, many of which are loosely based on 45 degree angles as originally perfected by Harry Beck and George Dow.
Naming conventions.
Most maps use text to label places and for such things as the map title, legend and other information. Although maps are often made in one specific language, place names often differ between languages. So a map made in English may use the name "Germany" for that country, while a German map would use "Deutschland" and a French map "Allemagne". A non-native term for a place is referred to as an exonym.
In some cases the correct name is not clear. For example, the nation of Burma officially changed its name to Myanmar, but many nations do not recognize the ruling junta and continue to use "Burma". Sometimes an official name change is resisted in other languages and the older name may remain in common use. Examples include the use of "Saigon" for Ho Chi Minh City, "Bangkok" for Krung Thep and "Ivory Coast" for Côte d'Ivoire.
Difficulties arise when transliteration or transcription between writing systems is required. Some well-known places have well-established names in other languages and writing systems, such as "Russia" or "Rußland" for Росси́я, but in other cases a system of transliteration or transcription is required. Even in the former case, the exclusive use of an exonym may be unhelpful for the map user. It will not be much use for an English user of a map of Italy to show Livorno "only" as "Leghorn" when road signs and railway timetables show it as "Livorno". In transliteration, the characters in one script are represented by characters in another. For example, the Cyrillic letter "Р" is usually written as "R" in the Latin script, although in many cases it is not as simple as a one-for-one equivalence. Systems exist for transliteration of Arabic, but the results may vary. For example, the Yemeni city of Mocha is written variously in English as Mocha, Al Mukha, al-Mukhā, Mocca and Moka. Transliteration systems are based on relating written symbols to one another, while transcription is the attempt to spell in one language the phonetic sounds of another. Chinese writing is now usually converted to the Latin alphabet through the Pinyin phonetic transcription systems. Other systems were used in the past, such as Wade-Giles, resulting in the city being spelled "Beijing" on newer English maps and "Peking" on older ones.
Further difficulties arise when countries, especially former colonies, do not have a strong national geographic naming standard. In such cases, cartographers may have to choose between various phonetic spellings of local names versus older imposed, sometimes resented, colonial names. Some countries have multiple official languages, resulting in multiple official placenames. For example, the capital of Belgium is both "Brussel" and "Bruxelles". In Canada, English and French are official languages and places have names in both languages. British Columbia is also officially named "la Colombie-Britannique". English maps rarely show the French names outside of Quebec, which itself is spelled "Québec" in French.
The study of placenames is called toponymy, while that of the origin and historical usage of placenames as words is etymology.
In order to improve legibility or to aid the illiterate, some maps have been produced using pictograms to represent places. The iconic example of this practice is Lance Wyman's early plans for the Mexico City Metro, on which stations were shown simply as stylized logos. Wyman also prototyped such a map for the Washington Metro, though ultimately the idea was rejected. Other cities experimenting with such maps are Fukuoka, Guadalajara and Monterrey.
Map symbology.
The quality of a map's design affects its reader's ability to extract information and to learn from the map. Cartographic symbology has been developed in an effort to portray the world accurately and effectively convey information to the map reader. A legend explains the pictorial language of the map, known as its symbology. The title indicates the region the map portrays; the map image portrays the region and so on. Although every map element serves some purpose, convention only dictates inclusion of some elements, while others are considered optional. A menu of map elements includes the neatline (border), compass rose or north arrow, overview map, bar scale, map projection and information about the map sources, accuracy and publication.
When examining a landscape, scale can be intuited from trees, houses and cars. Not so with a map. Even such a simple thing as a north arrow is crucial. It may seem obvious that the top of a map should point north, but this might not be the case.
Map coloring is also very important. How the cartographer displays the data in different hues can greatly affect the understanding or feel of the map. Different intensities of hue portray different objectives the cartographer is attempting to get across to the audience. Today, personal computers can display up to 16 million distinct colors at a time. This fact allows for a multitude of color options for even for the most demanding maps. Moreover, computers can easily hatch patterns in colors to give even more options. This is very beneficial, when symbolizing data in categories such as quintile and equal interval classifications.
Quantitative symbols give a visual measure of the relative size/importance/number that a symbol represents and to symbolize this data on a map, there are two major classes of symbols used for portraying quantitative properties. Proportional symbols change their visual weight according to a quantitative property. These are appropriate for extensive statistics. Choropleth maps portray data collection areas, such as counties or census tracts, with color. Using color this way, the darkness and intensity (or value) of the color is evaluated by the eye as a measure of intensity or concentration.
Map generalization.
A good map has to compromise between portraying the items of interest (or themes) in the right place on the map, and the need to show that item using text or a symbol, which take up space on the map and might displace some other item of information. The cartographer is thus constantly making judgements about what to include, what to leave out and what to show in a "slightly" incorrect place. This issue assumes more importance as the scale of the map gets smaller (i.e. the map shows a larger area) because the information shown on the map takes up more space "on the ground". A good example from the late 1980s was the Ordnance Survey's first digital maps, where the "absolute" positions of major roads were sometimes a scale distance of hundreds of metres away from ground truth, when shown on digital maps at scales of 1:250,000 and 1:625,000, because of the overriding need to annotate the features.
Map projections.
The Earth being spherical, any flat representation generates distortions such that shapes and areas cannot both be conserved simultaneously, and distances can never all be preserved. The mapmaker must choose a suitable map projection according to the space to be mapped and the purpose of the map.
Cartographic errors.
Some maps contain deliberate errors or distortions, either as propaganda or as a "watermark" to help the copyright owner identify infringement if the error appears in competitors' maps. The latter often come in the form of nonexistent, misnamed, or misspelled "trap streets". Other names and forms for this are paper townsites, fictitious entries, and copyright easter eggs.
Another motive for deliberate errors is cartographic "vandalism": a mapmaker wishing to leave his or her mark on the work. Mount Richard, for example, was a fictitious peak on the Rocky Mountains' continental divide that appeared on a Boulder County, Colorado map in the early 1970s. It is believed to be the work of draftsman Richard Ciacci. The fiction was not discovered until two years later.
Sandy Island (New Caledonia) is an example of a fictitious location that stubbornly survives, reappearing on new maps copied from older maps while being deleted from other new editions. The time and reason for its original placement on maps is unknown.

</doc>
<doc id="7295" url="http://en.wikipedia.org/wiki?curid=7295" title="Consumption">
Consumption

Consumption may refer to:

</doc>
<doc id="7296" url="http://en.wikipedia.org/wiki?curid=7296" title="Cardiac glycoside">
Cardiac glycoside

Cardiac glycosides are organic compounds containing a glycoside (sugar) that act on the contractile force of the cardiac muscle. Because of their potency in disrupting the function of the heart, most are extremely toxic. These glycosides are found as secondary metabolites in several plants, but also in some insects, such as the milkweed butterflies. 
Uses.
From ancient times, humans have used cardiac-glycoside-containing plants and their crude extracts as arrow, ordeal, homicidal, suicidal and rat poisons, heart tonics, diuretics and emetics. In modern times, purified extracts or synthetic analogues of a few have been adapted for the treatment of congestive heart failure and cardiac arrhythmia.
Therapeutic uses of cardiac glycosides primarily involve the treatment of cardiac failure. Their utility results from an increased cardiac output by increasing the force of contraction. By increasing intracellular calcium as described below, cardiac glycosides increase calcium-induced calcium release and thus contraction.
Bufalin, ouabain and digoxin are a few toxic cardiac glycosides. Digoxin from the foxglove plant is used clinically, whereas bufalin and ouabain are used only experimentally due to their extremely high potency.
Pharmacology.
Normally, sodium-potassium pumps in the membrane of cells (in this case, cardiac myocytes) pump potassium ions in and sodium ions out. Cardiac glycosides inhibit this pump by stabilizing it in the E2-P transition state, so that sodium cannot be extruded: intracellular sodium concentration therefore increases. A second membrane ion exchanger, NCX, is responsible for 'pumping' calcium ions out of the cell and sodium ions in (3Na/Ca); raised intracellular sodium levels inhibit this pump, so calcium ions are also not extruded and will begin to build up inside the cell, as well.
Increased cytoplasmic calcium concentrations cause increased calcium uptake into the sarcoplasmic reticulum via the SERCA2 transporter. Raised calcium stores in the SR allow for greater calcium release on stimulation, so the myocyte can achieve faster and more powerful contraction by cross-bridge cycling. The refractory period of the AV node is increased, so cardiac glycosides also function to regulate heart rate.
Binding of cardiac glycoside to Na-K ATPase is slow, and also, after binding, intracellular calcium increases gradually. Thus, the action of digitalis (even on IV injection) is delayed.
Raised extracellular potassium decreases binding of cardiac glycoside to Na-K ATPase. As a consequence, increased toxicity of these drugs is observed in the presence of Hypokalemia.
If SR calcium stores become too high, some ions are released spontaneously through SR ryanodine receptors. This effect leads initially to bigeminy: regular ectopic beats following each ventricular contraction. If higher glycoside doses are given, rhythm is lost and ventricular tachycardia ensues, followed by fibrillation.
Examples.
Examples of plants producing cardiac glycosides:
Examples of animals producing cardiac glycosides:

</doc>
<doc id="7299" url="http://en.wikipedia.org/wiki?curid=7299" title="Colonialism">
Colonialism

Colonialism is the establishment, exploitation, maintenance, acquisition, and expansion of colonies in one territory by people from another territory. It is a set of unequal relationships between the colonial power and the colony and often between the colonists and the indigenous population.
The "European colonial period" was the era from the 16th century to the mid-20th century when several European powers (particularly, but not exclusively, Portugal, Spain, Britain, the Netherlands, Russia, and France) established colonies in Asia, Africa, and the Americas. At first the countries followed mercantilist policies designed to strengthen the home economy at the expense of rivals, so the colonies were usually allowed to trade only with the mother country. By the mid-19th century, however, the powerful British Empire gave up mercantilism and trade restrictions and introduced the principle of free trade, with few restrictions or tariffs.
Colonialism was always portrayed in the colonizing country (in public) as bringing benefits for the colony. They included: increased standard of living, benefits of Christianity, improved health and education, establishing law and order, etc. The sincerity with which and the extent to which these benefits were provided are often at the very least questionable. Also, many now-independent colonies have not yet recovered from the psychological trauma of colonialism.
Definitions.
"Collins English Dictionary" defines colonialism as "the policy and practice of a power in extending control over weaker people or areas." The "Merriam-Webster Dictionary" offers four definitions, including "something characteristic of a colony" and "control by one power over a dependent area or people."
The 2006 "Stanford Encyclopedia of Philosophy" "uses the term 'colonialism' to describe the process of European settlement and political control over the rest of the world, including Americas, Australia, and parts of Africa and Asia." It discusses the distinction between colonialism and imperialism and states that "given the difficulty of consistently distinguishing between the two terms, this entry will use colonialism as a broad concept that refers to the project of European political domination from the sixteenth to the twentieth centuries that ended with the national liberation movements of the 1960s."
In his preface to Jürgen Osterhammel's "Colonialism: A Theoretical Overview", Roger Tignor says, "For Osterhammel, the essence of colonialism is the existence of colonies, which are by definition governed differently from other territories such as protectorates or informal spheres of influence." In the book, Osterhammel asks, "How can 'colonialism' be defined independently from 'colony?'" He settles on a three-sentence definition:
Types of colonialism.
Historians often distinguish between two overlapping forms of colonialism:
Plantation colonies would be considered exploitation colonialism; but colonizing powers would utilize either type for different territories depending on various social and economic factors as well as climate and geographic conditions.
Surrogate colonialism involves a settlement project supported by colonial power, in which most of the settlers do not come from the mainstream of the ruling power.
Internal colonialism is a notion of uneven structural power between areas of a nation state. The source of exploitation comes from within the state.
Socio-cultural evolution.
As colonialism often played out in pre-populated areas, sociocultural evolution included the formation of various ethnically hybrid populations. Colonialism gave rise to culturally and ethnically mixed populations such as the mestizos of the Americas, as well as racially-divided populations such as those found in French Algeria or in Southern Rhodesia. In fact, everywhere where colonial powers established a consistent and continued presence, hybrid communities existed.
Notable examples in Asia include the Anglo-Burmese, Anglo-Indian, Burgher, Eurasian Singaporean, Filipino mestizo, Kristang and Macanese peoples. In the Dutch East Indies (later Indonesia) the vast majority of "Dutch" settlers were in fact Eurasians known as Indo-Europeans, formally belonging to the European legal class in the colony (see also Indos in Pre-Colonial History and Indos in Colonial History).
History.
Activity that could be called colonialism has a long history, starting with the pre-colonial African empires which led to the Egyptians, Phoenicians, Greeks and Romans who all built colonies in antiquity. The word "metropole" comes from the Greek "metropolis" [Greek: "μητρόπολις"]—"mother city". The word "colony" comes from the Latin "colonia"—"a place for agriculture". Between the 11th and 18th centuries, the Vietnamese established military colonies south of their original territory and absorbed the territory, in a process known as nam tiến.
Modern colonialism started with the Age of Discovery. Portugal and Spain discovered new lands across the oceans and built trading posts or conquered large extensions of land. For some people, it is this building of colonies across oceans that differentiates colonialism from other types of expansionism. These new lands were divided between the Portuguese Empire and Spanish Empire, first by the papal bull Inter caetera and then by the Treaty of Tordesillas and the Treaty of Zaragoza (1529).
This period is also associated with the Commercial Revolution. The late Middle Ages saw reforms in accountancy and banking in Italy and the eastern Mediterranean. These ideas were adopted and adapted in western Europe to the high risks and rewards associated with colonial ventures.
The 17th century saw the creation of the French colonial empire and the Dutch Empire, as well as the English overseas possessions, which later became the British Empire. It also saw the establishment of a Danish colonial empire and some Swedish overseas colonies.
The spread of colonial empires was reduced in the late 18th and early 19th centuries by the American Revolutionary War and the Latin American wars of independence. However, many new colonies were established after this time, including the German colonial empire and Belgian colonial empire. In the late 19th century, many European powers were involved in the Scramble for Africa.
The Russian Empire, Ottoman Empire and Austrian Empire existed at the same time as the above empires, but did not expand over oceans. Rather, these empires expanded through the more traditional route of conquest of neighbouring territories. There was, though, some Russian colonization of the Americas across the Bering Strait. The Empire of Japan modelled itself on European colonial empires. The United States of America gained overseas territories after the Spanish-American War for which the term "American Empire" was coined.
After the First World War, the victorious allies divided up the German colonial empire and much of the Ottoman Empire between themselves as League of Nations mandates. These territories were divided into three classes according to how quickly it was deemed that they would be ready for independence. 
The colonial system was the major cause of the Second World War. The war in the Pacific was caused by Japan's efforts to create a colonial empire that conflicted with the existing empires held by the British, French, Dutch and the United States. The war in Europe and North Africa was caused by Germany and Italy's efforts to create colonial empires that conflicted with the existing British, French and Russian colonial empires in these areas. 
After World War II, decolonization progressed rapidly. This was caused for a number of reasons. First, the Japanese victories in the Pacific War showed Indians, Chinese and other subject peoples that a non-European could defeat the white settlers. Second, many of these people acquired weapons and training in the war among the colonial powers. 
Dozens of independence movements and global political solidarity projects such as the Non-Aligned Movement were instrumental in the decolonization efforts of former colonies. These included significant wars of liberation fought in Malaysia, Vietnam, Algeria, and Rhodesia (now Zimbabwe). Eventually, the European powers - pressured by the United States -- resigned themselves to decolonization. 
In 1962 the United Nations set up a Special Committee on Decolonization, often called the Committee of 24, to encourage this process.
European empires in 1914.
The major European empires consisted of the following colonies at the start of World War I (former colonies of the Spanish Empire became independent before 1914 and are not listed; former colonies of other European empires that previously became independent, such as the former French colony Haiti, are not listed).
Numbers of European settlers in the colonies (1500–1914).
By 1914, Europeans had migrated to the colonies in the millions. Some intended to remain in the colonies as temporary settlers, mainly as military personnel or on business. Others went to the colonies as immigrants. British people were by far the most numerous population to migrate to the colonies: 2.5 million settled in Canada; 1.5 million in Australia; 750,000 in New Zealand; 450,000 in the Union of South Africa; and 200,000 in India. French citizens also migrated in large numbers, mainly to the colonies in the north African Maghreb region: 1.3 million settled in Algeria; 200,000 in Morocco; 100,000 in Tunisia; while only 20,000 migrated to French Indochina. Dutch and German colonies saw relatively scarce European migration, since Dutch and German colonial expansion focused upon commercial goals rather than settlement. Portugal sent 150,000 settlers to Angola, 80,000 to Mozambique, and 20,000 to Goa. During the Spanish Empire, approximately 550,000 Spanish settlers migrated to Latin America.
Neocolonialism.
The term neocolonialism has been used to refer to a variety of contexts since decolonization that took place after World War II. Generally it does not refer to a type of direct colonization, rather, colonialism by other means. Specifically, neocolonialism refers to the theory that former or existing economic relationships, such as the General Agreement on Tariffs and Trade and the Central American Free Trade Agreement, created by former colonial powers were or are used to maintain control of their former colonies and dependencies after the colonial independence movements of the post–World War II period.
Colonialism and the history of thought.
Universalism.
The conquest of vast territories brings multitudes of diverse cultures under the central control of the imperial authorities. From the time of Ancient Greece and Ancient Rome, this fact has been addressed by empires adopting the concept of universalism, and applying it to their imperial policies towards their subjects far from the imperial capitol. The capitol, the metropole, was the source of ostensibly enlightened policies imposed throughout the distant colonies.
The empire that grew from Greek conquest, particularly by Alexander the Great, spurred the spread of Greek language, religion, science and philosophy throughout the colonies. The Greeks considered their own culture superior to all others. They referred to people speaking foreign languages as barbarians, dismissing foreign languages as inferior mutterings that sounded to Greek ears like "bar-bar".
Romans found efficiency in imposing a universalist policy towards their colonies in many matters. Roman law was imposed on Roman citizens, as well as colonial subjects, throughout the empire. Latin spread as the common language of government and trade, the lingua franca, throughout the Empire. Romans also imposed peace between their diverse foreign subjects, which they described in beneficial terms as the Pax Romana. The use of universal regulation by the Romans marks the emergence of a European concept of universalism and internationalism. Tolerance of other cultures and beliefs has always been secondary to the aims of empires, however. The Roman Empire was tolerant of diverse cultures and religious practises, so long as these did not threaten Roman authority. Napoleon's foreign minister, Charles Maurice de Talleyrand, once remarked: "Empire is the art of putting men in their place".
Colonialism and geography.
Settlers acted as the link between the natives and the imperial hegemony, bridging the geographical, ideological and commercial gap between the colonisers and colonised. Advanced technology made possible the expansion of European states. With tools such as cartography, shipbuilding, navigation, mining and agricultural productivity colonisers had an upper hand. Their awareness of the Earth's surface and abundance of practical skills provided colonisers with a knowledge that, in turn, created power.
Painter and Jeffrey argue that geography as a discipline was not and is not an objective science, rather it is based on assumptions about the physical world. Whereas it may have given "The West" an advantage when it came to exploration, it also created zones of racial inferiority. Geographical beliefs such as environmental determinism, the view that some parts of the world are underdeveloped, legitimised colonialism and created notions of skewed evolution. These are now seen as elementary concepts. Political geographers maintain that colonial behavior was reinforced by the physical mapping of the world, visually separating "them" and "us". Geographers are primarily focused on the spaces of colonialism and imperialism, more specifically, the material and symbolic appropriation of space enabling colonialism.
Colonialism and imperialism.
A colony is a part of an empire and so colonialism is closely related to imperialism. Assumptions are that colonialism and imperialism are interchangeable, however Robert J. C. Young suggests that imperialism is the concept while colonialism is the practice. Colonialism is based on an imperial outlook, thereby creating a consequential relationship. Through an empire, colonialism is established and capitalism is expanded, on the other hand a capitalist economy naturally enforces an empire. In the next section Marxists make a case for this mutually reinforcing relationship.
Marxist view of colonialism.
Marxism views colonialism as a form of capitalism, enforcing exploitation and social change. Marx thought that working within the global capitalist system, colonialism is closely associated with uneven development. It is an "instrument of wholesale destruction, dependency and systematic exploitation producing distorted economies, socio-psychological disorientation, massive poverty and neocolonial dependency." Colonies are constructed into modes of production. The search for raw materials and the current search for new investment opportunities is a result of inter-capitalist rivalry for capital accumulation. Lenin regarded colonialism as the root cause of imperialism, as imperialism was distinguished by monopoly capitalism via colonialism and as Lyal S. Sunga explains: "Vladimir Lenin advocated forcefully the principle of self-determination of peoples in his "Theses on the Socialist Revolution and the Right of Nations to Self-Determination" as an integral plank in the programme of socialist internationalism" and he quotes Lenin who contended that "The right of nations to self-determination implies exclusively the right to independence in the political sense, the right to free political separation from the oppressor nation. Specifically, this demand for political democracy implies complete freedom to agitate for secession and for a referendum on secession by the seceding nation." Non Russian marxists within the RSFSR and later the USSR, like Sultan Galiev and Vasyl Shakhrai, meanwhile, between 1918 and 1923 and then after 1929, considered the Soviet Regime a renewed version of the Russian imperialism and colonialism.
In his critique of colonialism in Africa, the Guyanese historian and political activist Walter Rodney states:
"Colonial Africa fell within that part of the international capitalist economy from which surplus was drawn to feed the metropolitan sector. As seen earlier, exploitation of land and labour is essential for human social advance, but only on the assumption that the product is made available within the area where the exploitation takes place.
Liberalism, capitalism and colonialism.
Classical liberals generally opposed colonialism (as opposed to colonization) and imperialism, including Adam Smith, Frédéric Bastiat, Richard Cobden, John Bright, Henry Richard, Herbert Spencer, H. R. Fox Bourne, Edward Morel, Josephine Butler, W. J. Fox and William Ewart Gladstone.
Adam Smith wrote in "Wealth of Nations" that Britain should liberate all of its colonies and also noted that it would be economically beneficial for British people in the average, although the merchants having mercantilist privileges would lose out.
Scientific thought in colonialism, race and gender.
The act of colonizing spread and synthesized social and political western ideas of a gender and racial hierarchy to colonized areas, as well as elicited the further development of ideas about the gender dichotomy and racial divisions in European society during the colonial era. Popular political practices of the time were to support colonialism rule by legitimizing European male authority and female and non European inferiority through studies of Craniology, Comparative Anatomy, and Phrenology. Biologists, naturalists, anthropologists, and ethnologists of the 1800s were focused on the study of colonized indigenous women, as in the case of Georges Cuvier's study of Sarah Baartman. Such cases embraced a natural superiority and inferiority relationship between the races based on European naturalists' observations; they gave rise to the perception that African women's anatomy, and especially genitalia, resembled those of mandrills, baboons, and monkeys, thus differentiating colonized Africans from what were viewed as the features of the evolutionarily superior, and thus rightfully authoritarian, European woman.
In addition to what would now be viewed as pseudo-scientific studies of race which supported new racially hierarchical and evolutionary ideology of the time, new science based ideology about gender was also emerging in reaction to the colonial era of European history. Female inferiority across all cultures was emerging as an idea based in craniology that led scientists to argue human women's brain size, based on skull measurements, was minuscule and therefore less developed and less evolutionarily advanced compared to men. The influence that led to such studies was the establishment of comparative anatomy of humans that developed in response to European scientists' delving into the question of biological racial difference.
Thus Non Europeans and women faced invasive study by colonial powers in the interest of scientific ideology and theory that encouraged the political institution of colonialism. Such studies of race and gender coincided with the era of colonialism and the introduction of foreign cultures, appearances, and gender roles into the line of vision of European scholars.
Post-colonialism.
Post-colonialism (or post-colonial theory) can refer to a set of theories in philosophy and literature that grapple with the legacy of colonial rule. In this sense, postcolonial literature may be considered a branch of postmodern literature concerned with the political and cultural independence of peoples formerly subjugated in colonial empires. Many practitioners take Edward Saïd's book "Orientalism" (1978) as the theory's founding work (although French theorists such as Aimé Césaire and Frantz Fanon made similar claims decades before Said).
Saïd analysed the works of Balzac, Baudelaire and Lautréamont arguing that they helped to shape a societal fantasy of European racial superiority. Writers of post-colonial fiction interact with the traditional colonial discourse, but modify or subvert it; for instance by retelling a familiar story from the perspective of an oppressed minor character in the story. Gayatri Chakravorty Spivak's "Can the Subaltern Speak?" (1998) gave its name to Subaltern Studies.
In "A Critique of Postcolonial Reason" (1999), Spivak argued that major works of European metaphysics (such as those of Kant and Hegel) not only tend to exclude the subaltern from their discussions, but actively prevent non-Europeans from occupying positions as fully human subjects. Hegel's "Phenomenology of Spirit" (1807), famous for its explicit ethnocentrism, considers Western civilization as the most accomplished of all, while Kant also had some traces of racialism in his work.
Impact of colonialism and colonization.
The impacts of colonization are immense and pervasive. Various effects, both immediate and protracted, include the spread of virulent diseases, unequal social relations, exploitation, enslavement, medical advances, the creation of new institutions, abolitionism, improved infrastructure, and technological progress. Colonial practices also spur the spread of colonist languages, literature and cultural institutions, while endangering or obliterating those of native peoples. The native cultures of the colonized peoples can also have a powerful influence on the imperial country.
Trade and commerce.
Economic expansion has accompanied imperial expansion since ancient times. Greek trade-networks spread throughout the Mediterranean region, while Roman trade expanded with the main goal of directing tribute from the colonized areas towards the Roman metropole. According to Strabo, by the time of emperor Augustus, up to 120 Roman ships would set sail every year from Myos Hormos in Roman Egypt to India. With the development of trade routes under the Ottoman Empire,
Aztec civilization developed into a large empire that, much like the Roman Empire, had the goal of exacting tribute from the conquered colonial areas. For the Aztecs, the most important tribute was the acquisition of sacrificial victims for their religious rituals.
On the other hand, European colonial empires sometimes attempted to channel, restrict and impede trade involving their colonies, funnelling activity through the metropole and taxing accordingly.
Slaves and indentured servants.
European nations entered their imperial projects with the goal of enriching the European metropole. Exploitation of non-Europeans and other Europeans to support imperial goals was acceptable to the colonizers. Two outgrowths of this imperial agenda were slavery and indentured servitude. In the 17th century, nearly two-thirds of English settlers came to North America as indentured servants.
African slavery had existed long before Europeans discovered it as an exploitable means of creating an inexpensive labour force for the colonies. Europeans brought transportation technology to the practise, bringing large numbers of African slaves to the Americas by sail. Spain and Portugal had brought African slaves to work at African colonies such as Cape Verde and the Azores, and then Latin America, by the 16th century. The British, French and Dutch joined in the slave trade in subsequent centuries. Ultimately, around 11 million Africans were taken to the Caribbean and North and South America as slaves by European colonizers.
Abolitionists in Europe and America protested the inhumane treatment of African slaves, which led to the elimination of the slave trade by the late 18th century. The labour shortage that resulted inspired European colonizers to develop a new source of labour, using a system of indentured servitude. Indentured servants consented to a contract with the European colonizers. Under their contract, the servant would work for an employer for a term of at least a year, while the employer agreed to pay for the servant's voyage to the colony, possibly pay for the return to the country of origin, and pay the employee a wage as well. The employee was "indentured" to the employer because they owed a debt back to the employer for their travel expense to the colony, which they were expected to pay through their wages. In practice, indentured servants were exploited through terrible working conditions and burdensome debts created by the employers, with whom the servants had no means of negotiating the debt once they arrived in the colony.
India and China were the largest source of indentured servants during the colonial era. Indentured servants from India travelled to British colonies in Asia, Africa and the Caribbean, and also to French and Portuguese colonies, while Chinese servants travelled to British and Dutch colonies. Between 1830 and 1930, around 30 million indentured servants migrated from India, and 24 million returned to India. China sent more indentured servants to European colonies, and around the same proportion returned to China.
Following the Scramble for Africa, an early but secondary focus for most colonial regimes was the suppression of slavery and the slave trade. By the end of the colonial period they were mostly successful in this aim, though slavery is still very active in Africa.
Military innovation.
Imperial expansion follows military conquest in most instances. Imperial armies therefore have a long history of military innovation in order to gain an advantage over the armies of the people they aim to conquer. Greeks developed the phalanx system, which enabled their military units to present themselves to their enemies as a wall, with foot soldiers using shields to cover one another during their advance on the battlefield. Under Philip II of Macedon, they were able to organize thousands of soldiers into a formidable battle force, bringing together carefully trained infantry and cavalry regiments. Alexander the Great exploited this military foundation further during his conquests.
The Spanish Empire held a major advantage over Mesoamerican warriors through the use of weapons made of stronger metal, predominantly iron, which was able to shatter the blades of axes used by the Aztec civilization and others. The European development of firearms using gunpowder cemented their military advantage over the peoples they sought to subjugate in the Americas and elsewhere.
The end of empire.
The populations of some colonial territories, such as Canada, enjoyed relative peace and prosperity as part of a European power, at least among the majority; however, minority populations such as First Nations peoples and French-Canadians experienced marginalization and resented colonial practises. Francophone residents of Quebec, for example, were vocal in opposing conscription into the armed services to fight on behalf of Britain during World War I, resulting in the Conscription crisis of 1917. Other European colonies had much more pronounced conflict between European settlers and the local population. Rebellions broke out in the later decades of the imperial era, such as India's Sepoy Rebellion.
The territorial boundaries imposed by European colonizers, notably in central Africa and South Asia, defied the existing boundaries of native populations that had previously interacted little with one another. European colonizers disregarded native political and cultural animosities, imposing peace upon people under their military control. Native populations were often relocated at the will of the colonial administrators. Once independence from European control was achieved, civil war erupted in some former colonies, as native populations fought to capture territory for their own ethnic, cultural or political group. The Partition of India, a 1947 civil war that came in the aftermath of India's independence from Britain, became a conflict with 500,000 killed. Fighting erupted between Hindu, Sikh and Muslim communities as they fought for territorial dominance. Muslims fought for an independent country to be partitioned where they would not be a religious minority, resulting in the creation of Pakistan.
Post-independence population movement.
In a reversal of the migration patterns experienced during the modern colonial era, post-independence era migration followed a route back towards the imperial country. In some cases, this was a movement of settlers of European origin returning to the land of their birth, or to an ancestral birthplace. 900,000 French colonists (known as the "Pied-Noirs") resettled in France following Algeria's independence in 1962. A significant number of these migrants were also of Algerian descent. 800,000 people of Portuguese origin migrated to Portugal after the independence of former colonies in Africa between 1974 and 1979; 300,000 settlers of Dutch origin migrated to the Netherlands from the Dutch West Indies after Dutch military control of the colony ended.
After WWII 300,000 Dutchmen from the Dutch East Indies, of which the majority were people of Eurasian descent called Indo Europeans, repatriated to the Netherlands. A significant number later migrated to the US, Canada, Australia and New Zealand.
Global travel and migration in general developed at an increasingly brisk pace throughout the era of European colonial expansion. Citizens of the former colonies of European countries may have a privileged status in some respects with regard to immigration rights when settling in the former European imperial nation. For example, rights to dual citizenship may be generous, or larger immigrant quotas may be extended to former colonies.
In some cases, the former European imperial nations continue to foster close political and economic ties with former colonies. The Commonwealth of Nations is an organization that promotes cooperation between and among Britain and its former colonies, the Commonwealth members. A similar organization exists for former colonies of France, the Francophonie; the Community of Portuguese Language Countries plays a similar role for former Portuguese colonies, and the Dutch Language Union is the equivalent for former colonies of the Netherlands.
Migration from former colonies has proven to be problematic for European countries, where the majority population may express hostility to ethnic minorities who have immigrated from former colonies. Cultural and religious conflict have often erupted in France in recent decades, between immigrants from the Maghreb countries of north Africa and the majority population of France. Nonetheless, immigration has changed the ethnic composition of France; by the 1980s, 25% of the total population of "inner Paris" and 14% of the metropolitan region were of foreign origin, mainly Algerian.
Impact on health.
Encounters between explorers and populations in the rest of the world often introduced new diseases, which sometimes caused local epidemics of extraordinary virulence. For example, smallpox, measles, malaria, yellow fever, and others were unknown in pre-Columbian America.
Disease killed the entire native (Guanches) population of the Canary Islands in the 16th century. Half the native population of Hispaniola in 1518 was killed by smallpox. Smallpox also ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlan alone, including the emperor, and Peru in the 1530s, aiding the European conquerors. Measles killed a further two million Mexican natives in the 17th century. In 1618–1619, smallpox wiped out 90% of the Massachusetts Bay Native Americans. Smallpox epidemics in 1780–1782 and 1837–1838 brought devastation and drastic depopulation among the Plains Indians. Some believe that the death of up to 95% of the Native American population of the New World was caused by Old World diseases. Over the centuries, the Europeans had developed high degrees of immunity to these diseases, while the indigenous peoples had no time to build such immunity.
Smallpox decimated the native population of Australia, killing around 50% of indigenous Australians in the early years of British colonisation. It also killed many New Zealand Māori. As late as 1848–49, as many as 40,000 out of 150,000 Hawaiians are estimated to have died of measles, whooping cough and influenza. Introduced diseases, notably smallpox, nearly wiped out the native population of Easter Island. In 1875, measles killed over 40,000 Fijians, approximately one-third of the population. The Ainu population decreased drastically in the 19th century, due in large part
to infectious diseases brought by Japanese settlers pouring into Hokkaido.
Conversely, researchers concluded that syphilis was carried from the New World to Europe after Columbus's voyages. The findings suggested Europeans could have carried the nonvenereal tropical bacteria home, where the organisms may have mutated into a more deadly form in the different conditions of Europe. The disease was more frequently fatal than it is today; syphilis was a major killer in Europe during the Renaissance. The first cholera pandemic began in Bengal, then spread across India by 1820. Ten thousand British troops and countless Indians died during this pandemic. Between 1736 and 1834 only some 10% of East India Company's officers survived to take the final voyage home. Waldemar Haffkine, who mainly worked in India, who developed and used vaccines against cholera and bubonic plague in the 1890s, is considered the first microbiologist.
Countering disease.
As early as 1803, the Spanish Crown organised a mission (the Balmis expedition) to transport the smallpox vaccine to the Spanish colonies, and establish mass vaccination programs there. By 1832, the federal government of the United States established a smallpox vaccination program for Native Americans. Under the direction of Mountstuart Elphinstone a program was launched to propagate smallpox vaccination in India. From the beginning of the 20th century onwards, the elimination or control of disease in tropical countries became a driving force for all colonial powers. The sleeping sickness epidemic in Africa was arrested due to mobile teams systematically screening millions of people at risk. In the 20th century, the world saw the biggest increase in its population in human history due to lessening of the mortality rate in many countries due to medical advances. The world population has grown from 1.6 billion in 1900 to over seven billion today.
Colonial migrations.
Nations and regions outside of Europe with significant populations of European ancestry

</doc>
<doc id="7300" url="http://en.wikipedia.org/wiki?curid=7300" title="Colonial">
Colonial

Colonial or The Colonial may refer to:

</doc>
<doc id="7301" url="http://en.wikipedia.org/wiki?curid=7301" title="Casablanca">
Casablanca

Casablanca (; , also "" lit: "White house"), the largest city of Morocco, is located in the western part of the country on the Atlantic Ocean. It is also the largest city in the Maghreb, as well as one of the largest and most important cities in Africa, both economically and demographically.
Casablanca is Morocco's chief port and industrial center. The 2012 census, adjusted with recent numbers, recorded a population of about 4 million in the prefecture of Casablanca and about 5 million in the region of Grand Casablanca. Casablanca is considered the economic and business center of Morocco, while the national political capital is Rabat.
The leading Moroccan companies and international corporations doing business there have their headquarters and main industrial facilities in Casablanca. Recent industrial statistics show Casablanca retains its historical position as the main industrial zone of the country. The Port of Casablanca is one of the largest artificial ports in the world, and the largest port of North Africa. It is also the primary naval base for the Royal Moroccan Navy.
Etymology.
The Latinized name of the city is a Portuguese word combination meaning "White house" (' "white", ' "house"). The modern Spanish version of the name came later. The city is now nicknamed "Casa" by many locals.
Anfa is generally considered the "original city" or "old city" of Casablanca; it is legally a prefecture (district) with half a million city inhabitants, and thus is part of the Grand Casablanca region.
History.
Early history.
The area which is today Casablanca was founded and settled by Berbers by at least the 7th century BC. It was used as a port by the Phoenicians and later the Romans. In his book "Wasf Afriquia", Al-Hassan al-Wazzan refers to ancient Casablanca as "Anfa", a great city founded in the Berber kingdom of Barghawata in 744 AD. He believed Casablanca was the most "prosperous city on the Atlantic coast because of its fertile land." Barghawata rose as an independent state around this time, and continued until it was conquered by the Almoravids in 1068.
Middle Ages to French conquest.
During the 14th century, under the Merinids, Anfa rose in importance as a port. The last of the Merinids was ousted by a popular revolt in 1465. In the early 15th century, the town became an independent state once again, and emerged as a safe harbour for pirates and privateers, leading to it being targeted by the Portuguese, who destroyed the town in 1468. The Portuguese used the ruins of Anfa to build a military fortress in 1515. The town that grew up around it was called "Casa Branca", meaning "white house" in Portuguese.
Between 1580 and 1640 the Crown of Portugal and the Crown of Spain were held by the same kings and therefore Casablanca and all other areas occupied by the Portuguese were under Spanish control, even though maintaining an autonomous Portuguese administration. As Portugal broke ties with the Spanish king in 1640, Casablanca came under fully Portuguese control once again. The Europeans eventually abandoned the area completely in 1755 following an earthquake which destroyed most of the town.
The town was finally reconstructed by Sultan Mohammed ben Abdallah (1756–1790), the grandson of Moulay Ismail and ally of George Washington with the help of Spaniards from the nearby emporium. The town was called الدار البيضاء "ad-Dār al-Bayḍāʼ", the Arabic translation of the Spanish "Casa Blanca", meaning "white house".
In the 19th century, the area's population began to grow as it became a major supplier of wool to the booming textile industry in Britain and shipping traffic increased (the British, in return, began importing Morocco's now famous national drink, gunpowder tea). By the 1860s, there were around 5,000 residents, and the population grew to around 10,000 by the late 1880s. Casablanca remained a modestly sized port, with a population reaching around 12,000 within a few years of the French conquest and arrival of French colonialists in the town, at first administrators within a sovereign sultanate, in 1906. By 1921, this was to rise to 110,000, largely through the development of shanty towns.
French rule.
In June 1907, the French attempted to build a light railway near the port and passing through a graveyard. Residents attacked the French, and riots ensued. French troops were landed in order to restore order, which was achieved only after severe damage to the town. The French then took control of Casablanca. This effectively began the process of colonization, although French control of Casablanca was not formalised until 1910. Under the French rule, there were Muslim Anti Jewish riots in 1908.
The famous 1942 film "Casablanca" underlined the city's colonial status at the time—depicting it as the scene of a power struggle between competing European powers. The film has a cosmopolitan cast of characters (American, French, German, Italian, Czech, Norwegian, Austrian, Bulgarian, Russian and some other nationalities).
Europeans formed almost half the population. During the 1940s and 1950s, Casablanca was a major centre of anti-French rioting. A bomb attack on Christmas Day of 1953 caused many casualties.
World War II.
Operation Torch (initially called Operation Gymnast) was the British-American invasion of French North Africa during the North African Campaign of World War II, which started on 8 November 1942.
The Americans attacked at three different locations in French North Africa, one of the three being the landings at Casablanca because of its important ports and the major administrative centers.
Casablanca was an important strategic port during World War II and hosted the Casablanca Conference in 1943, in which Churchill and Roosevelt discussed the progress of the war. Casablanca was the site of a large American air base, which was the staging area for all American aircraft for the European Theater of Operations during World War II.
Since independence.
In October 1930, Casablanca hosted a Grand Prix, held at the new Anfa Racecourse. In 1958, the race was held at Ain-Diab circuit "(see Moroccan Grand Prix)". Morocco gained independence from France on 2 March 1956. In 1983, Casablanca hosted the Mediterranean Games. The city is now developing a tourism industry. Casablanca has become the economic and business capital of Morocco, while Rabat is the political capital.
In March 2000, more than 60 women's groups organized demonstrations in Casablanca proposing reforms to the legal status of women in the country. Forty thousand women attended, calling for a ban on polygamy and the introduction of divorce law (divorce being a purely religious procedure at that time). Although the counter-demonstration attracted half a million participants, the movement for change started in 2000 was influential on King Mohammed VI, and he enacted a new "Mudawana", or family law, in early 2004, meeting some of the demands of women's rights activists.
On 16 May 2003, 33 civilians were killed and more than 100 people were injured when Casablanca was hit by a multiple suicide bomb attack carried out by Moroccans and claimed by some to have been linked to al-Qaeda. 12 suicide bombers struck five locations in the city.
A string of suicide bombings struck the city in early 2007. A suspected militant blew himself up at a Casablanca internet cafe on 11 March 2007. On 10 April, three suicide bombers blew themselves up during a police raid of their safe house. Two days later, police set up barricades around the city and detained two more men who had escaped the raid. On 14 April, two brothers blew themselves up in downtown Casablanca, one near the American Consulate, and one a few blocks away near the American Language Center. Only one person was injured aside from the bombers, but the Consulate was closed for more than a month.
As calls for reform spread through the Arab world in 2011, Moroccans joined in, but concessions by the ruler led to acceptance. However, in December thousands of people demonstrated in several parts of the city, especially the city center near la fontaine, desiring more significant political reforms.
Geography and climate.
Casablanca is located in the Chawiya plain which has historically been the breadbasket of Morocco. Apart from the Atlantic coast, the Bouskoura forest is the only natural attraction in the city. The forest was planted in the 20th century and consists mostly of Eucalyptus, Palm and Pine trees. It is located halfway to the city's international airport.
The only watercourse in Casablanca is Oued Bouskoura, a small seasonal creek that until 1912 reached the Atlantic Ocean near the actual port. Most of Oued Bouskoura's bed has been covered due to urbanization and only the part south of El-Jadida road can now be seen. The closest permanent river to Casablanca is Oum Er-Rbia River to the south-east.
Casablanca has a very mild Mediterranean climate (Köppen climate classification "Csa"). Casablanca's climate is strongly influenced by the cool currents of the Atlantic Ocean which tends to moderate temperature swings and produce a remarkably mild climate with little seasonal temperature variation and a lack of extreme heat and cold. Casablanca has an annual average of 74 days with significant precipitation, which amounts to 427 millimeters per year. The highest and lowest temperatures ever recorded in the city are and , respectively. The highest amount of rainfall recorded in a single day is 178 millimeters ( November 30, 2010)
Economy.
The Grand Casablanca region is considered the locomotive of the development of the Moroccan economy. It attracts 32% of the country's production units and 56% of industrial labor. The region uses 30% of the national electricity production. With MAD 93 billion, the region contributes to 44% of the Industrial production of the Kingdom. 33% of national industrial exportations, MAD 27 billions come from the Grand Casablanca. 30% of Moroccan banking network is concentrated in Casablanca.
One of the most important Casablancan exports is phosphate. Other industries include fishing, fish canning, sawmills, furniture production, building materials, glass, textiles, electronics, leather work, processed food, spirits, soft drinks, and cigarettes.
The Casablanca and Mohammedia seaports activity represent 50% of the international commercial flows of Morocco.
Almost the entire Casablanca waterfront is under development, mainly the construction of huge entertainment centres between the port and Hassan II Mosque, the Anfa Resort project near the business, entertainment and living centre of Megarama, the shopping and entertainment complex of Morocco Mall, as well as a complete renovation of the coastal walkway. The Sindbad park is planned to be totally renewed with rides, games and entertainment services.
Royal Air Maroc has its head office at the Casablanca-Anfa Airport. In 2004, it announced that it was moving its head office from Casablanca to a location in Province of Nouaceur, close to Mohammed V International Airport. The agreement to build the head office in Nouaceur was signed in 2009.
The biggest CBD of Casablanca and Maghreb is in the North of the town in Sidi Maarouf near the mosque of Hassan II and the biggest project of skycrapers of Maghreb and Africa Casablanca Marina.
Administrative divisions.
Casablanca is a commune, part of the Region of the Grand Casablanca. The commune is divided into 8 districts or prefectures, which are themselves divided into 16 subdivisions or arrondissements and 1 municipality. The 8 districts and their subdivisions are as follows:
Neighborhoods.
The list of neighborhoods is indicative and not complete:
Demographics.
The population of Grand Casablanca was estimated in 2005 to be 3.85 million. 98% live in urban areas. Around 25% of them are under 15 and 9% are over 60 years old. The population of the city is about 11% of the total population of Morocco. Grand Casablanca is also the largest urban area in the Maghreb. The number of inhabitants is however disputed by the locals, who point to a number between 5 and 6 million, citing recent drought years as a reason for many people moving into the city to find work.
Judaism in Casablanca.
There was a Sephardic Jewish community in Anfa up to its destruction by the Portuguese in 1468. Jews were slow to return to the town, but by 1750 the Rabbi Elijah Synagogue was built as the first Jewish synagogue in Casablanca. It was destroyed along with much of the town in the 1755 Lisbon earthquake. Today the Jewish cemetery of Casablanca is one of the major cemeteries of the city.
Main sites.
The French period Ville Nouvelle (New Town) of Casablanca was designed by the French architect Henri Prost, and was a model of a new town at that time. The main streets radiate south and east from Place des Nations Unies, previously the main market of Anfa. Former administrative buildings and modern hotels populate the area. Their style is a combination of Hispano-Mauresque and Art Deco.
Casablanca is home to the Hassan II Mosque, designed by the French architect Michel Pinseau. It is situated on a promontory on the Atlantic. The mosque has room for 25,000 worshippers inside, and a further 80,000 can be accommodated in the mosque's courtyard. Its minaret is the world's tallest at 210 metres. The mosque is also the largest in North Africa, and the third largest in the world.
Work on the mosque was started in 1980, and was intended to be completed for the 60th birthday of the former Moroccan king, Hassan II, in 1989. However, the building was not inaugurated until 1993. Authorities spent an estimated $800 million in the construction of the building.
The Parc de la Ligue Arabe (formally called Lyautey) is the city's largest public park. On its edge is the Casablanca Cathedral (Cathédrale Sacré-Coeur). It is no longer in use for religious purposes, but it is open to visitors and a splendid example of "Mauresque" architecture. The Old Medina (the part of town pre-dating the French protectorate) attracts fewer tourists than the medinas of cities like Fes and Marrakech. However, it has undergone some restoration in recent years. Included in this project have been the western walls of the medina, its "skala", or bastion, and its colonial-period clock tower.
A popular site among locals is the small island Marabout de Sidi Abderrahmane. It is possible to walk across to the rocky island at low tide. This outcrop contains the tomb of Sidi Abderrhamane Thaalibi, a Sufi from Baghdad and the founder of Algiers. He is considered a saint in Morocco. Because of this, many Moroccans make informal pilgrimages to this site "to reflect on life and to seek religious enlightenment". Some believe that the saint possessed magical powers and so his tomb still possesses these powers. People come and seek this magic in order to be cured. Non-Muslims may not enter the shrine.
Casablanca Marina.
Located on the seafront on 26 hectares, Casablanca Marina offers upscale residential products, office spaces which respect the international standards, shops and a convention Centre up to the national and international expectations for the organization and hosting congresses .
Casablanca Marina also embodies an ideal urban and environmental context, it has local services, parks and a professional team to manage the site. it aims to become an essential destination for recreational boaters seeking long stays, or just an unforgettable stopover on their way to West Africa, the Caribbean or the shores of North America.
It is noteworthy that the Works of the project has started at the beginning of 2012 and it is expected to be completed in the summer of 2014
Other Sites.
The Casablanca Technopark is an information technology Business cluster complex located at Casablanca, and was inaugurated in October 2001.
Shopping.
There are several shopping centers in Casablanca, of which the largest is Morocco Mall. It is the largest shopping center in Africa with of floor space in Casablanca. The mall, which opened on December 1, 2011, was designed by architect Davide Padoa of Design International, a global architecture boutique with its headquarters in London.
The mall features a massive 1,000,000 litre aquarium that contains over 40 different species of fish. The aquarium is called "Aquadream" and was designed and built by International Concept Management (ICM). Visitors have the opportunity to take a ride through the center of the cylinder shaped aquarium with a 360-degree view of the sea life. Visitors can also go scuba diving with a professional instructor inside the aquarium. Anfaplace Shopping Center is a shopping center located on the Corniche of Casablanca.
The Casablanca Twin Center is the new landmark of the business district of Casablanca. At over 100 meters in height, the twin towers dominate the skyline and the work environment are better co-ordinated and more dynamic which is, as evidenced by their design particularly studied in terms of technology, efficiency and comfort.
Sports.
Hosting.
Casablanca staged the 1961 Pan Arab Games, the 1983 Mediterranean Games and the 1988 Africa Cup of Nations. It will also be a venue for the 2015 Africa Cup of Nations.
Venues.
The Grand Stade de Casablanca is the proposed title of the planned football stadium to be built in the city. Once completed in 2014, it will be used mostly for football matches and will serve as the home of Raja Casablanca, Wydad Casablanca and the Morocco national football team. The stadium was designed with a capacity of 80,000 spectators, making it one of the highest-capacity stadiums in Africa. Once completed, it will replace the Stade Mohamed V. The initial idea of the stadium was for the 2010 FIFA World Cup, for which Morocco lost their bid to South Africa. Nevertheless, the Moroccan government supported the decision to go ahead with the plans. It will be completed in 2014, ready for the 2015 Africa Cup of Nations.
Association football.
Casablanca is home to two popular football clubs, Raja Club Athletic and Wydad Athletic Club. Raja's symbol is an eagle and Wydad's symbol is a goose. These two popular clubs have produced some of Morocco's best players such as: Salaheddine Bassir, Abdelmajid Dolmy, Baddou Zaki, Aziz Bouderbala and Noureddine Naybet. There are other football teams on top of these two major teams that are based in the city of Casablanca are Rachad Bernoussi, TAS de Casablanca, Majd Al Madina and Racing Casablanca.
Tennis.
Casablanca hosts The Grand Prix Hassan II, a professional male tennis tournament of the ATP tour. It first began in 1986. It is played on clay courts type at Complexe Al Amal.
Notable winners of the Hassan II Grand-Prix are: Thomas Muster in 1990, Hicham Arazi in 1997, Younes El Aynaoui in 2002 and Stanislas Wawrinka in 2010.
Transport.
Tram.
The Casablanca tramway is the rapid transit tram system in Casablanca in Morocco. The route is long, with 49 stops, and Y-shaped; further lines are planned in the future.
Air.
Casablanca's main airport is Mohammed V International Airport, Morocco's busiest airport. Regular domestic flights serve Marrakech, Rabat, Agadir, Oujda, and Tangier, Laayoune as well as other cities.
Casablanca is well served by international flights to Europe, especially French and Spanish airports, and has regular connections to North American, Middle Eastern and sub-Saharan African destinations. New York City, Paris, London and Dubai are important primary destinations.
The older, smaller Casablanca-Anfa Airport to the west of the city, that served certain destinations including Damascus, and Tunis, was largely closed to international civilian traffic in 2006. It currently services domestic flights and freight. Casablanca Tit Mellil Airport is located in the nearby community of Tit Mellil.
Coaches.
CTM coaches (intercity buses) and various private lines run services to most notable Moroccan towns as well as a number of European cities. These run from the Gare Routière on Rue Léon l'Africain in downtown Casablanca.
Metro.
"See also: Casablanca RER or Casablanca metro"
An underground railway system is currently being planned, which when constructed will potentially offer some relief to the problems of traffic congestion and poor air quality. The metro will not be ready before 2017, having a length of and costing 46.7 billion dirhams (approximately 5.8 billion USD). However, it should be noted that none of the preparatory works for this project have started.
Taxis.
Registered taxis in Casablanca are coloured red and known as "petit taxis" ("small taxis"), or coloured white and known as "grands taxis" ("big taxis"). As is standard Moroccan practice, "petits taxis," typically small-four door Dacia Logan, Peugeot 207 or similar cars, provide metered cab service in the central metropolitan areas. "Grands taxis," generally older Mercedes-Benz sedans, provide shared mini-bus like service within the city on pre-defined routes, or shared inter-city service. Grands Taxis may also be hired for private service by the hour or day.
Trains.
Casablanca is served by three principal railway stations run by the national rail service, the ONCF.
Casa-Voyageurs is the main inter-city station, from which trains run south to Marrakech or El Jadida and north to Mohammedia and Rabat, and then on either to Tangier or Meknes, Fes, Taza and Oujda/Nador. A dedicated airport shuttle service to Mohammed V International Airport also has its primary in-city stop at this station, for connections on to further destinations.
Casa-Port serves primarily commuter trains operating on the Casablanca – Kenitra rail corridor, with some connecting trains running on to Gare de Casa-Voyageurs. The station provides a direct interchange between train and shipping services, and is located near to several port-area hotels. It is the nearest station to the old town of Casablanca, and to the modern city centre, around the landmark Casablanca Twin Center. Casa-Port station is being rebuilt in a modern and enlarged configuration. During the construction the station is still operational. From 2013 it will provide a close connection from the rail network to the city's new tram network.
Casa-Oasis was originally a suburban commuter station which was fully redesigned and rebuilt in the early twenty-first century, and officially re-opened in 2005 as a primary city rail station. Owing to its new status, all southern inter-city train services to and from Casa-Voyageurs now call at Casa-Oasis. ONCF stated in 2005 that the refurbishment and upgrading of Casa-Oasis to inter-city standards was intended to relieve passenger congestion at Casa-Voyageurs station.
International relations.
Twin towns – sister cities.
Casablanca is twinned with:

</doc>
<doc id="7303" url="http://en.wikipedia.org/wiki?curid=7303" title="Cross">
Cross

A cross is a geometrical figure consisting of two lines or bars perpendicular to each other, dividing one or two of the lines in half. The lines usually run vertically and horizontally; if they run obliquely, the design is technically termed a saltire, although the arms of a saltire need not meet at right angles.
The cross is one of the most ancient human symbols, and has been used by many religions, most notably Christianity. It may be seen as a division of the world into four elements (Chevalier, 1997) or cardinal points, or alternately as the union of the concepts of divinity, the vertical line, and the world, the horizontal line (Koch, 1955).
Etymology.
The word "cross" comes ultimately from Latin "crux", a Roman torture device used for crucifixion, via Old Irish "cros". The word was introduced to English in the 10th century as the term for the instrument of the torturous execution of Jesus as described in the New Testament, gradually replacing the earlier word "rood".
History.
It is not known when the first cross image was made; after circles, crosses are one of the first symbols drawn by children of all cultures. Some of the earliest images of crosses were found in the Central Asian steppes, and some were found in Altay. The cross in the old Altaic religion called Tengriism symbolizes the god Tengri; it wasn't an elongated "dagger" cross, instead resembling a plus sign (+).
The first Christian books from Armenia and Syria contained evidence that the cross originated with horsemen from the east, possibly referring to the first Turkic people. In old Armenian temples, some stylistic Turkic influences are found in cross symbols. Named animal, the symbol was found in the plans of temples, with the pillars from above looking like an additional cross.
There are many cross-shaped incisions in European cult caves, dating back to the earliest stages of human cultural development in the stone age. Like other symbols from this period, their use continued in the Celtic and Germanic cultures in Europe. For example, Celtic coins minted many centuries before the Christian era may have an entire side showing this type of cross, sometimes with the cardinal points marked by concave depressions in the same style as in stone age carvings. Other coins may be showing the cross held by a rider on a horse and springing a fern leaf, sometimes identified as a Tree of Life symbol.
As of April 10, 2013, pictures of a possible contender for the first use of the cross symbol has been found at the Tell Khaiber excavation site in Ur, Iraq. The shape of one of the buildings, believed to be about 4,000 years old, is in the shape of a cross of the type used in the crucifixion of Jesus Christ. A picture of the building's floor can be seen here: This would predate the ancient Israelites Tabernacle in the wilderness and the first two temples by about 800 years.
As markings.
Written crosses are used for many different purposes, particularly in mathematics.
In heraldry.
There are numerous other variations on the cross in heraldry. See heraldry for background information.
James Parker's is online, and contains much information about variants of crosses used in heraldry.
In flags.
Several flags have crosses, including all the nations of Scandinavia, whose crosses are known as Scandinavian crosses, and many nations in the Southern Hemisphere, which incorporate the Southern Cross. The Flag of Switzerland since the 17th century has displayed an equilateral cross in a square (the only square flag of a sovereign state apart from the Flag of the Vatican City); the Red Cross emblem was based on the Swiss flag.
Other noteworthy crosses.
Crux, or the Southern Cross, is a cross-shaped constellation in the Southern Hemisphere. It appears on the national flags of Australia, Brazil, New Zealand, Niue, Papua New Guinea and Samoa.
The tallest cross, at 152.4 metres high, is part of Francisco Franco's monumental "Valley of the Fallen", the "Monumento Nacional de Santa Cruz del Valle de los Caidos" in Spain.
A cross at the junction of Interstates 57 and 70 in Effingham, Illinois, is purportedly the tallest in the United States, at 198 feet (60.3 m) tall.
The tallest freestanding cross in the United States is located in Saint Augustine, FL and stands 208 feet.
The tombs at Naqsh-e Rustam, Iran, made in the 5th century BC, are carved into the cliffside in the shape of a cross. They are known as the "Persian crosses".
As physical gestures.
The cross shape may be made by hand gestures including crossing the fingers of one hand, and also crossing the index fingers of both hands (the latter having folkloric use in warding off evil --and often seen in vampire movies). Cross-gestures involving more than just the hand include the "cross my heart" movement sometimes invoked when making a promise, the Christian sign of the cross, and the NFL referee "stop the clock" hand signal.

</doc>
<doc id="7304" url="http://en.wikipedia.org/wiki?curid=7304" title="Coordination complex">
Coordination complex

In chemistry, a coordination complex or metal complex consists of a central atom or ion, which is usually metallic and is called the "coordination centre", and a surrounding array of bound molecules or ions, that are in turn known as "ligands" or complexing agents. Many metal-containing compounds, especially those of transition metals, are coordination complexes.
Nomenclature and terminology.
Coordination complexes are so pervasive that the structure and reactions are described in many ways, sometimes confusingly. The atom within a ligand that is bonded to the central atom or ion is called the donor atom. In a typical complex, metal ion is bound to several donor atoms, which can be the same or different. Polydentate (multiple bonded) ligands consist of several donor atoms, several of which are bound to the central atom or ion. These complexes are called chelate complexes, the formation of such complexes is called chelation, complexation, and coordination.
The central atom or ion, together with all ligands comprise the coordination sphere. The central atoms or ion and the donor atoms comprise the first coordination sphere.
Coordination refers to the "coordinate covalent bonds" (dipolar bonds) between the ligands and the central atom. Originally, a complex implied a reversible association of molecules, atoms, or ions through such weak chemical bonds. As applied to coordination chemistry, this meaning has evolved. Some metal complexes are formed virtually irreversibly and many are bound together by bonds that are quite strong.
History.
Coordination complexes were known – although not understood in any sense – since the beginning of chemistry, e.g. Prussian blue and copper vitriol. The key breakthrough occurred when Alfred Werner proposed in 1893 that Co(III) bears six ligands in an octahedral geometry. His theory allows one to understand the difference between coordinated and ionic in a compound, for example chloride in the cobalt ammine chlorides and to explain many of the previously inexplicable isomers.
In 1914, Werner resolved the first coordination complex, called hexol, into optical isomers, overthrowing the theory that only carbon compounds could possess chirality.
Structures.
The ions or molecules surrounding the central atom are called ligands. Ligands are generally bound to the central atom by a coordinate covalent bond (donating electrons from a lone electron pair into an empty metal orbital), and are said to be coordinated to the atom. There are also organic ligands such as alkenes whose pi bonds can coordinate to empty metal orbitals. An example is ethene in the complex known as Zeise's salt, K+[PtCl3(C2H4)]−.
Geometry.
In coordination chemistry, a structure is first described by its coordination number, the number of ligands attached to the metal (more specifically, the number of donor atoms). Usually one can count the ligands attached, but sometimes even the counting can become ambiguous. Coordination numbers are normally between two and nine, but large numbers of ligands are not uncommon for the lanthanides and actinides. The number of bonds depends on the size, charge, and electron configuration of the metal ion and the ligands. Metal ions may have more than one coordination number.
Typically the chemistry of transition metal complexes is dominated by interactions between s and p molecular orbitals of the ligands and the d orbitals of the metal ions. The s, p, and d orbitals of the metal can accommodate 18 electrons (see 18-Electron rule). The maximum coordination number for a certain metal is thus related to the electronic configuration of the metal ion (to be more specific, the number of empty orbitals) and to the ratio of the size of the ligands and the metal ion. Large metals and small ligands lead to high coordination numbers, e.g. [Mo(CN)8]4−. Small metals with large ligands lead to low coordination numbers, e.g. Pt[P(CMe3)]2. Due to their large size, lanthanides, actinides, and early transition metals tend to have high coordination numbers.
Different ligand structural arrangements result from the coordination number. Most structures follow the points-on-a-sphere pattern (or, as if the central atom were in the middle of a polyhedron where the corners of that shape are the locations of the ligands), where orbital overlap (between ligand and metal orbitals) and ligand-ligand repulsions tend to lead to certain regular geometries. The most observed geometries are listed below, but there are many cases that deviate from a regular geometry, e.g. due to the use of ligands of different types (which results in irregular bond lengths; the coordination atoms do not follow a points-on-a-sphere pattern), due to the size of ligands, or due to electronic effects (see, e.g., Jahn–Teller distortion):
Some exceptions and provisions should be noted:
Isomerism.
The arrangement of the ligands is fixed for a given complex, but in some cases it is mutable by a reaction that forms another stable isomer.
There exist many kinds of isomerism in coordination complexes, just as in many other compounds.
Stereoisomerism.
Stereoisomerism occurs with the same bonds in different orientations relative to one another. Stereoisomerism can be further classified into:
Cis–trans isomerism and facial–meridional isomerism.
Cis–trans isomerism occurs in octahedral and square planar complexes (but not tetrahedral). When two ligands are adjacent they are said to be cis, when
opposite each other, trans. When three identical ligands occupy one face of an octahedron, the isomer is said to be facial, or fac. In a "fac" isomer, any two identical ligands are adjacent or "cis" to each other. If these three ligands and the metal ion are in one plane, the isomer is said to be meridional, or mer. A "mer" isomer can be considered as a combination of a "trans" and a "cis", since it contains both trans and cis pairs of identical ligands.
Optical isomerism.
Optical isomerism occurs when a molecule is not superimposable with its mirror image. It is so called because the two isomers are each optically active, that is, they rotate the plane of polarized light in opposite directions. The symbol Λ ("lambda") is used as a prefix to describe the left-handed propeller twist formed by three bidentate ligands, as shown. Likewise, the symbol Δ ("delta") is used as a prefix for the right-handed propeller twist.
Structural isomerism.
Structural isomerism occurs when the bonds are themselves different. There are four types of structural isomerism: ionisation isomerism, solvate or hydrate isomerism, linkage isomerism and coordination isomerism.
Electronic properties.
Many of the properties of transition metal complexes are dictated by their electronic structures. The electronic structure can be described by a relatively ionic model that ascribes formal charges to the metals and ligands. This approach is the essence of crystal field theory (CFT). Crystal field theory, introduced by Hans Bethe in 1929, gives a quantum mechanically based attempt at understanding complexes. But crystal field theory treats all interactions in a complex as ionic and assumes that the ligands can be approximated by negative point charges.
More sophisticated models embrace covalency, and this approach is described by ligand field theory (LFT) and Molecular orbital theory (MO). Ligand field theory, introduced in 1935 and built from molecular orbital theory, can handle a broader range of complexes and can explain complexes in which the interactions are covalent. The chemical applications of group theory can aid in the understanding of crystal or ligand field theory, by allowing simple, symmetry based solutions to the formal equations.
Chemists tend to employ the simplest model required to predict the properties of interest; for this reason, CFT has been a favorite for the discussions when possible. MO and LF theories are more complicated, but provide a more realistic perspective.
The electronic configuration of the complexes gives them some important properties:
Color of transition metal complexes.
Transition metal complexes often have spectacular colors caused by electronic transitions by the absorption of light. For this reason they are often applied as pigments. Most transitions that are related to colored metal complexes are either d–d transitions or charge transfer bands. In a d–d transition, an electron in a d orbital on the metal is excited by a photon to another d orbital of higher energy. A charge transfer band entails promotion of an electron from a metal-based orbital into an empty ligand-based orbital (Metal-to-Ligand Charge Transfer or MLCT). The converse also occurs: excitation of an electron in a ligand-based orbital into an empty metal-based orbital (Ligand to Metal Charge Transfer or LMCT). These phenomena can be observed with the aid of electronic spectroscopy; also known as UV-Vis. For simple compounds with high symmetry, the d–d transitions can be assigned using Tanabe–Sugano diagrams. These assignments are gaining increased support with computational chemistry.
Colors of Lanthanide complexes.
Superficially lanthanide complexes are similar to those of the transition metals in that some are coloured. However for the common Ln3+ ions (Ln = lanthanide) the colors are all pale, and hardly influenced by the nature of the ligand. The colors are due to 4f electron transitions. As the 4f orbitals in lanthanides are “buried” in the xenon core and shielded from the ligand by the 5s and 5p orbitals they are therefore not influenced by the ligands to any great extent leading to a much smaller crystal field splitting than in the transition metals. The absorption spectra of an Ln3+ ion approximates to that of the free ion where the electronic states are described by spin-orbit coupling (also called L-S coupling or Russell-Saunders coupling). This contrasts to the transition metals where the ground state is split by the crystal field. Absorptions for Ln3+ are weak as electric dipole transitions are parity forbidden (Laporte Rule forbidden) but can gain intensity due to the effect of a low-symmetry ligand field or mixing with higher electronic states (e.g. d orbitals). Also absorption bands are extremely sharp which contrasts with those observed for transition metals which generally have broad bands. This can lead to extremely unusual effects, such as significant color changes under different forms of lighting.
Magnetism.
Metal complexes that have unpaired electrons are magnetic. Considering only monometallic complexes, unpaired electrons arise because the complex has an odd number of electrons or because electron pairing is destabilized. Thus, monomeric Ti(III) species have one "d-electron" and must be (para)magnetic, regardless of the geometry or the nature of the ligands. Ti(II), with two d-electrons, forms some complexes that have two unpaired electrons and others with none. This effect is illustrated by the compounds TiX2[(CH3)2PCH2CH2P(CH3)2]2: when X = Cl, the complex is paramagnetic (high-spin configuration), whereas when X = CH3, it is diamagnetic (low-spin configuration). It is important to realize that ligands provide an important means of adjusting the ground state properties.
In bi- and polymetallic complexes, in which the individual centers have an odd number of electrons or that are high-spin, the situation is more complicated. If there is interaction (either direct or through ligand) between the two (or more) metal centers, the electrons may couple (antiferromagnetic coupling, resulting in a diamagnetic compound), or they may enhance each other (ferromagnetic coupling). When there is no interaction, the two (or more) individual metal centers behave as if in two separate molecules.
Reactivity.
Complexes show a variety of possible reactivities:
If the ligands around the metal are carefully chosen, the metal can aid in (stoichiometric or catalytic) transformations of molecules or be used as a sensor.
Classification.
Metal complexes, also known as coordination compounds, include all metal compounds, aside from metal vapors, plasmas, and alloys. The study of "coordination chemistry" is the study of "inorganic chemistry" of all alkali and alkaline earth metals, transition metals, lanthanides, actinides, and metalloids. Thus, coordination chemistry is the chemistry of the majority of the periodic table. Metals and metal ions exist, in the condensed phases at least, only surrounded by ligands.
The areas of coordination chemistry can be classified according to the nature of the ligands, in broad terms:
Mineralogy, materials science, and solid state chemistry – as they apply to metal ions – are subsets of coordination chemistry in the sense that the metals are surrounded by ligands. In many cases these ligands are oxides or sulfides, but the metals are coordinated nonetheless, and the principles and guidelines discussed below apply. In hydrates, at least some of the ligands are water molecules. It is true that the focus of mineralogy, materials science, and solid state chemistry differs from the usual focus of coordination or inorganic chemistry. The former are concerned primarily with polymeric structures, properties arising from a collective effects of many highly interconnected metals. In contrast, coordination chemistry focuses on reactivity and properties of complexes containing individual metal atoms or small ensembles of metal atoms.
Older classifications of isomerism.
Traditional classifications of the kinds of isomer have become archaic with the advent of modern structural chemistry. In the older literature, one encounters:
Naming complexes.
The basic procedure for naming a complex:
Examples:
The coordination number of ligands attached to more than one metal (bridging ligands) is indicated by a subscript to the Greek symbol μ placed before the ligand name. Thus the dimer of aluminium trichloride is described by Al2Cl4(μ2-Cl)2.

</doc>
<doc id="7305" url="http://en.wikipedia.org/wiki?curid=7305" title="Coleco">
Coleco

Coleco Inc. is an American company founded in 1932 by Maurice Greenberg as ""Co"nnecticut "Le"ather "Co"mpany". It became a highly successful toy company in the 1980s, known for its mass-produced version of Cabbage Patch Kids dolls and its video game consoles, the Coleco Telstar and ColecoVision. The company is headquartered in Manalapan, New Jersey.
History.
Coleco originally processed shoe leather, which later led to a business in leather craft kits in the 1950s. They began manufacturing plastic moulding and moved into plastic wading pools in the 1960s. The leather part of the business was then sold off.
Under CEO Arnold Greenberg, the company entered the video game console business with the Telstar in 1976. Dozens of companies were introducing game systems that year after Atari's successful Pong console. Nearly all of these new games were based on General Instrument's "Pong-on-a-chip". However, General Instrument had underestimated demand, and there were severe shortages. Coleco had been one of the first to place an order, and was one of the few companies to receive an order in full. Though dedicated game consoles did not last long on the market, their early order enabled Coleco to break even.
Coleco continued to do well in electronics. They transitioned next into handheld electronic games, a market popularized by Mattel. Coleco produced two very popular lines of games, the "head to head" series of two player sports games, (Football, Baseball, Basketball, Soccer, Hockey) and the Mini-Arcade series of licensed video arcade titles such as "Donkey Kong" and "Ms. Pac-Man". A third line of educational handhelds was also produced and included the Electronic Learning Machine, Lil Genius, Digits, and a trivia game called Quiz Wiz. Launched in 1982, their first four tabletop Mini-Arcades, for "Pac-Man", "Galaxian", "Donkey Kong", and "Frogger", sold approximately three million units within a year. Among these, 1.5 million units were sold for "Pac-Man" alone. In 1983, they released three more Mini-Arcades: for "Ms. Pac-Man", "Donkey Kong Junior", and "Zaxxon".
Coleco returned to the video game console market in 1982 with the launch of the ColecoVision. While the system was quite popular, Coleco hedged their bet on video games by introducing a line of ROM cartridges for the Atari 2600 and Intellivision. They also introduced the Coleco Gemini, a clone of the popular Atari 2600.
When the video game business began to implode in 1983, it seemed clear that video game consoles were being supplanted by home computers. Coleco's strategy was to introduce the Coleco Adam home computer, both as a stand-alone system and as an expansion module to the ColecoVision. This effort failed, in large part because Adams were often unreliable. The Adam flopped; Coleco withdrew from electronics early in 1985.
Also in 1983, Coleco released the Cabbage Patch Kids series of dolls which were wildly successful. Flush with success, they purchased beleaguered Selchow & Righter in 1986, manufacturers of Scrabble, Parcheesi, and Trivial Pursuit, sales of which had plummeted, leaving Selchow & Righter with warehouses full of unsold games. The purchase price was $75 million. That same year, Coleco introduced an ALF plush based on the furry alien character who had his own television series at the time, as well as a talking version and a cassette-playing "Storytelling ALF" doll. The combination of the purchase of Selchow & Righter, the disastrous Adam computer, and the public's waning infatuation with Cabbage Patch dolls all contributed to Coleco's financial decline. In 1988, the company filed for chapter 11 bankruptcy.
The reorganized Coleco sold off all of its North American assets and outsourced thousands of jobs to foreign countries, closing plants in Amsterdam, New York and other cities. In 1989, Hasbro and Canada based SLM Action Sports Inc. purchased Coleco's assets.
In 2005, River West Brands, a Chicago-based brand revitalization company, re-introduced Coleco to the marketplace. In late 2006, they introduced the Coleco Sonic, a handheld system containing twenty Sega Master System and Sega Game Gear games.

</doc>
<doc id="7306" url="http://en.wikipedia.org/wiki?curid=7306" title="ColecoVision">
ColecoVision

The ColecoVision is Coleco Industries' second generation home video game console, which was released in August 1982. The ColecoVision offered near-arcade-quality graphics and gaming style along with the means to expand the system's basic hardware. Released with a catalog of 12 launch titles, with an additional 10 games announced for 1982, approximately 145 titles in total were published as ROM cartridges for the system between 1982 and 1984. River West Brands currently owns the ColecoVision brand name.
In 2009, IGN named the ColecoVision their 12th best video game console out of their list of 25, citing "its incredible accuracy in bringing current-generation arcade hits home."
History.
Coleco licensed Nintendo's "Donkey Kong" as the official pack-in cartridge for all ColecoVision consoles, helping to boost the console's popularity. By Christmas of 1982, Coleco had sold more than 500,000 units, in part on the strength of its bundled game. The ColecoVision's main competitor was the arguably more advanced but less commercially successful Atari 5200.
The ColecoVision was distributed by CBS Electronics outside of North America, and was branded the CBS ColecoVision.
Sales quickly passed 1 million in early 1983, before the video game crash of 1983. By the beginning of 1984, quarterly sales of the ColecoVision had dramatically decreased.
Over the next 18 months, the Coleco company ramped down its video game division, ultimately withdrawing from the video game market by the end of the summer of 1985. The ColecoVision was officially discontinued by October 1985. Total sales of the ColecoVision are uncertain but were ultimately in excess of 2 million units, as sales had reached that number by the spring of 1984, while the console continued to sell modestly up until its discontinuation the following year.
In 1983 Spectravideo announced the SV-603 ColecoVision Video Game Adapter for its SV-318 computer. The company stated that the $70 product allowed users to "enjoy the entire library of exciting ColecoVision video-game cartridges". In 1986, Bit Corporation produced a ColecoVision clone called the Dina, which was sold in the United States by Telegames as the Telegames Personal Arcade.
Hardware.
The main console unit consists of a 14×8×2 inch rectangular plastic case that houses the motherboard, with a cartridge slot on the right side and connectors for the external power supply and RF jack at the rear. The controllers connect into plugs in a recessed area on the top of the unit.
The design of the controllers is similar to that of Mattel's Intellivision—the controller is rectangular and consists of a numeric keypad and a set of side buttons. In place of the circular control disc below the keypad, the Coleco controller has a short, 1.5-inch joystick. The keypad is designed to accept a thin plastic overlay that maps the keys for a particular game. Each ColecoVision console shipped with two controllers.
All first-party cartridges and most third-party software titles feature a 12-second pause before presenting the game select screen. This delay results from an intentional loop in the console's BIOS to enable on-screen display of the ColecoVision brand. Companies like Parker Brothers, Activision, and Micro Fun bypassed this loop, which necessitated embedding portions of the BIOS outside the delay loop, further reducing storage available to actual game programming.
Expansion Modules and accessories.
From its introduction, Coleco touted the ColecoVision's hardware expandability by highlighting the "Expansion Module Interface" on the front of the unit. These hardware expansion modules and accessories were sold separately.
Legacy.
In 1996, programmer Kevin Horton released the first homebrew game for the ColecoVision, a Tetris clone entitled "Kevtris".
In 1997, Telegames released "Personal Arcade Vol. 1", a collection of ColecoVision games for Microsoft Windows, and a 1998 follow-up, "Colecovision Hits Volume One".
In popular culture.
The value of the ColecoVision as a 1980s pop culture icon was discussed on VH1's "I Love The 80's Strikes Back". Several television series have aired episodes that reference or parody the console: "South Park", "Family Guy" and "Everybody Hates Chris".

</doc>
<doc id="7309" url="http://en.wikipedia.org/wiki?curid=7309" title="Telstar (game console)">
Telstar (game console)

The Telstar is a series of video game consoles produced by Coleco from 1976 to 1978. Starting with Telstar "Pong" clone based on General Instrument's AY-3-8500 chip in 1976, there were 14 consoles released in the Telstar branded series. One million Telstar units were sold.
Models.
The large product lineup and the impending fading out of the "Pong" machines led Coleco to face near-bankruptcy in 1980.

</doc>
<doc id="7310" url="http://en.wikipedia.org/wiki?curid=7310" title="Conventional warfare">
Conventional warfare

Conventional warfare is a form of warfare conducted by using conventional weapons and battlefield tactics between two or more states in open confrontation. The forces on each side are well-defined, and fight using weapons that primarily target the opponent's military. It is normally fought using conventional weapons, and not with chemical, biological, or nuclear weapons.
The general purpose of conventional warfare is to weaken or destroy the opponent's military, thereby negating its ability to engage in conventional warfare. In forcing capitulation, however, one or both sides may eventually resort to unconventional warfare tactics.
Formation of the state.
The state was first advocated by Plato, then found more acceptance in the consolidation of power under the Roman Catholic Church. European monarchs then gained power as the Catholic Church was stripped of temporal power and was replaced by the divine right of kings. In 1648, the powers of Europe signed the Treaty of Westphalia which ended the religious violence for purely political governance and outlook, signifying the birth of the modern 'state'.
Within this statist paradigm, only the state and its appointed representatives were allowed to bear arms and enter into war. In fact, war was only understood as a conflict between sovereign states. Kings strengthened this idea and gave it the force of law. Whereas previously any noble could start a war, the monarchs of Europe of necessity consolidated military power in response to the Napoleonic war.
The Clausewitzian paradigm.
Prussia was one country attempting to amass military power. Carl von Clausewitz, one of Prussia's officers, wrote "On War", a work rooted solely in the world of the state. All other forms of intrastate conflict, such as rebellion, are not accounted for because in theoretical terms, Clausewitz could not account for warfare before the state. However, near the end of his life, Clausewitz grew increasingly aware of the importance of non-state military actors. This is revealed in his conceptions of "the people in arms" which he noted arose from the same social and political sources as traditional inter-state warfare.
Practices such as raiding or blood feuds were then labeled criminal activities and stripped of legitimacy. This war paradigm reflected the view of most of the modernized world at the beginning of the 21st century, as verified by examination of the conventional armies of the time: large, high maintenance, technologically advanced armies designed to compete against similarly designed forces.
Clausewitz also forwarded the issue of casus belli. While previous wars were fought for social, religious, or even cultural reasons, Clausewitz taught that war is merely "a continuation of politics by other means." It is a rational calculation in which states fight for their interests (whether they are economic, security-related, or otherwise) once normal discourse has broken down.
Prevalence.
The majority of modern wars have been conducted using the means of conventional warfare. Confirmed use of biological warfare by a nation state has not occurred since 1945, and chemical warfare has been used only a few times (the latest known confrontation in which it was utilized being the Syrian Civil War). Nuclear warfare has only occurred once with the United States bombing the Japanese cities of Hiroshima and Nagasaki in August 1945.
Decline.
The state and Clausewitzian principles peaked in the World Wars of the 20th century, but also laid the groundwork for their dilapidation due to nuclear proliferation and the manifestation of culturally aligned conflict. The nuclear bomb was the result of the state perfecting its quest to overthrow its competitive duplicates. Ironically, this development seems to have pushed conventional conflict waged by the state to the sidelines. Were two conventional armies to fight, the loser would have redress in its nuclear arsenal.
Thus, no two nuclear powers have yet fought a conventional war directly, with the exception of brief skirmishes between for example, China and Russia in the 1969 Sino-Soviet conflict and between India and Pakistan in the 1999 Kargil War.
Replacement.
Conventional warfare, waged by the state, has become something not worthy of a declaration of war. Instead, those capable of fighting underneath the nuclear umbrella (supranational terrorists, corporate mercenaries, ethnic militias, and so on.) have now come to dominate the majority of conflict in the post-modern era. These conflicts cannot be explained under the statist system.
Samuel Huntington has posited that the world in the early 21st century exists as a system of nine distinct "civilizations," instead of many sovereign states. These civilizations are delineated along cultural lines, for example, Western, Islamic, Sinic, Hindu, Buddha and so on. In this way, cultures that have long been dominated by the West are reasserting themselves and looking to challenge the status quo. Thus, culture has replaced the state as the locus of war. This kind of civilizational war, in our time as in times long past, occurs where these cultures buffet up against one another. Some high-profile examples are the Pakistan/India conflict or the battles in the Sudan. This sort of war has defined the field since World War II.
These cultural forces will not contend with state-based armies in the traditional way. When faced with battalions of tanks, jets, and missiles, the cultural opponent dissolves away into the population. They benefit from the territorially constrained states, being able to move freely from one country to the next, while states must negotiate with other sovereign states. The state's spy networks are also severely limited by cultural factors.
See also.
Contrast:

</doc>
<doc id="7312" url="http://en.wikipedia.org/wiki?curid=7312" title="Chauvinism">
Chauvinism

Chauvinism, in its original meaning, is an exaggerated patriotism and a belligerent belief in national superiority and glory.
According to legend, French soldier Nicolas Chauvin was badly wounded in the Napoleonic wars. He received a pension for his injuries but it was not enough to live on. After Napoleon abdicated, Chauvin was a fanatical Bonapartist despite the unpopularity of this view in Bourbon Restoration France. His single-minded blind devotion to his cause, despite neglect by his faction and harassment by its enemies, started the use of the term.
By extension, it has come to include an extreme and unreasoning partisanship on behalf of any group to which one belongs, especially when the partisanship includes malice and hatred towards rival groups. Jingoism is the British parallel form of this French word, but its meaning has not expanded beyond nationalism in the same way that the word "chauvinism" has.
A contemporary use of the term in English is in the phrase "male chauvinism."
Chauvinism as nationalism.
In 1945, political theorist Hannah Arendt described the concept thus:
Technical Chauvinism has been used for those examples where inventors of a particular nationality have been idolised, one case being that of the ship's propeller. It had no sole inventor, but claims have been made for the Swede John Ericsson and the Czech Josef Ressel. The latter even has a national monument dedicated to him.
Male chauvinism.
Male chauvinism is the belief that men are superior to women. The first documented use of the phrase "male chauvinism" is in the 1935 Clifford Odets play "Till the Day I Die".
In the workplace.
The balance of the workforce changed during World War II through the dramatic rise of women’s participation as men left their positions to enlist in the military and fight in the war. After the war ended and men returned home to find jobs in the workplace, male chauvinism was on the rise according to Cynthia B. Lloyd. Previously, men had been the main source of labour, and they expected to come back to their previous employment, but women had stepped into many of their positions to fill the void says Lloyd. 
Lloyd and Michael Korda have argued that as they integrated back into the workforce, men returned to predominantly holding positions of power, and women worked as their secretaries, usually typing dictations and answering telephone calls. This division of labor was understood and expected, and women typically felt unable to challenge their position or male superiors, argue Korda and Lloyd.
Religio-cultural and geographic spread.
Although Hindu religion and Indian cultural practice does not strictly dictate the status of women, many conservative leaders and gurus continue to hold and espouse deeply misogynistic views publicly, leading to clashes with more liberal Indians, both verbal and otherwise.
Causes.
Chauvinism is seen by some as an influential factor in the TAT, a psychological personality test. Through cross-examinations, the TAT exhibits a tendency toward chauvinistic stimuli for its questions and has the "potential for unfavorable clinical evaluation" for women.
An often cited study done in 1976 by Sherwyn Woods, Some Dynamics of Male Chauvinism, attempts to find the underlying causes of male chauvinism.
Female chauvinism.
The term "female chauvinism" has been adopted by critics of some types or aspects of feminism; second-wave feminist Betty Friedan is a notable example. Ariel Levy used the term in similar, but opposite sense in her book, "Female Chauvinist Pigs," in which she argues that many young women in the United States and beyond are replicating male chauvinism and older misogynist stereotypes.

</doc>
<doc id="7316" url="http://en.wikipedia.org/wiki?curid=7316" title="Hypothetical types of biochemistry">
Hypothetical types of biochemistry

Hypothetical types of biochemistry are forms of biochemistry speculated to be scientifically viable but not proven to exist at this time. While the kinds of living beings currently known on Earth commonly use carbon for basic structural and metabolic functions, water as a solvent and DNA or RNA to define and control their form, it may be possible that undiscovered life-forms could exist that differ radically in their basic structures and biochemistry from that known to science.
The possibility of extraterrestrial life being based on these "alternative" biochemistries is a common subject in science fiction, but is also discussed in a non-fiction scientific context.
Shadow biosphere.
Apart from the prospect of finding different forms of life on other planets or moons, Earth itself has been suggested as a place where a shadow biosphere of biochemically unfamiliar micro-organisms might have lived in the past, or may still exist today.
Alternative-chirality biomolecules.
Perhaps the least unusual alternative biochemistry would be one with differing chirality of its biomolecules. In known Earth-based life, amino acids are almost universally of the form and sugars are of the form. Molecules of opposite chirality have identical chemical properties to their mirrored forms, so life that used amino acids or sugars may be possible; molecules of such a chirality, however, would be incompatible with organisms using the opposing chirality molecules. It is questionable, however, whether such a biochemistry would be truly alien; while it is certainly an alternative stereochemistry, molecules that are overwhelmingly found in one enantiomer throughout the vast majority of organisms can nonetheless often be found in another enantiomer in different (often basal) organisms such as in comparisons between members of Archea and other domains, making it an open topic whether an alternative stereochemistry is truly novel.
Non-carbon-based biochemistries.
On Earth, all known living things have a carbon-based structure and system. Scientists have speculated about the pros and cons of using atoms other than carbon to form the molecular structures necessary for life, but no one has proposed a theory employing such atoms to form all the necessary structures. However, as Carl Sagan argued, it is very difficult to be certain whether a statement that applies to all life on Earth will turn out to apply to all life throughout the universe. Sagan used the term "carbon chauvinism" for such an assumption. Carl Sagan regarded silicon and germanium as conceivable alternatives to carbon; but, on the other hand, he noted that carbon does seem more chemically versatile and is more abundant in the cosmos.
Silicon biochemistry.
The most commonly proposed basis for an alternative biochemical system is the silicon atom, because silicon has many chemical properties similar to those of carbon and is in the same group of the periodic table, the carbon group. Like carbon, silicon can create molecules that are sufficiently large to carry biological information.
However, silicon has several drawbacks as an alternative to carbon. Silicon, unlike carbon, lacks the ability to form chemical bonds with diverse types of atoms as is necessary for the chemical versatility required for metabolism. Elements creating organic functional groups with carbon include hydrogen, oxygen, nitrogen, phosphorus, sulfur, and metals such as iron, magnesium, and zinc. Silicon, on the other hand, interacts with very few other types of atoms. Moreover, where it does interact with other atoms, silicon creates molecules that have been described as "monotonous compared with the combinatorial universe of organic macromolecules". This is because silicon atoms are much bigger, having a larger mass and atomic radius, and so have difficulty forming double bonds (the double bonded carbon is part of the carbonyl group, a fundamental motif of bio-organic chemistry).
Silanes, which are chemical compounds of hydrogen and silicon that are analogous to the alkane hydrocarbons, are highly reactive with water, and long-chain silanes spontaneously decompose. Molecules incorporating polymers of alternating silicon and oxygen atoms instead of direct bonds between silicon, known collectively as silicones, are much more stable. It has been suggested that silicone-based chemicals would be more stable than equivalent hydrocarbons in a sulfuric-acid-rich environment, as is found in some extraterrestrial locations. Complex long-chain silicone molecules are still less stable than their carbon counterparts, though.
Finally, of the varieties of molecules identified in the interstellar medium , 84 are based on carbon while only 8 are based on silicon. Moreover, of those 8 compounds, four also include carbon within them. The cosmic abundance of carbon to silicon is roughly 10 to 1. This may suggest a greater variety of complex carbon compounds throughout the cosmos, providing less of a foundation upon which to build silicon-based biologies, at least under the conditions prevalent on the surface of planets. Somewhat in support, in September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar-medium conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". (Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.")
Also, even though Earth and other terrestrial planets are exceptionally silicon-rich and carbon-poor (the relative abundance of silicon to carbon in the Earth's crust is roughly 925:1), terrestrial life is carbon-based. The fact that carbon is used instead of silicon, may be evidence that silicon is poorly suited for biochemistry on Earth-like planets. For example: silicon is less versatile than carbon in forming compounds; the compounds formed by silicon are unstable and it blocks the flow of heat. Even so, biogenic silica is used by some Earth life, such as the silicate skeletal structure of diatoms. Silicon compounds may possibly be biologically useful under temperatures or pressures different from the surface of a terrestrial planet, either in conjunction with or in a role less directly analogous to carbon.
A. G. Cairns-Smith has proposed that the first living organisms to exist on Earth were clay minerals—which were probably based on silicon.
In cinematic and literary science fiction, at a moment when man-made machines cross from nonliving to living, it is often posited, this new form would be the first example of non-carbon-based life. Since the advent of the microprocessor in the late 1960s, these machines are often classed as computers (or computer-guided robots) and filed under "silicon-based life", even though the silicon backing matrix of these processors is not nearly as fundamental to their operation as carbon is for "wet life".
Arsenic as an alternative to phosphorus.
Arsenic, which is chemically similar to phosphorus, while poisonous for most life forms on Earth, is incorporated into the biochemistry of some organisms. Some marine algae incorporate arsenic into complex organic molecules such as arsenosugars and arsenobetaines. Fungi and bacteria can produce volatile methylated arsenic compounds. Arsenate reduction and arsenite oxidation have been observed in microbes ("Chrysiogenes arsenatis"). Additionally, some prokaryotes can use arsenate as a terminal electron acceptor during anaerobic growth and some can utilize arsenite as an electron donor to generate energy.
It has been speculated that the earliest life forms on Earth may have used arsenic in place of phosphorus in the structure of their DNA. A common objection to this scenario is that arsenate esters are so much less stable to hydrolysis than corresponding phosphate esters that arsenic would not be suitable for this function.
The authors of a 2010 geomicrobiology study, supported in part by NASA, have postulated that a bacterium, named GFAJ-1, collected in the sediments of Mono Lake in eastern California, can employ such 'arsenic DNA' when cultured without phosphorus. They proposed that the bacterium may employ high levels of poly-β-hydroxybutyrate or other means to reduce the effective concentration of water and stabilize its arsenate esters. This claim was heavily criticized almost immediately after publication for the perceived lack of appropriate controls. Science writer Carl Zimmer contacted several scientists for an assessment: "I reached out to a dozen experts ... Almost unanimously, they think the NASA scientists have failed to make their case".
Other authors were unable to reproduce their results and showed that the NASA scientists had issues with phosphate contamination (3 μM), which could sustain extremophile lifeforms.
Non-water solvents.
In addition to carbon compounds, all currently known terrestrial life also requires water as a solvent. This has led to discussions about whether water is the only liquid capable of filling that role. The idea that an extraterrestrial life-form might be based on a solvent other than water has been taken seriously in recent scientific literature by the biochemist Steven Benner, and by the astrobiological committee chaired by John A. Baross. Solvents discussed by the Baross committee include ammonia, sulfuric acid,
formamide, hydrocarbons,
and (at temperatures much lower than Earth's) liquid nitrogen, or hydrogen in the form of a supercritical fluid.
Carl Sagan once described himself as both a carbon chauvinist and a water chauvinist;
however on another occasion he said he was a carbon chauvinist but "not that much of a water chauvinist". He considered hydrocarbons, hydrofluoric acid, and ammonia as possible alternatives to water.
Some of the properties of water that are important for life processes include a large temperature range over which it is liquid, a high heat capacity (useful for temperature regulation), a large heat of vaporization, and the ability to dissolve a wide variety of compounds. Water is also amphoteric, meaning it can donate and accept an H+ ion, allowing it to act as an acid or a base. This property is crucial in many organic and biochemical reactions, where water serves as a solvent, a reactant, or a product. There are other chemicals with similar properties that have sometimes been proposed as alternatives. Additionally, water has the unusual property of being less dense as a solid (ice) than as a liquid. This is why bodies of water freeze over but do not freeze solid (from the bottom up). If ice were denser than liquid water (as is true for nearly all other compounds), then large bodies of liquid would slowly freeze solid, which would not be conducive to the formation of life.
Not all properties of water are necessarily advantageous for life, however. For instance, water ice has a high albedo, meaning that it reflects a significant quantity of light and heat from the Sun. During ice ages, as reflective ice builds up over the surface of the water, the effects of global cooling are increased.
There are some properties that make certain compounds and elements much more favorable than others as solvents in a successful biosphere. The solvent must be able to exist in liquid equilibrium over a range of temperatures the planetary object would normally encounter. Because boiling points vary with the pressure, the question tends not to be "does" the prospective solvent remain liquid, but "at what pressure". For example, hydrogen cyanide has a narrow liquid phase temperature range at 1 atmosphere, but in an atmosphere with the pressure of Venus, with of pressure, it can indeed exist in liquid form over a wide temperature range.
Ammonia.
The ammonia molecule (NH3), like the water molecule, is abundant in the universe, being a compound of hydrogen (the simplest and most common element) with another very common element, nitrogen. The possible role of liquid ammonia as an alternative solvent for life is an idea that goes back at least to 1954, when J.B.S. Haldane raised the topic at a symposium about life's origin.
Numerous chemical reactions are possible in an ammonia solution, and liquid ammonia has chemical similarities with water. Ammonia can dissolve most organic molecules at least as well as water does and, in addition, it is capable of dissolving many elemental metals. Haldane made the point that various common water-related organic compounds have ammonia-related analogs; for instance the ammonia-related amine group (-NH2) is analogous to the water-related alcohol group (-OH).
Ammonia, like water, can either accept or donate an H+ ion. When ammonia accepts an H+, it forms the ammonium cation (NH4+), analogous to hydronium (H3O+). When it donates an H+ ion, it forms the amide anion (NH2−), analogous to the hydroxide anion (OH−). Compared to water, however, ammonia is more inclined to accept an H+ ion, and less inclined to donate one; it is a stronger nucleophile. Ammonia added to water functions as Arrhenius base: it increases the concentration of the anion hydroxide. Conversely, using a solvent system definition of acidity and basicity, water added to liquid ammonia functions as an acid, because it increases the concentration of the cation ammonium. The carbonyl group (C=O), which is much used in terrestrial biochemistry, would not be stable in ammonia solution, but the analogous imine group (C=N) could be used instead.
However, ammonia has some problems as a basis for life. The hydrogen bonds between ammonia molecules are weaker than those in water, causing ammonia's heat of vaporization to be half that of water, its surface tension to be a third, and reducing its ability to concentrate non-polar molecules through a hydrophobic effect. Gerald Feinberg and Robert Shapiro have questioned whether ammonia could hold prebiotic molecules together well enough to allow the emergence of a self-reproducing system. Ammonia is also flammable in oxygen, and could not exist sustainably in an environment suitable for aerobic metabolism.
A biosphere based on ammonia would likely exist at temperatures or air pressures that are extremely unusual in relation to life on Earth. Life on Earth usually exists within the melting point and boiling point of water at normal pressure, between 0 °C (273 K) and 100 °C (373 K); at normal pressure ammonia's melting and boiling points are between −78 °C (195 K) and −33 °C (240 K). Chemical reactions generally proceed more slowly at a lower temperature. Therefore, ammonia-based life, if it exists, might metabolize more slowly and evolve more slowly than life on Earth. On the other hand, lower temperatures could also enable living systems to use chemical species which at Earth temperatures would be too unstable to be useful.
Ammonia could be a liquid at Earth-like temperatures, but at much higher pressures; for example, at 60 atm, ammonia melts at −77 °C (196 K) and boils at 98 °C (371 K).
Ammonia and ammonia–water mixtures remain liquid at temperatures far below the freezing point of pure water, so such biochemistries might be well suited to planets and moons orbiting outside the water-based habitability zone. Such conditions could exist, for example, under the surface of Saturn's largest moon Titan.
Methane and other hydrocarbons.
Methane (CH4) is a simple hydrocarbon: that is, a compound of two of the most common elements in the cosmos, hydrogen and carbon. It has a cosmic abundance comparable with ammonia. Hydrocarbons could act as a solvent over a wide range of temperatures, but would lack polarity. Isaac Asimov, the biochemist and science fiction writer, suggested in 1981 that poly-lipids could form a substitute for proteins in a non-polar solvent such as methane. Lakes composed of a mixture of hydrocarbons, including methane and ethane, have been detected on Titan by the Cassini spacecraft.
There is debate about the effectiveness of methane and other hydrocarbons as a medium for life compared to water or ammonia. Water is a stronger solvent than the hydrocarbons, enabling easier transport of substances in a cell. However, water is also more chemically reactive, and can break down large organic molecules through hydrolysis. A life-form whose solvent was a hydrocarbon would not face the threat of its biomolecules being destroyed in this way. Also, the water molecule's tendency to form strong hydrogen bonds can interfere with internal hydrogen bonding in complex organic molecules. Life with a hydrocarbon solvent could make more use of hydrogen bonds within its biomolecules. Moreover, the strength of hydrogen bonds within biomolecules would be appropriate to a low-temperature biochemistry.
Astrobiologist Chris McKay has argued, on thermodynamic grounds, that if life does exist on Titan's surface, using hydrocarbons as a solvent, it is likely also to use the more complex hydrocarbons as an energy source by reacting them with hydrogen, reducing ethane and acetylene to methane. Possible evidence for this form of life on Titan was identified in 2010 by Darrell Strobel of Johns Hopkins University; a greater abundance of molecular hydrogen in the upper atmospheric layers of Titan compared to the lower layers, arguing for a downward diffusion at a rate of roughly 1025 molecules per second and disappearance of hydrogen near Titan's surface. As Strobel noted, his findings were in line with the effects Chris McKay had predicted if methanogenic life-forms were present. The same year, another study showed low levels of acetylene on Titan's surface, which were interpreted by Chris McKay as consistent with the hypothesis of organisms reducing acetylene to methane. While restating the biological hypothesis, McKay cautioned that other explanations for the hydrogen and acetylene findings are to be considered more likely: the possibilities of yet unidentified physical or chemical processes (e.g. a non-living surface catalyst enabling acetylene to react with hydrogen), or flaws in the current models of material flow. He noted that even a non-biological catalyst effective at 95 K would in itself be a startling discovery.
Hydrogen fluoride.
Hydrogen fluoride (HF), like water, is a polar molecule, and due to its polarity it can dissolve many ionic compounds. Its melting point is −84 °C and its boiling point is 19.54 °C (at atmospheric pressure); the difference between the two is a little more than 100 K. HF also makes hydrogen bonds with its neighbor molecules, as do water and ammonia. It has been considered as a possible solvent for life by scientists such as Peter Sneath and Carl Sagan.
The biota in an HF ocean could use the fluorine as an electron acceptor to photosynthesize energy. HF is dangerous to the systems of molecules that Earth-life is made of, but certain other organic compounds, such as paraffin waxes, are stable with it. Like water and ammonia, liquid hydrogen fluoride supports an acid-base chemistry. Using a solvent system definition of acidity and basicity, nitric acid functions as a base when it is added to liquid HF.
However, hydrogen fluoride, unlike water, ammonia and methane, is cosmically rare.
Other solvents or cosolvents.
Other solvents sometimes proposed:
Hydrogen sulfide is the closest chemical analog to water, but is less polar and a weaker inorganic solvent. Hydrogen sulfide and hydrogen chloride are cosmically rarer than water and ammonia. Nonetheless, hydrogen sulfide is quite plentiful on Jupiter's moon Io, and may be in liquid form a short distance below the surface; and astrobiologist Dirk Schulze-Makuch has suggested it as a possible solvent for life there.
Sulfuric acid in liquid form is strongly polar. It is known to be abundant in the clouds of Venus, in the form of aerosol droplets. In a biochemistry that used sulfuric acid as a solvent, the alkene group (C=C), with two carbon atoms joined by a double bond, could function analogously to the carbonyl group (C=O) in water-based biochemistry.
A proposal has been made that life on Mars may exist and be using a mixture of water and hydrogen peroxide as its solvent. A 61.2% (by weight) mix of water and hydrogen peroxide has a freezing point of −56.5 °C, and also tends to super-cool rather than crystallize. It is also hygroscopic, an advantage in a water-scarce environment.
Other types of speculations.
Non-green photosynthesizers.
Physicists have noted that, although photosynthesis on Earth generally involves green plants, a variety of other-colored plants could also support photosynthesis, essential for most life on Earth, and that other colors might be preferred in places that receive a different mix of stellar radiation than Earth. These studies indicate that, although blue photosynthetic plants would be less likely (because absorbed blue light provides some of the highest photosynthetic yields in the light spectrum), yellow or red plants are plausible. These conclusions are, in part, based on the luminosity spectra of different types of stars, the transmission characteristics of hypothetical planetary atmospheres, and the absorption spectra of various photosynthetic pigments from organisms on Earth.
Alternative atmospheres.
The gases present in the atmosphere on Earth have varied greatly over its history. Traditional plant photosynthesis transformed the atmosphere by sequestering carbon from carbon dioxide, increasing the proportion of molecular oxygen, and by participating in the nitrogen cycle. Modern oxygen-breathing animals would have been biochemically impossible until earlier photosynthetic life transformed Earth's atmosphere. The first dramatic rise in atmospheric oxygen on Earth, to about a tenth of its present-day value, occurred approximately 2.5 billion years ago, and that level did not change significantly until the Cambrian era approximately 600 million years ago.
Changes in the gas mixture in the atmosphere, even in an atmosphere made up predominantly of the same molecules of Earth's atmosphere, impacts the biochemistry and morphology of life. For example, periods of high oxygen concentrations (determined from ice core samples) have been associated with larger fauna in the fossil record, whereas periods of low oxygen concentrations have been associated with smaller fauna in the fossil record.
Also, although it is customary to think of plants on one side of the oxygen and nitrogen cycles as being sessile, and of animals on the other side as being motile, this is not a biological imperative. There are animals that are sessile for all or most of their lives (such as corals), and there are plants (such as tumbleweeds, and venus fly traps) that exhibit more mobility than is customarily associated with plants. On a slowly rotating planet, for example, it might be adaptive for photosynthesis to be performed by "plants" that can move to remain in the light, like Earth's sunflowers; whereas non-photosynthetic "animals", much like Earth's fungi, might have a lesser need to move from place to place on their own. This would be a mirror image of Earth's ecology.
Variable environments.
Many Earth plants and animals undergo major biochemical changes during their life cycles as a response to changing environmental conditions, for example, by having a spore or hibernation state that can be sustained for years or even millennia between more active life stages. Thus, it would be biochemically possible to sustain life in environments that are only periodically consistent with life as we know it.
For example, frogs in cold climates can survive for extended periods of time with most of their body water in a frozen state, whereas desert frogs in Australia can become inactive and dehydrate in dry periods, losing up to 75% of their fluids, yet return to life by rapidly rehydrating in wet periods. Either type of frog would appear biochemically inactive (i.e. not living) during dormant periods to anyone lacking a sensitive means of detecting low levels of metabolism.
Nonplanetary life.
Dust and plasma-based.
In 2007, Vadim N. Tsytovich and colleagues proposed that lifelike behaviors could be exhibited by dust particles suspended in a plasma, under conditions that might exist in space. Computer models showed that, when the dust became charged, the particles could self-organize into microscopic helical structures capable of replicating themselves, interacting with other neighboring structures,
and evolving into more stable forms. Similar forms of life were described in Fred Hoyle's classic novel "The Black Cloud".
In fiction.
In the realm of science fiction, there have occasionally been forms of life proposed that, while often highly speculative and unsupported by rigorous theoretical examination, are nevertheless interesting and in some cases even plausible.
In Arthur C. Clarke's short story "Technical Error", there is an example of differing chirality. This is not a case of alien life, rather it is an accident. The concept of reversed chirality also figured prominently in the plot of James Blish's "Star Trek" novel "Spock Must Die!", where a transporter experiment gone awry ends up creating a duplicate Spock who turns out to be a perfect mirror-image of the original all the way down to the atomic level.
An example of silicon based life forms takes place in the Alan Dean Foster novel "Sentenced to Prism" in which the protagonist, Evan Orgell, is trapped on a planet whose entire ecosystem is mostly silicon-based.
Perhaps the most extreme example in science fiction is James White's "Sector General": a series of novels and short stories about multienvironment hospital for the strangest life-forms imaginable, some of them breathing methane, chlorine, water and sometimes also oxygen. Some of the species directly "metabolise" hard radiation and their environment doesn't differ much from the atmosphere of a star, while others live in near absolute zero temperatures. All life forms are classified according to their metabolism, internal and external features, and more extreme abilities (telepathy, empathy, hive mind, etc.) with four letter codes.
One of the major sentient species in Terry Pratchett's "Discworld" universe are the "Earth"-based (ranging from Detritus to Diamond) Trolls. Fred Hoyle's classic novel "The Black Cloud" features a life form consisting of a vast cloud of interstellar dust, the individual particles of which interact via electromagnetic signalling analogous to how the individual cells of multicellular terrestrial life interact.
Outside science-fiction, life in interstellar dust has been proposed as part of the panspermia hypothesis. The low temperatures and densities of interstellar clouds would seem to imply that life processes would operate much more slowly there than on Earth. Inorganic dust-based life has been speculated upon based on recent computer simulations. Similarly, Arthur C. Clarke's "Crusade" revolves around a planetwide life-form based on silicon and superfluid helium located in deep intergalactic space, processing its thoughts slowly by human standards, that sends probes to look for similar life in nearby galaxies. It concludes that it needs to make planets more habitable for similar life-forms, and sends out other probes to foment supernovae to do so.
Robert L. Forward's "Camelot 30K" describes an ecosystem on the surface of Kuiper belt objects that is based on a fluorocarbon chemistry with OF2 as the principal solvent instead of H2O. The organisms in this ecology keep warm by secreting a pellet of uranium-235 inside themselves and then moderating its nuclear fission using a boron-rich carapace around it. Kuiper belt objects are known to be rich in organic compounds, such as tholins, so some form of life existing on their surfaces is not entirely implausible–though perhaps not going so far as to develop natural internal nuclear reactors, as have Forward's. In Forward's "Rocheworld" series, an Earth-like biochemistry is proposed that uses a mixture of water and ammonia as its solvent. In "Dragon's Egg" and "Starquake", Forward proposes life on the surface of a neutron star utilizing "nuclear chemistry" in the degenerate matter crust. Since such life utilised strong nuclear forces instead of electromagnetic interactions, it was posited that life might function millions of times faster than typical on Earth.
Gregory Benford and David Brin's "Heart of the Comet" features a comet with a conventional carbon-and-water-based ecosystem that becomes active near the perihelion when the Sun warms it. Brin's own novel "Sundiver" is an example of science fiction proposing a form of life existing within the plasma atmosphere of a star using complex self-sustaining magnetic fields. Gregory Benford had a form of plasma-based life exist in the accretion disk of a primordial black hole in his novel "Eater". The suggestion that life could even occur within the plasma of a star has been picked up by other science fiction writers, as in David Brin's Uplift Saga or Frederik Pohl's novel "The World at the End of Time". The idea is that places where reactions occur–even an incredible environment as a star–presents a possible medium for some chain of events that could produce a system able to replicate.
The Outsiders in Larry Niven's Known Space universe are cryogenic creatures based on liquid helium. They derive thermoelectric energy from a temperature gradient by basking half their body in sunlight, keeping the other half in shadow and exposed to interstellar vacuum.
Stephen Baxter has, perhaps, imagined some of the most unusual exotic life-forms in his Xeelee series of novels and stories, including supersymmetric photino-based life that congregate in the gravity wells of stars, entities composed of quantum wave functions, and the Qax, who thrive in any form of convection cells, from swamp gas to the atmospheres of gas giants. In his book "", he also proposes natural robots, life forms made of iron, called the Gaijin, evolving from creatures in oceans of iron carbonyl.
The sentient ocean that covers much of the surface of Solaris in Stanislaw Lem's eponymous novel also seems, from much of the fictional research quoted and discussed in the book, to be based on some element other than carbon.
In his novel "Diaspora", Greg Egan posits entire virtual universes implemented on Turing Machines encoded by Wang Tiles in gargantuan polysaccharide 'carpets.' In the same novel, Egan describes lifeforms in the 6-D 'macrosphere' that use a collapsed atom chemistry with energetic processes of the same order as nuclear reactions, due to the peculiarities of higher-dimensional physics.
The webcomic "Schlock Mercenary" features a species, Carbosilicate Amorphs, evolved from self-repairing distributed data storage devices.
Alien warriors recruited by the god Klael in David Eddings' "Tamuli" trilogy are noted by their human opponents to breathe marsh-gas (methane). Within Eddings' universe, this limits their capacity for exertion in an oxygen atmosphere, and also determines the tactics used to fight them and eventually to destroy them in their encampments.
The eponymous organism in Michael Crichton's "The Andromeda Strain" is described as reproducing via the direct conversion of energy into matter.
"Star Trek".
A well-known example of a non–carbon-based life-form in science fiction is the Horta in the original "" episode "The Devil in the Dark". A highly intelligent silicon-based creature made almost entirely of pure rock, it tunnels through rock as easily as humans move through air. The entire species dies out every 50,000 years except for one who tends the eggs, which take the form of silicon nodules scattered throughout the caverns and tunnels of its home planet, . The inadvertent destruction of many of these eggs by a human mining colony led the mother Horta to respond by killing the colonists and sabotaging their equipment; it was only through a Vulcan mind meld that the race's benevolence and intelligence were discovered and peaceful relations established.
"Star Trek" would later offer other corporeal life-forms with an alternative biochemistry. The Tholians of "The Tholian Web" are depicted and described, in that episode and later in the ' episode "In a Mirror, Darkly" as being primarily of mineral-based composition and thriving only in superheated conditions. Another episode from "TOSs third season, "The Savage Curtain", depicted another rock creature called an Excalbian, which is believed in fanon to also have been silicon-based.
In "", the Crystalline Entity appeared in two episodes, "Datalore" and "Silicon Avatar". This was an enormous spacefaring crystal lattice that had taken thousands of lives in its quest for energy. It may have been unaware of this, however, but it was destroyed before communications could be established at a level sufficient to ascertain it. In another episode, "Home Soil", intelligent crystals that formed a "microbrain" were discovered during a terraforming mission, and they described the humans they encountered as "ugly bags of mostly water."
"The Disease", an episode of "" featured some artificially engineered silicon-based parasites, and an "Enterprise" episode, "Observer Effect", also presented a lethal silicon-based virus.
In another Voyager episode, "Hope and Fear", a xenon-based life-form was mentioned. In the "Enterprise" episode "", an alien species is encountered whose blood chemistry, while not explicitly stated, is sufficiently different from terrestrial organisms that it is not red and iron is toxic to it. Various Star Trek series also had episodes featuring photonic lifeforms.
"Star Wars".
In the "Star Wars" movie "The Empire Strikes Back", two life-forms were encountered by the characters that were non-carbon based entities. Although details of their physiology were not mentioned on screen, the space slug, (a giant worm-like creature that lived on asteroids in the vacuum of space), and the Mynock, (pesky bat-like vermin that would attach to spaceship hulls and chew through power conduits to feed off the raw energy),< are said to be silicon-based organisms in Star Wars Expanded Universe sources. Also from "The Empire Strikes Back", the bounty hunter Zuckuss is a member of the Gand race, an ammonia-based life-form. However, it is worth noting that the Gand are divided into two subspecies, only one of which breathes at all, the other drawing all their required sustenance from food intake and producing speech by essentially modulated flatulence.
Appearing only in the Star Wars Expanded Universe is the spice spider of Kessel, a creature made of glitterstim spice and silicon that spun crystalline webs harvested by miners as glitterstim spice, an illegal psychoactive narcotic. The spider used the webs to catch bogeys, tiny energy creatures that it consumed for energy.
Computer and video games.
In the "Command & Conquer" real-time strategy games, both the gameplay and storyline revolve heavily around the introduction to Earth, via meteor, of the extraterrestrial mutagen Tiberium, which displays strikingly lifelike behaviours, such as self-replication, evolution, and homeostasis, without undergoing anything like common carbon-based metabolic cycles, and which appears to be colonising the Earth, converting it into an environment unsuited to carbon-based biology. Earth creatures (such as animals, plants and even humans) exposed to Tiberium can either be killed because of the radiation or be transformed into Tiberium-based life-forms, to whom Tiberium radiation is curative rather than toxic. It is later revealed that Tiberium was introduced to Earth by the Scrin, an extremely advanced race of Tiberium-based aliens bent on mining the planet after the Tiberium deposits have reached maturity.
In the Halo franchise, the weak, low-ranking Grunts of the Covenant originate on a frozen exoplanet named Balaho, where methane is a primary constituent of the atmosphere, preventing the planet becoming even more frigid than it already is due to its distance from its parent star, and thus, Grunts have evolved to utilize the gas for respiration. Grunts are also shown to be able to use benzene as a recreational drug.
In the "Master of Orion" series of space strategy games, there exists an extraterrestrial race called Silicoids, whose appearance (and presumably composition) is similar to crystalline mineral structures. The game posits that this grants them immunity to the effects of hostile environments and pollution and they require no sustenance, at the expense of impeding their reproductive rate and their ability to interact with other intelligent species.
In the "Metroid Prime" series, Phazon is a highly radioactive, self-regenerating mineral with organic properties that is generated by the sentient planet Phaaze.
In "Metroid Prime Hunters", Spire is a rock-like, silicon-based alien. He is the last Diamont (presumably a play on the word diamond, which is composed of carbon).
In the "Star Control" series, the Chenjesu, are hyperintelligent, peaceful silicon-based life-forms that were the backbone of the Alliance of Free Stars. Their crystalline biology apparently gives them the ability to send and receive hyperwave transmissions. Also, there are the Slylandro, who are gas beings residing in the upper atmosphere of a gas giant. As well, there are evidences of another silicon-based race, the Taalo who are described by the xenophobic Ur-Quan as the only race to have not awakened their territorial instincts. The Taalo were also immune to mind control.
In the game of "Xenosaga", artificial life forms known as Realians have been created using silicon-based chemistry. They resemble humans in every aspect, except they are considered to be lower than humans on the social ladder.
In "Mass Effect" the alien Turians and Quarians are both based on dextro-amino acids, as opposed to all the other sentient species of the galaxy based on levo-amino acids. There are also the Volus, an ammonia based species that must wear pressure suits to survive in environments suited to the other races.
In "Spore", the Grox refer to the player and to other alien empires as "slow thinking carbon-based lifeforms" and "carbon wads", implying that the Grox (which are at least partly machine life) are not carbon-based. Also, the Grox can only exist on barren planets which cannot support other life, and when a planet is terraformed the Grox inhabiting it die immediately. The Grox seem to gather sustenance from the radiation from the galactic core, as the Grox colonies are larger the closer they are to the galactic core.
In "Muv Luv", the BETA which calls itself the "higher/superior existence" says they were created by a silicon-based being simply called "The Creator". As such, they don't consider any non-silicon-based creature to be alive, not even themselves. Its reasoning was that only silicon-based beings occur naturally and have the ability to reproduce and disperse. When the human main character, Takeru, argues that humans also have the ability to reproduce and disperse, the higher existence says carbon too easily mingles with other elements and therefore it would be impossible for a carbon-based existence to have evolved on its own. Thus, humans must be other biological machines created by a life form just as the BETA are.
Scientists who have published on this topic.
Scientists who have considered possible alternatives to carbon-water biochemistry include:

</doc>
<doc id="7322" url="http://en.wikipedia.org/wiki?curid=7322" title="Creation myth">
Creation myth

A creation myth is a symbolic narrative of how the world began and how people first came to inhabit it. While in popular usage the term "myth" often refers to false or fanciful stories, cultures regard their creation myths as true to varying degrees. In the society in which it is told, a creation myth is usually regarded as conveying profound truths, metaphorically, symbolically and sometimes in a historical or literal sense. They are commonly, although not always, considered cosmogonical myths—that is, they describe the ordering of the cosmos from a state of chaos or amorphousness.
Creation myths often share a number of features. They often are considered sacred accounts and can be found in nearly all known religious traditions. They are all stories with a plot and characters who are either deities, human-like figures, or animals, who often speak and transform easily. They are often set in a dim and nonspecific past, what historian of religion Mircea Eliade termed "in illo tempore" ("at that time"). Creation myths address questions deeply meaningful to the society that shares them, revealing their central worldview and the framework for the self-identity of the culture and individual in a universal context.
Creation myths develop in oral traditions and therefore typically have multiple versions and are the most common form of myth, found throughout human culture.
Definitions.
Creation myth definitions from modern references:
Religion professor Mircea Eliade defined the word "myth" in terms of creation:
Myth narrates a sacred history; it relates an event that took place in primordial Time, the fabled time of the "beginnings." In other words, myth tells how, through the deeds of Supernatural Beings, a reality came into existence, be it the whole of reality, the Cosmos, or only a fragment of reality – an island, a species of plant, a particular kind of human behavior, an institution.
Meaning and function.
All creation myths are in one sense etiological because they attempt to explain how the world was formed and where humanity came from.
Ethnologists and anthropologists who study these myths say that in the modern context theologians try to discern humanity's meaning from revealed truths and scientists investigate cosmology with the tools of empiricism and rationality, but creation myths define human reality in very different terms. In the past historians of religion and other students of myth thought of them as forms of primitive or early-stage science or religion and analyzed them in a literal or logical sense. However they are today seen as symbolic narratives which must be understood in terms of their own cultural context. Charles H. Long writes, "The beings referred to in the myth -- gods, animals, plants -- are forms of power grasped existentially. The myths should not be understood as attempts to work out a rational explanation of deity."
While creation myths are not literal explications they do serve to define an orientation of humanity in the world in terms of a birth story. They are the basis of a worldview that reaffirms and guides how people relate to the natural world, to any assumed spiritual world, and to each other. The creation myth acts as a cornerstone for distinguishing primary reality from relative reality, the origin and nature of being from non-being. In this sense they serve as a philosophy of life but one expressed and conveyed through symbol rather than systematic reason. And in this sense they go beyond etiological myths which mean to explain specific features in religious rites, natural phenomena or cultural life. Creation myths also help to orient human beings in the world, giving them a sense of their place in the world and the regard that they must have for humans and nature.
Historian David Christian has summarised issues common to multiple creation myths:
Each beginning seems to presuppose an earlier beginning. ... Instead of meeting a single starting point, we encounter an infinity of them, each of which poses the same problem. ... There are no entirely satisfactory solutions to this dilemma. What we have to find is not a solution but some way of dealing with the mystery ... And we have to do so using words. The words we reach for, from "God" to "gravity", are inadequate to the task. So we have to use language poetically or symbolically; and such language, whether used by a scientist, a poet, or a shaman, can easily be misunderstood. 
Classification.
Mythologists have applied various schemes to classify creation myths found throughout human cultures. Eliade and his colleague Charles H. Long developed a classification based on some common motifs that reappear in stories the world over. The classification identifies five basic types:
Marta Weigle further developed and refined this typology to highlight nine themes, adding elements such as "deus faber", a creation crafted by a deity, creation from the work of two creators working together or against each other, creation from sacrifice and creation from division/conjugation, accretion/conjunction, or secretion.
An alternative system based on six recurring narrative themes was designed by Raymond Van Over:
"Ex nihilo".
Creation "ex nihilo" (Latin "out of nothing"), also known as "creation "de novo"", is a common type of mythical creation. "Ex nihilo" creation is found in creation stories from ancient Egypt, the Rig Veda, the Bible and the Quran, and many animistic cultures in Africa, Asia, Oceania and North America. The Debate between sheep and grain is an example of an even earlier form of "ex nihilo" creation myth from ancient Sumer. In most of these stories the world is brought into being by the speech, dream, breath, or pure thought of a creator but creation "ex nihilo" may also take place through a creator's bodily secretions.
The literal translation of the phrase "ex nihilo" is "from nothing" but in many creation myths the line is blurred whether the creative act would be better classified as a creation "ex nihilo" or creation from chaos. In "ex nihilo" creation myths the potential and the substance of creation springs from within the creator. Such a creator may or may not be existing in physical surroundings such as darkness or water, but does not create the world from them, whereas in creation from chaos the substance used for creation is pre-existing within the unformed void.
A well known example of a creation myth is the one found in the Hebrew Bible.
Creation from chaos.
In creation from chaos myth, initially there is nothing but a formless, shapeless expanse. In these stories the word "chaos" means "disorder", and this formless expanse, which is also sometimes called a void or an abyss, contains the material with which the created world will be made. Chaos may be described as having the consistency of vapor or water, dimensionless, and sometimes salty or muddy. These myths associate chaos with evil and oblivion, in contrast to "order" ("cosmos") which is the good. The act of creation is the bringing of order from disorder, and in many of these cultures it is believed that at some point the forces preserving order and form will weaken and the world will once again be engulfed into the abyss.
World parent.
There are two types of world parent myths, both describing a separation or splitting of a primeval entity, the world parent or parents. One form describes the primeval state as an eternal union of two parents, and the creation takes place when the two are pulled apart. The two parents are commonly identified as Sky (usually male) and Earth (usually female) who in the primeval state were so tightly bound to each other that no offspring could emerge. These myths often depict creation as the result of a sexual union, and serve as genealogical record of the deities born from it.
In the second form of world parent myth, creation itself springs from dismembered parts of the body of the primeval being. Often in these stories the limbs, hair, blood, bones or organs of the primeval being are somehow severed or sacrificed to transform into sky, earth, animal or plant life, and other worldly features. These myths tend to emphasize creative forces as animistic in nature rather than sexual, and depict the sacred as the elemental and integral component of the natural world.
Emergence.
In emergence myths humanity emerges from another world into the one they currently inhabit. The previous world is often considered the womb of the earth mother, and the process of emergence is likened to the act of giving birth. The role of midwife is usually played by a female deity, like the spider woman of Native American mythology. Male characters rarely figure into these stories, and scholars often consider them in counterpoint to male oriented creation myths, like those of the ex nihilo variety.
Emergence myths commonly describe the creation of people and/or supernatural beings as a staged ascent or metamorphosis from nascent forms through a series of subterranean worlds to arrive at their current place and form. Often the passage from one world or stage to the next is impelled by inner forces, a process of germination or gestation from earlier, embryonic forms. The genre is most commonly found in Native American cultures where the myths frequently link the final emergence of people from a hole opening to the underworld to stories about their subsequent migrations and eventual settlement in their current homelands.
Earth-diver.
The earth-diver is a common character in various traditional creation myths. In these stories a supreme being usually sends an animal into the primal waters to find bits of sand or mud with which to build habitable land. Some scholars interpret these myths psychologically while others interpret them cosmogonically. In both cases emphasis is placed on beginnings emanating from the depths. Earth-diver myths are common in Native American folklore but can be found among the Chukchi and Yukaghir, the Tatars and many Finno-Ugrian traditions. The pattern of distribution of these stories suggest they have a common origin in the eastern Asiatic coastal region, spreading as peoples migrated west into Siberia and east to the North American continent.
Characteristic of many Native American myths, earth-diver creation stories begin as beings and potential forms linger asleep or suspended in the primordial realm. The earth-diver is among the first of them to awaken and lay the necessary groundwork by building suitable lands where the coming creation will be able to live. In many cases, these stories will describe a series of failed attempts to make land before the solution is found.

</doc>
<doc id="7324" url="http://en.wikipedia.org/wiki?curid=7324" title="Crucifix">
Crucifix

A crucifix (from Latin "cruci fixus" meaning "(one) fixed to a cross") is an image of Jesus on the cross, as distinct from a bare cross. The representation of Jesus himself attached to the cross is referred to in English as the "corpus" (Latin for "body").
The crucifix is a principal symbol for many groups of Christians, and one of the most common forms of the Crucifixion in the arts. It is especially important in the Latin Church of the Catholic Church, but is also used in Eastern Orthodox and Eastern Catholic Churches, in Coptic, Armenian and other Oriental Orthodox churches, as well as in Methodist, Lutheran and Anglican churches, but less often in churches of other Protestant denominations, which prefer to use a cross without the figure of Jesus (the "corpus"). The crucifix emphasizes Jesus' sacrifice — his death by crucifixion, which Christians believe brought about the redemption of mankind. Most crucifixes portray Jesus on a Latin cross, rather than any other shape, such as a Tau cross or a Coptic cross.
Western crucifixes usually have a three-dimensional "corpus", but in Eastern Orthodoxy Jesus' body is normally painted on the cross, or in low relief. Strictly speaking, to be a crucifix, the cross must be three-dimensional, but this distinction is not always observed. An entire painting of the Crucifixion of Jesus including a landscape background and other figures is not a crucifix either.
Large crucifixes high across the central axis of a church are known by the Old English term rood. By the late Middle Ages these were a near-universal feature of Western churches, but are now very rare. Modern Roman Catholic churches often have a crucifix above the altar on the wall; for the celebration of Mass, the Roman Rite of the Catholic Church requires that, "on or close to the altar there is to be a cross with a figure of Christ crucified".
Description.
The standard, four-pointed Latin crucifix consists of an upright post or "stipes" and a single crosspiece to which the sufferer's arms were nailed. There may also be a short projecting nameplate, showing the letters INRI (Greek: INBI). The Russian Orthodox crucifix usually has an additional third crossbar, to which the feet are nailed, and which is angled upward toward the penitent thief Saint Dismas (to the viewer's left) and downward toward the impenitent thief Gestas (to the viewer's right). The corpus of Eastern crucifixes is normally a two-dimensional or low relief icon that shows Jesus as already dead, his face peaceful and somber. They are rarely three-dimensional figures as in the Western tradition, although these may be found where Western influences are strong, but are more typically icons painted on a piece of wood shaped to include the double-barred cross and perhaps the edge of Christ's hips and halo, and no background. More sculptural small crucifixes in metal relief are also used in Orthodoxy (see gallery examples), including as pectoral crosses and blessing crosses.
Western crucifixes may show Christ dead or alive, the presence of the spear wound in his ribs traditionally indicating that he is dead. In either case his face very often shows his suffering. In Orthodoxy he has normally been shown as dead since around the end of the period of Byzantine Iconoclasm. Eastern crucifixes have Jesus' two feet nailed side by side, rather than crossed one above the other, as Western crucifixes have shown them for many centuries. The crown of thorns is also generally absent in Eastern crucifixes, since the emphasis is not on Christ's suffering, but on his triumph over sin and death. The "S"-shaped position of Jesus' body on the cross is a Byzantine innovation of the late 10th century, though also found in the German Gero Cross of the same date. Probably more from Byzantine influence, it spread elsewhere in the West, especially to Italy, by the Romanesque period, though it was more usual in painting than sculpted crucifixes. It's in Italy that the emphasis was put on Jesus' suffering and realistic details, during a process of general humanization of Christ favored by the Franciscan order. During the 13th century the suffering Italian model ("Christus patiens") triumphed over the traditional Byzantine one ("Christus gloriosus") anywhere in Europe also due to the works of artists such as Giunta Pisano and Cimabue. Since the Renaissance the "S"-shape is generally much less pronounced. Eastern Christian blessing crosses will often have the Crucifixion depicted on one side, and the Resurrection on the other, illustrating the understanding of Orthodox theology that the Crucifixion and Resurrection are two intimately related aspects of the same act of salvation.
Another, symbolic, depiction shows a triumphant Christ (), clothed in robes, rather than stripped as for His execution, with arms raised, appearing to rise up from the cross, sometimes accompanied by "rays of light", or an aureole encircling His Body. He may be robed as a prophet, crowned as a king, and vested in a stole as Great High Priest.
On some crucifixes a skull and crossbones are shown below the corpus, referring to Golgotha (Calvary), the site at which Jesus was crucified, which the Gospels say means in Hebrew "the place of the skull." Medieval tradition held that it was the burial-place of Adam and Eve, and that the cross of Christ was raised directly over Adam's skull, so many crucifixes manufactured in Catholic countries still show the skull and crossbones below the corpus.
Very large crucifixes have been built, the largest being the Cross in the Woods in Michigan, with a high statue.
Usage.
Prayer in front of a crucifix, which is seen as a sacramental, is often part of devotion for Christians, especially those worshipping in a church, and also privately. The person may sit, stand, or kneel in front of the crucifix, sometimes looking at it in contemplation, or merely in front of it with head bowed or eyes closed. During the Middle Ages small crucifixes, generally hung on a wall, became normal in the personal cells or living quarters first of monks, then all clergy, followed by the homes of the laity, spreading down from the top of society as these became cheap enough for the average person to afford. By the 19th century displaying a crucifix somewhere in the general reception areas of a house became typical of Catholic homes.
Roman Catholic (both Eastern and Western), Eastern Orthodox, Coptic and other Oriental Orthodox, Anglican and Lutheran Christians generally use the crucifix in public religious services. They believe use of the crucifix is in keeping with the statement by Saint Paul in Scripture, "we preach Christ crucified, a stumbling block to Jews and folly to Gentiles, but to those who are called, both Jews and Greeks, Christ the power of God and the wisdom of God".
In the West altar crosses and processional crosses began to be crucifixes in the 11th century, which became general around the 14th century, as they became cheaper. The Roman Rite requires that "either on the altar or near it, there is to be a cross, with the figure of Christ crucified upon it, a cross clearly visible to the assembled people. It is desirable that such a cross should remain near the altar even outside of liturgical celebrations, so as to call to mind for the faithful the saving Passion of the Lord." The requirement of the altar cross was also mentioned in pre-1970 editions of the Roman Missal, though not in the original 1570 Roman Missal of Pope Pius V. The Rite of Funerals says that the Gospel Book, the Bible, or a cross (which will generally be in crucifix form) may be placed on the coffin for a Requiem Mass, but a second standing cross is not to be placed near the coffin if the altar cross can be easily see from the body of the church.
Eastern Christian liturgical processions called crucessions include a cross or crucifix at their head. In the Eastern Orthodox Church, the crucifix is often placed above the iconostasis in the church. In the Russian Orthodox Church a large crucifix ("Golgotha") is placed behind the Holy Table (altar). During Matins of Good Friday, a large crucifix is taken in procession to the centre of the church, where it is venerated by the faithful. Sometimes the "soma" (corpus) is removable and is taken off the crucifix at Vespers that evening during the Gospel lesson describing the Descent from the Cross. The empty cross may then remain in the centre of the church until the Paschal vigil (local practices vary). The blessing cross which the priest uses to bless the faithful at the dismissal will often have the crucifix on one side and an icon of the Resurrection of Jesus on the other, the side with the Resurrection being used on Sundays and during Paschaltide, and the crucifix on other days.
Exorcist Gabriele Amorth has stated that the crucifix is one of the most effective means of averting or opposing demons. In folklore, it is believed to ward off vampires, incubi, succubi, and other evils.
Modern iconoclasts have used an inverted (upside-down) crucifix when showing disdain for Jesus Christ or the Catholic Church which believes in his divinity. According to Christian tradition, Saint Peter was martyred by being crucified upside-down.
Controversies.
Protestant Reformation.
Early Protestants generally rejected the use of the crucifix, and indeed the unadorned cross, along with other traditional religious imagery, as idolatrous. Martin Luther did not object to them, and this was among his differences with Andreas Karlstadt as early as 1525. Luther at the time of the Reformation retained the crucifix in the Lutheran Church. Only in America, where Lutheranism came under the influence of Calvinism, was the plain cross used. Calvin was violently opposed to both cross and crucifix. In England the Royal Chapels of Elizabeth I were most unusual among English churches in retaining crucifixes, following the Queen's personal conservative preferences. Under James I these disappeared, and their brief re-appearance in the early 1620s when James' heir was seeking a Spanish marriage was the subject of rumour and close observation by both Catholics and Protestants; when the match fell through they disappeared.
Modern.
In 2005, a mother accused her daughter's school in Derby, England of discriminating against Christians after the teenager was suspended for refusing to take off a crucifix necklace.
British Airways has faced legal action and calls for a boycott by Christians after it ruled an employee could not display a crucifix on her necklace (a rule it has now relaxed). A British prison ordered a multi-faith chapel to remove all crucifixes, presumably to avoid offending Muslims.
In 2008 in Spain, a local judge ordered crucifixes removed from public schools to settle a decades-old dispute over whether crucifixes should be displayed in public buildings in a non-confessional state. A 2008 Quebec government-commissioned report recommended that the crucifix of the National Assembly be removed to achieve greater pluralism, but the Liberal government refused.
On 18 March 2011, the European Court of Human Rights ruled, in the "Lautsi v. Italy" case, that the requirement in Italian law that crucifixes be displayed in classrooms of state schools does not violate the European Convention on Human Rights.
Crucifixes are common in most other Italian official buildings, including courts of law.
On 24 March 2011, the Constitutional Court of Peru ruled that the presence of crucifixes in courts of law does not violate the secular nature of the state.

</doc>
<doc id="7327" url="http://en.wikipedia.org/wiki?curid=7327" title="Copernican principle">
Copernican principle

In physical cosmology, the Copernican principle, named after Nicolaus Copernicus, states that the Earth is not in a central, specially favored position in the universe. More recently, the principle has been generalized to the relativistic concept that humans are not privileged observers of the universe. In this sense, it is equivalent to the mediocrity principle, with important implications for the philosophy of science.
Since the 1990s the term has been used (interchangeably with "the Copernicus method") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.
Origin and implications.
Michael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: "It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe."
Hermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets can be explained by reference to an assumption that the Sun and not the Earth is centrally located and stationary. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as "demoting" Earth from its central role it had in the Ptolemaic geocentric model, neither Copernicus nor other 15th- and 16th-century scientists and philosophers viewed it as such. It wasn't until the late 20th Century that Carl Sagan could claim "Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.".
In cosmology, if one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from our vantage-point on Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle. In practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. It becomes more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 200 million parsecs. However, on scales comparable to the radius of the observable universe, we see systematic changes with distance from the Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. While this might suggest that the Earth is at the center of the universe, the Copernican principle requires us to interpret it as evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach and shows us the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.
Modern mathematical cosmology is based on the assumption that the Cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations.
Bondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.
Tests of the principle.
The Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the Cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.
Historical.
Before the term Copernican principle was even coined, the Earth was repeatedly shown not to have any special location in the universe. The Copernican Revolution dethroned the Earth to just one of many planets orbiting the sun. William Herschel found that the solar system is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that our galaxy is just one of many galaxies in the universe. Examination of our galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.
Michelson-Morley and Relativity.
The Michelson-Morley experiments to measure the Earth's motion through the Luminiferous aether provided further indirect proof that the Earth had no special position, although early interpretations implying a complete lack of motion in the ether would have dis-proven the principle. The final interpretations of the result in terms of the electromagnetic nature of light, Lorentz invariance, and ultimately the general and special theories of relativity, contained no contradictions of the Copernican principle although still no strong proof.
Ecliptic alignment of cosmic microwave background anisotropy.
The Cosmic Microwave Background (CMB) radiation signature presents a direct large-scale view of the universe that can be used to identify whether our position or movement has any particular significance. There has been much publicity about analysis of results from the Wilkinson Microwave Anisotropy Probe (WMAP) and Planck mission that show both expected and unexpected anisotropies in the CMB.
Some anomalies in the background radiation have been reported which are aligned with the plane of the solar system, which contradicts the Copernican principle by suggesting that the solar system's alignment is special. Land and Magueijo dubbed this alignment the "axis of evil" owing to the implications for current models of the cosmos, although several later studies have shown systematic errors in the collection of that data and the way it is processed. Various studies of the CMB anisotropy data either confirm the Copernican principle, model the alignments in a non-homogeneous universe still consistent with the principle, or attempt to explain them as local phenomena. Some of these alternate explanations were discussed by Copi, "et al.", who claimed that data from the Planck satellite could shed significant light on whether the preferred direction and alignments were spurious.
Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, "I do think there is a bit of a psychological effect, people want to find unusual things." 
Modern tests.
Recent and planned tests relevant to the cosmological and copernican principles include:
Physics without the principle.
The standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general Cosmological principle and observations are largely consistent but there always unsolved problems. Some cosmologists and theoretical physicists design models lacking the Cosmological or Copernican principles, to constrain the valid values of observational results, to address specific known issues, and to propose tests to distinguish between current models and other possible models.
A prominent example in this context is the observed accelerating universe and the cosmological constant issue. An alternative proposal to dark energy is that the universe is much more inhomogeneous than currently assumed, and specifically that we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.

</doc>
<doc id="7329" url="http://en.wikipedia.org/wiki?curid=7329" title="Cyprinidae">
Cyprinidae

Cyprinidae is a large family of freshwater fishes, including the carps, the true minnows, and their relatives (for example, the barbs and barbels). Commonly called the carp family or the minnow family, its members are also known as cyprinids. It is the largest fish family and the largest family of vertebrate animals in general, with over 2,400 species in about 220 genera. The family belongs to the order Cypriniformes, of whose genera and species the cyprinids make up two-thirds. The family name is derived from the Ancient Greek "kyprînos" (κυπρῖνος, "carp").
Cyprinids are stomachless fish with toothless jaws. Even so, food can be effectively chewed by the gill rakers of the specialized last gill bow. These pharyngeal teeth allow the fish to make chewing motions against a chewing plate formed by a bony process of the skull. The pharyngeal teeth are species-specific and are used by specialists to determine species. Strong pharyngeal teeth allow fish such as the common carp and ide to eat hard baits like snails and bivalves.
Hearing is a well-developed sense, since the cyprinids have the Weberian organ, three specialized vertebral processes that transfer motion of the gas bladder to the inner ear. This construction is also used to observe motion of the gas bladder due to atmospheric conditions or depth changes. The cyprinids are physostomes because the pneumatic duct is retained in adult stages and the fish are able to gulp air to fill the gas bladder or they can dispose excess gas to the gut.
The fish in this family are native to North America, Africa, and Eurasia. The largest cyprinid in this family is the giant barb ("Catlocarpio siamensis"), which may grow up to . The largest North American species is the Colorado pikeminnow ("Ptychocheilus lucius"), of which individuals up to long and weighing over have been recorded.
Conversely, many species are smaller than . As of 2008, the smallest known freshwater fish is a cypriniform, "Danionella translucida", reaching at the longest. All fish in this family are egg-layers and most do not guard their eggs; however, a few species build nests and/or guard the eggs. The bitterling-like cyprinids (Acheilognathinae) are notable for depositing their eggs in bivalve molluscs, where the young grow until able to fend for themselves.
Most cyprinids feed mainly on invertebrates and vegetation, probably due to the lack of teeth and stomach, but some species, like the asp, specialize in fish. Many species (ide, common rudd) will eat small fish when they reach a certain size. Even small species, such as the moderlieschen, eat larvae of the common frog in artificial circumstances.
Some fish, such as the grass carp, are specialized in eating vegetation; others, such as the common nase, eat algae from hard surfaces, while others, such as the black carp, specialize in snails, and some, such as the silver carp, are specialized filter feeders. For this reason, they are often introduced as a management tool to control various factors in the aquatic environment, such as aquatic vegetation and diseases transmitted by snails.
Relationship with humans.
Cyprinids are highly important food fish; they are fished and farmed across Eurasia. In land-locked countries in particular, cyprinids are often the major species of fish eaten because they make the largest part of biomass in most water types except for fast-flowing rivers. In Eastern Europe, they are often prepared with traditional methods, such as drying and salting. The prevalence of inexpensive frozen fish products made this less important now than it was in earlier times. Nonetheless, in certain places, they remain popular for food, as well as recreational fishing, and have been deliberately stocked in ponds and lakes for centuries for this reason.
Cyprinids are popular for angling especially for match fishing (due to their dominance in biomass and numbers) and fishing for common carp because of its size and strength.
Several cyprinids have been introduced to waters outside their natural ranges to provide food, sport, or biological control for some pest species. The common carp ("Cyprinus carpio") and the grass carp ("Ctenopharyngodon idella") are the most important of these, for example in Florida. In some cases, they have become invasive species that compete with native fishes or disrupt the environment. Carp in particular can stir up sediment, reducing the clarity of the water and making it difficult for plants to grow.
Numerous cyprinids have become important in the aquarium hobby, most famously the goldfish, which was bred in China from the Prussian carp ("Carassius (auratus) gibelio"). First imported into Europe around 1728, it was much fancied by Chinese nobility as early as 1150 AD and after it arrived there in 1502, also in Japan. In the latter country, from the 18th century onwards, the common carp was bred into the ornamental variety known as koi – or more accurately "nishikigoi" (錦鯉), as "koi" (鯉) simply means "common carp" in Japanese.
Other popular aquarium cyprinids include danionins, rasborines, and true barbs. Larger species are bred by the thousands in outdoor ponds, particularly in Southeast Asia, and trade in these aquarium fishes is of considerable commercial importance. The small rasborines and danionines are perhaps only rivalled by characids and poecilid livebearers in their popularity for community aquaria.
One particular species of these small and undemanding danionines is the zebrafish ("Danio rerio"). It has become the standard model species for studying developmental genetics of vertebrates, in particular fish.
Habitat destruction and other causes have reduced the wild stocks of several cyprinids to dangerously low levels; some are already entirely extinct. In particular, the Leuciscinae from southwestern North America have been hit hard by pollution and unsustainable water use in the early to mid-20th century; most globally extinct Cypriniformes species are in fact Leuciscinae from the southwestern United States and northern Mexico.
Systematics.
The massive diversity of cyprinids has so far made it difficult to resolve their phylogeny in sufficient detail to make assignment to subfamilies more than tentative in many cases. Some distinct lineages obviously exist – for example, Cultrinae and Leuciscinae, regardless of their exact delimitation, are rather close relatives and stand apart from Cyprininae –, but the overall systematics and taxonomy of the Cyprinidae remain a subject of considerable debate. A large number of genera are "incertae sedis", too equivocal in their traits and/or too little-studied to permit assignment to a particular subfamily with any certainty.
Part of the solution seems that the delicate rasborines are the core group, consisting of minor lineages that have not shifted far from their evolutionary niche, or have coevolved for millions of years. These are among the most basal lineages of living cyprinids. Other "rasborines" are apparently distributed across the diverse lineages of the family.
The validity and circumscription of proposed subfamilies like Labeoninae or Squaliobarbinae also remains doubtful, although the latter do appear to correspond to a distinct lineage. The sometimes-seen grouping of the large-headed carps (Hypophthalmichthyinae) with "Xenocypris", though, seems quite in error. More likely, the latter are part of the Cultrinae.
The entirely paraphyletic "Barbinae" and the disputed Labeoninae might be better treated as part of the Cyprininae, forming a close-knit group whose internal relationships are still little known. The small African "barbs" do not belong in "Barbus" "sensu stricto" – indeed, they are as distant from the typical barbels and the typical carps ("Cyprinus") as these are from "Garra" (which is placed in the Labeoninae by most who accept the latter as distinct) and thus might form another as yet unnamed subfamily. However, as noted above, how various minor lineages tie into this has not yet been resolved; therefore, such a radical move, though reasonable, is probably premature.
The tench ("Tinca tinca"), a significant food species farmed in western Eurasia in large numbers, is unusual. It is most often grouped with the Leuciscinae, but even when these were rather loosely circumscribed, it always stood apart. A cladistic analysis of DNA sequence data of the S7 ribosomal protein intron 1 supports the view that it is distinct enough to constitute a monotypic subfamily. It also suggests it may be closer to the small East Asian "Aphyocypris", "Hemigrammocypris", and "Yaoshanicus". They would have diverged roughly at the same time from cyprinids of east-central Asia, perhaps as a result of the Alpide orogeny that vastly changed the topography of that region in the late Paleogene, when their divergence presumably occurred.
A DNA-based analysis of these fish places Rasborinae as the basal lineage with Cyprininae as a sister clade to Leuciscinae. The subfamilies Acheilognathinae, Gobioninae, and Leuciscinae are monophylitic.
Subfamilies and genera.
Subfamily Acheilognathinae – bitterling-like cyprinids. 
Subfamily Barbinae (barbs)
Subfamily Cultrinae
Subfamily Cyprininae (true carps)
Subfamily Danioninae – danionins
Subfamily Gobioninae – true gudgeons and relatives (including Gobiobotinae)
Subfamily Labeoninae (including Garrinae; might belong in Cyprininae)
Subfamily Leptobarbinae
Subfamily Leuciscinae – chubs, daces, true minnows, roaches, shiners and so on.
Subfamily Rasborinae – rasborines
Subfamily Squaliobarbinae
Subfamily Tincinae
Subfamily Xenocyprinae
Incertae sedis
Unlike most fish species, cyprinids generally increase in abundance in eutrophic lakes. Here, they contribute towards positive feedback as they are efficient at eating the zooplankton that would otherwise graze on the algae, reducing its abundance.

</doc>
<doc id="7330" url="http://en.wikipedia.org/wiki?curid=7330" title="Complementary DNA">
Complementary DNA

In genetics, complementary DNA (cDNA) is DNA synthesized from a messenger RNA (mRNA) template in a reaction catalysed by the enzymes reverse transcriptase. cDNA is often used to clone eukaryotic genes in prokaryotes. When scientists want to express a specific protein in a cell that does not normally express that protein (i.e., heterologous expression), they will transfer the cDNA that codes for the protein to the recipient cell. cDNA is also produced naturally by retroviruses (such as HIV-1, HIV-2, Simian Immunodeficiency Virus, etc.) and then integrated into the host's genome where it creates a provirus. The term cDNA is also used, typically in a bioinformatics context, to refer to an mRNA transcript's sequence, expressed as DNA bases (GCAT) rather than RNA bases (GCAU).
Overview.
According to the central dogma of molecular biology, when synthesizing a protein, a gene's DNA is transcribed into mRNA which is then translated into protein. One difference between eukaryotic and prokaryotic mRNA-coding genes is that eukaryotic genes can contain introns which are non coding sequences, in contrast with exons, sequences that code for mRNA. During transcription, all intron RNA is cut from the RNA primary transcript and the remaining pieces of the RNA primary transcript are spliced back together to become mRNA. The mRNA code is then translated into an amino acid chain (sequence) that constitutes the newly made protein. Prokaryotic genes have no introns, thus their RNA is not subject to cutting and splicing.
Often it is desirable to make prokaryotic cells express eukaryotic genes. An approach one might consider is to add eukaryotic DNA directly into a prokaryotic cell, and let it make the protein. However, because eukaryotic DNA has introns, and prokaryotes lack the machinery for removing introns from transcribed RNA, to make this approach work, all intron sequences must be removed from eukaryotic DNA prior to transferring it into the host. This 'intron-free' DNA is constructed using 'intron-free' mRNA as a template. It is therefore a 'complementary' copy of the mRNA, and is thus called complementary DNA (cDNA). To obtain expression of the protein encoded by the cDNA, prokaryotic regulatory sequences would also be required (e.g. a promoter).
Synthesis.
Though there are several methods for doing so, cDNA is most often synthesized from mature (fully spliced) mRNA using the enzyme reverse transcriptase. This enzyme, which naturally occurs in retroviruses, operates on a single strand of mRNA, generating its complementary DNA based on the pairing of RNA base pairs (A, U, G and C) to their DNA complements (T, A, C and G respectively).
To obtain eukaryotic cDNA whose introns have been removed:
Applications.
Complementary DNA is often used in gene cloning or as gene probes or in the creation of a cDNA library. When scientists transfer a gene from one cell into another cell in order to express the new genetic material as a protein in the recipient cell, the cDNA will be added to the recipient (rather than the entire gene), because the DNA for an entire gene may include DNA that does not code for the protein or that interrupts the coding sequence of the protein (e.g., introns). Partial sequences of cDNAs are often obtained as expressed sequence tags. 
With amplification of DNA sequences via polymerase chain reaction (PCR) now commonplace, one will typically conduct reverse transcription as an initial step, followed by PCR to obtain an exact sequence of cDNA for intra-cellular expression. This is achieved by designing sequence-specific DNA primers that hybridize to the 5' and 3' ends of a cDNA region coding for a protein. Once amplified, the sequence can be cut at each end with nucleases and inserted into one of many small circular DNA sequences known as expression vectors. Such vectors allow for self-replication inside cells, and potentially integration in the host DNA. They typically also contain a strong promoter to drive transcription of the target cDNA into mRNA, which is then translated into protein.
On June 13, 2013, the United States Supreme Court ruled in "Association for Molecular Pathology v. Myriad Genetics" that while human genes cannot be patented, cDNA can be. 
Viruses.
Some viruses also use cDNA to turn their viral RNA into mRNA (viral RNA → cDNA → mRNA). The mRNA is used to make viral proteins to take over the host cell.

</doc>
<doc id="7331" url="http://en.wikipedia.org/wiki?curid=7331" title="Cellular digital packet data">
Cellular digital packet data

Cellular Digital Packet Data (CDPD) was a wide-area mobile data service which used unused bandwidth normally used by AMPS mobile phones between 800 and 900 MHz to transfer data. Speeds up to 19.2 kbit/s were possible. The service was discontinued in conjunction with the retirement of the parent AMPS service; it has been functionally replaced by faster services such as 1xRTT, EV-DO, and UMTS/HSPA.
Developed in the early 1990s, CDPD was large on the horizon as a future technology. However, it had difficulty competing against existing slower but less expensive Mobitex and DataTac systems, and never quite gained widespread acceptance before newer, faster standards such as GPRS became dominant.
CDPD had very limited consumer products. AT&T Wireless first sold the technology in the United States under the PocketNet brand. It was one of the first products of wireless web service. Digital Ocean, Inc. an OEM licensee of the Apple Newton, sold the Seahorse product, which integrated the Newton handheld computer, an AMPS/CDPD handset/modem along with a web browser in 1996, winning the CTIA's hardware product of the year award as a smartphone, arguably the world's first. A company named OmniSky provided service for Palm V devices. Cingular Wireless later sold CDPD under the Wireless Internet brand (not to be confused with Wireless Internet Express, their brand for GPRS/EDGE data). PocketNet was generally considered a failure with competition from 2G services such as Sprint's Wireless Web. After the four phones AT&T Wireless had sold to the public (two from Panasonic, one from Mitsubishi and the Ericsson R289LX), AT&T Wireless eventually refused to activate the devices.
Despite its limited success as a consumer offering, CDPD was adopted in a number of enterprise and government networks. It was particularly popular as a first-generation wireless data solution for telemetry devices (machine to machine communications) and for public safety mobile data terminals.
In 2004, major carriers in the United States announced plans to shut down CDPD service. In July 2005, the AT&T Wireless and Cingular Wireless CDPD networks were shut down. Equipment for this service now has little to no residual value.
CDPD Network and system.
Primary elements of a CDPD network are:
1. End systems: physical & logical end systems that exchange information
2. Intermediate systems: CDPD infrastructure elements that store, forward & route the information
There are 2 kinds of End systems
1. Mobile end system: subscriber unit to access CDPD network over a wireless interface
2. Fixed end system: common host/server that is connected to the CDPD backbone and providing access to specific application and data
There are 2 kinds of Intermediate systems
1. Generic intermediate system: simple router with no knowledge of mobility issues
2. mobile data intermediate system: specialized intermediate system that routes data based on its knowledge of the current location of Mobile end system. It is a set of hardware and software functions that provide switching, accounting, registration, authentication, encryption, and so on.
The design of CDPD was based on several design objectives that are often repeated in designing overlay networks or new networks. A lot of emphasis was laid on open architectures and reusing as much of the existing RF infrastructure as possible. The design goal of CDPD included location independence and independence fro, service provider, so that coverage could be maximized ; application transparency and multiprotocol support, interoperability between products from multiple vendors.

</doc>
<doc id="7333" url="http://en.wikipedia.org/wiki?curid=7333" title="Chimera">
Chimera

Chimera, chimaira, or chimaera may refer to:

</doc>
<doc id="7335" url="http://en.wikipedia.org/wiki?curid=7335" title="Creature of statute">
Creature of statute

Creatures of statute (also known as creatures of the state) are legal entities, such as corporations, created by statute. Creatures of statute may include municipalities and other artificial legal entities or relationships. Thus, when a statute in some fashion requires the formation of a corporate body—often for governmental purposes—such bodies when formed are known as "creatures of statute." The same concept is also expressed with the phrase "creature of the state."
The term "creature of statute" is most common to the United States. In the United Kingdom, these bodies are simply called statutory corporations (or statutory bodies) and generally have some governmental function. The United Kingdom Atomic Energy Authority is an example. In a wider sense, most companies in the UK are created under statute since the Companies Act 1985 specifies how a company may be created by a member of the public, but these companies are not called 'statutory corporations'. Often, in American legal and business documents that speak of governing bodies ("e.g.", a board that governs small businesses in China) these bodies are described as "creatures of statute" to inform readers of their origins and format although the national governments that created them may not term them as creatures of statute. Australia also uses the term "creature of statute" to describe some governmental bodies.
The importance of a corporate body, regardless of its exact function, when such a body is a creature of statute is that its active functions can only be within the scope detailed by the statute which created that corporation. Thereby, the creature of statute is the tangible manifestation of the functions or work described by a given statute. The jurisdiction of a body that is a creature of statute is also therefore limited to the functional scope written into the laws that created that body. Unlike most (private) corporate bodies, creatures of statute cannot expand their business interests into other diverse areas.

</doc>
<doc id="7339" url="http://en.wikipedia.org/wiki?curid=7339" title="General Conference on Weights and Measures">
General Conference on Weights and Measures

The General Conference on Weights and Measures ( - CGPM) is the senior of the three Inter-governmental organizations established in 1875 under the terms of the Metre Convention () to represent the interests of member states. The treaty, which also set up two further bodies, the International Committee for Weights and Measures (- CIPM) and the International Bureau of Weights and Measures ( - BIPM), was drawn up to coordinate international metrology and to coordinate the development of the metric system.
The conference meets in Sèvres (south-west of Paris) every four to six years. Initially it was only concerned with the kilogram and the metre, but in 1921 the scope of the treaty was extended to accommodate all physical measurements and hence all aspects of the metric system. In 1960 the 11th CGPM approved the "Système International d'Unités", usually known as "SI".
Establishment.
On 20 May 1875 an international treaty known as the "Convention du Mètre" (Metre Convention) was signed by 17 states. This treaty established the following organisations to conduct international activities relating to a uniform system for measurements:
The CGPM acts on behalf of the governments of its members. In so doing, it appoints members to the CIPM, receives reports from the CIPM which it passes on to the governments and national laboratories on member states, examines and where appropriate approves proposals from the CIPM in respect of changes to the International System of Units (SI), approves the budget for the BIPM (over €10 million in 2012) and it decides all major issues concerning the organization and development of the BIPM.
Membership criteria.
The CGPM recognises two classes of membership - full membership for those states that wish to participate in the activities of the BIPM and associate membership for those countries or economies that only wish to participate in the MRA program. Associate members have observer status at the CGPM. Since all formal liaison between the convention organisations and national governments is handled by the member state's ambassador to France, it is implicit that member states must have diplomatic relations with France, though during both world wars, nations that were at war with France retained their membership of the CGPM. The opening session of each CGPM is chaired by the French foreign minister and subsequent sessions by the Président de l'Académie des Sciences de Paris.
Of the twenty countries that attended the Conference of the Metre in 1875, representatives of seventeen signed the convention on 20 May 1875. In April 1884 HJ Chaney, Warden of Standards in London unofficially contacted the BIPM inquiring whether the BIPM would calibrate some metre standards that had been manufactured in the United Kingdom. Broch, director of the BIPM replied that he was not authorised to perform any such calibrations for non-member states. On 17 September 1884, the British Government signed the convention on behalf of the United Kingdom. This number grew to 21 in 1900, 32 in 1950, and 49 in 2001. , there are 55 Member States and 38 Associate States and Economies of the General Conference (with year of partnership in parentheses):
Member States
 (1877)<br>
 (1947)<br>
 (1875)<br>
 (1875)<br>
 (1921)<br>
 (1911)<br>
 (1907)<br>
 (1908)<br>
 (1977)<br>
 (2012)<br> 
 (2008)<br>
 (1922)<br>
 (1875)<br>
 (1954)<br>
 (1962)<br>
 (1923)<br>
 (1875)<br>
 (1875)<br>
 (2001)<br>
 (1925)<br>
 (1957)<br>
 (1960)<br>
 (1975)<br>
 (1925)<br>
 (1985)<br>
 (1875)<br>
 (1885)<br>
 (2008)<br>
 (2010)<br>
 (2001)<br>
 (1890)<br>
 (1929)<br>
 (1991)<br>
 (1875)<br>
 (1973)<br>
 (1925)<br>
 (1876)<br>
 (1884)<br>
 (1875)<br>
 (2011)<br>
 (2001)<br>
 (1994)<br>
 (1922)<br>
 (1964)<br>
 (1959)<br>
 (1875)<br>
 (1875)<br>
 (1875)<br>
 (1912)<br>
 (2012)<br>
 (1875)<br>
 (1884)<br>
 (1878)<br>
 (1908)<br>
 (1879)
Notes
Associates
At its 21st meeting (October 1999), the CGPM created the category of "associate" for those states not yet members of the BIPM and for economic unions.
 (2007)<br>
 (2010)<br>
 (2003)<br>
 (2008)<br>
 (2011)<br>
 (2012)<br>
 (2005)<br>
 (2002)<br>
 (2004)<br>
 (2000)<br>
 (2000)<br>
 (2005)<br>
 (2008)<br>
 (2009)<br>
 (2000)<br>
 (2003)<br>
 (2001)<br>
 (2001)<br>
 (2006)<br>
 (2001)<br>
 (2010)<br>
 (2013)<br>
 (2011)<br>
 (2012)<br>
 (2012)<br>
 (2003)<br>
 (2009)<br>
 (2009)<br>
 (2002)<br>
 (2007)<br>
 (2010)<br>
 (2003)<br>
 (2007)<br>
 (2012)<br>
 (2002)<br>
 (2003)<br>
 (2010)<br>
 (2010)<br>

</doc>
<doc id="7341" url="http://en.wikipedia.org/wiki?curid=7341" title="Cowboy Bebop">
Cowboy Bebop

 is a 1998 Japanese anime series developed by Sunrise featuring a production team led by director Shinichirō Watanabe, screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical designer Kimitoshi Yamane, and composer Yoko Kanno. The twenty-six episodes ("sessions") of the series are set in the year 2071, and follow the adventures of a bounty hunter crew traveling on the "Bebop" (their spaceship). "Cowboy Bebop" explores philosophical concepts including existentialism, existential ennui, loneliness, and the past's influence.
The series premiered in Japan on TV Tokyo's 18:00 timeslot (previously occupied by "Kodomo no Omocha") from April 3 until June 26, 1998, broadcasting only twelve episodes and a special due to its controversial adult-themed content. The entire twenty-six episodes of the series were later broadcast on WOWOW from October 24 until April 24, 1999. The anime was adapted into two manga series which were serialized in Kadokawa Shoten's Asuka Fantasy DX. A was later released to theaters worldwide.
The anime series was dubbed in the English language by Animaze and ZRO Limit Productions, and was licensed by Bandai Entertainment in North America and is now licensed by Funimation. For English releases in the United Kingdom, it was licensed by Beez Entertainment and is now licensed by Anime Limited. Madman Entertainment has licensed it for releases in Australia and New Zealand. In 2001, "Cowboy Bebop" became the first anime title to be broadcast on Adult Swim in the United States.
"Cowboy Bebop" became a critical and commercial success both in Japanese and international markets (most notably in the United States), garnered several major anime and science fiction awards from Japanese publications, and received universal praise for its style, characters, story, voice acting, animation and soundtrack. In the years since its release, critics and reviewers, from the United States in particular, have hailed "Cowboy Bebop" as a masterpiece and frequently cite it as one of the greatest anime series of all time. Credited with helping introduce anime to a new wave of Western viewers in the early 2000s, "Cowboy Bebop" has also been labelled a gateway series for the medium as a whole.
Synopsis.
Setting.
"Cowboy Bebop" is set in the year 2071, when humanity has colonized the entire Solar System through the use of "Phase Difference Space Gates", which allow for swift travel in space. These Gates made it possible to deliver vast amounts of materials and energy, including sunlight, to distant planets, consequently making it immensely easier to make these worlds suitable for human habitation (known as terraforming).
In 2022, a catastrophic accident occurred in the Earth's orbit during the development of the Gates, damaging both the planet and the Moon. The Earth's surfaces became heavily irradiated, forcing most of mankind to evacuate via the Gates and leave for the planets and moons of the Solar System. Old governments and ethnic groups were forgotten, and were soon replaced by new allegiances and affiliations as new communities formed. The difficult times eventually brought rapid developments and the economy recovered. However, the widening gap between the rich and poor created a boom in criminal activity, leading to the rise of numerous criminal syndicates. As time passed, the planets and satellites became independent states, and a new generation grew up with no memories of the Earth. Gate technology became a trusted part of everyday life, and a necessary tool for the people. Interplanetary crime fell under the jurisdiction of the Inter Solar System Police (ISSP), an organisation which also introduced a bounty reward scheme. Once registered, bounty hunters were licensed to bring criminals to justice in exchange for high rewards. This created a new class of bounty hunters, also known as "cowboys".
Plot.
The story begins with the introduction of protagonist Spike Spiegel, an exiled former hitman of the criminal Red Dragon Syndicate, and his partner Jet Black, a former ISSP officer who retired following a mob hit that cost him his arm, as two bounty hunters who travel on Jet's spaceship the "Bebop". Through unintended circumstances, the duo are eventually joined by three new members: Ein, a hyper-intelligent genetically-engineered Welsh Corgi dog, Faye Valentine, an amnesiac femme fatale con artist, and Edward, a barefooted preteen female prolific computer hacker. The series is mostly episodic in nature, with the story regularly following the crew's adventures and bounty hunting quests, while also exploring their day-to-day lives. Aside from the stand-alone stories, there is also a main overarching story arc that centers on Spike.
Throughout the series, the "Bebop" crew members deal with unresolved issues from their pasts, with the show regularly utilizing flashbacks to illustrate their backstories. Over the course of the series' episodes, Spike occasionally encounters and battles his former friend Vicious, an enforcer of the Red Dragon Syndicate, while his past is shown to be tied to a mysterious woman named Julia. After a brief reunion with his former ISSP partner, Jet discovers he was betrayed and the mob hit that led to his arm loss was a trap carried out by his corrupt partner. Faye discovers she was born in the 20th century and was cryogenically frozen after a space shuttle accident left her severely injured. Edward encounters her long-lost father and eventually decides to leave the "Bebop", with Ein choosing to accompany her.
The series ends with the conclusion of Spike's story arc. It is revealed that Julia was Vicious' girlfriend who started a dangerous affair with Spike, who offered to abandon the Red Dragon Syndicate and elope with her. After Vicious found out, he confronted Julia and ordered her to kill Spike, threatening that both would be killed otherwise. To protect herself and Spike, Julia went into hiding, never meeting him as both of them had planned. Spike then faked his death to the Syndicate and disappeared, eventually meeting Jet and forming a bounty hunter partnership with him. In the present, Vicious stages a coup d'etat on the leaders of the Red Dragon Syndicate, becoming its new head and ordering a hit on Spike and Julia. With help, Spike manages to find Julia in their original meeting place. However, the two are shortly ambushed by hitmen sent by Vicious, and Julia is killed in the escape. Spike returns to the "Bebop" and shares a final moment with Jet and Faye, before flying to Red Dragon headquarters where Vicious is waiting. After fighting his way through numerous henchmen, he finally manages to confront Vicious. The two battle, Spike using his pistol, and Vicious using his katana. After a brief standoff, the two exchange blows simultaneously, leaving Vicious dead and Spike severely wounded. Spike descends the staircase, where he is confronted by the remainder of the Red Dragons members. Smiling, he mimics a gun with his fingers, utters a final word "Bang", and collapses.
Production.
"Cowboy Bebop" was developed by animation studio Sunrise and created by Hajime Yatate, the well-known pseudonym for the collective contributions of Sunrise's animation staff. The leader of the series' creative team was director Shinichirō Watanabe, most notable at the time for directing "Macross Plus" and "". Other leading members of Sunrise's creative team were screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical art designer Kimitoshi Yamane and composer Yoko Kanno. Most of them had previously worked together, in addition to having credits on other popular anime titles. Nobumoto had scripted "Macross Plus", Kawamoto had designed the characters for "Gundam", and Kanno had composed the music for "Macross Plus" and "The Vision of Escaflowne". Yamane had not worked with Watanabe yet, but his credits in anime included "Bubblegum Crisis" and "The Vision of Escaflowne".
"Cowboy Bebop" was Watanabe's first project as solo director, as he had been co-director in his previous works. The project had initially originated with Bandai's toy division as a sponsor, with the goal of selling spacecraft toys. Watanabe recalled his only instruction was "So long as there's a spaceship in it, you can do whatever you want." But upon viewing early footage, it became clear that Watanabe's vision for the series didn't match with that of Bandai's. Believing the series would never sell toy merchandise, Bandai pulled out of the project, leaving it in development hell until sister company Bandai Visual stepped in to sponsor it. Since there was no need to merchandise toys with the property any more, Watanabe had free rein in the development of the series.
Watanabe wanted to design not just a space adventure series for adolescent boys but a program that would also appeal to sophisticated adults. While the dialogue of the series was kept clean to avoid any profanities, its level of sophistication was made appropriate to adults in a criminal environment. Mature themes such as drug dealing and homosexuality were also key elements of some episodes. Watanabe would later describe "Cowboy Bebop" as "80% serious story and 20% humorous touch." When developing the series' story, Watanabe began by creating the characters first. He explained, "the first image that occurred to me was one of Spike, and from there I tried to build a story around him, trying to make him cool."
Watanabe noted that composer Yoko Kanno did not score the music exactly the way he told her to. He stated, "She gets inspired on her own, follows up on her own imagery and comes to me saying 'this is the song we need for "Cowboy Bebop",' and composes something completely on her own." Watanabe further explained that he would take inspiration from Kanno's music after listening to it and create new scenes for the story from it. These new scenes in turn would inspire Kanno and give her new ideas for the music and she would come to Watanabe with even more music. Watanabe cited as an example, "some songs in the second half of the series, we didn't even ask her for those songs, she just made them and brought them to us." He commented that while Kanno's method was normally "unforgivable and unacceptable," it was ultimately a "big hit" with "Cowboy Bebop". Watanabe described his collaboration with Kanno as "a game of catch between the two of us in developing the music and creating the TV series "Cowboy Bebop"."
The atmospheres of the planets and the ethnic groups in "Cowboy Bebop" mostly originated from Watanabe's ideas, with some collaboration from set designers Isamu Imakake, Shoji Kawamori, and Dai Satō. The animation staff established the particular planet atmospheres early in the production of the series before working on the ethnic groups. It was Watanabe who wanted to have several groups of ethnic diversity appear in the series. Mars was the planet most often used in "Cowboy Bebop"s storylines, with Satoshi Toba, the cultural and setting producer, explaining that the other planets "were unexpectedly difficult to use." He stated that each planet in the series had unique features, and the producers had to take into account the characteristics of each planet in the story. For the final episode, Toba explained that it was not possible for the staff to have the dramatic rooftop scene occur on Venus, so the staff "ended up normally falling back to Mars."
Regarding the longevity of his work, Watanabe said that during the making of "Bebop", he would try to rally the animation staff by telling them that the show would be something memorable in 10, 20, and even 30 years from then. While some of them were doubtful of that at the time, Watanabe many years later expressed his happiness to have been proven right in retrospect. He joked that if Bandai Visual hadn't intervened then "you might be seeing me working the supermarket checkout counter right now."
Analysis.
Style and appeal.
Several planets and space stations in the series are shown to be made in Earth's image. The streets of celestial objects such as Ganymede resemble a modern port city, while Mars is replete with shopping malls, theme parks, casinos and cities. "Cowboy Bebop"'s universe is filled with video players and hyperspace gates, eco-politics and fairgrounds, spaceships and Native American shamans. Futuristic elements are combined with the modern elements, "allowing audiences to easily connect with the "Cowboy Bebop" world".
In his review of "Cowboy Bebop", Miguel Douglas, editor-in-chief of iSugoi.com, describes the style of the series: 
the series distinctly establishes itself outside the realm of conventional Japanese animation and instead chooses to forge its own path. With a setting within the realm of science fiction, the series wisely offers a world that seems entirely realistic considering our present time. Free from many of the elements that accompany science fiction in general — whether that be space aliens, giant robots, or laser guns — the series delegates itself towards presenting a world that is quite similar to our own albeit showcasing some technological advances. Certainly not as pristine a future we would see in other series or films, "Cowboy Bebop" decides to deliver a future that closely reflects that of our own time. This aspect of familiarity does wonders in terms of relating to the viewer, and it presents a world that certainly resembles our very own.
Daryl Surat of Otaku USA commented on the series' "broad-ranging" appeal due to its style:
"Cowboy Bebop" was that rare breed of science-fiction: "accessible". Unlike many anime titles, viewers weren’t expected to have knowledge of Asian culture — character names, signs, and the like were primarily in English to begin with — or have seen any other anime series prior. 
Susan J. Napier argues, in her book "", that anime increasingly "exists at a nexus point in global culture…an amorphous new media territory that crosses and intermingles national boundaries". Napier goes on to point out that many Japanese commentators refer to anime with the term "mukokuseki", meaning "stateless". This implies that much anime is not specifically Japanese and therefore lacks a distinct national identity. Napier states that this "very quality of 'statelessness' has increasing attraction in our global culture". It is said that "Cowboy Bebop" reflects this and it is a great part of the show's appeal.
Genre and cultural references.
Watanabe's main inspiration for "Cowboy Bebop" was "Lupin III", a crime anime series from the late 1970s through the mid-1980s. According to Watanabe, the series paid subtle tribute to his favorite American films and series, which were shown in Japan during that time, including "Butch Cassidy and the Sundance Kid", Bruce Lee films, films with blues or jazz soundtracks, as well as Blaxploitation films. Individual movies from "Alien" to "Midnight Run" were pastiched.
The series covered genres such as comedy, detective caper, action and thriller. The musical style was emphasized in many of the episode titles, which were in English, such as: "Asteroid Blues", "Honky Tonk Woman", "Ballad of Fallen Angels", "Heavy Metal Queen", etc. The anime draws heavily on Western sources, such as pulp detective stories, film noir, and American Westerns. There are also strong Hong Kong influences, mainly of the heroic bloodshed mold which includes films such as "The Killer" or "Hard Boiled".
These continual borrowings from other genres and cultural products create a familiar access point for a western audience and perhaps in some part explain "Cowboy Bebop"'s popularity. The sense of the familiar is emphasised and reinforced by popular culture references throughout the series. Kung fu films are an obvious influence. In "Stray Dog Strut" the final fight between Spike and Hakim is influenced by Bruce Lee's "Game of Death" while in "Waltz for Venus", Spike's kung fu lesson is similar to a scene from Lee's "Enter the Dragon".
The western genre is a strong influence on "Cowboy Bebop". Several examples pervade throughout; a show called "Big Shot" informs the characters of the current bounties, the crew often visit saloons and desert worlds and engage in gunfights and stand-offs. The first episode contains a scene reminiscent of "A Fistful of Dollars". Even the title of the show is a reference to westerns, suggesting the prevalence of a lawless society. 
Science fiction is another substantial influence, both the space and futuristic setting, as well as in reference to science fiction films of the 70's and 80's. A homage of the "Alien" films is made in the episode "Toys in the Attic" where an unseen monster stalks the crew. The episode "Wild Horses" is strongly influenced by the original "Star Wars" films.
Film noir is perhaps the greatest genre influence on "Cowboy Bebop". This is shown with the characterization of Jet Black, a former cop who rails against the corruption of the police force but is thrown into a semi-lawless state of bounty hunting. As in film noir, characters are morally ambiguous – none more so than Faye Valentine who betrays her allies in the pursuit of a big bounty. The big-city rain-slicked settings of film noir are continually used, especially in the episode "Ganymede Elegy". Other visual and aural cues are also taken from film noir, in "Pierrot Le Fou" for instance, Spike battles an enraged homicidal clown across a fairground, accompanied by lighting and camera angles film noir would use.
Distribution.
Broadcast.
"Cowboy Bebop" almost did not appear on Japanese broadcast television due to its depictions of graphic violence. It was first sent to TV Tokyo, one of the main broadcasters of anime in Japan. The show had an aborted first run from April 3 until June 26, 1998, on TV Tokyo, broadcasting only episodes 2, 3, 7 to 15, 18 and a special, after which it was canceled due to low ratings. Later that year, the series was shown in its entirety from October 24 until April 24, 1999, on satellite network WOWOW. Because of TV Tokyo's cancellation of the anime, the production schedule was disrupted to the extent that the last episode was delivered to WOWOW on the day of its broadcast. The full series has also been broadcast across Japan by anime television network Animax, which has also aired the series via its respective networks across Southeast Asia, South Asia and East Asia.
In the United States, on September 2, 2001, "Cowboy Bebop" became the first anime title to be shown as part of the U.S.Adult Swim Launch. It was successful enough to be broadcast repeatedly for four years. It was rerun again in 2007, 2008, 2009, 2010, 2011, and 2013. In the United Kingdom it was first broadcast in 2002 as one of the highlights of the ill-fated "cartoon network for adults", CNX. From November 6, 2007, it was repeated on AnimeCentral until the channel's closure in August 2008. In Australia, "Cowboy Bebop" was first broadcast on pay-TV in 2002 on Adult Swim in Australia. It was broadcast on Sci Fi Channel on Foxtel. In Australia, "Cowboy Bebop" was first broadcast on free-to-air-TV on ABC2 (the national digital public television channel) on January 2, 2007. It has been repeated several times, most recently starting in 2008. "Cowboy Bebop: The Movie" also aired again on February 23, 2009, on SBS (a hybrid-funded Australian public broadcasting television network). In Canada, "Cowboy Bebop" was first broadcast on December 24, 2006, on Razer.
Home media.
"Cowboy Bebop" has been released in three separate editions in North America.
The first release was sold in 2000 individually, and featured uncut versions of the original 26 episodes. In 2001, these DVDs were collected in the special edition "Perfect Sessions" which included the first 6 DVDs, the first "Cowboy Bebop" soundtrack, and a collector's box. At the time of release, the art box from the Perfect Sessions was made available for purchase on The Right Stuff International as a solo item for collectors who already owned the series.
The second release, "The Best Sessions", was sold in 2002 and featured what Bandai considered to be the best 6 episodes of the series remastered in Dolby Digital 5.1 and DTS surround sound.
The third release, "Cowboy Bebop Remix", was also distributed on 6 discs and included the original 26 uncut episodes, with sound remastered in Dolby Digital 5.1 and video remastered under the supervision of Shinichiro Watanabe. This release also included various extras that were not present in the original release. Cowboy Bebop Remix was itself collected as the Cowboy Bebop Remix DVD Collection in 2008.
A fourth release in Blu-Ray format was released on December 21, 2012 exclusively in Japan.
In December 2012, newly founded distributor Anime Limited announced via Facebook and Twitter that they had acquired the home video license for the United Kingdom. Part 1 of the Blu-Ray collection was released on July 29, 2013, while Part 2 was released on October 14. The standard DVD Complete Collection was originally meant to be released on September 23, 2013 with Part 2 of the Blu-Ray release but due to mastering and manufacturing errors, the Complete Collection was delayed until November 27. Following the closure of Bandai Entertainment in 2012, Funimation and Sunrise had announced that they rescued "Cowboy Bebop", along with a handful of other former Bandai Entertainment properties, for home video and digital release. Funimation will release the series on Blu-ray and DVD on December 16, 2014. The series will be released in four separate editions: standard DVD, standard Blu-ray, an Amazon.com exclusive Blu-ray/DVD combo, and a Funimation.com exclusive Blu-ray/DVD combo.
Related media.
Music.
The music of "Cowboy Bebop" was scored by composer Yoko Kanno. Kanno formed the blues and jazz band Seatbelts to perform the music of the series. Since the series' broadcast, Kanno and the Seatbelts have released seven original soundtrack albums, two singles and extended plays, and two compilations through label Victor Entertainment.
The series' opening theme was "Tank!", performed by the Seatbelts and composed by Kanno; while the ending theme for most of the series was "The Real Folk Blues", performed by the Seatbelts with vocals by Mai Yamane and composed by Kanno with lyrics by Yuho Iwasato. The ending theme for the thirteenth episode "Jupiter Jazz (Part 2)" was "Space Lion", performed by the Seatbelts and composed by Kanno; while the ending theme for the twenty-sixth and final episode "The Real Folk Blues (Part 2)" was "Blue", performed by the Seatbelts with vocals by Yamane and composed by Kanno with lyrics by Tim Jensen.
Manga.
Two "Cowboy Bebop" manga series have been released, both published by Kadokawa Shoten and serialized in "Asuka Fantasy DX".<ref name="DX 10/1997"></ref><ref name="DX 11/1998"></ref> The first manga series, titled "Cowboy Bebop: Shooting Star" and written and illustrated by Cain Kuga, was serialized from October issue 1997, before the anime series' release, to July issue 1998. It was collected into two volumes in 1998, the first one in May and the second one in September. The second manga series, simply titled "Cowboy Bebop" and written and illustrated by Yutaka Nanten, was serialized from November issue 1998 to March issue 2000. It was collected into three volumes, the first two in April and October 1999 and the third one in April 2000. Both manga series were licensed by Tokyopop for release in North America.
Video games.
A "Cowboy Bebop" video game, developed and published by Bandai, was released in Japan for the PlayStation on May 14, 1998. A PlayStation 2 video game, ", was released in Japan on August 25, 2005, and an English version had been set for release in North America. However, in January 2007, IGN reported that the release had been likely been cancelled, speculating that it did not survive Bandai-Namco's merger to Bandai Namco Games.
Film.
An anime film titled "; also known as was released in Japan on September 2001 and in the United States in 2003.
On July 22, 2008, "If" published an article on its website regarding a rumor of a live-action "Cowboy Bebop" movie in development by 20th Century Fox. Producer Erwin Stoff said that the film's development was in the early stages, and that they had "just signed it". Keanu Reeves was to play the role of Spike Spiegel. Variety confirmed on January 15, 2009, that production company Sunrise Animation would be "closely involved with the development of the English language project". The site also confirmed Kenji Uchida, Shinichiro Watanabe, and series writer Keiko Nobumoto as associate producers, series producer Masahiko Minami as a production consultant, and Peter Craig as screenwriter. This was lauded by various sources as a promising move for the potential quality of the film. At the time it was slated to release in 2011, but problems with the budget delayed its production. The submitted script was sent back for rewrite to reduce the cost and little has been heard about it since an interview with producer Joshua Long on October 15, 2010; the project currently languishes in development hell. On May 31, 2013, Watanabe stated that the film is currently "underway" but the "details are a secret." On October 20, 2013 Reeves stated "Cowboy Bebop does not look like it is going to happen with me in it [...]" in a reddit IAmA.
Other.
An official side story titled "Cowboy Bebop: UT" tells the story of Ural and Victoria Terpsichore (V.T. from the episode "Heavy Metal Queen") when they were bounty hunters. The story was available in its own official site, however the site was closed and is currently available at the site mirror hosted by jazzmess.com.
Reception.
"Cowboy Bebop" has received universal critical acclaim, and upon release, it garnered several major science fiction awards and rankings from different anime publications. Several reviewers from various sources have praised the series for its style, characters, story, voice acting, animation and soundtrack. Over the years since its release, Western critics and viewers in particular have hailed "Cowboy Bebop" as a masterpiece and frequently consider it as one of the best anime series of all time.
Critical reception.
Anime News Network's Mike Crandol gave the series an 'A+' rating for the dubbed version, and an 'A' rating for the subbed version. He claimed the series was "one of the most popular and respected anime titles in history," before adding that it was "a unique television show which skillfully transcends all kinds of genres." Crandol praised its characters as "some of the most endearing characters to ever grace an anime," and commended the voice acting, especially the "flawless English cast," believing they "actually one-up the Japanese originals." He also complimented the series' "movie-quality" animation, "sophisticated" writing, and its "incredible" musical score. Crandol hailed "Cowboy Bebop" as a "landmark" anime "that will be remembered long after many others have been forgotten", and went on to call it "one of the greatest anime titles ever."
T.H.E.M. Anime Reviews gave the entire series a perfect score of 5 out of 5 stars, with reviewer Christina Carpenter believing "Cowboy Bebop" as "one of the best [anime]" and touting it as a masterpiece that "puts most anime...and Hollywood, to shame." She described it as a "very stylish, beautifully crafted series that deserves much more attention than it gets." Carpenter praised the animation as "a rarity and a marvel to behold" and that it was "beyond superb," and the plot and characterization as having "a sophistication and subtlety that is practically one-of-a-kind." She also praised the soundtrack, and hailed the opening theme as one of the best intro pieces she had ever heard. Carpenter went to say that "Bebop" was a "must-have for any serious collector of Japanese animation."
The Nihon Review's Kavik Ryx awarded "Cowboy Bebop" a maximum 10 out of 10. He praised the "fluid like water" animation, the "brilliant" jazz style soundtrack, the "quirky, dynamic, [...] likable" characters, the "epic" moments, the "fun" battles, and the English dub; he also described the story as "one of the most fun and addicting plots in anime." Ryx applauded "Bebop" as a "visually stunning" series with a style "that seems unique to anime," before noting the series' one drawback was that it was "so well done" that it "could have gone longer."
Accolades.
In the 1999 Anime Grand Prix awards for the anime of 1998, "Cowboy Bebop" won two 1st place awards: Spike Spiegel was awarded the best male character; and Megumi Hayashibara was awarded the best voice actor for her role as Faye Valentine. "Cowboy Bebop" also received rankings in other categories: the series itself was awarded the 2nd best anime series; Faye Valentine and Ed were ranked the 5th and 9th best female characters respectively; "Tank!" and "The Real Folk Blues" were ranked the 3rd and 15th best songs respectively; and "Ballad of Fallen Angels", "Speak Like a Child", "Jamming with Edward" and "Mish-Mash Blues" were ranked the 2nd, 8th, 18th and 20th best episodes respectively.
In the 2000 Anime Grand Prix awards for the anime of 1999, "Cowboy Bebop" won the same two 1st place awards again: best male character for Spike Spiegel; and best voice actor for Megumi Hayashibara. Other rankings the series received are: 2nd best anime series; 6th best female character for Faye Valentine; 7th and 12th best song for "Tank!" and "Blue" respectively; and 3rd and 17th best episode for "The Real Folk Blues (Part 2)" and "Hard Luck Woman" respectively. In the 2000 Seiun Awards, Cowboy Bebop was awarded for Best Media of the Year.
A 2004 poll in "Newtype USA", the US edition of the Japanese magazine "Newtype", asked its readers to vote the "Top 25 Anime Titles of All Time"; "Cowboy Bebop" ranked 2nd on the list (after "Neon Genesis Evangelion"), placing it as one of the most socially relevant and influential anime series ever created. In 2007, the American Anime magazine "Anime Insider" listed the "50 Best Anime Ever" by compiling lists of industry regulars and magazine staff, and ranked "Cowboy Bebop" as the #1 anime of all time. In 2012, Madman Entertainment compiled the votes of fans online for "The Top 20 Madman Anime Titles" and ranked "Cowboy Bebop" at #7.
"Cowboy Bebop" has been featured in several lists published by IGN. In the 2009 "Top 100 Animated TV Series" list, "Cowboy Bebop", labelled as "a very original -- and arguably one of the best -- anime", was placed 14th, making it the second highest ranking anime on the list (after "Evangelion") and one of the most influential series of the 1990s. In 2011, "Bebop" was ranked 29th in the "Top 50 Sci-Fi TV Shows" list, once again being the second highest ranking anime on the list (after "Evangelion"). In 2006, "Cowboy Bebop"s soundtrack was ranked #1 in "Top Ten Anime Themes and Soundtracks of All-Time" list, with the series being commented as "one of the best anime ever and certainly is tops when it comes to music." Spike Spiegel was ranked 4th place in the "Top 25 Anime Characters of All Time" article. IGN Movies also placed "Cowboy Bebop" in their list of "10 Cartoon Adaptations We'd Like to See".
Legacy.
In March 2009, the print and web editions of The Onion's A.V. Club called "Cowboy Bebop" "rightly a huge hit", and listed it as a gateway series to understanding the medium of anime as a whole.
American film director, screenwriter, and producer Rian Johnson has cited Cowboy Bebop as a visual influence on his films, most notably Brick.
Continuation rumors.
After the creation of the series, an interviewer asked Watanabe if he had any plans to create more "Cowboy Bebop" material. Watanabe responded by saying that he does not believe that he "should just keep on making "Cowboy Bebop" sequels for the sake of it". Watanabe added that ending production and "to quit while we're ahead when people still want more" is more "in keeping with the "Bebop" spirit". In a more recent interview from 2006 with "The Daily Texan", Watanabe was asked if there would ever be more "Cowboy Bebop". Watanabe's answer was "someday...maybe, someday".

</doc>
<doc id="7342" url="http://en.wikipedia.org/wiki?curid=7342" title="Clement of Alexandria">
Clement of Alexandria

Titus Flavius Clemens (; c. 150 – c. 215), known as Clement of Alexandria to distinguish him from the earlier Clement of Rome, was a Christian theologian who taught at the Catechetical School of Alexandria. A convert to Christianity, he was an educated man who was familiar with classical Greek philosophy and literature. As his three major works demonstrate, Clement was influenced by Hellenistic philosophy to a greater extent than any other Christian thinker of his time, and in particular by Plato and the Stoics. His secret works, which exist only in fragments, suggest that he was also familiar with pre-Christian Jewish esotericism and Gnosticism. In one of his works he argued that Greek philosophy had its origin among non-Greeks, claiming that both Plato and Pythagoras were taught by Egyptian scholars. Among his pupils were Origen and Alexander of Jerusalem.
Clement is regarded as a Church Father, like Origen. He is venerated as a saint in Oriental Orthodoxy, Eastern Catholicism and Anglicanism. He was previously revered in the Roman Catholic Church, but his name was removed from the Roman Martyrology in 1586 by Pope Sixtus V on the advice of Baronius.
Biography.
Neither Clement's birthdate or birthplace is known with any degree of certainty. It is conjectured that he was born in around 150. According to Epiphanius Scholasticus, he was born in Athens, but there is also a tradition of an Alexandrian birth.
His parents were pagans, and Clement was a convert to Christianity. In the "Protrepticus" he displays an extensive knowledge of Greek mythology and mystery religions, which could only have arisen from the practise of his family's religion.
Having rejected paganism as a young man due to its perceived moral corruption, he travelled in Greece, Asia Minor, Palestine and Egypt. Clement's journeys were primarily a religious undertaking. In Greece, he encountered an Ionian theologian, who has been identified as Athenagoras of Athens; while in the east, he was taught by an Assyrian, sometimes identified with Tatian, and a Jew, who was possibly Theophilus of Caesarea.
In around 180, Clement reached Alexandria, where he met Pantaenus, who taught at the Catechetical School of Alexandria. Eusebius suggests that Pantaenus was the head of the school, but it is controversial whether the institutions of the school were formalized in this way before the time of Origen. Clement studied under Pantaenus, and was ordained to the priesthood by Pope Julian before 189. Otherwise, virtually nothing is known of Clement's life in Alexandria. He may have been married, a conjecture supported by his writings.
During the Severian persecutions of 202–203, Clement left Alexandria. In 211, Alexander of Jerusalem wrote a letter commending him to the Church of Antioch, which may imply that Clement was living in Cappadocia or Jerusalem at that time. The date and location of his death are unknown.
Theological works.
Trilogy.
Three of Clement's major works have survived in full, and they are collectively referred to as the trilogy:
"Protrepticus".
The "Protrepticus" is, as its title suggests, an exhortation to the pagans of Greece to adopt Christianity, and within it Clement demonstrates his extensive knowledge of pagan mythology and theology. It is chiefly important due to Clement's exposition of religion as an anthropological phenomenon. After a short philosophical discussion, it opens with a history of Greek religion in seven stages. Clement suggests that at first, men mistakenly believed the Sun, the Moon and other heavenly bodies to be gods. The next development was the worship of the products of agriculture, from which he contends the cults of Demeter and Dionysus arose. Man then paid reverence to revenge, and deified human feelings of love and fear, among others. In the following stage, the poets Hesiod and Homer attempt to enumerate the Gods; Hesiod's Theogony giving the number of twelve. Finally, men proclaimed other men, such as Asclepius and Heracles, deities. Discussing idolatry, Clement contends that the objects of primitive religion were unshaped wood and stone, and idols thus arose when such natural items were carved. Following Plato, Clement is critical of all forms of visual art, suggesting that artworks are but illusions and "deadly toys".
Clement criticizes Greek paganism in the "Protrepticus" on the basis that its deities are both false and poor moral examples, and he attacks the mystery religions for their obscurantism and trivial rituals. In particular, the worshippers of Dionysus are ridiculed for their ritual use of children's toys. He suggests at some points that the pagan deities are based on humans, but at others that they are misanthropic demons, and he cites several classical sources in support of this second hypothesis. Clement, like many pre-Nicene fathers, writes favourably about Euhemerus and other rationalist philosophers, on the grounds that they at least saw the flaws in paganism. However, his greatest praise is reserved for Plato, whose apophatic views of God prefigure Christianity.
The figure of Orpheus is prominent throughout the narrative, and Clement contrasts his song, representing pagan superstition, with the divine Logos of Christ. According to Clement, through conversion to Christianity alone can man fully participate in the Logos, which is universal truth.
"Paedagogus".
This work's title, translatable as "tutor", refers to Christ as the teacher of all mankind, and it features an extended metaphor of Christians as children. It is not simply instructional : the author intends to show how the Christian should respond to the Love of God authentically. Clement, following Plato (Republic 4:441), divides life into three elements: character, actions and passions. The first having been dealt with in the "Protrepticus", he devotes the "Paedagogus" to reflections on Christ's role in teaching us to act morally and to control our passions. Despite its explicitly Christian nature, Clement's work draws on Stoic philosophy and pagan literature; Homer alone is cited over sixty times in the work.
Although Christ, like man, is made in the image of God, he alone shares the likeness of God the Father. Christ is both sinless and apathetic, and thus by striving to imitate Christ, man can achieve salvation. To Clement, sin is involuntary, and thus irrational [αλόγον], removed only through the wisdom of the Logos. God's guidance of us away from sin is thus a manifestation of God's universal love for mankind. The word play on λόγος and αλόγον is characteristic of Clement's writing, and may be rooted in the Epicurean belief that relationships between words are deeply reflective of relationships between the objects they signify.
Clement argues for the equality of sexes, on the grounds that salvation is extended to all of mankind equally. Unusually, he suggests that Christ is neither male or female, and that God the Father has both male and female aspects: the eucharist is described as milk from the breast (Christ) of the Father. He is supportive of women playing an active role in the leadership of the church, and provides a list of women he considers inspirational, which includes both Biblical and Classical Greek figures. It has been suggested that Clement's progressive views on gender as set out in the "Paedagogus" were influenced by Gnosticism. However, later in the work, he argues against the Gnostics that faith, not esoteric knowledge [γνῶσις], is required for salvation. According to Clement, it is through faith in Christ that we are enlightened and come to know God.
In the second book, Clement provides practical rules on living a Christian life. He argues against overindulgence in food and in favour of good table manners. While prohibiting drunkenness, he promotes the drinking of alcohol in moderation following . Clement argues for a simple way of life in accordance with the innate simplicity of Christian monotheism. He condemns elaborate and expensive furnishings and clothing, and argues against overly passionate music and perfumes. But Clement does not believe in the abandoning of worldly pleasures and argues that the Christian should be able to express his joy in God's creation through gaiety and partying. He opposes the wearing of garlands, because the picking of the flowers ultimately kills a beautiful creation of God, and the garland resembles the crown of thorns. Clement treats sex at some length. He argues that both promiscuity and sexual abstinence are unnatural, and that the main goal of human sexuality is procreation. Homosexuality, prostitution, concubinage, adultery and coitus with pregnant women should all be avoided as they will not act towards the generation of legitimate offspring.
The third book continues along a similar vein, condemning cosmetics on the grounds that it is our souls, not our bodies, that we should seek to beautify. Clement also opposes the dyeing of men's hair and male depilation as effeminacy. He advises choosing one's company carefully, to avoid being corrupted by immoral people, and while arguing that material wealth is no sin in itself, it is too likely to distract one from the infinitely more important spiritual wealth which is found in Christ. The work finishes with selections of scripture supporting Clement's argument, and following a prayer, the lyrics of a hymn.
"Stromata".
The contents of the "Stromata", as its title suggests, are miscellaneous. Its place in the trilogy is disputed – Clement initially intended to write the "Didasculus", a work which would complement the practical guidance of the "Paedagogus" with a more intellectual schooling in theology. The "Stromata" is less systematic and ordered than Clement's other works, and it has been theorized by André Méhat that it was intended for a limited, esoteric readership. Although Eusebius wrote of eight books of the work, only seven undoubtably survive. Photius, writing in the 9th century, found various text appended to manuscripts of the seven canonical books, which lead Daniel Heinsius to suggest that the original eighth book is lost, and he identified the text purported to be from the eighth book as fragments of the "Hypopotoses".
The first book starts on the topic of Greek philosophy. Consistent with his other writing, Clement affirms that philosophy had a propaedeutic role for the Greek, similar to the function of the law for the Jews. He then embarks on a discussion of the origins of Greek culture and technology, arguing that most of the important figures in the Greek world were foreigners, and (erroneously) that Jewish culture was the most significant influence on Greece. In an attempt to demonstrate the primacy of Moses, Clement gives an extended chronology of the world, wherein he dates the birth of Christ to 25 April or May, 4-2 B.C., and the creation of the world to 5592 B.C. The books ends with a discussion on the origin of languages and the possibility of a Jewish influence on Plato.
The second book is largely devoted to the respective roles of faith and philosophical argument. Clement contends that while both are important, the fear of God is foremost, because through faith one receives divine wisdom. To Clement, scripture is an innately true primitive philosophy which is complemented by human reason through the Logos. Faith is voluntary, and the decision to believe is a crucial fundamental step in becoming closer to God. It is never irrational, as it is founded on the knowledge of the truth of the Logos, but all knowledge proceeds from faith, as first principles are unprovable outside a systematic structure.
The third book covers asceticism. He discusses marriage, which is treated similarly in the "Paedagogus". Clement rejects the Gnostic opposition to marriage, arguing that only men who are uninterested in women should remain celibate, and that sex is a positive good if performed within marriage for the purposes of procreation. However it has not always been so: the Fall occurred because Adam and Eve succumbed to their desire for each other, and copulated before the allotted time. He argues against the idea that Christians should reject their family for an ascetic life, which stems from Luke , contending that Jesus would not have contradicted the precept to "Honour thy Father and thy Mother" (Exodus ), one of the Ten Commandments. Clement concludes that asceticism will only be rewarded if the motivation is Christian in nature, and thus the asceticism of non-Christians such as the gymnosophists is pointless.
Clement begins the fourth book with a belated explanation of the disorganized nature of the work, and gives a brief description of his aims for the remaining three or four books. The fourth book focuses on martyrdom. While all good Christians should be unafraid of death, Clement condemns those who actively seek out a martyr's death, arguing that they do not have sufficient respect for God's gift of life. He is ambivalent whether any believing Christian can become a martyr by virtue of the manner of their death, or whether martyrdom is reserved for those who have lived exceptional lives. Marcionites cannot become martyrs, because they do not believe in the divinity of God the Father – their sufferings are in vain. There is then a digression to the subject of theological epistemology. According to Clement, there is no way of empirically testing the existence of God the Father, because the Logos has revelatory, not analysable meaning, although Christ was an object of the senses. God had no beginning, and is the universal first principle.
The fifth book returns to the subject of faith. Clement argues that truth, justice and goodness can be seen only by the mind, not the eye; faith is a way of accessing the unseeable. He stresses that knowledge of God can only be achieved through faith once ones moral faults have been corrected. This parallels Clement's earlier insistence that martyrdom can only be achieved by those who practice their faith in Christ through good deeds, not those who simply profess their faith. God transcends matter entirely, and thus the materialist cannot truly come to know God. Although Christ was God incarnate, it is our spiritual, not physical comprehension of him which is important.
In the beginning of the sixth book, Clement intends to demonstrate that the works of Greek poets were derived from the prophetic books of the Bible. In order to reinforce his position that the Greeks were inclined towards plagiarism, he cites numerous instances of such inappropriate appropriation by classical Greek writers, reported second-hand from "On Plagiarism", an anonymous 3rd century BC work sometimes ascribed to Aretades. Clement then digresses to the subject of sin and hell, arguing that Adam was not perfect when created, but given the potential to achieve perfection. He espouses broadly universalist doctrine, holding that Christ's promise of salvation is available to all, even those condemned to hell.
The final extant book begins with a description of the nature of Christ, and that of the true Christian, who aims to be as similar as possible to both the Father and the Son. Clement then criticizes the simplistic anthropomorphism of most ancient religions, quoting Xenophanes' famous description of African, Thracian and Egyptian deities. The Greek gods may also have had their origins in the personification of material objects: Ares representing iron, and Dionysus wine. Prayer, and the relationship between love and knowledge are then discussed. seems to contradict the characterization of the true Christian as one who knows; but to Clement knowledge vanishes only in that it is subsumed by the universal love expressed by the Christian in his reverence for his Creator. Following Socrates, he argues that vice arises from a state of ignorance, not from intention. The Christian is a "labourer in God's vineyard", responsible both for his own path to salvation and that of his neighbor. The work ends with an extended passage against the contemporary divisions and heresies within the church.
Other works.
Besides the great trilogy, Clement's only other extant work is the treatise "Salvation for the rich", also known as "Who is the Rich Man who is Saved?". Having begun with a scathing criticism of the corrupting effects of money and misguided servile attitudes towards the wealthy, Clement discusses the implications of . The rich are either unconvinced by the promise of eternal life, or unaware of the conflict between the possession of material and spiritual wealth, and the good Christian has a duty to guide them towards a better life through the Gospel. Jesus' words are not to be taken literally – we should seek the supercelestial [ὑπερουράνιος] meaning in which the true route to salvation is revealed. The holding of material wealth in itself is not a wrong, as long as it is used charitably, but men should be careful not to let their wealth dominate their spirit. It is more important to give up sinful passions than external wealth. If the rich man is to be saved, all he must do is to follow the two commandments, and while material wealth is of no value to God, it can be used to alleviate the suffering of our neighbor.
Other known works exist in fragments alone, including the four eschatological works in the secret tradition: "Hypotyposes", "Excerpta ex Theodoto", "Eclogae Propheticae" and the "Adumbraetiones". These cover Clement's celestial hierarchy, a complex schema in which the universe is headed by the Face of God, below which lie seven "protoctists", followed by archangels, angels and humans. According to Jean Daniélou, this schema is inherited from a Judaeo-Christian esotericism, followed by the Apostles, which was only imparted orally to those Christians who could be trusted which such mysteries. The "proctocists" are the first beings created by God, and act as priests to the archangels. Clement identifies them both as the "Eyes of the Lord" and with the Thrones. Clement characterizes the celestial forms as entirely different from anything earthly, although he argues that members of each order only seem incorporal to those of lower orders. According to the "Eclogae Propheticae", every thousand years every member of each order moves up a degree, and thus men can become angels. Even the "protoctists" can be elevated, although their new position in the hierarchy is not clearly defined. The apparent contradiction between the fact that there can be only seven "protoctists" but also a vast number of archangels to be promoted to their order is problematic. The commonest modern explanation is that the number seven is not meant to be taken literally, but has a principally numerological significance.
We know the titles of several lost works because of a list in Eusebius' "Ecclesiastical History", 6.13.1-3. They include the "Outlines", in eight books, and "Against Judaizers". Others are known only from mentions in Clement's own writings, including "On Marriage" and "On Prophecy", although few are attested by other writers and it is difficult to separate works which he intended to write from those which were actually completed.
The Mar Saba letter was attributed to Clement by Morton Smith, but there remains much debate today over whether it is an authentic letter from Clement, an ancient pseudepigraph or a modern forgery. If authentic, its main significance would be in its relating that the apostle Mark came to Alexandria from Rome and there wrote a more spiritual gospel, which he entrusted to the church in Alexandria on his death: if genuine, the letter pushes back the tradition related by Eusebius connecting Mark with Alexandria by a century.
Legacy.
Eusebius is the first writer to provide an account of Clement's life and works, in the "Church History". There are two separate sections of the work dedicated to Clement (5.11 and 6.11), the latter of which seems decidedly out of place, and Valesius argued that this was evidence that Eusebius never revised his work. Eusebius provides a list of Clement's works, biographical information, and an extended quotation from the "Stromata".
Photios I of Constantinople writes against Clement's theology in the "Bibliotheca", although he is appreciative of Clement's learning and the literary merits of his work. In particular, he is highly critical of the "Hypotyposes", a work of biblical exegesis of which only a few fragments have survived. Photius compared Clement's treatise, which like his other works was highly syncretic, featuring ideas of Hellenistic, Jewish and Gnostic origin, unfavourably against the prevailing orthodoxy of the 9th century. Among the particular ideas Photius deemed heretical were:
Down to the seventeenth century he was venerated as a saint in Catholicism. His name was to be found in the martyrologies, and his feast fell on the fourth of December. But when the Roman Martyrology was revised by Pope Clement VIII his name was dropped from the calendar on the advice of Cardinal Baronius. Benedict XIV maintained this decision of his predecessor on the grounds that Clement's life was little known, that he had never obtained public cultus in the Church, and that some of his doctrines were, if not erroneous, at least suspect. Thus Clement is not revered as a saint in contemporary Roman Catholicism, nor is he considered a saint in much of Eastern Orthodox Christianity. Clement's veneration is somewhat limited; he is commemorated nonetheless in Anglicanism. As well, the Universal Catholic Church's cathedral in Dallas is dedicated to him.
As one of the earliest of the Church fathers whose works have survived, he is the subject of a significant amount of recent academic work, mainly focusing on the relationship between his thought and non-Christian philosophy and his influence on Origen.

</doc>
<doc id="7344" url="http://en.wikipedia.org/wiki?curid=7344" title="Cogito ergo sum">
Cogito ergo sum

 (, ; Classical Latin: , "I think, therefore I am", or better "I am thinking, therefore I exist") is a philosophical proposition by . The simple meaning of the Latin phrase is that thinking about one’s existence proves—in and of itself—that an "I" exists to do the thinking; or, as Descartes explains, "[W]e cannot doubt of our existence while we doubt … ."
This proposition became a fundamental element of Western philosophy, as it was perceived to form a foundation for all knowledge. While other knowledge could be a figment of imagination, deception or mistake, the very act of doubting one's own existence arguably serves as proof of the reality of one's own existence, or at least of one's thought.
Descartes' original phrase, (), appeared in his "Discourse on the Method" (1637), which was written in French rather than Latin to reach a wider audience in his country than scholars. He used the Latin "cogito ergo sum" in the later "Principles of Philosophy" (1644).
The argument is popularly known in the English speaking world as "the argument" or, more briefly, as "the ".
In Descartes' writings.
Descartes first wrote the phrase in French in his 1637 "Discours De la Méthode". He referred to it in Latin without explicitly stating the familiar form of the phrase in his 1641 "Meditationes de Prima Philosophia". The earliest written record of the phrase in Latin is in his 1644 "Principia Philosophiae", where he also provides a clear explanation of his intent in a margin note. Fuller forms of the phrase are due to other authors. [Formatting note: "cogito" variants in this section are highlighted in boldface to facilitate comparison; "italics" only as in originals.]
In "Discours de la Méthode" (1637).
The phrase first appeared (in French) in Descartes' 1637 "Discours de la Méthode" (full title in English: "Discourse on the Method of Rightly Conducting the Reason, and Seeking Truth in the Sciences"). From the first paragraph of Part IV:
In "Meditationes de Prima Philosophia" (1641).
In 1641, Descartes published (in Latin) "Meditationes de Prima Philosophia" (English: "Meditations on first philosophy") in which he referred to the proposition, though not explicitly as "cogito ergo sum" in Meditation II:
In "Principia Philosophiae" (1644).
In 1644, Descartes published (in Latin), "Principia Philosophiae" (English: Principles of Philosophy) where the phrase "ego cogito, ergo sum" appears in Part 1, article 7:
Descartes' margin note for the above paragraph is:
Other forms.
The proposition is sometimes given as . This fuller form was penned by the eloquent French literary critic, Antoine Léonard Thomas, in an award-winning 1765 essay in praise of Descartes, where it appeared as In English, this is "Since I doubt, I think; since I think I exist"; with rearrangement and compaction, "I doubt, therefore I think, therefore I am", or in Latin, "dubito, ergo cogito, ergo sum".
A further expansion, ("…—a thinking thing") extends the "cogito" with Descartes' statement in the subsequent Meditation, , or, in English, "I am a thinking (conscious) thing, that is, a being who doubts, affirms, denies, knows a few objects, and is ignorant of many …". This has been referred to as "the expanded "cogito"".
Interpretation.
The phrase "cogito ergo sum" is not used in Descartes' "Meditations on First Philosophy" but the term "the "cogito"" is used to refer to an argument from it. In the Meditations, Descartes phrases the conclusion of the argument as "that the proposition, "I am, I exist," is necessarily true whenever it is put forward by me or conceived in my mind." ("Meditation" II)
At the beginning of the second meditation, having reached what he considers to be the ultimate level of doubt — his argument from the existence of a deceiving god — Descartes examines his beliefs to see if any have survived the doubt. In his belief in his own existence, he finds that it is impossible to doubt that he exists. Even if there were a deceiving god (or an evil demon), one's belief in their own existence would be secure, for there is no way one could be deceived unless one existed in order to be deceived.
But I have convinced myself that there is absolutely nothing in the world, no sky, no earth, no minds, no bodies. Does it now follow that I, too, do not exist? No. If I convinced myself of something [or thought anything at all], then I certainly existed. But there is a deceiver of supreme power and cunning who deliberately and constantly deceives me. In that case, I, too, undoubtedly exist, if he deceives me; and let him deceive me as much as he can, he will never bring it about that I am nothing, so long as I think that I am something. So, after considering everything very thoroughly, I must finally conclude that the proposition, "I am, I exist," is necessarily true whenever it is put forward by me or conceived in my mind. (AT VII 25; CSM II 16–17)
There are three important notes to keep in mind here. First, he claims only the certainty of "his own" existence from the first-person point of view — he has not proved the existence of other minds at this point. This is something that has to be thought through by each of us for ourselves, as we follow the course of the meditations. Second, he does not say that his existence is necessary; he says that "if he thinks", then necessarily he exists (see the instantiation principle). Third, this proposition "I am, I exist" is held true not based on a deduction (as mentioned above) or on empirical induction but on the clarity and self-evidence of the proposition.
Descartes does not use this first certainty, the "cogito", as a foundation upon which to build further knowledge; rather, it is the firm ground upon which he can stand as he works to restore his beliefs. As he puts it:
Archimedes used to demand just one firm and immovable point in order to shift the entire earth; so I too can hope for great things if I manage to find just one thing, however slight, that is certain and unshakable. (AT VII 24; CSM II 16)
According to many of Descartes' specialists, including Étienne Gilson, the goal of Descartes in establishing this first truth is to demonstrate the capacity of his criterion — the immediate clarity and distinctiveness of self-evident propositions — to establish true and justified propositions despite having adopted a method of generalized doubt. As a consequence of this demonstration, Descartes considers science and mathematics to be justified to the extent that their proposals are established on a similarly immediate clarity, distinctiveness, and self-evidence that presents itself to the mind. The originality of Descartes' thinking, therefore, is not so much in expressing the cogito — a feat accomplished by other predecessors, as we shall see — but on using the cogito as demonstrating the most fundamental epistemological principle, that science and mathematics are justified by relying on clarity, distinctiveness, and self-evidence.
Baruch Spinoza in "Principia philosophiae cartesianae" at its "Prolegomenon" identified "cogito ergo sum" the "ego sum cogitans" (I am a thinking being) as the thinking substance with his ontological interpretation. It can also be considered that "Cogito ergo sum" is needed before any living being can go further in life".
Predecessors.
Although the idea expressed in "cogito ergo sum" is widely attributed to Descartes, he was not the first to mention it. Plato spoke about the "knowledge of knowledge" (Greek "νόησις νοήσεως" - "nóesis noéseos") and Aristotle explains the idea in full length:
But if life itself is good and pleasant (...) and if one who sees is conscious that he sees, one who hears that he hears, one who walks that he walks and similarly for all the other human activities there is a faculty that is conscious of their exercise, so that whenever we perceive, we are conscious that we perceive, and whenever we think, we are conscious that we think, and to be conscious that we are perceiving or thinking is to be conscious that we exist... ("Nicomachean Ethics", 1170a25 ff.)
Augustine of Hippo in "De Civitate Dei" writes "Si […] fallor, sum" ("If I am mistaken, I am") (book XI, 26), and also anticipates modern refutations of the concept. Furthermore, in the "Enchiridion" Augustine attempts to refute skepticism by stating, "[B]y not positively affirming that they are alive, the skeptics ward off the appearance of error in themselves, yet they do make errors simply by showing themselves alive; one cannot err who is not alive. That we live is therefore not only true, but it is altogether certain as well" (Chapter 7 section 20). Another predecessor was Avicenna's "Floating Man" thought experiment on human self-awareness and self-consciousness.
The 8th Century Hindu philosopher Adi Shankara wrote in a similar fashion, No one thinks, 'I am not', arguing that one's existence cannot be doubted, as there must be someone there to doubt.
Criticisms.
There have been a number of criticisms of the argument. One concerns the nature of the step from "I am thinking" to "I exist." The contention is that this is a syllogistic inference, for it appears to require the extra premise: "Whatever has the property of thinking, exists", a premise Descartes did not justify. In fact, he conceded that there would indeed be an extra premise needed, but denied that the "cogito" is a syllogism (see below).
To argue that the "cogito" is not a syllogism, one may call it self-evident that "Whatever has the property of thinking, exists". In plain English, it seems incoherent to actually doubt that one exists and is doubting. Strict skeptics maintain that only the property of 'thinking' is indubitably a property of the meditator (presumably, they imagine it possible that a thing thinks but does not exist). This countercriticism is similar to the ideas of
Jaakko Hintikka, who offers a nonsyllogistic interpretation of "cogito ergo sum". He claimed that one simply cannot doubt the proposition "I exist". To be mistaken about the proposition would mean something impossible: I do not exist, but I am still wrong.
Perhaps a more relevant contention is whether the "I" to which Descartes refers is justified.
In "Descartes, The Project of Pure Enquiry", Bernard Williams provides a history and full evaluation of this issue. Apparently, the first scholar who raised the problem was Pierre Gassendi. He "points out that recognition that one has a set of thoughts does not imply that one is a particular thinker or another. Were we to move from the observation that there is thinking occurring to the attribution of this thinking to a particular agent, we would simply assume what we set out to prove, namely, that there exists a particular person endowed with the capacity for thought ". In other words, "the only claim that is indubitable here is the agent-independent claim that there is cognitive activity present" The objection, as presented by Georg Lichtenberg, is that rather than supposing an entity that is thinking, Descartes should have said: "thinking is occurring." That is, whatever the force of the "cogito", Descartes draws too much from it; the existence of a thinking thing, the reference of the "I," is more than the "cogito" can justify. Friedrich Nietzsche criticized the phrase in that it presupposes that there is an "I", that there is such an activity as "thinking", and that "I" know what "thinking" is. He suggested a more appropriate phrase would be "it thinks." In other words the "I" in "I think" could be similar to the "It" in "It is raining." David Hume claims that the philosophers who argue for a self that can be found using reason are confusing "similarity" with "identity". This means that the similarity of our thoughts and the continuity of them in this similarity do not mean that we can identify ourselves as a self but that our thoughts are similar.
Williams' argument in detail.
In addition to the preceding two arguments against the "cogito", other arguments have been advanced by Bernard Williams. He claims, for example, that what we are dealing with when we talk of thought, or when we say "I am thinking," is something conceivable from a third-person perspective; namely objective "thought-events" in the former case, and an objective thinker in the latter.
Williams provides a meticulous and exhaustive examination of this objection. He argues, first, that it is impossible to make sense of "there is thinking" without relativizing it to "something." However, this something cannot be Cartesian egos, because it is impossible to differentiate objectively between things just on the basis of the pure content of consciousness.
The obvious problem is that, through introspection, or our experience of consciousness, we have no way of moving to conclude the existence of any third-personal fact, to conceive of which would require something above and beyond just the purely subjective contents of the mind.
Søren Kierkegaard's critique.
The Danish philosopher Søren Kierkegaard provided a critical response to the "cogito". Kierkegaard argues that the "cogito" already presupposes the existence of "I", and therefore concluding with existence is logically trivial. Kierkegaard's argument can be made clearer if one extracts the premise "I think" into two further premises:
""x" thinks"
"I am that "x""
"Therefore I think"
"Therefore I am"
Where "x" is used as a placeholder in order to disambiguate the "I" from the thinking thing.
Here, the "cogito" has already assumed the "I"'s existence as that which thinks. For Kierkegaard, Descartes is merely "developing the content of a concept", namely that the "I", which already exists, thinks.
Kierkegaard argues that the value of the "cogito" is not its logical argument, but its psychological appeal: a thought must have something that exists to think the thought. It is psychologically difficult to think "I do not exist". But as Kierkegaard argues, the proper logical flow of argument is that existence is already assumed or presupposed in order for thinking to occur, not that existence is concluded from that thinking.
John Macmurray's Rejection.
The Scottish Philosopher John Macmurray rejects the "cogito" outright in order to place action at the center of a philosophical system. "We must reject this, both as standpoint and as method. If this be philosophy, then philosophy is a bubble floating in an atmosphere of unreality." The reliance on thought creates an irreconcilable dualism between thought and action in which the unity of experience is lost. In order to formulate a more adequate "cogito", Macmurray proposes the substitution of "I do" for "I think".
Skepticism.
Many philosophical skeptics and particularly radical skeptics would say that indubitable knowledge does not exist, is impossible, or has not been found yet, and would apply this criticism to the assertion that the "cogito" is beyond doubt.

</doc>
<doc id="7345" url="http://en.wikipedia.org/wiki?curid=7345" title="Carl Barks">
Carl Barks

Carl Barks (March 27, 1901 – August 25, 2000) was an American cartoonist, author, and painter. He is best known for his comics about Donald Duck and as the creator of Scrooge McDuck. He worked anonymously until late in his career; fans dubbed him The Duck Man and The Good Duck Artist. In 1987, Barks was one of the three inaugural inductees of the Will Eisner Comic Book Hall of Fame.
Barks worked for the Disney Studio and Western Publishing where he created Duckburg and many of its inhabitants, such as Scrooge McDuck (1947), Gladstone Gander (1948), the Beagle Boys (1951), The Junior Woodchucks (1951), Gyro Gearloose (1952), Cornelius Coot (1952), Flintheart Glomgold (1956), John D. Rockerduck (1961) and Magica De Spell (1961). Cartoonist Will Eisner called him "the Hans Christian Andersen of comic books."
Biography.
Barks was born in Merrill, Oregon to William Barks and his wife Arminta Johnson. He had an older brother named Clyde. Barks once stated that his paternal ancestors were Dutch and his maternal ancestors were Scottish. His paternal grandparents were David Barks and his wife Ruth Shrum. His maternal grandparents were Carl Johnson and his wife Suzanna Massey, but little else is known about his ancestors. Barks was the descendant of Jacob Barks who came to Missouri from North Carolina around 1800. They lived in Marble Hill in Bollinger County. Jacob Barks' son Isaac was the father of the David Barks noted above.
Childhood.
According to Barks' description of his childhood, he was a rather lonely child. His parents owned one square mile (2.6 km²) of land that served as their farm. The nearest neighbor lived half a mile (800 m) away, but he was more an acquaintance to Barks' parents than a friend. The closest school was about two miles (3 km) away and Barks had to walk that distance every day. The rural area had few children, though, and Barks later remembered that his school had only about eight or ten students including him. He had high praise for the quality of the education he received in that small school. "Schools were good in those days," he used to say.
The lessons lasted from nine o'clock in the morning to four o'clock in the afternoon and then he had to return to the farm. There he remembered not having anybody to talk to, as his parents were busy and he had little in common with his brother.
In 1908, William Barks (in an attempt to increase the family income) moved with his family to Midland, Oregon, some miles north of Merrill, to be closer to the new railway lines. He established a new stock-breeding farm and sold his produce to the local slaughterhouses.
Nine-year-old Clyde and seven-year-old Carl worked long hours there. But Carl later remembered that the crowd which gathered at Midland's market place made a strong impression on him. This was expected, as he was not used to crowds up until then. According to Barks, his attention was mostly drawn to the cowboys that frequented the market with their revolvers, strange nicknames for each other and sense of humor.
By 1911, they had been successful enough to move to Santa Rosa, California. There they started cultivating vegetables and set up some orchards. Unfortunately, the profits were not as high as William expected and they started having financial difficulties. William's anxiety over them was probably what caused his first nervous breakdown.
As soon as William recovered, he made the decision to move back to Merrill. The year was 1913, and Barks was already 12 years old; but, due to the constant moving, he had not yet managed to complete grade school. He resumed his education at this point and finally managed to graduate in 1916.
1916 served as a turning point in Barks' life for various reasons. First, Arminta, his mother, died in this year. Second, his hearing problems, which had already appeared earlier, had at the time become severe enough for him to have difficulties listening to his teachers talking. His hearing would continue to get worse later, but at that point he had not yet acquired a hearing aid. Later in life, he couldn't do without one. Third, the closest high school to their farm was five miles (8 km) away and even if he did enlist in it, his bad hearing was likely to contribute to his learning problems. He had to decide to stop his school education, much to his disappointment.
From job to job.
Barks started taking various jobs but had little success in such occupations as a farmer, woodcutter, turner, mule driver, cowboy and printer. From his jobs he learned, he later averred, how eccentric, stubborn and unpredictable men, animals and machines can be. At the same time he interacted with colleagues, fellow breadwinners who had satirical disposition towards even their worst troubles. Barks later declared that he was sure that if not for a little humor in their troubled lives, they would certainly go insane. It was an attitude towards life that Barks would adopt. Later he would say it was natural for him to satirize the secret yearnings and desires, the pompous style and the disappointments of his characters. According to Barks, this period of his life would later influence his best known fictional characters: Walt Disney's Donald Duck and his own Scrooge McDuck.
Donald's drifting from job to job was reportedly inspired by Barks' own experiences. So was his usual lack of success. And even in those that he was successful this would be temporary, just until a mistake or chance event caused another failure, another disappointment for the frustrated duck. Barks also reported that this was another thing he was familiar with.
Scrooge's main difference to Donald, according to Barks, was that he too had faced the same difficulties in his past but through intelligence, determination and hard work, he was able to overcome them. Or, as Scrooge himself would say to Huey, Dewey and Louie: by being "tougher than the toughies and smarter than the smarties." Even in the present of his stories Scrooge would work to solve his many problems, even though the stories would often point out that his constant efforts seemed futile at the end. In addition, Scrooge was quite similar to his creator in appearing often to be as melancholic, introspective and secretive as he was.
Through both characters Barks would often exhibit his rather sarcastic sense of humor. It seems that this difficult period for the artist helped shape many of his later views in life that were expressed through his characters.
Professional artist.
At the same time Barks had started thinking about turning a hobby that he always enjoyed into a profession: that of drawing. Since his early childhood he spent his free time by drawing on any material he could find. He had attempted to improve his style by copying the drawings of his favorite comic strip artists from the newspapers where he could find them. As he later said, he wanted to create his own facial expressions, figures and comical situations in his drawings but wanted to study the master comic artists' use of the pen and their use of color and shading.
Among his early favorites were Winsor McCay (mostly known for "Little Nemo") and Frederick Burr Opper (mostly known for "Happy Hooligan") but he would later study any style that managed to draw his attention.
At 16 he was mostly self-taught but at this point he decided to take some lessons through correspondence. He only followed the first four lessons and then had to stop because his working left him with little free time. But as he later said, the lessons proved very useful in improving his style.
By December 1918, he left his father's home to attempt to find a job in San Francisco, California. He worked for a while in a small publishing house while attempting to sell his drawings to newspapers and other printed material with little success.
First and second marriages.
While he continued drifting through various jobs, he met Pearl Turner (1904–1987). In 1921 they married and had two daughters:
In 1923 he returned to his paternal farm in Merrill in an attempt to return to the life of a farmer, but that ended soon. He continued searching for a job while attempting to sell his drawings. He soon managed to sell some of them to "Judge" magazine and then started having success submitting to the Minneapolis-based "Calgary-Eye-Opener", a racy men's cartoon magazine of the era. He was eventually hired as editor and scripted and drew most of the contents while continuing to sell occasional work to other magazines. His salary of 90 dollars a month was considered respectable enough for the time. A facsimile of one of the racy magazines he did cartoons for in this period, "Coo Coo" #1, was published by Hamilton Comics in 1997.
Meanwhile he had his first divorce. He and Pearl were separated in 1929 and divorced in 1930. After he moved to Minneapolis, Minnesota, where "Calgary-Eye-Opener" had its offices he met Clara Balken who in 1938 became his second wife.
Disney.
In November 1935, when he learned that Walt Disney was seeking more artists for his studio, Barks decided to apply. He was approved for a try-out which entailed a move to Los Angeles, California. He was one of two in his class of trainees who was hired. His starting salary was 20 dollars a week. He started at Disney Studios in 1935, more than a year after the debut of Donald Duck on June 9, 1934 in the short animated film "The Wise Little Hen".
Barks initially worked as an inbetweener. This involved being teamed and supervised by one of the head animators who did the key poses of character action (often known as extremes) for which the inbetweeners did the drawings between the extremes to create the illusion of movement. While an inbetweener, Barks submitted gag ideas for cartoon story lines being developed and showed such a knack for creating comical situations that by 1937 he was transferred to the story department. His first story sale was the climax of Modern Inventions, for a sequence where a robot barber chair gives Donald Duck a haircut on his bottom.
In 1937 when Donald Duck became the star of his own series of cartoons instead of co-starring with Mickey Mouse and Goofy as previously, a new unit of storymen and animators was created devoted solely to this series. Though he originally just contributed gag ideas to some duck cartoons by 1937 Barks was (principally with partner Jack Hannah) originating story ideas that were storyboarded and (if approved by Walt) put into production. He collaborated on such cartoons as "Donald's Nephews" (1938), "Donald's Cousin Gus" (1939), "Mr. Duck Steps Out" (1940),"Timber" (1941), "The Vanishing Private" (1942) and "The Plastics Inventor" (1944).
The Good Duck Artist.
Unhappy at the emerging wartime working conditions at Disney, and bothered by ongoing sinus problems caused by the studio's air conditioning, Barks quit in 1942. Shortly before quitting, he moonlighted as a comic book artist, contributing half the artwork for a one-shot comic book (the other half of the art being done by story partner Jack Hannah) titled "Donald Duck Finds Pirate Gold". This 64-page story was adapted by Donald Duck comic strip writer Bob Karp from an unproduced feature, and published in October 1942 in Dell Comics "Four Color Comics" #9. It was the first Donald Duck story originally produced for an American comic book and also the first involving Donald and his nephews in a treasure hunting expedition, in this case for the treasure of Henry Morgan. Barks would later use the treasure hunting theme in many of his stories. This actually was not his first work in comics, as earlier the same year Barks along with Hannah and fellow storyman Nick George scripted "Pluto Saves the Ship", which was among the first original Disney comic book stories published in the United States.
After quitting the Disney Studio, Barks relocated to the Hemet/San Jacinto area in the semi-desert inland empire region east of Los Angeles where he hoped to start a chicken farm.
When asked which of his stories was a favorite in several interviews Barks cited the ten-pager in "Walt Disney's Comics and Stories" #146 (Nov. 1952) in which Donald tells the story of the chain of unfortunate events that took place when he owned a chicken farm in a town which subsequently was renamed Omelet. Likely one reason it was a favorite is that it was inspired by Barks' own experiences in the poultry business.
But to earn a living in the meantime he inquired whether Western Publishing, which had published "Pirate Gold", had any need for artists for Donald Duck comic book stories. He was immediately assigned to illustrate the script for a ten-page Donald Duck story for the monthly "Walt Disney's Comics and Stories". At the publisher's invitation he revised the storyline and the improvements impressed the editor sufficiently to invite Barks to try his hand at contributing both the script and the artwork of his follow-up story. This set the pattern for Barks' career in that (with rare exceptions) he provided art (pencil, inking, solid blacks and lettering) and scripting for his stories.
"The Victory Garden", that initial ten-page story published in April, 1943 was the first of about 500 stories featuring the Disney ducks Barks would produce for Western Publishing over the next three decades, well into his purported retirement. These can be mostly divided into three categories:
Barks' artistic growth during his first decade in comics saw a transformation from rather rudimentary storytelling derived from his years as an animation artist and storyman into a virtuoso creator of complex narratives, notably in his longer adventure tales. According to critic Geoffrey Blum, the process that saw its beginnings in 1942's Pirate Gold first bore its full fruit in 1950's 'Vacation Time,' which he describes as 'a visual primer for reading comics and understanding... the form..."
He surrounded Donald Duck and nephews Huey, Dewey and Louie with a cast of eccentric and colorful characters, such as the aforementioned Scrooge McDuck, the wealthiest duck in the world; Gladstone Gander, Donald's obscenely lucky cousin; inventor Gyro Gearloose; the persistent Beagle Boys; the sorceress Magica De Spell; Scrooge's rivals Flintheart Glomgold and John D. Rockerduck; Daisy's nieces April, May and June; Donald's neighbor Jones, and The Junior Woodchucks organization.
Barks's stories (whether humorous adventures or domestic comedies) often exhibited a wry, dark irony born of hard experience. The ten-pagers showcased Donald as everyman, struggling against the cruel bumps and bruises of everyday life with the nephews often acting as a Greek chorus commenting on the unfolding disasters Donald wrought upon himself. Yet while seemingly defeatist in tone, the humanity of the characters shines through in their persistence despite the obstacles. These stories found popularity not only among young children but adults as well. Despite the fact that Barks had done little traveling his adventure stories often had the duck clan globe trotting to the most remote or spectacular of places. This allowed Barks to indulge his penchant for elaborate backgrounds that hinted at his thwarted ambitions of doing realistic stories in the vein of Hal Foster's "Prince Valiant".
Third marriage.
As Barks blossomed creatively, his marriage to Clara deteriorated. This is the period referred to in Barks' famed quip that he could feel his creative juices flowing while the whiskey bottles hurled at him by a tipsy Clara flew by his head. They were divorced in 1951, his second and last divorce. In this period Barks dabbled in fine art, exhibiting paintings at local art shows. It was at one of these in 1952 he became acquainted with fellow exhibitor Margaret Wynnfred Williams (1917 – March 10, 1993), nicknamed Garé. She was an accomplished landscape artist, some of whose paintings are in the collection of the Leanin' Tree Museum of Western Art. During her lifetime, and to this day, note cards of her paintings are available from Leanin' Tree. Her nickname appears as a store name in the story "Christmas in Duckburg", featured on page 1 of Walt Disney’s Christmas Parade #9, published in 1958. Soon after they met, she started assisting Barks, handling the solid blacks and lettering, both of which he had found onerous. They married in 1954 and the union lasted until her death.
No longer anonymous.
People who worked for Disney (and its comic book licensees) generally did so in relative anonymity; stories would only carry Walt Disney's name and (sometimes) a short identification number. Prior to 1960 Barks' identity remained a mystery to his readers. However, many readers recognized Barks' work and drawing style, and began to call him the Good Duck Artist, a label which stuck even after his true identity was discovered by fans in the late 1950s. Malcolm Willits was the first person to learn Barks's name and address, but two brothers named John and Bill Spicer became the first fans to contact Barks after independently discovering the same information. After Barks received a 1960 visit from the Spicer brothers and Ron Leonard, he was no longer anonymous, as word of his identity spread through the emerging network of comic book fandom fanzines and conventions.
Later life.
Carl Barks retired in 1966, but was persuaded by editor Chase Craig to continue to script stories for Western. The last new comic book story drawn by Carl Barks was a Daisy Duck tale ("The Dainty Daredevil") published in "Walt Disney Comics Digest" issue 5 (Nov. 1968). When bibliographer Michael Barrier asked Barks why he drew it, Barks' vague recollection was no one was available and he was asked to do it as a favor by Craig.
He wrote one Uncle Scrooge story, three Donald Duck stories and from 1970–1974 was the main writer for the Junior Woodchucks comic book (issues 6 through 25). The latter included environmental themes that Barks first explored in 1957 ["Land of the Pygmy Indians", Uncle Scrooge #18]. Barks also sold a few sketches to Western that were redrawn as covers. For a time the Barkses lived in Goleta, California before returning to the Inland Empire by moving to Temecula.
To make a little extra money beyond what his pension and scripting earnings brought in, Barks started doing oil paintings to sell at the local art shows where he and Garé exhibited. Subjects included humorous depictions of life on the farm and portraits of Native American princesses. These skillfully rendered paintings encouraged fan Glenn Bray to ask Barks if he could commission a painting of the ducks ("A Tall Ship and a Star to Steer Her By", taken from the cover of "Walt Disney's Comics and Stories" #108 by Barks). This prompted Barks to contact George Sherman at Disney's Publications Department to request permission to produce and sell oil paintings of scenes from his stories. In July 1971 Barks was granted a royalty-free license by Disney. When word spread that Barks was taking commissions from those interested in purchasing an oil of the ducks, much to his astonishment the response quickly outstripped what he reasonably could produce in the next few years.
When Barks expressed dismay at coping with the backlog of orders he faced, fan/dealers Bruce Hamilton and Russ Cochran suggested Barks instead auction his paintings at conventions and via Cochran's catalog "Graphic Gallery". By September 1974 Barks had discontinued taking commissions.
At Boston's NewCon convention, in October 1975, the first Carl Barks oil painting auctioned at a comic book convention ("She Was Spangled and Flashy") sold for $2,500. Subsequent offerings saw an escalation in the prices realized.
In 1976, Barks and Garé went to Boston for the NewCon show, their first comic convention appearance. Among the other attendees was famed Little Lulu comic book scripter John Stanley; despite both having worked for Western Publishing this was the first time they met. The highlight of the convention was the auctioning of what was to that time the largest duck oil painting Barks had done, "July Fourth in Duckburg", which included depictions of several prominent Barks fans and collectors. It sold for a then record high amount: $6,400.
Soon thereafter a fan sold unauthorized prints of some of the Scrooge McDuck paintings, leading Disney to withdraw permission for further paintings. To meet demand for new work Barks embarked on a series of paintings of non-Disney ducks and fantasy subjects such as Beowulf and Xerxes. These were eventually collected in the limited-edition book "Animal Quackers".
As the result of heroic efforts by ' producer Gary Kurtz and screenwriter Edward Summer, Disney relented and in 1981, allowed Barks to do a now seminal oil painting called "Wanderers of Wonderlands" for a breakthrough limited edition book entitled '. The book collected 11 classic Barks stories of Uncle Scrooge colored by artist Peter Ledger along with a new Scrooge story by Barks done storybook style with watercolor illustrations, "Go Slowly, Sands of Time". After being turned down by every major publisher in New York City, Kurtz and Summer published the book through Celestial Arts, which Kurtz acquired partly for this purpose. The book went on to become the model for virtually every important collection of comic book stories. It was the first book of its kind ever reviewed in "Time Magazine "and subsequently in "Newsweek", and the first book review in "Time Magazine" with large color illustrations.
In 1977 and 1982, Barks attended the San Diego Comic Con. As with his appearance in Boston, the response to his presence was overwhelming, with long lines of fans waiting to meet Barks and get his autograph.
In 1981, Bruce Hamilton and Russ Cochran, two long-time Disney comics fans, decided to combine forces to bring greater recognition to the works of Carl Barks. Their first efforts went into establishing Another Rainbow Publishing, the banner under which they produced and issued the award-winning book, "The Fine Art of Walt Disney´s Donald Duck by Carl Barks", a comprehensive collection of the Disney duck paintings of this artist and storyteller. Not long after, the company began producing fine art lithographs of many of these paintings, in strictly limited editions, all signed by Barks, who eventually produced many original works for the series.
In 1983 Another Rainbow took up the daunting task of collecting the entire Disney comic book ouvré of Barks—over 500 stories in all—in the ten-set, thirty-volume "Carl Barks Library". These oversized hardbound volumes reproduced Barks´ pages in pristine black and white line art, as close as possible to the way he would originally drawn them, and included mountains of special features, articles, reminiscences, interviews, storyboards, critiques, and more than a few surprises. This monumental project was finally completed in mid-1990.
In 1985 a new division was founded, Gladstone Publishing, which took up the then-dormant Disney comic book license. Gladstone introduced a whole new generation of Disney comic book readers to the wondrous storytelling of such luminaries as Barks, Paul Murry, and Floyd Gottfredson, as well as presenting the first works of modern Disney comics masters Don Rosa and William Van Horn. Seven years after Gladstone's founding, the "Carl Barks Library" was revived as the "Carl Barks Library in Color", a full-color, high-quality squarebound comic albums (including the first-ever Carl Barks trading cards).
Barks relocated one last time to Grants Pass, Oregon near where he grew up, partly at the urging of friend and "Broom Hilda" artist Russell Myers, who lived in the area. The move also was motivated, Barks stated in another famous quip, by Temecula being too close to Disneyland and thus facilitating a growing torrent of drop-in visits by vacationing fans. In this period Barks made only one public appearance, at a comic book shop near Grants Pass.
From 1993 to 1998, Barks' career was managed by the "Carl Barks Studio" (Bill Grandey and Kathy Morby—They had sold Barks original art since 1979). This involved numerous art projects and activities, including a tour of 11 European countries in 1994, Iceland being the first foreign country he ever visited. Barks appeared at the first of many Disneyana conventions in 1993. Silk screen prints of paintings along with high-end art objects (such as original water colors, bronze figurines and ceramic tiles) were produced based on designs by Barks.
During the summer of 1994 and until his death, Carl Barks & his studio personally assigned Peter Reichelt, a museum exhibition producer from Mannheim, Germany, as his agent for Europe. Publisher "Edition 313" put out numerous lithographs. In 1997, tensions between Barks and the Studio eventually resulted in a lawsuit that was settled with an agreement that included the disbanding of the Studio. Barks never traveled to make another Disney appearance. He was represented by Rev. Ed Bergen, as he completed a final project. Gerry Tank and Jim Mitchell were to assist Barks in his final years.
During his Carl Barks Studio years, Barks created two more stories: the script for the final Uncle Scrooge story "Horsing Around with History", which was first published in Denmark in 1994 with Bill Van Horn art. The Barks outlines for Barks final Donald Duck story "Somewhere in Nowhere", were first published in 1997, in Italy, with art by Pat Block.
Austrian artist Gottfried Helnwein curated and organized the first solo museum-exhibition of Carl Barks. Between 1994 and 1998 the retrospective was shown in ten European museums and seen by more than 400,000 visitors.
At the same time in spring 1994, Reichelt and Ina Brockmann designed a special museum exhibition tour about Barks' life and work. Also represented for the first time at this exhibition were Disney artists Al Taliaferro and Floyd Gottfredson. Since 1995, more than 500,000 visitors have attended the shows in Europe.
Reichelt also translated the Michael Barrier Barks biography into German and published it in 1994.
Final days and death.
Still living in a new home in Grants Pass, Oregon which he and Garé had built next door to their original home, Barks died in 2000 at the age of 99, seven years after Garé had died.
Although he was undergoing chemotherapy for leukemia he was, according to caregiver Serene Hunickle, "funny up to the end."
Barks' influence.
Barks' Donald Duck stories were rated #7 on "The Comics Journal" list of 100 top comics; his Uncle Scrooge stories were rated #20.
Steven Spielberg and George Lucas have acknowledged that the rolling-boulder booby trap in the opening scene of "Raiders of the Lost Ark" was inspired by the 1954 Carl Barks Uncle Scrooge adventure "The Seven Cities of Cibola" ("Uncle Scrooge" #7). Lucas and Spielberg have also said that some of Barks's stories about space travel and the depiction of aliens had an influence on them.
Lucas wrote the foreword to the 1982 "Uncle Scrooge McDuck: His Life and Times". In it he calls Barks’s stories "cinematic" and "a priceless part of our literary heritage".
The Walt Disney Treasures DVD set "" includes a salute to Barks.
Carl Barks has an asteroid named after him, 2730 Barks. A Cornell scientist was inspired by Barks' tale "Island in the Sky".
In Almere, Netherlands a street was named after him: Carl Barksweg. The same neighborhood also includes a Donald Ducklaan and a Goofystraat.
Osamu Tezuka, the influential manga creator of "Astro Boy" whose artistic style set the standard and defining aspects for post World War II manga and anime, said he owed it all to Barks' Scrooge McDuck. 
A 1949 Donald Duck ten-pager features Donald raising a yacht from the ocean floor by filling it with ping pong balls. In December 1965 Karl Krøyer, a Dane, lifted the sunken freight vessel "Al Kuwait" in the Kuwait Harbor by filling the hull with 27 million tiny inflatable balls of polystyrene.
Although the suggestion is often made, Krøyer denies having been inspired by this Barks story. Some sources claim Krøyer was denied a Dutch patent registration (application number NL 6514306) for his invention on the grounds that the Barks story was a prior publication of the invention. However no definite proof of this story is available. Krøyer later successfully raised another ship off Greenland using the same method, and several other sunken vessels worldwide have since been raised by modified versions of this concept. The television show MythBusters also tested this method and was able to raise a small boat.
For those currently drawing Disney Duck comics, the influence of Barks cannot be overstated. For artists such as Daan Jippes and Freddy Milton, Barks' comics have made a great impact. Don Rosa, one of the most popular living Disney artists, and possibly the one who has been most keen on connecting the various stories into a coherent universe and chronology, considers (with few exceptions) all Barks' duck stories as canon, and all others as apocryphal. Rosa has said that a number of novelists and movie-makers cite Carl Barks as their 'major influence and inspiration'.
The popularity of Barks' work in Europe is high, and has been that way for years. When the news of Barks' passing was hardly covered by the press in America, "in Europe the sad news was flashed instantly across the airwaves and every newspaper — they realized the world had lost one of the most beloved, influential and well-known creators in international culture."
Dozens of noted comic book artists have taken up elements of Barks' style, especially his ink and pen work. In the US elements of Barks' oil painting style of the ducks were evident in the computer animated, 3-D look "Mickey's Twice Upon a Christmas" released to video in 2005.
The video game "" is dedicated to the memory of Carl Barks.
Carl Barks drew an early Andy Panda comic book story published in "New Funnies" #76, 1943. It is one of his few stories to feature humans interacting with funny animal characters (another is "Dangerous Disguise", Four Color #308, 1951). See List of Fictional Pandas.
The life story of Carl Barks, largely drawing upon his relationship with Disney and the phonetic similarity of his name to Karl Marx, serves as a loose inspiration to one of the subplots in "The Last Song of Manuel Sendero" by Ariel Dorfman, though his biography in this novel veers sharply into science fiction fantasy and symbolism.
The first image ever to be displayed on an Apple Macintosh was a scan of Carl Barks' Scrooge McDuck.
Filmography.
Films where Barks served as storyman or story director: 
Art materials.
Barks was an enthusiastic user of Esterbrook pens. He particularly used a Nº 356 model to ink and letter his Donald Duck comic-book pages.

</doc>
<doc id="7346" url="http://en.wikipedia.org/wiki?curid=7346" title="Centimetre–gram–second system of units">
Centimetre–gram–second system of units

The centimetre–gram–second system (abbreviated CGS or cgs) is a variant of the metric system of physical units based on centimetre as the unit of length, gram as a unit of mass, and second as a unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways of extending the CGS system to cover electromagnetism.
The CGS system has been largely supplanted by the MKS system, based on metre, kilogram, and second. MKS was in turn extended and replaced by the International System of Units (SI). The latter adopts the three base units of MKS, plus the ampere, mole, candela and kelvin. In many fields of science and engineering, SI is the only system of units in use. However, there remain certain subfields where CGS is prevalent.
In measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 arising from the relations and . For example, the CGS-derived unit of force is the dyne, equal to , while the SI-derived unit of force is the newton, . Thus it is straightforward to show that .
On the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is much more subtle and involved. In fact, formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on which system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian, "ESU", "EMU", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase "CGS units" is often used to refer specifically to CGS-Gaussian units.
History.
The CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss to base a system of absolute units on the three fundamental units of length, mass and time. Gauss chose the units of millimetre, milligram and second. In 1874, it was extended by the British physicists James Clerk Maxwell and William Thomson with a set of electromagnetic units and the selection of centimetre, gram and second and the naming of "C.G.S".
The sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimetres long, such as humans, rooms and buildings. Thus the CGS system never gained wide general use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (metre–kilogram–second) system, which in turn developed into the modern SI standard.
Since the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide, in the United States more slowly than elsewhere. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as the "Astrophysical Journal". CGS units are still occasionally encountered in technical literature, especially in the United States in the fields of material science, electrodynamics and astronomy. The continued usage of CGS units is most prevalent in magnetism and related fields, as the primary MKS unit, the tesla, is inconvenienently large, leading to the continued common use of the gauss, the CGS equivalent.
The units gram and centimetre remain useful "as prefixed units" within the SI system, especially for instructional physics and chemistry experiments, where they match the small scale of table-top setups. However, where derived units are needed, the SI ones are generally used and taught instead of the CGS ones today. For example, a physics lab course might ask students to record lengths in centimetres, and masses in grams, but force (a derived unit) in newtons, a usage consistent with the SI system.
Definition of CGS units in mechanics.
In mechanics, the CGS and SI systems of units are built in an identical way. The two systems differ only in the scale of two out of the three base units (centimetre versus metre and gram versus kilogram, respectively), while the third unit (second as the unit of time) is the same in both systems.
There is a one-to-one correspondence between the base units of mechanics in CGS and SI, and the laws of mechanics are not affected by the choice of units. The definitions of all derived units in terms of the three base units are therefore the same in both systems, and there is an unambiguous one-to-one correspondence of derived units:
Thus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time:
Expressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems:
Derivation of CGS units in electromagnetism.
CGS approach to electromagnetic units.
The conversion factors relating electromagnetic units in the CGS and SI systems are much more complex – so much so that formulae expressing physical laws of electromagnetism are different depending on what system of units one uses. This illustrates the fundamental difference in the ways the two systems are built: 
Alternate derivations of CGS units in electromagnetism.
Electromagnetic relationships to length, time and mass may be derived by several equally appealing methods. Two of them rely on the forces observed on charges. Two fundamental laws relate (independently of each other) the electric charge or its rate of change (electric current) to a mechanical quantity such as force. They can be written in system-independent form as follows:
Maxwell's theory of electromagnetism relates these two laws to each other. It states that the ratio of proportionality constants formula_10 and formula_14 must obey formula_17, where "c" is the speed of light in vacuum. Therefore, if one derives the unit of charge from the Coulomb's law by setting formula_18, it is obvious that the Ampère's force law will contain a prefactor formula_19. Alternatively, deriving the unit of current, and therefore the unit of charge, from the Ampère's force law by setting formula_20 or formula_21, will lead to a constant prefactor in the Coulomb's law.
Indeed, both of these mutually exclusive approaches have been practiced by the users of CGS system, leading to the two independent and mutually exclusive branches of CGS, described in the subsections below. However, the freedom of choice in deriving electromagnetic units from the units of length, mass, and time is not limited to the definition of charge. While the electric field can be related to the work performed by it on a moving electric charge, the magnetic force is always perpendicular to the velocity of the moving charge, and thus the work performed by the magnetic field on any charge is always zero. This leads to a choice between two laws of magnetism, each relating magnetic field to mechanical quantities and electric charge:
These two laws can be used to derive Ampère's force law above, resulting in the relationship: formula_25. Therefore, if the unit of charge is based on the Ampère's force law such that formula_26, it is natural to derive the unit of magnetic field by setting formula_27. However, if it is not the case, a choice has to be made as to which of the two laws above is a more convenient basis for deriving the unit of magnetic field.
Furthermore, if we wish to describe the electric displacement field D and the magnetic field H in a medium other than vacuum, we need to also define the constants ε0 and μ0, which are the vacuum permittivity and permeability, respectively. Then we have (generally) formula_28 and formula_29, where P and M are polarization density and magnetization vectors. The factors λ and λ′ are rationalization constants, which are usually chosen to be formula_30, a dimensionless quantity. If λ = λ′ = 1, the system is said to be "rationalized": the laws for systems of spherical geometry contain factors of 4π (for example, point charges), those of cylindrical geometry – factors of 2π (for example, wires), and those of planar geometry contain no factors of π (for example, parallel-plate capacitors). However, the original CGS system used λ = λ′ = 4π, or, equivalently, formula_31. Therefore, Gaussian, ESU, and EMU subsystems of CGS (described below) are not rationalized.
Various extensions of the CGS system to electromagnetism.
The table below shows the values of the above constants used in some common CGS subsystems:
The constant "b" in SI system is a unit-based scaling factor defined as: formula_32.
Also, note the following correspondence of the above constants to those in Jackson and Leung:
In system-independent form, Maxwell's equations can be written as:
formula_37
Note that of all these variants, only in Gaussian and Heaviside–Lorentz systems formula_38 equals formula_39 rather than 1. As a result, vectors formula_40 and formula_41 of an electromagnetic wave propagating in vacuum have the same units and are equal in magnitude in these two variants of CGS.
Electrostatic units (ESU).
In one variant of the CGS system, Electrostatic units (ESU), charge is defined via the force it exerts on other charges, and current is then defined as charge per time. It is done by setting the Coulomb force constant formula_42, so that Coulomb's law does not contain an explicit prefactor.
The ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: Therefore, in electrostatic CGS units, a franklin is equal to a centimetre times square root of dyne:
The unit of current is defined as:
Dimensionally in the ESU CGS system, charge "q" is therefore equivalent to m1/2L3/2t−1. Hence, neither charge nor current is an independent physical quantity in ESU CGS. This reduction of units is the consequence of the Buckingham π theorem.
ESU notation.
All electromagnetic units in ESU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix "stat" or with a separate abbreviation "esu".
Electromagnetic units (EMU).
In another variant of the CGS system, Electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well). In the EMU CGS subsystem, this is done by setting the Ampere force constant formula_45, so that Ampère's force law simply contains 2 as an explicit prefactor (this prefactor 2 is itself a result of integrating a more general formulation of Ampère's law over the length of the infinite wire).
The EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows:
 Therefore, in electromagnetic CGS units, a biot is equal to a square root of dyne:
The unit of charge in CGS EMU is:
Dimensionally in the EMU CGS system, charge "q" is therefore equivalent to m1/2L1/2. Hence, neither charge nor current is an independent physical quantity in EMU CGS.
EMU notation.
All electromagnetic units in EMU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix "ab" or with a separate abbreviation "emu".
Relations between ESU and EMU units.
The ESU and EMU subsystems of CGS are connected by the fundamental relationship formula_17 (see above), where "c" = 29,979,245,800 ≈ 3·1010 is the speed of light in vacuum in centimetres per second. Therefore, the ratio of the corresponding "primary" electrical and magnetic units (e.g. current, charge, voltage, etc. – quantities proportional to those that enter directly into Coulomb's law or Ampère's force law) is equal either to "c"−1 or "c":
and
Units derived from these may have ratios equal to higher powers of "c", for example:
Other variants.
There were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These also include the Gaussian units and the Heaviside–Lorentz units.
Further complicating matters is the fact that some physicists and electrical engineers in North America use hybrid units, such as volts per "centimetre" for electric fields and amperes per "centimetre" for magnetic fields. However, these are essentially the same as the SI units, by the simple conversion of all lengths used from metres into centimetres. 
Electromagnetic units in various CGS systems.
In this table, "c" = 29,979,245,800 ≈ 3·1010 is the speed of light in vacuum in the CGS units of centimetres per second. The symbol "↔" is used instead of "=" as a reminder that the SI and CGS units are "corresponding" but not "equal" because they have incompatible dimensions. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1 F in SI, then it has a capacitance of (10−9 "c"2) cm in ESU; "but" it is usually incorrect to replace "1 F" with "(10−9 "c"2) cm" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is "always" correct to replace "1 m" with "100 cm" within an equation or formula.)
One can think of the SI value of the Coulomb constant "k"C as:
This explains why SI to ESU conversions involving factors of "c"2 lead to significant simplifications of the ESU units, such as 1 statF = 1 cm and 1 statΩ = 1 s/cm: this is the consequence of the fact that in ESU system "k"C = 1. For example, a centimetre of capacitance is the capacitance between a sphere of radius 1 cm in vacuum and infinity. The capacitance "C" between two concentric spheres of radii "R" and "r" in ESU CGS system is:
By taking the limit as "R" goes to infinity we see "C" equals "r".
Pro and contra.
While the absence of explicit prefactors in some CGS subsystems simplifies some theoretical calculations, it has the disadvantage that sometimes the units in CGS are hard to define through experiment. Also, lack of unique unit names leads to a great confusion: thus "15 emu" may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. On the other hand, SI starts with a unit of current, the ampere, that is easier to determine through experiment, but which requires extra multiplicative factors in the electromagnetic equations. With its system of uniquely named units, the SI also removes any confusion in usage: 1.0 ampere is a fixed value of a specified quantity, and so are 1.0 henry, 1.0 ohm, and 1.0 volt .
A key virtue of the Gaussian CGS system is that electric and magnetic fields have the same units, formula_54 is replaced by formula_55, and the only dimensional constant appearing in the Maxwell equations is formula_56, the speed of light. The Heaviside–Lorentz system has these desirable properties as well (with formula_57 equaling 1), but it is a "rationalized" system (as is SI) in which the charges and fields are defined in such a way that there are many fewer factors of formula_58 appearing in the formulas, and it is in Heaviside–Lorentz units that the Maxwell equations take their simplest form.
In SI, and other rationalized systems (for example, Heaviside–Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4π, those concerning coils of current and straight wires contain 2π and those dealing with charged surfaces lack π entirely, which was the most convenient choice for applications in electrical engineering. However, modern hand calculators and personal computers have reduced this "advantage" to nothing. In some fields where formulas concerning spheres are common (for example, in astrophysics), it has been argued that the nonrationalized CGS system can be somewhat more convenient notationally.
In fact, in certain fields, specialized unit systems are used to simplify formulas even further than "either" SI "or" CGS, by using some system of natural units. For example, those in particle physics use a system where every quantity is expressed by only one unit, the electron-volt, with lengths, times, and so on all converted into electron-volts by inserting factors of c and the Planck constant formula_59. This unit system is very convenient for calculations in particle physics, but it would be impractical in all other contexts.

</doc>
<doc id="7355" url="http://en.wikipedia.org/wiki?curid=7355" title="Christology">
Christology

Christology (from Greek Χριστός "Khristós" and , "-logia") is the field of study within Christian theology which is primarily concerned with the nature and person of Jesus as recorded in the canonical Gospels and the epistles of the New Testament. Primary considerations include the relationship of Jesus' nature and person with the nature and person of God the Father. As such, Christology is concerned with the details of Jesus' ministry, his acts and teachings, to arrive at a clearer understanding of who he is in his person, and his role in salvation. A major component of the Christology of the Apostolic Age was that of Paul the Apostle. His central themes were the notion of the pre-existence of Christ and the worship of Christ as "Kyrios" (Greek: "Lord").
The pre-existence of Christ is considered a central theme of Christology. Proponents of Christ's deity argue the Old Testament has many cases of Christophany: "The pre-existence of Christ is further substantiated by the many recorded Christophanies in the Bible." Christophany is often considered a more accurate term than the term Theophany due to the belief that all the visible manifestations of God (or Yahweh) are in fact the preincarnate Christ. Many argue that the appearances of "the Angel of the Lord" in the Old Testament were the preincarnate Christ. "Many understand the angel of the Lord as a true theophany. From the time of Justin on, the figure has been regarded as the preincarnate Logos."
Following the Apostolic Age, there was fierce and often politicized debate in the early church on many interrelated issues. Christology was a major focus of these debates, and was addressed at every one of the first seven ecumenical councils. The second through fourth of these councils are generally entitled "Christological councils," with the latter three mainly elucidating what was taught in them and condemning incorrect interpretations. The Council of Chalcedon in 451 issued a formulation of the being of Christ — that of two natures, one human and one divine, "united with neither confusion nor division." This is called the doctrine of the hypostatic union, which is still held today amongst most Protestant, Catholic, and Eastern Orthodox Christians, referred to as Chalcedonian Christianity. Due to politically charged differences in the 4th century, schisms developed, and the first denominations (from the Latin, "to take a new name") formed.
In the 13th century, Saint Thomas Aquinas provided the first systematic Christology that consistently resolved a number of the existing issues. In his Christology from above, Aquinas also championed the principle of perfection of Christ's human attributes. The Middle Ages also witnessed the emergence of the "tender image of Jesus" as a friend and a living source of love and comfort, rather than just the "Kyrios" image. According to Catholic theologian Karl Rahner, the purpose of modern Christology is to formulate the Christian belief that "God became man and that God-made-man is the individual Jesus Christ" in a manner that this statement can be understood consistently, without the confusions of past debates and mythologies.
Terms and concepts.
Over the centuries, a number of terms and concepts have been developed within the framework of Christology to address the seemingly simple questions: "who was Jesus and what did he do?" A good deal of theological debate has ensued and significant schisms within Christian denominations took place in the process of providing answers to these questions. After the Middle Ages, systematic approaches to Christology were developed.
The term "Christology from above" refers to approaches that begin with the divinity and pre-existence of Christ as the "Logos" (the Word), as expressed in the (). These approaches interpret the works of Christ in terms of his divinity. Christology from above was emphasized in the ancient Church, beginning with Ignatius of Antioch in the second century. The term "Christology from below", on the other hand, refers to approaches that begin with the human aspects and the ministry of Jesus (including the miracles, parables, etc.) and move towards his divinity and the mystery of incarnation.
The concept of "Cosmic Christology", first elaborated by Saint Paul, focuses on how the arrival of Jesus as the Son of God forever changed the nature of the cosmos. The terms "functional", "ontological" and "soteriological" have been used to refer to the perspectives that analyze the "works", the "being" and the "salvific" standpoints of Christology. Some essential sub-topics within the field of Christology include the incarnation, the resurrection, and salvation.
The term "monastic Christology" has been used to describe spiritual approaches developed by Anselm of Canterbury, Peter Abelard and Bernard of Clairvaux. The Franciscan piety of the 12th and 13th centuries led to "popular Christology". Systematic approaches by theologians, such as Thomas Aquinas, are called "scholastic Christology".
Beginnings.
Early Christians found themselves confronted with a set of new concepts and ideas relating to the life, death and resurrection of Jesus, as well the notions of salvation and redemption, and had to use a new set of terms, images and ideas to deal with them. The existing terms and structures available to them were often insufficient to express these religious concepts, and taken together, these new forms of discourse led to the beginnings of Christology as an attempt to understand, explain and discuss their understanding of the nature of Christ.
Furthermore, as early Christians (following the Great Commission) had to explain their concepts to a new audience which
had at times been influenced by Greek philosophy, they had to present arguments that at times
resonated with, and at times confronted, the beliefs of that audience. A key example is the Apostle Paul's Areopagus sermon that appears in . Here, the apostle attempted to convey the underlying concepts about Christ to a Greek audience, and the sermon illustrates some key elements of future Christological discourses that were first brought forward by Paul.
The "Kyrios" title for Jesus is central to the development of New Testament Christology, for the early Christians placed it at the center of their understanding, and from that center attempted to understand the other issues related to the Christian mysteries. The question of the deity of Christ in the New Testament is inherently related to the "Kyrios" title of Jesus used in the early Christian writings and its implications for the absolute lordship of Jesus. In early Christian belief, the concept of "Kyrios" included the pre-existence of Christ, for they believed if Christ is one with God, he must have been united with God from the very beginning.
In everyday Aramaic, "Mari" was a very respectful form of polite address, which means more than just "Teacher" and was somewhat similar to Rabbi. In Greek, this has at times been translated as "Kyrios". While the term "Mari" expressed the relationship between Jesus and his disciples during his life, the Greek "Kyrios" came to represent his lordship over the world.
Apostolic Christology.
No writings were left by Jesus, and the study of the various Christologies of the Apostolic Age is based on early Christian documents. The Gospels provide episodes from the life of Jesus and some of his works, but the authors of the New Testament show little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life, and as in , the Gospels do not claim to be an exhaustive list of his works.
Christologies that can be gleaned from the three Synoptic Gospels generally emphasize the humanity of Jesus, his sayings, his parables, and his miracles. The Gospel of John provides a different perspective that focuses on his divinity. The first 14 verses of the Gospel of John are devoted to the divinity of Jesus as the "Logos", usually translated as "Word", along with his pre-existence, and they emphasize the cosmic significance of Christ, e.g. : "All things were made through him, and without him was not any thing made that was made." In the context of these verses, the Word made flesh is identical with the Word who was in the beginning with God, being exegetically equated with Jesus.
A foremost contribution to the Christology of the Apostolic Age is that of Paul. The central Christology of Paul conveys the notion of Christ's pre-existence and the identification of Christ as "Kyrios". The Pauline epistles use "Kyrios" to identify Jesus almost 230 times, and express the theme that the true mark of a Christian is the confession of Jesus as the true Lord. Paul viewed the superiority of the Christian revelation over all other divine manifestations as a consequence of the fact that Christ is the Son of God.
The Pauline epistles also advanced the "cosmic Christology" later developed in the fourth gospel, elaborating the cosmic implications of Jesus' existence as the Son of God, as in : "Therefore, if anyone is in Christ, he is a new creation. The old has passed away; behold, the new has come." Also, in : "He is the image of the invisible God, the firstborn of all creation."
Post-Apostolic controversies.
Following the Apostolic Age, from the second century onwards, a number of controversies developed about how the human and divine are related within the person of Jesus. As of the second century, a number of different and opposing approaches developed among various groups. For example, Arianism did not endorse divinity, Ebionism argued Jesus was an ordinary mortal, while Gnosticism held docetic views which argued Christ was a spiritual being who only appeared to have a physical body. The resulting tensions led to schisms within the church in the second and third centuries, and ecumenical councils were convened in the fourth and fifth centuries to deal with the issues. Eventually, by the Ecumenical Council of Chalcedon in 451, the "Hypostatic union" was decreed—the proposition that Christ has one human nature "[physis]" and one divine nature "[physis]", united with neither confusion nor division—making this part of the creed of orthodox Christianity. Although some of the debates may seem to various modern students to be over a theological iota, they took place in controversial political circumstances, reflecting the relations of temporal powers and divine authority, and certainly resulted in schisms, among others what separated the Church of the East from the Church of the Roman Empire.
In 325, the First Council of Nicaea defined the persons of the Godhead and their relationship with one another, decisions which were re-ratified at the First Council of Constantinople in 381. The language used was that the one God exists in three persons (Father, Son, and Holy Spirit); in particular, it was affirmed that the Son was "homoousios" (of same substance) as the Father. The Nicene Creed declared the full divinity and full humanity of Jesus.
In 431, the First Council of Ephesus was initially called to address the views of Nestorius on Mariology, but the problems soon extended to Christology, and schisms followed. The 431 council was called because in defense of his loyal priest Anastasius, Nestorius had denied the "Theotokos" title for Mary and later contradicted Proclus during a sermon in Constantinople. Pope Celestine I (who was already upset with Nestorius due to other matters) wrote about this to Cyril of Alexandria, who orchestrated the council. During the council, Nestorius defended his position by arguing there must be two persons of Christ, one human, the other divine, and Mary had given birth only to a human, hence could not be called the "Theotokos", i.e. "the one who gives birth to God". The debate about the single or dual nature of Christ ensued in Ephesus.
The Council of Ephesus debated hypostasis (coexisting natures) versus monophysitism (only one nature) versus miaphysitism (two natures united as one) versus Nestorianism (disunion of two natures). From the Christological viewpoint, the council adopted "hypostasis", i.e. coexisting natures, but its language was less definitive than the 451 Council of Chalcedon. The Oriental Orthodox rejected this and subsequent councils and to date consider themselves to be "miaphysite". By contrast, Roman Catholics to date (and most Protestants) believe in the hypostatic union and the Trinity. The council also confirmed the "Theotokos" title and excommunicated Nestorius.
The 451 Council of Chalcedon was highly influential and marked a key turning point in the Christological debates that broke apart the church of the Eastern Roman Empire in the fifth century. It is the last council which many Anglicans and most Protestants consider ecumenical. It fully promulgated the hypostatic union, stating the human and divine natures of Christ coexist, yet each is distinct and complete. Although, the Chalcedonian Creed did not put an end to all Christological debate, it did clarify the terms used and became a point of reference for many future Christologies. Most of the major branches of Christianity — Roman Catholicism, Eastern Orthodoxy, Anglicanism, Lutheranism, and Reformed — subscribe to the Chalcedonian Christological formulation, while many branches of Eastern Christianity - Syrian Orthodoxy, Assyrian Church, Coptic Orthodoxy, Ethiopian Orthodoxy, and Armenian Apostolicism - reject it.
Christological issues.
Person of Christ.
The Person of Christ refers to the study of the human and divine natures of Jesus Christ as they coexist within one person. There are no direct discussions in the New Testament regarding the dual nature of the Person of Christ as both divine and human. Hence, since the early days of Christianity, theologians have debated various approaches to the understanding of these natures, at times resulting in schisms.
Historically in the Alexandrian school of thought (fashioned on the Gospel of John), Jesus Christ is the eternal "Logos" who already possesses unity with the Father before the act of Incarnation. In contrast, the Antiochian school views Christ as a single, unified human person apart from his relationship to the divine.
John Calvin maintained there was no human element in the Person of Christ which could be separated from the Person of The Word. Calvin also emphasized the importance of the "Work of Christ" in any attempt at understanding the Person of Christ and cautioned against ignoring the Works of Jesus during his ministry.
The study of the Person of Christ continued into the 20th century, with modern theologians such as Karl Rahner and Hans von Balthasar. Rahner pointed out the coincidence between the Person of Christ and the Word of God, referring to and which state whoever is ashamed of the words of Jesus is ashamed of the Lord himself. Balthasar argued the union of the human and divine natures of Christ was achieved not by the "absorption" of human attributes, but by their "assumption". Thus, in his view, the divine nature of Christ was not affected by the human attributes and remained forever divine.
Nativity and the Holy Name.
The Nativity of Jesus impacted the Christological issues about his Person from the earliest days of Christianity. Luke's Christology centers on the dialectics of the dual natures of the earthly and heavenly manifestations of existence of the Christ, while Matthew's Christology focuses on the mission of Jesus and his role as the savior. The salvific emphasis of later impacted the theological issues and the devotions to Holy Name of Jesus.
 provides a key to the "Emmanuel Christology" of Matthew. Beginning with 1:23, Matthew shows a clear interest in identifying Jesus as "God with us" and in later developing the Emmanuel characterization of Jesus at key points throughout the rest of his Gospel. The name Emmanuel does not appear elsewhere in the New Testament, but Matthew builds on it in ("I am with you always, even unto the end of the world") to indicate Jesus will be with the faithful to the end of the age. According to Ulrich Luz, the Emmanuel motif brackets the entire Gospel of Matthew between 1:23 and 28:20, appearing explicitly and implicitly in several other passages.
Crucifixion and Resurrection.
The accounts of the crucifixion and subsequent resurrection of Jesus provides a rich background for Christological analysis, from the canonical Gospels to the Pauline Epistles.
A central element in the Christology presented in the Acts of the Apostles is the affirmation of the belief that the death of Jesus by crucifixion happened "with the foreknowledge of God, according to a definite plan". In this view, as in , the cross is not viewed as a scandal, for the crucifixion of Jesus "at the hands of the lawless" is viewed as the fulfilment of the plan of God.
Paul's Christology has a specific focus on the death and resurrection of Jesus. For Paul, the crucifixion of Jesus is directly related to his resurrection and the term "the cross of Christ" used in Galatians 6:12 may be viewed as his abbreviation of the message of the gospels. For Paul, the crucifixion of Jesus was not an isolated event in history, but a cosmic event with significant eschatological consequences, as in Cor 2:8. In the Pauline view, Jesus, obedient to the point of death (Phil 2:8), died "at the right time" (Rom 4:25) based on the plan of God. For Paul, the "power of the cross" is not separable from the resurrection of Jesus.
Threefold office.
The threefold office (Latin "munus triplex") of Jesus Christ is a Christian doctrine based upon the teachings of the Old Testament. It was described by Eusebius and more fully developed by John Calvin. It states that Jesus Christ performed three functions (or "offices") in his earthly ministry - those of prophet (), priest (), and king (). In the Old Testament, the appointment of someone to any of these three positions could be indicated by anointing him or her by pouring oil over the head. Thus, the term messiah, meaning "anointed one", is associated with the concept of the threefold office. While the office of king is that most frequently associated with the Messiah, the role of Jesus as priest is also prominent in the New Testament, being most fully explained in chapters 7 to 10 of the Book of Hebrews.
Mariology.
Some Christians, notably Roman Catholics, view Mariology as a key component of Christology. In this view, not only is Mariology a logical and necessary consequence of Christology, but without it, Christology is incomplete, since the figure of Mary contributes to a fuller understanding of who Christ is and what he did. Certain Christian traditions of Protestant heritage tend not to hold this view.
Joseph Cardinal Ratzinger (later Pope Benedict XVI) expressed this sentiment about Roman Catholic Mariology when in two separate occasions he stated, "The appearance of a truly Marian awareness serves as the touchstone indicating whether or not the Christological substance is fully present" and "It is necessary to go back to Mary, if we want to return to the truth about Jesus Christ."

</doc>
<doc id="7357" url="http://en.wikipedia.org/wiki?curid=7357" title="Complaint">
Complaint

In legal terminology, a complaint is any formal legal document that sets out the facts and legal reasons (see: cause of action) that the filing party or parties (the plaintiff(s)) believes are sufficient to support a claim against the party or parties against whom the claim is brought (the defendant(s)) that entitles the plaintiff(s) to a remedy (either money damages or injunctive relief)]). For example, the Federal Rules of Civil Procedure (FRCP) that govern civil litigation in United States courts provide that a civil action is commenced with the filing or service of a pleading called a complaint. Civil court rules in states that have incorporated the Federal Rules of Civil Procedure use the same term for the same pleading.
In some jurisdictions, specific types of criminal cases may also be commenced by the filing of a complaint, also sometimes called a criminal complaint or felony complaint. All criminal cases are prosecuted in the name of the governmental authority that promulgates criminal statutes and enforces the police power of the state with the goal of seeking criminal sanctions, such as the State (also sometimes called the People) or Crown (in Commonwealth realms). In the United States, the complaint is often associated with misdemeanor criminal charges presented by the prosecutor without the grand jury process. In most U.S. jurisdictions, the charging instrument presented to and authorized by a grand jury is referred to as an indictment.
United States.
Virtually every U.S. state has some forms available on web for most common complaints for lawyers and self-representing litigants; if a petitioner cannot find an appropriate form in their state, they often can modify a form from another state to fit his or her request. Several United States federal courts published general guidelines for the petitioners and Civil Rights complaint forms. 
A complaint generally has the following structural elements:
After the complaint has been filed with the court, it has to be properly served to the opposite parties, but usually petitioners are not allowed to serve the complaint personally. Court also 
can issue summons - official summary document which plaintiff needs serve together with the complaint. The defendants have limited time to respond, depending on the State or Federal rules. A defendant's failure to answer a complaint can result in a default judgment in favor of the petitioner. 
For example, in United States federal courts, any person who is at least 18 years old and not a party may serve a summons and complaint in a civil case. The defendant must submit an answer within 21 days after being served with the summons and complaint, or request a waiver, according to FRCP Rule 12. After the civil complaint was served to the defendants, plaintiff must as soon as practicable initiate a conference between the parties to plan for the rest of the discovery process and then parties should submit a proposed discovery plan to the judge within 14 days after the conference.
In many U.S. jurisdictions, a complaint submitted to a court must be accompanied by a Case Information Statement, which sets forth specific key information about the case and the lawyers representing the parties. This allows the judge to make determinations about which deadlines to set for different phases of the case, as it moves through the court system. 
There are also freely accessible web search engines to assist parties in finding court decisions that can be cited in the complaint as an example or analogy to resolve similar questions of law. Google Scholar is the biggest database of full text state and federal courts decisions that can be accessed without charge. These web search engines often allow one to select specific state courts to search.
Federal courts created the Public Access to Court Electronic Records (PACER) system to obtain case and docket information from the United States district courts, United States courts of appeals, and United States bankruptcy courts. The system is managed by the Administrative Office of the United States Courts; it allows lawyers and self-represented clients to obtain documents entered in the case much faster than regular mail.
Filing and privacy.
In addition to Federal Rules of Civil Procedure, many of the U.S. district courts have developed their own requirements included in Local Rules for filing with the Court. Local Rules can set up a limit on the number of pages, establish deadlines for motions and responses, explain whether it is acceptable to combine motion petition with a response, specify if a judge needs an additional copy of the documents (called "judge’s copy"), etc. Local Rules can define page layout elements like: margins, text font/size, distance between lines, mandatory footer text, page numbering, and provide directions on how the pages need to be bind together – i.e. acceptable fasteners, number and location of fastening holes, etc. If the filed motion does not comply with the Local Rules then the judge can choose to strike the motion completely, or order the party to re-file its motion, or grant a special exception to the Local Rules.
According to Federal Rules of Civil Procedure (FRCP) 5.2, sensitive text like Social Security number, Taxpayer Identification Number, birthday, bank accounts and children’s names, should be redacted off the filings made with the court and accompanying exhibits, (however, exhibits normally do not need to be attached to the original complaint, but should be presented to Court after the discovery). The redacted text can be erased with black-out or white-out, and the page should have an indication that it was redacted - most often by stamping word "redacted" on the bottom. Alternately, the filing party may ask the court’s permission to file some exhibits completely under seal. A minor's name of the petitions should be replaced with initials.
A person making a redacted filing can file an unredacted copy under seal, or the Court can choose to order later that an additional filing be made under seal without redaction. Copies of both redacted and unredacted documents filed with court should be provided to the other parties in the case. Some courts also require that additional electronic courtesy copy be emailed to the other parties.
Attorney fees.
Before filing the complaint, it is important for plaintiffs to remember that Federal courts can impose liability for the prevailing party's attorney fees to the losing party, if the judge considers the case frivolous or for purpose of harassment, even when the case was voluntarily dismissed. In the case of Fox v. Vice, U.S. Supreme Court held that reasonable attorneys' fees could be awarded to the defendant under 42 U.S.C. Sec. 1988, but only for costs that the defendant would not have incurred "but for the frivolous claims." Even when there is no actual trial or judgment, if there is only pre-trial motion practice such as motions to dismiss, attorney fee shifting still can be awarded under FRCP Rule 11 when the opposing party files a Motion for Sanctions and the court issue an order identifying the sanctioned conduct and the basis for the sanction. The losing party has a right to appeal any order for sanctions in the higher court. In the state courts, however, each party is generally responsible only for its own attorney fees, with certain exceptions.

</doc>
<doc id="7362" url="http://en.wikipedia.org/wiki?curid=7362" title="Casimir III the Great">
Casimir III the Great

Casimir III the Great (; 30 April 1310 – 5 November 1370) who reigned from 1333 to 1370, was the last King of Poland from the Piast dynasty, the son of King Władysław I ("the Elbow-high") and Duchess Hedwig of Kalisz.
Born in Kowal, Casimir first married Anna, or Aldona Ona, the daughter of Grand Duke Gediminas of Lithuania. The marriage produced two daughters, Cunigunde (d. 1357), who was married to Louis VI the Roman, the son of Louis IV, Holy Roman Emperor, and Elisabeth, who was married to Duke Bogislaus V of Pomerania. Aldona died in 1339, and Casimir then married Adelaide of Hesse. He divorced Adelaide in 1356, married Christina, divorced her, and while Adelaide and possibly Christina as well were still alive (ca. 1365), he married Hedwig of Głogów and Sagan. He had three daughters by his fourth wife, and they were still very young when he died, and regarded as of dubious legitimacy because of Casimir's bigamy. Because all of the five children he fathered with his first and fourth wife were daughters, Casimir left no lawful male heir to his throne. 
Casimir died in 1370 from an injury received while hunting. His nephew, King Louis I of Hungary, succeeded him to become king of Poland in personal union with Hungary.
The Great King.
Casimir is the only Polish king who both received and kept the title of "Great" in Polish history (Bolesław I Chrobry is also called "Great", but his title Chrobry (Valiant) is now more common). When he came to the throne, his hold on it was in danger, as even his neighbours did not recognise his title and instead called him "king of Kraków". The economy was ruined, and the kingdom was depopulated and exhausted by war.
Upon his death, Casimir left a kingdom that had doubled in size (mostly through the addition of lands in modern day Ukraine, then called the Duchy of Halicz), was prosperous, wealthy, and held great prospects for the future. Although depicted as a peaceful king in children's books, he in fact waged many victorious wars and was readying for others just before he died. He built extensively during his reign (Wawel Castle, Orle Gniazda), and reformed the Polish army and the Polish civil and criminal law. At the Sejm in Wiślica, on 11 March 1347, he introduced salutary legal reforms to the judicial system of his kingdom. He sanctioned a codes of laws for Great and Lesser Poland, which gained for him the title of "the Polish Justinian", and founded the University of Kraków, the oldest Polish University. He organized a meeting of kings in Kraków (1364) in which he exhibited the wealth of the Polish kingdom.
In 1335, in the Treaty of Trentschin, Casimir relinquished "in perpetuity" his claims to Silesia. In 1355 in Buda, Casimir designated Louis I of Hungary as his successor, like his father did with Charles I of Hungary for his help against Bohemia. In exchange Casimir gained Hungarian favourable attitude, needed in disputes with the hostile Teutonic Order and Kingdom of Bohemia. We must remember that Casimir was still in his early years and having a son did not seem to be a problem for him (he already had a few bastard children). Unexpectedly Casimir left no legal son, but he tried to adopt his grandson Casimir IV, Duke of Pomerania, in his last will. This part of the testament was invalidated by Louis I of Hungary, who came to Cracow Kraków quickly after the death of Casimir and bribed the nobles with future privileges.
His second daughter, Elisabeth, Duchess of Pomerania, bore a son in 1351, Casimir IV of Pomerania. He was slated to become the heir, but did not succeed to the throne, dying childless in 1377, 7 years after King Casimir. He was the only male descendant of King Casimir who lived during his lifetime. His son-in-law Louis VI the Roman of Bavaria, Margrave and Prince-elector of Brandenburg, was considered a possible successor but was deemed ineligible as his wife, Casimir's daughter Cunigunde had died in 1357 without issue. The Poles repulsed many raids of the Tatar-Mongols.
Casimir had no legitimate sons. Apparently, he deemed his own descendants either unsuitable or too young to inherit. Thus, and in order to provide a clear line of succession and avoid dynastic uncertainty, he arranged for his nephew, King Louis I of Hungary, to be his successor in Poland. Louis was proclaimed king on Casimir's death in 1370, and Casimir's sister Elisabeth (Louis's mother) held much of the real power until her death in 1380. 
Society under the reign of Casimir.
Casimir was facetiously named "the Peasants' King". The codes of laws of Greater and Lesser Poland, he introduced were his attempt to put the overwhelming superiority of the nobility to an end. During his reign all three major classes (nobility, priesthood and the bourgeoisie) were more or less counterbalanced, so that Casimir could strengthen his monarchic position. He was known for siding with the weaker, when the law did not protect them from greedy nobles and clergymen. Reportedly he did even support a peasant, whose house has been demolished against his own mistress, after she ordered to pull it down, because it's look disturbed her the enjoyment of the beautiful landscape.
Relationship with Polish Jews.
King Casimir was favorably disposed toward Jews. On 9 October 1334, he confirmed the privileges granted to Jewish Poles in 1264 by Bolesław V the Chaste. Under penalty of death, he prohibited the kidnapping of Jewish children for the purpose of enforced Christian baptism. He inflicted heavy punishment for the desecration of Jewish cemeteries. Although Jews had lived in Poland since before the reign of King Casimir, he allowed them to settle in Poland in great numbers and protected them as "people of the king".
Relationships and children.
Casimir III married four times.
Aldona of Lithuania.
On 30 April or 16 October 1325, Casimir married Aldona of Lithuania. She was a daughter of Gediminas of Lithuania and Jewna. They had two children:
Aldona died on 26 May 1339. Casimir remained a widower for two years.
Adelheid of Hesse.
On 29 September 1341, Casimir married his second wife, Adelaide of Hesse. She was a daughter of Henry II, Landgrave of Hesse, and Elizabeth of Meissen. They had no children. Casimir started living separately from Adelaide soon thereafter. Their loveless marriage lasted until 1356. 
Christina.
Casimir effectively divorced Adelaide and married his mistress Christina Rokiczana, the widow of Miklusz Rokiczani, a wealthy merchant. Her own origins are unknown. Following the death of her first husband she had entered the court of Bohemia in Prague as a lady-in-waiting. Casimir brought her with him from Prague and convinced the abbot of the Benedictine abbey of Tyniec to marry them. The marriage was held in a secret ceremony but soon became known. Queen Adelaide renounced it as bigamous and returned to Hesse without permission. Casimir continued living with Christine despite complaints by Pope Innocent VI on behalf of Queen Adelaide. The marriage lasted until 1363–64 when Casimir again declared himself divorced. They had no children. 
Hedwig of Żagań.
In about 1365, Casimir married his fourth wife Hedwig of Żagań. She was a daughter of Henry V of Iron, Duke of Żagań and Anna of Mazovia. They had three children:
With Adelaide still alive and Christine possibly surviving, the marriage to Hedwig was also considered bigamous. The legitimacy of the three last daughters was disputed. Casimir managed to have Anne and Cunigunde legitimated by Pope Urban V on 5 December 1369. Hedwig the younger was legitimated by Pope Gregory XI on 11 October 1371. 
Cudka.
Casimir had three illegitimate sons by his mistress Cudka, wife of a castellan.
Title and style.
Casimir's full title was: "Casimir by the grace of God king of Poland, lord and heir of the land of Kraków, Sandomierz, Sieradz, Łęczyca, Kuyavia, Pomerania (Pomerelia) and Ruthenia". The title in Latin was: "Kazimirus, Dei gracia rex Poloniæ ac terrarum Cracoviæ, Sandomiriæ, Syradiæ, Lanciciæ, Cuyaviæ, Pomeraniæ, Russiequæ dominus et heres." 
External links.
 
 

</doc>
<doc id="7363" url="http://en.wikipedia.org/wiki?curid=7363" title="Complexity">
Complexity

Complexity is generally used to characterize something with many parts where those parts interact with each other in multiple ways. The study of these complex linkages is the main goal of complex systems theory.
In science, there are at this time a number of approaches to characterizing complexity, many of which are reflected in this article. Neil Johnson admits that "even among scientists, there is no unique definition of complexity - and the scientific notion has traditionally been conveyed using particular examples..." Ultimately he adopts the definition of 'complexity science' as "the study of the phenomena which emerge from a collection of interacting objects."
Overview.
Definitions of complexity often depend on the concept of a "system"—a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements. However, what one sees as complex and what one sees as simple is relative and changes with time.
Warren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity.
Phenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront "dealing simultaneously with a sizable number of factors which are interrelated into an organic whole". Weaver's 1948 paper has influenced subsequent thinking about complexity.
The approaches that embody concepts of systems, multiple elements, multiple relational regimes, and state spaces might be summarized as implying that complexity arises from the number of distinguishable relational regimes (and their associated state spaces) in a defined system.
Some definitions relate to the algorithmic basis for the expression of a complex phenomenon or model or mathematical expression, as later set out herein.
Disorganized complexity vs. organized complexity.
One of the problems in addressing complexity issues has been formalizing the intuitive conceptual distinction between the large number of variances in relationships extant in random collections, and the sometimes large, but smaller, number of relationships between elements in systems where constraints (related to correlation of otherwise independent elements) simultaneously reduce the variations from element independence and create distinguishable regimes of more-uniform, or correlated, relationships, or interactions.
Weaver perceived and addressed this problem, in at least a preliminary way, in drawing a distinction between "disorganized complexity" and "organized complexity".
In Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a "disorganized complexity" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.
A prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits — the latter can be predicted by applying Newton's laws of motion. Of course, most real-world systems, including planetary orbits, eventually become theoretically unpredictable even using Newtonian dynamics; as discovered by modern chaos theory.
Organized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity vis a vis to other systems than the subject system can be said to "emerge," without any "guiding hand".
The number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers. An example of organized complexity is a city neighborhood as a living mechanism, with the neighborhood people among the system's parts.
Sources and factors of complexity.
There are generally rules which can be invoked to explain the origin of complexity in a given system.
The source of disorganized complexity is the large number of parts in the system of interest, and the lack of correlation between elements in the system.
In the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g. Robert Ulanowicz's treatment of ecosystems.
Complexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used. Random Access Machines allow one to even more decrease time complexity (Greenlaw and Hoover 1998: 226), while inductive Turing machines can decrease even the complexity class of a function, language or set (Burgin 2005). This shows that tools of activity can be an important factor of complexity.
Varied meanings of complexity.
In several scientific fields, "complexity" has a precise meaning:
Other fields introduce less precisely defined notions of complexity:
Study of complexity.
Complexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex-—displaying variation without being random – is most worthy of interest given the rewards found in the depths of exploration.
The use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting "stovepipes" and effective "integrated" solutions. This means that complex is the opposite of independent, while complicated is the opposite of simple.
While this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains, or stock markets. One such interdisciplinary group of fields is relational order theories.
Complexity topics.
Complex behaviour.
The behavior of a complex system is often said to be due to emergence and self-organization. Chaos theory has investigated the sensitivity of systems to variations in initial conditions as one cause of complex behaviour.
Complex mechanisms.
Recent developments around artificial life, evolutionary computation and genetic algorithms have led to an increasing emphasis on complexity and complex adaptive systems.
Complex simulations.
In social science, the study on the emergence of macro-properties from the micro-properties, also known as macro-micro view in sociology. The topic is commonly recognized as social complexity that is often related to the use of computer simulation in social science, i.e.: computational sociology.
Complex systems.
Systems theory has long been concerned with the study of complex systems (in recent times, "complexity theory" and "complex systems" have also been used as names of the field). These systems are present in the research of a variety disciplines, including biology, economics, and technology. Recently, complexity has become a natural domain of interest of real world socio-cognitive systems and emerging systemics research. Complex systems tend to be high-dimensional, non-linear, and difficult to model. In specific circumstances, they may exhibit low-dimensional behaviour.
Complexity in data.
In information theory, algorithmic information theory is concerned with the complexity of strings of data.
Complex strings are harder to compress. While intuition tells us that this may depend on the codec used to compress a string (a codec could be theoretically created in any arbitrary language, including one in which the very small command "X" could cause the computer to output a very complicated string like "18995316"), any two Turing-complete languages can be implemented in each other, meaning that the length of two encodings in different languages will vary by at most the length of the "translation" language—which will end up being negligible for sufficiently large data strings.
These algorithmic measures of complexity tend to assign high values to random noise. However, those studying complex systems would not consider randomness as complexity.
Information entropy is also sometimes used in information theory as indicative of complexity.
Recent work in machine learning has examined the complexity of the data as it affects the performance of supervised classification algorithms. Ho and Basu present a set of complexity measures for binary classification problems. The complexity measures broadly cover 1) the overlaps in feature values from differing classes, 2) the separability of the classes, and 3) measures of geometry, topology, and density of manifolds. Instance hardness is another approach seeks to characterize the data complexity with the goal of determining how hard a data set is to classify correctly and is not limited to binary problems. Instance hardness is a bottom-up approach that first seeks to identify instances that are likely to be misclassified (or, in other words, which instances are the most complex). The characteristics of the instances that are likely to be misclassified are then measured based on the output from a set of hardness measures. The hardness measures are based on several supervised learning techniques such as measuring the number of disagreeing neighbors or the likelihood of the assigned class label given the input features. The information provided by the complexity measures has been examined for use in meta learning to determine for which data sets filtering (or removing suspected noisy instances from the training set) is the most beneficial and could be expanded to other areas.
Complexity in molecular recognition.
A recent study based on molecular simulations and compliance constants describes molecular recognition as a phenomenon of organisation.
Even for small molecules like carbohydrates, the recognition process can not be predicted or designed even assuming that each individual hydrogen bond's strength is exactly known.
Applications of complexity.
Computational complexity theory is the study of the complexity of problems—that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm—usually a computer program—to solve them as a function of the problem size. Some problems are difficult to solve, while others are easy. For example, some difficult problems need algorithms that take an exponential amount of time in terms of the size of the problem to solve. Take the travelling salesman problem, for example. It can be solved in time formula_1 (where "n" is the size of the network to visit—let's say the number of cities the travelling salesman must visit exactly once). As the size of the network of cities grows, the time needed to find the route grows (more than) exponentially.
Even though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space. Computational complexity may be approached from many different aspects. Computational complexity can be investigated on the basis of time, memory or other resources used to solve the problem. Time and space are two of the most important and popular considerations when problems of complexity are analyzed.
There exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.
There is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity
Bejan and Lorente showed that complexity is modest (not maximum, not increasing), and is a feature of the natural phenomenon of design generation in nature, which is predicted by the Constructal law.
Bejan and Lorente also showed that all the optimality (max,min) statements have limited ad-hoc applicability, and are unified under the Constructal law of design and evolution in nature.

</doc>
<doc id="7366" url="http://en.wikipedia.org/wiki?curid=7366" title="Chastity">
Chastity

Chastity is sexual behavior of a man or woman that is acceptable to the moral standards and guidelines of their culture, civilization or religion. In the Western world, the term has become closely associated (and is often used interchangeably) with sexual abstinence, especially before marriage. 
Etymology.
The words "chaste" and "chastity" stem from the Latin adjective "castus" meaning "pure". The words entered the English language around the middle of the 13th century; at that time they meant slightly different things. "Chaste" meant "virtuous or pure from unlawful sexual intercourse" (referring to extramarital sex),
while "chastity" meant "virginity". It was not until the late 16th century that the two words came to have the same basic meaning as a related adjective and noun.
In Abrahamic religions.
For Muslims and many Christians, acts of sexual nature are restricted to marriage. For unmarried persons, chastity is identified with sexual abstinence. Sexual acts outside or apart from marriage, such as adultery, fornication and prostitution, are considered sinful.
In Christianity.
In many Christian traditions, chastity is synonymous with sexual purity. Chastity means not having any sexual relations before marriage. It also means fidelity to husband or wife during marriage. In Catholic morality, chastity is placed opposite the deadly sin of lust, and is classified as one of seven virtues. The moderation of sexual desires is required to be virtuous. Reason, will and desire can harmoniously work together to do what is good.
In marriage, the spouses commit to a lifelong relationship which excludes sexual intimacy with other persons. Within marriage, various Abrahamic religions consider several practices to be considered unchaste, such as sexual intimacy during or shortly after menstruation or childbirth. After marriage, a third form of chastity, often called "vidual chastity", is expected of a woman while she is in mourning for her late husband. For example, Jeremy Taylor defined 5 rules in "Holy Living" (1650), including abstaining from marrying "so long as she is with child by her former husband" and "within the year of mourning".
The particular ethical system may not prescribe each of these. For example, Roman Catholics view sex within marriage as chaste, but prohibit the use of artificial contraception as an offense against chastity, seeing contraception as unnatural, contrary to God's will and design of human sexuality. Many Anglican communities allow for artificial contraception, seeing the restriction of family size as possibly not contrary to God's will. A stricter view is held by the Shakers, who prohibit marriage (and sexual intercourse under any circumstances) as a violation of chastity. The Catholic Church has set up various rules regarding clerical celibacy, while most Protestant communities allow clergy to marry.
Celibacy is required of monastics—monks, nuns and friars—even in a rare system of double cloisters, in which husbands could enter the (men's) monastery while their wives entered a (women's) sister monastery. Required celibacy among the clergy is a relatively recent practice: it became Church policy at the Second Lateran Council in 1139. It was not uniformly enforced among the clergy until 200 years later. Certain Latin-Rite Catholic may receive a dispensation to be married before ordination, and Eastern Catholic priests in many countries are also permitted to be married, provided they are so before ordination.
"Vows of chastity" can also be taken by laypersons, either as part of an organised religious life (such as Roman Catholic Beguines and Beghards in the past) or on an individual basis: as a voluntary act of devotion, or as part of an ascetic lifestyle (often devoted to contemplation), or both. 
The voluntary aspect has led it to being included among the main counsels of perfection. 
Chastity is a central and pivotal concept in Roman Catholic praxis. Chastity's importance in traditional Roman Catholic teaching stems from the fact that it is regarded as essential in maintaining and cultivating the unity of body with spirit and thus the integrity of the human being. It is also regarded as fundamental to the practise of the Catholic life because it involves an "apprenticeship in self-mastery". By attaining mastery over one's passions, reason, will and desire can harmoniously work together to do what is good.
In Eastern religions.
Hinduism.
Hinduism's view on premarital sex is rooted in its concept of the stages of life. The first of these stages, known as "Brahmacharya," roughly translates as chastity. Celibacy is considered the appropriate behavior for both male and female students during this stage, which precedes the stage of the married householder. Many Sadhus (Hindu monks) are also celibate as part of their ascetic discipline.
Jainism.
Although the Digambara followers of Jainism are celibate monks, most Jains belong to the Shvetambara sect, which allows spouses and children. The general Jain code of ethics requires that one do no harm to any living being in thought, action, or word. Adultery is clearly a violation of a moral agreement with one's spouse, and therefore forbidden, and fornication too is seen as a violation of the state of chastity.
Buddhism.
The teachings of Buddhism include the Noble Eightfold Path, comprising a division called right action. Under the Five Precepts ethical code, Upāsaka and Upāsikā lay followers should abstain from sexual misconduct, while Bhikkhu and Bhikkhuni monastics should practice strict chastity.
Daoism.
The Five Precepts of the Daoist religion include No Sexual Misconduct, which is interpreted as prohibiting extramarital sex for lay practitioners and marriage or sexual intercourse for monks and nuns.
Erotic chastity.
A number of devices came on the market in the 2000's and 2010's to ensure chastity, primarily for men being placed or placing themselves in chastity, but with some items for women. It is a type of erotic humiliation.

</doc>
<doc id="7376" url="http://en.wikipedia.org/wiki?curid=7376" title="Cosmic microwave background">
Cosmic microwave background

The cosmic microwave background (CMB) is the thermal radiation assumed to be left over from the "Big Bang" of cosmology. In older literature, the CMB is also variously known as cosmic microwave background radiation (CMBR) or "relic radiation." The CMB is a cosmic background radiation that is fundamental to observational cosmology because it is the oldest light in the universe, dating to the epoch of recombination. With a traditional optical telescope, the space between stars and galaxies (the "background") is completely dark. However, a sufficiently sensitive radio telescope shows a faint background glow, almost exactly the same in all directions, that is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the radio spectrum. The accidental discovery of CMB in 1964 by American radio astronomers Arno Penzias and Robert Wilson was the culmination of work initiated in the 1940s, and earned the discoverers the 1978 Nobel Prize.
The CMB is well explained as radiation left over from an early stage in the development of the universe, and its discovery is considered a landmark test of the Big Bang model of the universe. When the universe was young, before the formation of stars and planets, it was denser, much hotter, and filled with a uniform glow from a white-hot fog of hydrogen plasma. As the universe expanded, both the plasma and the radiation filling it grew cooler. When the universe cooled enough, protons and electrons combined to form neutral atoms. These atoms could no longer absorb the thermal radiation, and so the universe became transparent instead of being an opaque fog. Cosmologists refer to the time period when neutral atoms first formed as the "recombination epoch", and the event shortly afterwards when photons started to travel freely through space rather than constantly being scattered by electrons and protons in plasma is referred to as photon decoupling. The photons that existed at the time of photon decoupling have been propagating ever since, though growing fainter and less energetic, since the expansion of space causes their wavelength to increase over time (and wavelength is inversely proportional to energy according to Planck's relation). This is the source of the alternative term "relic radiation". The "surface of last scattering" refers to the set of points in space at the right distance from us so that we are now receiving photons originally emitted from those points at the time of photon decoupling.
Precise measurements of the CMB are critical to cosmology, since any proposed model of the universe must explain this radiation. The CMB has a thermal black body spectrum at a temperature of . The spectral radiance dEν/dν peaks at 160.2 GHz, in the microwave range of frequencies. (Alternatively if spectral radiance is defined as dEλ/dλ then the peak wavelength is 1.063 mm.)
The glow is very nearly uniform in all directions, but the tiny residual variations show a very specific pattern, the same as that expected of a fairly uniformly distributed hot gas that has expanded to the current size of the universe. In particular, the spectral radiance at different angles of observation in the sky contains small anisotropies, or irregularities, which vary with the size of the region examined. They have been measured in detail, and match what would be expected if small thermal variations, generated by quantum fluctuations of matter in a very tiny space, had expanded to the size of the observable universe we see today. This is a very active field of study, with scientists seeking both better data (for example, the Planck spacecraft) and better interpretations of the initial conditions of expansion. Although many different processes might produce the general form of a black body spectrum, no model other than the Big Bang has yet explained the fluctuations. As a result, most cosmologists consider the Big Bang model of the universe to be the best explanation for the CMB.
The high degree of uniformity throughout the observable universe and its faint but measured anisotropy lend strong support for the Big Bang model in general and the ΛCDM ("Lambda Cold Dark Matter") model in particular. Moreover, the WMAP and BICEP experiments have observed coherence of these fluctuations on angular scales that are larger than the apparent cosmological horizon at recombination. Either such coherence is acausally fine-tuned, or cosmic inflation occurred.<ref name="hep-ph/0309057"></ref>
On 17 March 2014, astronomers from the California Institute of Technology, the Harvard-Smithsonian Center for Astrophysics, Stanford University, and the University of Minnesota announced their detection of signature patterns of polarized light in the CMB, attributed to gravitational waves in the early universe, which if confirmed would provide strong evidence of cosmic inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported; and on 19 September 2014, even more lowered confidence.
Features.
The cosmic microwave background radiation is an emission of uniform, black body thermal energy coming from all parts of the sky. The radiation is isotropic to roughly one part in 100,000: the root mean square variations are only 18 µK, after subtracting out a dipole anisotropy from the Doppler shift of the background radiation. The latter is caused by the peculiar velocity of the Earth relative to the comoving cosmic rest frame as the planet moves at some 371 km/s towards the constellation Leo. The CMB dipole as well as aberration at higher multipoles have been measured, consistent with galactic motion.
In the Big Bang model for the formation of the universe, Inflationary Cosmology predicts that after about 10−37 seconds the nascent universe underwent exponential growth that smoothed out nearly all inhomogeneities. The remaining inhomogeneities were caused by quantum fluctuations in the inflaton field that caused the inflation event. After 10−6 seconds, the early universe was made up of a hot, interacting plasma of photons, electrons, and baryons. As the universe expanded, adiabatic cooling caused the energy density of the plasma to decrease until it became favorable for electrons to combine with protons, forming hydrogen atoms. This recombination event happened when the temperature was around 3000 K or when the universe was approximately 379,000 years old. At this point, the photons no longer interacted with the now electrically neutral atoms and began to travel freely through space, resulting in the decoupling of matter and radiation.
The color temperature of the ensemble of decoupled photons has continued to diminish ever since; now down to , it will continue to drop as the universe expands. The intensity of the radiation also corresponds to black-body radiation at 2.726 K because red-shifted black-body radiation is just like black-body radiation at a lower temperature. According to the Big Bang model, the radiation from the sky we measure today comes from a spherical surface called "the surface of last scattering". This represents the set of locations in space at which the decoupling event is estimated to have occurred and at a point in time such that the photons from that distance have just reached observers. Most of the radiation energy in the universe is in the cosmic microwave background, making up a fraction of roughly of the total density of the universe.
Two of the greatest successes of the Big Bang theory are its prediction of the almost perfect black body spectrum and its detailed prediction of the anisotropies in the cosmic microwave background. The CMB spectrum has become the most precisely measured black body spectrum in nature.
History.
The cosmic microwave background was first predicted in 1948 by Ralph Alpher, and Robert Herman. Alpher and Herman were able to estimate the temperature of the cosmic microwave background to be 5 K, though two years later they re-estimated it at 28 K. This high estimate was due to a mis-estimate of the Hubble constant by Alfred Behr, which could not be replicated and was later abandoned for the earlier estimate. Although there were several previous estimates of the temperature of space, these suffered from two flaws. First, they were measurements of the "effective" temperature of space and did not suggest that space was filled with a thermal Planck spectrum. Next, they depend on our being at a special spot at the edge of the Milky Way galaxy and they did not suggest the radiation is isotropic. The estimates would yield very different predictions if Earth happened to be located elsewhere in the Universe.
The 1948 results of Alpher and Herman were discussed in many physics settings through about 1955, when both left the Applied Physics Laboratory at Johns Hopkins University. The mainstream astronomical community, however, was not intrigued at the time by cosmology. Alpher and Herman's prediction was rediscovered by Yakov Zel'dovich in the early 1960s, and independently predicted by Robert Dicke at the same time. The first published recognition of the CMB radiation as a detectable phenomenon appeared in a brief paper by Soviet astrophysicists A. G. Doroshkevich and Igor Novikov, in the spring of 1964. In 1964, David Todd Wilkinson and Peter Roll, Dicke's colleagues at Princeton University, began constructing a Dicke radiometer to measure the cosmic microwave background. In 1964, Arno Penzias and Robert Woodrow Wilson at the Crawford Hill location of Bell Telephone Laboratories in nearby Holmdel Township, New Jersey had built a Dicke radiometer that they intended to use for radio astronomy and satellite communication experiments. On 20 May 1964 they made their first measurement clearly showing the presence of the microwave background, with their instrument having an excess 4.2K antenna temperature which they could not account for. After receiving a telephone call from Crawford Hill, Dicke famously quipped: "Boys, we've been scooped." A meeting between the Princeton and Crawford Hill groups determined that the antenna temperature was indeed due to the microwave background. Penzias and Wilson received the 1978 Nobel Prize in Physics for their discovery.
The interpretation of the cosmic microwave background was a controversial issue in the 1960s with some proponents of the steady state theory arguing that the microwave background was the result of scattered starlight from distant galaxies. Using this model, and based on the study of narrow absorption line features in the spectra of stars, the astronomer Andrew McKellar wrote in 1941: "It can be calculated that the 'rotational temperature' of interstellar space is 2 K." However, during the 1970s the consensus was established that the cosmic microwave background is a remnant of the big bang. This was largely because new measurements at a range of frequencies showed that the spectrum was a thermal, black body spectrum, a result that the steady state model was unable to reproduce.
Harrison, Peebles, Yu and Zel'dovich realized that the early universe would have to have inhomogeneities at the level of 10−4 or 10−5. Rashid Sunyaev later calculated the observable imprint that these inhomogeneities would have on the cosmic microwave background. Increasingly stringent limits on the anisotropy of the cosmic microwave background were set by ground based experiments during the 1980s. RELIKT-1, a Soviet cosmic microwave background anisotropy experiment on board the Prognoz 9 satellite (launched 1 July 1983) gave upper limits on the large-scale anisotropy. The NASA COBE mission clearly confirmed the primary anisotropy with the Differential Microwave Radiometer instrument, publishing their findings in 1992. The team received the Nobel Prize in physics for 2006 for this discovery.
Inspired by the COBE results, a series of ground and balloon-based experiments measured cosmic microwave background anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the scale of the first acoustic peak, which COBE did not have sufficient resolution to resolve. This peak corresponds to large scale density variations in the early universe that are created by gravitational instabilities, resulting in acoustical oscillations in the plasma. The first peak in the anisotropy was tentatively detected by the Toco experiment and the result was confirmed by the BOOMERanG and MAXIMA experiments. These measurements demonstrated that the geometry of the Universe is approximately flat, rather than curved. They ruled out cosmic strings as a major component of cosmic structure formation and suggested cosmic inflation was the right theory of structure formation.
The second peak was tentatively detected by several experiments before being definitively detected by WMAP, which has also tentatively detected the third peak. As of 2010, several experiments to improve measurements of the polarization and the microwave background on small angular scales are ongoing. These include DASI, WMAP, BOOMERanG, QUaD, Planck spacecraft, Atacama Cosmology Telescope, South Pole Telescope and the QUIET telescope.
Relationship to the Big Bang.
The cosmic microwave background radiation and the cosmological redshift-distance relation are together regarded as the best available evidence for the Big Bang theory. Measurements of the CMB have made the inflationary Big Bang theory the Standard Model of Cosmology. The discovery of the CMB in the mid-1960s curtailed interest in alternatives such as the steady state theory.
The CMB essentially confirms the Big Bang theory. In the late 1940s Alpher and Herman reasoned that if there was a big bang, the expansion of the Universe would have stretched and cooled the high-energy radiation of the very early Universe into the microwave region and down to a temperature of about 5 K. They were slightly off with their estimate, but they had exactly the right idea. They predicted the CMB. It took another 15 years for Penzias and Wilson to stumble into discovering that the microwave background was actually there.
The CMB gives a snapshot of the universe when, according to standard cosmology, the temperature dropped enough to allow electrons and protons to form hydrogen atoms, thus making the universe transparent to radiation. When it originated some 380,000 years after the Big Bang—this time is generally known as the "time of last scattering" or the period of recombination or decoupling—the temperature of the universe was about 3000 K. This corresponds to an energy of about 0.25 eV, which is much less than the 13.6 eV ionization energy of hydrogen.
Since decoupling, the temperature of the background radiation has dropped by a factor of roughly 1,100 due to the expansion of the universe. As the universe expands, the CMB photons are redshifted, making the radiation's temperature inversely proportional to a parameter called the universe's scale length. The temperature "T"r of the CMB as a function of redshift, z, can be shown to be proportional to the temperature of the CMB as observed in the present day (2.725 K or 0.235 meV):
For details about the reasoning that the radiation is evidence for the Big Bang, see Cosmic background radiation of the Big Bang.
Primary anisotropy.
The anisotropy of the cosmic microwave background is divided into two types: primary anisotropy, due to effects which occur at the last scattering surface and before; and secondary anisotropy, due to effects such as interactions of the background radiation with hot gas or gravitational potentials, which occur between the last scattering surface and the observer.
The structure of the cosmic microwave background anisotropies is principally determined by two effects: acoustic oscillations and diffusion damping (also called collisionless damping or Silk damping). The acoustic oscillations arise because of a conflict in the photon–baryon plasma in the early universe. The pressure of the photons tends to erase anisotropies, whereas the gravitational attraction of the baryons—moving at speeds much slower than light—makes them tend to collapse to form dense haloes. These two effects compete to create acoustic oscillations which give the microwave background its characteristic peak structure. The peaks correspond, roughly, to resonances in which the photons decouple when a particular mode is at its peak amplitude.
The peaks contain interesting physical signatures. The angular scale of the first peak determines the curvature of the universe (but not the topology of the universe). The next peak—ratio of the odd peaks to the even peaks—determines the reduced baryon density. The third peak can be used to get information about the dark matter density.
The locations of the peaks also give important information about the nature of the primordial density perturbations. There are two fundamental types of density perturbations—called "adiabatic" and "isocurvature". A general density perturbation is a mixture of both, and different theories that purport to explain the primordial density perturbation spectrum predict different mixtures.
The CMB spectrum can distinguish between these two because these two types of perturbations produce different peak locations. Isocurvature density perturbations produce a series of peaks whose angular scales ("l"-values of the peaks) are roughly in the ratio 1:3:5:..., while adiabatic density perturbations produce peaks whose locations are in the ratio 1:2:3:... Observations are consistent with the primordial density perturbations being entirely adiabatic, providing key support for inflation, and ruling out many models of structure formation involving, for example, cosmic strings.
Collisionless damping is caused by two effects, when the treatment of the primordial plasma as fluid begins to break down:
These effects contribute about equally to the suppression of anisotropies at small scales, and give rise to the characteristic exponential damping tail seen in the very small angular scale anisotropies.
The depth of the LSS refers to the fact that the decoupling of the photons and baryons does not happen instantaneously, but instead requires an appreciable fraction of the age of the Universe up to that era. One method of quantifying how long this process took uses the "photon visibility function" (PVF). This function is defined so that, denoting the PVF by P(t), the probability that a CMB photon last scattered between time t and t+dt is given by P(t)dt.
The maximum of the PVF (the time when it is most likely that a given CMB photon last scattered) is known quite precisely. The first-year WMAP results put the time at which P(t) is maximum as 372,000 years. This is often taken as the "time" at which the CMB formed. However, to figure out how "long" it took the photons and baryons to decouple, we need a measure of the width of the PVF. The WMAP team finds that the PVF is greater than half of its maximum value (the "full width at half maximum", or FWHM) over an interval of 115,000 years. By this measure, decoupling took place over roughly 115,000 years, and when it was complete, the universe was roughly 487,000 years old.
Late time anisotropy.
Since the CMB came into existence, it has apparently been modified by several subsequent physical processes, which are collectively referred to as late-time anisotropy, or secondary anisotropy. When the CMB photons became free to travel unimpeded, ordinary matter in the universe was mostly in the form of neutral hydrogen and helium atoms. However, observations of galaxies today seem to indicate that most of the volume of the intergalactic medium (IGM) consists of ionized material (since there are few absorption lines due to hydrogen atoms). This implies a period of reionization during which some of the material of the universe was broken into hydrogen ions.
The CMB photons are scattered by free charges such as electrons that are not bound in atoms. In an ionized universe, such charged particles have been liberated from neutral atoms by ionizing (ultraviolet) radiation. Today these free charges are at sufficiently low density in most of the volume of the Universe that they do not measurably affect the CMB. However, if the IGM was ionized at very early times when the universe was still denser, then there are two main effects on the CMB:
Both of these effects have been observed by the WMAP spacecraft, providing evidence that the universe was ionized at very early times, at a redshift more than 17. The detailed provenance of this early ionizing radiation is still a matter of scientific debate. It may have included starlight from the very first population of stars (population III stars), supernovae when these first stars reached the end of their lives, or the ionizing radiation produced by the accretion disks of massive black holes.
The time following the emission of the cosmic microwave background—and before the observation of the first stars—is semi-humorously referred to by cosmologists as the dark age, and is a period which is under intense study by astronomers (See 21 centimeter radiation).
Two other effects which occurred between reionization and our observations of the cosmic microwave background, and which appear to cause anisotropies, are the Sunyaev–Zel'dovich effect, where a cloud of high-energy electrons scatters the radiation, transferring some of its energy to the CMB photons, and the Sachs–Wolfe effect, which causes photons from the Cosmic Microwave Background to be gravitationally redshifted or blueshifted due to changing gravitational fields.
Polarization.
The cosmic microwave background is polarized at the level of a few microkelvin. There are two types of polarization, called "E"-modes and "B"-modes. This is in analogy to electrostatics, in which the electric field ("E"-field) has a vanishing curl and the magnetic field ("B"-field) has a vanishing divergence. The "E"-modes arise naturally from Thomson scattering in a heterogeneous plasma. The "B"-modes are not sourced by standard scalar type perturbations. Instead they can be 
sourced by two mechanisms: first one is by gravitational lensing of E-modes, which has been measured by South Pole Telescope in 2013. Second one is from gravitational waves arising from cosmic inflation. Detecting the "B"-modes is extremely difficult, particularly as the degree of foreground contamination is unknown, and the weak gravitational lensing signal mixes the relatively strong "E"-mode signal with the "B"-mode signal.
Microwave background observations.
Subsequent to the discovery of the CMB, hundreds of cosmic microwave background experiments have been conducted to measure and characterize the signatures of the radiation. The most famous experiment is probably the NASA Cosmic Background Explorer (COBE) satellite that orbited in 1989–1996 and which detected and quantified the large scale anisotropies at the limit of its detection capabilities. Inspired by the initial COBE results of an extremely isotropic and homogeneous background, a series of ground- and balloon-based experiments quantified CMB anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the angular scale of the first acoustic peak, for which COBE did not have sufficient resolution. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the right theory. During the 1990s, the first peak was measured with increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. Together with other cosmological data, these results implied that the geometry of the Universe is flat. A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum.
In June 2001, NASA launched a second CMB space mission, WMAP, to make much more precise measurements of the large scale anisotropies over the full sky. WMAP used symmetric, rapid-multi-modulated scanning, rapid switching radiometers to minimize non-sky signal noise. The first results from this mission, disclosed in 2003, were detailed measurements of the angular power spectrum at a scale of less than one degree, tightly constraining various cosmological parameters. The results are broadly consistent with those expected from cosmic inflation as well as various other competing theories, and are available in detail at NASA's data bank for Cosmic Microwave Background (CMB) (see links below). Although WMAP provided very accurate measurements of the large scale angular fluctuations in the CMB (structures about as broad in the sky as the moon), it did not have the angular resolution to measure the smaller scale fluctuations which had been observed by former ground-based interferometers.
All-sky map.
A third space mission, the ESA (European Space Agency) Planck Surveyor, was launched in May 2009 and is currently performing an even more detailed investigation. Planck employs both HEMT radiometers and bolometer technology and will measure the CMB at a smaller scale than WMAP. Its detectors were trialled in the Antarctic Viper telescope as ACBAR (Arcminute Cosmology Bolometer Array Receiver) experiment—which has produced the most precise measurements at small angular scales to date—and in the Archeops balloon telescope.
On 21 March 2013, the European-led research team behind the Planck cosmology probe released the mission's all-sky map (, ) of the cosmic microwave background. The map suggests the universe is slightly older than researchers thought. According to the map, subtle fluctuations in temperature were imprinted on the deep sky when the cosmos was about 370,000 years old. The imprint reflects ripples that arose as early, in the existence of the universe, as the first nonillionth of a second. Apparently, these ripples gave rise to the present vast cosmic web of galaxy clusters and dark matter. According to the team, the universe is 13.798 ± 0.037 billion years old, and contains 4.9% ordinary matter, 26.8% dark matter and 68.3% dark energy. Also, the Hubble constant was measured to be 67.80 ± 0.77 (km/s)/Mpc.
Additional ground-based instruments such as the South Pole Telescope in Antarctica and the proposed Clover Project, Atacama Cosmology Telescope and the QUIET telescope in Chile will provide additional data not available from satellite observations, possibly including the B-mode polarization.
Data reduction and analysis.
Raw CMBR data from the space vehicle (i.e. WMAP) contain foreground effects that completely obscure the fine-scale structure of the cosmic microwave background. The fine-scale structure is superimposed on the raw CMBR data but is too small to be seen at the scale of the raw data. The most prominent of the foreground effects is the dipole anisotropy caused by the Sun's motion relative to the CMBR background. The dipole anisotropy and others due to Earth's annual motion relative to the Sun and numerous microwave sources in the galactic plane and elsewhere must be subtracted out to reveal the extremely tiny variations characterizing the fine-scale structure of the CMBR background.
The detailed analysis of CMBR data to produce maps, an angular power spectrum, and ultimately cosmological parameters is a complicated, computationally difficult problem. Although computing a power spectrum from a map is in principle a simple Fourier transform, decomposing the map of the sky into spherical harmonics, in practice it is hard to take the effects of noise and foreground sources into account. In particular, these foregrounds are dominated by galactic emissions such as Bremsstrahlung, synchrotron, and dust that emit in the microwave band; in practice, the galaxy has to be removed, resulting in a CMB map that is not a full-sky map. In addition, point sources like galaxies and clusters represent another source of foreground which must be removed so as not to distort the short scale structure of the CMB power spectrum.
Constraints on many cosmological parameters can be obtained from their effects on the power spectrum, and results are often calculated using Markov Chain Monte Carlo sampling techniques.
CMBR dipole anisotropy.
From the CMB data it is seen that our local group of galaxies (the galactic cluster that includes the Solar System's Milky Way Galaxy) appears to be moving at relative to the reference frame of the CMB (also called the CMB rest frame, or the frame of reference in which there is no motion through the CMB) in the direction of galactic longitude "l" = , "b" = . This motion results in an anisotropy of the data (CMB appearing slightly warmer in the direction of movement than in the opposite direction). The standard interpretation of this temperature variation is a simple velocity red shift and blue shift due to motion relative to the CMB, but alternative cosmological models can explain some fraction of the observed dipole temperature distribution in the CMB.
Low multipoles and other anomalies.
With the increasingly precise data provided by WMAP, there have been a number of claims that the CMB exhibits anomalies, such as very large scale anisotropies, anomalous alignments, and non-Gaussian distributions.<ref name="arXiv:astro-ph/0511666"></ref><ref name="arXiv:astro-ph/0503213"></ref> The most longstanding of these is the low-"l" multipole controversy. Even in the COBE map, it was observed that the quadrupole ("l" =2, spherical harmonic) has a low amplitude compared to the predictions of the Big Bang. In particular, the quadrupole and octupole ("l" =3) modes appear to have an unexplained alignment with each other and with both the ecliptic plane and equinoxes, an alignment sometimes referred to as the "axis of evil". A number of groups have suggested that this could be the signature of new physics at the greatest observable scales; other groups suspect systematic errors in the data. Ultimately, due to the foregrounds and the cosmic variance problem, the greatest modes will never be as well measured as the small angular scale modes. The analyses were performed on two maps that have had the foregrounds removed as far as possible: the "internal linear combination" map of the WMAP collaboration and a similar map prepared by Max Tegmark and others. Later analyses have pointed out that these are the modes most susceptible to foreground contamination from synchrotron, dust, and Bremsstrahlung emission, and from experimental uncertainty in the monopole and dipole. A full Bayesian analysis of the WMAP power spectrum demonstrates that the quadrupole prediction of Lambda-CDM cosmology is consistent with the data at the 10% level and that the observed octupole is not remarkable. Carefully accounting for the procedure used to remove the foregrounds from the full sky map further reduces the significance of the alignment by ~5%.
Recent observations with the Planck telescope, which is very much more sensitive than WMAP and has a larger angular resolution, confirm the observation of the axis of evil. Since two different instruments recorded the same anomaly, instrumental error (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, "I do think there is a bit of a psychological effect; people want to find unusual things." 

</doc>
<doc id="7378" url="http://en.wikipedia.org/wiki?curid=7378" title="Comparative law">
Comparative law

Comparative law is the study of differences and similarities between the law of different countries. More specifically, it involves study of the different legal systems in existence in the world, including the common law, the civil law, socialist law, Jewish Law, Islamic law, Hindu law, and Chinese law. It includes the description and analysis of foreign legal systems, even where no explicit comparison is undertaken. The importance of comparative law has increased enormously in the present age of internationalism, economic globalization and democratization.
History.
The origins of modern comparative law can be traced back to 18th century Europe, although, prior to that, legal scholars had always practiced comparative methodologies.
Montesquieu is generally regarded as an early founding figure of comparative law. His comparative approach is obvious in the following excerpt from Chapter III of Book I of his masterpiece, "De l'esprit des lois" (1748; first translated by Thomas Nugent, 1750):
Also, in Chapter XI (entitled 'How to compare two different Systems of Laws') of Book XXIX, discussing the French and English systems for punishment of false witnesses, he advises that "to determine which of those systems is most agreeable to reason, we must take them each as a whole and compare them in their entirety." Yet another place where Montesquieu's comparative approach is evident is the following, from Chapter XIII of Book XXIX:
The modern founding figure of comparative and anthropological jurisprudence was Sir Henry Maine, a British jurist and legal historian. In his 1861 work "Ancient Law: Its Connection with the Early History of Society, and Its Relation to Modern Ideas", he set out his views on the development of legal institutions in primitive societies and engaged in a comparative discussion of Eastern and Western legal traditions. This work placed comparative law in its historical context and was widely read and influential. 
The first university course on the subject was established at the University of Oxford in 1869, with Maine taking up the position of professor.
Comparative law in the US was brought by a legal scholar fleeing persecution in Germany, Rudolf Schlesinger. Schlesinger eventually became professor of comparative law at Cornell Law School helping to spread the discipline throughout the US.
Purpose.
Comparative law is an academic study of separate legal systems, each one analysed in its constitutive elements; how they differ in the different legal systems, and how their elements combine into a system.
Several disciplines have developed as separate branches of comparative law, including comparative constitutional law, comparative administrative law, comparative civil law (in the sense of the law of torts, delicts, contracts and obligations), comparative commercial law (in the sense of business organisations and trade), and comparative criminal law. Studies of these specific areas may be viewed as micro- or macro-comparative legal analysis, i.e. detailed comparisons of two countries, or broad-ranging studies of several countries. Comparative civil law studies, for instance, show how the law of private relations is organised, interpreted and used in different systems or countries. It appears today the principal purposes of comparative law are:
Relationship with other legal subjects.
Comparative law is different from the fields of general jurisprudence (legal theory), international law, including both public international law and private international law (also known as conflict of laws).
Despite the differences between comparative law and these other legal fields, comparative law helps inform all of these areas of normativity. For example, comparative law can help international legal institutions, such as those of the United Nations System, in analyzing the laws of different countries regarding their treaty obligations. Comparative law would be applicable to private international law when developing an approach to interpretation in a conflicts analysis. Comparative law may contribute to legal theory by creating categories and concepts of general application. Comparative law may also provide insights into the question of legal transplants, i.e. the transplanting of law and legal institutions from one system to another. The notion of legal transplants was coined by Alan Watson, one of the world's renowned legal scholars specializing in comparative law.
Also, the usefulness of comparative law for sociology of law and law and economics (and vice versa) is very large. The comparative study of the various legal systems may show how different legal regulations for the same problem function in practice. Conversely, sociology of law and law & economics may help comparative law answer questions, such as: How do regulations in different legal systems really function in the respective societies? Are certain legal rules comparable? How do the similarities and differences between legal systems get explained?
Classifications of legal systems.
Arminjon, Nolde, and Wolff.
Arminjon, Nolde, and Wolff believed that, for purposes of classifying the (then) contemporary legal systems of the world, it was required that those systems "per se" get studied, irrespective of external factors, such as geographical ones. They proposed the classification of legal system into seven groups, or so-called 'families', in particular the
David.
David proposed the classification of legal systems, according to the different ideology inspiring each one, into five groups or families:
Especially with respect to the aggregating by David of the Romano-Germanic and Anglo-Saxon Laws into a single family, David argued that the antithesis between the Anglo-Saxon Laws and Romano-German Laws, is of a technical rather than of an ideological nature. Of a different kind is, for instance, the antithesis between (say) the Italian and the American Law, and of a different kind that between the Soviet, Muslim, Hindu, or Chinese Law. According to David, the Romano-Germanic legal systems included those countries where legal science was formulated according to Roman Law, whereas common law countries are those where law was created from the judges. The characteristics that he believed uniquely differentiate the Western legal family from the other four are
Zweigert and Kötz.
Konrad Zweigert and Hein Kötz propose a different, multidimensional methodology for categorizing laws, i.e. for ordering families of laws. They maintain that, to determine such families, five criteria should be taken into account, in particular: the historical background, the characteristic way of thought, the different institutions, the recognized sources of law, and the dominant ideology. Using the aforementioned criteria, they classify the legal systems of the world into six families:
Up to the second German edition of their introduction to comparative law, Zweigert and Kötz also used to mention Soviet or socialist law as another family of laws.

</doc>
<doc id="7380" url="http://en.wikipedia.org/wiki?curid=7380" title="CD (disambiguation)">
CD (disambiguation)

A CD or compact disc is a thin plastic silvery disc for audio recordings.
CD or cd may also refer to:

</doc>
<doc id="7381" url="http://en.wikipedia.org/wiki?curid=7381" title="Cyberspace">
Cyberspace

Cyberspace is "the notional environment in which communication over computer networks occurs." The term was first used in science fiction and cinema in the 1980s, was adopted by computer professionals and became a household term in the 1990s. During this period, the uses of the internet, networking, and digital communication were all growing dramatically and the term "cyberspace" was able to represent the many new ideas and phenomena that were emerging.
The parent term of cyberspace is "cybernetics", derived from the Ancient Greek κυβερνήτης (kybernētēs, steersman, governor, pilot, or rudder), a word introduced by Norbert Wiener for his pioneering work in electronic communication and control science.
As a social experience, individuals can interact, exchange ideas, share information, provide social support, conduct business, direct actions, create artistic media, play games, engage in political discussion, and so on, using this global network. They are sometimes referred to as "cybernauts". The term "cyberspace" has become a conventional means to describe anything associated with the Internet and the diverse Internet culture. The United States government recognizes the interconnected information technology and the interdependent network of information technology infrastructures operating across this medium as part of the US national critical infrastructure. Amongst individuals on cyberspace, there is believed to be a code of shared rules and ethics mutually beneficial for all to follow, referred to as cyberethics. Many view the right to privacy as most important to a functional code of cyberethics. Such moral responsibilities go hand in hand when working online with global networks, specifically, when opinions are involved with online social experiences.
According to Chip Morningstar and F. Randall Farmer, cyberspace is defined more by the social interactions involved rather than its technical implementation. In their view, the computational medium in cyberspace is an augmentation of the communication channel between real people; the core characteristic of cyberspace is that it offers an environment that consists of many participants with the ability to affect and influence each other. They derive this concept from the observation that people seek richness, complexity, and depth within a virtual world.
Origins of the term.
The term "cyberspace" began appearing in fiction in the 1980s (for example, the 1980 Vernor Vinge novella "True Names", and the 1980 John M. Ford novel "Web of Angels.") Yet it was through the work of cyberpunk science fiction author William Gibson, that the word became prominently identified with online computer networks, beginning with the 1982 story "Burning Chrome" and popularized by his 1984 novel "Neuromancer." The portion of "Neuromancer" cited in this respect is usually the following:
Now widely used, the term has since been criticized by Gibson, who commented on the origin of the term in the 2000 documentary "No Maps for These Territories":
Metaphorical.
Don Slater uses a metaphor to define cyberspace, describing the "sense of a social setting that exists purely within a space of representation and communication . . . it exists entirely within a computer space, distributed across increasingly complex and fluid networks."
The term "Cyberspace" started to become a de facto synonym for the internet, and later the World Wide Web, during the 1990s, especially in academic circles and activist communities. Author Bruce Sterling, who popularized this meaning, credits John Perry Barlow as the first to use it to refer to "the present-day nexus of computer and telecommunications networks." Barlow describes it thus in his essay to announce the formation of the Electronic Frontier Foundation (note the spatial metaphor) in June, 1990:
As Barlow, and the EFF, continued public education efforts to promote the idea of "digital rights", the term was increasingly used during the internet boom of the late 1990s.
Virtual environments.
Although the present-day, loose use of the term "cyberspace" no longer implies or suggests immersion in a virtual reality, current technology allows the integration of a number of capabilities (sensors, signals, connections, transmissions, processors, and controllers) sufficient to generate a virtual interactive experience that is accessible regardless of a geographic location.
In 1989, Autodesk, an American multinational corporation that focuses on 2D and 3D design software, developed a virtual design system called Cyberspace.
Recent definitions of Cyberspace.
Although you can find several definitions of cyberspace both in scientific literature and in official governamental sources there is no fully agreed official definition yet. According to F. D. Kramer there are 28 different definitions of the term cyberspace.
The most recent draft definition is the following:
Cyberspace is a global and dynamic domain (subject to constant change) characterized
by the combined use of electrons and electromagnetic spectrum, whose purpose is to
create, store, modify, exchange, share and extract, use, eliminate information and disrupt
physical resources.Cyberspace includes: a) physical infrastructures and telecommunications devices that allow for the
connection of technological and communication system networks, understood in
the broadest sense (SCADA devices, smartphones/tablets, computers, servers,
etc.); b) computer systems (see point a) and the related (sometimes embedded) software
that guarantee the domain's basic operational functioning and connectivity; c) networks between computer systems;d) networks of networks that connect computer systems (the distinction between networks and networks of networks is mainly organizational);e) the access nodes of users and intermediaries routing nodes; f) constituent data (or resident data).Often, in common parlance, and sometimes in commercial language, networks of
networks are called internet (with a lowercase i), while networks between computers are
called intranet. Internet (with a capital I, in journalistic language sometimes called the Net)
can be considered a part of the system a). A distinctive and constitutive feature of
cyberspace is that no central entity exercises control over all the networks that make up
this new domain.
Just as in the real world there is no world government, cyberspace lacks an institutionally
predefined hierarchical center. To cyberspace, a domain without a hierarchical ordering
principle, we can therefore extend the definition of international politics coined by
Kenneth Waltz: as being "with no system of law enforceable." This does not mean that the dimension of power in cyberspace is absent, nor that power
is dispersed and scattered into a thousand invisible streams, nor that it is evenly spread
across myriad people and organizations, as some scholars had predicted. On the contrary, cyberspace is characterized by a precise structuring of hierarchies of power.
Cyberspace as an internet metaphor.
While cyberspace should not be confused with the Internet, the term is often used to refer to objects and identities that exist largely within the communication network itself, so that a website, for example, might be metaphorically said to "exist in cyberspace". According to this interpretation, events taking place on the internet are not happening in the locations where participants or servers are physically located, but "in cyberspace".
Firstly, cyberspace describes the flow of digital data through the network of interconnected computers: it is at once not "real", since one could not spatially locate it as a tangible object, and clearly "real" in its effects. Secondly, cyberspace is the site of computer-mediated communication (CMC), in which online relationships and alternative forms of online identity were enacted, raising important questions about the social psychology of internet use, the relationship between "online" and "offline" forms of life and interaction, and the relationship between the "real" and the virtual. Cyberspace draws attention to remediation of culture through new media technologies: it is not just a communication tool but a social destination, and is culturally significant in its own right. Finally, cyberspace can be seen as providing new opportunities to reshape society and culture through "hidden" identities, or it can be seen as borderless communication and culture.
The "space" in cyberspace has more in common with the abstract, mathematical meanings of the term (see space) than physical space. It does not have the duality of positive and negative volume (while in physical space for example a room has the negative volume of usable space delineated by positive volume of walls, internet users cannot enter the screen and explore the unknown part of the internet as an extension of the space they are in), but spatial meaning can be attributed to the relationship between different pages (of books as well as webservers), considering the unturned pages to be somewhere "out there." The concept of cyberspace therefore refers not to the content being presented to the surfer, but rather to the possibility of surfing among different sites, with feedback loops between the user and the rest of the system creating the potential to always encounter something unknown or unexpected.
Videogames differ from text-based communication in that on-screen images are meant to be figures that actually occupy a space and the animation shows the movement of those figures. Images are supposed to form the positive volume that delineates the empty space. A game adopts the cyberspace metaphor by engaging more players in the game, and then figuratively representing them on the screen as avatars. Games do not have to stop at the avatar-player level, but current implementations aiming for more immersive playing space (i.e. Laser tag) take the form of augmented reality rather than cyberspace, fully immersive virtual realities remaining impractical.
Although the more radical consequences of the global communication network predicted by some cyberspace proponents (i.e. the diminishing of state influence envisioned by John Perry Barlow) failed to materialize and the word lost some of its novelty appeal, it remains current as of 2006.
Some virtual communities explicitly refer to the concept of cyberspace, for example Linden Lab calling their customers "Residents" of Second Life, while all such communities can be positioned "in cyberspace" for explanatory and comparative purposes (as did Sterling in "The Hacker Crackdown", followed by many journalists), integrating the metaphor into a wider cyber-culture.
The metaphor has been useful in helping a new generation of thought leaders to reason through new military strategies around the world, led largely by the US Department of Defense (DoD). The use of cyberspace as a metaphor has had its limits, however, especially in areas where the metaphor becomes confused with physical infrastructure. It has also been critiqued as being unhelpful for falsely employing a spatial metaphor to describe what is inherently a network.
Alternate realities in philosophy and art.
Predating computers.
A forerunner of the modern ideas of cyberspace is the Cartesian notion that people might be deceived by an evil demon that feeds them a false reality. This argument is the direct predecessor of modern ideas of a brain in a vat and many popular conceptions of cyberspace take Descartes's ideas as their starting point.
Visual arts have a tradition, stretching back to antiquity, of artifacts meant to fool the eye and be mistaken for reality. This questioning of reality occasionally led some philosophers and especially theologians to distrust art as deceiving people into entering a world which was not real (see Aniconism). The artistic challenge was resurrected with increasing ambition as art became more and more realistic with the invention of photography, film (see "Arrival of a Train at La Ciotat"), and immersive computer simulations.
Influenced by computers.
Philosophy.
American counterculture exponents like William S. Burroughs (whose literary influence on Gibson and cyberpunk in general is widely acknowledged) and Timothy Leary were among the first to extoll the potential of computers and computer networks for individual empowerment.
Some contemporary philosophers and scientists (e.g. David Deutsch in "The Fabric of Reality") employ virtual reality in various thought experiments. For example Philip Zhai in "Get Real: A Philosophical Adventure in Virtual Reality" connects cyberspace to the platonic tradition:
Note that this brain-in-a-vat argument conflates cyberspace with reality, while the more common descriptions of cyberspace contrast it with the "real world".
Art.
Having originated among writers, the concept of cyberspace remains most popular in literature and film. Although artists working with other media have expressed interest in the concept, such as Roy Ascott, "cyberspace" in digital art is mostly used as a synonym for immersive virtual reality and remains more discussed than enacted.
Indian epic Mahabaratha written by sage Vyasar talks about concepts what is called today Virtual reality, Transportation in to matrix and web conferencing.
Computer crime.
Cyberspace also brings together every service and facility imaginable to expedite money laundering. One can purchase anonymous credit cards, bank accounts, encrypted global mobile telephones, and false passports. From there one can pay professional advisors to set up IBCs (International Business Corporations, or corporations with anonymous ownership) or similar structures in OFCs (Offshore Financial Centers). Such advisors are loath to ask any penetrating questions about the wealth and activities of their clients, since the average fees criminals pay them to launder their money can be as much as 20 percent.
5-level model.
In 2010, a was designed in France. According to this model, cyberspace is composed of 5 layers based on information discoveries: language, writing, printing, Internet, etc. This original model links the world of information to telecommunication technologies.

</doc>
<doc id="7382" url="http://en.wikipedia.org/wiki?curid=7382" title="The Maritimes">
The Maritimes

The Maritime provinces, also called the Maritimes or the Canadian Maritimes, is a region of Eastern Canada consisting of three provinces, New Brunswick, Nova Scotia, and Prince Edward Island. On the Atlantic coast, the Maritimes is often mentioned in conjunction with the northeastern province of Newfoundland and Labrador; together they represent Atlantic Canada. The population of the Maritime provinces was 1,813,102 in 2011.
The Maritimes front the Atlantic Ocean and its various sub-basins such as the Gulf of Maine and Gulf of St. Lawrence systems. The region is located northeast of New England, southeast of Quebec's Gaspé Peninsula, and southwest of the island of Newfoundland.
There was talk of a Maritime Union of the three provinces to have greater political power; however, the first discussions on the subject in 1864 at the Charlottetown Conference led to the process of Canadian Confederation which formed the larger Dominion of Canada instead.
The Maritimes are home to Mi'kmaq, Maliseet and Passamaquoddy people and have an extensive history of French and British settlement dating back to the seventeenth century, forming a unique culture that predates Canada.
Name.
The word maritime is an adjective that simply means "of the sea", thus any land associated with the sea can be considered a maritime state or province (e.g. All the provinces of Canada except Alberta and Saskatchewan border water). The term "Maritimes" has historically been collectively applied to New Brunswick, Nova Scotia and Prince Edward Island.
History.
Following the northerly retreat of glaciers at the end of the Wisconsin glaciation over ten thousand years ago, human settlement by First Nations began in the Maritimes with Paleo-Indians during the "Early Period", ending around six thousand years ago.
The "Middle Period", starting six thousand years ago, and ending three thousand years ago, was dominated by rising sea levels from the melting glaciers in polar regions. This is also when what is called the "Laurentian tradition" started among Archaic Indians, existing First Nations peoples of the time. Evidence of Archaic Indian burial mounds and other ceremonial sites existing in the St. John River valley has been uncovered.
The "Late Period" extended from three thousand years ago until first contact with European settlers and was dominated by the organization of First Nations peoples into the Algonquian-influenced Abenaki Nation which existed largely in present-day interior Vermont, New Hampshire, and Maine, and the Mi'kmaq Nation which inhabited all of Nova Scotia, Prince Edward Island, eastern New Brunswick and the southern Gaspé. The primarily agrarian Maliseet Nation settled throughout the St. John River and Allagash River valleys of present-day New Brunswick and Maine. The Passamaquoddy Nation inhabited the northwestern coastal regions of the present-day Bay of Fundy. The Mi'kmaq Nation is also assumed to have crossed the present-day Cabot Strait at around this time to settle on the south coast of Newfoundland but were in a minority position compared to the Beothuk Nation.
European contact.
The Maritimes were the second area in Canada to be settled by Europeans, after Newfoundland. There is evidence that Viking explorers discovered and settled in the Vinland region around 1000 AD, which is when the L'Anse aux Meadows settlement in Newfoundland and Labrador has been dated, and it is possible that further exploration was made into the present-day Maritimes and northeastern United States.
Both Giovanni Caboto (John Cabot) and Giovanni da Verrazzano are reported to have sailed in or near Maritime waters during their voyages of discovery for England and France respectively. Several Portuguese explorers/cartographers have also documented various parts of the Maritimes, namely Diogo Homem. However, it was French explorer Jacques Cartier who made the first detailed reconnaissance of the region for a European power, and in so doing, claimed the region for the King of France. Cartier was followed by nobleman Pierre Dugua, Sieur de Monts who was accompanied by explorer/cartographer Samuel de Champlain in a 1604 expedition where they established the second permanent European settlement in North America, following Spain's settlement at St. Augustine. Champlain's settlement at Saint Croix Island, later moved to Port-Royal, survived where the ill-fated English settlement at Roanoke did not, and pre-dated the more successful English settlement at Jamestown by three years. Champlain went on to greater fame as the founder of New France's province of Canada which comprises much of the present-day lower St. Lawrence River valley in the province of Quebec.
Acadia.
Champlain's success in the region, which came to be called "Acadie", led to the fertile tidal marshes surrounding the southeastern and northeastern reaches of the Bay of Fundy being populated by French immigrants who called themselves "Acadien". Acadians eventually built small settlements throughout what is today mainland Nova Scotia and New Brunswick, as well as Île-Saint-Jean (Prince Edward Island), Île-Royale (Cape Breton Island), and other shorelines of the Gulf of St. Lawrence in present-day Newfoundland and Labrador, and Quebec. Acadian settlements had primarily agrarian economies, although there were many early examples of Acadian fishing settlements in southwestern Nova Scotia and in Île-Royale, as well as along the south and west coasts of Newfoundland, the Gaspé Peninsula, and the present-day Côte-Nord region of Quebec. Most Acadian fishing activities were overshadowed by the comparatively enormous seasonal European fishing fleets based out of Newfoundland which took advantage of proximity to the Grand Banks.
The growing English colonies along the American seaboard to the south and various European wars between England and France during the 17th and 18th centuries brought Acadia to the centre of world-scale geopolitical forces. In 1613, Virginian raiders captured Port Royale, and in 1621 Acadia was ceded to Scotland's Sir William Alexander who renamed it "Nova Scotia". By 1632, Acadia was returned from Scotland to France under the "Treaty of Saint-Germain-en-Laye", and the Port Royale settlement was moved to the site of nearby present-day Annapolis Royal. More French settlers, primarily from the Vienne, Normandie, and Brittany regions of France, continued to populate the colony of Acadia during the latter part of the 17th and early part of the 18th centuries. Important settlements also began in the Beaubassin region of the present-day Isthmus of Chignecto, and in the St. John River valley, and settlers began to establish communities on Île-Saint-Jean and Île-Royale as well.
In 1654, New England raiders attacked Acadian settlements on the Annapolis Basin, starting a period of uncertainty for Acadians throughout the English constitutional crises under Oliver Cromwell, and only being properly resolved under the Treaty of Breda in 1667 when France's claim to the region was reaffirmed. Colonial administration by France throughout the history of Acadia was contemptuous at best. France's priorities were in settling and strengthening its claim on New France and the exploration and settlement of interior North America and the Mississippi River valley.
Colonial Wars.
Over seventy-four years (1689-1763) there were six colonial wars, which involved continuous warfare between New England and Acadia (see the French and Indian Wars as well as Father Rale's War and Father Le Loutre's War). Throughout these wars, New England was allied with the Iroquois Confederacy and Acadia was allied with the Wabanaki Confederacy. In the first war, King William's War, natives from the Maritime region participated in numerous attacks with the French on the Acadia/ New England border in southern Maine (e.g., Raid on Salmon Falls). New England retaliatory raids on Acadia, such as the Raid on Chignecto (1696), were conducted by Benjamin Church. In the second war, Queen Anne's War, the British conducted the Conquest of Acadia, while the region remained primarily in control of Maliseet militia, Acadia militia and Mi'kmaq militia. 
In 1719, to further protect strategic interests in the Gulf of St. Lawrence and St. Lawrence River, France began the 20-year construction of a large fortress at Louisbourg on Île-Royale. Massachusetts was increasingly concerned over reports of the capabilities of this fortress, and of privateers staging out of its harbour to raid New England fishermen on the Grand Banks. In the forth war, King George's War, the British engaged successfully in the Siege of Louisbourg (1745). The British returned control of Île-Royale to France with the fortress virtually intact three years later under the Treaty of Aix-la-Chapelle and the French reestablished their forces there. 
In 1749, to counter the rising threat of Louisbourg, Halifax was founded and the Royal Navy established a major naval base and citadel. The founding of Halifax sparked Father Le Loutre's War.
During the sixth and final colonial war, the French and Indian War, the military conflicts in Nova Scotia continued. The British Conquest of Acadia happened in 1710. Over the next forty-five years the Acadians refused to sign an unconditional oath of allegiance to Britain. During this time period Acadians participated in various militia operations against the British and maintained vital supply lines to the French Fortress of Louisbourg and Fort Beausejour. The British sought to neutralize any military threat Acadians posed and to interrupt the vital supply lines Acadians provided to Louisbourg by deporting Acadians from Acadia.
The British began the Expulsion of the Acadians with the Bay of Fundy Campaign (1755). Over the next nine years over 12,000 Acadians were removed from Nova Scotia.
In 1758, the fortress of Louisbourg was laid siege for a second time within 15 years, this time by more than 27,000 British soldiers and sailors with over 150 warships. After the French surrender, Louisbourg was thoroughly destroyed by British engineers to ensure it would never be reclaimed. With the fall of Louisbourg, French and Mi'kmaq resistance in the region crumbled. British forces seized remaining French control over Acadia in the coming months, with Île-Saint-Jean falling in 1759 to British forces on their way to Quebec City for the Siege of Quebec and ensuing Battle of the Plains of Abraham.
The war ended and Britain had gained control over the entire Maritime region.
American Revolution.
Following the Seven Years' War, empty Acadian lands were settled first by New England Planters and then by immigrants brought from Yorkshire. Île-Royale was renamed Cape Breton Island and incorporated into the Colony of Nova Scotia.
Both the colonies of Nova Scotia (present-day Nova Scotia and New Brunswick) and St. John's Island (Prince Edward Island) were affected by the American Revolutionary War, largely by privateering against American shipping, but several coastal communities were also the targets of American raiders. Charlottetown, the capital of the new colony of St. John's Island, was ransacked in 1775 with the provincial secretary kidnapped and the Great Seal stolen. The largest military action in the Maritimes during the revolutionary war was the attack on Fort Cumberland (the renamed Fort Beausejour) in 1776 by a force of American sympathizers led by Jonathan Eddy. The fort was partially overrun after a month-long siege, but the attackers were ultimately repelled after the arrival of British reinforcements from Halifax.
The most significant impact from this war was the settling of large numbers of Loyalist refugees in the region, especially in Shelburne and Parrtown (Saint John). Following the Treaty of Paris in 1783, Loyalist settlers in what would become New Brunswick persuaded British administrators to split the Colony of Nova Scotia to create the new colony of New Brunswick in 1784. At the same time, another part of the Colony of Nova Scotia, Cape Breton Island, was split off to become the Colony of Cape Breton Island.
The Colony of St. John's Island was renamed to Prince Edward Island on November 29, 1798.
The War of 1812 had some effect on the shipping industry in the Maritime colonies of New Brunswick, Nova Scotia, Prince Edward Island, and Cape Breton Island; however, the significant Royal Navy presence in Halifax and other ports in the region prevented any serious attempts by American raiders. Maritime and American privateers targeted unprotected shipping of both the United States and Britain respectively, further reducing trade. The American border with New Brunswick did not have any significant action during this conflict, although British forces did occupy a portion of coastal Maine at one point. The most significant incident from this war which occurred in the Maritimes was the British capture and detention of the American frigate USS "Chesapeake" in Halifax.
19th century.
In 1820, the Colony of Cape Breton Island was merged back into the Colony of Nova Scotia for the second time by the British government.
British settlement of the Maritimes, as the colonies of Nova Scotia, New Brunswick and Prince Edward Island came to be known, accelerated throughout the late 18th century and into the 19th century with significant immigration to the region as a result of Scottish migrants displaced by the Highland Clearances and Irish escaping the Great Irish Famine (1845-1849). As a result, significant portions of the three provinces are influenced by Celtic heritages, with Scottish Gaelic (and to a lesser degree, Irish Gaelic) having been widely spoken, particularly in Cape Breton, although it is less prevalent today.
During the American Civil War, a significant number of Maritimers volunteered to fight for the armies of the Union, while a small handful joined the Confederate Army. However, the majority of the conflict's impact was felt in the shipping industry. Maritime shipping boomed during the war due to large-scale Northern imports of war supplies which were often carried by Maritime ships as Union ships were vulnerable to Confederate naval raiders. Diplomatic tensions between Britain and the Unionist North had deteriorated after some interests in Britain expressed support for the secessionist Confederate South. The Union navy, although much smaller than the British Royal Navy and no threat to the Maritimes, did posture off Maritime coasts at times chasing Confederate naval ships which sought repairs and reprovisioning in Maritime ports, especially Halifax.
The immense size of the Union army (the largest on the planet toward the end of the Civil War), however, was viewed with increasing concern by Maritimers throughout the early 1860s. Another concern was the rising threat of Fenian raids on border communities in New Brunswick by those seeking to end British rule of Ireland. This combination of events, coupled with an ongoing decline in British military and economic support to the region as the Home Office favoured newer colonial endeavours in Africa and elsewhere, led to a call among Maritime politicians for a conference on Maritime Union, to be held in early September 1864 in Charlottetown – chosen in part because of Prince Edward Island's reluctance to give up its jurisdictional sovereignty in favour of uniting with New Brunswick and Nova Scotia into a single colony. New Brunswick and Nova Scotia felt that if the union conference were held in Charlottetown, they might be able to convince Island politicians to support the proposal.
The Charlottetown Conference, as it came to be called, was also attended by a slew of visiting delegates from the neighbouring colony of Canada, who had largely arrived at their own invitation with their own agenda. This agenda saw the conference dominated by discussions of creating an even larger union of the entire territory of British North America into a united colony. The Charlottetown Conference ended with an agreement to meet the following month in Quebec City, where more formal discussions ensued, culminating with meetings in London and the signing of the British North America Act. Of the Maritime provinces, only Nova Scotia and New Brunswick were initially party to the BNA Act, Prince Edward Island's reluctance, combined with a booming agricultural and fishing export economy having led to that colony opting not to sign on.
Major population centres.
The major communities of the region include Halifax and Cape Breton in Nova Scotia, Saint John, Fredericton and Moncton in New Brunswick, and Charlottetown in Prince Edward Island.
Society and culture.
Maritime society is based upon a mixture of traditions and class backgrounds. Predominantly rural until recent decades, the region traces many of its cultural activities to those rural resource-based economies of fishing, agriculture, forestry, and coal mining.
While Maritimers are predominantly of west European heritage (Scottish, Irish, English, and Acadian), immigration to Industrial Cape Breton during the heyday of coal mining and steel manufacturing brought people from eastern Europe as well as from Newfoundland. The Maritimes also have a black population who are mostly descendants of African American loyalists or refugees from the War of 1812, largely concentrated in Nova Scotia but also in various communities throughout southern New Brunswick, Cape Breton (where the black population is largely of West Indian descent), and Prince Edward Island. The Mi'kmaq Nation's reserves throughout Nova Scotia, Prince Edward Island and eastern New Brunswick dominate aboriginal culture in the region, compared to the much smaller population of the Maliseet Nation in western New Brunswick.
Cultural activities are fairly diverse throughout the region, with the music, dance, theatre, and literary art forms tending to follow the particular cultural heritage of specific locales. Notable Nova Scotian folklorist and cultural historian Helen Creighton spent the majority of her lifetime recording the various Celtic musical and folk traditions of rural Nova Scotia during the mid-20th century, prior to this knowledge being wiped out by mass media assimilation with the rest of North America. A fragment of Gaelic culture remains in Nova Scotia but primarily on Cape Breton Island.
Canada has witnessed a "Celtic revival" in which many Maritime musicians and songs have risen to prominence in recent decades. Some companies, particularly breweries such as Alexander Keith's and Moosehead have played up a connection between folklore with alcohol consumption during their marketing campaigns. The Maritimes were among the strongest supporters of prohibition (Prince Edward Island lasting until 1949), and some predominantly rural communities maintain "dry" status, banning the retail sale of alcohol as a vestige of the original temperance movement in the region.
Economy.
Present status.
Given the small population of the region (compared with the Central Canadian provinces or the New England states), the regional economy is a net exporter of natural resources, manufactured goods, and services. The regional economy has long been tied to natural resources such as fishing, logging, farming, and mining activities. Significant industrialisation in second half of the 19th century brought steel to Trenton, Nova Scotia, and subsequent creation of a widespread industrial base to take advantage of the region's large underground coal deposits. After Confederation, however, this industrial base withered with technological change, and trading links to Europe and the U.S. were reduced in favour of those with Ontario and Quebec. In recent years, however, the Maritime regional economy has begun increased contributions from manufacturing again and the steady transition to a service economy.
Important manufacturing centres in the region include Pictou County, Truro, the Annapolis Valley and the South Shore, and the Strait of Canso area in Nova Scotia, as well as Summerside in Prince Edward Island, and the Miramichi area, the North Shore and the upper Saint John River valley of New Brunswick.
Some predominantly coastal areas have become major tourist centres, such as parts of Prince Edward Island, Cape Breton Island, the South Shore of Nova Scotia and the Gulf of St. Lawrence and Bay of Fundy coasts of New Brunswick. Additional service-related industries in information technology, pharmaceuticals, insurance and financial sectors—as well as research-related spin-offs from the region's numerous universities and colleges—are significant economic contributors.
Another important contribution to Nova Scotia's provincial economy is through spin-offs and royalties relating to off-shore petroleum exploration and development. Mostly concentrated on the continental shelf of the province's Atlantic coast in the vicinity of Sable Island, exploration activities began in the 1960s and resulted in the first commercial production field for oil beginning in the 1980s. Natural gas was also discovered in the 1980s during exploration work, and this is being commercially recovered, beginning in the late 1990s. Initial optimism in Nova Scotia about the potential of off-shore resources appears to have diminished with the lack of new discoveries, although exploration work continues and is moving farther off-shore into waters on the continental margin.
Regional transportation networks have also changed significantly in recent decades with port modernizations, with new expressways and ongoing arterial highway construction, the abandonment of various low-capacity railway branchlines (including the entire railway system of Prince Edward Island and southwestern Nova Scotia), and the construction of the Canso Causeway and the Confederation Bridge. There have been airport improvements at various centres providing improved connections to markets and destinations in the rest of North America and overseas.
Improvements in infrastructure and the regional economy notwithstanding, the three provinces remain one of the poorer regions of Canada. While urban areas are growing and thriving, economic adjustments have been harsh in rural and resource-dependent communities, and emigration has been an ongoing phenomenon for some parts of the region. Another problem is seen in the lower average wages and family incomes within the region. Property values are depressed, resulting in a smaller tax base for these three provinces, particularly when compared with the national average which benefits from central and western Canadian economic growth.
This has been particularly problematic with the growth of the welfare state in Canada since the 1950s, resulting in the need to draw upon equalization payments to provide nationally mandated social services. Since the 1990s the region has experienced an exceptionally tumultuous period in its regional economy with the collapse of large portions of the ground fishery throughout Atlantic Canada, the closing of coal mines and a steel mill on Cape Breton Island, and the closure of military bases in all three provinces.
Historical.
Growth.
While the economic underperformance of the Maritime economy has been long lasting, it has not always been present. The mid-19th century, especially the 1850s and 1860s, has long been seen as a "Golden Age" in the Maritimes. Growth was strong, and the region had one of British North America's most extensive manufacturing sectors as well as a large international shipping industry. The question of why the Maritimes fell from being a centre of Canadian manufacturing to being an economic hinterland is thus a central one to the study of the region's pecuniary difficulties. The period in which the decline occurred had a great many potential culprits. In 1867 Nova Scotia and New Brunswick merged with the Canadas in Confederation, with Prince Edward Island joining them six years later in 1873. Canada was formed only a year after free trade with the United States (in the form of the Reciprocity Agreement) had ended. In the 1870s John A. Macdonald's National Policy was implemented, creating a system of protective tariffs around the new nation. Throughout the period there was also significant technological change both in the production and transportation of goods.
Was there a Golden Age?
Several scholars have explored the so-called "golden age" of the Maritimes in the years just before Confederation. In Nova Scotia, the population grew steadily from 277,000 in 1851 to 388,000 in 1871, mostly from natural increase since immigration was slight. The era has been called a golden age, but that was a myth created in the 1930s to lure tourists to a romantic era of tall ships and antiques. Recent historians using census data have shown that is a fallacy. In 1851-1871 there was an overall increase in per capita wealth holding. However most of the gains went to the urban elite class, especially businessmen and financiers living in Halifax. The wealth held by the top 10% rose considerably over the two decades, but there was little improvement in the wealth levels in rural areas, which comprised the great majority of the population. Likewise Gwyn reports that gentlemen, merchants, bankers, colliery owners, shipowners, shipbuilders, and master mariners flourished. However the great majority of families were headed by farmers, fishermen, craftsmen and laborers. Most of them—and many widows as well—lived in poverty. Out migration became an increasingly necessary option. Thus the era was indeed a golden age but only for a small but powerful and highly visible elite.
Decline.
The cause of economic malaise in the Maritimes is an issue of great debate and controversy among historians, economists, and geographers. The differing opinions can approximately be divided into the "structuralists," who argue that poor policy decisions are to blame, and the others, who argue that unavoidable technological and geographical factors caused the decline.
The exact date that the Maritimes began to fall behind the rest of Canada is difficult to determine. Historian Kris Inwood places the date very early, at least in Nova Scotia, finding clear signs that the Maritimes "Golden Age" of the mid-nineteenth century was over by 1870, before Confederation or the National Policy could have had any significant impact. Richard Caves places the date closer to 1885. T.W. Acheson takes a similar view and provides considerable evidence that the early 1880s were in fact a booming period in Nova Scotia and this growth was only undermined towards the end of that decade. David Alexander argues that any earlier declines were simply part of the global Long Depression, and that the Maritimes first fell behind the rest of Canada when the great boom period of the early twentieth century had little effect on the region. E.R. Forbes, however, emphasizes that the precipitous decline did not occur until after the First World War during the 1920s when new railway policies were implemented. Forbes also contends that significant Canadian defence spending during the Second World War favoured powerful political interests in Central Canada such as C.D. Howe, when major Maritime shipyards and factories, as well as Canada's largest steel mill, located in Cape Breton Island, fared poorly.
One of the most important changes, and one that almost certainly had an effect, was the revolution in transportation that occurred at this time. The Maritimes were connected to central Canada by the Intercolonial Railway in the 1870s, removing a longstanding barrier to trade. For the first time this placed the Maritime manufacturers in direct competition with those of Central Canada. Maritime trading patterns shifted considerably from mainly trading with New England, Britain, and the Caribbean, to being focused on commerce with the Canadian interior, enforced by the federal government's tariff policies.
Simultaneous with the construction of railways in the region, the age of the wooden sailing ship began to come to an end, being replaced by larger and faster steel steam ships. The Maritimes had long been a centre for shipbuilding, and this industry was hurt by the change. The larger ships were also less likely to call on the smaller population centres such as Saint John and Halifax, preferring to travel to cities like New York and Montreal. Even the Cunard Line, founded by Haligonian Samuel Cunard, stopped making more than a single ceremonial voyage to Halifax each year.
More controversial than the role of technology is the argument over the role of politics in the origins of the region's decline. Confederation and the tariff and railway freight policies that followed have often been blamed for having a deleterious effect on the Maritime economies. Arguments have been made that the Maritimes' poverty was caused by control over policy by Central Canada which used the national structures for its own enrichment. This was the central view of the Maritime Rights Movement of the 1920s, which advocated greater local control over the region's finances. T.W. Acheson is one of the main proponents of this theory. He notes the growth that was occurring during the early years of the National Policy in Nova Scotia demonstrates how the effects of railway fares and the tariff structure helped undermine this growth. Capitalists from Central Canada purchased the factories and industries of the Maritimes from their bankrupt local owners and proceeded to close down many of them, consolidating the industry in Central Canada.
The policies in the early years of Confederation were designed by Central Canadian interests, and they reflected the needs of that region. The unified Canadian market and the introduction of railroads created a relative weakness in the Maritime economies. Central to this concept, according to Acheson, was the lack of metropolises in the Maritimes.
Montreal and Toronto were well suited to benefit from the development of large-scale manufacturing and extensive railway systems in Quebec and Ontario, these being the goals of the Macdonald and Laurier governments. In the Maritimes the situation was very different. Today New Brunswick has several mid-sized centres in Saint John, Moncton, and Fredericton but no significant population centre. Nova Scotia has a growing metropolitan area surrounding Halifax, but a contracting population in industrial Cape Breton, and several smaller centres in Bridgewater, Kentville, Yarmouth, and Pictou County. Prince Edward Island's only significant population centres are in Charlottetown and Summerside. During the late 19th and early 20th centuries, just the opposite was the case with little to no population concentration in major industrial centres as the predominantly rural resource-dependent Maritime economy continued on the same path as it had since European settlement on the region's shores.
Despite the region's absence of economic growth on the same scale as other parts of the nation, the Maritimes has changed markedly throughout the 20th century, partly as a result of global and national economic trends, and partly as a result of government intervention. Each sub-region within the Maritimes has developed over time to exploit different resources and expertise. Saint John became a centre of the timber trade and shipbuilding and is currently a centre for oil refining and some manufacturing. The northern New Brunswick communities of Edmundston, Campbellton, Dalhousie, Bathurst, and Miramichi are focused on the pulp and paper industry and some mining activity. Moncton was a centre for railways and has changed its focus to becoming a multi-modal transportation centre with associated manufacturing and retail interests. The Halifax metropolitan area has come to dominate peninsular Nova Scotia as a retail and service centre, but that province's industries were spread out from the coal and steel industries of industrial Cape Breton and Pictou counties, the mixed farming of the North Shore and Annapolis Valley, and the fishing industry was primarily focused on the South Shore and Eastern Shore. Prince Edward Island is largely dominated by farming, fishing, and tourism.
Given the geographic diversity of the various sub-regions within the Maritimes, policies to centralize the population and economy were not initially successful, thus Maritime factories closed while those in Ontario and Quebec prospered.
The traditional staples thesis, advocated by scholars such as S.A. Saunders, looks at the resource endowments of the Maritimes and argues that it was the decline of the traditional industries of shipbuilding and fishing that led to Maritime poverty, since these processes were rooted in geography, and thus all but inevitable. Kris Inwood has revived the staples approach and looks at a number of geographic weaknesses relative to Central Canada. He repeats Acheson's argument that the region lacks major urban centres, but adds that the Maritimes were also lacking the great rivers that led to the cheap and abundant hydro-electric power, key to Quebec and Ontario's urban and manufacturing development, that the extraction costs of Maritime resources were higher (particularly in the case of Cape Breton coal), and that the soils of the region were poorer and thus the agricultural sector weaker.
The Maritimes are the only provinces in Canada which entered Confederation in the 19th century and have kept their original colonial boundaries. All three provinces have the smallest land base in the country and have been forced to make do with resources within. By comparison, the former colony of the United Province of Canada (divided into the District of Canada East, and the District of Canada West) and the western provinces were dozens of times larger and in some cases were expanded to take in territory formerly held in British Crown grants to companies such as the Hudson's Bay Company; in particular the November 19, 1869 sale of Rupert's Land to the Government of Canada under the "Rupert's Land Act 1868" was facilitated in part by Maritime taxpayers. The economic riches of energy and natural resources held within this larger land base were only realized by other provinces during the 20th century.
One comparison made with the wealthier areas of Canada is that of the region's political and/or work culture. Today few academics make such a claim, but it still a common explanation in other circles. Some writers have also alleged that Maritime business people were unwilling to take risks or invest in manufacturing, a thesis Acheson devotes much attention to debunking.
Industries.
The maritime provinces' main industry is fishing, fishing can be found in any maritime province. This includes fishing for lobster, mackerel, tuna, salmon and many more kinds of fish.
Nova Scotia.
Nova Scotia is very strong in agriculture, forestry and fishing.
Prince Edward Island.
Tourism is a big part of PEI besides the fishing. Anne of Green Gables was written there so every year, it attracts families to see where the beloved story was based but also enjoy beaches and sunlight.
PEI is also known for its agriculture.
New Brunswick.
Agriculture and forestry are mainly found in New Brunswick.
Politics.
Maritime conservatism since the Second World War has been very much part of the Red Tory tradition, key influences being former Premier of Nova Scotia and federal Progressive Conservative Party leader Robert Stanfield and New Brunswick Tory strategist Dalton Camp.
In recent years, the social democratic New Democratic Party (NDP) has made significant inroads both federally and provincially in the region. The NDP has elected Members of Parliament (MPs) from New Brunswick, but most of the focus of the party at the federal and provincial levels is currently in the Halifax area of Nova Scotia. Industrial Cape Breton has historically been a region of labour activism, electing Co-operative Commonwealth Federation (and later NDP) MPs, and even produced many early members of the Communist Party of Canada in the pre-World War II era. In the 2004 federal election, the NDP captured 28.45% of the vote in Nova Scotia, more than any other province. In the 2009 provincial election the NDP formed a majority government, the first in the region.
The Maritimes are generally socially conservative but unlike Alberta, they also have fiscally socialist tendencies. It is because of the lack of support for fiscal conservatism that federal parties such as the Canadian Alliance never had much success in the region, and the level of support for the new Conservative Party of Canada in the region is uncertain. In the 2004 federal election, the Conservatives had one of the worst showings in the region for a right-wing party, going back to Confederation, with the possible exception of the 1993 election.
An area within the region where both fiscal and social conservatism do coincide and where the federal Reform Party and Canadian Alliance have met success is in the central-western part of New Brunswick, in the St. John River valley north of Saint John and south of Grand Falls. Contributing demographics include a predominantly Anglophone population residing in a largely rural agrarian setting. One influence might be proximity to the International Boundary and the state of Maine. The valley is also settled by descendants of United Empire Loyalists, some of whom established fundamentalist Christian congregations in the area which continue to influence certain segments of society. There are also a large number of active and retired military personnel located in the Fredericton and Oromocto area as a result of the large military base at CFB Gagetown. Another area in the region with smatterings of coinciding fiscal and social conservatism is the Annapolis Valley of Nova Scotia.
The Liberal Party of Canada has done well in the Maritimes in the past because of its interventionist policies. The Acadian Peninsula region of New Brunswick, long dependent upon seasonal employment in the Gulf of St. Lawrence fishery, tends to vote for the Liberals or NDP for this reason. In the 1997 federal election, Prime Minister Jean Chrétien's Liberals endured a bitter defeat to the PCs and NDP in many ridings as a result of unpopular cuts to unemployment benefits for seasonal workers, as well as closures of several Canadian Forces Bases, the refusal to honour a promise to rescind the Goods and Services Tax, cutbacks to provincial equalization payments, health care, post-secondary education and regional transportation infrastructure such as airports, fishing harbours, seaports, and railways. The Liberals held onto seats in Prince Edward Island and New Brunswick, while being shut out of Nova Scotia entirely, the second time in history (the only other time being the Diefenbaker sweep).
The Maritimes is currently represented in the Canadian Parliament by 25 Members of the House of Commons (Nova Scotia – 11, New Brunswick – 10, Prince Edward Island – 4) and 24 Senators (Nova Scotia and New Brunswick – 10 each, Prince Edward Island – 4). This level of representation was established at the time of Confederation when the Maritimes had a much larger proportion of the national population. The comparatively large population growth of western and central Canada during the immigration boom of the 20th century has reduced the Maritimes' proportion of the national population to less than 10%, resulting in an over-representation in Parliament, with some federal ridings having fewer than 35,000 people, compared to central and western Canada where ridings typically contain 100,000-120,000 people.
The Senate of Canada is structured along regional lines, giving an equal number of seats (24) to the Maritimes, Ontario, Quebec, and western Canada, in addition to the later entry of Newfoundland and Labrador, as well as the three territories. Enshrined in the Constitution, this model was developed to ensure that no area of the country is able to exert undue influence in the Senate. The Maritimes, with its much smaller proportion of the national population (compared to the time of Confederation) also have an over-representation in the Senate, particularly compared to the population growth of Ontario and the western provinces. This has led to calls to reform the Senate; however, such a move would entail constitutional changes.
Another factor related to the number of Senate seats is that a constitutional amendment in the early 20th century mandated that no province can have fewer Members of Parliament than it has senators. This court decision resulted from a complaint by the Government of Prince Edward Island after that province's number of MPs was proposed to change from 4 to 3, accounting for its declining proportion of the national population at that time. When PEI entered Confederation in 1873, it was accorded 6 MPs and 4 Senators; however this was reduced to 4 MPs by the early twentieth century. Senators being appointed for life at this time, these coveted seats rarely went unfilled for a long period of time anywhere in Canada. As a result, PEI's challenge was accepted by the federal government, and its level of federal representation was secured. In the aftermath of the 1989 budget, which saw a fillibuster by Liberal Senators in attempt to kill legislation creating the Goods and Services Tax, Prime Minister Brian Mulroney "stacked" the Senate by creating additional seats in several provinces across Canada, including New Brunswick; however, there was no attempt by these provinces to increase the number of MPs to reflect this change in Senate representation.

</doc>
<doc id="7383" url="http://en.wikipedia.org/wiki?curid=7383" title="Cyril of Alexandria">
Cyril of Alexandria

Cyril of Alexandria (; c. 376 – 444) was the Patriarch of Alexandria from 412 to 444. He was enthroned when the city was at the height of its influence and power within the Roman Empire. Cyril wrote extensively and was a leading protagonist in the Christological controversies of the later 4th and 5th centuries. He was a central figure in the First Council of Ephesus in 431, which led to the deposition of Nestorius as Patriarch of Constantinople.
Cyril is counted among the Church Fathers and the Doctors of the Church, and his reputation within the Christian world has resulted in his titles "Pillar of Faith" and "Seal of all the Fathers", but Theodosius II, the Roman Emperor, condemned him for behaving like a "proud pharaoh", and the Nestorian bishops at the Council of Ephesus declared him a heretic, labelling him as a "monster, born and educated for the destruction of the church."
Cyril is well-known due to his dispute with Nestorius and his supporter Patriarch John of Antioch, whom Cyril excluded from the Council of Ephesus for arriving late. He is also known for his involvement in the expulsion of Novatians and Jews from Alexandria and the murder of the Hellenistic philosopher Hypatia by Coptic monks. Historians disagree over the extent of his responsibility for these events.
The Roman Catholic Church did not commemorate Saint Cyril in the Tridentine Calendar: it added his feast only in 1882, assigning to it the date of 9 February. The 1969 revision moved it to 27 June, considered to be the day of the saint's death, as celebrated by the Coptic Orthodox Church. The same date has been chosen for the Lutheran calendar. The Eastern Orthodox Church and Eastern Catholic Church celebrate his feast day on 9 June and also, together with Pope Athanasius I of Alexandria, on 18 January.
Early life.
Little is known for certain of Cyril's early life. He was born c. 376, in the small town of Theodosios, Egypt, near modern day El-Mahalla El-Kubra. A few years after his birth, his maternal uncle Theophilus rose to the powerful position of Patriarch of Alexandria. His mother remained close to her brother and under his guidance, Cyril was well educated. His writings show his knowledge of Christian writers of his day, including Eusebius, Origen, Didymus the Blind, and writers of the Church of Alexandria. He received the formal Christian education standard for his day: he studied grammar from age twelve to fourteen (390-392), rhetoric and humanities from fifteen to twenty (393-397) and finally theology and biblical studies (398-402). In 403 he accompanied his uncle to attend a synod in Constantinople.
Patriarch of Alexandria.
Theophilus died on 15 October 412, and Cyril was made Pope or Patriarch of Alexandria on 18 October 412, against the party favouring Archdeacon Timothy.
Relationship with the Novatians and Jews.
Thus, Cyril followed his uncle in a position that had become powerful and influential, rivalling that of the prefect in a time of turmoil and frequently violent conflict between the cosmopolitan city's Pagan, Jewish, and Christian inhabitants.
He began to exert his authority by causing the churches of the Novatians to be closed and their sacred vessels to be seized.
Orestes, "Praefectus augustalis" of the Diocese of Egypt, steadfastly resisted Cyril's agenda of ecclesiastical encroachment onto secular prerogatives. On one occasion, Cyril sent the "grammaticus" Hierax to secretly discover the content of an edict that Orestes was to promulgate on the mimes shows, which attracted great crowds. When the Jews, with whom Cyril had clashed before, discovered the presence of Hierax, they rioted, complaining that Hierax's presence was aimed at provoking them. Then Orestes had Hierax tortured in public in a theatre. This order had two aims: the first was to quell the riot, the other to mark Orestes' authority over Cyril.
According to Socrates Scholasticus, upon hearing of Hierex's severe and public punishment, Cyril threatened to retaliate against the Jews of Alexandria with "the utmost severities" if the harassment of Christians did not cease immediately. In response to Cyril's threat, the Jews of Alexandria grew even more furious, eventually resorting to violence against the Christians. They plotted to flush the Christians out at night by running through the streets claiming that the Church of Alexander was on fire. When Christians responded to what they were led to believe was the burning down of their church, "the Jews immediately fell upon and slew them" by using rings to recognize one another in the dark and killing everyone else in sight. When the morning came, the Jews of Alexandria could not hide their guilt, and Cyril, along with many of his followers, took to the city’s synagogues in search of the perpetrators of the massacre.
After Cyril rounded up all the Jews in Alexandria, he ordered them to be stripped of all possessions, banished them from Alexandria, and allowed their goods to be pillaged by the remaining citizens of Alexandria. With Cyril's banishment of the Jews, "Orestes [...] was filled with great indignation at these transactions, and was excessively grieved that a city of such magnitude should have been suddenly bereft of so large a portion of its population." Because of this, the feud between Cyril and Orestes intensified, and both men wrote to the emperor regarding the situation. Eventually, Cyril attempted to reach out to Orestes through several peace overtures, including attempted mediation and, when that failed, showed him the Gospels, which would mean that the religious authority of Cyril would require Orestes' acquiescence in the bishop's policy. Nevertheless, Orestes remained unmoved by such gestures.
This refusal almost cost Orestes his life. Nitrian monks came from the desert and instigated a riot against Orestes among the population of Alexandria. These monks' violence had already been used, 15 years before, by Theophilus (Cyril's uncle) against the "Tall Brothers"; furthermore, it is said that Cyril had spent five years among them in ascetic training. The monks assaulted Orestes and accused him of being a pagan. Orestes rejected the accusations, showing that he had been baptised by the Archbishop of Constantinople. However, the monks were not satisfied, and one of them, Ammonius, threw a stone and hit Orestes in the head, and so much blood flowed out that he was covered in it. Orestes' guard, fearing to be stoned by the monks, fled leaving Orestes alone. The people of Alexandria, however, came to his help, captured Ammonius and put the monks to flight. Orestes was cured and put Ammonius under torture in a public place, killing him. The prefect then wrote to the emperor Theodosius II, telling him of the events. Cyril also wrote to the Emperor, telling his version of the events. The bishop also seized the body of Ammonius and put it in a church, conferring upon him the title of "Thaumasius" and putting his name in the list of the martyrs. However, the Christian population of Alexandria knew that Ammonius had been killed for his assault and not for his faith, and Cyril was obliged to remain silent about the events.
Murder of Hypatia.
Prefect Orestes enjoyed the political backing of Hypatia, an astronomer, philosopher and mathematician who had considerable moral authority in the city of Alexandria, and who had extensive influence. Indeed many students from wealthy and influential families came to Alexandria purposely to study privately with Hypatia, and many of these later attained high posts in government and the Church. Several Christians thought that Hypatia's influence had caused Orestes to reject all reconciliatory offerings by Cyril. Modern historians think that Orestes had cultivated his relationship with Hypatia to strengthen a bond with the Pagan community of Alexandria, as he had done with the Jewish one, to handle better the difficult political life of the Egyptian capital. A Christian mob, however, possibly led by parabalani, took Hypatia from her chariot and brutally murdered her, hacking her body apart and burning the pieces outside the city walls.
Modern studies represent Hypatia's death as the result of a struggle between two Christian factions, the moderate Orestes, supported by Hypatia, and the more rigid Cyril. According to lexicographer William Smith, "She was accused of too much familiarity with Orestes, prefect of Alexandria, and the charge spread among the clergy, who took up the notion that she interrupted the friendship of Orestes with their archbishop, Cyril."
Conflict with Nestorius.
Another major conflict was between the Alexandrian and Antiochian schools of ecclesiastical reflection, piety, and discourse. This long running conflict widened with the third canon of the First Council of Constantinople which granted the see of Constantinople primacy over the older sees of Alexandria and Antioch. Thus, the struggle between the sees of Alexandria and Antioch now included Constantinople. The conflict came to a head in 428 after Nestorius, who originated in Antioch, was made Archbishop of Constantinople.
Cyril gained an opportunity to restore Alexandria's pre-eminence over both Antioch and Constantinople when an Antiochine priest who was in Constantinople at Nestorius' behest began to preach against calling Mary the "Mother of God". As the term "Mother of God" had long been attached to Mary, the laity in Constantinople complained against the priest. Rather than repudiating the priest, Nestorius intervened on his behalf. Nestorius argued that Mary was neither a "Mother of Man" nor "Mother of God" as these referred to Christ's two natures; rather, Mary was the "Mother of Christ". Christ, according to Nestorius, was the conjunction of the Godhead with his "temple" (which Nestorius was fond of calling his human nature). The controversy seemed to be centered on the issue of the suffering of Christ. Cyril maintained that the Son of God or the divine Word, truly suffered "in the flesh." However, Nestorius claimed that the Son of God was altogether incapable of suffering, even within his union with the flesh. Eusebius of Dorylaeum went so far as to accuse Nestorius of adoptionism. By this time, news of the controversy in the capital had reached Alexandria. At Easter 429 A.D., Cyril wrote a letter to the Egyptian monks warning them of Nestorius' views. A copy of this letter reached Constantinople where Nestorius preached a sermon against it. This began a series of letters between Cyril and Nestorius which gradually became more strident in tone. Finally, Emperor Theodosius II convoked the Council of Ephesus (in 431) to solve the dispute. Cyril selected Ephesus as the venue since it supported the veneration of Mary. The council was convoked before Nestorius's supporters from Antioch and Syria had arrived and thus Nestorius refused to attend when summoned. Predictably, the Council ordered the deposition and exile of Nestorius for heresy.
However, when John of Antioch and the other pro-Nestorius bishops finally reached Ephesus, they assembled their own Council, condemned Cyril for heresy, deposed him from his see, and labelled him as a "monster, born and educated for the destruction of the church". Theodosius, by now old enough to hold power by himself, annulled the verdict of the Council and arrested Cyril, but Cyril eventually escaped. Having fled to Egypt, Cyril bribed Theodosius' courtiers, and sent a mob led by Dalmatius, a hermit, to besiege Theodosius' palace, and shout abuse; the Emperor eventually gave in, sending Nestorius into minor exile (Upper Egypt). 
Cyril died about 444, but the controversies were to continue for decades, from the "Robber Synod" of Ephesus (449) to the Council of Chalcedon (451) and beyond.
Theology.
Cyril regarded the embodiment of God in the person of Jesus Christ to be so mystically powerful that it spread out from the body of the God-man into the rest of the race, to reconstitute human nature into a graced and deified condition of the saints, one that promised immortality and transfiguration to believers. Nestorius, on the other hand, saw the incarnation as primarily a moral and ethical example to the faithful, to follow in the footsteps of Jesus. Cyril's constant stress was on the simple idea that it was God who walked the streets of Nazareth (hence Mary was Theotokos (God Bearer)), and God who had appeared in a transfigured humanity. Nestorius spoke of the distinct 'Jesus the man' and 'the divine Logos' in ways that Cyril thought were too dichotomous, widening the ontological gap between man and God in a way that some of his contemporaries believed would annihilate the person of Christ.
The main issue that prompted this dispute between Cyril and Nestorius was the question which arose at the Council of Constantinople: What exactly was the being to which Mary gave birth? Cyril posited that the composition of the Trinity consisted of one divine essence (ousia) in three distinct modes of being (hypostases.) These distinct modes of being were the Father, the Son and the Holy Spirit. Then, when the Son became flesh and entered into the world, these two divine and human natures both remained but became "united" in the person of Jesus. This resulted in the slogan "One Nature united out of two" being used to encapsulate the theological position of this Alexandrian bishop. 
According to Cyril's theology, there were two states for the Son: the state that existed "prior" to the Son (or Word/Logos) becoming enfleshed in the person of Jesus and the state that actually became enfleshed. Thus, only the Logos incarnate suffered and died on the Cross and therefore the Son was able to suffer without suffering. Cyril's concern was that there needed to be continuity of the divine subject between the Logos and the incarnate Word—and so in Jesus Christ the divine Logos was really present in the flesh and in the world.
Mariology.
Cyril of Alexandria became noted in Church history because of his spirited fight for the title "Theotokos" during the First Council of Ephesus (431).
His writings include the homily given in Ephesus and several other sermons. Some of his alleged homilies are in dispute as to his authorship. In several writings, Cyril focuses on the love of Jesus to his mother. On the Cross, he overcomes his pain and thinks of his mother. At the wedding in Cana, he bows to her wishes. Cyril is credited with creating a basis for all other mariological developments through his teaching of the blessed Virgin Mary, as the Mother of God.
St. Cyril received an important recognition of his preachings by the Second Council of Constantinople (553 d.C.) which declared;
In modern culture.
Cyril plays a role in the Arabic novel "Azazel" by the Egyptian scholar Youssef Ziedan. The novel, which won the 2009 International Prize for Arabic Fiction is set in 5th-century Egypt and Syria and deals with the early history of Christianity. The book depicts religious fanaticism and mob violence among early Christians in Roman Egypt. The narrator, Hypa, witnesses the lynching of Hypatia and finds himself involved in the schism of 431, when Cyril deposed Nestorius. Cyril is portrayed as a fanatic who kills Jews and others who have not converted to Christianity from the traditional religions of antiquity. This portrayal angered many of Egypt's Coptic Christians.
Cyril has also been portrayed in Ki Longfellow's "Flow Down Like Silver, Hypatia of Alexandria". Though Longfellow does not accuse Cyril of ordering the death of Hypatia, her work does not shy away from speculating on his part in the murder.
In the 2009 film "Agora", Cyril is played by Sami Samir as an extremist who opposes Orestes's attempts to harmonize the different communities of Alexandria.
Works.
Cyril was a scholarly archbishop and a prolific writer. In the early years of his active life in the Church he wrote several exegetical documents. Among these were: "Commentaries on the Old Testament", "Thesaurus", "Discourse Against Arians", "Commentary on St. John's Gospel", and "Dialogues on the Trinity". In 429 as the Christological controversies increased, the output of his writings was so extensive that his opponents could not match it. His writings and his theology have remained central to the tradition of the Fathers and to all Orthodox to this day.
External links.
 

</doc>
<doc id="7387" url="http://en.wikipedia.org/wiki?curid=7387" title="Cyril of Jerusalem">
Cyril of Jerusalem

Cyril of Jerusalem (Greek Κύριλλος Α΄ Ἱεροσολύμων) was a distinguished theologian of the early Church (ca. 313 – 386). He is venerated as a saint by the Roman Catholic Church, the Eastern Orthodox Church, and the Anglican Communion. In 1883, Cyril was declared a Doctor of the Church by Pope Leo XIII. He is highly respected in the Palestinian Christian Community.
Life and character.
Little is known of his life before he became a bishop; the assignment of his birth to the year 315 rests on conjecture. According to Butler, Cyril was born at or near the city of Jerusalem, and was apparently well-read in both the Church fathers and the pagan philosophers.
Cyril was ordained a deacon by Bishop St. Macarius of Jerusalem in about 335 and a priest some eight years later by Bishop St. Maximus. About the end of 350 he succeeded St. Maximus in the See of Jerusalem.
Episcopacy.
Soon after his appointment, Cyril in his "Letter to Constantius" of 351 recorded the appearance of a cross of light in the sky above Golgotha, witnessed by the whole population of Jerusalem. The Greek church commemorates this miracle on the 7th of May. Though in modern times the authenticity of the "Letter" has been questioned, on the grounds that the word "homoousios" occurs in the final blessing, many scholars believe this may be a later interpolation, and accept the letter's authenticity on the grounds of other pieces of internal evidence.
Relations between Metropolitan Acacius of Caesarea and Cyril became strained. Acacius is presented as a leading Arian by the orthodox historians, and his opposition to Cyril in the 350s is attributed by these writers to this. Sozomen also suggests that the tension may have been increased by Acacius's jealousy of the importance assigned to St. Cyril's See by the Council of Nicaea, as well as by the threat posed to Caesarea by the rising influence of the see of Jerusalem as it developed into the prime Christian holy place and became a centre of pilgrimage.
Acacius charged Cyril with selling church property. The city of Jerusalem had suffered drastic food shortages at which point church historians Sozomen and Theodoret report “Cyril secretly sold sacramental ornaments of the church and a valuable holy robe, fashioned with gold thread that the emperor Constantine had once donated for the bishop to wear when he performed the rite of Baptism”. It was believed that Cyril sold some plate, ornaments and imperial gifts to keep his people from starving.
For two years, Cyril resisted Acacius' summons to account for his actions in selling off church property, but a council held under Acacius's influence in 357 deposed St. Cyril in his absence (having officially charged him with selling church property to help the poor) and forced him to retire to Tarsus. The following year, 359, in an atmospohere hostile to Acacius, the Council of Seleucia reinstated Cyril and deposed Acacius. In 360, though, this was reversed by Emperor Constantius, and Cyril suffered another year's exile from Jerusalem until the Emperor Julian's accession allowed him to return.
Cyril was once again banished from Jerusalem by the Arian Emperor Valens in 367. St. Cyril was able to return again at the accession of Emperor Gratian in 378, after which he remained undisturbed until his death in 386. St. Cyril's jurisdiction over Jerusalem was expressly confirmed by the First Council of Constantinople (381), at which he was present. At that council he voted for acceptance of the term "homoousios", having been finally convinced that there was no better alternative. His story is perhaps best representative of those Eastern bishops (perhaps a majority), initially mistrustful of Nicaea, who came to accept the creed of that council, and the doctrine of the "homoousion".
Theological position.
Though his theology was at first somewhat indefinite in phraseology, he undoubtedly gave a thorough adhesion to the Nicene orthodoxy. Even if he did avoid the debatable term "homooussios", he expressed its sense in many passages, which exclude equally Patripassianism, Sabellianism, and the formula "there was a time when the Son was not" attributed to Arius. In other points he takes the ordinary ground of the Eastern Fathers, as in the emphasis he lays on the freedom of the will, the "autexousion" (αὐτεξούσιον), and in his view of the nature of sin. To him sin is the consequence of freedom, not a natural condition. The body is not the cause, but the instrument of sin. The remedy for it is repentance, on which he insists. Like many of the Eastern Fathers, he focuses on high moral living as essential to true Christianity. His doctrine of the Resurrection is not quite so realistic as that of other Fathers; but his conception of the Church is decidedly empirical: the existing catholic Church form is the true one, intended by Christ, the completion of the Church of the Old Testament. His interpretation of the Eucharist is disputed. If he sometimes seems to approach the symbolic view, at other times he comes very close to a strong realistic doctrine. The bread and wine are not mere elements, but the body and blood of Christ.
Cyril of Jerusalem is often renowned for his beliefs in the nature of Jesus and God. His writings are filled with the loving and forgiving nature of God which was somewhat uncommon during his time period. Cyril fills his writings with great lines of the healing power of forgiveness and the Holy Spirit like “The Spirit comes gently and makes himself known by his fragrance. He is not felt as a burden for God is light, very light. Rays of light and knowledge stream before him as the Spirit approaches. The Spirit comes with the tenderness of a true friend to save, to heal, to teach, to counsel, to strengthen and to console”. Cyril truly believes in the forgiving aspect of Christianity and knows the power it holds to turn those in pain towards the light of God. Cyril himself followed God's message of forgiveness himself many times throughout his life. Most clearly seen in his two major exiles where Cyril was disgraced and forced to leave his position and his people behind. He never wrote or showed any ill will towards those who wronged him. Cyril’s central messages also contain the primary principle of faith. Cyril knew religion wasn’t about proving the existence of God or proving the divinity of Christ but rather instilling a faith in people. Cyril knew the power and importance of faith and tried at every opportunity to pass his faith onto others, allowing them to feel the presence of the Holy Spirit. Through his simple message Cyril became recognized as one of the most profound and admired Bishops in church history, which ultimately led to his canonization by the Christian church.
Catechetical Lectures.
Cyril's famous twenty-three lectures given to catechumens in Jerusalem being prepared for, and after, baptism are best considered in two parts: the first eighteen lectures are common known as the "Catechecical Lectures", "Catechetical Orations" or "Catechetical Homilies", while the final five are often called the "Mystagogic Catecheses" (μυσταγωγικαί), because they deal with the "mysteries" (μυστήρια) i.e. Sacraments of Baptism, Confirmation and the Eucharist.
His catechetical lectures (Greek "Κατηχήσεις") are generally assumed, on the basis of limited evidence, to have been delivered either in Cyril's early years as a bishop, around 350, or perhaps in 348, while Cyril was still a priest, deputising for his bishop, Maximus. The "Catechetical Lectures" were given in the "Martyrion", the basilica erected by Constantine. The contain instructions on the principal topics of Christian faith and practice, in rather a popular than a scientific manner, full of a warm pastoral love and care for the catechumens to whom they were delivered. Each lecture is based upon a text of Scripture, and there is an abundance of Scriptural quotation throughout. In the "Catechetical Lectures", parallel with the exposition of the Creed as it was then received in the Church of Jerusalem are vigorous polemics against pagan, Jewish, and heretical errors. They are of great importance for the light which they throw upon the method of instruction usual of that age, as well as upon the liturgical practises of the period, of which they give the fullest account extant.
In the 13th lecture, Cyril of Jerusalem discusses the Crucifixion and burial of Jesus Christ. The main themes that Cyril focuses on in these lectures are Original sin and Jesus’ sacrificing himself to save us from our sins. Also, the burial and Resurrection which occurred three days later proving the divinity of Jesus Christ and the loving nature of the Father. Cyril was very adamant about the fact that Jesus went to his death with full knowledge and willingness. Not only did he go willingly but throughout the process he maintained his faith and forgave all those who betrayed him and engaged in his execution. Cyril writes “who did not sin, neither was deceit found in his mouth, who, when he was reviled, did not revile, when he suffered did not threaten”. This line by Cyril shows his belief in the selflessness of Jesus especially in this last final act of Love. The lecture also gives a sort of insight to what Jesus may have be feeling during the execution from the whippings and beatings, to the crown of thorns, to the nailing on the cross. Cyril intertwines the story with the messages Jesus told throughout his life before his execution relating to his final act. For example Cyril writes “I gave my back to those who beat me and my cheeks to blows; and my face I did not shield from the shame of spitting”. This clearly reflects the teachings of Jesus to turn the other cheeks and not raising your hands against violence because violence just begets violence begets violence. The segment of the Catechesis really reflects the voice Cyril maintained in all of his writing. The writings always have the central message of the bible; Cyril doesn’t try to add his own beliefs in reference to religious interpretation and remains grounded in true biblical teachings.
Eschatology.
Cyril identified the four beast-kingdoms of Daniel 7: “For as the first kingdom which became renowned was that of the Assyrians, and the second, that of the Medes and Persians together, and after these, that of the Macedonians was the third, so the fourth kingdom now is that of the Romans." The fourth beast he said would be "a fourth kingdom upon earth, which shall surpass all kingdoms. And that this kingdom is that of the Romans, has been the tradition of the Church's interpreters.”
The “little horn” that uproots three other of the ten horns of the fourth beast, Cyril called the Antichrist and it was soon to appear. “There shall rise up together ten kings of the Romans, reigning in different parts perhaps, but all about the same time; and after these an eleventh, the Antichrist, who by his magical craft shall seize upon the Roman power will deceive both Jew and Gentile; and of the kings who reigned before him, three he shall humble, and the remaining seven he shall keep in subjection to himself." Then 3 ½ years later the Antichrist would be slain in the second advent of Jesus.
He said that even now Antichrists – heretics in disguise – were in the churches. He charged his hearers to be prepared for the possible imminent coming of Antichrist in the church.
While discussing Nebuchadezzar’s image of Daniel 2 and the stone cut out of a mountain he states, “And in the days of those kingdoms the God of heaven shall set up a kingdom, which shall never be destroyed, and His kingdom shall not be left to another people." The stone kingdom that supersedes the earthly kingdoms had not yet been established, according to Cyril, and Christ's coming kingdom shall never end.
Cyril used the day-year principle to interpret the sixty-nine weeks of Daniel 9. Using the Olympiads as did Eusebius, he calculated the time period, as extending from the restoration of the temple in the sixth year of Darius to the time of Herod, in whose reign Christ was born.
Cyril looked forward to the Second Advent as surely as he knew of the first. “In His former advent, He was wrapped in swaddling clothes in the manger; in His second, He covereth Himself with light as with a garment. In His first coming, He endured the Cross, despising shame; in His second, He comes attended by a host of Angels, receiving glory. We rest not then upon His first advent only, but look also for His second." He looked forward to the Second Advent which would bring an end to the world and then the created world to be re-made anew. At the Second Advent he expected to rise in the resurrection if it came after his time on earth.
"Mystagogic Catecheses".
There has been considerable controversy over the date and authorship of the "Mystagogic Catecheses", addressed to the newly baptized, in preparation for the reception of Holy Communion, with some scholars having attributed them to Cyril's successor as Bishop of Jerusalem, John. Many scholars would currently view the "Mystagogic Catecheses" as being written by Cyril, but in the 370s or 380s, rather than at the same time as the "Catechetical Lectures".
According to the Spanish pilgrim Egeria, these "mystagogical catecheses" were given to the newly baptised in the Church of the "Anastasis" in the course of Easter Week.

</doc>
<doc id="7388" url="http://en.wikipedia.org/wiki?curid=7388" title="Hanukkah">
Hanukkah

Hanukkah ( ; , Tiberian: , usually spelled , pronounced in Modern Hebrew; a transliteration also romanized as Chanukah or Chanukkah), also known as the Festival of Lights, Feast of Dedication, is an eight-day Jewish holiday commemorating the rededication of the Holy Temple (the Second Temple) in Jerusalem at the time of the Maccabean Revolt against the Seleucid Empire of the 2nd century BCE. Hanukkah is observed for eight nights and days, starting on the 25th day of Kislev according to the Hebrew calendar, which may occur at any time from late November to late December in the Gregorian calendar.
The festival is observed by the kindling of the lights of a unique candelabrum, the nine-branched "menorah" or "hanukiah", one additional light on each night of the holiday, progressing to eight on the final night. The typical menorah consists of eight branches with an additional raised branch. The extra light is called a "shamash" (, "attendant") and is given a distinct location, usually above or below the rest. The purpose of the "shamash" is to have a light available for practical use, as using the Hanukkah lights themselves for purposes other than publicizing and meditating upon Hanukkah is forbidden.
Etymology.
The name "Hanukkah" derives from the Hebrew verb "", meaning "to dedicate". On Hanukkah, the Maccabean forces regained control of Jerusalem and rededicated the Temple.
Many homiletical explanations have been given for the name:
Historical sources.
Maccabees, Mishna and Tamud.
The story of Hanukkah, along with its laws and customs, is entirely missing in the Mishna apart from several passing references (Bikkurim 1:6, Rosh HaShanah 1:3, Taanit 2:10, Megillah 3:4 and 3:6, Moed Katan 3:9, and Bava Kama 6:6).
Rav Nissim Gaon postulates in his "Hakdamah Le'mafteach Hatalmud" that information on the holiday was so commonplace that the Mishna felt no need to explain it. A modern-day scholar Reuvein Margolies suggests that as the Mishnah was redacted after the Bar Kochba revolt, its editors were reluctant to include explicit discussion of a holiday celebrating another relatively recent revolt against a foreign ruler, for fear of antagonizing the Romans.
The story of Hanukkah is preserved in the books of the First and Second Maccabees. These books are not part of the Tanakh (Hebrew Bible); they are Jewish apocryphal books instead. The miracle of the one-day supply of oil miraculously lasting eight days is first described in the Talmud, committed to writing about 600 years after the events described in the books of Maccabees.
The Gemara (Talmud), in tractate "Shabbat," page 21b, focuses on Shabbat candles and moves to Hanukkah candles and says that after the forces of Antiochus IV had been driven from the Temple, the Maccabees discovered that almost all of the ritual olive oil had been profaned. They found only a single container that was still sealed by the High Priest, with enough oil to keep the menorah in the Temple lit for a single day. They used this, yet it burned for eight days (the time it took to have new oil pressed and made ready).
The Talmud presents three options:
In Sephardic families, the head of the household lights the candles, while in Ashkenazic families, all family members light.
Except in times of danger, the lights were to be placed outside one's door, on the opposite side of the Mezuza, or in the window closest to the street. Rashi, in a note to "Shabbat 21b," says their purpose is to publicize the miracle. The blessings for Hanukkah lights are discussed in tractate "Succah," p. 46a.
Narrative of Josephus.
The Jewish historian Titus Flavius Josephus narrates in his book, Jewish Antiquities XII, how the victorious Judas Maccabeus ordered lavish yearly eight-day festivities after rededicating the Temple in Jerusalem that had been profaned by Antiochus IV Epiphanes. Josephus does not say the festival was called Hannukkah but rather the "Festival of Lights":
Other ancient sources.
The story of Hanukkah is alluded to in the book of 1 Maccabees and 2 Maccabees. The eight-day rededication of the temple is described in 1 Maccabees 4:36 "et seq", though the name of the festival and the miracle of the lights do not appear here. A story similar in character, and obviously older in date, is the one alluded to in 2 Maccabees 1:18 "et seq" according to which the relighting of the altar fire by Nehemiah was due to a miracle which occurred on the 25th of Kislev, and which appears to be given as the reason for the selection of the same date for the rededication of the altar by Judah Maccabee.
Another source is the Megillat Antiochus. This work (also known as "Megillat HaHasmonaim", "Megillat Hanukkah" or "Megillat Yevanit") is in both Aramaic and Hebrew; the Hebrew version is a literal translation from the Aramaic original. Recent scholarship dates it to somewhere between the 2nd and 5th Centuries, probably in the 2nd century, with the Hebrew dating to the 7th century. It was published for the first time in Mantua in 1557. Saadia Gaon, who translated it into Arabic in the 9th century, ascribed it to the Maccabees themselves, disputed by some, since it gives dates as so many years before the destruction of the second temple in 70 CE. The Hebrew text with an English translation can be found in the Siddur of Philip Birnbaum.
In the Christian Greek Scriptures, it is stated that Jesus was at the Jerusalem Temple during "the Feast of Dedication and it was winter", in John 10:22–23. The Greek term that is used is "the renewals" (Greek "ta engkainia" τὰ ἐγκαίνια). Josephus refers to the festival as "lights."
Story of Hanukkah.
Background.
Judea was part of the Ptolemaic Kingdom of Egypt until 200 BCE when King Antiochus III the Great of Syria defeated King Ptolemy V Epiphanes of Egypt at the Battle of Panium. Judea became at that moment part of the Seleucid Empire of Syria. King Antiochus III the Great wanting to conciliate his new Jewish subjects guaranteed their right to "live according to their ancestral customs" and to continue to practice their religion in the Temple of Jerusalem. However in 175 BCE, Antiochus IV Epiphanes, the son of Antiochus III invaded Judea, ostensibly at the request of the sons of Tobias. The Tobiads, who led the Hellenizing Jewish faction in Jerusalem, were expelled to Syria around 170 BCE when the high priest Onias and his pro-Egyptian faction wrested control from them. The exiled Tobiads lobbied Antiochus IV Epiphanes to recapture Jerusalem. As the ancient Jewish historian Flavius Josephus tells us:
Traditional view.
When the Second Temple in Jerusalem was looted and services stopped, Judaism was outlawed. In 167 BCE Antiochus ordered an altar to Zeus erected in the Temple. He banned brit milah (circumcision) and ordered pigs to be sacrificed at the altar of the temple (the sacrifice of pigs to the Greek gods was standard ritual practice in the Ancient Greek religion).
Antiochus's actions provoked a large-scale revolt. Mattathias (Mattityahu), a Jewish priest, and his five sons Jochanan, Simeon, Eleazar, Jonathan, and Judah led a rebellion against Antiochus. Judah became known as Yehuda HaMakabi ("Judah the Hammer"). By 166 BCE Mattathias had died, and Judah took his place as leader. By 165 BCE the Jewish revolt against the Seleucid monarchy was successful. The Temple was liberated and rededicated. The festival of Hanukkah was instituted to celebrate this event. Judah ordered the Temple to be cleansed, a new altar to be built in place of the polluted one and new holy vessels to be made. According to the Talmud, unadulterated and undefiled pure olive oil with the seal of the kohen gadol (high priest) was needed for the menorah in the Temple, which was required to burn throughout the night every night. The story goes that one flask was found with only enough oil to burn for one day, yet it burned for eight days, the time needed to prepare a fresh supply of kosher oil for the menorah. An eight-day festival was declared by the Jewish sages to commemorate this miracle.
The version of the story in 1 Maccabees states that an eight-day celebration of songs and sacrifices was proclaimed upon re-dedication of the altar, and makes no mention of the miracle of the oil.
Modern scholarship.
Some modern scholars argue that the king was intervening in an internal civil war between the traditionalist Jews (Pharisees) and the Hellenized Jews (Sadducees) in Jerusalem.
These competed violently over who would be the High Priest, with traditionalists with Hebrew/Aramaic names like Onias contesting with Hellenizing High Priests with Greek names like Jason and Menelaus. In particular Jason's Hellenistic reforms would prove to be a decisive factor leading to eventual conflict within the ranks of Judaism. Other authors point to possible socioeconomic reasons in addition to the religious reasons behind the civil war.
What began in many respects as a civil war escalated when the Hellenistic kingdom of Syria sided with the Hellenizing Jews in their conflict with the traditionalists. As the conflict escalated, Antiochus took the side of the Hellenizers by prohibiting the religious practices the traditionalists had rallied around. This may explain why the king, in a total departure from Seleucid practice in all other places and times, banned a traditional religion.
Hanukkah rituals.
Hanukkah is celebrated with a series of rituals that are performed every day throughout the 8-day holiday, some are family-based and others communal. There are special additions to the daily prayer service, and a section is added to the blessing after meals.
Hanukkah is not a "Sabbath-like" holiday, and there is no obligation to refrain from activities that are forbidden on the Sabbath, as specified in the "Shulkhan Arukh". Adherents go to work as usual, but may leave early in order to be home to kindle the lights at nightfall. There is no religious reason for schools to be closed, although, in Israel, schools close from the second day for the whole week of Hanukkah. Many families exchange gifts each night, such as books or games. Fried foods (such as latke potato pancakes, jelly doughnut sufganiyot) are eaten to commemorate the importance of oil during the celebration of Hanukkah.
Kindling the Hanukkah lights.
Each night, throughout the 8 day holiday, a candle or oil-based light, is lit. As a universally practiced "beautification" (hiddur mitzvah) of the mitzvah, the number of lights lit is increased by one each night. An extra light called a "shamash", meaning "attendant" or "sexton," is also lit each night, and is given a distinct location, usually higher, lower, or to the side of the others. The purpose of the extra light is to adhere to the prohibition, specified in the Talmud (Tracate Shabbat 21b–23a), against using the Hanukkah lights for anything other than publicizing and meditating on the Hanukkah miracle. This differs from Sabbath candles which are meant to be used for illumination and lighting. Hence, if one were to need extra illumination on Hanukkah, the "shamash" candle would be available and one would avoid using the prohibited lights. Some light the "shamash" candle first and then use it to light the others. So all together, including the "shamash", two lights are lit on the first night, three on the second and so on, ending with nine on the last night, for a total of 44 (36, excluding the "shamash").
The lights can be candles or oil lamps. Electric lights are sometimes used and are acceptable in places where open flame is not permitted, such as a hospital room, or for the very elderly and infirm. Most Jewish homes have a special candelabrum referred to as either a "chanukkiah" (the modern Israeli term), or a "menorah" (the traditional classical name), or oil lamp holder for Hanukkah, which holds eight lights plus the additional "shamash" light. Since the 1970s the worldwide Chabad Hasidic movement has initiated public menorah lightnings in open public places in many countries.
The reason for the Hanukkah lights is not for the "lighting of the house within", but rather for the "illumination of the house without," so that passersby should see it and be reminded of the holiday's miracle (i.e. the triumph of the few over the many and of the pure over the impure). Accordingly, lamps are set up at a prominent window or near the door leading to the street. It is customary amongst some Ashkenazi Jews to have a separate menorah for each family member (customs vary), whereas most Sephardi Jews light one for the whole household. Only when there was danger of antisemitic persecution were lamps supposed to be hidden from public view, as was the case in Persia under the rule of the Zoroastrians, or in parts of Europe before and during World War II. However, most Hasidic groups light lamps near an inside doorway, not necessarily in public view. According to this tradition, the lamps are placed on the opposite side from the "mezuzah", so that when one passes through the door he is surrounded by the holiness of "mitzvot" (the commandments).
Generally women are exempt in Jewish law from time-bound positive commandments, although the Talmud requires that women engage in the mitzvah of lighting Hanukkah candles “for they too were involved in the miracle.” In practice, only the male members of Orthodox households are obliged to light the menorah.
Candle-lighting time.
Hanukkah lights should burn for at least one half hour after it gets dark. The custom of the Vilna Gaon observed by many residents of Jerusalem as the custom of the city, is to light at sundown, although most Hasidim light later, even in Jerusalem. Many Hasidic Rebbes light much later, because they fulfill the obligation of publicizing the miracle by the presence of their Hasidim when they kindle the lights.
Inexpensive small wax candles sold for Hanukkah burn for approximately half an hour, so on most days this requirement can be safely ignored.
Friday night presents a problem, however. Since candles may not be lit on the Shabbat itself, the candles must be lit before sunset. However, they must remain lit until the regular it is time for you to use longer candles, or the traditional oil lamps. In keeping with the above-stated prohibition, the Hanukkah menorah is lit first, followed by the Shabbat candles which signify its onset.
Blessings over the candles.
Typically three blessings ("brachot"; singular: "brachah") are recited during this eight-day festival when lighting the candles:
On the first night of Hanukkah, Jews recite all three blessings; on all subsequent nights, they recite only the first two.
The blessings are said before or after the candles are lit depending on tradition. On the first night of Hanukkah one light (candle or oil) is lit on the right side of the menorah, on the following night a second light is placed to the left of the first, and so on, proceeding from right to left over the eight nights. On each night, the leftmost candle is lit first, and lighting proceeds from left to right.
For the full text of the blessings, see List of Jewish prayers and blessings: Hanukkah.
"Hanerot Halalu".
During or after the lights are kindled the hymn "Hanerot Halalu" is recited. There are several differing versions; the version presented here is recited in many Ashkenazic communities:
"Maoz Tzur".
Each night after the lighting of the candles, the hymn Ma'oz Tzur is sung. The song contains six stanzas. The first and last deal with general themes of divine salvation, and the middle four deal with events of persecution in Jewish history, and praises God for survival despite these tragedies (the exodus from Egypt, the Babylonian captivity, the miracle of the holiday of Purim, the Hasmonean victory), and a longing for the days when Judea will finally triumph over Rome.
The song was composed in the thirteenth century by a poet only known through the acrostic found in the first letters of the original five stanzas of the song: Mordecai. It became the traditional hymn sung after the candlelighting in Ashkenazi homes. The familiar tune is most probably a derivation of a German Protestant church hymn or a popular folk song.
Other customs.
After lighting the candles and Ma'oz Tzur, singing other Hanukkah songs is customary in many Jewish homes. Some Hasidic and Sephardi Jews recite Psalms, such as , , and . In North America and in Israel it is common to exchange presents or give children presents at this time. In addition, many families encourage their children to give tzedakah (charity) in lieu of presents for themselves.
Special additions to daily prayers.
An addition is made to the "hoda'ah" (thanksgiving) benediction in the Amidah (thrice-daily prayers), called "Al ha-Nissim" ("On/about the Miracles"). This addition refers to the victory achieved over the Syrians by the Hasmonean Mattathias and his sons.
The same prayer is added to the grace after meals. In addition, the "Hallel" (praise) ( - ) are sung during each morning service and the "Tachanun" penitential prayers are omitted.
The Torah is read every day in the shacharit morning services in synagogue, on the first day beginning from (according to some customs, ), and the last day ending with . Since Hanukkah lasts eight days it includes at least one, and sometimes two, Jewish Sabbaths (Saturdays). The weekly Torah portion for the first Sabbath is almost always "Miketz", telling of Joseph's dream and his enslavement in Egypt. The "Haftarah" reading for the first Sabbath Hanukkah is – . When there is a second Sabbath on Hanukkah, the "Haftarah" reading is from - .
The Hanukkah "menorah" is also kindled daily in the synagogue, at night with the blessings and in the morning without the blessings.
The menorah is not lit on the Sabbath, but rather prior to the beginning of the Sabbath at night and not at all during the day.
During the Middle Ages "Megillat Antiochus" was read in the Italian synagogues on Hanukkah just as the Book of Esther is read on Purim. It still forms part of the liturgy of the Yemenite Jews.
"Zot Hanukkah".
The last day of Hanukkah is known as "Zot Hanukkah", from the verse read on this day in the synagogue , "Zot Chanukat Hamizbe'ach": "This was the dedication of the altar"). According to the teachings of Kabbalah and Hasidism, this day is the final "seal" of the High Holiday season of Yom Kippur, and is considered a time to repent out of love for God. In this spirit, many Hasidic Jews wish each other "Gmar chatimah tovah" ("may you be sealed totally for good"), a traditional greeting for the Yom Kippur season. It is taught in Hasidic and Kabbalistic literature that this day is particularly auspicious for the fulfillment of prayers.
Symbolic importance.
Many people define major Jewish holidays as those that feature traditional holiday meals, kiddush, holiday candle-lighting, etc., and when all forms of work are forbidden. Only biblical holidays fit this criteria, and Hanukah was instituted some two centuries after the Bible was completed and canonized. Nevertheless, though Hanukah is of rabbinic origin, it is traditionally celebrated in a major and very public fashion. The requirement to position the menorah, or Hanukiah, at the door or window symbolizes the desire to give the Hanukah miracle a high profile.
The classical rabbis downplayed the military and nationalistic dimensions of Hanukkah, and some even interpreted the emphasis upon the story of the miracle oil as a diversion away from the struggle with empires that had led to the disastrous downfall of Jerusalem to the Romans.
Some Jewish historians suggest a different explanation for the rabbinic reluctance to laud the militarism. First, the rabbis wrote after Hasmonean leaders had led Judea into Rome’s grip and so may not have wanted to offer the family much praise. Second, they clearly wanted to promote a sense of dependence on God, urging Jews to look toward the divine for protection. They likely feared inciting Jews to another revolt that might end in disaster, like the 135 C.E. experience.
With the advent of Zionism and the state of Israel, however, these themes were reconsidered. In modern Israel, the national and military aspects of Hanukkah became, once again, more dominant.
In North America especially, Hanukkah gained increased importance with many Jewish families in the final decades of the 20th century, including large numbers of secular Jews, who wanted a Jewish alternative to the Christmas celebrations that often overlap with Hanukkah. Though it was traditional among Ashkenazi Jews to give "gelt" or money coins to children during Hanukkah, in many families this has changed into gifts in order to prevent Jewish children from feeling left out of the Christmas gift giving.
While Hanukkah is a relatively minor Jewish holiday, as indicated by the lack of religious restrictions on work other than a few minutes after lighting the candles, in North America, Hanukkah in the 21st century has taken a place equal to Passover as a symbol of Jewish identity. Both the Israeli and North American versions of Hanukkah emphasize resistance, focusing on some combination of national liberation and religious freedom as the defining meaning of the holiday.
Hanukkah music.
A large number of songs have been written on Hanukkah themes, perhaps more so than for any other Jewish holiday. Some of the best known are "Hanukkiah Li Yesh" ("I Have a Hanukkah Menorah"), "Ocho Kandelikas" ("Eight Little Candles"), "Kad Katan" ("A Small Jug"), "S'vivon Sov Sov Sov" ("Dreidel, Spin and Spin"), "Haneirot Halolu"" ("These Candles which we light"), "Mi Yimalel" ("Who can Retell") and "Ner Li, Ner Li" ("I have a Candle"). The most well known in English-speaking countries include "Dreidel, Dreidel, Dreidel" and "Chanukah, Oh Chanukah".
Hanukkah foods.
There is a custom of eating foods fried or baked in oil (preferably olive oil) to commemorate the miracle of a small flask of oil keeping the flame that was in the temple alight for eight days. Traditional foods include potato pancakes, known as "latkes" in Yiddish, especially among Ashkenazi families. Sephardi, Polish and Israeli families eat jam-filled doughnuts ( "pontshkes"), bimuelos (fritters) and sufganiyot which are deep-fried in oil.
Bakeries in Israel have popularized many new types of fillings for "sufganiyot" besides the traditional strawberry jelly filling, including chocolate cream, vanilla cream, caramel, cappuccino and others. In recent years, downsized, "mini" sufganiyot containing half the calories of the regular, 400-to-600-calorie version have become popular.
There is also a tradition of eating cheese products on Hanukkah recorded in rabbinic literature. This custom is seen as a commemoration of the involvement of Judith and women in the events of Hanukkah.
Dreidel.
The dreidel, or "sevivon" in Hebrew, is a four-sided spinning top that children play with on Hanukkah. Each side is imprinted with a Hebrew letter. These letters are an acronym for the Hebrew words ("Nes Gadol Haya Sham", "A great miracle happened there"), referring to the miracle of the oil that took place in the Beit Hamikdash.
On dreidels sold in Israel, the fourth side is inscribed with the letter "(Pe)", rendering the acronym ("Nes Gadol Haya Po", "A great miracle happened here"), referring to the fact that the miracle occurred in the land of Israel. Stores in Haredi neighborhoods sell the traditional "Shin" dreidels as well.
Some Jewish commentators ascribe symbolic significance to the markings on the dreidel. One commentary, for example, connects the four letters with the four exiles to which the nation of Israel was historically subject: Babylonia, Persia, Greece, and Rome.
After lighting the Hanukkah menorah, it is customary in many homes to play the dreidel game: Each player starts out with 10 or 15 coins (real or of chocolate), nuts, raisins, candies or other markers, and places one marker in the "pot." The first player spins the dreidel, and depending on which side the dreidel falls on, either wins a marker from the pot or gives up part of his stash. The code (based on a Yiddish version of the game) is as follows:
Another version differs:
The game may last until one person has won everything.
The dreidel is believed to commemorate a game devised by the Jews to camouflage the fact that they were studying Torah, which was outlawed by Greeks. The Jews would gather in caves to study, posting a lookout to alert the group to the presence of Greek soldiers. If soldiers were spotted, the Jews would hide their scrolls and spin tops, so the Greeks thought they were gambling, not learning.
The historical context may be from the time of the Bar-Kohba war, 132-135 C.E. when the penalty for teaching Torah was death, so decreed by Rome. Others trace the "dreidel" itself to the children's top game Teetotum.
Hanukkah gelt.
Hanukkah gelt (Yiddish for "money") is often distributed to children to add to the holiday "excitement". The amount is usually in small coins, although grandparents or relatives may give larger sums. In Israel, Hanukkah "gelt" is known as "dmei Hanukkah". The tradition of giving Chanukah "gelt" dates back to a long-standing East European custom of children presenting their teachers with a small sum of money at this time of year as a token of gratitude. The connection may be etymological: In Hebrew, the words "Hanukkah" (dedication) and "hinnukh" (education) come from the same root. In time, money was also given to children to keep for themselves. According to Magen Avraham (Abraham Abele Gombiner 1635–1682), poor yeshiva students would receive a gift of money from their Jewish benefactors on Hanukkah. In the 1920s, American chocolatiers picked up on the gift/coin concept by creating chocolate "gelt".
Many Hasidic Rebbes distribute coins to those who visit them during Hanukkah. Hasidic Jews consider this to be a "segulah" (merit / good luck) for success.
Judith and Holofernes.
The eating of dairy foods, especially cheese, on Hanukkah is a minor custom that has its roots in the story of Judith. The deuterocanonical book of Judith (Yehudit or Yehudis in Hebrew), which is not part of the Tanakh, records that Holofernes, an Assyrian general, had surrounded the village of Bethulia as part of his campaign to conquer Judea.
After intense fighting, the water supply of the Jews is cut off and the situation became desperate. Judith, a pious widow, told the city leaders that she had a plan to save the city. Judith went to the Assyrian camps and pretended to surrender. She met Holofernes, who was smitten by her beauty. She went back to his tent with him, where she plied him with cheese and wine. When he fell into a drunken sleep, Judith beheaded him and escaped from the camp, taking the severed head with her (the beheading of Holofernes by Judith has historically been a popular theme in art).
When Holofernes' soldiers found his corpse, they were overcome with fear; the Jews, on the other hand, were emboldened, and launched a successful counterattack. The town was saved, and the Assyrians defeated.
Alternative spellings.
In Hebrew, the word Hanukkah is written or (). It is most commonly transliterated to English as "" or "Hanukkah", the former because the sound represented by "CH" (, similar to the Scottish pronunciation of "loch") does not exist in the English language. Furthermore, the letter "ḥet" (), which is the first letter in the Hebrew spelling, is pronounced differently in modern Hebrew (voiceless uvular fricative) than in classical Hebrew (voiceless pharyngeal fricative ), and neither of those sounds is unambiguously representable in English spelling. Moreover, the 'kaf' consonant is geminate in classical (but not modern) Hebrew. Adapting the classical Hebrew pronunciation with the geminate and pharyngeal can lead to the spelling "Hanukkah"; while adapting the modern Hebrew pronunciation with no gemination and uvular leads to the spelling "". It has also been spelled as "Hannukah".
Historic timeline.
Battles of the Maccabean revolt.
Key battles between the Maccabees and the Seleucid Syrian-Greeks:
Dates.
The dates of Hanukkah are determined by the Hebrew calendar. Hanukkah begins at the 25th day of Kislev, and concludes on the 2nd or 3rd day of Tevet (Kislev can have 29 or 30 days). The Jewish day begins at sunset, whereas the Gregorian calendar begins the day at midnight. Hanukkah begins at sunset of the date listed.
In 2013, on November 28, the American holiday of Thanksgiving fell during Hanukkah for only the third time since Thanksgiving was declared a national holiday by President Abraham Lincoln. The last time was 1899; and due to the Gregorian and Jewish calendars being slightly out of sync with each other, it will not happen again in the foreseeable future. This convergence prompted the creation of the portmanteau neologism Thanksgivukkah.
Hanukkah in the White House.
The United States has a history of recognizing and celebrating Hanukkah in a number of ways, from menorah lighting ceremonies to a 1996 postage stamp, jointly issued with Israel, to special receptions in the White House (although the United States has not had any Jewish presidents).
One of the earliest links with the White House occurred in 1951, when Israeli Prime Minister David Ben-Gurion presented United States President Harry Truman with a Hanukkah Menorah. But it was not until 1979 that a sitting president, Jimmy Carter took part in a public Hanukkah candle-lighting ceremony on the National Mall, followed by the first Hanukkah candle-lighting ceremony in the White House itself, led by President Bill Clinton.
In 2001, President George W. Bush held an official Hanukkah reception in the White House in conjunction with the candle-lighting ceremony, and since then this ceremony has become an annual tradition attended by Jewish leaders from around the country. In 2008, George Bush linked the occasion to the 1951 gift by using that menorah for the ceremony, with a grandson of Ben-Gurion and a grandson of Truman lighting the candles.
On Nov. 28th, 2013 President Obama marked the beginning of Hanukkah with a reception at the White House and noted the holiday's convergence with Thanksgiving saying, “For the first time since the late 1800s – and for the last time until some 70,000 years from now – the first day of Hanukkah falls on Thanksgiving. It’s an event so rare some have even coined it 'Thanksgivukkah.' As we gather with loved ones around the turkey, the menorah, or both, we celebrate some fortunate timing and give thanks for miracles both great and small."
Green Hanukkah.
Some Jews in North America and Israel have taken up environmental concerns in relation to Hanukkah's "miracle of the oil", emphasizing reflection on energy conservation and energy independence. An example of this is the Coalition on the Environment and Jewish Life's renewable energy campaign.

</doc>
<doc id="7390" url="http://en.wikipedia.org/wiki?curid=7390" title="Christian views on marriage">
Christian views on marriage

Most Christian authorities and bodies view marriage (also called Holy Matrimony) as a state instituted and ordained by God for the lifelong relationship between one man as husband and one woman as wife. They consider it the most intimate of human relationships, a gift from God, and a sacred institution. Protestants consider it to be sacred, holy, and even central to the community of faith, while Catholics and Orthodox Christians consider it a Sacrament. Biblically, it is to be "held in honour among all…."
Jesus Christ underscored the importance and sacredness of lifelong marriage in his own teachings. He stated that God had created mankind as male and female, and that in marriage the two will become one flesh'. So they are no longer two, but one flesh. Therefore what God has joined together, let no one separate."
Civil laws recognize marriage as having social and political statuses. Christian theology affirms the secular status of marriage, but additionally views it from a moral and religious perspective that transcends all social interests.
While marriage is honored and affirmed among Christians and throughout the Bible, there is no suggestion that it is necessary for everyone. Single people who either have chosen to remain unmarried or who have lost their spouse for some reason are neither incomplete in Christ nor personal failures.
The New Testament teaches that sex is reserved for marriage. It calls sex outside of marriage the sin of adultery (for the married person) if either sexual participant is married to another person, while it calls voluntary sexual intercourse between two unmarried persons the sin of fornication.
Background.
Christians seek to uphold the seriousness of wedding vows. Yet, they respond with compassion to deep hurts by recognizing that divorce, though less than the ideal, is sometimes necessary to relieve one partner of intolerable hardship, unfaithfulness or desertion. While the voice of God had said, "I hate divorce", some authorities believe the divorce rate in the church is nearly comparable to that of the culture at large.
There is considerable disagreement among Christians as to the biblical way to define the "roles" of each marriage partner, and how each should interact in the family to create healthy family relationships and to please God. Roles in Christian marriages between opposite-sex couples challenge deep-rooted beliefs, teachings, and traditions—most dating from biblical days. Opinions and teachings vary among three principal groups—one group that believes in a full and co-equal partnership of the husband and wife, and two others which advocate a male-dominant hierarchical structure in marriage:
Some Christian authorities used to permit polygamy (specifically polygyny) in the past, but this practice, besides being illegal in Western cultures, is now considered to be out of the Christian mainstream and continues to be practised only by fringe fundamentalist sects.
Family authority and responsibilities.
Christians today hold three competing views as to what is the biblically ordained relationship between husbands and wives. These views range from one that believes the New Testament teaches complete equality of authority and responsibility between the man and woman in marriage, all the way to one that calls for a return to complete patriarchy in which relationships are based on male-dominant power and authority in marriage.
The great debate about marriage in contemporary Christian circles is among three primary groups—"Christian egalitarians", "Complementarians", and "Biblical patriarchists".
Much of the dispute hinges on how one interprets the New Testament Household Code "(Haustafel)" which has as its main focus hierarchical relationships between three pairs of social classes that were controlled by Roman law: husbands/wives, parents/children, and masters/slaves. The Code, with variations, occurs in four epistles (letters) by the Apostle Paul and in 1  Peter. The Roman law of "Manus" gave the husband nearly absolute autocratic power over his wife, including life and death. The law of "Patria Potestas" (Latin for "Rule of the Fathers") gave a husband equally severe power over his children and slaves. Theologian Frank Stagg finds the basic tenets of the Code in Aristotle's discussion of the household in Book 1 of "Politics" and in Philo's "Hypothetica 7.14". Serious study of the New Testament Household Code "(Haustafel)" began with Martin Dilbelius in 1913, with a wide range of studies since then. In a Tübingen dissertation by James E. Crouch, he concludes that the early Christians found in Hellenistic Judaism a code which they adapted and Christianized.
Biblical egalitarians, Complementarians, and Biblical patriarchists each differ on how the provisions of the New Testament Household Code are to be interpreted today, both as to meaning and to intended audiences. Some authorities view them as applicable to 1st century new Christians living under an oppressive Roman legal system, while others believe they were intended to apply to all peoples of all times to come, including today.
Stagg believes the several occurrences of the Code in the New Testament were intended to meet the needs for "order" within the churches and in the society of the day. He maintains that the New Testament Household Codes are attempts by Paul and Peter to Christianize these harsh Codes for Roman citizens who had become followers of Christ. Stagg writes that there is some suggestion in scripture that because Paul had taught that they had newly found freedom "in Christ", wives, children, and slaves were taking improper advantage of the "Haustafel" both in the home and the church. "The form of the code stressing reciprocal social duties is traced to Judaism's own Oriental background, with its strong moral/ethical demand but also with a low view of woman... At bottom is probably to be seen the perennial tension between freedom and order... What mattered to (Paul) was 'a new creation' and 'in Christ' there is 'not any Jew not Greek, not any slave nor free, not any male and female. Such codes existed in Greek tradition. Two of these Christianized codes are found in (which contains the phrases "husband is the head of the wife" and "wives, submit to your husband") and in (which instructs wives to subordinate themselves to their husbands).
The importance of the meaning of "head" as used by the Apostle Paul is pivotal in the conflict between the Complementarian position and the Egalitarian view. The word Paul used for "head", transliterated from Greek, is "kephalē". Today's English word "cephalic" ( ) stems from the Greek "kephalē" and means "Of or relating to the head; or located on, in, or near the head. " A thorough concordance search by Catherine Kroeger shows that the most frequent use of "head" "(kephalē)" in the New Testament is to refer to "the anatomical head of a body". She found that its second most frequent use in the New Testament was to convey the metaphorical sense of "source". Other Egalitarian authors such as Margaret Howe agree with Kroeger, writing that "The word 'head' (in and other similar passages) must be understood not as 'ruler' but as 'source.
Wayne Grudem criticizes commonly rendering "kephalē" in those same passages only to mean "source", and argue that it denotes "authoritative head" in such texts as . They interpret that verse to mean that God the father is the authoritative head over the Son, and in turn Jesus is the authoritative head over the church, not simply its source. By extension, they then conclude that in marriage and in the church, the man is the authoritative head over the woman.
Another potential way to define the word "head", and hence the relationship between husband and wife as found in Bible, is through the example given in the surrounding context in which the word is found. In that context the husband and wife are compared to Christ and his church. The context seems to imply an authority structure based on a man sacrificing himself for his wife, as Christ did for the church; a love-based authority structure, where submission is not required but freely given based on the care given to the wife.
Some biblical references on this subject are debated depending on one’s school of theology. The historical grammatical method is a hermeneutic technique that strives to uncover the meaning of the text by taking into account not just the grammatical words, but also the syntactical aspects, the cultural and historical background, and the literary genre. Thus references to a patriarchal Biblical culture may or may not be relevant to other societies. What is believed to be a timeless truth to one person or denomination may be considered a cultural norm or minor opinion to another.
Egalitarian view.
Christian Egalitarians (from the French word "égal" meaning "equal") believe that Christian marriage is intended to be a marriage without any hierarchy—a full and equal partnership between the wife and husband. They emphasize that nowhere in the New Testament is there a requirement for a wife to "obey" her husband. While "obey" was introduced into marriage vows for much of the church during the Middle Ages, its only New Testament support is found in , with that only being by implication from Sarah's obedience to Abraham. Scriptures such as state that in Christ, right relationships are restored and in him, "there is neither Jew nor Greek, slave nor free, male nor female."
Christian Egalitarians interpret scripture to mean that God intended spouses to practice "mutual submission", each in equality with the other. The phrase "mutual submission" comes from a verse in which precedes advice for the three domestic relationships of the day, including slavery. It reads, "Submit to one another ('mutual submission') out of reverence for Christ", wives to husbands, children to parents, and slaves to their master. Christian Egalitarians believe that full partnership in marriage is the most biblical view, producing the most intimate, wholesome, and reciprocally fulfilling marriages.
The Christian Egalitarian view of marriage asserts that gender, in and of itself, neither privileges nor curtails a believer's gifting or calling to any ministry in the church or home. It does not imply that women and men are identical or undifferentiated, but affirms that God designed men and women to complement and benefit one another. A foundational belief of Christian Egalitarians is that the husband and wife are created equally and are ordained of God to "become one", a biblical principle first ordained by God in , reaffirmed by Jesus in and , and by the Apostle Paul in . Therefore, they see that "oneness" as pointing to gender equality in marriage. They believe the biblical model for Christian marriages is therefore for the spouses to share equal responsibility within the family—not one over the other nor one under the other.
David Dykes, theologian, author, and pastor of a 15,000-member Baptist church, sermonized that "When you are in Christ, you have full equality with all other believers". In a sermon he entitled "The Ground Is Level at the Foot of the Cross", he said that some theologians have called one particular Bible verse the Christian "Magna Carta". The Bible verse reads: "There is neither Jew nor Greek, slave nor free, male nor female, for you are all one in Christ Jesus" (). Acknowledging that there are differences between men and women, he said "in Christ, these differences don't define who we are. The only category that really matters in the world is whether you are in Christ. At the cross, Jesus destroyed all the made-made barriers of hostility:" ethnicity, social status, and gender.
Those of the egalitarian persuasion point to the biblical instruction that all Christian believers, irrespective of gender, are to submit or be subject "to one another in the fear of God" or "out of reverence for Christ". Gilbert Bilezikian writes that in the highly debated Ephesians 5 passage, the verb "to be subject" or "to be submitted" appears in verse 21 which he describes as serving as a "hinge" between two different sections. The first section consists of verses 18-20, verse 21 is the connection between the two, and the second section consists of verses 22-33. When discussion begins at verse 22 in Ephesians 5, Paul appears to be reaffirming a chain of command principle within the family. However,
Advocates of Christian egalitarianism believe that this model has firm biblical support:
The egalitarian paradigm leaves it up to the couple to decide who is responsible for what task or function in the home. Such decisions should be made rationally and wisely, not based on gender or tradition. Examples of a couple's decision logic might include:
Complementarian view.
Complementarians hold to a hierarchical structure between husband and wife. They believe men and women have different gender-specific roles that allow each to "complement" the other, hence the designation "Complementarians". The Complementarian view of marriage is that while the husband and wife are of equal worth before God, husbands and wives are given different functions and responsibilities by God that are based on gender, and that male leadership is biblically ordained so that the husband is always the senior authority figure. They state they "observe with deep concern" "accompanying distortions or neglect of the glad harmony portrayed in Scripture between the loving, humble leadership of redeemed husbands and the intelligent, willing support of that leadership by redeemed wives". They believe "the Bible presents a clear chain of authority—above all authority and power is God; God is the head of Christ. Then in descending order, Christ is the head of man, man is the head of woman, and parents are the head of their children." Complementarians teach that God intended men to lead their wives as "heads" of the family. Wayne Grudem, in an article that interprets the "mutual submission" of as being hierarchical, writes that it means "being considerate of one another, and caring for one another’s needs, and being thoughtful of one another, and sacrificing for one another."
Scriptures such as 1 Corinthians 11:3: "But I would have you know, that the head of every man is Christ; and the head of the woman is the man; and the head of Christ is God," (KJV) are understood as meaning the wife is to be subject to her husband, if not unconditionally.
According to Complementarian authors John Piper, Wayne Grudem, and others, historically, but to a significantly lesser extent in most of Christianity today, the predominant position in both Catholicism and conservative Protestantism places the male as the "head" in the home and in the church. They hold that women are commanded to be in subjection to male leadership, with wives obedient to their head, based upon Old Testament precepts and principles. This view holds that, "God has created men and women equal in their essential dignity and human personhood, but different and complementary in function with male headship in the home and in the Church."
Grudem also acknowledges exceptions to the submission of wives to husbands where moral issues are involved. Rather than unconditional obedience, Complementarian authors such as Piper and Grudem are careful to caution that a wife's submission should never cause her to "follow her husband into sin."
Roman Catholic Church teaching on the role of women includes that of Pope Leo XIII in his 1880 encyclical "Arcanum," which states:
Though each of their churches is autonomous and self-governed, the official position of the Southern Baptist Convention (the largest Protestant denomination) is:
Biblical patriarchy.
Biblical patriarchy is similar to Complementarianism but with differences of degree and emphasis. They carry the husband-headship model considerably further and with more militancy. While Complementarians also hold to exclusively male leadership in both the home and the church, Biblical patriarchy extends that exclusion to the civic sphere as well, so that women should not be civil leaders and indeed should not have careers outside the home.
Biblical patriarchists see what they describe as a crisis of this era being what they term to be a systematic attack on the "timeless truths of biblical patriarchy." They believe such an attack includes the movement to "subvert the biblical model of the family, and redefine the very meaning of fatherhood and motherhood, masculinity, femininity, and the parent and child relationship." Arguing from the biblical presentation of God revealing himself "as masculine, not feminine", they believe God ordained distinct gender roles for man and woman as part of the created order. They say "Adam’s headship over Eve was established at the beginning, before sin entered the world". Their view is that the male has God-given authority and mandate to direct "his" household in paths of obedience to God. They refer to man's "dominion" beginning within the home, and a man’s qualification to lead and ability to lead well in the public square is based upon his prior success in "ruling his household".
Thus, William Einwechter refers to the traditional Complementarian view as "two-point Complementarianism" (male leadership in the family and church), and regards the biblical patriarchy view as "three-point" or "full" complementarianism (male leadership in family, church "and society").
The patriarchists teach that "the woman was created as a helper to her husband, as the bearer of children, and as a "keeper at home,"aa concluding that the God-ordained and proper sphere of dominion for a wife is the household. Biblical patriarchists consider that "faithfulness to Christ requires that (Biblical patriarchy) be believed, taught, and lived." They claim that the "man is...the image and glory of God in terms of authority, while the woman is the glory of man." They teach that a wife is to be "obedient" to her "head" (husband), based upon Old Testament teachings and models.
Other views.
See Christian feminism
Biblical foundations and history.
Christians believe that marriage is considered in its ideal according to the purpose of God. At the heart of God's design for marriage is companionship and intimacy.
The biblical picture of marriage expands into something much broader, with the husband and wife relationship illustrating the relationship between Christ and the church.
It is also considered in its actual occurrence, sometimes involving failure. Therefore, the Bible speaks on the subject of divorce. The New Testament recognizes a place for singleness. Salvation within Christianity is not dependent on the continuation of a biological lineage.
Old Testament.
Christians regard the foundational principle of the lifelong union of a man and a woman to have been first articulated biblically in . It was reaffirmed by Jesus in and and by the Apostle Paul in . The Old Testament describes a number of marriages, some of the best known being Adam and Eve; Abraham, Sarah and Hagar; Isaac and Rebekah; Jacob, Rachel and Leah; Boaz and Ruth; David, Michal, Ahinoam, Abigail, Maachah, Haggith, Abital, Eglah and Bathsheba; and Hosea and the prostitute Gomer, whom he married at God's command.
Polygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Old Testament, yet scholars doubt that it was common among average Israelites because of the wealth needed to practice it.
Betrothal ("erusin"), which is merely a binding promise to get married, is distinct from marriage itself ("nissu'in"), with the time between these events varying substantially. Since a wife was regarded as property in biblical times, the betrothal ("erusin") was effected simply by purchasing her from her father (or guardian); the girl’s consent is not explicitly required by any biblical law.
Like the adjacent Arabic culture (in the pre-Islamic period), the act of marriage appears mainly to have consisted of the groom fetching the bride, although among the Israelites (unlike the Arabs) the procession was a festive occasion, accompanied by music, dancing, and lights. To celebrate the marriage, week-long feasts were sometimes held.
In Old Testament times, a wife was regarded as chattel, belonging to her husband. The descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and bigamous men were expected to ensure that they give their first wife food, clothing, and sexual activity.
Since a wife was regarded as property, her husband was originally free to divorce her for any reason, at any time. A divorced couple could get back together unless the wife had married someone else after her divorce.
Jesus on marriage, divorce, and remarriage.
The Bible clearly addresses marriage and divorce. Those in troubled marriages are encouraged to seek counseling and restoration because most divorces are neither necessary nor unavoidable.
In both Matthew and Mark, Jesus appealed to God's will in creation. He builds upon the narrative in and where male and female are created together and for one another. Thus Jesus takes a firm stand on the permanence of marriage in the original will of God. This corresponds closely with the position of the Pharisee school of thought led by Shammai, at the start of the first millennium, with which Jesus would have been familiar. By contrast, Rabbinic Judaism subsequently took the opposite view, espoused by Hillel, the leader of the other major Pharisee school of thought at the time; in Hillel's view, men were allowed to divorce their wives for any reason.
Some hold that marriage vows are unbreakable, so that even in the distressing circumstances in which a couple separates, they are still married from God’s point of view. This is so in the Roman Catholic church, although occasionally it will declare a marriage to be null (in other words, it never really was a marriage). William Barclay (1907-1978) has written:
Jesus brought together two passages from Genesis, reinforcing the basic position on marriage found in Jewish scripture. Thus, he implicitly emphasized that it is God-made ("God has joined together"), "male and female," lifelong ("let no one separate"), and monogamous ("a man…his wife").
Jesus used the image of marriage and the family to teach the basics about the Kingdom of God. He inaugurated his ministry by blessing the wedding feast at Cana. In the Sermon on the Mount he set forth a new commandment concerning marriage, teaching that lustful looking constitutes adultery. He also superseded a Mosaic Law allowing divorce with his teaching that "…anyone who divorces his wife, except for sexual immorality (Gk. "porneia"), causes her to become an adulteress, and anyone who marries the divorced woman commits adultery". Similar Pauline teachings are found in . The exception clause—"except for…"—uses the Greek word "porneia" which is variously translated "fornication" (KJV), "marital unfaithfulness" (NIV 1984), "sexual immorality" (NIV 2011), "unchastity" (RSV), "et al". "The KJV New Testament Greek Lexicon, KJV" says "porneia" includes a variety of sexual "deviations" to include "illicit sexual intercourse, adultery, fornication, homosexuality, lesbianism, intercourse with animals, etc., sexual intercourse with close relatives…."
Theologian Frank Stagg says that manuscripts disagree as to the presence in the original text of the phrase "except for fornication". Stagg writes: "Divorce always represents failure…a deviation from God's will…. There is grace and redemption where there is contrition and repentance…. There is no clear authorization in the New Testament for remarriage after divorce." Stagg interprets the chief concern of as being "to condemn the criminal act of the man who divorces an innocent wife…. Jesus was rebuking the husband who victimizes an innocent wife and thinks that he makes it right with her by giving her a divorce". He points out that Jesus refused to be trapped by the Pharisees into choosing between the strict and liberal positions on divorce as held at the time in Judaism. When they asked him, "Is it lawful for a man to divorce his wife for any cause?" he answered by reaffirming God's will as stated in Genesis and , that in marriage husband and wife are made "one flesh", and what God has united man must not separate.
There is no evidence that Jesus himself ever married, and considerable evidence that he remained single. In contrast to Judaism and many other traditions, he taught that there is a place for voluntary singleness in Christian service. He believed marriage could be a distraction from an urgent mission, that he was living in a time of crisis and urgency where the Kingdom of God would be established where there would be no marriage nor giving in marriage:
New Testament beyond the Gospels.
The Apostle Paul quoted passages from Genesis almost verbatim in two of his New Testament books. He used marriage not only to describe the kingdom of God, as Jesus had done, but to define also the nature of the 1st-century Christian church. His theological view was a Christian development of the Old Testament parallel between marriage and the relationship between God and Israel. He analogized the church as a bride and Christ as the bridegroom─drawing parallels between Christian marriage and the relationship between Christ and the Church.
There is no hint in the New Testament that Jesus was ever married, and no clear evidence that Paul was ever married. However, both Jesus and Paul seem to view marriage as the preferred norm according to the purpose of God for Christians. They provide "exceptions" to being married because of extraordinary circumstances ("because of the impending crisis"), see also Pauline privilege. Their concerns were that marriage might be a distraction from the work of discipleship.
Some scholars have speculated that Paul may have been a widower since prior to his conversion to Christianity he was a Pharisee and member of the Sanhedrin, positions in which the social norm of the day required the men to be married. But it is just as likely that he never married at all.
Yet, Paul acknowledges the mutuality of marital relations, and recognizes that his own singleness is "a particular gift from God" that others may not necessarily have. "Now to the unmarried and the widows I say: It is good for them to stay unmarried, as I am. But if they cannot control themselves, they should marry, for it is better to marry than to burn with passion."
Paul indicates that bishops, deacons, and elders must be "husbands of one wife", and that women must have one husband. This is usually understood to legislate against polygamy rather than to require marriage:
In the Roman Age, female widows who did not remarry were considered more pure than those who did. Such widows were known as "one man woman" ("enos andros gune") in the epistles of Paul. Paul writes:
Paul allowed widows to remarry. Paul says that only "one-man women" older than 60 years can make the list of Christian widows who did special tasks in the community, but that younger widows should remarry to hinder sin.
Marriage and early Church Fathers.
Building on what they saw the example of Jesus and Paul advocating, some early Church Fathers placed less value on the family and saw celibacy and freedom from family ties as a preferable state.
Nicene Fathers such as Augustine believed that marriage was a sacrament because it was a symbol used by Paul to express Christ's love of the Church. However, there was also an apocalyptic dimension in his teaching, and he was clear that if everybody stopped marrying and having children that would be an admirable thing; it would mean that the Kingdom of God would return all the sooner and the world would come to an end. Such a view reflects the Manichaean past of Augustine.
While upholding the New Testament teaching that marriage is "honourable in all and the bed undefiled," Augustine believed that "yet, whenever it comes to the actual process of generation, the very embrace which is lawful and honourable cannot be effected without the ardour of lust...This is the carnal concupiscence, which, while it is no longer accounted sin in the regenerate, yet in no case happens to nature except from sin."
Both Tertullian and Gregory of Nyssa were church fathers who were married. They each stressed that the happiness of marriage was ultimately rooted in misery. They saw marriage as a state of bondage that could only be cured by celibacy. They wrote that at the very least, the virgin woman could expect release from the "governance of a husband and the chains of children."
Tertullian argued that second marriage, having been freed from the first by death,"will have to be termed no other than a species of fornication," partly based on the reasoning that this involves desiring to marry a woman out of sexual ardor, which a Christian convert is to avoid.
Also advocating celibacy and virginity as preferable alternatives to marriage, Jerome wrote: "It is not disparaging wedlock to prefer virginity. No one can make a comparison between two things if one is good and the other evil." On First Corinthians 7:1 he reasons, "It is good, he says, for a man not to touch a woman. If it is good not to touch a woman, it is bad to touch one: for there is no opposite to goodness but badness. But if it be bad and the evil is pardoned, the reason for the concession is to prevent worse evil."
St. John Chrysostom wrote: "...virginity is better than marriage, however good... Celibacy is...an imitation of the angels. Therefore, virginity is as much more honorable than marriage, as the angel is higher than man. But why do I say angel? Christ, Himself, is the glory of virginity."
Cyprian, Bishop of Carthage, said that the first commandment given to men was to increase and multiply, but now that the earth was full there was no need to continue this process of multiplication.
This view of marriage was reflected in the lack of any formal liturgy formulated for marriage in the early Church. No special ceremonial was devised to celebrate Christian marriage—despite the fact that the Church had produced liturgies to celebrate the Eucharist, Baptism and Confirmation. It was not important for a couple to have their nuptials blessed by a priest. People could marry by mutual agreement in the presence of witnesses.
At first, the old Roman pagan rite was used by Christians, although modified superficially. The first detailed account of a Christian wedding in the West dates from the 9th century. This system, known as Spousals, persisted after the Reformation.
Denominational beliefs and practice.
Catholic Church.
The Catholic Church teaches that God himself is the author of the sacred institution of marriage, which is His way of showing love for those He created. Marriage is a divine institution that can never be broken, even if the husband or wife legally divorce in the civil courts; as long as they are both alive, the Church considers them bound together by God. Holy Matrimony is another name for sacramental marriage.
Marriage is intended to be a faithful, exclusive, lifelong union of a man and a woman. Committing themselves completely to each other, a Catholic husband and wife strive to sanctify each other, bring children into the world, and educate them in the Catholic way of life. Man and woman, although created differently from each other, complement each other. This complementarity draws them together in a mutually loving union.
The valid marriage of baptized Christians is one of the seven Catholic sacraments. The sacrament of marriage is the only sacrament that a priest does not administer directly; a priest, however, is the chief witnesses of the husband and wife's administration of the sacrament to each other at the wedding ceremony in a Catholic church.
The Catholic Church views that Christ himself established the sacrament of marriage at the wedding feast of Cana; therefore, since it is a divine institution, neither the Church nor state can alter the basic meaning and structure of marriage. Husband and wife give themselves totally to each other in a union that lasts until death.
Priests are instructed that marriage is part of God's natural law and to support the couple if they do choose to marry. Today it is common for Catholics to enter into a "mixed marriage" between a Catholic and a baptized non-Catholic. Couples entering into a mixed marriage are usually allowed to marry in a Catholic church provided their decision is of their own accord and they intend to remain together for life, to be faithful to each other, and to have children which are brought up in the Catholic faith.
In Catholicism, marriage has two ends: the good of the spouses themselves, and the procreation and education of children (1983 code of canon law, c.1055; 1994 catechism, par.2363). Hence "entering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment." It is normal procedure for a priest to ask the prospective bride and groom about their plans to have children before officiating at their wedding. The Catholic Church may refuse to marry anyone unwilling to have children, since procreation by "the marriage act" is a fundamental part of marriage. Thus usage of any form of contraception, in vitro fertilization, or birth control besides Natural Family Planning is a grave offense against the sanctity of marriage and ultimately against God.
Protestants.
Purposes.
Essentially all Protestant denominations hold marriage to be ordained by God for the union between a man and a woman. They see the primary purposes of this union as intimate companionship, rearing children and mutual support for both husband and wife to fulfill their life callings. Protestants generally approve of birth control and consider marital sexual pleasure to be a gift of God. While condoning divorce only under limited circumstances, most Protestant churches allow for divorce and remarriage.
Conservative Protestants take a stricter view of the nature of marriage. They consider marriage a solemn covenant between wife, husband and God. Most view sexual relations as appropriate only within a marriage. Divorce is permissible, if at all, only in very specific circumstances (for example, sexual immorality or abandonment by the non-believer).
Roles and responsibilities.
Roles and responsibilities of husband and wives now vary considerably on a continuum between the long-held male dominant/female submission view and a shift toward equality (without sameness) of the woman and the man. There is considerable debate among many Christians today—not just Protestants—whether equality of husband and wife or male headship is the biblically ordained view, and even if it is biblically permissible. The divergent opinions fall into two main groups: Complementarians (who call for husband-headship and wife-submission) and Christian Egalitarians (who believe in full partnership equality in which couples can discover and negotiate roles and responsibilities in marriage).
There is no debate that presents a historically benevolent husband-headship/wife-submission model for marriage. The questions are (a) how these New Testament household codes are to be reconciled with the calls earlier in Chapter 5 (cf. verses 1, 18, 21) for mutual submission among all believers, and (b) the meaning of "head" in v.23. It is important to note that verse 22 contains no verb in the original manuscripts:
 (NIV)
Eastern Orthodox Church.
In Eastern Orthodoxy, marriage is treated as a Sacred Mystery (sacrament), and as an ordination. It serves to unite a woman and a man in eternal union before God. It refers to the 1st centuries of the church, where spiritual union of spouses in the first sacramental marriage was eternal. Therefore, it is considered a martyrdom as each spouse learns to die to self for the sake of the other. Like all Mysteries, Orthodox marriage is more than just a celebration of something which already exists: it is the creation of something new, the imparting to the couple of the grace which transforms them from a 'couple' into husband and wife within the Body of Christ.
Marriage is an icon (image) of the relationship between Jesus and the Church. This is somewhat akin to the Old Testament prophets' use of marriage as an analogy to describe the relationship between God and Israel. Marriage is the simplest, most basic unity of the church: a congregation where "two or three are gathered together in Jesus' name." The home is considered a consecrated space (the ritual for the Blessing of a House is based upon that of the Consecration of a Church), and the husband and wife are considered the ministers of that congregation. However, they do not "perform" the Sacraments in the house church; they "live" the Sacrament of Marriage. Because marriage is considered to be a pilgrimage wherein the couple walk side by side toward the Kingdom of Heaven, marriage to a non-Orthodox partner is discouraged, though it may be permitted.
Unlike Western Christianity, Eastern Christians do not consider the sacramental aspect of the marriage to be conferred by the couple themselves. Rather, the marriage is conferred by the action of the Holy Spirit acting through the priest. Furthermore, no one besides a bishop or priest—not even a deacon—may perform the Sacred Mystery.
The external sign of the marriage is the placing of wedding crowns upon the heads of the couple, and their sharing in a "Common Cup" of wine. Once crowned, the couple walk a circle three times in a ceremonial "dance" in the middle of the church, while the choir intones a joyous three-part antiphonal hymn, "Dance, Isaiah"
The sharing of the Common Cup symbolizes the transformation of their union from a common marriage into a sacred union. The wedding is usually performed after the Divine Liturgy at which the couple receives Holy Communion. Traditionally, the wedding couple would wear their wedding crowns for eight days, and there is a special prayer said by the priest at the removal of the crowns.
Divorce is discouraged. Sometimes out of "economia" (mercy) a marriage may be dissolved if there is no hope whatever for a marriage to fulfill even a semblance of its intended sacramental character. The standard formula for remarriage is that the Orthodox Church joyfully blesses the first marriage, merely performs the second, barely tolerates the third, and invariably forbids the fourth.
Early church texts forbid marriage between an Orthodox Christian and a heretic or schismatic (which would include all non-Orthodox Christians). Traditional Orthodox Christians forbid mixed marriages with other denominations. More liberal ones perform them, provided that the couple formally commit themselves to rearing their children in the Orthodox faith.
All people are called to celibacy—human beings are all born into virginity, and Orthodox Christians are expected by Sacred Tradition to remain in that state unless they are called into marriage and that call is sanctified. The church blesses two paths on the journey to salvation: monasticism and marriage. Mere celibacy, without the sanctification of monasticism, can fall into selfishness and tends to be regarded with disfavour by the Church.
Orthodox priests who serve in parishes are usually married. They must marry prior to their ordination. If they marry after they are ordained they are not permitted to continue performing sacraments. If their wife dies, they are forbidden to remarry; if they do, they may no longer serve as a priest. A married man may be ordained as a priest or deacon. However, a priest or deacon is not permitted to enter into matrimony after ordination. Bishops must always be monks and are thus celibate. However, if a married priest is widowed, he may receive monastic tonsure and thus become eligible for the episcopate.
The Eastern Orthodox Church believes that marriage is an eternal union of spouses, but in Heaven there will not be a procreative bond of marriage.
Oriental Orthodox Church.
The Oriental Orthodox Churches hold views almost identical to those of the Eastern Orthodox Churches. The Coptic Orthodox Church of Alexandria allows second marriages only in cases of adultery or death of spouse.
Non-Trinitarian denominations.
The Church of Jesus Christ of Latter-day Saints.
In the teachings of The Church of Jesus Christ of Latter-day Saints (LDS Church), celestial (or eternal) marriage is a covenant between a man, a woman, and God performed by a priesthood authority in a temple of the church. Celestial marriage is intended to continue forever into the afterlife if the man and woman do not break their covenants. Thus, eternally married couples are often referred to as being "sealed" to each other. Sealed couples who keep their covenants are also promised to have their posterity sealed to them in the afterlife. (Thus, the slogan of the LDS Church: "families are forever.") A celestial marriage is considered a requirement for exaltation.
In some countries, celestial marriages can be recognized as civil marriages; in other cases, couples are civilly married outside of the temple and are later sealed in a celestial marriage. (The church will no longer perform a celestial marriage on a couple unless they are first (or simultaneously) legally married.) The church encourages its members to be in good standing with it so that they may marry or be sealed in the temple. A celestial marriage is not annulled by a civil divorce: a "cancellation of a sealing" may be granted, but only by the First Presidency, the highest authority in the church. Civil divorce and marriage outside the temple carries somewhat of a stigma in the Mormon culture; the church teaches that the "gospel of Jesus Christ—including repentance, forgiveness, integrity, and love—provides the remedy for conflict in marriage." Regarding marriage and divorce, the church instructs its leaders: "No priesthood officer is to counsel a person whom to marry. Nor should he counsel a person to divorce his or her spouse. Those decisions must originate and remain with the individual. When a marriage ends in divorce, or if a husband and wife separate, they should always receive counseling from Church leaders."
In church temples, members of the LDS Church perform vicarious celestial marriages for deceased couples who were legally married.
New Church (or Swedenborgian Church).
The New Church teaches that marriage love (sometimes translated "conjugial love") is "the precious jewel of human life and the repository of the Christian religion" because the love shared between a husband and a wife is the source of all peace and joy. Emanuel Swedenborg coined the term "conjugial" (not to be confused with the more general term for marriage, "conjugal.") to describe the special love experienced by married partners. When a husband and wife work together to build their marriage on earth, that marriage continues after the death of their bodies and they live as angels in heaven into eternity. Swedenborg claimed to have spoken to angel couples who had been married for thousands of years. Those who never married in the natural world will, if they wish, find a spouse in heaven.
Jehovah's Witnesses.
The Jehovah's Witnesses view marriage to be a permanent arrangement with the only possible exception being adultery. Divorce is strongly discouraged even when adultery is committed since the wronged spouse is free to forgive the unfaithful one. There are provisions for a domestic separation in the event of "failure to provide for one's household" and domestic violence, or spiritual resistance on the part of a partner. Even in such situations though divorce would be considered grounds for loss of privileges in the congregation. Remarrying after death or a proper divorce is permitted. Marriage is the only situation where any type of sexual interaction is acceptable, and even then certain restrictions apply to acts such as oral and anal sex. Married persons who are known to commit such acts may in fact lose privileges in the congregation as they are supposed to be setting a good example to the congregation.
Same-sex marriage.
A small number of mainline Protestant denominations such as the Episcopalians, the United Church of Christ, the United Church of Canada and some non-trinitarian denominations perform weddings between same-sex couples. Other churches perform ceremonies blessing same sex unions, but do not refer to them as marriages. The Roman Catholic Church, the Orthodox Christian Church, and the vast majority of Protestant denominations do not perform or recognize same-sex marriage because they do not consider it as marriage at all. Whether or not to bless same-sex marriages and unions is a matter of debate within a few Protestant denominations.

</doc>
<doc id="7392" url="http://en.wikipedia.org/wiki?curid=7392" title="Class (computer programming)">
Class (computer programming)

In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions, methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (subroutine that creates objects), and as the type of objects generated by the type, and these distinct concepts are easily conflated.
When an object is created by a constructor of the class, the resulting object is called an "instance" of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.
In some languages, classes are only a compile-time feature (new classes cannot be declared at runtime), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type codice_1 or similar). In these languages, a class that creates classes is called a metaclass.
Class vs. type.
In casual use people often refer to the "class" of an object, but narrowly speaking objects have "type" – the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy – while a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation. In type theory terms, a class is an implementation – a "concrete" data structure and collection of subroutines – while a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system) – for example, one might implement the type codice_2 with two classes, codice_3 (fast for small stacks, but scales poorly) and codice_4 (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.
Types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a codice_5 type would represent the properties and functionality of bananas in general, the codice_6 and codice_7 classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The codice_6 class could then produce particular bananas: instances of the codice_6 class would be objects of type codice_5. Often only a single implementation of a type is given, in which case the class name is often identical with the type name.
Design and implementation.
Classes are composed from structural and behavioral constituents. Programming languages that include classes as a programming construct offer support for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.
Structure.
A class contains data field descriptions (or "properties", "fields", "data members", or "attributes"). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.
Some programming languages support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.
Behavior.
The behavior of class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it. Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods—such as constructors, destructors, and conversion operators—are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.
The concept of class interface.
Every class implements (or "realizes") an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented. There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing implementation.
Languages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from. In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.
Object-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object. 
Example.
The buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the "power" button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together comprise the interface. (Other television sets that are the same model as yours would have the same interface.) In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.
A television set also has a myriad of "attributes", such as size and whether it supports color, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).
Getting the total number of televisions manufactured could be a "static method" of the television class. This method is clearly associated with the class, yet is outside the domain of each individual instance of the class. Another example would be a static method that finds a particular instance out of the set of all television objects.
Member accessibility.
The following is a common set of access specifiers:
Although many object-oriented languages support the above access specifiers, their semantics may differ.
Object-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants—constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.
Access specifiers do not necessarily control visibility, in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from client code will be prevented by the type checker.
The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile.
 In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.
Some languages feature other accessibility schemes:
Inter-class relationships.
In addition to the design of standalone classes, programming languages may support more advanced class design based upon relationships between classes. The inter-class relationship design capabilities commonly provided are "compositional" and "hierarchical".
Compositional.
Classes can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a has-a relationship. For example, a class "Car" could be composed of and contain a class "Engine". Therefore, a Car has an Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime. For example, in Objective-C 2.0:
This codice_11 class has an instance of codice_12 (string object), codice_13, and codice_14 (array object).
Hierarchical.
Classes can be "derived" from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes ("base classes", "parent classes" or "superclasses") and the derived class ("child class" or "subclass") . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship. For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a Control. Structural and behavioral members of the parent classes are "inherited" by the child class. Derived classes can define additional structural members (data fields) and/or behavioral members (methods) in addition to those that they "inherit" and are therefore "specializations" of their superclasses. Also, derived classes can override inherited methods if the language allows.
Not all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class. If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.
Example (Simplified Objective-C 2.0 code, from iPhone SDK):
In this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.
Definitions of subclass.
Conceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve codice_15 as a superclass of codice_16 and codice_17, while codice_18 would be a subclass of codice_16. These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.
A common conceptual error is to mistake a "part of" relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the component parts of the car as subclass relations. E.g., a car is composed of an engine and body but it would not be appropriate to model engine or body as a subclass of car.
In object-oriented modeling these kinds of relations are typically modeled as object properties. In this example the Car class would have a property called codice_20. codice_20 would be typed to hold a collection of objects such as instances of codice_22.
Object modeling languages such as UML include capabilities to model various aspects of part of and other kinds of relations. Data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code beside the basic data definitions for the objects. Things such as error checking on get and set methods.
One important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets it would be rare to find sets that didn't intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.
Most modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.
However, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.
A similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rational is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.
Orthogonality of the class concept and inheritance.
Although class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as "object-based languages", support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.
Within object-oriented analysis.
In object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship. Associations may be labeled according to their name or purpose.
An association role is given end of an association and describes the role of the corresponding class. For example, a "subscriber" role describes the way instances of the class "Person" participate in a "subscribes-to" association with the class "Magazine". Also, a "Magazine" has the "subscribed magazine" role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are "0..1", "1..1", "1..*" and "0..*", where the "*" specifies any number of instances.
Taxonomy of classes.
There are many categories of classes; however, these categories do not necessarily divide classes into distinct partitions.
Abstract and concrete.
In a language that supports inheritance, an abstract class, or "abstract base class" (ABC), is a class that cannot be instantiated because it is either labeled as abstract or it simply specifies abstract methods (or "virtual methods"). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.
Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java and PHP, the keyword "abstract" is used. In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).
A class consisting of only virtual methods is called a Pure Abstract Base Class (or "Pure ABC") in C++ and is also known as an "interface" by users of the language. Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods. 
A concrete class is a class that can be instantiated, as opposed to abstract classes, which cannot. 
Local and inner.
In some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.
An Inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is "inner types", also known as "inner data type" or "nested type", which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via "typedef" declarations).
Another type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.
Metaclasses.
Metaclasses are classes whose instances are classes. A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.
In some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass which is built into the language.
The Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses.
Non-subclassable.
Non-subclassable classes allow programmers to design classes and hierarchies of classes which at some level in the hierarchy, further derivation is prohibited. (A stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to "abstract" classes, which imply, encourage, and require derivation in order to be used at all. A non-subclassable class is implicitly "concrete".
A non-subclassable class is created by declaring the class as codice_23 in C# or as codice_24 in Java or PHP.
For example, Java's class is designated as "final".
Non-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes.
Partial.
In languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.
The primary motivation for introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it "unifies" all the parts of a partial class. Then, compilation can proceed as usual.
Other benefits and effects of the partial class feature include:
Partial classes have existed in Smalltalk under the name of "Class Extensions" for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.
Example in VB.NET.
This simple example, written in Visual Basic .NET, shows how parts of the same class are defined in two different files.
When compiled, the result is the same as if the two files were written as one, like this:
Example in Objective-C.
In Objective-C, partial classes, aka categories may even spread over multiple libraries and executables, like this example:
In Foundation, header file NSData.h:
In user-supplied library, a separate binary from Foundation framework, header file NSData+base64.h:
And in an app, yet another separate binary file, source code file main.m:
The dispatcher will find both methods called over the NSData instance and invoke both of them correctly.
Uninstantiable.
Uninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.
For example, in C#, a class marked "static" can not be instantiated, can only have static members (fields, methods, other), may not have "instance constructors", and is "sealed".
Unnamed.
An unnamed class or anonymous class is a class which is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.
Benefits.
The benefits of organizing software into object classes fall into three categories:
Object classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.
Object classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and it's component parts. This reduces the potential for unwanted side effects from maintenance enhancements.
Software re-use is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailor some aspect of the behavior or data accordingly. Re-use via interfaces (aka methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.
These benefits come with a cost of course. One of most serious obstacles to using object classes has been performance. Interpreted environments that support languages such as Smalltalk and CLOS provided rapid development but the resulting code was not nearly as fast as what could be achieved in some procedural languages such as C. This has been partly addressed by the development of object-oriented languages that are not interpreted such as C++ and Java. Also, due to Moore's law the processing power of computers has increased to the point where efficient code is not as critical for most systems as it was in the past. Still, no matter how well designed the language, there will always be an inevitable bit of required extra overhead to create a class rather than use procedural code and in some circumstances, especially where performance or memory are required to be optimal, that using object classes may not be the best approach.
Also, getting the benefits of object classes requires that they be used appropriately and that requires training. Without the proper training developers may simply code procedural programs in an object-oriented environment and end up with the worst of both worlds.
Run-time representation.
As a data type, a class is usually considered as a compile-time construct. A language may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.
For example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject.

</doc>
<doc id="7394" url="http://en.wikipedia.org/wiki?curid=7394" title="Canterbury (disambiguation)">
Canterbury (disambiguation)

Canterbury is a city located in the county of Kent in southeast England. It may also refer to:

</doc>
<doc id="7397" url="http://en.wikipedia.org/wiki?curid=7397" title="Color blindness">
Color blindness

Color blindness, or color vision deficiency, is the inability or decreased ability to see color, or perceive color differences, under normal lighting conditions. Color blindness affects a significant percentage of the population. There is no actual blindness but there is a deficiency of color vision. The most usual cause is a fault in the development of one or more sets of retinal cones that perceive color in light and transmit that information to the optic nerve. This type of color blindness is usually a sex-linked condition. The genes that produce photopigments are carried on the X chromosome; if some of these genes are missing or damaged, color blindness will be expressed in males with a higher probability than in females because males only have one X chromosome (in females, a functional gene on only one of the two X chromosomes is sufficient to yield the needed photopigments).
Color blindness can also be produced by physical or chemical damage to the eye, the optic nerve, or parts of the brain. For example, people with achromatopsia suffer from a completely different disorder, but are nevertheless unable to see colors.
The English chemist John Dalton published the first scientific paper on this subject in 1798, "Extraordinary facts relating to the vision of colours", after the realization of his own color blindness. Because of Dalton's work, the general condition has been called "daltonism", although in English this term is now used more narrowly for deuteranopia alone.
Color blindness is usually classified as a mild disability, however there are occasional circumstances where it can give an advantage. Some studies conclude that color blind people are better at penetrating certain color camouflages. Such findings may give an evolutionary reason for the high prevalence of red–green color blindness. There is also a study suggesting that people with some types of color blindness can distinguish colors that people with normal color vision are not able to distinguish.
Background.
Color blindness affects a large number of individuals, with protanopia and deuteranopia being the most common types. In individuals with Northern European ancestry, as many as 8 percent of men and 0.5 percent of women experience the common form of red-green color blindness. The typical human retina contains two kinds of light cells: the rod cells (active in low light) and the cone cells (active in normal daylight). Normally, there are three kinds of cone cells, each containing a different pigment, which are activated when the pigments absorb light. The spectral sensitivities of the cones differ; one is most sensitive to short wavelengths, one to medium wavelengths, and the third to medium-to-long wavelengths within the visible spectrum, with their peak sensitivities in the blue, green, and yellow-green regions of the spectrum, respectively. The absorption spectra of the three systems overlap, and combine to cover the visible spectrum. These receptors are often called S cones, M cones, and L cones, for short, medium, and long wavelength; but they are also often referred to as blue cones, green cones, and red cones, respectively.
Although these receptors are often referred to as "blue, green, and red" receptors, this terminology is inaccurate. The receptors are each responsive to a wide range of wavelengths. For example, the long wavelength, "red", receptor has its peak sensitivity in the yellow-green, some way from the red end (longest wavelength) of the visible spectrum. The sensitivity of normal color vision actually depends on the overlap between the absorption ranges of the three systems: different colors are recognized when the different types of cone are stimulated to different degrees. Red light, for example, stimulates the long wavelength cones much more than either of the others, and reducing the wavelength causes the other two cone systems to be increasingly stimulated, causing a gradual change in hue.
Many of the genes involved in color vision are on the X chromosome, making color blindness much more common in males than in females because males only have one X chromosome, while females have two. Because this is an X-linked trait, an estimated 2–3% of women have a 4th color cone and can be considered tetrachromats, although it is not clear that this provides an advantage in color discrimination.
Classification.
By cause.
Color vision deficiencies can be classified as acquired or inherited.
By clinical appearance.
Based on clinical appearance, color blindness may be described as total or partial. Total color blindness is much less common than partial color blindness. There are two major types of color blindness: those who have difficulty distinguishing between red and green, and who have difficulty distinguishing between blue and yellow.
Immunofluorescent imaging is a way to determine red-green color coding. Conventional color coding is difficult for individuals with red-green color blindness (protanopia or deuteranopia) to discriminate. Replacing red with magenta (top) or green with turquoise (bottom) improves visibility for such individuals.
Causes.
Genetics.
Color blindness can be inherited. It is most commonly inherited from mutations on the X chromosome but the mapping of the human genome has shown there are many causative mutations—mutations capable of causing color blindness originate from at least 19 different chromosomes and 56 different genes (as shown online at the Online Mendelian Inheritance in Man (OMIM) database at Johns Hopkins University).
Two of the most common inherited forms of color blindness are protanopia, and deuteranopia.
One of the common color vision defects is the red-green deficiency which is present in about 8 percent of males and 0.5 percent of females of Northern European ancestry.
Some of the inherited diseases known to cause color blindness are:
Inherited color blindness can be congenital (from birth), or it can commence in childhood or adulthood. Depending on the mutation, it can be stationary, that is, remain the same throughout a person's lifetime, or progressive. As progressive phenotypes involve deterioration of the retina and other parts of the eye, certain forms of color blindness can progress to legal blindness, i.e., an acuity of 6/60 or worse, and often leave a person with complete blindness.
Color blindness always pertains to the cone photoreceptors in retinas, as the cones are capable of detecting the color frequencies of light.
About 8 percent of males, but only 0.5 percent of females, are color blind in some way or another, whether it is one color, a color combination, or another mutation. The reason males are at a greater risk of inheriting an X linked mutation is that males only have one X chromosome (XY, with the Y chromosome carrying altogether different genes than the X chromosome), and females have two (XX); if a woman inherits a normal X chromosome in addition to the one that carries the mutation, she will not display the mutation. Men do not have a second X chromosome to override the chromosome that carries the mutation. If 5% of variants of a given gene are defective, the probability of a single copy being defective is 5%, but the probability that two copies are both defective is 0.05 × 0.05 = 0.0025, or just 0.25%.
Other causes.
Other causes of color blindness include brain or retinal damage caused by shaken baby syndrome, accidents and other trauma which produce swelling of the brain in the occipital lobe, and damage to the retina caused by exposure to ultraviolet light (10–300 nm). Damage often presents itself later on in life.
Color blindness may also present itself in the spectrum of degenerative diseases of the eye, such as age-related macular degeneration, and as part of the retinal damage caused by diabetes. Another factor that may affect color blindness includes a deficiency in Vitamin A.
Types.
The different kinds of inherited color blindness result from partial or complete loss of function of one or more of the different cone systems. When one cone system is compromised, dichromacy results. The most frequent forms of human color blindness result from problems with either the middle or long wavelength sensitive cone systems, and involve difficulties in discriminating reds, yellows, and greens from one another. They are collectively referred to as "red–green color blindness", though the term is an over-simplification and is somewhat misleading. Other forms of color blindness are much more rare. They include problems in discriminating blues from greens and yellows from reds/pinks, and the rarest forms of all, complete color blindness or "monochromacy", where one cannot distinguish any color from grey, as in a black-and-white movie or photograph.
Congenital.
Congenital color vision deficiencies are subdivided based on the number of primary hues needed to match a given sample in the visible spectrum.
Monochromacy.
Monochromacy is the condition of possessing only a single channel for conveying information about color. Monochromats possess a complete inability to distinguish any colors and perceive only variations in brightness. It occurs in two primary forms:
Dichromacy.
<br>
Protanopes, deuteranopes, and tritanopes are dichromats; that is, they can match any color they see with some mixture of just two primary colors (whereas normally humans are trichromats and require three primary colors). These individuals normally know they have a color vision problem and it can affect their lives on a daily basis. Two percent of the male population exhibit severe difficulties distinguishing between red, orange, yellow, and green. A certain pair of colors, that seem very different to a normal viewer, appear to be the same color (or different shades of same color) for a such dichromat. The terms protanopia, deuteranopia, and tritanopia come from Greek and literally mean "inability to see ("anopia") with the first ("prot-"), second ("deuter-"), or third ("trit-") [cone]", respectively.
Anomalous trichromacy.
Those with protanomaly, deuteranomaly, or tritanomaly are trichromats, but the color matches they make differ from the normal. They are called anomalous trichromats. In order to match a given spectral yellow light, protanomalous observers need more red light in a red/green mixture than a normal observer, and deuteranomalous observers need more green. From a practical standpoint though, many protanomalous and deuteranomalous people have very little difficulty carrying out tasks that require normal color vision. Some may not even be aware that their color perception is in any way different from normal.
Protanomaly and deuteranomaly can be diagnosed using an instrument called an anomaloscope, which mixes spectral red and green lights in variable proportions, for comparison with a fixed spectral yellow. If this is done in front of a large audience of males, as the proportion of red is increased from a low value, first a small proportion of the audience will declare a match, while most will see the mixed light as greenish; these are the deuteranomalous observers. Next, as more red is added the majority will say that a match has been achieved. Finally, as yet more red is added, the remaining, protanomalous, observers will declare a match at a point where normal observers will see the mixed light as definitely reddish.
Total color blindness.
"Achromatopsia" is strictly defined as the inability to see color. Although the term may refer to acquired disorders such as cerebral achromatopsia also known as color agnosia, it typically refers to congenital color vision disorders (i.e. more frequently rod monochromacy and less frequently cone monochromacy).
In cerebral achromatopsia, a person cannot perceive colors even though the eyes are capable of distinguishing them. Some sources do not consider these to be true color blindness, because the failure is of perception, not of vision. They are forms of visual agnosia.
Red–green color blindness.
Protanopia, deuteranopia, protanomaly, and deuteranomaly are widely common inherited color blindness that affects a substantial portion of the human population, in which those affected have difficulty with discriminating red and green hues due to the absence or mutation of the red or green retinal photoreceptors. It is sex-linked: genetic red–green color blindness affects males much more often than females, because the genes for the red and green color receptors are located on the X chromosome, of which males have only one and females have two. Females (46, XX) are red–green color blind only if "both" their X chromosomes are defective with a similar deficiency, whereas males (46, XY) are color blind if their single X chromosome is defective.
The gene for red–green color blindness is transmitted from a color blind male to all his daughters who are heterozygote carriers and are usually unaffected. In turn, a carrier woman has a fifty percent chance of passing on a mutated X chromosome region to each of her male offspring. The sons of an affected male will not inherit the trait from him, since they receive his Y chromosome and not his (defective) X chromosome. Should an affected male have children with a carrier or colorblind woman, their daughters may be colorblind by inheriting an affected X chromosome from each parent.
Because one X chromosome is inactivated at random in each cell during a woman's development, it is possible for her to have four different cone types, as when a carrier of protanomaly has a child with a deuteranomalic man. Denoting the normal vision alleles by P and D and the anomalous by p and d, the carrier is PD pD and the man is Pd. The daughter is either PD Pd or pD Pd. Suppose she is pD Pd. Each cell in her body expresses either her mother's chromosome pD or her father's Pd. Thus her red–green sensing will involve both the normal and the anomalous pigments for both colors. Such females are tetrachromats, since they require a mixture of four spectral lights to match an arbitrary light.
Red-green color blindness can be caused by ethambutol.
Blue–yellow color blindness.
Those with tritanopia and tritanomaly have difficulty discriminating between bluish and greenish hues, as well as yellowish and reddish hues.
Color blindness involving the inactivation of the short-wavelength sensitive cone system (whose absorption spectrum peaks in the bluish-violet) is called tritanopia or, loosely, blue–yellow color blindness. The tritanopes neutral point occurs near a yellowish 570 nm; green is perceived at shorter wavelengths and red at longer wavelengths. Mutation of the short-wavelength sensitive cones is called tritanomaly. Tritanopia is equally distributed among males and females. Jeremy H. Nathans (with the Howard Hughes Medical Institute) demonstrated that the gene coding for the blue receptor lies on chromosome 7, which is shared equally by males and females. Therefore it is not sex-linked. This gene does not have any neighbor whose DNA sequence is similar. Blue color blindness is caused by a simple mutation in this gene.
Diagnosis.
The Ishihara color test, which consists of a series of pictures of colored spots, is the test most often used to diagnose red–green color deficiencies. A figure (usually one or more Arabic digits) is embedded in the picture as a number of spots in a slightly different color, and can be seen with normal color vision, but not with a particular color defect. The full set of tests has a variety of figure/background color combinations, and enable diagnosis of which particular visual defect is present. The anomaloscope, described above, is also used in diagnosing anomalous trichromacy.
Because the Ishihara color test contains only numerals, it may not be useful in diagnosing young children, who have not yet learned to use numerals. In the interest of identifying these problems early on in life, alternative color vision tests were developed using only symbols (square, circle, car).
Besides the Ishihara color test, the US Navy and US Army also allow testing with the Farnsworth Lantern Test. This test allows 30% of color deficient individuals, whose deficiency is not too severe, to pass.
Another test used by clinicians to measure chromatic discrimination is the Farnsworth-Munsell 100 hue test. The patient is asked to arrange a set of colored caps or chips to form a gradual transition of color between two anchor caps.
The HRR color test (developed by Hardy, Rand, and Rittler) is a red-green color test that, unlike the Ishihara, also has plates for the detection of the tritan defects.
Most clinical tests are designed to be fast, simple, and effective at identifying broad categories of color blindness. In academic studies of color blindness, on the other hand, there is more interest in developing flexible tests to collect thorough datasets, identify copunctal points, and measure just noticeable differences.
Management.
There is generally no treatment to cure color deficiencies. Optometrists can supply colored spectacle lenses or a single red-tint contact lens to wear on the non-dominant eye but, although this may improve discrimination of some colors, it can make other colors more difficult to distinguish. A 1981 review of various studies to evaluate the effect of the X-chrom contact lens concluded that, while the lens may allow the wearer to achieve a better score on certain color vision tests, it did not correct color vision in the natural environment.
The GNOME desktop environment provides colorblind accessibility using the gnome-mag and the libcolorblind software. Using a gnome applet, the user may switch a color filter on and off, choosing from a set of possible color transformations that will displace the colors in order to disambiguate them. The software enables, for instance, a colorblind person to see the numbers in the Ishihara test.
Many applications for iPhone and iPad have been developed to help colorblind people to view the colors in a better way. Many applications launch a sort of simulation of colorblind vision to make normal-view people understand how the colorblinds see the world. Other ones allow a correction of the image grabbed from the camera with a special "daltonizer" algorithm.
In September 2009, the journal "Nature" reported that researchers at the University of Washington and University of Florida were able to give trichromatic vision to squirrel monkeys, which normally have only dichromatic vision, using gene therapy.
In 2003, a cybernetic device called eyeborg was developed to allow the wearer to hear sounds representing different colors. Achromatopsic artist Neil Harbisson was the first to use such a device in early 2004; the eyeborg allowed him to start painting in color by memorizing the sound corresponding to each color. In 2012, at a TED Conference, Harbisson explained how he could now perceive colors outside the ability of human vision.
Portuguese Designer Miguel Neiva developed a code system, named ColorADD®, based on 5 basic shapes that, when combined, make it easier to identify various colors for colorblind people.
Its use is currently expanding in Portugal (hospitals, transportation, education) and in other countries.
Epidemiology.
Color blindness affects a significant number of people, although exact proportions vary among groups. In Australia, for example, it occurs in about 8 percent of males and only about 0.4 percent of females. Isolated communities with a restricted gene pool sometimes produce high proportions of color blindness, including the less usual types. Examples include rural Finland, Hungary, and some of the Scottish islands. In the United States, about 7 percent of the male population—or about 10.5 million men—and 0.4 percent of the female population either cannot distinguish red from green, or see red and green differently from how others do (Howard Hughes Medical Institute, 2006). More than 95 percent of all variations in human color vision involve the red and green receptors in male eyes. It is very rare for males or females to be "blind" to the blue end of the spectrum.
Society and culture.
Design implications of color blindness.
Color codes present particular problems for those with color deficiencies as they are often difficult or impossible for them to perceive.
Good graphic design avoids using color coding or using color contrasts alone to express information; this not only helps color blind people, but also aids understanding by normally sighted people.
Designers need to take into account that color-blindness is highly sensitive to differences in material. For example, a red–green colorblind person who is incapable of distinguishing colors on a map printed on paper may have no such difficulty when viewing the map on a computer screen or television. In addition, some color blind people find it easier to distinguish problem colors on artificial materials, such as plastic or in acrylic paints, than on natural materials, such as paper or wood. Third, for some color blind people, color can only be distinguished if there is a sufficient "mass" of color: thin lines might appear black, while a thicker line of the same color can be perceived as having color.
Designers should also note that red-blue and yellow-blue color combinations are generally safe. So instead of the ever popular "red means bad and green means good" system, using these combinations can lead to a much higher ability to use color coding effectively. This will still cause problems for those with monochromatic color blindness, but it is still something worth considering.
When the need to process visual information as rapidly as possible arises, for example in an emergency situation, the visual system may operate only in shades of gray, with the extra information load in adding color being dropped. This is an important possibility to consider when designing, for example, emergency brake handles or emergency phones.
Occupations.
Color blindness may make it difficult or impossible for a person to engage in certain occupations. Persons with color blindness may be legally or practically barred from occupations in which color perception is an essential part of the job ("e.g.," mixing paint colors), or in which color perception is important for safety ("e.g.," operating vehicles in response to color-coded signals). This occupational safety principle originates from the Lagerlunda train crash of 1875 in Sweden. Following the crash, Professor Alarik Frithiof Holmgren, a physiologist, investigated and concluded that the color blindness of the engineer (who had died) had caused the crash. Professor Holmgren then created the first test using different-colored skeins to exclude people from jobs in the transportation industry on the basis of color blindness. However there is a claim that there is no ﬁrm evidence that color deﬁciency did cause the collision, and that it might have not been the sole cause.
Color vision is important for occupations using telephone or computer networking cabling, as the individual wires inside the cables are color-coded using green, orange, brown, blue and white colors. Electronic wiring, transformers, resistors, and capacitors are color-coded as well, using black, brown, red, orange, green, yellow, blue, violet, gray, white, silver, gold.
Driving motor vehicles.
Some countries (for example, Romania) have refused to grant driving licenses to individuals with color blindness. In Romania, there is an ongoing campaign to remove the legal restrictions that prohibit colorblind citizens from getting drivers' licenses.
The usual justification for such restrictions is that drivers of motor vehicles must be able to recognize color-coded signals, such as traffic lights or warning lights.
Piloting aircraft.
While many aspects of aviation depend on color coding, only a few of them are critical enough to be interfered with by some milder types of color blindness. Some examples include color-gun signaling of aircraft that have lost radio communication, color-coded glide-path indications on runways, and the like. Some jurisdictions restrict the issuance of pilot credentials to persons who suffer from color blindness for this reason. Restrictions may be partial, allowing color-blind persons to obtain certification but with restrictions, or total, in which case color-blind persons are not permitted to obtain piloting credentials at all.
In the United States, the Federal Aviation Administration requires that pilots be tested for normal color vision as part of their medical clearance in order to obtain the required medical certificate, a prerequisite to obtaining a pilot's certification. If testing reveals color blindness, the applicant may be issued a license with restrictions, such as no night flying and no flying by color signals—such a restriction effectively prevents a pilot from holding certain flying occupations, such as that of an airline pilot, although commercial pilot certification is still possible, and there are a few flying occupations that do not require night flight and thus are still available to those with restrictions due to color blindness (e.g., agricultural aviation). The government allows several types of tests, including medical standard tests ("e.g.," the Ishihara, Dvorine, and others) and specialized tests oriented specifically to the needs of aviation. If an applicant fails the standard tests, they will receive a restriction on their medical certificate that states: "Not valid for night flying or by color signal control". They may apply to the FAA to take a specialized test, administered by the FAA. Typically, this test is the "color vision light gun test". For this test an FAA inspector will meet the pilot at an airport with an operating control tower. The color signal light gun will be shone at the pilot from the tower, and they must identify the color. If they pass they may be issued a waiver, which states that the color vision test is no longer required during medical examinations. They will then receive a new medical certificate with the restriction removed. This was once a Statement of Demonstrated Ability (SODA), but the SODA was dropped, and converted to a simple waiver (letter) early in the 2000s.
Research published in 2009 carried out by the City University of London's Applied Vision Research Centre, sponsored by the UK's Civil Aviation Authority and the US Federal Aviation Administration, has established a more accurate assessment of color deficiencies in pilot applicants' red–green and yellow–blue color range which could lead to a 35% reduction in the number of prospective pilots who fail to meet the minimum medical threshold.
Art.
Inability to distinguish color does not necessarily preclude the ability to become a celebrated artist. The expressionist painter Clifton Pugh, three-time winner of Australia's Archibald Prize, on biographical, gene inheritance and other grounds has been identified as a protanope. 19th century French artist Charles Méryon became successful by concentrating on etching rather than painting after he was diagnosed as having a red–green deficiency.
Rights of people with color blindness.
Brazil.
A Brazilian court ruled that people with color blindness are protected by the Inter-American Convention on the Elimination of All Forms of Discrimination against Person with Disabilities.
At trial, it was decided that the carriers of color blindness have a right of access to wider knowledge, or the full enjoyment of their human condition.
Problems and compensations.
Color blindness very rarely means complete monochromatism. In almost all cases, color blind people retain blue–yellow discrimination, and most color-blind individuals are anomalous trichromats rather than complete dichromats. In practice this means that they often retain a limited discrimination along the red–green axis of color space, although their ability to separate colors in this dimension is severely reduced.
Dichromats often confuse red and green items. For example, they may find it difficult to distinguish a Braeburn apple from a Granny Smith and in some cases, the red and green of traffic light without other clues (for example, shape or position). The vision of dichromats may also be compared to images produced by a color printer that has run out of the ink in one of its three color cartridges (for protanopes and deuteranopes, the magenta cartridge, and for tritanopes, the yellow cartridge). Dichromats tend to learn to use texture and shape clues and so are often able to penetrate camouflage that has been designed to deceive individuals with color-normal vision.
Traffic-light colors are confusing to some dichromats as there is insufficient apparent difference between the red/amber traffic lights, and that of sodium street lamps; also the green can be confused with a grubby white lamp. This is a risk factor on high-speed undulating roads where angular cues cannot be used. British Rail color lamp signals use more easily identifiable colors: the red is blood red, the amber is yellow and the green is a bluish color. Most British road traffic lights are mounted vertically on a black rectangle with a white border (forming a "sighting board") and so dichromats can look for the position of the light within the rectangle—top, middle or bottom. In the Eastern provinces of Canada horizontally mounted traffic lights are generally differentiated by shape to facilitate identification for those with color blindness. In the United States, this is not done, as since the red light is always on the left if the light is horizontal, this is assumed not to be necessary. A famous traffic light on Tipperary Hill in Syracuse, New York, is upside-down due to the sentiments of its Irish American community, but has been criticized due to the potential hazard it poses for color-blind persons.

</doc>
<doc id="7398" url="http://en.wikipedia.org/wiki?curid=7398" title="Computer security">
Computer security

Computer security (also known as cybersecurity or IT security) is information security as applied to computing devices such as computers and smartphones, as well as computer networks such as private and public networks, including the whole Internet.
The field covers all the processes and mechanisms by which computer-based equipment, information and services are protected from unintended or unauthorized access, change or destruction, and is of growing importance in line with the increasing reliance on computer systems of most societies worldwide.
Vulnerabilities.
To understand the techniques for securing a computer system, it is important to first understand the various types of "attacks" that can be made against it. These threats can typically be classified into one of these seven categories:
Backdoors.
A backdoor in a computer system, a cryptosystem or an algorithm, is a method of bypassing normal authentication, securing remote access to a computer, obtaining access to plaintext, and so on, while attempting to remain undetected. A special form of asymmetric encryption attacks, known as kleptographic attack, resists to be useful to the reverse engineer even after it is detected and analyzed.
The backdoor may take the form of an installed program (e.g., Back Orifice), or could be a modification to an existing program or hardware device. A specific form of backdoor is a rootkit, which replaces system binaries and/or hooks into the function calls of an operating system to hide the presence of other programs, users, services and open ports. It may also fake information about disk and memory usage.
Denial-of-service attack.
Unlike other exploits, denial of service attacks are not used to gain unauthorized access or control of a system. They are instead designed to render it unusable. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim account to be locked, or they may overload the capabilities of a machine or network and block all users at once. These types of attack are, in practice, very hard to prevent, because the behaviour of whole networks needs to be analyzed, not only the behaviour of small pieces of code. Distributed denial of service (DDoS) attacks are common, where a large number of compromised hosts (commonly referred to as "zombie computers", used as part of a botnet with, for example; a worm, trojan horse, or backdoor exploit to control them) are used to flood a target system with network requests, thus attempting to render it unusable through resource exhaustion. Another technique to exhaust victim resources is through the use of an attack amplifier, where the attacker takes advantage of poorly designed protocols on third-party machines, such as FTP or DNS, in order to instruct these hosts to launch the flood.
Some vulnerabilities in applications or operating systems can be exploited to make the computer or application malfunction or crash to create a denial-of-service.
Direct access attacks.
Someone who has gained access to a computer can install different types of devices to compromise security, including operating system modifications, software worms, key loggers, and covert listening devices. The attacker can also easily download large quantities of data onto backup media, for instance CD-R/DVD-R, tape; or portable devices such as keydrives, digital cameras or digital audio players. Another common technique is to boot an operating system contained on a CD-ROM or other bootable media and read the data from the harddrive(s) this way. The only way to defeat this is to encrypt the storage media and store the key separate from the system.
Eavesdropping.
Eavesdropping is the act of surreptitiously listening to a private conversation, typically between hosts on a network. For instance, programs such as Carnivore and NarusInsight have been used by the FBI and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon via monitoring the faint electro-magnetic transmissions generated by the hardware such as TEMPEST.
Exploits.
An exploit (from the same word in the French language, meaning "achievement", or "accomplishment") is a piece of software, a chunk of data, or sequence of commands that take advantage of a software "bug" or "glitch" in order to cause unintended or unanticipated behaviour to occur on computer software, hardware, or something electronic (usually computerized). This frequently includes such things as gaining control of a computer system or allowing privilege escalation or a denial of service attack. The term "exploit" generally refers to small programs designed to take advantage of a software flaw that has been discovered, either remote or local. The code from the exploit program is frequently reused in trojan horses and computer viruses. In some cases, a vulnerability can lie in certain programs' processing of a specific file type, such as a non-executable media file. Some security web sites maintain lists of currently known unpatched vulnerabilities found in common programs (see "External links" below).
Indirect attacks.
An indirect attack is an attack launched by a third-party computer. By using someone else's computer to launch an attack, it becomes far more difficult to track down the actual attacker. There have also been cases where attackers took advantage of public anonymizing systems, such as the tor onion router system.
Social engineering and human error.
A computer system is no more secure than the persons responsible for its operation. Malicious individuals have regularly penetrated well-designed, secure computer systems by taking advantage of the carelessness of trusted individuals, or by deliberately deceiving them, for example sending messages that they are the system administrator and asking for passwords. This deception is known as social engineering.
In the world of information technology there are different types of cyber attack–like code injection to a website or utilising malware (malicious software) such as virus, trojans, or similar. Attacks of these kinds are counteracted managing or improving the damaged product. But there is one last type, social engineering, which does not directly affect the computers but instead their users, which are also known as "the weakest link". This type of attack is capable of achieving similar results to other class of cyber attacks, by going around the infrastructure established to resist malicious software; since being more difficult to calculate or prevent, it is many times a more efficient attack vector.
The main target is to convince the user by means of psychological ways to disclose his or her personal information such as passwords, card numbers, etc. by, for example, impersonating the services company or the bank.
Vulnerable areas.
Computer security is critical in almost any technology-driven industry which operates on computer systems. The issues of computer based systems and addressing their countless vulnerabilities are an integral part of maintaining an operational industry.
Aviation.
The aviation industry is especially important when analyzing computer security because the involved risks include human life, expensive equipment, cargo, and transportation infrastructure. Security can be compromised by hardware and software malpractice, human error, and faulty operating environments. Threats that exploit computer vulnerabilities can stem from sabotage, espionage, industrial competition, terrorist attack, mechanical malfunction, and human error.
The consequences of a successful deliberate or inadvertent misuse of a computer system in the aviation industry range from loss of confidentiality to loss of system integrity, which may lead to more serious concerns such as exfiltration (data theft or loss), network and air traffic control outages, which in turn can lead to airport closures, loss of aircraft, loss of passenger life. Military systems that control munitions can pose an even greater risk.
A proper attack does not need to be very high tech or well funded; for a power outage at an airport alone can cause repercussions worldwide. One of the easiest and, arguably, the most difficult to trace security vulnerabilities is achievable by transmitting unauthorized communications over specific radio frequencies. These transmissions may spoof air traffic controllers or simply disrupt communications altogether. These incidents are very common, having altered flight courses of commercial aircraft and caused panic and confusion in the past. Controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore. Beyond the radar's sight controllers must rely on periodic radio communications with a third party.
Lightning, power fluctuations, surges, brownouts, blown fuses, and various other power outages instantly disable all computer systems, since they are dependent on an electrical source. Other accidental and intentional faults have caused significant disruption of safety critical systems throughout the last few decades and dependence on reliable communication and electrical power only jeopardizes computer safety.
Financial cost of security breaches.
Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. “Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal.”
Insecurities in operating systems have led to a massive black market for rogue software. An attacker can use a security hole to install software that tricks the user into buying a product. At that point, an affiliate program pays the affiliate responsible for generating that installation about $30. The software is sold for between $50 and $75 per license.
Reasons.
There are many similarities (yet many fundamental differences) between computer and physical security. Just like real-world security, the motivations for breaches of computer security vary between attackers, sometimes called hackers or crackers. Some are thrill-seekers or vandals (the kind often responsible for defacing web sites); similarly, some web site defacements are done to make political statements. However, some attackers are highly skilled and motivated with the goal of compromising computers for financial gain or espionage. An example of the latter is Markus Hess (more diligent than skilled), who spied for the KGB and was ultimately caught because of the efforts of Clifford Stoll, who wrote a memoir, "The Cuckoo's Egg", about his experiences.
For those seeking to prevent security breaches, the first step is usually to attempt to identify what might motivate an attack on the system, how much the continued operation and information security of the system are worth, and who might be motivated to breach it. The precautions required for a home personal computer are very different for those of banks' Internet banking systems, and different again for a classified military network. Other computer security writers suggest that, since an attacker using a network need know nothing about you or what you have on your computer, attacker motivation is inherently impossible to determine beyond guessing. If true, blocking all possible attacks is the only plausible action to take.
Computer protection.
There are numerous ways to protect computers, including utilizing security-aware design techniques, building on secure operating systems and installing hardware devices designed to protect the computer systems.
Security and systems design.
Although there are many aspects to take into consideration when designing a computer system, security can prove to be very important. According to Symantec, in 2010, 94 percent of organizations polled expect to implement security improvements to their computer systems, with 42 percent claiming cyber security as their top risk.
At the same time, many organizations are improving security and many types of cyber criminals are finding ways to continue their activities. Almost every type of cyber attack is on the rise. In 2009 respondents to the CSI Computer Crime and Security Survey admitted that malware infections, denial-of-service attacks, password sniffing, and web site defacements were significantly higher than in the previous two years.
Security measures.
A state of computer "security" is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:
Today, computer security comprises mainly "preventive" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real time filtering and blocking. Another implementation is a so-called physical firewall which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.
However, relatively few organisations maintain computer systems with effective detection systems, and fewer still have organised response mechanisms in place. As result, as Reuters points out: “Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets”. The primary obstacle to effective eradication of cyber crime could be traced to excessive reliance on firewalls and other automated "detection" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.
Difficulty with response.
Responding forcefully to attempted security breaches (in the manner that one would for attempted physical security breaches) is often very difficult for a variety of reasons:
Reducing vulnerabilities.
Computer code is regarded by some as a form of mathematics. It is theoretically possible to prove the correctness of certain classes of computer programs, though the feasibility of actually achieving this in large-scale practical systems is regarded as small by some with practical experience in the industry; see Bruce Schneier et al.
It is also possible to protect messages in transit (i.e., communications) by means of cryptography. One method of encryption—the one-time pad—is unbreakable when correctly used. This method was used by the Soviet Union during the Cold War, though flaws in their implementation allowed some cryptanalysis; see the Venona project. The method uses a matching pair of key-codes, securely distributed, which are used once-and-only-once to encode and decode a single message. For transmitted computer encryption this method is difficult to use properly (securely), and highly inconvenient as well. Other methods of encryption, while breakable in theory, are often virtually impossible to directly break by any means publicly known today. Breaking them requires some non-cryptographic input, such as a stolen key, stolen plaintext (at either end of the transmission), or some other extra cryptanalytic information.
Social engineering and direct computer access (physical) attacks can only be prevented by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Even in a highly disciplined environment, such as in military organizations, social engineering attacks can still be difficult to foresee and prevent.
In practice, only a small fraction of computer program code is mathematically proven, or even goes through comprehensive information technology audits or inexpensive but extremely valuable computer security audits, so it is usually possible for a determined hacker to read, copy, alter or destroy data in well secured computers, albeit at the cost of great time and resources. Few attackers would audit applications for vulnerabilities just to attack a single specific system. It is possible to reduce an attacker's chances by keeping systems up to date, using a security scanner or/and hiring competent people responsible for security. The effects of data loss/damage can be reduced by careful backing up and insurance.
Security by design.
Security by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered as a main feature.
Some of the techniques in this approach include:
Security architecture.
The Open Security Architecture organization defines IT security architecture as "the design artifacts that describe how the security controls (security countermeasures) are positioned, and how they relate to the overall information technology architecture. These controls serve the purpose to maintain the system's quality attributes: confidentiality, integrity, availability, accountability and assurance services".
Hardware protection mechanisms.
While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.
Secure operating systems.
One use of the term "computer security" refers to technology that is used to implement secure operating systems. Much of this technology is based on science developed in the 1980s and used to produce what may be some of the most impenetrable operating systems ever. Though still valid, the technology is in limited use today, primarily because it imposes some changes to system management and also because it is not widely understood. Such ultra-strong secure operating systems are based on operating system kernel technology that can guarantee that certain security policies are absolutely enforced in an operating environment. An example of such a Computer security policy is the Bell-LaPadula model. The strategy is based on a coupling of special microprocessor hardware features, often involving the memory management unit, to a special correctly implemented operating system kernel. This forms the foundation for a secure operating system which, if certain critical parts are designed and implemented correctly, can ensure the absolute impossibility of penetration by hostile elements. This capability is enabled because the configuration not only imposes a security policy, but in theory completely protects itself from corruption. Ordinary operating systems, on the other hand, lack the features that assure this maximal level of security. The design methodology to produce such secure systems is precise, deterministic and logical.
Systems designed with such methodology represent the state of the art of computer security although products using such security are not widely known. In sharp contrast to most kinds of software, they meet specifications with verifiable certainty comparable to specifications for size, weight and power. Secure operating systems designed this way are used primarily to protect national security information, military secrets, and the data of international financial institutions. These are very powerful security tools and very few secure operating systems have been certified at the highest level (Orange Book A-1) to operate over the range of "Top Secret" to "unclassified" (including Honeywell SCOMP, USAF SACDIN, NSA Blacker and Boeing MLS LAN). The assurance of security depends not only on the soundness of the design strategy, but also on the assurance of correctness of the implementation, and therefore there are degrees of security strength defined for COMPUSEC. The Common Criteria quantifies security strength of products in terms of two components, security functionality and assurance level (such as EAL levels), and these are specified in a Protection Profile for requirements and a Security Target for product descriptions. None of these ultra-high assurance secure general purpose operating systems have been produced for decades or certified under Common Criteria.
In USA parlance, the term High Assurance usually suggests the system has the right security functions that are implemented robustly enough to protect DoD and DoE classified information. Medium assurance suggests it can protect less valuable information, such as income tax information. Secure operating systems designed to meet medium robustness levels of security functionality and assurance have seen wider use within both government and commercial markets. Medium robust systems may provide the same security functions as high assurance secure operating systems but do so at a lower assurance level (such as Common Criteria levels EAL4 or EAL5). Lower levels mean we can be less certain that the security functions are implemented flawlessly, and therefore less dependable. These systems are found in use on web servers, guards, database servers, and management hosts and are used not only to protect the data stored on these systems but also to provide a high level of protection for network connections and routing services.
Secure coding.
If the operating environment is not based on a secure operating system capable of maintaining a domain for its own execution, and capable of protecting application code from malicious subversion, and capable of protecting the system from subverted code, then high degrees of security are understandably not possible. While such secure operating systems are possible and have been implemented, most commercial systems fall in a 'low security' category because they rely on features not supported by secure operating systems (like portability, and others). In low security operating environments, applications must be relied on to participate in their own protection. There are 'best effort' secure coding practices that can be followed to make an application more resistant to malicious subversion.
In commercial environments, the majority of software subversion vulnerabilities result from a few known kinds of coding defects. Common software defects include buffer overflows, format string vulnerabilities, integer overflow, and code/command injection. These defects can be used to cause the target system to execute putative data. However, the "data" contain executable instructions, allowing the attacker to gain control of the processor.
Some common languages such as C and C++ are vulnerable to all of these defects (see Seacord, "Secure Coding in C and C++"). Other languages, such as Java, are more resistant to some of these defects, but are still prone to code/command injection and other software defects which facilitate subversion.
Another bad coding practice occurs when an object is deleted during normal operation yet the program neglects to update any of the associated memory pointers, potentially causing system instability when that location is referenced again. This is called dangling pointer, and the first known exploit for this particular problem was presented in July 2007. Before this publication the problem was known but considered to be academic and not practically exploitable.
Unfortunately, there is no theoretical model of "secure coding" practices, nor is one practically achievable, insofar as the code (ideally, read-only) and data (generally read/write) generally tends to have some form of defect.
Capabilities and access control lists.
Within computer systems, two security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.
Capabilities have been mostly restricted to research operating systems, while commercial OSs still use ACLs. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open source project in the area is the E language.
The most secure computers are those not connected to the Internet and shielded from any interference. In the real world, the most secure systems are operating systems where security is not an add-on.
Hacking back.
There has been a significant debate regarding the legality of hacking back against digital attackers (who attempt to or successfully breach an individual's, entity's, or nation's computer). The arguments for such counter-attacks are based on notions of equity, active defense, vigilantism, and the Computer Fraud and Abuse Act (CFAA). The arguments against the practice are primarily based on the legal definitions of "intrusion" and "unauthorized access", as defined by the CFAA. As of October 2012, the debate is ongoing.
Notable computer security breaches.
Some illustrative examples of different types of computer security breaches are given below.
Robert Morris and the first computer worm.
In 1988, only 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On November 2, 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers - the first internet "computer worm". The software was traced back to 23 year old Cornell University graduate student Robert Tappan Morris, Jr. who said 'he wanted to count how many machines were connected to the Internet'.
Rome Laboratory.
In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as
a trusted Rome center user.
TJX loses 45.7 customer credit card details.
In early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.
Stuxnet attack.
The computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges by disrupting industrial programmable logic controllers (PLCs) in a targeted attack generally believed to have been launched by Israel and the United States although neither has publicly acknowledged this.
Global surveillance disclosures.
In early 2013, thousands of thousands of classified documents were disclosed by NSA contractor Edward Snowden. Called the "most significant leak in U.S. history" it also revealed for the first time the massive breaches of computer security by the NSA, including deliberately inserting a backdoor in a NIST standard for encryption and tapping the links between Google's data centres.
Legal issues and global regulation.
Conflict of laws in cyberspace<ref name="http://perry4law.org/clic/"></ref> has become a major cause of concern for computer security community. Some of the main challenges and complaints about the antivirus industry are the lack of global web regulations, a global base of common rules to judge, and eventually punish, cyber crimes and cyber criminals. There is no global cyber law<ref name="http://perry4law.org/blog/?p=53"></ref> and cyber security treaty<ref name=" http://ptlb.in/csrdci/wp-content/uploads/2014/01/International-Cyber-Security-Treaty-Is-Required.pdf"></ref> that can be invoked for enforcing global cyber security issues.
International legal issues of cyber attacks<ref name="http://perry4law.co.in/cyber_security/"></ref> are really tricky and complicated in nature.<ref name="http://ptlb.in/csrdci/?p=294"></ref> For instance, even if an antivirus firm locates the cyber criminal behind the creation of a particular virus or piece of malware or again one form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute. This is mainly caused by the fact that many countries have their own regulations regarding cyber crimes. Authorship attribution for cyber crimes and cyber attacks has become a major problem for international law enforcement agencies.<ref name="http://perry4law.org/cecsrdi/?p=568"></ref>
"[Computer viruses] switch from one country to another, from one jurisdiction to another — moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world." (Mikko Hyppönen) Use of dynamic DNS, fast flux and bullet proof servers have added own complexities to this situation.<ref name="http://perry4law.org/cecsrdi/?p=939"></ref>
Businesses are eager to expand to less developed countries due to the low cost of labor, says White et al. (2012). However, these countries are the ones with the least amount of Internet safety measures, and the Internet Service Providers are not so focused on implementing those safety measures (2010). Instead, they are putting their main focus on expanding their business, which exposes them to an increase in criminal activity.
In response to the growing problem of cyber crime, the European Commission established the European Cybercrime Centre (EC3). The EC3 effectively opened on 1 January 2013 and will be the focal point in the EU's fight against cyber crime, contributing to faster reaction to online crimes. It will support member states and the EU's institutions in building an operational and analytical capacity for investigations, as well as cooperation with international partners.
Computer security policies.
Country-specific computer security policies are discussed below.
United States.
Cybersecurity Act of 2010.
On July 1, 2009, Senator Jay Rockefeller (D-WV) introduced the "Cybersecurity Act of 2009 - S. 773" in the Senate; the bill, co-written with Senators Evan Bayh (D-IN), Barbara Mikulski (D-MD), Bill Nelson (D-FL), and Olympia Snowe (R-ME), was referred to the Committee on Commerce, Science, and Transportation, which approved a revised version of the same bill (the "Cybersecurity Act of 2010") on March 24, 2010. The bill seeks to increase collaboration between the public and the private sector on cybersecurity issues, especially those private entities that own infrastructures that are critical to national security interests (the bill quotes John Brennan, the Assistant to the President for Homeland Security and Counterterrorism: "our nation’s security and economic prosperity depend on the security, stability, and integrity of communications and information infrastructure that are largely privately owned and globally operated" and talks about the country's response to a "cyber-Katrina"), increase public awareness on cybersecurity issues, and foster and fund cybersecurity research. Some of the most controversial parts of the bill include Paragraph 315, which grants the President the right to "order the limitation or shutdown of Internet traffic to and from any compromised Federal Government or United States critical infrastructure information system or network." The Electronic Frontier Foundation, an international non-profit digital rights advocacy and legal organization based in the United States, characterized the bill as promoting a "potentially dangerous approach that favors the dramatic over the sober response".
International Cybercrime Reporting and Cooperation Act.
On March 25, 2010, Representative Yvette Clarke (D-NY) introduced the "International Cybercrime Reporting and Cooperation Act - H.R.4962" in the House of Representatives; the bill, co-sponsored by seven other representatives (among whom only one Republican), was referred to three House committees. The bill seeks to make sure that the administration keeps Congress informed on information infrastructure, cybercrime, and end-user protection worldwide. It also "directs the President to give priority for assistance to improve legal, judicial, and enforcement capabilities with respect to cybercrime to countries with low information and communications technology levels of development or utilization in their critical infrastructure, telecommunications systems, and financial industries" as well as to develop an action plan and an annual compliance assessment for countries of "cyber concern".
Protecting Cyberspace as a National Asset Act of 2010.
On June 19, 2010, United States Senator Joe Lieberman (I-CT) introduced a bill called "Protecting Cyberspace as a National Asset Act of 2010 - S.3480" which he co-wrote with Senator Susan Collins (R-ME) and Senator Thomas Carper (D-DE). If signed into law, this controversial bill, which the American media dubbed the "Kill switch bill", would grant the President emergency powers over the Internet. However, all three co-authors of the bill issued a statement claiming that instead, the bill "[narrowed] existing broad Presidential authority to take over telecommunications networks".
White House proposes cybersecurity legislation.
On May 12, 2011, the White House sent Congress a proposed cybersecurity law designed to force companies to do more to fend off cyberattacks, a threat that has been reinforced by recent reports about vulnerabilities in systems used in power and water utilities.
Executive order "Improving Critical Infrastructure Cybersecurity" was signed February 12, 2013.
Germany.
Berlin starts National Cyber Defense Initiative.
On June 16, 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum, which is located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt für Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt für den Militärischen Abschirmdienst and other national organisations in Germany taking care of national security aspects. According to the Minister the primary task of the new organisation founded on February 23, 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet.
South Korea.
Following cyberattacks in the first half of 2013, whereby government, news-media, television station, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart on these attacks, as well as incidents that occurred in 2009, 2011, and 2012, but Pyongyang denies the accusations.
Seoul, March 7, 2011 - South Korean police have contacted 35 countries to ask for cooperation in tracing the origin of a massive cyber attack on the Web sites of key government and financial institutions, amid a nationwide cyber security alert issued against further threats.
The Web sites of about 30 key South Korean government agencies and financial institutions came under a so-called distributed denial-of-service (DDoS) attack for two days from Friday, with about 50,000 "zombie" computers infected with a virus seeking simultaneous access to selected sites and swamping them with traffic.
As soon as the copies of overseas servers are obtained, the cyber investigation unit will analyse the data to track down the origin of the attacks made from countries, including the United States, Russia, Italy and Israel, the NPA noted.
In late September 2013, a computer-security competition jointly sponsored by the defense ministry and the National Intelligence Service was announced. The winners will be announced on September 29, 2013 and will share a total prize pool of 80 million won (US$74,000).
India.
India has no specific law for dealing with cyber security related issues.<ref name=" http://perry4law.org/cecsrdi/?p=1095"></ref> Some provisions for cyber security have been incorporated into rules framed under the Information Technology Act 2000 but they are grossly insufficient. Further, the National Cyber Security Policy 2013 has remained ineffective and non-implementable until now.<ref name=" http://perry4law.org/cecsrdi/?p=1068"></ref> The cyber security trends and developments in India 2013 have listed the shortcomings of Indian cyber security policy in general and Indian cyber security initiatives in particular.<ref name=" http://ptlb.in/csrdci/wp-content/uploads/2013/12/Cyber-Security-Trends-And-Developments-In-India-2013.pdf"></ref> Indian cyber security policy has also failed to protect civil liberties of Indians including privacy rights.<ref name=" http://perry4law.org/cecsrdi/?p=1017"></ref> Civil liberties protection in cyberspace has been blatantly ignored by Indian government and e-surveillance projects have been kept intact by the Narendra Modi government.<ref name=" http://ptlb.in/clpic/"></ref> As a result Indian cyber security efforts are inadequate and not up to the mark. There is also no legal obligation for cyber security breach disclosures in India as well.<ref name=" http://perry4law.org/cecsrdi/?p=544"></ref>
However, the Indian Companies Act 2013 has introduced cyber law<ref name=" http://perry4law.org/cyberlawsinindia/?p=102"></ref> and cyber security obligations<ref name=" http://ptlb.in/csrdci/?p=305"></ref> on the part of Indian directors. Cyber security obligations for e-commerce business in India have also been recognised lately.<ref name=" http://ptlb.in/ecommerce/?p=357"></ref>
The cyber security job market.
Cyber Security is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breach. Commercial, government and non-governmental all employ cybersecurity professional, but the use of the term "cybersecurity" is government job descriptions is more prevalent than in non-government job descriptions, in part due to government "cybersecurity" initiatives (as opposed to corporation's "IT security" initiatives) and the establishment of government institutions like the US Cyber Command and the UK Defence Cyber Operations Group.
Typical cybersecurity job titles and descriptions include:
Student programs are also available to people interested in beginning a career in cybersecurity.
Terminology.
The following terms used with regards to engineering secure systems are explained below.

</doc>
<doc id="7400" url="http://en.wikipedia.org/wiki?curid=7400" title="Chris Cunningham">
Chris Cunningham

Chris Cunningham is a British video artist. He was born in Reading, Berkshire in 1970 and grew up in Lakenheath, Suffolk.
Cunningham has primarily directed music videos for ambient music and electronica acts such as Autechre and Aphex Twin. He has also created art installations and directed short movies. He was approached to direct a movie version of the cyberpunk novel "Neuromancer", but nothing came of early discussions. In the 2000s, Cunningham began doing music production work. He has also designed album artwork for a variety of musicians.
The video collection "The Work of Director Chris Cunningham" was released in November 2004 as part of the Directors Label set. This DVD includes selected highlights from 1995–2000.
Early work.
After seeing Cunningham's work on the 1995 film version of "Judge Dredd", Stanley Kubrick head-hunted Cunningham to design and supervise animatronic tests of the central robot child character in his version of the film "A.I. Artificial Intelligence". Cunningham worked for over a year on the film before leaving to pursue a career as a director.
Earlier work in film included model-making, prosthetic make-up and concept illustrations for "Hardware" and "Dust Devil" for director Richard Stanley; work on "Nightbreed" for Clive Barker; and on "Alien3" for David Fincher. Between 1990 and 1992, he contributed the occasional cover painting and strip to "Judge Dredd Megazine", working under the pseudonym "Chris Halls"; Halls is his stepfather's surname.
Music videos.
Cunningham has had close ties to Warp Records since his first production for Autechre. Videos for Aphex Twin's "Come to Daddy" and "Windowlicker" are perhaps his best known. His video for Björk's "All Is Full of Love" won multiple awards, including an MTV music video award for Breakthrough Video and was nominated for a Grammy for Best Short Form Music Video. It was also the first ever music video to win a Gold Pencil at the D&AD Awards. It can still be seen at the Museum of Modern Art in New York. His video for Aphex Twin's "Windowlicker" was nominated for the "Best Video" award at the Brit Awards 2000. He also directed Madonna's "Frozen" video which became an international hit and won the award for Best Special Effects at the 1998 MTV Music Video Awards. Cunningham also came out of a seven year hiatus from making music videos to direct the video for "Sheena Is a Parasite" by The Horrors.
Video art.
His video installation "Flex" was first shown in 2000 at the Royal Academy of Arts, and subsequently at the Anthony d'Offay Gallery and other art galleries. "Flex" was commissioned by the Anthony d'Offay Gallery for the exhibition curated by Norman Rosenthal and Max Wigram at the Royal Academy of Arts in 2000.
The Anthony d'Offay Gallery also commissioned "Monkey Drummer", a 2½ minute piece intended for exhibition as a companion to "Flex" at the 2000 "Apocalypse" exhibition at the Royal Academy of Arts: however, the piece was not finished in time. In it an automaton with nine appendages and the head of a monkey plays the drums to "Mt Saint Michel + Saint Michaels Mount", the 10th track on Aphex Twin's 2001 album "drukqs". "Monkey Drummer" debuted as part of Cunningham's installation at the 49th International Exhibition of Art at the 2001 Venice Biennale, which consisted of a loop of "Monkey Drummer", "Flex", and his video for Björk's "All Is Full of Love". In 2002 both "Flex" and "Monkey Drummer" were exhibited by 5th Gallery in Dublin, Ireland, in an exhibition curated by Artist/Curator Paul Murnaghan,
In 2007, an excerpt from "Flex" was shown in the Barbican's exhibition Seduced: Art and Sex from Antiquity to Now curated by Martin Kemp, Marina Wallace and Joanne Bernstein. alongside other pieces by Bacon, Klimt, Rembrandt, Rodin and Picasso.
Short films.
In 2005, Cunningham released the short film "Rubber Johnny" as a DVD accompanied by a book of photographs and drawings. "Rubber Johnny", a six-minute experimental short film cut to a soundtrack by Aphex Twin, remixed by Cunningham was shot between 2001 and 2004. Shot on DV night-vision, it was made in Cunningham's own time as a home movie of sorts, and took three and half years of weekends to complete. The Telegraph called it "like a Looney Tunes short for a generation raised on video nasties and rave music".
During this period Cunningham also made another short film for Warp Films, Spectral Musicians, which remains unreleased. The short film was edited to music by Squarepusher, My Fucking Sound, from the album Go Plastic and a piece called Mutilation Colony, which was written especially for the short and was released on the EP Do You Know Squarepusher.
Commercials.
Cunningham has directed a handful of commercials for companies including Gucci, PlayStation, Levis, Telecom Italia, Nissan and Orange.
Music production.
In 2004/2005, Cunningham took a sabbatical from filmmaking to learn about music production and recording and to develop his own music projects.
In December 2007 Cunningham produced two tracks, "Three Decades" and "Primary Colours", for "Primary Colours", the second album by The Horrors. In the summer of 2008, due to scheduling conflicts with his feature film script writing he could not work on the rest of the album which was subsequently recorded by Geoff Barrow from Portishead.
In 2008, he produced and arranged a new version of 'I Feel Love' for the Gucci commercial that he also directed. He travelled to Nashville to work with Donna Summer to record a brand new vocal for it.
"Chris Cunningham Live".
In 2005, Cunningham played a 45 minute audio visual piece performed live in Tokyo and Osaka in front of 30,000+ fans over the two nights at Japan’s premier electronic music event Electraglide. These performances evolved into "Chris Cunningham Live", a 55 minute long performance piece combining original and remixed music and film. It features remixed, unreleased and brand new videos and music dynamically edited together into a new live piece spread over three screens. The sound accompanying these images includes Cunningham’s first publicly performed compositions interspersed with his remixes of other artist’s work. "Chris Cunningham Live" debuted as one of the headline attractions at Warp 20 in Paris on 8 May 2009 with other performances scheduled at festivals in UK, and a number of European cities later in the year. "Chris Cunningham Live" continued in June 2011, with performances in London, Barcelona, and Sydney, Australia.
Photography.
Cunningham has created photography and cover artwork for various people including Björk's "All Is Full of Love", Aphex Twin's "Windowlicker" and "Come to Daddy".
In 2008, Cunningham produced a fashion shoot for "Dazed & Confused" using Grace Jones as a model to create "Nubian versions" of Rubber Johnny. In an interview for BBC's "The Culture Show", it was suggested that the collaboration may expand into a video project.
In November 2008, Cunningham followed on with another photoshoot for "Vice Magazine".
"Neuromancer".
In 2000, Cunningham and cyberpunk author William Gibson began work on the script for Gibson's 1984 novel "Neuromancer". However, because "Neuromancer" was due to be a big budget studio film, it is rumoured that Cunningham pulled out due to being a first time director without final cut approval. He also felt that too much of the original book's ideas had been cannibalised by other recent films.
On 18 November 2004, in the FAQ on the William Gibson Board, Gibson was asked:
In an August 1999 "Spike Magazine" interview, Gibson stated "He (Chris) was brought to my attention by someone else. We were told, third-hand, that he was extremely wary of the Hollywood process, and wouldn't return calls. But someone else told us that "Neuromancer" had been his "Wind In The Willows", that he'd read it when he was a kid. I went to London and we met." Gibson is also quoted in the article as saying "Chris is my own 100 per cent personal choice...My only choice. The only person I've met who I thought might have a hope in hell of doing it right. I went back to see him in London just after he'd finished the Bjork video, and I sat on a couch beside this dead sex little Bjork robot, except it was wearing Aphex Twin's head. We talked."
It is rumoured that the character of Damien Pease in Gibson's 2003 novel "Pattern Recognition" was based on Cunningham, with the character's apartment featuring a female robot which had appeared in one of Cunningham's videos.
Development funding was in place for Cunningham to direct and co-write his first feature film for Warp Films, to whom he was at the time committed "for all future full-length film projects." He has since left Warp Films to set up his own production company 'CC Co' to produce his films independently.
Personal life.
Cunningham is married to Warpaint's bassist Jenny Lee Lindberg.

</doc>
<doc id="7401" url="http://en.wikipedia.org/wiki?curid=7401" title="Centaur">
Centaur

A centaur (; , "Kéntauros", ) or hippocentaur is a mythological creature with the head, arms, and torso of a human and the body and legs of a horse.
In early Attic and Beotian vase-paintings (see below), they are depicted with the hindquarters of a horse attached to them; in later renderings centaurs are given the torso of a human joined at the waist to the horse's withers, where the horse's neck would be.
This half-human and half-horse composition has led many writers to treat them as liminal beings, caught between the two natures, embodied in contrasted myths, both as the embodiment of untamed nature, as in their battle with the Lapiths (their kin), or conversely as teachers, like Chiron.
The centaurs were usually said to have been born of Ixion and Nephele (the cloud made in the image of Hera). Another version, however, makes them children of a certain Centaurus, who mated with the Magnesian mares. This Centaurus was either himself the son of Ixion and Nephele (inserting an additional generation) or of Apollo and Stilbe, daughter of the river god Peneus. In the later version of the story his twin brother was Lapithes, ancestor of the Lapiths, thus making the two warring peoples cousins.
Centaurs were said to have inhabited the region of Magnesia and Mount Pelion in Thessaly, the Foloi oak forest in Elis, and the Malean peninsula in southern Laconia. They continued to feature in literary forms of Roman mythology. A pair of them draw the chariot of Constantine the Great and his family in the Great Cameo of Constantine ("c"314-16), which embodies wholly pagan imagery.
Centauromachy.
The Centaurs are best known for their fight with the Lapiths, which was caused by their attempt to carry off Hippodamia and the rest of the Lapith women on the day of Hippodamia's marriage to Pirithous, king of the Lapithae, himself the son of Ixion. The strife among these cousins is a metaphor for the conflict between the lower appetites and civilized behavior in humankind. Theseus, a hero and founder of cities, who happened to be present, threw the balance in favour of the right order of things, and assisted Pirithous. The Centaurs were driven off or destroyed. Another Lapith hero, Caeneus, who was invulnerable to weapons, was beaten into the earth by Centaurs wielding rocks and the branches of trees. Centaurs are thought of in many Greek myths as wild as untamed horses. Like the Titanomachy, the defeat of the Titans by the Olympian gods, the contests with the Centaurs typify the struggle between civilization and barbarism.
The Centauromachy is most famously portrayed in the Parthenon metopes by Phidias and in a Renaissance-era sculpture by Michelangelo.
Earliest representations.
The tentative identification of two fragmentary Mycenaean terracotta figures as centaurs, among the extensive Mycenaean pottery found at Ugarit, suggests a Bronze Age origin for these creatures of myth. A painted terracotta centaur was found in the "Hero's tomb" at Lefkandi, and by the Geometric period, centaurs figure among the first representational figures painted on Greek pottery. An often-published Geometric period bronze of a warrior face-to-face with a centaur is at the Metropolitan Museum of Art.
Theories of origin.
The most common theory holds that the idea of centaurs came from the first reaction of a non-riding culture, as in the Minoan Aegean world, to nomads who were mounted on horses. The theory suggests that such riders would appear as half-man, half-animal (Bernal Díaz del Castillo reported that the Aztecs had this misapprehension about Spanish cavalrymen). Horse taming and horseback culture arose first in the southern steppe grasslands of Central Asia, perhaps approximately in modern Kazakhstan.
The Lapith tribe of Thessaly, who were the kinsmen of the Centaurs in myth, were described as the inventors of horse-back riding by Greek writers. The Thessalian tribes also claimed their horse breeds were descended from the centaurs.
Of the various Classical Greek authors who mentioned centaurs, Pindar was the first who describes undoubtedly a combined monster. Previous authors (Homer) tend to use words such as "pheres" (cf. "theres", "beasts") that could also mean ordinary savage men riding ordinary horses, though Homer does specifically refer to a centaur ("kentauros") in the Odyssey Contemporaneous representations of hybrid centaurs can be found in archaic Greek art.
Lucretius in his first century BC philosophical poem "On the Nature of Things" denied the existence of centaurs based on their differing rate of growth. He states that at three years old horses are in the prime of their life while, at three humans are still little more than babies, making hybrid animals impossible.
Robert Graves (relying on the work of Georges Dumézil argued for tracing the centaurs back to the Indian gandharva), speculated that the centaurs were a dimly remembered, pre-Hellenic fraternal earth cult who had the horse as a totem. A similar theory was incorporated into Mary Renault's "The Bull from the Sea." Kinnaras, another half-man half-horse mythical creature from the Indian mythology, appeared in various ancient texts, arts as well as sculptures from all around India. It is shown as a horse with the torso of a man in place of where the horse's head has to be, that is similar to a Greek centaur.
The Greek word "kentauros" is generally regarded as of obscure origin. The etymology from "ken – tauros", "piercing bull-stickers" was a euhemerist suggestion in Palaephatus' rationalizing text on Greek mythology, "On Incredible Tales" (Περὶ ἀπίστων): mounted archers from a village called "Nephele" eliminating a herd of bulls that were the scourge of Ixion's kingdom. Another possible related etymology can be "bull-slayer". Some say that the Greeks took the constellation of Centaurus, and also its name "piercing bull", from Mesopotamia, where it symbolized the god Baal who represents rain and fertility, fighting with and "piercing" with his horns the demon Mot who represents the summer drought. In Greece, the constellation of Centaurus was noted by Eudoxus of Cnidus in the fourth century BC and by Aratus in the third century.
Female centaurs.
Though female centaurs, called Kentaurides, are not mentioned in early Greek literature and art, they do appear occasionally in later antiquity. A Macedonian mosaic of the 4th century BC is one of the earliest examples of the Centauress in art. Ovid also mentions a centauress named Hylonome who committed suicide when her husband Cyllarus was killed in the war with the Lapiths.
In a description of a painting in Neapolis, the Greek rhetorician Philostratus the Elder describes them as sisters and wives of the male centaurs who live on Mount Pelion with their children.
"How beautiful the Centaurides are, even where they are horses; for some grow out of white mares, others are attached to chestnut mares, and the coats of others are dappled, but they glisten like those of horses that are well cared for. There is also a white female Centaur that grows out of a black mare, and the very opposition of the colours helps to produce the united beauty of the whole."
The idea, or possibility, of female centaurs was certainly known in early modern times, as evidenced by Shakespeare's "King Lear", Act IV, Scene vi, ln.124–125:
"Down from the waist they're centaurs, / Though women all above"
In the Disney animated film "Fantasia", during the Pastoral Symphony, some of the main characters are female centaurs, referred to as "Centaurettes" by the Disney studio.
Persistence in the medieval world.
Centaurs preserved a Dionysian connection in the 12th century Romanesque carved capitals of Mozac Abbey in the Auvergne, where other capitals depict harvesters, boys riding goats (a further Dionysiac theme) and griffins guarding the chalice that held the wine.
Centaurs are shown on a number of Pictish carved stones from north-east Scotland, erected in the 8th–9th centuries AD (e.g., at Meigle, Perthshire). Though outside the limits of the Roman Empire, these depictions appear to be derived from Classical prototypes.
Jerome's version of the "Life" of St Anthony the Great, the hermit monk of Egypt, written by Athanasius of Alexandria, was widely disseminated in the Middle Ages; it relates Anthony's encounter with a centaur, who challenged the saint but was forced to admit that the old gods had been overthrown. The episode was often depicted; notably, in the "The Meeting of St Anthony Abbot and St Paul the Hermit" by Stefano di Giovanni called "Sassetta", of two episodic depictions in a single panel of the hermit Anthony's travel to greet the hermit Paul, one is his encounter along the pathway with the demonic figure of a centaur in a wood.
A centaur-like half-human half-equine creature called "Polkan" appeared in Russian folk art, and lubok prints of the 17th–19th centuries. Polkan is originally based on "Pulicane", a half-dog from Andrea da Barberino's poem "I Reali di Francia", which was once popular in the Slavonic world in prosaic translations.
Modern day.
The John C. Hodges library at The University of Tennessee hosts a permanent exhibit of a "Centaur from Volos", in its library. The exhibit, made by sculptor Bill Willers, by combining a study human skeleton with the skeleton of a Shetland pony is entitled "Do you believe in Centaurs?" and was meant to mislead students in order to make them more critically aware, according to the exhibitors.
Another exhibit by Willers is now on long term display at the International Wildlife Museum in Tucson, Arizona. The full-mount skeleton of a Centaur, built by Skulls Unlimited International, is on display, along with several other fabled creatures, including the Cyclops, Unicorn and Griffin.
A centaur is one of the symbols associated with both the Iota Phi Theta and the Delta Lambda Phi fraternities. Whereas centaurs in Greek mythology were generally symbolic of chaos and unbridled passions, Delta Lambda Phi's centaur is modeled after Chiron and represents honor, moderation and tempered masculinity.
Similarly, C.S. Lewis' popular "The Chronicles of Narnia" series depicts centaurs as the wisest and noblest of creatures. Narnian Centaurs are gifted at stargazing, prophecy, healing, and warfare, a fierce and valiant race always faithful to the High King Aslan the Lion. Lewis generally used the species to inspire awe in his readers.
In J.K. Rowling's "Harry Potter" series, centaurs live in the Forbidden Forest close to Hogwarts, preferring to avoid contact with humans. Although different from those seen in Narnia, they live in societies called herds and are skilled at archery, healing and astrology. Although film depictions include very animalistic facial features, the reaction of the Hogwarts girls to Firenze suggests a more classical appearance.
With the exception of Chiron, the centaurs in Rick Riordan's "Percy Jackson & the Olympians" are seen as party-goers who use a lot of American slang. Chiron is more like the classical centaurs, being trainer of the heroes and skilled in archery. In Riordan's subsequent series, "Heroes of Olympus", another group of centaurs are depicted with more animalistic features (such as horns) and appear as villains, serving the Gigantes.
Philip Jose Farmer's "World of Tiers" series (1965) includes centaurs, called Half-Horses or Hoi Kentauroi. His creations address several of the metabolic problems of such creatures—how could the human mouth and nose intake sufficient air to sustain both itself and the horse body and, similarly, how could the human ingest sufficient food to sustain both parts.
Brandon Mull's "Fablehaven" series features Centaurs that live in an area called Grunhold. The Centaurs are portrayed as a proud, elitist group of beings that consider themselves superior to all other creatures. The fourth book also has a variation on the species called an Alcetaur, which is part man, part moose.
Centaur appears in the novel by John Updike (The Centaur, 1963). The author depicts a rural Pennsylvanian town as seen through the optics of the myth of Centaur. An unknown and marginalized local school teacher, just like the mythological Chiron did for Prometheus, gave up his life for the future of his son who had chosen to be an independent artist in New York.
See also.
Other hybrid creatures appear in Greek mythology, always with some liminal connection that links Hellenic culture with archaic or non-Hellenic cultures:
Also,

</doc>
<doc id="7403" url="http://en.wikipedia.org/wiki?curid=7403" title="Chemotaxis">
Chemotaxis

Chemotaxis (from "chemo-" + "taxis") is movement of an organism in response to a chemical stimulus. Somatic cells, bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming toward the highest concentration of food molecules, or to flee from poisons (e.g., phenol). In multicellular organisms, chemotaxis is critical to early development (e.g., movement of sperm towards the egg during fertilization) and subsequent phases of development (e.g., migration of neurons or lymphocytes) as well as in normal function. In addition, it has been recognized that mechanisms that allow chemotaxis in animals can be subverted during cancer metastasis.
"Positive" chemotaxis occurs if the movement is toward a higher concentration of the chemical in question. However, "negative" chemotaxis occurs if the movement is in the opposite direction. Chemically prompted kinesis (randomly directed or nondirectional) can be called chemokinesis.
History of chemotaxis research.
Aside from the skin, neutrophils are the body's first line of defense against bacterial infections. After leaving nearby blood vessels, these cells recognize chemicals produced by bacteria in a cut or scratch and migrate "toward the smell". The above neutrophils were placed in a gradient of fMLP (N-formyl-methionine-leucine-phenylalanine), a peptide chain produced by some bacteria. Although migration of cells was detected from the early days of the development of microscopy (Leeuwenhoek), erudite description of chemotaxis was first made by T W. Engelmann (1881) and W.F. Pfeffer (1884) in bacteria and H.S. Jennings (1906) in ciliates. The Nobel Prize laureate I. Metchnikoff also contributed to the study of the field with investigations of the process as an initial step of phagocytosis. The significance of chemotaxis in biology and clinical pathology was widely accepted in the 1930s. The most fundamental definitions belonging to the phenomenon were also drafted by this time. The most important aspects in quality control of chemotaxis assays were described by H. Harris in the 1950s. In the 1960s and 1970s, the revolution of modern cell biology and biochemistry provided a series of novel techniques that became available to investigate the migratory responder cells and subcellular fractions responsible for chemotactic activity. The pioneering works of J. Adler represented a significant turning point in understanding the whole process of intracellular signal transduction of bacteria.
On November 3, 2006, Dr. Dennis Bray of University of Cambridge was awarded the Microsoft Award for his work on chemotaxis on "E. coli".
Chemoattractants and chemorepellents.
Chemoattractants and chemorepellents are inorganic or organic substances possessing chemotaxis-inducer effect in motile cells. Effects of chemoattractants are elicited via described or hypothetic chemotaxis receptors, the chemoattractant moiety of a ligand is target cell specific and concentration dependent. Most frequently investigated chemoattractants are formyl peptides and chemokines. Responses to chemorepellents result in axial swimming and they are considered a basic motile phenomena in bacteria. The most frequently investigated chemorepellents are inorganic salts, amino acids, and some chemokines.
Bacterial chemotaxis.
Some bacteria, such as "E. coli", have several flagella per cell (4–10 typically). These can rotate in two ways:
The directions of rotation are given for an observer outside the cell looking down the flagella toward the cell.
Behavior.
The overall movement of a bacterium is the result of alternating tumble and swim phases. If one watches a bacterium swimming in a uniform environment, its movement will look like a random walk with relatively straight swims interrupted by random tumbles that reorient the bacterium. Bacteria such as "E. coli" are unable to choose the direction in which they swim, and are unable to swim in a straight line for more than a few seconds due to rotational diffusion. In other words, bacteria "forget" the direction in which they are going. By repeatedly evaluating their course, and adjusting if they are moving in the wrong direction, bacteria can direct their motion to find favorable locations with high concentrations of attractants (usually food) and avoid repellents (usually poisons).
In the presence of a chemical gradient bacteria will chemotax, or direct their overall motion based on the gradient. If the bacterium senses that it is moving in the correct direction (toward attractant/away from repellent), it will keep swimming in a straight line for a longer time before tumbling. If it is moving in the wrong direction, it will tumble sooner and try a new direction at random. In other words, bacteria like "E. coli" use temporal sensing to decide whether their situation is improving or not. In this way, it finds the location with the highest concentration of attractant (usually the source) quite well. Even under very high concentrations, it can still distinguish very small differences in concentration. Fleeing from a repellent works with the same efficiency.
This biased random walk is a result of simply choosing between two methods of random movement; namely tumbling and straight swimming. In fact, chemotactic responses such as "forgetting" direction and "choosing" movements resemble the decision-making abilities of higher life-forms with brains that process sensory data.
The helical nature of the individual flagellar filament is critical for this movement to occur. As such, the protein that makes up the flagellar filament, flagellin, is quite similar among all flagellated bacteria. Vertebrates seem to have taken advantage of this fact by possessing an immune receptor (TLR5) designed to recognize this conserved protein.
As in many instances in biology, there are bacteria that do not follow this rule. Many bacteria, such as "Vibrio", are monoflagellated and have a single flagellum at one pole of the cell. Their method of chemotaxis is different. Others possess a single flagellum that is kept inside the cell wall. These bacteria move by spinning the whole cell, which is shaped like a corkscrew.
Signal transduction.
Chemical gradients are sensed through multiple transmembrane receptors, called methyl-accepting chemotaxis proteins (MCPs), which vary in the molecules that they detect. These receptors may bind attractants or repellents directly or indirectly through interaction with proteins of periplasmatic space. The signals from these receptors are transmitted across the plasma membrane into the cytosol, where "Che proteins" are activated. The Che proteins alter the tumbling frequency, and alter the receptors.
Flagellum regulation.
The proteins CheW and CheA bind to the receptor. The activation of the receptor by an external stimulus causes autophosphorylation in the histidine kinase, CheA, at a single highly conserved histidine residue. CheA in turn transfers phosphoryl groups to conserved aspartate residues in the response regulators CheB and CheY [Note: CheA is a histidine kinase and it does not actively transfer the phosphoryl group. The response regulator CheB takes the phosphoryl group from CheA]. This mechanism of signal transduction is called a two-component system and is a common form of signal transduction in bacteria. CheY induces tumbling by interacting with the flagellar switch protein FliM, inducing a change from counter-clockwise to clockwise rotation of the flagellum. Change in the rotation state of a single flagellum can disrupt the entire flagella bundle and cause a tumble.
Receptor regulation.
CheB, when activated by CheA, acts as a methylesterase, removing methyl groups from glutamate residues on the cytosolic side of the receptor. It works antagonistically with CheR, a methyltransferase, which adds methyl residues to the same glutamate residues. If the level of an attractant remains high, the level of phosphorylation of CheA (and, therefore, CheY and CheB) will remain low, the cell will swim smoothly, and the level of methylation of the MCPs will increase (because CheB-P is not present to demethylate). However, the MCPs no longer respond to the attractant when they are fully methylated. Therefore, even though the level of attractant might remain high, the level of CheA-P (and CheB-P) increases and the cell begins to tumble. However, now the MCPs can be demethylated by CheB-P, and, when this happens, the receptors can once again respond to attractants. The situation is the opposite with regard to repellents (fully methylated MCPs respond best to repellents, while least-methylated MCPs respond worst to repellents). This regulation allows the bacterium to 'remember' chemical concentrations from the recent past, a few seconds, and compare them to those it is currently experiencing, thus 'know' whether it is traveling up or down a gradient. Although the methylation system accounts for the wide range of sensitivity that bacteria have to chemical gradients, other mechanisms are involved in increasing the absolute value of the sensitivity on a given background. Well-established examples are the ultra-sensitive response of the motor to the CheY-P signal, and the clustering of chemoreceptors.
Eukaryotic chemotaxis.
The mechanism that eukaryotic cells employ is quite different from that in bacteria; however, sensing of chemical gradients is still a crucial step in the process. Due to their size, prokaryotes cannot detect effective concentration gradients, therefore these cells scan and evaluate their environment by a constant swimming (consecutive steps of straight swims and tumbles). In contrast to prokaryotes, the size of eukaryotic cells allows for the possibility of detecting gradients, which results in a dynamic and polarized distribution of receptors. Induction of these receptors by chemoattractants or chemorepellents results in migration towards or away from the chemotactic substance.
Levels of receptors, intracellular signalling pathways and the effector mechanisms all represent diverse, eukaryotic-type components. In eukaryotic unicellular cells, amoeboid movement and cilium or the eukaryotic flagellum are the main effectors (e.g., Amoeba or Tetrahymena). Some eukaryotic cells of higher vertebrate origin, such as immune cells also move to where they need to be. Besides immune competent cells (granulocyte, monocyte, lymphocyte) a large group of cells - considered previously to be fixed into tissues - are also motile in special physiological (e.g., mast cell, fibroblast, endothelial cells) or pathological conditions (e.g., metastases). Chemotaxis has high significance in the early phases of embryogenesis as development of germ layers is guided by gradients of signal molecules.
Motility.
Unlike motility in bacterial chemotaxis, the mechanism by which eukaryotic cells physically move is unclear. There appear to be mechanisms by which an external chemotactic gradient is sensed and turned into an intracellular PIP3 gradient, which results in a gradient and the activation of a signaling pathway, culminating in the polymerisation of actin filaments. The growing distal end of actin filaments develops connections with the internal surface of the plasma membrane via different sets of peptides and results in the formation of pseudopods.
Cilia of eukaryotic cells can also produce chemotaxis; in this case, it is mainly a Ca2+-dependent induction of the microtubular system of the basal body and the beat of the 9+2 microtubules within cilia. The orchestrated beating of hundreds of cilia is synchronized by a submembranous system built between basal bodies.
The details of the signaling pathways are still not totally clear.
Chemotaxis related migratory responses.
Although chemotaxis is the most frequently studied form of migration there are several other forms of locomotion in the cellular level.
Receptors.
In general, eukaryotic cells sense the presence of chemotactic stimuli through the use of 7-transmembrane (or serpentine) heterotrimeric G-protein-coupled receptors. This class of receptors is huge, representing a significant portion of the genome. Some members of this gene superfamily are used in eyesight (rhodopsins) as well as in olfaction (smelling).
The main classes of chemotaxis receptors are triggered by formyl peptides - formyl peptide receptors (FPR), chemokines - chemokine receptors (CCR or CXCR) and leukotrienes - leukotriene receptors (BLT); however, induction of a wide set of membrane receptors (e.g., amino acids, insulin, vasoactive peptides) also elicit migration of the cell.
Chemotactic selection.
While some chemotaxis receptors are expressed in the surface membrane with long-term characteristics, as they are determined genetically, others have short-term dynamics, as they are assembled "ad hoc" in the presence of the ligand. The diverse features of the chemotaxis receptors and ligands allows for the possibility of selecting chemotactic responder cells with a simple chemotaxis assay. By chemotactic selection, we can determine whether a still-uncharacterized molecule acts via the long- or the short-term receptor pathway.
The term "chemotactic selection" is also used to designate a technique that separates eukaryotic or prokaryotic cells according to their chemotactic responsiveness to selector ligands.
Chemotactic ligands.
The number of molecules capable of eliciting chemotactic responses is relatively high, and we can distinguish primary and secondary chemotactic molecules. The main groups of the primary ligands are as follows:
Investigations of the three-dimensional structures of chemokines proved that a characteristic composition of beta-sheets and an alpha helix provides expression of sequences required for interaction with the chemokine receptors. Formation of dimers and their increased biological activity was demonstrated by crystallography of several chemokines, e.g. IL-8.
Chemotactic range fitting (CRF).
Chemotactic responses elicited by the ligand-receptor interactions are, in general, distinguished upon the optimal effective concentration(s) of the ligand. Nevertheless, correlation of the amplitude elicited and ratio of the responder cells compared to the total number are also characteristic features of the chemotactic signaling. Investigations of ligand families (e.g., amino acids or oligo peptides) proved that there is a fitting of ranges (amplitudes; number of responder cells) and chemotactic activities: Chemoattractant moiety is accompanied by wide ranges, whereas chemorepellent character by narrow ranges.
Clinical significance.
A changed migratory potential of cells has relatively high importance in the development of several clinical symptoms and syndromes.
Altered chemotactic activity of extracellular (e.g., Escherichia coli) or intracellular (e.g., Listeria monocytogenes) pathogens itself represents a significant clinical target. Modification of endogenous chemotactic ability of these microorganisms by pharmaceutical agents can decrease or inhibit the ratio of infections or spreading of infectious diseases.
Apart from infections, there are some other diseases wherein impaired chemotaxis is the primary etiological factor, as in Chediak-Higashi syndrome, where giant intracellular vesicles inhibit normal migration of cells.
Mathematical models.
Several mathematical models of chemotaxis were developed depending on the type of
Although interactions of the factors listed above make the behavior of the solutions of mathematical models of chemotaxis rather complex, it is possible to describe the basic phenomenon of chemotaxis-driven motion in a straightforward way.
Indeed, let us denote with formula_1 the spatially non-uniform concentration of the chemo-attractant and with formula_2 its gradient. Then the chemotactic cellular flow (also called current) formula_3 that is generated by the chemotaxis is linked to the above gradient by the law: formula_4, where formula_5 is the spatial density of the cells and formula_6 is the so-called ’Chemotactic coefficient’. However, note that in many cases formula_6 is not constant: It is, instead, a decreasing function of the concentration of the chemo-attractant formula_8: formula_9.
In the mirror of publications.
Research on cell migration - as discussed above in ’History of chemotaxis research’– requires complementary application of classic and modern techniques. The field contributes to both basic research and applied science. In the last 20–25 years, due to the factors mentioned above, there has been an increase in the number of publications dealing mainly with chemotaxis. Nevertheless, other publications written in genetics, biochemistry, cell-physiology, pathology and clinical sciences could also incorporate data about migration or especially the chemotaxis of cells. A curiosity of migration research is that, among several works investigating taxes (e.g., thermotaxis, geotaxis, phototaxis), chemotaxis research shows a significantly high ratio, which points to the underlined importance of chemotaxis research both in biology and medicine.
Measurement of chemotaxis.
A wide range of techniques is available to evaluate chemotactic activity of cells or the chemoattractant and chemorepellent character of ligands.
The basic requirements of the measurement are as follows:
Despite the fact that an ideal chemotaxis assay is still not available, there are several protocols and pieces of equipment that offer good correspondence with the conditions described above. The most commonly used are summarised in the table below:
Artificial.
"Chemical robots" that use artificial chemotaxis to navigate autonomously have been designed. Applications include targeted delivery of drugs in the body.

</doc>
<doc id="7406" url="http://en.wikipedia.org/wiki?curid=7406" title="Cheshire">
Cheshire

Cheshire ( or ; archaically the County Palatine of Chester; abbreviated Ches.) is a ceremonial county in North West England, in the United Kingdom.
The western edge of the county forms part of England's border with Wales. Cheshire's county town is the city of Chester, although the largest town is Warrington, which historically was in Lancashire. Other major towns include Widnes, Congleton, Crewe, Ellesmere Port, Runcorn, Macclesfield, Winsford, Northwich, and Wilmslow. Historically the county contained the Wirral, Stockport, Sale, Altrincham and other towns. The county is bordered by Merseyside and Greater Manchester to the north, Derbyshire to the east, Staffordshire and Shropshire to the south, and Wrexham and Flintshire in Wales to the west. Cheshire is also a part of the Welsh Marches.
Cheshire's area is and its population is around 1 million. Apart from the large towns along the River Mersey and the historic city of Chester, it is mostly rural, with a number of small towns and villages that support an agricultural industry. It is historically famous as a former principality and for the production of Cheshire cheese, salt, bulk chemicals, and woven silk.
History.
Toponymy.
Cheshire's name was originally derived from an early name for Chester, and was first recorded as "Legeceasterscir" in the "Anglo-Saxon Chronicle", meaning "the shire of the city of legions". Although the name first appears in 980, it is thought that the county was created by Edward the Elder around 920. In the Domesday Book, Chester was recorded as having the name "Cestrescir" (Chestershire), derived from the name for Chester at the time. A series of changes that occurred as English itself changed, together with some simplifications and elision, resulted in the name Cheshire, as it occurs today.
Because of the historically close links with the land bordering Cheshire to the west, which became modern Wales, there is a history of interaction between Cheshire and North Wales. The Domesday Book records Cheshire as having two complete Hundreds (Atiscross and Exestan) that later became the principal part of Flintshire. Additionally, another large portion of the Duddestan Hundred later became known as Maelor Saesneg when it was transferred to North Wales. For this and other reasons, the Welsh name for Cheshire ("Swydd Gaerlleon") is sometimes used within Wales and by Welsh speakers.
Administrative history.
After the Norman conquest of 1066 by William I, dissent and resistance continued for many years after the invasion. In 1069 local resistance in Cheshire was finally put down using draconian measures as part of the Harrying of the North. The ferocity of the campaign against the Saxon populace was enough to end all future resistance. Examples were made of major landowners such as Saxon Earl Edwin of Mercia, their properties confiscated and redistributed amongst Norman barons. William I made Cheshire a county palatine and gave Gerbod the Fleming the new title of Earl of Chester. When Gerbod returned to Normandy in about 1070, the king used his absence to declare the earldom forfeit and gave the title to Hugh d'Avranches (nicknamed Hugh Lupus, or "wolf"). Due to Cheshire's strategic location on Welsh Marches, the Earl had complete autonomous powers to rule on behalf of the king in the county palatine.
Palatine hundreds.
Cheshire in the "Domesday Book" (1086) is recorded as a much larger county than it is today. It included two hundreds, Atiscross and Exestan, that later became part of North Wales. At the time of the "Domesday Book", it also included as part of Duddestan Hundred the area of land later known as English Maelor (which used to be a detached part of Flintshire) in Wales. The area between the Mersey and Ribble (referred to in the Domesday Book as "Inter Ripam et Mersam") formed part of the returns for Cheshire. Although this has been interpreted to mean that at that time south Lancashire was part of Cheshire, more exhaustive research indicates that the boundary between Cheshire and what was to become Lancashire remained the River Mersey. With minor variations in spelling across sources, the complete list of hundreds of Cheshire at this time are: Atiscross, Bochelau, Chester, Dudestan, Exestan, Hamestan, Middlewich, Riseton, Roelau, Tunendune, Warmundestrou and Wilaveston.
Palatine feudal baronies.
Feudal baronies or baronies by tenure were granted by the Earl as forms of feudal land tenure within the palatinate in a similar way to which the king granted English feudal baronies within England proper. An example is the barony of Halton. One of Hugh d'Avranche's barons has been identified as Robert Nicholls, Baron of Halton and Montebourg.
Lands devolved to Lancashire.
In 1182 the land north of the Mersey became administered as part of the new county of Lancashire, thus resolving any uncertainty about the county in which the land "Inter Ripam et Mersam" was. Over the years, the ten hundreds consolidated and changed names to leave just seven—Broxton, Bucklow, Eddisbury, Macclesfield, Nantwich, Northwich and Wirral.
Acquires Welsh-March lands.
In 1397 the county had lands in the march of Wales added to its territory, and was promoted to the rank of principality. This was because of the support the men of the county had given to King Richard II, in particular by his standing armed force of about 500 men called the "Cheshire Guard". As a result the King's title was changed to "King of England and France, Lord of Ireland, and Prince of Chester". No other English county has been honoured in this way, although it lost the distinction on Richard's fall in 1399.
Lands devolved to Greater Manchester.
Through the Local Government Act 1972, which came into effect on 1 April 1974, some areas in the north-west became part of the metropolitan counties of Greater Manchester and Merseyside. Stockport (previously a county borough), Altrincham, Hyde, Dukinfield and Stalybridge in the north-east became part of Greater Manchester. Much of the Wirral Peninsula in the north-west, including the county boroughs of Birkenhead and Wallasey, joined Merseyside. At the same time the Tintwistle Rural District was transferred to Derbyshire. The area of Lancashire south of the Merseyside/Greater Manchester area, including Widnes and the county borough of Warrington, was added to the new non-metropolitan county of Cheshire.
Unitary Authorities created.
Halton and Warrington became unitary authorities independent of Cheshire County Council on 1 April 1998, but remain part of Cheshire for ceremonial purposes and also for fire and policing.
Regional Assemblies proposed.
A referendum for a further local government reform connected with an elected regional assembly was planned for 2004, but was abandoned.
Abolition of Cheshire County Council.
As part of the local government restructuring in April 2009, Cheshire County Council and the Cheshire districts were abolished and replaced by two new unitary authorities, Cheshire East and Cheshire West and Chester. The existing unitary authorities of Halton and Warrington were not affected by the change.
Buildings and structures.
Prehistoric burial grounds have been discovered at The Bridestones, near Congleton (Neolithic) and Robin Hood's Tump, near Alpraham (Bronze Age). The remains of Iron Age hill forts are found on sandstone ridges at several locations in Cheshire. Examples include Maiden Castle on Bickerton Hill, Helsby Hillfort and Woodhouse Hillfort at Frodsham. The Roman fortress and walls of Chester, perhaps the earliest building works in Cheshire remaining above ground, are constructed from purple-grey sandstone.
The distinctive local red sandstone has been used for many monumental and ecclesiastical buildings throughout the county: for example, the medieval Beeston Castle, Chester Cathedral and numerous parish churches. Occasional residential and industrial buildings, such as Helsby railway station (1849), are also in this sandstone.
Many surviving buildings from the 15th to 17th centuries are timbered, particularly in the southern part of the county. Notable examples include the moated manor house Little Moreton Hall, dating from around 1450, and many commercial and residential buildings in Chester, Nantwich and surrounding villages.
Early brick buildings include Peover Hall near Macclesfield (1585), Tattenhall Hall (pre-1622), and the Pied Bull Hotel in Chester (17th century). From the 18th century, orange, red or brown brick became the predominant building material used in Cheshire, although earlier buildings are often faced or dressed with stone. Examples from the Victorian period onwards often employ distinctive brick detailing, such as brick patterning and ornate chimney stacks and gables. Notable examples include Arley Hall near Northwich, Willington Hall near Chester (both by Nantwich architect George Latham) and Overleigh Lodge, Chester. From the Victorian era, brick buildings often incorporate timberwork in a mock Tudor style, and this hybrid style has been used in some modern residential developments in the county. Industrial buildings, such as the Macclesfield silk mills (for example, ), are also usually in brick.
Physical geography.
Cheshire covers a boulder clay plain separating the hills of North Wales and the Peak District of Derbyshire (the area is also known as the Cheshire Gap). This was formed following the retreat of ice age glaciers which left the area dotted with kettle holes, locally referred to as meres. The bedrock of this region is almost entirely Triassic sandstone, outcrops of which have long been quarried, notably at Runcorn, providing the distinctive red stone for Liverpool Cathedral and Chester Cathedral.
The eastern half of the county is Upper Triassic Mercia Mudstone laid down with large salt deposits which were mined for hundreds of years around Northwich. Separating this area from Lower Triassic Sherwood Sandstone to the west is a prominent sandstone ridge known as the Mid Cheshire Ridge. A footpath, the Sandstone Trail, follows this ridge from Frodsham to Whitchurch passing Delamere Forest, Beeston Castle and earlier Iron Age forts.
The highest point in Cheshire is Shining Tor on the Derbyshire/Cheshire border between Macclesfield and Buxton, at above sea level. Before county boundary alterations in 1974, the county top was Black Hill () near Crowden in the far east of the historic county on the border with the West Riding of Yorkshire. Black Hill is now the highest point in West Yorkshire.
Demography.
Population.
Based on the Census of 2001, the overall population of Cheshire is 673,781, of which 51.3% of the population were male and 48.7% were female. Of those aged between 0–14 years, 51.5% were male and 48.4% were female; and of those aged over 75 years, 62.9% were female and 37.1% were male.
The population density of Cheshire is 32 people per km², lower than the North West average of 42 people/km² and the England and Wales average of 38 people/km². Ellesmere Port and Neston has a greater urban density than the rest of the county with 92 people/km².
The population for 2021 is forecast to be 708,000.
Ethnicity.
Ethnic white groups accounted for 98% (662,794) of the population with 10,994 (2%) in ethnic groups other than white.
Of the 2% in non-white ethnic groups:
Politics and administration.
Current.
Cheshire is a ceremonial county. This means that although there is no county-wide elected local council, Cheshire has a Lord Lieutenant and High Sheriff for ceremonial purposes under the Lieutenancies Act 1997.
Local government functions apart from the Police and Fire/Rescue services are carried out by four smaller unitary authorities: Cheshire East, Cheshire West and Chester, Halton, and Warrington. All four unitary authority areas have borough status.
Policing and fire and rescue services are still provided across the County as a whole, but by unelected bodies. The Cheshire Police Authority and Cheshire Fire Authority consist of members of the four councils.
Transition from the previous (1974) arrangement.
From 1 April 1974 the area under the control of the county council was divided into eight local government districts; Chester, Congleton, Crewe and Nantwich, Ellesmere Port and Neston, Halton, Macclesfield, Vale Royal and Warrington. Halton (which includes the towns of Runcorn and Widnes) and Warrington became unitary authorities in 1998. The remaining districts and the county were abolished as part of local government restructuring on 1 April 2009. The Halton and Warrington boroughs were not affected by the 2009 restructuring.
On 25 July 2007, the Secretary of State Hazel Blears announced she was 'minded' to split Cheshire into two new unitary authorities, Cheshire West and Chester, and Cheshire East. She confirmed she had not changed her mind on 19 December 2007 and therefore the proposal to split two-tier Cheshire into two would proceed.
Cheshire County Council leader Paul Findlow, who attempted High Court legal action against the proposal, claimed that splitting Cheshire would only disrupt excellent services while increasing living costs for all. A widespread sentiment that this decision was taken by the European Union long ago has often been portrayed via angered letters from Cheshire residents to local papers. On 31 January 2008 "The Standard", Cheshire & district newspaper, announced that the legal action had been dropped. Members against the proposal were advised that they may be unable to persuade the court that the decision of Hazel Blears was "manifestly absurd".
The Cheshire West and Chester unitary authority covers the area formerly occupied by the City of Chester and the boroughs of Ellesmere Port and Neston and Vale Royal; Cheshire East now covers the area formerly occupied by the boroughs of Congleton, Crewe and Nantwich, and Macclesfield. The changes were implemented on 1 April 2009.
Congleton Borough Council pursued an appeal against the judicial review it lost in October 2007. The appeal was dismissed on 4 March 2008.
Borders.
The ceremonial county borders Merseyside, Greater Manchester, Derbyshire, Staffordshire and Shropshire in England along with Flintshire and Wrexham in Wales, arranged by compass directions as shown in the table. below. Cheshire also forms part of the North West England region.
Religion.
In the 2001 Census, 81% of the population (542,413) identified themselves as Christian; 124,677 (19%) did not identify with any religion or did not answer the question; 5,665 (1%) identified themselves as belonging to other major world religions; and 1,033 belonged to other religions.
The boundary of the Church of England Diocese of Chester follows most closely the pre-1974 county boundary of Cheshire, so it includes all of Wirral, Stockport, and the Cheshire panhandle that included Tintwistle Rural District council area. In terms of Roman Catholic church administration, most of Cheshire falls into the Roman Catholic Diocese of Shrewsbury.
Economy and industry.
Cheshire has a diverse economy with significant sectors including agriculture, automotive, bio-technology, chemical, financial services, food and drink, ICT, and tourism. The county is famous for the production of Cheshire cheese, salt and silk.
A mainly rural county, Cheshire has a high concentration of villages. Agriculture is generally based on the dairy trade, and cattle are the predominant livestock. Land use given to agriculture has fluctuated somewhat, and in 2005 totalled 1558 km² over 4,609 holdings. Based on holdings by EC farm type in 2005, 8.51 km² was allocated to dairy farming, with another 11.78 km² allocated to cattle and sheep.
The chemical industry in Cheshire was founded in Roman times, with the mining of salt in Middlewich and Northwich. Salt is still mined in the area by British Salt. The salt mining has led to a continued chemical industry around Northwich, with Brunner Mond based in the town. Other chemical companies, including Ineos (formerly ICI), have plants at Runcorn. The Shell Stanlow Refinery is at Ellesmere Port. The oil refinery has operated since 1924 and has a capacity of 12 million tonnes per year. 
Crewe was once the centre of the British railway industry, and remains a major railway junction. The Crewe railway works, built in 1840, employed 20,000 people at its peak, although the workforce is now less than 1,000. Crewe is also the home of Bentley cars. Also within Cheshire are manufacturing plants for Jaguar and Vauxhall Motors in Ellesmere Port. The county also has an aircraft industry, with the BAE Systems facility at Woodford Aerodrome, part of BAE System's Military Air Solutions division. The facility designed and constructed Avro Lancaster and Avro Vulcan bombers and the Hawker-Siddeley Nimrod. On the Cheshire border with Flintshire is the Broughton aircraft factory, more recently associated with Airbus.
Tourism in Cheshire from within the UK and overseas continues to perform strongly. Over 8 million nights of accommodation (both UK and overseas) and over 2.8 million visits to Cheshire were recorded during 2003.
At the start of 2003, there were 22,020 VAT-registered enterprises in Cheshire, an increase of 7% since 1998, many in the business services (31.9%) and wholesale/retail (21.7%) sectors. Between 2002 and 2003 the number of businesses grew in four sectors: public administration and other services (6.0%), hotels and restaurants (5.1%), construction (1.7%), and business services (1.0%). The county saw the largest proportional reduction between 2001 and 2002 in employment in the energy and water sector and there was also a significant reduction in the manufacturing sector. The largest growth during this period was in the other services and distribution, hotels and retail sectors.
Cheshire is considered to be an affluent county. Due to its proximity to the cities of Manchester and Liverpool, counter urbanisation is common. Cheshire West has a fairly large proportion of residents who work in Liverpool, while Cheshire East falls within Manchester's sphere of influence.
Education.
"See also: List of schools in Cheshire East; List of schools in Cheshire West and Chester; List of schools in Halton; List of schools in Warrington
All four local education authorities in Cheshire operate only comprehensive state school systems. When Altrincham, Sale and Bebbington were moved from Cheshire to Trafford and Merseyside in 1974, they took some former Cheshire selective schools. Today, there are three universities based in the county, the University of Chester, the Crewe campus of Manchester Metropolitan University and the Chester campus of The University of Law.
Culture, media and sports.
Cheshire has one league football team, Crewe Alexandra who play in League One, and two Conference Premier teams, Macclesfield Town and Chester. Chester City were also a League Two team until they were relegated to the Conference Premier in April 2009 and later dissolved. Stockport County, another former League club, were relegated to the Conference North (the sixth tier of English football) in 2012–13; Altrincham and Stalybridge Celtic are historic Cheshire's other representatives in the Conference North.
Cheshire also is represented in the highest level basketball league in the UK, the BBL, by Cheshire Phoenix (formerly Cheshire Jets).
Warrington Wolves are the premier Rugby League team in Cheshire and play in the Super League. Widnes Vikings are currently in Super League from the 2012 season. There are also numerous junior clubs in the county, including Chester Gladiators.
Cheshire County Cricket Club is one of the clubs that make up the Minor counties of English and Welsh cricket.
The county has also been home to many notable sportsmen and athletes, including footballers Dean Ashton, Djibril Cissé, Peter Crouch, Seth Johnson, Michael Owen and Wayne Rooney. Other local athletes have included cricketer Ian Botham, marathon runner Paula Radcliffe, oarsman Matthew Langridge, hurdler Shirley Strong, sailor Ben Ainslie, cyclist Sarah Storey and mountaineer George Mallory, who died in 1924 on Mount Everest.
Each May, Europe's largest motorcycle event, the Thundersprint, is held in Northwich.
Cheshire has also produced a military hero in Norman Cyril Jones, a World War I flying ace who won the Distinguished Flying Cross.
The county has produced several notable popular musicians, including Gary Barlow (Take That, born and raised in Frodsham), Harry Styles (singer with One Direction, raised in Holmes Chapel), John Mayall (John Mayall & the Bluesbreakers), Ian Astbury (The Cult), Tim Burgess (Charlatans) and Ian Curtis (Joy Division). Concert pianist Stephen Hough, singer Thea Gilmore and her producer husband Nigel Stonier also reside in Cheshire.
The county has also been home to several writers, including Hall Caine (1853–1931), popular romantic novelist and playwright; Alan Garner; Victorian novelist Elizabeth Gaskell, whose novel "Cranford" features her home town of Knutsford; and most famously Lewis Carroll, born and raised in Daresbury, hence the Cheshire Cat (a fictional cat popularised by Carroll in Alice's Adventures in Wonderland and known for its distinctive mischievous grin). Artists from the county include ceramic artist Emma Bossons and sculptor and photographer Andy Goldsworthy. Actors from Cheshire include Daniel Craig, the 6th James Bond; Dame Wendy Hiller; and Lewis McGibbon, best known for his role in "Millions".
Local radio stations in the county include Dee 106.3, Heart and Gold for Chester and West Cheshire, Silk FM for the east of the county, Signal 1 for the south, Wire FM for Warrington and Wish FM, which covers Widnes. Cheshire is one of the only counties (along with County Durham and Surrey) that does not have its own designated BBC Radio station. The majority of the county (south and east) are covered by BBC Radio Stoke, whilst BBC Radio Merseyside tends to cover the west. The BBC directs readers to Stoke and Staffordshire when Cheshire is selected on their website. The BBC covers the west with BBC Radio Merseyside, the north and east with BBC Radio Manchester and the south with BBC Radio Stoke. There were plans to launch BBC Radio Cheshire, but those were shelved in 2007 after a lower than expected BBC licence fee settlement.
Modern county emblem.
As part of a 2002 marketing campaign, the plant conservation charity Plantlife chose the cuckooflower as the county flower. Previously, a sheaf of golden wheat was the county emblem, a reference to the Earl of Chester's arms in use from the 12th century.
Settlements.
The county is home to some of the most affluent areas of northern England, including Alderley Edge, Wilmslow, Prestbury, Tarporley and Knutsford, named in 2006 as the most expensive place to buy a house in the north of England. The former Cheshire town of Altrincham was in second place. The area is sometimes referred to as The Golden Triangle on account of the area in and around the aforementioned towns and villages.
The cities and towns in Cheshire are:
Some settlements which were historically part of the county now fall under the counties of Derbyshire, Merseyside and Greater Manchester:
Transport.
Rail and road.
The main railway line through the county is the West Coast Main Line. Many trains call at Crewe (in the south of the county) and Warrington Bank Quay (in the north of the county) en route to London and Scotland, as well as Runcorn on the Liverpool branch of the WCML.
The major interchanges are:
In the east of Cheshire, Macclesfield station is served by Virgin Trains and CrossCountry, on the Manchester-London line. Services from Manchester to the south coast frequently stop at Macclesfield.
Cheshire has of roads, including of the M6, M62, M53 and M56 motorways, with 23 interchanges and four service areas. The M6 motorway at the Thelwall Viaduct carries 140,000 vehicles every 24 hours.
Waterways.
The Cheshire canal system includes several canals originally used to transport the county's industrial products (mostly chemicals). Nowadays they are mainly used for tourist traffic. The Cheshire Ring is formed from the Rochdale, Ashton, Peak Forest, Macclesfield, Trent and Mersey and Bridgewater canals.
The Manchester Ship Canal is a wide, stretch of water opened in 1894. It consists of the rivers Irwell and Mersey made navigable to Manchester for seagoing ships leaving the Mersey estuary. The canal passes through the north of the county via Runcorn and Warrington.

</doc>
<doc id="7407" url="http://en.wikipedia.org/wiki?curid=7407" title="County town">
County town

A county town is a county's administrative centre in the United Kingdom or Ireland. County towns are usually the location of administrative or judicial functions, or established over time as the "de facto" main town of a county. The concept of a county town eventually became detached from its original meaning of where the county administration or county hall is based. In fact, many county towns are no longer part of "their" administrative county. For example, Nottingham is administered by a unitary authority entirely separate from the rest of Nottinghamshire. Many county towns are classified as cities, but all are referred to as county towns regardless of whether city status is held or not.
County towns prior to the late 19th century reforms.
United Kingdom.
Historic counties of England.
This list shows county towns prior to the reforms of 1889.
Historic counties of Wales.
This list shows county towns prior to the reforms of 1889.
Counties of Northern Ireland.
Note – Despite the fact that Belfast is the capital of Northern Ireland, it is not the county town of any county. Greater Belfast straddles two counties ("Antrim" and "Down").
Republic of Ireland.
Notes:
^ indicates that the county no longer has an administration purpose
County towns post the late 19th century reforms.
With the creation of elected county councils in 1889 the location of administrative headquarters in some cases moved away from the traditional county town. Furthermore, in 1965 and 1974 there were major boundary changes in England and Wales and administrative counties were replaced with new metropolitan and non-metropolitan counties. The boundaries underwent further alterations between 1995 and 1998 to create unitary authorities and some of the ancient counties and county towns were restored. (Note: not all headquarters are or were called County Halls or Shire Halls e.g.: Cumbria County Council's HQ is called "The Courts"). Before 1974 many of the county halls were located in towns and cities that had the status of a county borough i.e.: a borough outside of the county council's jurisdiction.

</doc>
<doc id="7411" url="http://en.wikipedia.org/wiki?curid=7411" title="Constitution of Canada">
Constitution of Canada

The Constitution of Canada is the supreme law in Canada; the country's constitution is an amalgamation of codified acts and uncodified traditions and conventions. It is one of the oldest working constitutions in the world, with a basis in the Magna Carta. The constitution outlines Canada's system of government, as well as the civil rights of all Canadian citizens and those in Canada. Interpretation of the Constitution is called Canadian constitutional law.
The composition of the Constitution of Canada is defined in subsection 52(2) of the Constitution Act, 1982 as consisting of the Canada Act 1982 (including the Constitution Act, 1982), all acts and orders referred to in the schedule (including the Constitution Act, 1867, formerly The British North America Act, 1867), and any amendments to these documents. The Supreme Court of Canada held that the list is not exhaustive and includes a number of pre-confederation acts and unwritten components as well. See list of Canadian constitutional documents for details.
History of the constitution.
The first semblance of a constitution for Canada was the Royal Proclamation of 1763. The act renamed the northeasterly portion of the former French province of New France as Province of Quebec, roughly coextensive with the southern third of contemporary Quebec. The proclamation, which established an appointed colonial government, was the de facto constitution of Quebec until 1774, when the British parliament passed the Quebec Act, which expanded the province's boundaries to the Ohio and Mississippi Rivers, which was one of the grievances listed in the United States Declaration of Independence. Significantly, the Quebec Act also replaced the French criminal law presumption of guilty until proven innocent with the English criminal law presumption of innocent until proven guilty; but the French code or civil law system was retained for non-criminal matters.
The Treaty of Paris of 1783 ended the American War of Independence and sent a wave of British loyalist refugees northward to Quebec and Nova Scotia. In 1784, the two provinces were divided; Nova Scotia was split into Nova Scotia, Cape Breton Island (rejoined to Nova Scotia in 1820), Prince Edward Island, and New Brunswick, while Quebec was split into Lower Canada (southern Quebec) and Upper Canada (southern through lower northern Ontario). The winter of 1837–38 saw rebellion in both of the Canadas, with the result they were rejoined as the Province of Canada in 1841. This was reversed by the British North America Act in 1867 which established the Dominion of Canada.
Initially, on 1 July 1867, there were four provinces in confederation as "One dominion under the name of Canada": Canada West (former Upper Canada, now Ontario), Canada East (former Lower Canada, now Quebec), Nova Scotia, and New Brunswick. Title to the Northwest Territories was transferred by the Hudson’s Bay Company in 1870 and the province of Manitoba (the first to be established by the Parliament of Canada) was in the same year the first created out of it. British Columbia joined confederation in 1871, followed by Prince Edward Island in 1873. The Yukon Territory was created by Parliament in 1898, followed by Alberta and Saskatchewan in 1905. The Dominion of Newfoundland, Britain's oldest colony in the Americas, joined Canada as a province in 1949. Nunavut was created in 1999.
An Imperial Conference in 1926 that included the leaders of all Dominions and representatives from India (which then included Burma, Bangladesh, and Pakistan), led to the eventual creation of the Statute of Westminster in 1931. The statute, an essential transitory step from the British Empire to the Commonwealth of Nations, provided that all existing Dominions became fully sovereign of the United Kingdom (upon its ratification by the federal legislature for Canada) and all new Dominions would be fully sovereign upon the grant of Dominion status. Newfoundland never ratified the statute, so it was still subject to imperial authority when its entire system of government and economy collapsed in the mid-1930s. Canada did ratify the statute, but had requested an exception because the Canadian federal and provincial governments could not agree on an amending formula for the Canadian constitution. It would be another 50 years before this was achieved. In the interim, the British parliament periodically passed enabling acts with respect to amendments to Canada's constitution; this was never anything but a rubber stamp.
The patriation of the Canadian constitution was achieved in 1982 when the British and Canadian parliaments passed parallel acts: the Canada Act, 1982 ([UK] 1982, c.11), in London, and the Constitution Act, 1982, in Ottawa. Thereafter, the United Kingdom was formally absolved of any remaining responsibility for, or jurisdiction over, Canada and Canada became responsible for her own destiny. In a formal ceremony on Parliament Hill in Ottawa, Queen Elizabeth II signed both acts into law on 17 April 1982. The Canada Act/Constitution Act included the Canadian Charter of Rights and Freedoms. Prior to the charter, there were various statutes which protected an assortment of civil rights and obligations, but nothing was enshrined in the constitution until 1982. The charter has thus placed a strong focus upon individual and collective rights of the people of Canada.
Enactment of the Charter of Rights and Freedoms has also fundamentally changed much of Canadian constitutional law. The Magna Carta, which has constitutional status in Canada, was occasionally called into service in legal argument. Since 1982, however, the arguments have been easier to make, because lawyers have been able to cite the relevant sections of the constitution rather than rely upon legal abstraction. The act also codified many previously oral constitutional conventions and has made amendment of the constitution significantly more difficult. Previously, the Canadian federal constitution could be amended by solitary act of the Canadian or British parliaments, by formal or informal agreement between the federal and provincial governments, or even simply by adoption as ordinary custom of an oral convention or unwritten tradition that was perceived to be the best way to do something. Since the act, amendments must now conform to certain specified provisions in the written portion of the Canadian constitution.
Constitution Act, 1867.
This was an Act of the British parliament, originally called the British North America Act 1867. It outlined Canada's system of government, which combines Britain's Westminster model of parliamentary government with division of sovereignty (federalism). Although it is the first of 20 British North America Acts, it is still the most famous of these and is understood to be the document of Canadian Confederation. With the patriation of the Constitution in 1982, this Act was renamed "Constitution Act, 1867". In recent years, the 1867 document has mainly served as the basis on which the division of powers between the provinces and federal government have been analyzed.
Constitution Act, 1982.
Endorsed by all provincial governments except that of Quebec (led by René Lévesque), this was the formal Canadian Act of Parliament that achieved full and final political independence from the United Kingdom. Part V of this act established an amending formula for the Canadian constitution, the lack of which (due to more than 50 years of disagreement between the federal and provincial governments) was the only reason Canada's constitutional amendments still required approval by the British parliament after ratification of the Statute of Westminster in 1931.
In UK, the parallel act passed simultaneously by the British parliament was called the Canada Act 1982. As a bilingual act of parliament, the Canada Act 1982 has the distinction of being the only legislation in French that has been passed by an English or British parliament since Norman French (Law French) ceased to be the language of government in England.
Canadian Charter of Rights and Freedoms.
As noted above, this is Part I of the Constitution Act, 1982. The Charter is the constitutional guarantee of the civil rights and liberties of every citizen in Canada, such as freedom of expression, of religion, and of mobility. Part II addresses the rights of Canada's Aboriginal people.
It is written in plain language in order to ensure accessibility to the average citizen. It only applies to government and government actions with the intention to prevent government from creating laws that are unconstitutional.
Amending formula.
With the Constitution Act, 1982, amendments to the constitution must be done in accordance with Part V of the Constitution Act, 1982, which provides for five different amending formulae. Amendments can be brought forward under section 46(1) by any province or either level of the federal government. The general formula is set out in section 38(1), known as the "7/50 formula", requires: (a) assent from both the House of Commons and the Senate; (b) the approval of two-thirds of the provincial legislatures (at least seven provinces) representing at least 50% of the population (effectively, this would include at least Quebec or Ontario, as they are the most populous provinces). This formula specifically applies to amendments related to the proportionate representation in Parliament, powers, selection, and composition of the Senate, the Supreme Court and the addition of provinces or territories.
The other amendment formulae are for exceptional cases as provided by in the act. In the case of an amendment related to the Office of the Queen, the use of either official language (subject to section 43), the amending formula itself, or the composition of the Supreme Court, the amendment must be adopted by unanimous consent of all the provinces in accordance with section 41. In the case of an amendment related to provincial boundaries or the use of an official language within a province alone, the amendment must be passed by the legislatures affected by the amendment (section 43). In the case of an amendment that affects the federal government only, the amendment does not need approval of the provinces (section 44). The same applies to amendments affecting the provincial government alone (section 45).
Vandalism of the proclamation paper.
In 1983, Peter Greyson, an art student, entered Ottawa's National Archives (known today as Library and Archives Canada) and poured red paint mixed with glue over a copy of the proclamation of the 1982 constitutional amendment. He said he was displeased with the federal government's decision to allow United States missile testing in Canada and had wanted to "graphically illustrate to Canadians" how wrong he believed the government to be. A grapefruit-sized stain remains on the original document; restoration specialists opted to leave most of the paint intact, fearing that removal attempts would only cause further damage.
Sources of the constitution.
There are three general methods of constitutional entrenchment:
Unwritten sources.
The existence of an unwritten constitution was reaffirmed by the Supreme Court in "Reference re Secession of Quebec".
"The Constitution is more than a written text. It embraces the entire global system of rules and principles which govern the exercise of constitutional authority. A superficial reading of selected provisions of the written constitutional enactment, without more, may be misleading."
In practice, there have been three sources of unwritten constitutional law:

</doc>
<doc id="7424" url="http://en.wikipedia.org/wiki?curid=7424" title="Crochet">
Crochet

Crochet (; ) is a process of creating fabric from yarn, thread, or other material strands using a crochet hook. The word is derived from the French word "crochet", meaning "hook." Hooks can be made of materials such as metals, woods, or plastic and are commercially manufactured as well as produced by artisans. Crocheting, like knitting, consists of pulling loops of material through other loops, but additionally incorporates wrapping the working material around the hook one or more times. Crochet differs from knitting in that only one stitch is active at one time (exceptions being Tunisian crochet and broomstick lace), stitches made with the same diameter of yarn are comparably taller, and a single crochet hook is used instead of two knitting needles. Additionally, crochet has its own system of symbols to represent stitch types.
Etymology.
The word crochet comes from Old French crochet, "hook", diminutive of croche, feminine of croc, "of Germanic origin".
History.
Origins.
Lis Paludan theorizes that crochet evolved from traditional practices in Iran, South America, or China, but there is no decisive evidence of the craft being performed before its popularity in Europe during the 19th century. The earliest written reference to crochet refers to "slip stitch crochet|shepherd's knitting" from "The Memoirs of a Highland Lady" by Elizabeth Grant (1797–1830) in the 19th century. Some claim that the first published crochet patterns appeared in the Dutch magazine "Pénélopé" in 1824. Crochet patterns have recently been found in the Swedish magazine "Konst och nyhetsmagasin för medborgare av alla klasser" from 1819, discarding this earlier notion. There might exist even earlier examples in publications not previously scrutinised. Other indicators that crochet was new in the 19th century include the 1847 publication "A Winter's Gift", which provides detailed instructions for performing crochet stitches, although it presumes that readers understand the basics of other needlecrafts. Early references to the craft in "Godey's Lady's Book" in 1846 and 1847 refer to "crotchet" before the spelling standardized in 1848.
Kooler proposes that early industrialization is key to the development of crochet. Machine spun cotton thread became widely available and inexpensive in Europe and North America after the invention of the cotton gin and the spinning jenny, displacing hand spun linen for many uses. Crochet technique consumes more thread than comparable textile production methods and cotton is well suited to crochet.
Knit and knotted textiles survive from very early periods, but there are no surviving samples of crocheted fabric in any ethnological collection, or archeological source prior to 1800. These writers point to the tambour hooks used in tambour lace|tambour embroidery in France in the 18th century, and contend that the hooking of loops through fine fabric in tambour work evolved into "crochet in the air." Most samples of early work claimed to be crochet turn out to actually be samples of nålebinding.
Donna Kooler identifies a possible problem with the tambour hypothesis: period tambour hooks that survive in modern collections cannot produce crochet because the integral wing nut necessary for tambour work interferes with attempts at crochet.
However, Mrs. Gaugain, in her 1840 "The Lady's Assistant for Executing Useful and Fancy Designs in Knitting, Netting, and Crotchet Work", refers to "Tambour, or Crotchet," then proceeds to call it "tambour" in all the instructions, indicating a strong connection believed in at the time of crochet's beginning, and that it was, perhaps, the older name. That those hooks that survive cannot be used is the constant problem in archaeology: what was commonly used may have been usually worn out and didn't survive. Tambour lace on fine net is commonly taught with the crochet hook, these hooks with "integral wing nuts" being an expensive item.
Early crochet hooks ranged from primitive bent needles in a cork handle, used by poor Irish lace workers, to expensively crafted silver, brass, steel, ivory and bone hooks set into a variety of handles, some of which were better designed to show off a lady's hands than they were to work with thread. By the early 1840s, instructions for crochet were being published in England, particularly by Eleanor Riego de la Blanchardiere and Frances Lambert. These early patterns called for cotton and linen thread for lace, and wool yarn for clothing, often in vivid color combinations.
Early history.
In the 19th century, as Ireland was facing the Great Irish Famine (1845-1849), crochet lace work was introduced as a form of famine relief (the production of crocheted lace being an alternative way of making money for impoverished Irish workers). Mademoiselle Riego de la Blanchardiere is generally credited with the invention of Irish Crochet, publishing the first book of patterns in 1846. Irish lace became popular in Europe and America, and was made in quantity until the first World War.
Modern practice and culture.
Fashions in crochet changed with the end of the Victorian era in the 1890s. Crocheted laces in the new Edwardian era, peaking between 1910 and 1920, became even more elaborate in texture and complicated stitching.
The strong Victorian colours disappeared, though, and new publications called for white or pale threads, except for fancy purses, which were often crocheted of brightly colored silk and elaborately beaded. After World War I, far fewer crochet patterns were published, and most of them were simplified versions of the early 20th century patterns. After World War II, from the late 1940s until the early 1960s, there was a resurgence in interest in home crafts, particularly in the United States, with many new and imaginative crochet designs published for colorful doilies, potholders, and other home items, along with updates of earlier publications. These patterns called for thicker threads and yarns than in earlier patterns and included wonderful variegated colors. The craft remained primarily a homemaker's art until the late 1960s and early 1970s, when the new generation picked up on crochet and popularized granny squares, a motif worked in the round and incorporating bright colors.
Although crochet underwent a subsequent decline in popularity, the early 21st century has seen a revival of interest in handcrafts and DIY, as well as great strides in improvement of the quality and varieties of yarn. There are many more new pattern books with modern patterns being printed, and most yarn stores now offer crochet lessons in addition to the traditional knitting lessons. There are many books you can purchase from local book stores to teach yourself how to crochet whether it be as a beginner or intermediate. There are also many books for children and teenagers who are hoping to take up the hobby. 
Filet crochet, Tunisian crochet, tapestry crochet, broomstick lace, hairpin lace, cro-hooking, and Irish crochet are all variants of the basic crochet method.
Crochet has experienced a revival on the catwalk as well. Christopher Kane's Fall 2011 Ready-to-Wear collection makes intensive use of the granny square, one of the most basic of crochet motifs. In addition, crochet has been utilized many times by designers on the popular reality show "Project Runway". Even websites such as Etsy and Ravelry have made it easier for individual hobbyists to sell and distribute their patterns or projects across the internet.
Laneya Wiles released a music video titled "Straight Hookin'" which makes a play on the word "hookers," which has a double meaning for both "one who crochets" and "a prostitute."
Materials.
Basic materials required for crochet are a hook and some type of material that will be crocheted, most commonly yarn or thread. Additional tools are convenient for keeping stitches counted, measuring crocheted fabric, or making related accessories. Examples include cardboard cutouts, which can be used to make tassels, fringe, and many other items; a pom-pom circle, used to make pom-poms; a tape measure and a gauge measure, both used for measuring crocheted work and counting stitches; a row counter; and occasionally plastic rings, which are used for special projects.
In recent years, yarn selections have moved beyond synthetic and plant and animal-based fibers to include bamboo, qiviut, hemp, and banana stalks, to name a few.
Hook.
The crochet hook comes in many sizes and materials, such as bone, bamboo, aluminium, plastic, and steel. Because sizing is categorized by the diameter of the hook's shaft, a crafter aims to create stitches of a certain size in order to reach a particular gauge specified in a given pattern. If gauge is not reached with one hook, another is used until the stitches made are the needed size. Crafters may have a preference for one type of hook material over another due to aesthetic appeal, yarn glide, or hand disorders such as arthritis, where bamboo or wood hooks are favored over metal for the perceived warmth and flexibility during use. Hook grips and ergonomic hook handles are also available to assist crafters.
Steel crochet hooks range in size from 0.4 to 3.5 millimeters, or from 00 to 16 in American sizing. These hooks are used for fine crochet work such as doilies and lace.
Aluminium, bamboo, and plastic crochet hooks are available from 2.5 to 19 millimeters in size, or from B to S in American sizing.
Artisan-made hooks are often made of hand-turned woods, sometimes decorated with semi-precious stones or beads.
Crochet hooks used for Tunisian crochet are elongated and have a stopper at the end of the handle, while double-ended crochet hooks have a hook on both ends of the handle. There is also a double hooked apparatus called a Cro-hook that has become popular.
A hairpin loom is often used to create lacy and long stitches, known as hairpin lace. While this is not in itself a hook, it is a device used in conjunction with a crochet hook to produce stitches.
See : List of United States standard crochet hook and knitting needle sizes
Yarn.
Yarn for crochet is usually sold as balls or skeins (hanks), although it may also be wound on spools or cones. Skeins and balls are generally sold with a "yarn band", a label that describes the yarn's weight, length, dye lot, fiber content, washing instructions, suggested needle size, likely gauge, etc. It is a common practice to save the yarn band for future reference, especially if additional skeins must be purchased. Crocheters generally ensure that the yarn for a project comes from a single dye lot. The dye lot specifies a group of skeins that were dyed together and thus have precisely the same color; skeins from different dye lots, even if very similar in color, are usually slightly different and may produce a visible stripe when added onto existing work. If insufficient yarn of a single dye lot is bought to complete a project, additional skeins of the same dye lot can sometimes be obtained from other yarn stores or online.
The thickness or weight of the yarn is a significant factor in determining the gauge, i.e., how many stitches and rows are required to cover a given area for a given stitch pattern. Thicker yarns generally require large-diameter crochet hooks, whereas thinner yarns may be crocheted with thick or thin hooks. Hence, thicker yarns generally require fewer stitches, and therefore less time, to work up a given project. Patterns and motifs are coarser with thicker yarns and produce bold visual effects, whereas thinner yarns are best for refined or delicate patternwork. Yarns are standardly grouped by thickness into six categories: superfine, fine, light, medium, bulky and superbulky. Quantitatively, thickness is measured by the number of wraps per inch (WPI). The related "weight per unit length" is usually measured in tex or denier.
Before use, hanks are wound into balls in which the yarn emerges from the center, making crocheting easier by preventing the yarn from becoming easily tangled. The winding process may be performed by hand or done with a ballwinder and swift.
A yarn's usefulness is judged by several factors, such as its "loft" (its ability to trap air), its "resilience" (elasticity under tension), its washability and colorfastness, its "hand" (its feel, particularly softness vs. scratchiness), its durability against abrasion, its resistance to pilling, its "hairiness" (fuzziness), its tendency to twist or untwist, its overall weight and drape, its blocking and felting qualities, its comfort (breathability, moisture absorption, wicking properties) and its appearance, which includes its color, sheen, smoothness and ornamental features. Other factors include allergenicity, speed of drying, resistance to chemicals, moths, and mildew, melting point and flammability, retention of static electricity, and the propensity to accept dyes. Desirable properties may vary for different projects, so there is no one "best" yarn.
Although crochet may be done with ribbons, metal wire or more exotic filaments, most yarns are made by spinning fibers. In spinning, the fibers are twisted so that the yarn resists breaking under tension; the twisting may be done in either direction, resulting in an Z-twist or S-twist yarn. If the fibers are first aligned by combing them and the spinner uses a worsted type drafting method such as the short forward draw, the yarn is smoother and called a "worsted"; by contrast, if the fibers are carded but not combed and the spinner uses a woolen drafting method such as the long backward draw, the yarn is fuzzier and called "woolen-spun". The fibers making up a yarn may be continuous "filament" fibers such as silk and many synthetics, or they may be "staples" (fibers of an average length, typically a few inches); naturally filament fibers are sometimes cut up into staples before spinning. The strength of the spun yarn against breaking is determined by the amount of twist, the length of the fibers and the thickness of the yarn. In general, yarns become stronger with more twist (also called "worst"), longer fibers and thicker yarns (more fibers); for example, thinner yarns require more twist than do thicker yarns to resist breaking under tension. The thickness of the yarn may vary along its length; a "slub" is a much thicker section in which a mass of fibers is incorporated into the yarn.
The spun fibers are generally divided into animal fibers, plant and synthetic fibers. These fiber types are chemically different, corresponding to proteins, carbohydrates and synthetic polymers, respectively. Animal fibers include silk, but generally are long hairs of animals such as sheep (wool), goat (angora, or cashmere goat), rabbit (angora), llama, alpaca, dog, cat, camel, yak, and muskox (qiviut). Plants used for fibers include cotton, flax (for linen), bamboo, ramie, hemp, jute, nettle, raffia, yucca, coconut husk, banana trees, soy and corn. Rayon and acetate fibers are also produced from cellulose mainly derived from trees. Common synthetic fibers include acrylics, polyesters such as dacron and ingeo, nylon and other polyamides, and olefins such as polypropylene. Of these types, wool is generally favored for crochet, chiefly owing to its superior elasticity, warmth and (sometimes) felting; however, wool is generally less convenient to clean and some people are allergic to it. It is also common to blend different fibers in the yarn, e.g., 85% alpaca and 15% silk. Even within a type of fiber, there can be great variety in the length and thickness of the fibers; for example, Merino wool and Egyptian cotton are favored because they produce exceptionally long, thin (fine) fibers for their type.
A single spun yarn may be crochet as is, or braided or plied with another. In plying, two or more yarns are spun together, almost always in the opposite sense from which they were spun individually; for example, two Z-twist yarns are usually plied with an S-twist. The opposing twist relieves some of the yarns' tendency to curl up and produces a thicker, "balanced" yarn. Plied yarns may themselves be plied together, producing "cabled yarns" or "multi-stranded yarns". Sometimes, the yarns being plied are fed at different rates, so that one yarn loops around the other, as in bouclé. The single yarns may be dyed separately before plying, or afterwords to give the yarn a uniform look.
The dyeing of yarns is a complex art. Yarns need not be dyed; or they may be dyed one color, or a great variety of colors. Dyeing may be done industrially, by hand or even hand-painted onto the yarn. A great variety of synthetic dyes have been developed since the synthesis of indigo dye in the mid-19th century; however, natural dyes are also possible, although they are generally less brilliant. The color-scheme of a yarn is sometimes called its colorway. Variegated yarns can produce interesting visual effects, such as diagonal stripes.
Process.
Crocheted fabric is begun by placing a slip-knot loop on the hook (though other methods, such as a magic ring or simple folding over of the yarn may be used), pulling another loop through the first loop, and repeating this process to create a chain of a suitable length. The chain is either turned and worked in rows, or joined to the beginning of the row with a slip stitch and worked in rounds. Rounds can also be created by working many stitches into a single loop. Stitches are made by pulling one or more loops through each loop of the chain. At any one time at the end of a stitch, there is only one loop left on the hook. Tunisian crochet, however, draws all of the loops for an entire row onto a long hook before working them off one at a time. Like knitting, crochet can be worked either flat or in the round.
Types of Stitches.
There are five main types of basic stitches. 1. Chain Stitch - the most basic of all stitches and used to begin most projects. 2. Slip Stitch - used to join chain stitch to form a ring. 3. Single Crochet Stitch - easiest stitch to master 4. Half Double Crochet Stitch - the 'in-between' stitch 5. Double Crochet Stitch - many uses for this unlimited use stitch 
The more advanced stitches include the Shell Stitch, V Stitch, Spike Stitch, Afghan Stitch, Butterfly Stitch, Popcorn Stitch, and Crocodile Stitch.
International crochet terms and notations.
In the English-speaking crochet world, basic stitches have different names that vary by country. The differences are usually referred to as UK/US or British/American. To help counter confusion when reading patterns, a diagramming system using a standard international notation has come into use (illustration, left).
Another terminological difference is known as "tension" (UK) and "gauge" (US). Individual crocheters work yarn with a loose or a tight hold and, if unmeasured, these differences can lead to significant size changes in finished garments that have the same number of stitches. In order to control for this inconsistency, printed crochet instructions include a standard for the number of stitches across a standard swatch of fabric. An individual crocheter begins work by producing a test swatch and compensating for any discrepancy by changing to a smaller or larger hook. North Americans call this "gauge", referring to the end result of these adjustments; British crocheters speak of "tension", which refers to the crafter's grip on the yarn while producing stitches.
Differences from and similarities to knitting.
One of the more obvious differences is that crochet uses one hook while much knitting uses two needles. In most crochet, the artisan usually has only one live stitch on the hook (with the exception being Tunisian crochet), while a knitter keeps an entire row of stitches active simultaneously. Dropped stitches, which can unravel a fabric, rarely interfere with crochet work, due to a second structural difference between knitting and crochet. In knitting, each stitch is supported by the corresponding stitch in the row above and it supports the corresponding stitch in the row below, whereas crochet stitches are only supported by and support the stitches on either side of it. If a stitch in a finished crocheted item breaks, the stitches above and below remain intact, and because of the complex looping of each stitch, the stitches on either side are unlikely to come loose unless heavily stressed.
Round or cylindrical patterns are simple to produce with a regular crochet hook, but cylindrical knitting requires either a set of circular needles or three to five special double-ended needles. Many crocheted items are composed of individual motifs which are then joined together, are by sewing or crocheting, whereas knitting is usually composed of one fabric, such as entrelac.
Freeform crochet is a technique that can create interesting shapes in three dimensions because new stitches can be made independently of previous stitches almost anywhere in the crocheted piece. It is generally accomplished by building shapes or structural elements onto existing crocheted fabric at any place the crafter desires.
Knitting can be accomplished by machine, while many crochet stitches can only be crafted by hand. The height of knitted and crocheted stitches is also different: a single crochet stitch is twice the height of a knit stitch in the same yarn size and comparable diameter tools, and a double crochet stitch is about four times the height of a knit stitch.
While most crochet is made with a hook, there is also a method of crocheting with a knitting loom. This is called loomchet. Slip stitch crochet is very similar to knitting. Each stitch in slip stitch crochet is formed the same way as a knit or purl stitch which is then bound off. A person working in slip stitch crochet can follow a knitted pattern with knits, purls, and cables, and get a similar result.
It is a common perception that crochet produces a thicker fabric than knitting, tends to have less "give" than knitted fabric, and uses approximately a third more yarn for a comparable project than knitted items. Though this is true when comparing a single crochet swatch with a stockinette swatch, both made with the same size yarn and needle/hook, it is not necessarily true for crochet in general. Most crochet uses far less than 1/3 more yarn than knitting for comparable pieces, and a crocheter can get similar feel and drape to knitting by using a larger hook or thinner yarn. Tunisian crochet and slip stitch crochet can in some cases use less yarn than knitting for comparable pieces. According to sources claiming to have tested the 1/3 more yarn assertion, a single crochet stitch (sc) uses approximately the same amount of yarn as knit garter stitch, but more yarn than stockinette stitch. Any stitch using yarnovers uses less yarn than single crochet to produce the same amount of fabric. Cluster stitches, which are in fact multiple stitches worked together, will use the most length.
Standard crochet stitches like sc and dc also produce a thicker fabric, more like knit garter stitch. This is part of why they use more yarn. Slip stitch can produce a fabric much like stockinette that is thinner and therefore uses less yarn.
Charity.
It has been very common for people and groups to crochet clothing and other garments and then donate them to soldiers during war. People have also crocheted clothing and then donated it to hospitals, for sick patients and also for newborn babies. Sometimes groups will crochet for a specific charity purpose, such as crocheting for homeless shelters, nursing homes, etc.
It is also becoming increasingly popular to crochet hats (commonly referred to as "chemo caps") and donate them to cancer treatment centers, for those undergoing chemotherapy. During the month of October pink hats and scarves are made and proceeds are donated to breast cancer funds.
Mathematics and hyperbolic crochet.
Crochet patterns have an underlying mathematical structure—the pattern created by the regular presence or omission of stitches is the very essence of this artform. The similarities to Base2 math, with its series of 0s and 1s, are obvious. That is to say, a present stitch is like a "1", and a missing stitch is like a "0". The craft has been used to illustrate shapes in hyperbolic space that are difficult to reproduce using other media or are difficult to understand when viewed two-dimensionally. A hyperbolic model of a coral reef has also been constructed for environmental purposes
A paper model based on the pseudosphere was created by William Thurston, however, it was quite delicate. Crochet has been used by mathematician Daina Taimina in order to create a version of the hyperbolic plane. Daina Taimina used the art of crochet to create a strong, durable model (see related image), which received an exhibition by the Institute For Figuring.
As hyperbolic and mathematics-based crochet has continued to become more popular, there have been several events highlighting work from various fiber artists. Two such shows include at the Smithsonian in Washington D.C. and at Lafayette College in Pennsylvania.
Architecture.
In "Style in the technical arts", Gottfried Semper looks at the textile with great promise and historical precedent. In Section 53, he writes of the "loop stitch, or Noeud Coulant: a knot that, if untied, causes the whole system to unravel." In the same section, Semper confesses his ignorance of the subject of crochet but believes strongly that it is a technique of great value as a textile technique and possibly something more.
There are a small number of architects currently interested in the subject of crochet as it relates to architecture. The following publications, explorations and thesis projects can be used as a resource to see how crochet is being used within the capacity of architecture.
Graffiti.
In the past few years, a practice called yarn bombing, or the use of knitted or crocheted cloth to modify and beautify one's (usually outdoor) surroundings, emerged in the US and spread worldwide. Yarn bombers sometimes target existing pieces of graffiti for beautification. In 2010, an entity dubbed “the Midnight Knitter” hit West Cape May. Residents awoke to find knit cozies hugging tree branches and sign poles.

</doc>
<doc id="7425" url="http://en.wikipedia.org/wiki?curid=7425" title="Electromagnetic coil">
Electromagnetic coil

An electromagnetic coil is an electrical conductor such as a wire in the shape of a coil, spiral or helix. Electromagnetic coils are used in electrical engineering, in applications where electric currents interact with magnetic fields, in devices such as inductors, electromagnets, transformers, and sensor coils. Either an electric current is passed through the wire of the coil to generate a magnetic field, or conversely an external "time-varying" magnetic field through the interior of the coil generates an EMF (voltage) in the conductor.
A current through any conductor creates a circular magnetic field around the conductor due to Ampere's law. The advantage of using a coil shape is that it increases the strength of magnetic field produced by a given current. The magnetic fields generated by the separate turns of wire all pass through the center of the coil and add (superpose) to produce a strong field there. The more turns of wire, the stronger the field produced. Conversely, a "changing" external magnetic flux induces a voltage in a conductor such as a wire, due to Faraday's law of induction. The induced voltage can be increased by winding the wire into a coil, because the field lines intersect the circuit multiple times.
The direction of the magnetic field produced by a coil can be determined by the right hand grip rule. If the fingers of the right hand are wrapped around the magnetic core of a coil in the direction of conventional current through the wire, the thumb will point in the direction the magnetic field lines pass through the coil. The end of a magnetic core from which the field lines emerge is defined to be the North pole.
There are many different types of coils used in electric and electronic equipment.
Windings and taps.
The wire or conductor which constitutes the coil is called the winding. The hole in the center of the coil is called the core area or "magnetic axis". Each loop of wire is called a turn. In windings in which the turns touch, the wire must be insulated with a coating of nonconductive insulation such as plastic or enamel to prevent the current from passing between the wire turns. The winding is often wrapped around a "coil form" made of plastic or other material to hold it in place. The ends of the wire are brought out and attached to an external circuit. Windings may have additional electrical connections along their length; these are called taps. A winding which has a single tap in the center of its length is called center-tapped.
Coils can have more than one winding, insulated electrically from each other. When there are two or more windings around a common magnetic axis, the windings are said to be inductively coupled or magnetically coupled. A time-varying current through one winding will create a time-varying magnetic field which passes through the other winding, which will induce a time-varying voltage in the other windings. This is called a transformer.
Magnetic core.
Many electromagnetic coils have a magnetic core, a piece of ferromagnetic material like iron in the center to increase the magnetic field. The current through the coil magnetizes the iron, and the field of the magnetized material adds to the field produced by the wire. This is called a ferromagnetic-core or iron-core coil. A ferromagnetic core can increase the magnetic field of a coil by hundreds or thousands of times over what it would be without the core. A ferrite core coil is a variety of coil with a core made of ferrite, a ferrimagnetic ceramic compound. Ferrite coils have lower losses at high frequencies.
A coil without a ferromagnetic core is called an air-core coil. This includes coils wound on plastic or other nonmagnetic forms, as well as coils which actually have empty air space inside their windings.
Types of coils.
Coils can be classified by the frequency of the current they are designed to operate with:
Coils can be classified by their function:
Electromagnets.
Electromagnets are coils that generate a magnetic field for some external use, often to exert a mechanical force on something. A few specific types: 
Inductors.
Inductors or reactors are coils which generate a magnetic field which interacts with the coil itself, to induce a back EMF which opposes changes in current through the coil. Inductors are used as circuit elements in electrical circuits, to temporarily store energy or resist changes in current. A few types:
Transformers.
A transformer is a device with two or more magnetically coupled windings (or sections of a single winding). A time varying current in one coil (called the primary winding) generates a magnetic field which induces a voltage in the other coil (called the secondary winding). A few types: 
Transducer coils.
These are coils used to translate time-varying magnetic fields to electric signals, and vice versa. A few types:
There are also types of coil which don't fit into these categories.

</doc>
