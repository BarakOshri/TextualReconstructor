<doc id="9093" url="http://en.wikipedia.org/wiki?curid=9093" title="De Havilland Mosquito">
De Havilland Mosquito

The de Havilland DH.98 Mosquito was a British multi-role combat aircraft with a two-man crew that served during and after the Second World War. It was one of few operational front-line aircraft of the era constructed almost entirely of wood and was nicknamed "The Wooden Wonder". The Mosquito was also known affectionately as the "Mossie" to its crews. Originally conceived as an unarmed fast bomber, the Mosquito was adapted to roles including low to medium-altitude daytime tactical bomber, high-altitude night bomber, pathfinder, day or night fighter, fighter-bomber, intruder, maritime strike aircraft, and fast photo-reconnaissance aircraft. It was also used by the British Overseas Airways Corporation (BOAC) as a fast transport to carry small high-value cargoes to, and from, neutral countries, through enemy-controlled airspace.
When the Mosquito began production in 1941, it was one of the fastest operational aircraft in the world. Entering widespread service in 1942, the Mosquito was a high-speed, high-altitude photo-reconnaissance aircraft, continuing in this role throughout the war. From mid-1942 to mid-1943 Mosquito bombers flew high-speed, medium or low-altitude missions against factories, railways and other pinpoint targets in Germany and German-occupied Europe. From late 1943, Mosquito bombers were formed into the Light Night Strike Force and used as pathfinders for RAF Bomber Command's heavy-bomber raids. They were also used as "nuisance" bombers, often dropping Blockbuster bombs - 4,000 lb (1,812 kg) "cookies" - in high-altitude, high-speed raids that German night fighters were almost powerless to intercept.
As a night fighter, from mid-1942, the Mosquito intercepted "Luftwaffe" raids on the United Kingdom, notably defeating "Operation Steinbock" in 1944. Starting in July 1942, Mosquito night-fighter units raided "Luftwaffe" airfields. As part of 100 Group, it was a night fighter and intruder supporting RAF Bomber Command's heavy bombers and reduced bomber losses during 1944 and 1945. As a fighter-bomber in the Second Tactical Air Force, the Mosquito took part in "special raids", such as the attack on Amiens Prison in early 1944, and in precision attacks against Gestapo or German intelligence and security forces. Second Tactical Air Force Mosquitos supported the British Army during the 1944 Normandy Campaign. From 1943 Mosquitos with RAF Coastal Command strike squadrons attacked "Kriegsmarine" U-boats (particularly in the 1943 Bay of Biscay, where significant numbers were sunk or damaged) and intercepting transport ship concentrations.
The Mosquito flew with the Royal Air Force (RAF) and other air forces in the European, Mediterranean and Italian theatres. The Mosquito was also operated by the RAF in the South East Asian theatre, and by the Royal Australian Air Force (RAAF) based in the Halmaheras and Borneo during the Pacific War.
Development.
By the early-mid-1930s, de Havilland had a reputation for innovative high-speed aircraft with the DH.88 Comet racer. The later DH.91 Albatross airliner pioneered the composite wood construction that the Mosquito used. The 22-passenger Albatross could cruise at at , better than the Handley Page H.P.42 and other biplanes it was replacing. The wooden monocoque construction not only saved weight and compensated for the low power of the de Havilland Gipsy Twelve engines used by this aircraft, but simplified production and reduced construction time.
Air Ministry bomber requirements and concepts.
On 8 September 1936, the British Air Ministry issued Specification P.13/36 which called for a twin-engined medium bomber capable of carrying a bomb load of for with a maximum speed of at ; a maximum bomb load of which could be carried over shorter ranges was also specified. Aviation firms entered heavy designs with new high-powered engines and multiple defensive turrets, leading to the production of the Avro Manchester and Handley Page Halifax.
In May 1937, as a comparison to P.13/36, George Volkert, the chief designer of Handley Page, put forward the concept of a fast unarmed bomber. In 20 pages, Volkert planned an aerodynamically clean medium bomber to carry of bombs at a cruising speed of . There was support in the RAF and Air Ministry; Captain R N Liptrot, Research Director Aircraft 3 (RDA3), appraised Volkert's design, calculating that its top speed would exceed the new Supermarine Spitfire. There were, however, counter-arguments that, although such a design had merit, it would not necessarily be faster than enemy fighters for long. The ministry was also considering using non-strategic materials for aircraft production, which, in 1938, had led to specification B.9/38 and the Armstrong Whitworth Albemarle medium bomber, largely constructed from spruce and plywood attached to a steel-tube frame. The idea of a small, fast bomber gained support at a much earlier stage than sometimes acknowledged though it was unlikely that the Air Ministry envisaged it not using light alloy components.
Inception of the de Havilland fast bomber.
Geoffrey de Havilland also believed a bomber with an aerodynamic design, with minimal skin area, was better than the P.13/36 specification. He thought that adapting the Albatross to meet the RAF's requirements could save time. In April 1938, performance estimates were produced for a twin Rolls-Royce Merlin-powered DH.91, with the Bristol Hercules (radial engine) and Napier Sabre (H-engine) as alternatives. On 7 July 1938, Geoffrey de Havilland wrote to Air Marshal Wilfrid Freeman, the Air Council's member for Research and Development, discussing the specification and arguing that in war there would be shortages of duralumin and steel but should be plenty of wood. Although inferior torsionally, the strength to weight ratio of wood was as good as duralumin or steel, and a different approach to a high-speed bomber was possible.
A follow-up letter to Freeman on 27 July said that the P.13/36 specification could not be met by a twin Merlin-powered aircraft and either the top speed or load capacity would be compromised, depending on which was paramount. For example, a larger, slower, turret armed aircraft would have a range of carrying a bomb load, with a maximum of at , and a cruising speed of at . De Havilland believed that too much of a compromise, and that getting rid of surplus equipment would lead to a better design. On 4 October 1938, de Havilland projected the performance of another design based on the Albatross, powered by two Merlin Xs, with a three-man crew and six or eight forward-firing guns, plus one or two manually operated guns and a tail turret. Based on a total loaded weight of it would have a top speed of and cruising speed of at .
Still believing this could be improved, and after examining more concepts based on the Albatross and the new all-metal DH.95 Flamingo, de Havilland settled on designing a new aircraft that would be aerodynamically clean, wooden and powered by the Merlin, which offered substantial future development. The new design would be faster than foreseeable enemy fighter aircraft, and could dispense with a defensive armament, which would slow it and make interception or losses to anti-aircraft guns more likely. Instead, high speed and good manoeuvrability would make it easier to evade fighters and ground fire. The lack of turrets simplified production and reduced production time, with a delivery rate far in advance of competing designs. Without armament, the crew could be reduced to a pilot and navigator. Contemporary RAF design philosophy required well-armed heavy bombers more akin to the German "schnellbomber". At a meeting in early October 1938 with Geoffrey de Havilland and Charles Walker (de Havilland's chief engineer), the Air Ministry showed little interest, and instead asked de Havilland to build wings for other bombers as a sub-contractor.
By September 1939 de Havilland had produced preliminary estimates for single- and twin-engined variations of light-bomber designs using different engines, speculating on the effects of defensive armament on their designs. One design, completed on 6 September, was for an aircraft powered by a single Napier Sabre, with a wingspan of and capable of carrying a bomb load . On 20 September, in another letter to Wilfred Freeman, de Havilland wrote "...we believe that we could produce a twin-engine bomber which would have a performance so outstanding that little defensive equipment would be needed." By 4 October work had progressed to a twin-engine light bomber with a wingspan of , and powered by Merlin or Griffon engines, the Merlin favoured because of availability.
On 7 October 1939, with war a month old, the nucleus of a design team under Eric Bishop moved to the security and secrecy of Salisbury Hall and started on what was now known as the DH.98. To make the DH.98 more versatile, Bishop made provision for four 20 mm cannon in the forward half of the bomb bay, under the cockpit, firing via blast tubes and troughs under the fuselage.
The DH.98 was too radical for the Ministry, which wanted a heavily armed, multi-role aircraft, combining medium bomber, reconnaissance and general purpose roles, as well as capable of carrying torpedoes. With outbreak of war, the Ministry became more receptive, but still sceptical about an unarmed bomber. It thought the Germans would produce fighters faster than expected. It suggested two forward and two rear-firing machine guns for defence. The Ministry also opposed a two-man bomber, wanting at least a third crewman to reduce the work of the others on long flights. The Air Council added further requirements, such as remotely controlled guns, a top speed of at on two-thirds engine power, and a range of with a bomb load. To appease the Ministry, de Havilland built mock-ups with a gun turret just aft of the cockpit but, apart from this compromise, de Havilland made no changes.
On 12 November, at a meeting considering fast bomber ideas put forward by de Havilland, Blackburn, and Bristol, Marshal Freeman directed de Havilland to produce a fast aircraft, powered initially by Merlin engines, with options of using progressively more powerful engines, including the Rolls-Royce Griffon and the Napier Sabre. Although estimates were presented for a slightly larger Griffon-powered aircraft, armed with a four-gun tail turret, Freeman got the requirement for defensive weapons dropped, and a draft requirement was raised calling for a high-speed light reconnaissance bomber capable of at .
On 12 December, the Vice-Chief of the Air Staff, Director General of Research and Development, Air Officer Commanding-in-Chief (AOC-in-C) of RAF Bomber Command met to finalise the design and decide how to fit it in the RAF's aims. The AOC-in-C would not accept an unarmed bomber, but insisted it would be suitable for reconnaissance missions with F8 or F24 cameras. After representatives of the company, the Ministry and the RAF's operational commands examined a full scale mock-up at Hatfield on 29 December 1939, the project received backing. This was confirmed on 1 January 1940, when Air Marshal Freeman chaired another meeting with Geoffrey de Havilland, John Buchanan, (Deputy of Aircraft Production) and John Connolly, who was Buchanan's chief of staff. Claiming the DH.98 was the "fastest bomber in the world", de Havilland added "it must be useful". Freeman supported its production for RAF service and ordered a single prototype for an unarmed bomber variant to specification B.1/40/dh, which called for a light bomber/reconnaissance aircraft powered by two Rolls-Royce RM3SM (experimental designation for what became known as the Merlin 21) with ducted radiators, capable of carrying a bomb load. The aircraft was to have a speed of at and a cruising speed of at with a range of at on full tanks. Maximum service ceiling was to be .
On 1 March 1940, Air Marshal Roderic Hill issued a contract under Specification B.1/40, for 50 bomber-reconnaissance variants of the DH.98: this contract included the prototype, which was given the factory serial "E0234". In May 1940, specification F.21/40 was issued, calling for a long-range fighter armed with four 20 mm cannon and four .303 machine guns in the nose, after which de Havilland were authorised to build a prototype of a fighter version of the DH.98. It was decided after debate that this prototype, given the military serial number "W4052", would carry Airborne Interception (AI) Mk.IV equipment as a day and night fighter. By June 1940, the DH.98 had been named "Mosquito". Having the fighter variant kept the Mosquito project alive because there was still criticism within the government and Air Ministry of the usefulness of an unarmed bomber, even after the prototype had shown its capabilities.
Project Mosquito.
Once design of the DH.98 had started, de Havilland built mock-ups, the most detailed at Salisbury Hall, in the hangar where "E0234" was being built. Initially, this was designed with the crew enclosed in the fuselage behind a transparent nose (similar to the Bristol Blenheim or Heinkel He 111H), but this was quickly altered to a more solid nose with a more conventional canopy.
The construction of the prototype began in March 1940, but work was cancelled again after the Battle of Dunkirk, when Lord Beaverbrook, as Minister of Aircraft Production, decided there was no production capacity for aircraft like the DH.98, which was not expected to be in service until early 1941. Although Lord Beaverbrook told Air Vice-Marshal Freeman that work on the project had better stop, he did not issue a specific instruction, and Freeman ignored the request. In June 1940, however, Lord Beaverbrook and the Air Staff ordered that production was to focus on five existing types, namely the Supermarine Spitfire, Hawker Hurricane, Vickers Wellington, Armstrong-Whitworth Whitley and the Bristol Blenheim. Work on the DH.98 prototype stopped, and it seemed that the project would be shut down when the design team were denied the materials with which to build their prototype.
The Mosquito was only reinstated as a priority in July 1940, after de Havilland's General Manager L.C.L Murray, promised Lord Beaverbrook 50 Mosquitoes by December 1941, and this, only after Beaverbrook was satisfied that Mosquito production would not hinder de Havilland's primary work of producing Tiger Moth and Oxford trainers and repairing Hurricanes as well as the licence manufacture of Merlin engines. In promising Beaverbrook 50 Mosquitoes by the end of 1941, de Havilland was taking a gamble, because it was unlikely that 50 Mosquitos could be built in such a limited time; as it transpired only 20 Mosquitos were built in 1941, but the other 30 were delivered by mid-March 1942.
During the Battle of Britain, nearly a third of de Havilland's factory time was lost because the workers took cover in the factory's bomb shelters. Nevertheless, work on the prototype went quickly, such that "E0234" was rolled out on 19 November 1940.
In the aftermath of the Battle of Britain, the original order was changed to 20 bomber variants and 30 fighters. It was still uncertain whether the fighter version should have dual or single controls, or should carry a turret, so three prototypes were eventually built: "W4052", "W4053" and "W4073". The latter, both turret armed, were later disarmed, to become the prototypes for the T.III trainer. This caused some delays as half-built wing components had to be strengthened for the expected higher combat load requirements. The nose sections also had to be altered, omitting the clear perspex bomb-aimer's position, to solid noses designed to house four .303 machine guns and their ammunition.
Prototypes and test flights.
On 3 November 1940, the aircraft, still coded "E0234", was transported by road to Hatfield, and placed in a small blast-proof assembly building, where successful engine runs were made on 19 November. Two Merlin 21 two-speed single-stage supercharged engines were installed driving three-bladed de Havilland Hydromatic constant-speed, controllable-pitch propellers. "E0234" had a wingspan of and was the only Mosquito to be built with Handley Page slats on the outer leading edges of the wings. Test flights showed that these were not needed and they were disconnected and faired over with doped fabric (these slats can still be seen on "W4050"). On 24 November 1940, taxiing trials were carried out by Geoffrey de Havilland, Jr., who was the company's chief test pilot and responsible for maiden flights. The tests were successful and the prototype was subsequently readied for flight testing, making its first flight piloted by Geoffrey de Havilland, on 25 November. The flight was made 11 months after the start of detailed design work, a remarkable achievement considering the conditions of the time.
For this maiden flight, "E0234", weighing , took off from a field beside the shed it was built in. John E. Walker, Chief Engine Installation designer, accompanied de Havilland. The takeoff was "straight forward and easy" and the undercarriage was not retracted until a considerable height was obtained. The aircraft reached , with the only problem being the undercarriage doors - which were operated by bungee cords attached to the main undercarriage legs - that remained open by some at that speed. This problem persisted for some time. The left wing of "E0234" also had a tendency to drag to port slightly, so a rigging adjustment was carried out before further flights.
On 5 December 1940, the prototype experienced tail buffeting at speeds between and . The pilot noticed this most in the control column, with handling becoming more difficult. During testing on 10 December, wool tufts were attached to the suspect areas to investigate the direction of airflow. The conclusion was that the airflow separating from rear section of the inner engine nacelles was disturbed, leading to a localised stall and the disturbed airflow was striking the tailplane, causing the buffeting. In an attempt to smooth the air flow and deflect it from striking the tailplane with such force, the company experimented with non-retractable slots, fitted to the inner engine nacelles and to the leading edge of the tailplane. These slots, and wing root fairings fitted to the forward fuselage and leading edge of the radiator intakes, stopped some of the vibration experienced by the pilot but failed to cure the tailplane buffeting. In February 1941, the buffeting was eliminated by incorporating triangular fillets on the trailing edge of the wings and lengthening the nacelles, the trailing edge of which curved up to fair into the fillet some behind the wing's trailing edge: this meant the flaps had to be divided into inboard and outboard sections. 
By January 1941, the prototype was carrying the military serial number "W4050". With the buffeting problems largely resolved, John Cunningham flew "W4050" on 9 February 1941. He was greatly impressed by the "lightness of the controls and generally pleasant handling characteristics". Cunningham concluded that when the type was fitted with AI equipment, it would be a perfect replacement for the Bristol Beaufighter.
During its trials on 16 January 1941, "W4050" outpaced a Spitfire at . The original estimates were that as the Mosquito prototype had twice the surface area and over twice the weight of the Spitfire Mk II, but also with twice its power, the Mosquito would end up being faster. Over the next few months, "W4050" surpassed this estimate, easily beating the Spitfire Mk II in testing at RAF Boscombe Down in February 1941, reaching a top speed of at altitude, compared to a top speed of at for the Spitfire. On 19 February, official trials began at the Aeroplane and Armament Experimental Establishment (A&AEE) based at Boscombe Down, although de Havilland's representative was surprised by a delay in starting the tests. On 24 February, as "W4050" taxied across the rough airfield, the tailwheel jammed leading to the fuselage fracturing: this was replaced by the fuselage of the Photo-Reconnaissance prototype "W4051" in early March. In spite of this setback, the "Initial Handling Report 767" issued by the A&AEE stated "The aeroplane is pleasant to fly ... Aileron control light and effective ..." The maximum speed reached was at , with an estimated maximum ceiling of and a maximum rate of climb of at .
"W4050" continued to be used for long and varied testing programs, being essentially the experimental "workhorse" for the Mosquito family. In late October 1941, it was taken back to the factory to be fitted with Merlin 61s, the first production Merlins fitted with a two-speed, two-stage supercharger. The first flight with the new engines was on 20 June 1942. "W4050" recorded a maximum speed of at (fitted with straight-through air intakes with snowguards, engines in F.S. gear) and at without snowguards.In October 1942, in connection with development work on the NF Mk XV, "W4050" was fitted with extended wingtips increasing the span to , first flying in this configuration on 8 December. Finally, fitted with high-altitude rated two-stage, two-speed Merlin 77s, it reached in December 1943. Soon after these flights,"W4050" was grounded and was scheduled to be scrapped, but instead continued to serve as an instructional airframe at Hatfield. In September 1958, "W4050" was returned to the Salisbury Hall hangar in which it had been built, was restored to its original configuration and is now one of the primary exhibits of the de Havilland Aircraft Heritage Centre.
"W4051", which was designed from the outset to be the prototype for the photo-reconnaissance versions of the Mosquito, was slated to make its first flight in early 1941. However, the fuselage fracture in "W4050" meant that "W4051's" fuselage was used as a replacement; "W4051" was then rebuilt using a production standard fuselage and first flew on 10 June 1941. This prototype continued to use the short engine nacelles, single-piece trailing edge flaps and the "No. 1" tailplane used by "W4050", but had production standard wings and, thus configured, became the only Mosquito prototype to fly operationally.
Construction of the fighter prototype, "W4052" was also carried out at the secret Salisbury Hall facility. The new prototype differed from its bomber brethren in a number of ways. It was powered by Merlin 21s and had an altered canopy structure with a flat bullet-proof windscreen. Four 20 mm (.79 in) Hispano Mk II cannon were housed in a compartment under the cockpit floor, with the breeches projecting into the bomb bay. The bomb bay doors were replaced by manually operated bay doors which incorporated cartridge ejector chutes. The solid nose mounted four .303 in (7.7 mm) Browning machine guns and their ammunition boxes which were accessed by a single large, sideways hinged panel. In accordance with its role as a day/night fighter prototype "W4052" was equipped with AI Mk.IV equipment, complete with an "arrowhead"-shaped transmission aerial mounted between the central Brownings, and receiving aerials through each outer wingtip, and it was painted in overall black RDM2a "Special Night" finish. It was also the first prototype constructed with the extended engine nacelles.
"W4052" was later tested with other modifications including bomb racks, drop tanks, barrage balloon cable cutters in the leading edge of the wings, Hamilton airscrews and braking propellers, as well as drooping aileron systems which enabled steep approaches, and a larger rudder tab. The prototype continued to serve as a test machine until it was scrapped on 28 January 1946. "W4055" flew the first operational Mosquito flight on 17 September 1941.
During flight testing, the Mosquito prototypes were modified to test a number of experimental configurations. "W4050" was fitted with a turret behind the cockpit for drag tests, after which the idea was abandoned in July 1941. "W4052" had the first version of the Youngman Frill airbrake fitted to the fighter prototype. The frill was opened by bellows and venturi effect to provide rapid deceleration during interceptions and was tested between January - August 1942, but was also abandoned when it was discovered that lowering the undercarriage had the same effect with less buffeting.
Early on in the Mosquito's operational life, the cooling intake shrouds that were to cool the exhausts on production aircraft overheated after a while. Flame dampers prevented exhaust glow on night operations, but they had an effect on performance. Multiple ejector and open-ended exhaust stubs helped solve the problem and were used in the PR.VIII, B.IX and B.XVI variants. This increased speed performance in the B.IX alone by per hour.
Production plans and American interest.
The Air Ministry had authorised mass production plans to be drawn up on 21 June 1941, by which time the Mosquito had become one of the world's fastest operational aircraft. The Air Ministry ordered 19 photo-reconnaissance (PR) models and 176 fighters. A further 50 were unspecified; in July 1941, the Air Ministry confirmed these would be unarmed fast bombers. By the end of January 1942, contracts had been awarded for 1,378 Mosquitos of all variants, including 20 T.III trainers and 334 FB.VI bombers. Another 400 were to be built by de Havilland Canada.
On 20 April 1941, "W4050" was demonstrated to Lord Beaverbrook, the Minister of Aircraft Production. The Mosquito made a series of flights, including one rolling climb on one engine. Also present were US General Henry H. Arnold and his aide Major Elwood Quesada, who wrote "I ... recall the first time I saw the Mosquito as being impressed by its performance, which we were aware of. We were impressed by the appearance of the airplane that looks fast usually is fast, and the Mosquito was, by the standards of the time, an extremely well streamlined airplane, and it was highly regarded, highly respected."
The trials set up future production plans between Britain, Australia and Canada. The Americans did not pursue their interest. It was thought the Lockheed P-38 Lightning could handle the same duties just as easily. Arnold felt the design was being overlooked, and urged the strategic personalities in the United States Army Air Forces to learn from the design if they chose not to adopt it. Several days after the attack on Pearl Harbor, the USAAF then requested one airframe to evaluate on 12 December 1941, signifying that the USAAF realised that they had entered the war without a fast dual-purpose reconnaissance aircraft.
Design.
Overview.
The Mosquito was a fast, twin-engined aircraft with shoulder-mounted wings. The most-produced variant, designated the FB Mk VI (Fighter-bomber Mark 6), was powered by two Merlin Mk 23 or Mk 25 engines driving three-bladed de Havilland hydromatic propellers. The typical fixed armament for an FB Mk VI was four Browning .303 machine guns and four 20 mm Hispano cannon while the offensive load consisted of up to of bombs, or eight RP-3 unguided rockets.
Construction.
The oval-section fuselage was a frameless monocoque shell built in two halves being formed to shape by band clamps over a mahogany or concrete mould, each holding one half of the fuselage, split vertically. The shell halves were made of sheets of Ecuadorean balsawood sandwiched between sheets of Canadian birch, but in areas needing extra strength— such as along cut-outs— stronger woods replaced the balsa filler; the overall thickness of the birch and balsa sandwich skin was only . This sandwich skin was so stiff that no internal reinforcement was necessary from the wing's rear spar to the tail bearing bulkhead. The join was along the vertical centre line. This split construction greatly aided the assembly of the internal equipment as it allowed the technicians easy access to the fuselage interior. While the glue in the plywood skin dried, carpenters cut a sawtooth joint into the edges of the fuselage shells, while other workers installed the controls and cabling on the inside wall. When the glue completely dried, the two halves were glued and screwed together. The fuselage was strengthened internally by seven bulkheads made up of two plywood skins parted by spruce blocks, which formed the basis on each half for the outer shell. Each bulkhead was a repeat of the spruce design for the fuselage halves; a balsa sheet sandwich between two plywood sheets/skins. Bulkhead number seven carried the fittings and loads for the tailplane and rudder, The type of glue originally used was Casein, which was later replaced by "Aerolite", a synthetic urea-formaldehyde, which was more durable. Many other types of screws and flanges (made of various woods) also held the structure together.
The fuselage construction joints were made from balsa wood and plywood strips with the spruce multi-ply being connected by a balsa V joint, along with the interior frame. The spruce would be reinforced by plywood strips at the point where the two halves joined to form the V-joint. Located on top of the joint the plywood formed the outer skin. During the joining of the two halves ("boxing up"), two laminated wooden clamps would be used in the after portion of the fuselage to act as support. A covering of doped Madapolam (a fine plain woven cotton) fabric was stretched tightly over the shell and a coat of silver dope was applied, after which the exterior camouflage was applied. The fuselage had a large ventral section cut-out, which was braced during construction, to allow it to be lowered onto the wing centre-section. Once the wing was secured the lower panels were replaced, and the bomb bay or armament doors fitted.
The all-wood wing was built as a one-piece structure and was not divided into separate construction sections. It was made up of two main spars, spruce and plywood compression ribs, stringers, and a plywood covering. The outer plywood skin was covered and doped like the fuselage. The wing was installed into the roots by means of four large attachment points. The engine radiators were fitted in the inner wing, just outboard of the fuselage on either side. These gave less drag. The radiators themselves were split into three sections: an oil cooler section outboard, the middle section forming the coolant radiator and the inboard section serving the cabin heater. The wing contained metal framed and skinned ailerons, but the flaps were made of wood and were hydraulically controlled. The nacelles were mostly wood, although, for strength, the engine mounts were all metal as were the undercarriage parts. Engine mounts of welded steel tube were added, along with simple landing gear oleos filled with rubber blocks. Wood was used to carry only in-plane loads, with metal fittings used for all triaxially loaded components such as landing gear, engine mounts, control surface mounting brackets, and the wing-to-fuselage junction. The outer leading wing edge had to be brought further forward to accommodate this design. The main tail unit was all wood built. The control surfaces, the rudder and elevator, were aluminium framed and fabric covered. The total weight of metal castings and forgings used in the aircraft was only .
In November 1944, several crashes occurred in the Far East. At first, it was thought these were as a result of wing structure failures. The casein glue, it was said, cracked when exposed to extreme heat and/or monsoon conditions. This caused the upper surfaces to "lift" from the main spar. An investigating team led by Major Hereward de Havilland travelled to India and produced a report in early December 1944 stating that "the accidents were not caused by the deterioration of the glue but by shrinkage of the airframe during the wet monsoon season". However a later inquiry by Cabot & Myers definitely attributed the accidents to faulty manufacture and this was confirmed by a further investigation team by the Ministry of Aircraft Production at Defford which found faults in six different Marks of Mosquito (all built at de Havilland's Hatfield and Leavesden plants) which showed similar defects, and none of the aircraft had been exposed to monsoon conditions or termite attack; thus it was concluded that there were construction defects found at the two plants. It was found that the "Standard of glueing...left much to be desired”. Records at the time showed that accidents caused by "loss of control" were three times more frequent on Mosquitoes than on any other type of aircraft. The Air Ministry forestalled any loss of confidence in the Mosquito by holding to Major de Havilland's initial investigation in India that the accidents were caused "largely by climate" To solve the problem, a sheet of plywood was set along the span of the wing to seal the entire length of the skin joint along the main spar.
Systems.
The fuel systems allowed the Mosquito to have a good range and endurance, using up to nine fuel tanks. Two outer wing tanks each contained of fuel. These were complemented by two inner wing fuel tanks, each containing , located between the wing root and engine nacelle. In the central fuselage were twin fuel tanks mounted between bulkhead number two and three aft of the cockpit. In the FB.VI, these tanks contained each, while in the B.IV and other unarmed Mosquitos each of the two centre tanks contained . Both the inner wing, and fuselage tanks are listed as the "main tanks" and the total internal fuel load of was initially deemed appropriate for the type. In addition, the FB Mk VI could have larger fuselage tanks, increasing the capacity to .
Drop tanks of or could be mounted under each wing, increasing the total fuel load to .
In order to reduce fuel vaporisation at the high altitudes at which the photographic reconnaissance variants flew, the central and inner wing tanks were pressurised. The pressure venting cock located behind the pilot's seat controlled the pressure valve; as the altitude increased, the valve increased the volume applied by a pump. This system was extended to include field modifications of the fuel tank system.
The engine oil tanks were in the engine nacelles. Each nacelle contained a oil tank, including a air space. The oil tanks themselves had no separate coolant controlling systems. The coolant header tank was in the forward nacelle, behind the propeller. The remaining coolant systems were controlled by the coolant radiators shutters in the forward inner wing compartment, between the nacelle and the fuselage and behind the main engine cooling radiators which were fitted in the leading edge. Electric-pneumatic operated radiator shutters directed and controlled airflow through the ducts and into the coolant valves, to predetermined temperatures.
Electrical power came from a 24 volt DC generator on the starboard (No. 2) engine and an alternator on the port engine which supplied AC power for radios. The radiator shutters, supercharger gear change, gun camera, bomb bay, bomb/rocket release and all the other crew controlled instruments were powered by a 24 volt battery. The radio communication devices included VHF and HF communications, GEE navigation, and IFF and G.P. devices.
The electric generators also powered the fire extinguishers. Located on the starboard side of the cockpit, the switches would operate automatically in the event of a crash. In flight, a warning light would flash to indicate a fire, should the pilot not already be aware of it. In later models, to save liquids and engine clean up time in case of belly landing, the fire extinguisher was changed to semi-automatic triggers.
The design of the Mark VI allowed for a provisional long-range fuel tank to increase range for action over enemy territory, for the installation of bomb release equipment specific for depth charges for strikes against enemy shipping, or for the simultaneous use of rocket projectiles along with a drop tank under each wing supplementing the main fuel cells. The FB.VI had a wingspan of , a length (over guns) of . It had a maximum speed of at . Maximum take-off weight was and the range of the aircraft was with a service ceiling of .
The main landing gear were housed in the nacelles behind the engines. These were raised and lowered hydraulically. The main landing gear shock absorbers were de Havilland manufactured and used a system of rubber in compression, rather than hydraulic oleos, with twin pneumatic brakes for each wheel. The Dunlop-Marstrand anti-shimmy tailwheel was retractable.
Flight characteristics.
The design was noted for having light and effective control surfaces which allowed for good manoeuvrability. It was noted that the rudder should not be used aggressively at high speeds, and the poor aileron control at low speeds when landing and taking off was also a problem for inexperienced crews. For flying at low speeds, the flaps had to be set at 15°, speed reduced to and rpm set to 2,650. The speed could be reduced to an acceptable for low speed flying. For cruising the maximum speed for obtaining maximum range was at weight.
The Mosquito had a low stall speed of with undercarriage and flaps raised. When both were lowered, the stall speed decreased to . Stall speed at normal approach angle and conditions was . Warning of the stall was given by buffeting and would occur before stall was reached. The conditions and impact of the stall were not severe. The wing did not drop unless the control column was pulled back. The nose drooped gently and recovery was easy.
Variants.
Until the end of 1942 the RAF always used Roman numerals (I, II, ...) for mark numbers; 1943-1948 was a transition period during which new aircraft entering service were given Arabic numerals (1, 2, ...) for mark numbers, but older aircraft retained their Roman numerals. From 1948 onwards, Arabic numerals were used exclusively.
Prototypes.
Three prototypes were built, each with a different configuration. The first to fly was "W4050" on 25 November 1940, followed by the fighter "W4052" on 15 May 1941 and the photo-reconnaissance prototype "W4051" on 10 June 1941. "W4051" later flew operationally with 1 Photographic Reconnaissance Unit (1 PRU).
Photo-reconnaissance.
<br>
A total of 10 Mosquito PR Mk Is were built, four of them "long range" versions equipped with a overload fuel tank in the fuselage. The contract called for 10 of the PR Mk I airframes to be converted to B Mk IV Series 1s. All of the PR Mk Is, and the B Mk IV Series 1s, had the original short engine nacelles and short span (19 ft 5.5 in) tailplanes. Their engine cowlings incorporated the original pattern of integrated exhaust manifolds which, after a relatively brief flight time, had a troublesome habit of burning and blistering the cowling panels. The first operational sortie by a Mosquito was made by a PR Mk I, W4055, on 17 September 1941; during this sortie the unarmed Mosquito PR.I evaded three Messerschmitt Bf 109s at . Powered by two Merlin 21s, the PR Mk I had a maximum speed of , a cruise speed of , a ceiling of , a range of , and a climb rate of per minute.
Over 30 Mosquito B Mk IV bombers were converted into the PR Mk IV photo-reconnaissance aircraft. The first operational flight by a PR Mk IV was made by "DK284" in April 1942.
The Mosquito PR Mk VIII, built as a stopgap pending the introduction of the refined PR Mk IX, was the next photo-reconnaissance version. The five VIIIs were converted from B Mk IVs and became the first operational Mosquito version to be powered by two-stage, two-speed supercharged engines, using Rolls-Royce Merlin 61 engines in place of Merlin 21/22s. The first PR Mk VIII, "DK324" first flew on 20 October 1942. The PR Mk VIII had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, a ceiling of , a range of , and a climb rate of 2,500 ft per minute (760 m).
The Mosquito PR Mk IX, 90 of which were built, was the first Mosquito variant with two-stage, two-speed engines to be produced in quantity; the first of these, "LR405", first flew in April 1943. The PR Mk IX was based on the Mosquito B Mk IX bomber and was powered by two Merlin 72/73 or 76/77 engines. It could carry either two , two or two droppable fuel tanks.
The Mosquito PR Mk XVI had a pressurised cockpit and, like the Mk IX, was powered by two Rolls-Royce Merlin 72/73 or 76/77 piston engines. This version was equipped with three overload fuel tanks, totalling in the bomb bay, and could also carry two or drop tanks. A total of 435 of the PR Mk XVI were built. The PR Mk XVI had a maximum speed of , a cruise speed of , ceiling of , a range of , and a climb rate of 2,900 feet per minute (884 m).
The Mosquito PR Mk 32 was a long-range, high-altitude, pressurised photo-reconnaissance version. It was powered by a pair of two-stage supercharged Rolls-Royce Merlin 113 and Merlin 114 piston engines, the Merlin 113 on the starboard side and the Merlin 114 on the port. First flown in August 1944, only five were built and all were conversions from PR.XVIs.
The Mosquito PR Mk 34 and PR Mk 34A was a very long-range unarmed high altitude photo-reconnaissance version. The fuel tank and cockpit protection armour were removed. Additional fuel was carried in a bulged bomb bay: 1,192 gallons which was the equivalent of . A further two 200-gallon (910-litre) drop tanks under the outer wings gave a range of cruising at . Powered by two Merlin 114s first used in the PR.32. The port Merlin 114 drove a Marshal cabin supercharger. A total of 181 were built, including 50 built by Percival Aircraft Company at Luton. The PR.34's maximum speed (TAS) was at sea level, at and at .
All PR.34s were installed with four split F52 vertical cameras, two forward, two aft of the fuselage tank and one F24 oblique camera. Sometimes a K-17 camera was used for air surveys. In August 1945, the PR.34A was the final photo-reconnaissance variant with one Merlin 113A and 114A each delivering .
Colonel Roy M. Stanley II, USAF (RET) wrote: "I consider the Mosquito the best photo-reconnaissance aircraft of the war".
After the end of World War II Spartan Air Services used ten ex-RAF Mosquitoes, mostly B.35's plus one of only six PR.35's built, for high-altitude photographic survey work in Canada.
Bombers.
<br>
On 21 June 1941 the Air Ministry ordered that the last 10 Mosquitoes, ordered as photo-reconnaissance aircraft, should be converted to bombers. These 10 aircraft were part of the original 1 March 1940 production order and became the B Mk IV Series 1. "W4052" was to be the prototype and flew for the first time on 8 September 1941.
The bomber prototype led to the B Mk IV, of which 273 were built: apart from the 10 Series 1s, all of the rest were built as Series 2s with extended nacelles, revised exhaust manifolds, with integrated flame dampers, and larger tailplanes. Series 2 bombers also differed from the Series 1 in having a larger bomb bay to increase the payload to four bombs, instead of the four bombs of Series 1. This was made possible by shortening the tail of the bomb so that these four larger weapons could be carried (or a 2,000 lb (920;kg) total load). The B Mk IV entered service in May 1942 with 105 Squadron.
In April 1943 it was decided to convert a B Mk IV to carry a 4,000 lb (1,812 kg), thin-cased high explosive bomb (nicknamed "Cookie"). The conversion, including modified bomb bay suspension arrangements, bulged bomb bay doors and fairings, was relatively straightforward, and 54 B.IVs were subsequently modified and distributed to squadrons of RAF Bomber Command's Light Night Striking Force. 27 B Mk IVs were later converted for special operations with the Highball anti-shipping weapon, and were used by 618 Squadron, formed in April 1943 specifically to use this weapon. A B Mk IV, "DK290" was initially used as a trials aircraft for the bomb, followed by "DZ471,530 and 533". The B Mk IV had a maximum speed of , a cruising speed of , ceiling of , a range of , and a climb rate of 2,500 ft per minute (762 m).
Other bomber variants of the Mosquito included the Merlin 21 powered B Mk V high-altitude version. Trials with this configuration were made with "W4057" which had the wings strengthened and an addition of two fuel tanks, or alternatively two bombs. This design was not produced in Britain, but formed the basic design of the Canadian-built B.VII. Only "W4057" was built in prototype form. The Merlin 31 powered B Mk VII was built by de Havilland Canada. It was based on the B.V. and first flown on 24 September 1942. It only saw service in Canada and only 25 were built. Six were handed over to the United States Army Air Forces.
B Mk IX (54 built) was powered by the Merlin 72,73, 76 or 77. The two-stage Merlin variant was based on the PR.IX. The prototype "DK 324" was converted from a PR.VIII and first flew on 24 March 1943. In October 1943 it was decided that all B Mk IVs and all B Mk IXs then in service would be converted to carry the "Cookie", and all B Mk IXs built after that date were designed to allow them to be converted to carry the weapon. The B Mk IX had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, ceiling of , a range of , and a climb rate of 2,850 feet per minute (869 m). The IX could carry a maximum load of of bombs. A Mosquito B Mk IX holds the record for the most combat operations flown by an Allied bomber in the Second World War. "LR503", known as "F for Freddie" (from its squadron code letters, GB*F), first served with No. 109 and subsequently, No. 105 RAF squadrons. It flew 213 sorties during the war, only to crash at Calgary airport during the Eighth Victory Loan Bond Drive on 10 May 1945, two days after Victory in Europe Day, killing both the pilot, Flt. Lt. Maurice Briggs, DSO, DFC, DFM and navigator Fl. Off. John Baker, DFC and Bar.
The B Mk XVI was powered by the same variations as the B.IX. All B Mk XVIs were capable of being converted to carry the "Cookie". The two-stage powerplants were added along with a pressurised cabin. "DZ540" first flew on 1 January 1944. The prototype was converted from a IV (402 built). The next variant, the B Mk XX, was powered by Packard Merlins 31 and 33s. It was the Canadian version of the IV. Altogether, 245 were built. The B Mk XVI had a maximum speed of , an economical cruise speed of at 20,000 ft, and at 30,000 ft, ceiling of , a range of , and a climb rate of 2,800 ft per minute (853 m). The type could carry of bombs.
The B.35 was powered by Merlin 113 and 114As. Some were converted to TT.35s (Target Tugs) and others were used as PR.35s (photo-reconnaissance). The B.35 had a maximum speed of , a cruising speed of , ceiling of , a range of , and a climb rate of 2,700 ft per minute (823 m). A total of 174 B.35s were delivered up to the end of 1945. A further 100 were delivered from 1946 for a grand total of 274, 65 of which were built by Airspeed Ltd.
Fighters.
<br>
Developed during 1940, the first prototype of the Mosquito F Mk II was completed on 15 May 1941. These Mosquitos were fitted with four Hispano cannon in the fuselage belly and four .303 (7.7 mm) Browning machine guns mounted in the nose. On production Mk IIs the machine guns and ammunition tanks were accessed via two centrally hinged, sideways opening doors in the upper nose section. To arm and service the cannon the bomb bay doors were replaced by manually operated bay doors: the F and NF Mk IIs could not carry bombs. The type was also fitted with a gun camera in a compartment above the machine guns in the nose and was fitted with exhaust flame dampers to reduce the glare from the Merlin XXs.
In August 1942 Britain experienced several incursions of high-altitude Luftwaffe bombers, including the Junkers Ju-86P. As a result of these raids, starting in September 1942, five Mosquito B Mk IV bombers were quickly converted into F Mk XV high-altitude, pressurised fighters, powered by two-stage Merlin 73s and 77s fitted with four-bladed propellers.
The first of these conversions was "MP469", which was wheeled into the experimental shop on 7 September and first flew on 14 September. The bomber nose forward of the cockpit was cut off and a standard fighter nose, complete with four .303 Brownings, grafted on in its place. The pressurised crew cabin retained the bomber's canopy structure, with a vee-shaped, two-piece windscreen, while the bomber's control wheel was replaced by the fighter's control stick. Extended, pointed, wingtips increased the wingspan to . The airframe was lightened by removing all armour plating, bullet-proofing from the fuel and oil tanks, and the outer wing and fuselage tanks (leaving the inner wing tanks with a total capacity of ). Other ancillary equipment was also removed, and smaller-diameter main wheels were fitted after the first few flights. At a loaded weight of the Mk XV was lighter than a standard Mk II, and reached an altitude of . Four more B Mk IVs were converted into F Mk XVs, and, in late 1942, were further modified to become NF MK XVs.
All fighters and fighter bombers, apart from the F Mk XV, featured a modified canopy structure incorporating a flat, single piece armoured windscreen, and the crew entry/exit door was moved from the bottom of the forward fuselage to the right side of the nose, just forward of the wing leading edge. The pilot was provided with a fighter-style control stick rather than a wheel.
Night fighters.
<br>
At the end of 1940, the Air Staff's preferred night fighter design was the Gloster F.18/40 (derived from their F.9/37). However, although in agreement as to the quality of the Gloster company's design, the Ministry of Aircraft Production was concerned that Gloster would not be able to work on the F.18/40 and also the jet fighter design, considered the greater priority. Consequently in mid-1941 the Air Staff and MAP agreed that the Gloster aircraft would be dropped and the Mosquito considered for the night fighter requirement instead.
The first production night fighter Mosquitos were designated NF Mk II. A total of 466 were built with the first entering service with No. 157 Squadron in January 1942, replacing the Douglas Havoc. These aircraft were similar to the F Mk II, but were fitted with the AI Mk IV metric wavelength radar. The herring-bone transmitting antenna was mounted on the nose and the dipole receiving antennae were carried under the outer wings. A number of NF IIs had their radar equipment removed and additional fuel tanks installed in the bay behind the cannon for use as night intruders. These aircraft, designated NF II (Special) were first used by 23 Squadron in operations over Europe in 1942. 23 Squadron was then deployed to Malta on 20 December 1942, and operated against targets in Italy.
Ninety-seven NF Mk IIs were upgraded with centimetric AI Mk VIII radar and these were designated NF Mk XII. The NF Mk XIII, of which 270 were built, was the production equivalent of the Mk XII conversions. The centimetric radar sets were mounted in a solid "thimble" (Mk XII / XIII) or universal "bull nose" (Mk XVII / XIX) radome, which required the machine guns to be dispensed with.
Four F Mk XVs were converted to the NF Mk XV. These were fitted with AI Mk VIII in a "thimble" radome, and the .303 Brownings were moved into a gun pack fitted under the forward fuselage.
 NF Mk XVII was the designation for 99 NF Mk II conversions, with single-stage Merlin 21, 22, or 23 engines, but British AI.X (US SCR-720) radar.
The NF Mk XIX was an improved version of the NF XIII. It could be fitted with American or British AI radars; 220 were built.
The NF Mk 30 was the final wartime variant and was a high-altitude version, powered by two Rolls-Royce Merlin 76s. The NF Mk 30 had a maximum speed of at . It also carried early electronic countermeasures equipment. 526 were built.
Other Mosquito night fighter variants which were planned for, but never built, included the NF Mk X and NF Mk XIV (the latter based on the NF Mk XIII), both of which were to have two-stage Merlins. The NF Mk 31 was a variant of the NF Mk 30, but powered by Packard Merlins.
Mosquito night intruders of 100 Group, Bomber Command, were also fitted with a device called "Serrate" to allow them to track down German night fighters from their Lichtenstein B/C and SN-2 radar emissions, as well as a device named "Perfectos" that tracked German IFF signals.
After the war, two more night fighter versions were developed:
The NF Mk 36 was similar to the Mosquito NF Mk 30, but fitted with the American-built AI.Mk X radar. Powered by two Rolls-Royce Merlin 113/114 piston engines; 266 built. Max level speeds (TAS) with flame dampers fitted were at sea level, at , and at .
The NF Mk 38, 101 of which were built, was also similar to the Mosquito NF Mk 30, but fitted with the British-built AI Mk IX radar. This variant suffered from stability problems and did not enter RAF service: 60 were eventually sold to Yugoslavia. According to the Pilot's Notes and Air Ministry 'Special Flying Instruction TF/487', which posted limits on the Mosquito's maximum speeds, the NF Mk 38 had a VNE of 370 knots (425 mph), without under-wing stores, and within the altitude range of sea level to . However, from 10,000 to the maximum speed was 348 knots (400 mph). As the height increased other recorded speeds were; 15,000 to 320 knots (368 mph); 20,000 to , 295 knots (339 mph); 25,000 to , 260 knots (299 mph); 30,000 to , 235 knots (270 mph). With two added 100-gallon fuel tanks this performance fell; between sea level and 15,000 feet 330 knots (379 mph); between 15,000 to 320 knots (368 mph); 20,000 to , 295 knots (339 mph); 25,000 to , 260 knots (299 mph); 30,000 to , 235 knots (270 mph). Little difference was noted above .
Fighter-bombers.
The FB Mk VI, which first flew on 1 June 1942, was powered by two Merlin 21s or Merlin 25s, and introduced a re-stressed and reinforced "basic" wing structure capable of carrying single or bombs on racks housed in streamlined fairings under each wing, or up to eight RP-3 25lb or 60 lb rockets. In addition fuel lines were added to the wings to enable single or drop tanks to be carried under each wing. The usual fixed armament was four 20 mm Hispano Mk.II cannon and four .303 (7.7 mm) Browning machine guns, while two or bombs could be carried in the bomb bay. Unlike the F Mk II, the ventral bay doors were split into two pairs, with the forward pair being used to access the cannon, while the rear pair acted as bomb bay doors.
The maximum fuel load which could be carried was distributed between internal fuel tanks, plus two overload tanks, each of capacity, which could be fitted in the bomb bay, and two drop tanks. All-out level speed is often given as , although this speed applies to aircraft fitted with saxophone exhausts. The test aircraft ("HJ679") fitted with stub exhausts was found to be performing below expectations. It was returned to de Havilland at Hatfield where it was serviced. Its top speed was then tested and found to be , in line with expectations. 2,298 FB Mk VIs were built, nearly one-third of Mosquito production. 18 built by Airspeed Ltd were eventually modified to become FB.XVIIIs. Two were converted to TR.33 carrier-borne, maritime strike prototypes.
The FB Mk VI proved capable of holding its own with single-engine fighter aircraft in addition to bombing. For example, on 15 January 1945 Mosquito FB Mk VIs of 143 Squadron were engaged by 30 Focke-Wulf Fw 190s from "Jagdgeschwader 5": nonetheless, the Mosquitos sank an armed trawler and two merchant ships, losing five Mosquitos (two to flak) but shooting down five Fw 190s.
Another fighter-bomber variant was the Mosquito FB Mk XVIII (sometimes known as the "Tsetse") of which 18 were purpose-built and 27 converted from Mk VIs. The Mk XVIII was armed with a Molins "6-pounder Class M" cannon: this was a modified QF 6-pounder (57 mm) anti-tank gun fitted with an auto-loader to allow both semi- or fully automatic fire. 25 rounds were carried, with the entire installation weighing . In addition, of armour was added within the engine cowlings, around the nose and under the cockpit floor to protect the engines and crew from the heavily armed U-boats which were intended to be the primary target of the Mk XVIII. Two or four .303 (7.7 mm) Browning machine guns were retained in the nose and were used to "sight" the main weapon onto the target.
The Air Ministry initially suspected that this variant would not work, but tests proved otherwise. Although the gun provided the Mosquito with yet more anti-shipping firepower for use against U-boats, it required a steady approach run to aim and fire the gun, making it vulnerable to anti-aircraft fire. The gun had a muzzle velocity of and an excellent range of some 1,800 - 1,500 yards (1,650 - 1,370 m). It was sensitive to sidewards movement; an attack required a dive from at a 30° angle with the turn and bank indicator on centre. A move during the dive could jam the gun. The prototype "HJ732" was converted from a FB.VI and was first flown on 8 June 1943.
The effect of the new weapon was demonstrated on 10 March 1944 when Mk XVIIIs from 248 Squadron (escorted by four Mk VIs) engaged a German convoy of one U-boat and four destroyers, protected by 10 Ju 88s. Three of the Ju 88s were shot down. Pilot Tony Phillips destroyed one Ju 88 with four shells, one of which tore an engine off the Ju 88. The U-boat was damaged. On 25 March, "U-976" was sunk by Molins-equipped Mosquitos. On 10 June, "U-821" was abandoned in the face of intense air attack from No. 248 Squadron, and was later sunk by a Liberator of No. 206 Squadron. On 5 April 1945 Mosquitos with Molins attacked five German surface ships in the Kattegat and again demonstrated their value by setting them all on fire and sinking them. A German "Sperrbrecher" ("minefield breaker") was lost with all hands, with some 200 bodies being recovered by Swedish vessels. Some 900 German soldiers died in total. On 9 April, German U-boats "U-804", "U-843" and "U-1065" were spotted in formation heading for Norway. All were sunk. U-251 and U-2359 followed on 19 April and 2 May 1945.
Despite the preference for rockets, a further development of the large gun idea was carried out using the even larger QF 32-pounder, a gun based on the QF 3.7 inch AA gun designed for tank use, the airborne version using a novel form of muzzle brake. Developed to prove the feasibility of using such a large weapon in the Mosquito, this installation was not completed until after the war, when it was flown and fired in a single aircraft without problems, then scrapped.
Designs based on the Mk VI were the FB Mk 26, built in Canada, and the FB Mk 40, built in Australia, powered by Packard Merlins. The FB.26 improved from the FB.21 using single stage Packard Merlin 225s. Some 300 were built and another 37 converted to T.29 standard. 212 FB.40s were built by de Havilland Australia. Six were converted to PR.40; 28 to PR.41s, one to FB.42 and 22 to T.43s tactical assault aircraft. Most were powered by Packard-built Merlin 31 or 33s.
Trainers.
The Mosquito was also built as the Mosquito T Mk III two-seat trainer. This version, powered by two Rolls-Royce Merlin 21s, was unarmed and had a modified cockpit fitted with dual control arrangements. A total of 348 of the T Mk III were built for the RAF and Fleet Air Arm. de Havilland Australia built 11 T Mk 43 trainers, similar to the Mk III.
Torpedo-bombers.
To meet specification N.15/44 for a navalised Mosquito for Royal Navy use as a torpedo bomber, de Havilland produced a carrier-borne variant. A Mosquito FB.VI was modified as a prototype designated Sea Mosquito TR Mk 33 with folding wings, arrester hook, thimble nose radome, Merlin 25 engines with four-bladed propellers and a new oleo-pneumatic landing gear rather than the standard rubber-in-compression gear. Initial carrier tests of the Sea Mosquito were carried out by Eric "Winkle" Brown aboard HMS "Indefatigable", the first landing-on taking place on 25 March 1944. An order for 100 TR.33s was placed although only 50 were built at Leavesden. Armament was four 20 mm cannon, two 500 lb bombs in the bomb bay (another two could be fitted under the wings), eight 60 lb rockets (four under each wing) and a standard torpedo under the fuselage. The first production TR.33 flew on 10 November 1945. This series was followed by six Sea Mosquito TR Mk 37s, which differed in having ASV Mk XIII radar instead of the TR.33's AN/APS-6.
Target tugs.
The Royal Navy also operated the Mosquito TT Mk 39 for target towing. A number of B Mk XVIs bombers were converted into TT Mk 39 target tug aircraft. The RAF's target tug version was the Mosquito TT Mk 35 which were the last aircraft to remain in operational service, with No 3 CAACU at Exeter, finally being retired in 1963 (these aircraft were then featured in the film 633 Squadron). Two ex-RAF FB.6s were converted to TT.6 standard at Manchester (Ringway) Airport by Fairey Aviation in 1953–1954, being delivered to the Belgian Air Force for use as towing aircraft from the Sylt firing ranges.
Canadian-built.
A total of 1,133 (to 1945) Mosquitos were built by De Havilland Canada at Downsview Airfield in Downsview Ontario (now Downsview Park in Toronto Ontario).
Highball.
A number of Mosquito IVs were modified by Vickers-Armstrongs to carry Highball "bouncing bombs" and were allocated Vickers Type numbers:
Production.
Details.
In England, fuselage shells were mainly made by furniture companies, including Ronson, E. Gomme, Parker Knoll and Styles & Mealing. Some of the specialized wood veneer used in the construction of the Mosquito was made by Roddis Manufacturing in Marshfield, Wisconsin, United States. Hamilton Roddis had teams of young women ironing the (unusually thin) strong wood veneer before shipping to the UK. Wing spars were made by J.B. Heath and Dancer & Hearne. Many of the other parts, including flaps, flap shrouds, fins, leading edge assemblies and bomb doors were also produced in High Wycombe, Buckinghamshire, which was well suited to these tasks because of a well-established furniture manufacturing industry. Dancer & Hearne processed much of the wood from start to finish, receiving timber and transforming it into finished wing spars at their High Wycombe factory.
About 5,000 of the 7,781 Mosquitos made contained parts made in High Wycombe. In Canada, fuselages were built in the Oshawa, Ontario plant of General Motors of Canada Limited. These were shipped to De Havilland Canada in Toronto for mating to fuselages and completion. As a secondary manufacturer, de Havilland Australia started construction in Sydney. These production lines added 1,133 from Canada and 212 from Australia.
Total Mosquito production was 7,781 of which 6,710 were built during the war. The ferry operation of the Mosquito from Canada to the war front was problematic, as a small fraction of the aircraft mysteriously disappeared over mid-Atlantic. The theory of "auto-explosion" was offered, and, although a considerable effort by de Havilland Canada to resolve production problems with engine and oil systems reduced the number of aircraft lost, the actual cause of the losses was never established. The company introduced an additional five hours flight testing to clear production aircraft before the ferry flight. By the end of the war, nearly 500 Mosquito bombers and fighter-bombers had been delivered successfully by the Canadian operation.
In total, both during the war and after, de Havilland exported 46 FB.VIs and 29 PR. XVIs to Australia, two FB.VI and 18 NF.30s to Belgium, approximately 205 FB.26, T.29 and T.27s to Nationalist China (5 of these were captured by the People's Liberation Army during the Chinese Civil War), 19 FB.VIs to Czechoslovakia in 1948, 6 FB.VIs to Dominica, a few B.IVs, 57 FB.VIs, 29 PR.XVIs and 23 NF.XXXs to France. Some T.IIIs were exported to Israel along with 60 FB.VIs, and at least five PR.XVIs and 14 naval versions. Four T.IIIs, 76 FB.VIs, one FB.40 and four T.43s were exported to New Zealand. Three T.IIIs were exported to Norway, and 18 FB.VIs which were later converted to night fighter standard. South Africa received two F.II and 14 PR.XVI/XIs and Sweden received 46 NF.XIXs. Turkey received 96 FB.VIs and several T.IIIs, and Yugoslavia had 60 NF.38s, 80 FB.VIs and three T.IIIs delivered.
Operational history.
The de Havilland Mosquito operated in many roles during the Second World War, being tasked to perform medium bomber, reconnaissance, tactical strike, anti-submarine warfare and shipping attack and night fighter duties, both defensive and offensive, until the end of the war.
In July 1941, the first production Mosquito "W 4051" (a production fuselage combined with some prototype flying surfaces – see section of Article "Prototypes and test flights") was sent to No. 1 Photographic Reconnaissance Unit (PRU), operating at the time at RAF Benson. Consequently, the secret reconnaissance flights of this aircraft were the first active service missions of the Mosquito. In 1944, the journal "Flight" gave 19 September 1941 as date of the first PR mission, at an altitude "of some 20 000 ft."
On 15 November 1941, 105 Squadron, RAF, took delivery of the first operational Mosquito Mk. B.IV bomber, serial no. "W4064". Throughout 1942, 105 Sdn., based at RAF Horsham St. Faith, then from 29 September, RAF Marham, undertook daylight low-level and shallow dive attacks. Apart from the famous Oslo raid, these were mainly on industrial targets in occupied Netherlands, plus northern and western Germany. The crews faced deadly flak and fighters, particularly FW 190’s, which they called “snappers.” Germany still controlled Continental airspace, and the FW 190’s were often already airborne and at an advantageous altitude. It was the Mosquito’s excellent handling capabilities, rather than pure speed, that facilitated those evasions that were successful. During this daylight-raiding phase, aircrew losses were high – even the losses incurred in the squadron’s dangerous Blenheim era were exceeded in percentage terms. The Roll of Honour shows 51 aircrew deaths from the end of May 1942 to April 1943. In the corresponding period, crews gained three Mentions in Despatches, two DFM’s and three DFC’s.
The Mosquito was first announced publicly on 26 September 1942 after the Oslo Mosquito raid of 25 September. It was featured in "The Times" on the 28 September, and the next day the newspaper published two captioned photographs illustrating the bomb strikes and damage.
Mosquitos were widely used by the RAF Pathfinder Force, marking targets for the main night-time strategic bombing force, as well as flying "nuisance raids" in which Mosquitos often dropped 4,000 lb "Cookies". Despite an initially high loss rate, the Mosquito ended the war with the lowest losses of any aircraft in RAF Bomber Command service. Post war, the RAF found that when finally applied to bombing, in terms of useful damage done, the Mosquito had proved 4.95 times cheaper than the Lancaster. In April 1943, in response to "political humiliation" caused by the Mosquito, Hermann Göring ordered the formation of special "Luftwaffe" units ("Jagdgeschwader 25", commanded by "Oberstleutnant" Herbert Ihlefeld and "Jagdgeschwader 50", under "Major" Hermann Graf) to combat the Mosquito attacks, though these units, which were "little more than glorified squadrons", were not very successful against the elusive RAF aircraft.
In one example of the daylight precision raids carried out by the Mosquito, on 20 January 1943, the 10th anniversary of the Nazis' seizure of power, a Mosquito attack knocked out the main Berlin broadcasting station while Commander in Chief Reichsmarschall Hermann Göring was speaking, putting his speech off air. Göring himself had strong views about the Mosquito, lecturing a group of German aircraft manufacturers in 1943 that:
The Mosquito also proved a very capable night fighter. Some of the most successful RAF pilots flew the Mosquito. Bob Braham claimed around a third of his 29 kills in a Mosquito, flying mostly daytime operations, while on night fighters Wing Commander Branse Burbridge claimed 21 kills, and Wing Commander John Cunningham claimed 19 of his 20 victories at night on Mosquitos. Mosquitos of No. 100 Group RAF were responsible for the destruction of 257 German aircraft from December 1943 to April 1945. Mosquito fighters from all units accounted for 487 German aircraft during the war, the vast majority of which were night fighters.
One Mosquito is listed as belonging to German secret operations unit "Kampfgeschwader 200", which tested, evaluated and sometimes clandestinely operated captured enemy aircraft during the war. The aircraft was listed on the order of battle of "Versuchsverband OKL"s, "2 Staffel", "Stab Gruppe" on 10 November and 31 December 1944. However, on both lists, the Mosquito is listed as unserviceable.
The Mosquito flew its last official European war mission on 21 May 1945, when Mosquitos of 143 Squadron and 248 Squadron RAF were ordered to continue to hunt German submarines that might be tempted to continue the fight; instead of submarines all the Mosquitos encountered were passive E-boats.
Survivors.
There are approximately 30 non-flying Mosquitos around the world with two airworthy examples. The largest collection of Mosquitos is at the de Havilland Aircraft Heritage Centre in the United Kingdom, which owns three aircraft, including the first prototype, "W4050", possibly the only initial prototype of a Second World War aircraft design still in existence in the 21st century.
Specifications.
DH.98 Mosquito F Mk II.
Fighter version.
DH.98 Mosquito B Mk XVI.
The definitive bomber version.

</doc>
<doc id="9099" url="http://en.wikipedia.org/wiki?curid=9099" title="Dave Thomas (businessman)">
Dave Thomas (businessman)

Rex David "Dave" Thomas (July 2, 1932January 8, 2002) was an American businessman and philanthropist. Thomas was the founder and chief executive officer of Wendy's, a fast-food restaurant chain specializing in hamburgers. He is also known for appearing in more than 800 commercial advertisements for the chain from 1989 to 2002, more than any other company founder in television history.
Early life.
Dave Thomas was born on July 2, 1932 in Atlantic City, New Jersey to a young unmarried woman he never knew. He was adopted at 6 weeks by Rex and Auleva Thomas, and as an adult became a well-known advocate for adoption, founding the Dave Thomas Foundation for Adoption. After his adoptive mother's death when he was 5, his father moved around the country seeking work. Thomas spent time in Michigan with his grandmother, Minnie Sinclair, who he credited with teaching him the importance of service and treating others well and with respect, lessons that helped him in his future business life.
At 12, Thomas got his first job at Regas Restaurant, a fine dining restaurant in downtown Knoxville, Tennessee, then lost it in a dispute with his boss; decades later, Regas Restaurant installed a large autographed poster-photo of Thomas just inside their entrance until the business closed down December 31, 2010. He vowed never to lose another job. Moving with his father, by 15 he was working in Fort Wayne, Indiana at the Hobby House Restaurant owned by the Clauss family. When his father prepared to move again, Dave decided to stay in Fort Wayne, dropping out of high school to work full-time at the restaurant. Thomas, who considered ending his schooling the greatest mistake of his life, did not graduate from high school until 1993, when he obtained a GED.
He subsequently became an education advocate and founded the Dave Thomas Education Center in Coconut Creek, Florida, 
U.S. Army.
At the outbreak of the Korean War in 1950, rather than waiting for the draft, he volunteered for the U.S. Army to have some choice in assignments. Having food production and service experience, Thomas requested the Cook's and Baker's School at Fort Benning, Georgia. He was sent overseas to Germany as a mess sergeant and was responsible for the daily meals of 2000 soldiers, rising to the rank of staff sergeant. After his discharge in 1953, Thomas returned to Fort Wayne and the Hobby House.
Fast food career.
Kentucky Fried Chicken.
In the mid-1950s, Kentucky Fried Chicken founder Col. Harland Sanders came to Fort Wayne to find restaurateurs with established businesses in order to try to sell KFC franchises to them.
At first, Thomas, who was the head cook at a restaurant, and the Clausses declined Sanders' offer, but the Colonel persisted and the Clauss family franchised their restaurant with KFC and later also owned many other KFC franchises in the Midwest. During this time, Thomas worked with Sanders on many projects to make KFC more profitable and to give it brand recognition. Among other things Thomas suggested to Sanders that were implemented: KFC's signature chicken bucket (to keep the chicken crisp), reduce the number of items on the menu, focus on a signature dish. Thomas also suggested Sanders make commercials that he appear in himself. Thomas was sent by the Clauss family in the mid-1960s to help turn around four failing KFC stores they owned in Columbus, Ohio.
By 1968 Thomas had increased sales in the four fried chicken restaurants so much that he sold his share in them back to Sanders for more than $1.5 million. This experience would prove invaluable to Thomas when he began Wendy's about a year later.
Wendy's.
Thomas opened his first Wendy's in Columbus, Ohio, November 15, 1969. (This original restaurant remained operational until March 2, 2007, when it was closed due to lagging sales.) Thomas named the restaurant after his eight-year-old daughter Melinda Lou, whose nickname was "Wendy", stemming from the child's inability to say her own name at a young age. According to "Bio TV", Dave claims himself that people nicknamed his daughter "Wenda. Not Wendy, but Wenda. 'I'm going to call it Wendy's Old Fashioned Hamburgers'."
In 1982, Thomas resigned from his day-to-day operations at Wendy’s. However, by 1985, several company business decisions, including an awkward new breakfast menu and loss in brand awareness due to fizzled marketing efforts, caused the company’s new president to urge Thomas back into a more active role with Wendy's. Thomas began to visit franchises and espouse his hardworking, so-called “mop-bucket attitude.” In 1989, he took on a significant role as the TV spokesman in a series of commercials for the brand. Thomas was not a natural actor, and initially, his performances were criticized as stiff and ineffective by advertising critics.
By 1990, after efforts by Wendy's agency, Backer Spielvolgel Bates, to get humor into the campaign, a decision was made to portray Thomas in a more self-deprecating and folksy manner, which proved much more popular with test audiences. Consumer brand awareness of Wendy's eventually regained levels it had not achieved since octogenarian Clara Peller's wildly popular "Where's the beef?" campaign of 1984.
With his natural self-effacing style and his relaxed manner, Thomas quickly became a household name. A company survey during the 1990s, a decade during which Thomas starred in every Wendy’s commercial that aired, found that 90% of Americans knew who Thomas was. After more than 800 commercials, it was clear that Thomas played a major role in Wendy's status as the country's third most popular burger restaurant. 
In 1994, Thomas made a cameo appearance as himself in "Bionic Ever After?", a reunion TV movie based upon "The Six Million Dollar Man" and "The Bionic Woman".
Personal Life.
Dave Thomas had three children besides Melinda: two daughters, Pam and Molly, and a son, Kenny. They still continue to be the image of their dad's restaurant.
Honors and memberships.
Thomas, realizing that his success as a high school dropout might convince other teenagers to quit school (something he later claimed was a mistake), became a student at Coconut Creek High School. He earned a GED in 1993. Thomas was inducted into the Junior Achievement U.S. Business Hall of Fame in 1999.
Thomas was a Freemason at Sol. D. Bayless Lodge No. 359 Fort Wayne, Indiana, and a member of the Shriners; he received the honorary 33rd degree in 1995. He was also an honorary Kentucky colonel, as was former boss Colonel Sanders.
Thomas was posthumously awarded the Presidential Medal of Freedom in 2003.
Death.
On January 8, 2002, Thomas died at his home in Fort Lauderdale, Florida, after a decade-long battle with neuroendocrine and carcinoid cancer that had spread to his liver. He was 69 years old. Thomas was buried in Union Cemetery in Columbus, Ohio. At the time of his death, there were more than 6,000 Wendy's restaurants operating in North America.

</doc>
<doc id="9101" url="http://en.wikipedia.org/wiki?curid=9101" title="Device driver">
Device driver

In computing, a device driver (commonly referred to as a "driver") is a computer program that operates or controls a particular type of device that is attached to a computer. A driver provides a software interface to hardware devices, enabling operating systems and other computer programs to access hardware functions without needing to know precise details of the hardware being used.
A driver typically communicates with the device through the computer bus or communications subsystem to which the hardware connects. When a calling program invokes a routine in the driver, the driver issues commands to the device. Once the device sends data back to the driver, the driver may invoke routines in the original calling program. Drivers are hardware-dependent and operating-system-specific. They usually provide the interrupt handling required for any necessary asynchronous time-dependent hardware interface.
Purpose.
Device drivers simplify programming by acting as translator between a hardware device and the applications or operating systems that use it. Programmers can write the higher-level application code independently of whatever specific hardware the end-user is using.
For example, a high-level application for interacting with a serial port may simply have two functions for "send data" and "receive data". At a lower level, a device driver implementing these functions would communicate to the particular serial port controller installed on a user's computer. The commands needed to control a 16550 UART are much different from the commands needed to control an FTDI serial port converter, but each hardware-specific device driver abstracts these details into the same (or similar) software interface.
Development.
Writing a device driver requires an in-depth understanding of how the hardware and the software works for a given platform function. Because drivers require low-level access to hardware functions in order to operate, drivers typically operate in a highly privileged environment and can cause disaster if they get things wrong. In contrast, most user-level software on modern operating systems can be stopped without greatly affecting the rest of the system. Even drivers executing in user mode can crash a system if the device is erroneously programmed. These factors make it more difficult and dangerous to diagnose problems.
The task of writing drivers thus usually falls to software engineers or computer engineers who work for hardware-development companies. This is because they have better information than most outsiders about the design of their hardware. Moreover, it was traditionally considered in the hardware manufacturer's interest to guarantee that their clients can use their hardware in an optimum way. Typically, the "logical device driver" (LDD) is written by the operating system vendor, while the "physical device driver" (PDD) is implemented by the device vendor. But in recent years non-vendors have written numerous device drivers, mainly for use with free and open source operating systems. In such cases, it is important that the hardware manufacturer provides information on how the device communicates. Although this information can instead be learned by reverse engineering, this is much more difficult with hardware than it is with software.
Microsoft has attempted to reduce system instability due to poorly written device drivers by creating a new framework for driver development, called Windows Driver Foundation (WDF). This includes User-Mode Driver Framework (UMDF) that encourages development of certain types of drivers—primarily those that implement a message-based protocol for communicating with their devices—as user-mode drivers. If such drivers malfunction, they do not cause system instability. The Kernel-Mode Driver Framework (KMDF) model continues to allow development of kernel-mode device drivers, but attempts to provide standard implementations of functions that are known to cause problems, including cancellation of I/O operations, power management, and plug and play device support.
Apple has an open-source framework for developing drivers on Mac OS X called the I/O Kit.
In Linux environments, programmers can build device drivers as parts of the kernel, separately as loadable modules, or as user-mode drivers (for certain types of devices where kernel interfaces exist, such as for USB devices). Makedev includes a list of the devices in Linux: ttyS (terminal), lp (parallel port), hd (disk), loop, sound (these include mixer, sequencer, dsp, and audio)...
The Microsoft Windows .sys files and Linux .ko modules contain loadable device drivers. The advantage of loadable device drivers is that they can be loaded only when necessary and then unloaded, thus saving kernel memory.
Kernel mode vs. user mode.
Device drivers, particularly on Microsoft Windows platforms, can run in kernel-mode (Ring 0 on x86 CPUs) or in user-mode (Ring 3 on x86 CPUs). The primary benefit of running a driver in user mode is improved stability, since a poorly written user mode device driver cannot crash the system by overwriting kernel memory. On the other hand, user/kernel-mode transitions usually impose a considerable performance overhead, thereby prohibiting user-mode drivers for low latency and high throughput requirements.
Kernel space can be accessed by user module only through the use of system calls. End user programs like the UNIX shell or other GUI-based applications are part of the user space. These applications interact with hardware through kernel supported functions.
Applications.
Because of the diversity of hardware and operating systems, drivers operate in many different environments. Drivers may interface with:
Common levels of abstraction for device drivers include:
So choosing and installing the correct device drivers for given hardware is often a key component of computer system configuration.
Virtual device drivers.
Virtual device drivers represent a particular variant of device drivers. They are used to emulate a hardware device, particularly in virtualization environments, for example when a DOS program is run on a Microsoft Windows computer or when a guest operating system is run on, for example, a Xen host. Instead of enabling the guest operating system to dialog with hardware, virtual device drivers take the opposite role and emulate a piece of hardware, so that the guest operating system and its drivers running inside a virtual machine can have the illusion of accessing real hardware. Attempts by the guest operating system to access the hardware are routed to the virtual device driver in the host operating system as e.g., function calls. The virtual device driver can also send simulated processor-level events like interrupts into the virtual machine.
Virtual devices may also operate in a non-virtualized environment. For example a virtual network adapter is used with a virtual private network, while a virtual disk device is used with iSCSI. A good example for virtual device drivers can be Daemon Tools.
There are several variants of virtual device drivers, such as VxDs, VLMs, VDDs.
Open drivers.
Solaris descriptions of commonly used device drivers
Identifiers.
A device on the PCI bus or USB is identified by two IDs which consist of 4 hexadecimal numbers each. The vendor ID identifies the vendor of the device. The device ID identifies a specific device from that manufacturer/vendor.
A PCI device has often an ID pair for the main chip of the device, and also a subsystem ID pair which identifies the vendor, which may be different from the chip manufacturer.

</doc>
<doc id="9103" url="http://en.wikipedia.org/wiki?curid=9103" title="Dimona">
Dimona

Dimona () is an Israeli city in the Negev desert, to the south of Beersheba and west of the Dead Sea above the Arava valley in the Southern District of Israel. Its population at the end of 2007 was 33,600.
Etymology.
The city's name is derived from a biblical town, mentioned in Joshua 15:21-22.
History.
Dimona was one of the development towns created in the 1950s under the leadership of Israel's first Prime Minister, David Ben-Gurion. Dimona itself was conceived in 1953, and settled in 1955, mostly by new immigrants from Northern Africa, who also constructed the city's houses. The emblem of Dimona (as a local council), adopted 2 March 1961, appeared on a stamp issued on 24 March 1965.
When the Israeli nuclear program started later that decade, a location not far from the city was chosen for the Negev Nuclear Research Center due to its relative isolation in the desert and availability of housing.
In spite of a gradual decrease during the 1980s, the city's population began to grow once again with the beginning of the Russian immigration in the 1990s. Currently, Dimona is the third largest city in the Negev, with the population of 33,900. Due to projected rapid population growth in the Negev, the city is expected to triple in size by 2025.
Population.
Dimona is described as "mini-India" by many for its 7,500-strong Indian Jewish community. It is also home to Israel's Black Hebrew community, governed by its founder and spiritual leader, Ben Ammi Ben-Israel. The Black Hebrews number about 3000 in Dimona, with additional families in Arad, Mitzpe Ramon and the Tiberias area. Their official status in Israel was an ongoing issue for many years, but in May 1990, the issue was resolved with the issuing of first B/1 visas, and a year later, issuing of temporary residency. Status was extended to August 2003, when the Israeli Ministry of Interior granted permanent residency.
Economy.
In the early 1980s, textile plants, such as Dimona Textiles Ltd., dominated the industrial landscape. Many plants have since closed. Dimona Silica Industries Ltd. manufactures precipitated silica and calcium carbonate fillers. About a third of the city's population works in industrial workplaces (chemical plants near the Dead Sea like the Dead Sea Works, high-tech companies and textile shops), and another third in the area of services. Due to the introduction of new technologies, many workers have been made redundant in the recent years, creating a total unemployment rate of about 10%. Dimona has taken part of Israel's solar transformation. The Rotem Industrial Complex outside of the city has dozens of solar mirrors that focus the sun's rays on a tower that in turn heats a water boiler to create steam, turning a turbine to create electricity. Luz II, Ltd. plans to use the solar array to test new technology for the three new solar plants to be built in California for Pacific Gas and Electric Company.
Geography and climate.
Dimona is at an average height of about 550–600 meters above sea level. It is in the Negev Desert, therefore it has a desert climate with low humidity for most of the year and little precipitation. Summers are hot with an average max temperature of about 33C in August, the hottest month of the year. Average annual precipitation is about , mostly during the winter.
Transportation.
In the early 1950s, an extension to Dimona and south was constructed from the Railway to Beersheba, designed for freight traffic. A passenger service began in 2005, after pressure from Dimona's municipality. Dimona Railway Station is located in the southwestern part of the city. The main bus terminal is the Dimona Central Bus Station, with lines to Beersheba, Tel Aviv, Eilat, and nearby towns.
Twin towns.
Dimona is twinned with:

</doc>
<doc id="9105" url="http://en.wikipedia.org/wiki?curid=9105" title="DC Comics">
DC Comics

DC Comics, Inc. is an American comic book publisher. It is the publishing unit of DC Entertainment, a company of Warner Bros. Entertainment, which itself is owned by Time Warner. DC Comics is one of the largest and most successful companies operating in American comic books, and produces material featuring many well-known characters, including Superman, Batman, Wonder Woman, Green Lantern, the Flash, Aquaman, Cyborg, Hawkgirl, and Green Arrow. The fictional DC universe also features superhero teams such as the Justice League, Justice Society, and the Teen Titans, as well as antagonists such as the Joker, Lex Luthor, Darkseid and Professor Zoom.
The initials "DC" came from the company's popular series "Detective Comics", which featured Batman's debut and subsequently became part of the company's name. Originally in Manhattan at 432 Fourth Avenue, the DC Comics offices have been located at 480 and later 575 Lexington Avenue; 909 Third Avenue; 75 Rockefeller Plaza; 666 Fifth Avenue; and 1325 Avenue of the Americas. DC has its headquarters at 1700 Broadway, Midtown Manhattan, New York City, but it was announced in October 2013 that DC Entertainment would relocate its headquarters from New York to Burbank, California in 2015.
Random House distributes DC Comics' books to the bookstore market, while Diamond Comic Distributors supplies the comics shop specialty market. DC Comics and its major, longtime competitor Marvel Comics (owned by The Walt Disney Company, Warner Bros.'s main rival, since 2009) together shared over 80% of the American comic book market in 2008.
History.
Origins.
Entrepreneur Major Malcolm Wheeler-Nicholson's National Allied Publications debuted with the tabloid-sized "New Fun: The Big Comic Magazine" #1 with a cover date of February 1935. The company's second title, "New Comics" #1 (Dec. 1935), appeared in a size close to what would become comic books' standard during the period fans and historians call the Golden Age of Comic Books, with slightly larger dimensions than today's. That title evolved into "Adventure Comics", which continued through issue #503 in 1983, becoming one of the longest-running comic-book series. In 2009 DC revived it with its original numbering.
Wheeler-Nicholson's third and final title, "Detective Comics", advertised with a cover illustration dated December 1936, eventually premiered three months late with a March 1937 cover date. The themed anthology series would become a sensation with the introduction of Batman in issue #27 (May 1939). By then, however, Wheeler-Nicholson had gone. In 1937, in debt to printing-plant owner and magazine distributor Harry Donenfeld — who also published pulp magazines and operated as a principal in the magazine distributorship Independent News — Wheeler-Nicholson had to take Donenfeld on as a partner in order to publish "Detective Comics" #1. Detective Comics, Inc. was formed, with Wheeler-Nicholson and Jack S. Liebowitz, Donenfeld's accountant, listed as owners. Major Wheeler-Nicholson remained for a year, but cash-flow problems continued, and he was forced out. Shortly afterward, Detective Comics Inc. purchased the remains of National Allied, also known as Nicholson Publishing, at a bankruptcy auction.
Detective Comics Inc. soon launched a fourth title, "Action Comics", the premiere of which introduced Superman. "Action Comics" #1 (June 1938), the first comic book to feature the new character archetype — soon known as "superheroes" — proved a sales hit. The company quickly introduced such other popular characters as the Sandman and Batman.
On February 22, 2010, a copy of "Action Comics" #1 (June 1938) sold at an auction from an anonymous seller to an anonymous buyer for $1 million dollars, besting the $317,000 record for a comic book set by a different copy, in lesser condition, the previous year.
The Golden Age.
National Allied Publications soon merged with Detective Comics Inc. to form National Comics, which in 1944 absorbed an affiliated concern, Max Gaines' and Liebowitz' All-American Publications. That year, Gaines let Liebowitz buy him out, and kept only "Picture Stories from the Bible" as the foundation of his own new company, EC Comics. At that point, "Liebowitz promptly orchestrated the merger of All-American and Detective Comics into National Comics... Next he took charge of organizing National Comics, [the self-distributorship] Independent News, and their affiliated firms into a single corporate entity, National Periodical Publications". National Periodical Publications became publicly traded on the stock market in 1961.
Despite the official names "National Comics" and "National Periodical Publications", the company began branding itself as "Superman-DC" as early as 1940, and the company became known colloquially as DC Comics for years before the official adoption of that name in 1977.
The company began to move aggressively against what it saw as copyright-violating imitations from other companies, such as Fox Comics' Wonder Man, which (according to court testimony) Fox started as a copy of Superman. This extended to DC suing Fawcett Comics over Captain Marvel, at the time comics' top-selling character. Despite the fact that parallels between Captain Marvel and Superman seemed more tenuous (Captain Marvel's powers came from magic, unlike Superman's), the courts ruled that substantial and deliberate copying of copyrighted material had occurred. Faced with declining sales and the prospect of bankruptcy if it lost, Fawcett capitulated in 1955 and ceased comics publication. Years later, Fawcett ironically sold the rights for Captain Marvel to DC — which in 1974 revived Captain Marvel in the new title "Shazam!" featuring artwork by his creator, C. C. Beck. In the meantime, the abandoned trademark had been seized by Marvel Comics in 1967, with the creation of their Captain Marvel, disallowing the DC comic itself to be called that. While Captain Marvel did not recapture his old popularity, he later appeared in a Saturday morning live action TV adaptation and gained a prominent place in the mainstream continuity DC calls the DC Universe.
When the popularity of superheroes faded in the late 1940s, the company focused on such genres as science fiction, Westerns, humor, and romance. DC also published crime and horror titles, but relatively tame ones, and thus avoided the mid-1950s backlash against such comics. A handful of the most popular superhero-titles, including "Action Comics" and "Detective Comics", the medium's two longest-running titles as of 2013, continued publication.
The Silver Age.
In the mid-1950s, editorial director Irwin Donenfeld and publisher Liebowitz directed editor Julius Schwartz (whose roots lay in the science-fiction book market) to produce a one-shot Flash story in the try-out title "Showcase". Instead of reviving the old character, Schwartz had writers Robert Kanigher and John Broome, penciler Carmine Infantino, and inker Joe Kubert create an entirely new super-speedster, updating and modernizing the Flash's civilian identity, costume, and origin with a science-fiction bent. The Flash's reimagining in "Showcase" #4 (October 1956) proved sufficiently popular that it soon led to a similar revamping of the Green Lantern character, the introduction of the modern all-star team Justice League of America (JLA), and many more superheroes, heralding what historians and fans call the Silver Age of comic books.
National did not reimagine its continuing characters (primarily Superman, Batman, and Wonder Woman), but radically overhauled them. The Superman family of titles, under editor Mort Weisinger, introduced such enduring characters as Supergirl, Bizarro, and Brainiac. The Batman titles, under editor Jack Schiff, introduced the successful Batwoman, Bat-Girl, Ace the Bat-Hound, and Bat-Mite in an attempt to modernize the strip with non-science-fiction elements. Schwartz, together with artist Infantino, then revitalized Batman in what the company promoted as the "New Look", re-emphasizing Batman as a detective. Meanwhile, editor Kanigher successfully introduced a whole family of Wonder Woman characters having fantastic adventures in a mythological context.
DC's introduction of the reimagined superheroes did not go unnoticed by other comics companies. In 1961, with DC's JLA as the specific spur, Marvel Comics writer-editor Stan Lee and legendary creator Jack Kirby ushered in the sub-Silver Age "Marvel Age" of comics with the debut issue of "The Fantastic Four".
Since the 1940s, when Superman, Batman, and many of the company's other heroes began appearing in stories together, DC's characters inhabited a shared continuity that, decades later, was dubbed the "DC Universe" by fans. With the story "Flash of Two Worlds", in "Flash" #123 (September 1961), editor Schwartz (with writer Gardner Fox and artists Infantino and Joe Giella) introduced a concept that allowed slotting the 1930s and 1940s Golden Age heroes into this continuity via the explanation that they lived on an other-dimensional "Earth 2", as opposed to the modern heroes' "Earth 1" — in the process creating the foundation for what would later be called the DC Multiverse.
A 1966 Batman TV show on the ABC network sparked a temporary spike in comic book sales, and a brief fad for superheroes in Saturday morning animation (Filmation created most of DC's initial cartoons) and other media. DC significantly lightened the tone of many DC comics — particularly "Batman" and "Detective Comics" — to better complement the "camp" tone of the TV series. This tone coincided with the famous "Go-Go Checks" checkerboard cover-dress which featured a black-and-white checkerboard strip at the top of each comic, a misguided attempt by then-managing editor Irwin Donenfeld to make DC's output "stand out on the newsracks."
In 1967, Batman artist Infantino (who had designed popular Silver Age characters Batgirl and the Phantom Stranger) rose from art director to become DC's editorial director. With the growing popularity of upstart rival Marvel Comics threatening to topple DC from its longtime number-one position in the comics industry, he attempted to infuse the company with new titles and characters, also recruiting major talents such as ex-Marvel artist and Spider-Man co-creator Steve Ditko and promising newcomer Neal Adams. He also replaced some existing DC editors with artist-editors, including Joe Kubert and Dick Giordano, to give DC's output a more artistic critical eye.
These new editors recruited youthful new creators, in part in an effort to capture a market which had grown from being dominated by children, to include older teens and even college students. Some new talent, such as Dennis O'Neil, who had worked for both Marvel and Charlton, gained critical and popular acclaim on titles including "Batman" and "Green Lantern" (his "Green Lantern" run with artist Neal Adams became a key title in the burgeoning 1970s Bronze Age, and the move away from the Comics Code Authority). Nevertheless, the period was plagued by short-lived series that started out strong but petered out rapidly.
Kinney National subsidiary.
In 1967, National Periodical Publications was purchased by Kinney National Company, which later purchased Warner Bros.-Seven Arts and became Warner Communications.
In 1970, Jack Kirby moved from Marvel Comics to DC, at the end of the Silver Age of Comics, in which Kirby's contributions to Marvel played a large, integral role. Given carte blanche to write and illustrate his own stories, he created a handful of thematically linked series he called collectively The Fourth World. In the existing series "Superman's Pal Jimmy Olsen" and in his own, newly launched series "New Gods", "Mister Miracle", and "The Forever People", Kirby introduced such enduring characters and concepts as archvillain Darkseid and the otherdimensional realm Apokolips. While sales were respectable, they did not meet DC management's initially high expectations, and also suffered from a lack of comprehension and internal support from Infantino. By 1973 the "Fourth World" was all cancelled, although Kirby's conceptions would soon become integral to the broadening of the DC Universe. Kirby went on to create other series for DC, including "Kamandi", about a teenaged boy in a post-apocalyptic world of anthropomorphic talking animals.
The Bronze Age.
Following the science-fiction innovations of the Silver Age, the comics of the 1970s and 1980s would become known as the Bronze Age, as fantasy gave way to more naturalistic and sometimes darker themes. Illegal drug use, banned by the Comics Code Authority, explicitly appeared in comics for the first time in Marvel Comics' "The Amazing Spider-Man" in early 1971, and after the Code's updating in response, DC offered a drug-fueled storyline in writer Dennis O'Neil and artist Neal Adams' "Green Lantern", beginning with the story "Snowbirds Don't Fly" in the retitled "Green Lantern / Green Arrow" #85 (Sept. 1971), which depicted Speedy, the teen sidekick of superhero archer Green Arrow, as having become a heroin addict.
Jenette Kahn, a former children's magazine publisher, replaced Infantino as editorial director in January 1976. DC had attempted to compete with the now-surging Marvel by dramatically increasing its output and attempting to win the market by flooding it. This included launching series featuring such new characters as "Firestorm" and "Shade, the Changing Man", as well as an increasing array of non-superhero titles, in an attempt to recapture the pre-Wertham days of post-War comicdom. In June 1978, five months before the release of the first Superman movie, Kahn expanded the line further, increasing the number of titles and story pages, and raising the price from 35 cents to 50 cents. Most series received eight-page back-up features while some had full-length twenty-five page stories. This was a move the company called the "DC Explosion". The move was not successful, however, and corporate partner Warner dramatically cut back on these largely unsuccessful titles, firing many staffers in what industry watchers dubbed "the DC Implosion." In September 1978, the line was dramatically reduced and standard-size books returned to 17 story pages but for a still-increased 40 cents. By 1980, the books returned to 50 cents with a 25-page story count but the story pages replaced house ads in the books.
Seeking new ways to boost market share, the new team of publisher Kahn, vice-president Paul Levitz, and managing editor Giordano addressed the issue of talent instability. To that end — and following the example of Atlas/Seaboard Comics and such independent companies as Eclipse Comics — DC began to offer royalties in place of the industry-standard work-for-hire agreement in which creators worked for a flat fee and signed away all rights, giving talent a financial incentive tied to the success of their work. In addition, emulating the era's new television form, the miniseries while addressing the matter of an excessive number of ongoing titles fizzling out within a few issues of their start, DC created the industry concept of the comic book limited series. This publishing format allowed for the deliberate creation of finite storylines within a more flexible publishing format that could showcase creations without forcing the talent into unsustainable openended commitments.
These changes in policy shaped the future of the medium as a whole, and in the short term allowed DC to entice creators away from rival Marvel, and encourage stability on individual titles. In November 1980 DC launched the ongoing series "The New Teen Titans", by writer Marv Wolfman and artist George Pérez, two popular talents with a history of success. Their superhero-team comic, superficially similar to Marvel's ensemble series "X-Men", but rooted in DC history, earned significant sales in part due to the stability of the creative team, who both continued with the title for six full years. In addition, Wolfman and Pérez took advantage of the limited-series option to create a spin-off title, "Tales of the New Teen Titans", to present origin stories of their original characters without having to break the narrative flow of the main series or oblige them to double their work load with another ongoing title.
Modern Age.
This successful revitalization of the Silver Age Teen Titans led DC's editors to seek the same for the wider DC Universe. The result, the Wolfman/Pérez 12-issue limited series "Crisis on Infinite Earths", gave the company an opportunity to realign and jettison some of the characters' complicated backstory and continuity discrepancies. A companion publication, two volumes entitled "The History of the DC Universe", set out the revised history of the major DC characters. "Crisis" featured many key deaths that would shape the DC Universe for the following decades, and separate the timeline of DC publications into pre- and post-"Crisis".
Meanwhile, a parallel update had started in the non-superhero and horror titles. Since early 1984, the work of British writer Alan Moore had revitalized the horror series "The Saga of the Swamp Thing", and soon numerous British writers, including Neil Gaiman and Grant Morrison, began freelancing for the company. The resulting influx of sophisticated horror-fantasy material led to DC in 1993 establishing the Vertigo mature-readers imprint, which did not subscribe to the Comics Code Authority.
Two DC limited series, "" by Frank Miller and "Watchmen" by Moore and artist Dave Gibbons, drew attention in the mainstream press for their dark psychological complexity and promotion of the antihero. These titles helped pave the way for comics to be more widely accepted in literary-criticism circles and to make inroads into the book industry, with collected editions of these series as commercially successful trade paperbacks.
The mid-1980s also saw the end of many long-running DC war comics, including series that had been in print since the 1960s. These titles, all with over 100 issues, included "Sgt. Rock", "G.I. Combat", "The Unknown Soldier", and "Weird War Tales".
Time Warner unit.
In March 1989, Warner Communications merged with Time Inc., making DC Comics a subsidiary of Time Warner. In June, the first Tim Burton directed Batman movie was released, and DC began publishing its hardcover series of DC Archive Editions, collections of many of their early, key comics series, featuring rare and expensive stories unseen by many modern fans. Restoration for many of the Archive Editions was handled by Rick Keene with color restoration by DC's long-time resident colorist, Bob LeRose. These collections attempted to retroactively credit many of the writers and artists who had worked without much recognition for DC during the early period of comics, when individual credits were few and far between.
The comics industry experienced a brief boom in the early 1990s, thanks to a combination of speculative purchasing (mass purchase of the books as collectible items, with intent to resell at a higher value as the rising value of older issues was thought to imply that "all" comics would rise dramatically in price) and several storylines which gained attention from the mainstream media. DC's extended storylines in which Superman was killed, , and superhero "Green Lantern" turned into the supervillain Parallax resulted in dramatically increased sales, but the increases were as temporary as the hero's replacements. Sales dropped off as the industry went into a major slump, while manufactured "collectibles" numbering in the millions replaced quality with quantity until fans and speculators alike deserted the medium in droves.
DC's Piranha Press and other imprints (including the mature readers line Vertigo, and Helix, a short-lived science fiction imprint) were introduced to facilitate compartmentalized diversification and allow for specialized marketing of individual product lines. They increased the use of non-traditional contractual arrangements, including the dramatic rise of creator-owned projects, leading to a significant increase in critically lauded work (much of it for Vertigo) and the licensing of material from other companies. DC also increased publication of book-store friendly formats, including trade paperback collections of individual serial comics, as well as original graphic novels.
One of the other imprints was Impact Comics from 1991 to 1992 in which the Archie Comics superheroes were licensed and revamped. The stories in the line were part of its own shared universe.
DC entered into a publishing agreement with Milestone Media that gave DC a line of comics featuring a culturally and racially diverse range of superhero characters. Although the Milestone line ceased publication after a few years, it yielded the popular animated series "Static Shock". DC established Paradox Press to publish material such as the large-format "Big Book of..." series of multi-artist interpretations on individual themes, and such crime fiction as the graphic novel "Road to Perdition". In 1998, DC purchased Wildstorm Comics, Jim Lee's imprint under the Image Comics banner, continuing it for many years as a wholly separate imprint - and fictional universe - with its own style and audience. As part of this purchase, DC also began to publish titles under the fledgling WildStorm sub-imprint America's Best Comics (ABC), a series of titles created by Alan Moore, including "The League of Extraordinary Gentlemen", "Tom Strong", and "Promethea". Moore strongly contested this situation, and DC eventually stopped publishing ABC.
2000s.
In March 2003 DC acquired publishing and merchandising rights to the long-running fantasy series "Elfquest", previously self-published by creators Wendy and Richard Pini under their WaRP Graphics publication banner. This series then followed another non-DC title, Tower Comics' series T.H.U.N.D.E.R. Agents, in collection into DC Archive Editions. In 2004 DC temporarily acquired the North American publishing rights to graphic novels from European publishers 2000 AD and Humanoids. It also rebranded its younger-audience titles with the mascot Johnny DC, and established the CMX imprint to reprint translated manga. In 2006, CMX took over from Dark Horse Comics publication of the webcomic "Megatokyo" in print form. DC also took advantage of the demise of Kitchen Sink Press and acquired the rights to much of the work of Will Eisner, such as his "The Spirit" series and his graphic novels.
In 2004, DC began laying the groundwork for a full continuity-reshuffling sequel to "Crisis on Infinite Earths", promising substantial changes to the DC Universe (and side-stepping the 1994 "Zero Hour" event which similarly tried to ret-con the history of the DCU). In 2005, the critically lauded "Batman Begins" film was released; also, the company published several limited series establishing increasingly escalated conflicts among DC's heroes, with events climaxing in the "Infinite Crisis" limited series. Immediately after this event, DC's ongoing series jumped forward a full year in their in-story continuity, as DC launched a weekly series, "52", to gradually fill in the missing time. Concurrently, DC lost the copyright to "Superboy" (while retaining the trademark) when the heirs of Jerry Siegel used a provision of the 1976 revision to the copyright law to regain ownership.
In 2005, DC launched its "All-Star" line (evoking the title of the 1940s publication), designed to feature some of the company's best-known characters in stories that eschewed the long and convoluted continuity of the DC Universe. The line began with "All-Star Batman & Robin the Boy Wonder" and "All-Star Superman", with "All Star Wonder Woman" and "All Star Batgirl" announced in 2006 but neither being released nor scheduled as of the end of 2009.
DC licensed characters from the Archie Comics imprint Red Circle Comics by 2007. They appeared in the Red Circle line, based in the DC Universe, with a series of one-shots followed by a miniseries that lead into two ongoing titles, each lasting 10 issues.
In 2011 DC reboots all it's running titles following the Flashpoint Paradox story line.The reboot is called the New 52. It gives new origin story's and costume designs for all their characters.
DC Entertainment.
In September 2009, Warner Bros. announced that DC Comics would become a subsidiary of DC Entertainment, Inc., with Diane Nelson, President of Warner Premiere, becoming president of the newly formed company and DC Comics President and Publisher Paul Levitz moving to the position of Contributing Editor and Overall Consultant there.
On February 18, 2010, DC Entertainment named Jim Lee and Dan DiDio as Co-Publishers of DC Comics, Geoff Johns as Chief Creative Officer, John Rood as EVP of Sales, Marketing and Business Development, and Patrick Caldon as EVP of Finance and Administration.
DC licensed pulp characters including Doc Savage and the Spirit which it then used, along with some DC heroes, as part the First Wave comics line launched in 2010 and lasting through fall 2011.
In May 2011, DC announced it would begin releasing digital versions of their comics on the same day as paper versions.
On June 1, 2011, DC announced that it would end all ongoing series set in the DC Universe in August and relaunch its comic line with 52 issue #1s, starting with "Justice League" on August 31 (written by Geoff Johns and drawn by Jim Lee), with the rest to follow later on in September.
On June 4, 2013, DC unveiled two new digital comic innovations to enhance interactivity: "DC2" and "DC2 Multiverse". "DC2" layers dynamic artwork onto digital comic panels, adding a new level of dimension to digital storytelling, while "DC2 Multiverse" allows readers to determine a specific story outcome by selecting individual characters, storylines and plot developments while reading the comic, meaning one digital comic has multiple outcomes. "DC2" will first appear in the upcoming digital-first title, "Batman '66", based on the 1960s television series and "DC2 Multiverse" will first appear in "", a digital-first title based on the .
In October 2013, DC Entertainment (DCE) announced that the DC Comics offices would be moved from New York City to Warner Bros. Burbank, California, headquarters in 2015 joining the other DCE units, animation, movie, TV and portfolio planning, that moved there in 2010.
Logo.
DC's first logo appeared on the April 1940 issues of its titles. The letters "DC" stood for "Detective Comics", the name of Batman's flagship title. The small logo, with no background, read simply, "A DC Publication."
The November 1941 DC titles introduced an updated logo. This version was almost twice the size of the previous one, and was the first version with a white background. The name "Superman" was added to "A DC Publication," effectively acknowledging both Superman and Batman. This logo was the first to occupy the top-left corner of the cover, where the logo has usually resided since. The company now referred to itself in its advertising as "Superman-DC."
In November 1949, the logo was modified to incorporate the company's formal name, National Comics Publications. This logo would also serve as the round body of Johnny DC, DC's mascot in the 1960s.
In October 1970, DC briefly retired the circular logo in favor of a simple "DC" in a rectangle with the name of the title, or the star of the book; the logo on many issues of "Action Comics", for example, read "DC Superman." An image of the lead character either appeared above or below the rectangle. For books that did not have a single star, such as anthologies like "House of Mystery" or team series such as "Justice League of America", the title and "DC" appeared in a stylized logo, such as a bat for "House of Mystery." This use of characters as logos helped to establish the likenesses as trademarks, and was similar to Marvel's contemporaneous use of characters as part of its cover branding.
DC's titles and later 100-page and "Giant" issues published from 1972 to 1974 featured a logo exclusive to these editions: the letters "DC" in a simple sans-serif typeface within a circle. A variant had the letters in a square.
The July 1972 DC titles featured a new circular logo. The letters "DC" were rendered in a block-like typeface that would remain through later logo revisions until 2005. The title of the book usually appeared inside the circle, either above or below the letters.
In December 1973, this logo was modified with the addition of the words "The Line of DC Super-Stars" and the star motif that would continue in later logos. This logo was placed in the top center of the cover from August 1975 to October 1976.
When Jenette Kahn became DC's publisher in late 1976, she commissioned graphic designer Milton Glaser to design a new logo. Popularly referred to as the "DC bullet", this logo premiered on the February 1977 titles. Although it varied in size and color and was at times cropped by the edges of the cover, or briefly rotated 4 degrees, it remained essentially unchanged for nearly three decades. Despite logo changes since 2005, the old "DC bullet" continues to be used only on the DC Archive Editions series.
In July 1987, DC released variant editions of "Justice League" #3 and "The Fury of Firestorm" #61 with a new DC logo. It featured a picture of Superman in a circle surrounded by the words "SUPERMAN COMICS". The company released these variants to newsstands in certain markets as a marketing test.
On May 8, 2005, a new logo (dubbed the "DC spin") was unveiled, debuting on DC titles in June 2005 with "DC Special: The Return of Donna Troy" #1 and the rest of the titles the following week. In addition to comics, it was designed for DC properties in other media, which was used for movies since "Batman Begins", with "Superman Returns" showing the logo's normal variant, and the TV series "Smallville", the animated series "Justice League Unlimited" and others, as well as for collectibles and other merchandise. The logo was designed by Josh Beatman of Brainchild Studios and DC executive Richard Bruning.
In January 2012, a new logo was unveiled after DC Comics sued DC Shoes because their logo was too similar to the new DC Comics spin logo. The outcome of the case was that DC hadn't done their trademark registration properly and they wound up being required to pay DC Shoes an ongoing license fee. The new logo, consisting of the letter “D” flipping back to reveal the letter “C” and "DC ENTERTAINMENT", was deployed in March 2012. "The Dark Knight Rises" was the first film to use the new logo, while the TV series "Arrow" is the first series to feature the new logo.

</doc>
<doc id="9109" url="http://en.wikipedia.org/wiki?curid=9109" title="Diophantine equation">
Diophantine equation

In mathematics, a Diophantine equation is a polynomial equation in two or more unknowns such that only the integer solutions are searched or studied (an integer solution is a solution such that all the unknowns take integer values). A linear Diophantine equation is an equation between two sums of monomials of degree zero or one. An exponential Diophantine equation is one in which exponents on terms can be unknowns.
Diophantine problems have fewer equations than unknown variables and involve finding integers that work correctly for all equations. In more technical language, they define an algebraic curve, algebraic surface, or more general object, and ask about the lattice points on it.
The word "Diophantine" refers to the Hellenistic mathematician of the 3rd century, Diophantus of Alexandria, who made a study of such equations and was one of the first mathematicians to introduce symbolism into algebra. The mathematical study of Diophantine problems that Diophantus initiated is now called Diophantine analysis.
While individual equations present a kind of puzzle and have been considered throughout history, the formulation of general theories of Diophantine equations (beyond the theory of quadratic forms) was an achievement of the twentieth century.
Examples.
In the following Diophantine equations, "x", "y", and "z" are the unknowns and the other letters are given constants:
Linear Diophantine equations.
One equation.
The simplest linear Diophantine equation takes the form "ax" + "by" = "c", where "a", "b" and "c" are given integers. The solutions are completely described by the following theorem: "This Diophantine equation has a solution" (where "x" and "y" are integers) "if and only if" "c" "is a multiple of the greatest common divisor of" "a" "and" "b". "Moreover, if" ("x", "y") "is a solution, then the other solutions have the form" ("x" + "kv", "y" - "ku"), "where" "k" "is an arbitrary integer, and" "u" "and" "v" "are the quotients of" "a" "and" "b" "(respectively) by the greatest common divisor of" "a" "and" "b".
Proof: If "d" is this greatest common divisor, Bézout's identity asserts the existence of integers "e" and "f" such that "ae" + "bf" = "d". If "c" is a multiple of "d", then "c" = "dh" for some integer "h", and ("eh", "fh") is a solution. On the other hand, for every integers "x" and "y", the greatest common divisor "d" of "a" and "b" divides "ax" + "by". Thus, if the equation has a solution, then "c" must be a multiple of "d". If "a" = "ud" and "b" = "vd", then for every solution ("x", "y"), we have
showing that ("x" + "kv", "y" - "ku") is another solution. Finally, given two solutions such that , one deduces that . As "u" and "v" are coprime, Euclid's lemma shows that there exists an integer "k" such that and . Therefore and , which completes the proof.
Chinese remainder theorem.
The Chinese remainder theorem describes an important class of linear Diophantine systems of equations: let "n"1, ..., "n""k" be "k" pairwise coprime integers greater than one, "a"1, ..., "a""k" be "k" arbitrary integers, and "N" be the product "n"1 ··· "n""k". The Chinese remainder theorem asserts that the following linear Diophantine system has exactly one solution such that , and that the other solutions are obtained by adding to "x" a multiple of "N":
System of linear Diophantine equations.
More generally, every system of linear Diophantine equations may be solved by computing the Smith normal form of its matrix, in a way that is similar to the use of the Reduced row echelon form to solve a system of linear equations over a field. Using matrix notation every system of linear Diophantine equations may be written
where is a "m"×"n" matrix of integers, is a "n"×1 column matrix of unknowns and is a "m"×1 column matrix of integers.
The computation of the Smith normal form of provides two unimodular matrices (that is matrices that are invertible over the integers, which have ±1 as determinant) and of respective dimensions "m"×"m" and "n"×"n", such that the matrix
is such that is not zero for "i" not greater than some integer "k", and all the other entries are zero. The system to be solved may thus be rewritten as
Calling the entries of formula_6 and those of formula_7 this leads to the system
This system is equivalent to the given one in the following sense: A column matrix of integers is a solution of the given system if and only if for some column matrix of integers such that .
It follows that the system has a solution if and only if divides for "i" ≤ "k" and for "i" > "k". If this condition is fulfilled, the solutions of the given system are
where are arbitrary integers.
Diophantine analysis.
Typical questions.
The questions asked in Diophantine analysis include:
These traditional problems often lay unsolved for centuries, and mathematicians gradually came to understand their depth (in some cases), rather than treat them as puzzles.
Typical problem.
The given information is that a father's age is 1 less than twice that of his son, and that the digits AB making up the father's age are reversed in the son's age (i.e. BA). This leads to the equation 10A + B = 2 (10B + A) - 1, thus 19B - 8A = 1. Inspection gives the result A = 7, B = 3, and thus AB = 73 years and BA = 37 years. One may easily show that there is not any other solution with A and B positive integers less than 10.
17th and 18th centuries.
In 1637, Pierre de Fermat scribbled on the margin of his copy of "Arithmetica": "It is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second into two like powers." Stated in more modern language, "The equation "a""n" + "b""n" = "c""n" has no solutions for any "n" higher than 2." And then he wrote, intriguingly: "I have discovered a truly marvelous proof of this proposition, which this margin is too narrow to contain." Such a proof eluded mathematicians for centuries, however, and as such his statement became famous as Fermat's Last Theorem. It wasn't until 1995 that it was proven by the British mathematician Andrew Wiles.
In 1657, Fermat attempted to solve the Diophantine equation 61"x"2 + 1 = "y"2 (solved by Brahmagupta over 1000 years earlier). The equation was eventually solved by Euler in the early 18th century, who also solved a number of other Diophantine equations.The smallest solution of this equation in positive integers is "x" = 226153980, "y" = 1766319049 (see Chakravala method).
Hilbert's tenth problem.
In 1900, in recognition of their depth, David Hilbert proposed the solvability of all Diophantine problems as the tenth of his celebrated problems. In 1970, a novel result in mathematical logic known as Matiyasevich's theorem settled the problem negatively: in general Diophantine problems are unsolvable.
Diophantine geometry.
Diophantine geometry, which is the application of techniques from algebraic geometry in this field, has continued to grow as a result; since treating arbitrary equations is a dead end, attention turns to equations that also have a geometric meaning. The central idea of Diophantine geometry is that of a rational point, namely a solution to a polynomial equation or a system of polynomial equations, which is a vector in a prescribed field "K", when "K" is "not" algebraically closed.
Modern research.
One of the few general approaches is through the Hasse principle. Infinite descent is the traditional method, and has been pushed a long way.
The depth of the study of general Diophantine equations is shown by the characterisation of Diophantine sets as equivalently described as recursively enumerable. In other words, the general problem of Diophantine analysis is blessed or cursed with universality, and in any case is not something that will be solved except by re-expressing it in other terms.
The field of Diophantine approximation deals with the cases of "Diophantine inequalities". Here variables are still supposed to be integral, but some coefficients may be irrational numbers, and the equality sign is replaced by upper and lower bounds.
The most celebrated single question in the field, the conjecture known as Fermat's Last Theorem, was solved by Andrew Wiles but using tools from algebraic geometry developed during the last century rather than within number theory where the conjecture was originally formulated. Other major results, such as Faltings' theorem, have disposed of old conjectures.
Infinite Diophantine equations.
An example of an infinite diophantine equation is:
which can be expressed as "How many ways can a given integer "N" be written as the sum of a square plus twice a square plus thrice a square and so on?" The number of ways this can be done for each "N" forms an integer sequence. Infinite Diophantine equations are related to theta functions and infinite dimensional lattices. This equation always has a solution for any positive "N". Compare this to:
which does not always have a solution for positive "N".
Exponential Diophantine equations.
If a Diophantine equation has as an additional variable or variables occurring as exponents, it is an exponential Diophantine equation. One example is the Ramanujan–Nagell equation, 2"n" − 7 = "x"2. A general theory for such equations is not available; particular cases such as Catalan's conjecture have been tackled. However, the majority are solved via ad hoc methods such as Størmer's theorem or even trial and error.

</doc>
<doc id="9110" url="http://en.wikipedia.org/wiki?curid=9110" title="Diophantus">
Diophantus

Diophantus of Alexandria (; born sometime between AD 201 and 215; died aged 84 sometime between AD 285 and 299), sometimes called "the father of algebra", was an Alexandrian Greek mathematician and the author of a series of books called "Arithmetica", many of which are now lost. These texts deal with solving algebraic equations. While reading Claude Gaspard Bachet de Méziriac's edition of Diophantus' "Arithmetica," Pierre de Fermat concluded that a certain equation considered by Diophantus had no solutions, and noted in the margin without elaboration that he had found "a truly marvelous proof of this proposition," now referred to as Fermat's Last Theorem. This led to tremendous advances in number theory, and the study of Diophantine equations ("Diophantine geometry") and of Diophantine approximations remain important areas of mathematical research. Diophantus coined the term παρισὀτης (parisotes) to refer to an approximate equality. This term was rendered as "adaequalitat" in Latin, and became the technique of adequality developed by Pierre de Fermat to find maxima for functions and tangent lines to curves. Diophantus was the first Greek mathematician who recognized fractions as numbers; thus he allowed positive rational numbers for the coefficients and solutions. In modern use, Diophantine equations are usually algebraic equations with integer coefficients, for which integer solutions are sought. Diophantus also made advances in mathematical notation.
Biography.
Little is known about the life of Diophantus. He lived in Alexandria, Egypt, probably from between AD 200 and 214 to 284 or 298. Much of our knowledge of the life of Diophantus is derived from a 5th-century Greek anthology of number games and puzzles created by Metrodorus. One of the problems (sometimes called his epitaph) states:
This puzzle implies that Diophantus' age formula_1 can be expressed as
which gives formula_3 a value of 84 years. However, the accuracy of the information cannot be independently confirmed.
In popular culture, this puzzle was the Puzzle No.142 in "Professor Layton and Pandora's Box" as one of the hardest solving puzzles in the game, which needed to be unlocked by solving other puzzles first.
"Arithmetica".
The Arithmetica is the major work of Diophantus and the most prominent work on algebra in Greek mathematics. It is a collection of problems giving numerical solutions of both determinate and indeterminate equations. Of the original thirteen books of which Arithmetica consisted only six have survived, though there are some who believe that four Arab books discovered in 1968 are also by Diophantus. Some Diophantine problems from Arithmetica have been found in Arabic sources.
It should be mentioned here that Diophantus never used general methods in his solutions. Hermann Hankel, renowned German mathematician made the following remark regarding Diophantus.
“Our author (Diophantos) not the slightest trace of a general, comprehensive method is discernible; each problem calls for some special method which refuses to work even for the most closely related problems. For this reason it is difficult for the modern scholar to solve the 101st problem even after having studied 100 of Diophantos’s solutions” 
History.
Like many other Greek mathematical treatises, Diophantus was forgotten in Western Europe during the so-called Dark Ages, since the study of ancient Greek, and literacy in general, had greatly declined. The portion of the Greek "Arithmetica" that survived, however, was, like all ancient Greek texts transmitted to the early modern world, copied by, and thus known to, medieval Byzantine scholars. In addition, some portion of the "Arithmetica" probably survived in the Arab tradition (see above). In 1463 German mathematician Regiomontanus wrote:
"Arithmetica" was first translated from Greek into Latin by Bombelli in 1570, but the translation was never published. However, Bombelli borrowed many of the problems for his own book "Algebra". The "editio princeps" of "Arithmetica" was published in 1575 by Xylander. The best known Latin translation of "Arithmetica" was made by Bachet in 1621 and became the first Latin edition that was widely available. Pierre de Fermat owned a copy, studied it, and made notes in the margins.
Margin-writing by Fermat and Chortasmenos.
The 1621 edition of Arithmetica by Bachet gained fame after Pierre de Fermat wrote his famous "Last Theorem" in the margins of his copy:
Fermat's proof was never found, and the problem of finding a proof for the theorem went unsolved for centuries. A proof was finally found in 1994 by Andrew Wiles after working on it for seven years. It is believed that Fermat did not actually have the proof he claimed to have. Although the original copy in which Fermat wrote this is lost today, Fermat's son edited the next edition of Diophantus, published in 1670. Even though the text is otherwise inferior to the 1621 edition, Fermat's annotations—including the "Last Theorem"—were printed in this version.
Fermat was not the first mathematician so moved to write in his own marginal notes to Diophantus; the Byzantine scholar John Chortasmenos (1370–1437) had written "Thy soul, Diophantus, be with Satan because of the difficulty of your theorems" next to the same problem.
Other works.
Diophantus wrote several other books besides "Arithmetica", but very few of them have survived.
The "Porisms".
Diophantus himself refers to a work which consists of a collection of lemmas called "The Porisms" (or "Porismata"), but this book is entirely lost.
Although "The Porisms" is lost, we know three lemmas contained there, since Diophantus refers to them in the "Arithmetica". One lemma states that the difference of the cubes of two rational numbers is equal to the sum of the cubes of two other rational numbers, i.e. given any "a" and "b", with "a" > "b", there exist "c" and "d", all positive and rational, such that
Polygonal numbers and geometric elements.
Diophantus is also known to have written on polygonal numbers, a topic of great interest to Pythagoras and Pythagoreans. Fragments of a book dealing with polygonal numbers are extant.
A book called "Preliminaries to the Geometric Elements" has been traditionally attributed to Hero of Alexandria. It has been studied recently by Wilbur Knorr, who suggested that the attribution to Hero is incorrect, and that the true author is Diophantus.
Influence.
Diophantus' work has had a large influence in history. Editions of Arithmetica exerted a profound influence on the development of algebra in Europe in the late sixteenth and through the 17th and 18th centuries. Diophantus and his works have also influenced Arab mathematics and were of great fame among Arab mathematicians. Diophantus' work created a foundation for work on algebra and in fact much of advanced mathematics is based on algebra. As far as we know Diophantus did not affect the lands of the Orient much and how much he affected India is a matter of debate.
The father of algebra?
Diophantus is often called “the father of algebra" because he contributed greatly to number theory, mathematical notation, and because Arithmetica contains the earliest known use of syncopated notation. However, it seems that many of the methods for solving linear and quadratic equations used by Diophantus go back to Babylonian mathematics. For this, and other, reasons mathematical historian Kurt Vogel writes: “Diophantus was not, as he has often been called, the father of algebra. Nevertheless, his remarkable, if unsystematic, collection of indeterminate problems is a singular achievement that was not fully appreciated and further developed until much later.”
Diophantine analysis.
Today, Diophantine analysis is the area of study where integer (whole-number) solutions are sought for equations, and Diophantine equations are polynomial equations with integer coefficients to which only integer solutions are sought. It is usually rather difficult to tell whether a given Diophantine equation is solvable. Most of the problems in Arithmetica lead to quadratic equations. Diophantus looked at 3 different types of quadratic equations: formula_6, formula_7, and formula_8. The reason why there were three cases to Diophantus, while today we have only one case, is that he did not have any notion for zero and he avoided negative coefficients by considering the given numbers formula_9 to all be positive in each of the three cases above. Diophantus was always satisfied with a rational solution and did not require a whole number which means he accepted fractions as solutions to his problems. Diophantus considered negative or irrational square root solutions "useless", "meaningless", and even "absurd". To give one specific example, he calls the equation formula_10 'absurd' because it would lead to a negative value for "x". One solution was all he looked for in a quadratic equation. There is no evidence that suggests Diophantus even realized that there could be two solutions to a quadratic equation. He also considered simultaneous quadratic equations.
Mathematical notation.
Diophantus made important advances in mathematical notation, becoming the first person known to use algebraic notation and symbolism. Before him everyone wrote out equations completely. Diophantus introduced an algebraic symbolism that used an abridged notation for frequently occurring operations, and an abbreviation for the unknown and for the powers of the unknown. Mathematical historian Kurt Vogel states:
“The symbolism that Diophantus introduced for the first time, and undoubtedly devised himself, provided a short and readily comprehensible means of expressing an equation... Since an abbreviation is also employed for the word ‘equals’, Diophantus took a fundamental step from verbal algebra towards symbolic algebra.”
Although Diophantus made important advances in symbolism, he still lacked the necessary notation to express more general methods. This caused his work to be more concerned with particular problems rather than general situations. Some of the limitations of Diophantus' notation are that he only had notation for one unknown and, when problems involved more than a single unknown, Diophantus was reduced to expressing "first unknown", "second unknown", etc. in words. He also lacked a symbol for a general number n. Where we would write formula_11, Diophantus has to resort to constructions like : ... a sixfold number increased by twelve, which is divided by the difference by which the square of the number exceeds three.
Algebra still had a long way to go before very general problems could be written down and solved succinctly.

</doc>
<doc id="9111" url="http://en.wikipedia.org/wiki?curid=9111" title="Dong">
Dong

Dong or DONG may refer to:

</doc>
<doc id="9118" url="http://en.wikipedia.org/wiki?curid=9118" title="Duke Kahanamoku">
Duke Kahanamoku

Duke Paoa Kahinu Mokoe Hulikohola Kahanamoku (August 24, 1890 – January 22, 1968) was an American competition swimmer who was also known as an actor, lawman, early beach volleyball player and businessman credited with spreading the sport of surfing. Kahanamoku was a five-time Olympic medalist in swimming.
Early years.
His birthplace is disputed with many sources stating Haleakalā on Maui or Waikiki on Oahu. According to Kahanamoku, he stated he was born at Honolulu at Haleʻākala, the home of Bernice Pauahi Bishop which was later converted into the Arlington Hotel. He had five brothers and three sisters, including Samuel Kahanamoku. In 1893, the family moved to Kālia, Waikiki (near the present site of the Hilton Hawaiian Village), to be closer to his mother's parents and family. Duke grew up with his siblings and 31 Paoa cousins. Duke attended the Waikiki Grammar School, Kaahumanu School, and the Kamehameha Schools, although he never graduated because he had quit to help support the family.
"Duke" was not a title or a nickname, but a given name. He was named after his father, , who was christened by Bernice Pauahi Bishop in honor of Prince Alfred, Duke of Edinburgh, who was visiting Hawaii at the time. The younger Duke, as eldest son, inherited the name. His father was a policeman. His mother was a deeply religious woman with a strong sense of family ancestry.
When Duke became a household name due to his swimming feats, many people thought he was of Hawaiian royalty. It was assumed by many that he was a duke and that it was his title. He was a very modest and unassuming man who got a chuckle of being thought of as royalty and never hesitated to set the record straight about his lineage.
However, his parents were from prominent Hawaiian families; the Kahanamoku and the Paoa clans were considered to be lower-ranking chiefs or nobles, who were of service to the "aliʻi nui" or royalty. His paternal grandparents were Kahanamoku and grandmother Kapiolani Kaoeha, a descendant of King Alapainui. They were "kahu", retainers and trusted advisors of the Kamehamehas, whom they were related to. His maternal grandparents Paoa, son of Paoa Hoolae and Hiikaalani, and Mele Uliama were also of chiefly descent.
Growing up on the outskirts of Waikiki, Kahanamoku spent his youth as a bronzed beach boy. At Waikiki Beach he developed his surfing and swimming skills. In his youth, Kahanamoku preferred a traditional surf board, which he called his "papa nui", constructed after the fashion of ancient Hawaiian "olo" boards. Made from the wood of a koa tree, it was long and weighed . The board was without a skeg, which had yet to be invented. In his later career, he would often use smaller boards but always preferred those made of wood.
On August 11, 1911, Kahanamoku was timed at 55.4 seconds in the freestyle, beating the existing world record by 4.6 seconds, in the salt water of Honolulu Harbor. He also broke the record in the and equaled it in the . But the Amateur Athletic Union (AAU), in disbelief, would not recognize these feats until many years later. The AAU initially claimed that the judges must have been using alarm clocks rather than stopwatches and later claimed that ocean currents aided Kahanamoku.
Career and legacy.
Kahanamoku easily qualified for the U.S. Olympic swimming team in 1912, breaking the record for the 200 meter freestyle in his trial heat for the 4×200 relay. He went on to win a gold medal in the 100 meter freestyle in the 1912 Olympics in Stockholm, and a silver with the relay team. During the 1920 Olympics in Antwerp, he won gold medals both in the 100 meters (bettering fellow Hawaiian Pua Kealoha) and in the relay. He finished the 100 meters with a silver medal during the 1924 Olympics in Paris, with the gold going to Johnny Weissmuller and the bronze to Duke's brother, Samuel Kahanamoku. At age 34, this was Kahanamoku's last Olympic medal. He also was an alternate for the U.S. water polo team at the 1932 Summer Olympics.
Between Olympic competitions, and after retiring from the Olympics, Kahanamoku traveled internationally to give swimming exhibitions. It was during this period that he popularized the sport of surfing, previously known only in Hawaii, by incorporating surfing exhibitions into these visits as well. His surfing exhibition at Sydney's Freshwater Beach on December 23, 1914 is widely regarded as a seminal event in the development of surfing in Australia. The board that Kahanamoku built from a piece of pine from a local hardware store is retained by the Freshwater Surf Club. There is a statue of Kahanamoku on the headland at Freshwater. He made surfing popular in mainland America first in 1912 while in Southern California.
During his time living in Southern California, Kahanamoku performed in Hollywood as a background actor and a character actor in several films. In this way, he made connections with people who could further publicize the sport of surfing. Kahanamoku was involved with the Los Angeles Athletic Club, acting as lifeguard and competing on both swimming and water polo teams.
While living in Newport Beach, California on June 14, 1925, Kahanamoku rescued eight men from a fishing vessel that capsized in heavy surf while attempting to enter the city's harbor. 29 fishermen went into the water and 17 perished. Using his surfboard, he was able to make quick trips back and forth to shore to increase the number of sailors rescued. Two other surfers saved four more fishermen. Newport's police chief at the time called Duke's efforts "the most superhuman surfboard rescue act the world has ever seen." 
In 1940, he married Nadine Alexander, who accompanied him when he traveled. Kahanamoku was the first person to be inducted into both the Swimming Hall of Fame and the Surfing Hall of Fame. The Duke Kahanamoku Invitational Surfing Championships are named in his honor. He is a member of the U.S. Olympic Hall of Fame. He served as sheriff of Honolulu, Hawaii from 1932 to 1961, serving 13 consecutive terms. During this period, he also appeared in a number of television programs and films, such as "Mister Roberts" (1955).
Kahanamoku was a friend and surfing companion of heiress Doris Duke, who built a home (now a museum) on Oahu named Shangri-la.
Hawaii music promoter Kimo Wilder McVay capitalized on Kahanamoku's popularity by naming his Waikiki showroom "Duke Kahanamoku's", and giving Kahanamoku a piece of the financial action in exchange for the use of his name. It was a major Waikiki showroom in the 1960s and is remembered as the home of Don Ho & The Aliis from 1964 through 1969.
Kahanamoku's name is also used by Duke's Canoe Club & Barefoot Bar, a beachfront bar and restaurant in the Outrigger Waikiki On The Beach Hotel. There is a chain of restaurants named after him in California and Hawaii called Duke's. A bronze statue at Waikiki beach in Honolulu honors his memory. It shows Kahanamoku standing in front of his surfboard with his arms outstretched. Many honor him by placing leis on his statue. There is a webcam watching the statue, allowing visitors from around the world to wave to their friends.
On August 24, 2002, which was also the 112th anniversary of the birth of Duke Kahanamoku, a 37c first-class letter rate postage stamp of the United States Postal Service with Duke's picture on, was issued. The First Day Ceremony was held at the Hilton Hawaiian Village in Waikiki and was attended by thousands. At this ceremony, attendees could attach the Duke stamp to an envelope and get it canceled with a First Day of Issue postmark. These First Day Covers are very collectable.
"Duncan v. Kahanamoku".
Kahanamoku was the "pro forma" defendant in the landmark Supreme Court case "Duncan v. Kahanamoku". While Kahanamoku was a military police officer during World War II, he arrested Duncan for public intoxication. At the time, Hawaii, not yet a state, was being administered under the Hawaiian Organic Act which effectively instituted martial law on the island. Duncan was therefore tried by a military tribunal and appealed to the Supreme Court. In a "post hoc" ruling, the court ruled that trial by military tribunal was, in this case, unconstitutional.
Death.
Kahanamoku died of a heart attack on January 22, 1968 at the age of 77. For his burial at sea a long motorcade of mourners, accompanied by a 30-man police escort, moved across town to Waikiki Beach. Reverend Abraham Akaka, the pastor of Kawaiahao Church, performed the service. A group of beach boys sang Hawaiian songs, including "Aloha Oe". His ashes were scattered into the ocean.

</doc>
<doc id="9119" url="http://en.wikipedia.org/wiki?curid=9119" title="Distinguished Service Medal (U.S. Army)">
Distinguished Service Medal (U.S. Army)

The Distinguished Service Medal (DSM) is a military award of the United States Army that is presented to any person who, while serving in any capacity with the United States military, has distinguished himself or herself by exceptionally meritorious service to the Government in a duty of great responsibility. The performance must be such as to merit recognition for service that is clearly exceptional. Exceptional performance of normal duty will not alone justify an award of this decoration.
Separate Distinguished Service Medals exist for the different branches of the military as well as a fifth version of the medal which is a senior award of the United States Department of Defense. The Army version of the Distinguished Service Medal is typically referred to simply as the "Distinguished Service Medal" while the other branches of service use the service name as a prefix.
For service not related to actual war, the term "duty of a great responsibility" applies to a narrower range of positions than in time of war, and requires evidence of conspicuously significant achievement. However, justification of the award may accrue by virtue of exceptionally meritorious service in a succession of high positions of great importance.
Awards may be made to persons other than members of the Armed Forces of the United States for wartime services only, and then only under exceptional circumstances, with the express approval of the President in each case.
Criteria.
The Distinguished Service Medal is awarded to any person who, while serving in any capacity with the United States Army, has distinguished himself or herself by exceptionally meritorious service to the Government in a duty of great responsibility.
The performance must be such as to merit recognition for service which is clearly exceptional. Exceptional performance of normal duty will not alone justify an award of this decoration. For service not related to actual war, the term "duty of a great responsibility" applies to a narrower range of positions than in time of war and requires evidence of a conspicuously significant achievement. However, justification of the award may accrue by virtue of exceptionally meritorious service in a succession of high positions of great importance. Awards may be made to persons other than members of the Armed Forces of the United States for wartime services only, and only then under exceptional circumstances with the express approval of the President in each case.
History of the Distinguished Service Medal.
The Distinguished Service Medal was authorized by Presidential Order dated 1918-01-02, and confirmed by Congress on 1918-07-09. It was announced by War Department General Order No. 6, 1918-01-12, with the following information concerning the medal: "A bronze medal of appropriate design and a ribbon to be worn in lieu thereof, to be awarded by the President to any person who, while serving in any capacity with the Army shall hereafter distinguish himself or herself, or who, since 1917-04-06, has distinguished himself or herself by exceptionally meritorious service to the Government in a duty of great responsibility in time of war or in connection with military operations against an armed enemy of the United States." The Act of Congress on 1918-07-09, recognized the need for different types and degrees of heroism and meritorious service and included such provisions for award criteria. The current statutory authorization for the Distinguished Service Medal is Title 10, United States Code, Section 3743.
Recipients.
More than 2,000 awards were made during World War I, and by the time the United States entered World War II, approximately 2,800 awards had been made. From July 1, 1941 to June 6, 1969, when the Army stopped publishing awards of the DSM in Department of the Army General Orders, over 2,800 further awards were made.
Until the first award of the Air Force Distinguished Service Medal in 1965, United States Air Force personnel received this award as well, as was the case with several other Army decorations until the Air Force fully established its own system of decorations.
Notable recipients.
Because the Army Distinguished Service Medal is principally awarded to general officers, a list of notable recipients would include nearly every general and admiral since 1918, many of whom received multiple awards, as well as a few civilians and sergeants major prominent for their contributions to national defense. Generals of the Army Douglas MacArthur and Dwight Eisenhower are tied for the record of the greatest number of awards received of the Army Distinguished Service Medal at five each. They also each received one award of the Navy Distinguished Service Medal. Among notable recipients below flag rank are: X-1 test pilot Chuck Yeager and X-15 test pilot Robert M. White, who both received the DSM as U.S. Air Force majors; Air Force Major Rudolf Anderson, the U-2 pilot shot down during the Cuban Missile Crisis; director Frank Capra, decorated in 1945 as an Army colonel; actor James Stewart, decorated in 1945 as an Army Air Forces colonel (later Air Force Brigadier General); Col. Wendell Fertig, who led Filipino guerrillas behind Japanese lines; Col. (later Major General) John K. Singlaub, who led partisan forces in the Korean War; and Maj. Maude C. Davison, who led the "Angels of Bataan and Corregidor" during their imprisonment by the Japanese, and Colonel William S. Taylor, Program Manager Multiple Launch Rocket System. Among notable civilian recipients are Harry L. Hopkins, Robert S. McNamara and Henry L. Stimson.
Notable American and foreign recipients include:

</doc>
<doc id="9120" url="http://en.wikipedia.org/wiki?curid=9120" title="Defense Distinguished Service Medal">
Defense Distinguished Service Medal

The Defense Distinguished Service Medal is a United States military award which is presented for exceptionally distinguished performance of duty contributing to National security or defense of the United States. The medal was created on July 9, 1970 by President Richard Nixon in .
Criteria.
It is the United States's highest non-combat related military award and it is the highest joint service decoration. The Defense Distinguished Service Medal is awarded only while assigned to a joint activity. Normally, such responsibilities deserving of the Defense Distinguished Service Medal are held by the most senior officers such as the Chairman and Vice Chairman of the Joint Chiefs of Staff, the Chiefs and Vice Chiefs of the Services, and Commanders and Deputy Commanders of the Combatant Commands, the Director of the Joint Staff etc., whose duties bring them frequently into direct contact with the Secretary of Defense, the Deputy Secretary of Defense, and other senior government officials. In addition, the medal may also be awarded to other service members whose direct and individual contributions to National security or National defense are recognized as being so exceptional in scope and value as to be equivalent to contributions normally associated with positions encompassing broader responsibilities.
This decoration takes precedence over the Distinguished Service Medals of the separate services and is not to be awarded to any individual for a period of service for which an Army, Navy, Air Force or Coast Guard Distinguished Service Medal is awarded.
Appearance.
The medal is gold in color and on the obverse it features a medium blue enameled pentagon (point up). Superimposed on this is an American bald eagle with wings outspread facing left grasping three crossed arrows in its talons and on its breast is a shield of the United States. The pentagon and eagle are enclosed within a gold pieced circle consisting, in the upper half of 13 five-pointed stars and in the lower half, a wreath of laurel on the left and olive on the right. At the top is a suspender of five graduated gold rays. The reverse of the medal has the inscription ""For Distinguished Service" at the top in raised letters, and within the pentagon the inscription "From The Secretary of Defense To,"" all in raised letters. 
Additional awards of the Defense Distinguished Service Medal are denoted by oak leaf clusters.

</doc>
<doc id="9121" url="http://en.wikipedia.org/wiki?curid=9121" title="Dacoity">
Dacoity

Dacoity is a term used for "banditry" in Hindi, Kannada and Urdu. The spelling is the anglicized version of the Hindi word and as a colloquial Anglo-Indian word with this meaning, it appears in the "Glossary of Colloquial Anglo-Indian Words and Phrases" (1903). Banditry is criminal activity involving robbery by groups of armed bandits. The East India Company established the Thuggee and Dacoity Department in 1830, and the Thuggee and Dacoity Suppression Acts, 1836–1848 were enacted in British India under East India Company rule. Areas with ravines or forests, such as Chambal and Chilapata Forests, were once known for dacoits.
Etymology.
The word "dacoity" is the anglicized version of the Hindustani word "ḍakaitī" (historically spelled "dakaitee", Hindi डकैती or Urdu ڈکیتی or Bengali ডাকাতি), which comes from "ḍākū" (historically spelled "dakoo", Hindi: डाकू, Urdu: ڈاکو, meaning "armed robber") or Bengali "ḍakat" (ডাকাত).
In Urdu, "ḍākū" ڈاکو is singular and "ḍakait" ڈکیت plural for bandits. The crime of banditry is known as "dakaitee" ڈکیتی.
In Hindi dacoity (Hindi: डकैती "ḍakaitī", Urdu: ڈکیتی "ḍakaitī", ) means "armed robbery".
In Tamil Nadu, the crime of banditry is known as "Dakalti".
The term dacoit (Hindi: डकैत "ḍakait", Urdu: ڈکیت "ḍakait", ) means "a bandit", according to "OED" ("A member of a class of robbers in India and Burma, who plunder in armed bands.") Dacoits existed in Burma as well as India, and Rudyard Kipling's fictional Private Mulvaney was hunting Burmese "dacoits" in The Taking of Lungtungpen. Sax Rohmer's criminal mastermind Dr. Fu Manchu also employed Burmese dacoits as his henchmen. The term was also applied, according to "OED", to "pirates who formerly infested the Ganges between Calcutta and Burhampore".
"Known Dacoit" (K.D.) is a term used by the Indian police forces to classify criminals. 
The dacoity have had a large impact in the Morena and Chambal regions.
Dacoits.
India's Phoolan Devi authored an autobiography and the movie "Bandit Queen", released in 1994, was based on her life.
Dadua was one of the most infamous dacoit in U.P. and M.P. having very strong political links. His son Veer Sing is a MLA and brother Bal Kumar patel is MP. his nephew Ram Singh is also a MLA.
Daku Man Singh Roopa Pandit committed over 1,000 armed robberies, 185 murders, and many ransom kidnappings between 1939 and 1955. He was involved in 90 police encounters and killed 32 police officers.
Daku Paan Singh Tomar was another infamous dacoit in Chambal. He became an international athlete after retirement. Paan Singh Tomar became so infamous that the government had to call up around 10,000 men from BSF, CRPF and state police in an attempt to capture him. Other famous well known dacoits are Jagga Jatt and Sucha Singh Soorma from Punjab.
Veerappan evaded authorities for decades until he was shot and killed in 2004. He was active for a period of years in an area covering 6,000 km² in the states of Karnataka, Kerala and Tamil Nadu.
Chambal dacoit Nirbhay Singh Gujjar achieved national infamy before being killed in 2005.
Other infamous dacoits were Sultana Daku in the Bijnor district, and Dhira, mostly active in present Amritsar, particularly in the Majitha area.
Protection measures.
In Madhya Pradesh State, women belonging to a village defence group have been issued firearm permits to fend off dacoity. The chief minister of the state, Shivraj Singh Chouhan, recognized the role the women had played in defending their villages without guns. He stated that he wanted to enable these women to better defend both themselves and their villages, and issued the gun permits to advance this goal.
Popular culture.
As the dacoits flourished through the 1950s-1970s, they were the subject of several films made during this era, beginning with "Ganga Jamuna" (1961) and Raj Kapoor’s "Jis Desh Mein Ganga Behti Hai" (1960), and Sunil Dutt's classic, "Mujhe Jeene Do" (1963). Other films in this genre were, "Khote Sikkay" (1973), "Mera Gaon Mera Desh" (1971), and "Kuchhe Dhaage" (1973) both by Raj Khosla, the latter inspired the blockbuster, "Sholay" (1975) where the character of Gabbar Singh was played by Amjad Khan. "Sholay" became a classic in the genre, and its success lead to a surge in films in this genre, including "Ganga Ki Saugandh" (1978) once again with Amitabh Bachchan, and Amjad Khan.
Punjabi biopic "Jatt Jeona Morh" about the noted dacoit Jatt Jeona Morh was made in 1991; also in that same year came, "Jagga Daku" based on a noted outlaw and dacoit during the British Raj, Jagga Daku.
A Hindi novel पैंसठ लाख की डकैती (Painstth Lakh ki Dacoity, 1977) was written by Surender Mohan Pathak; it was translated as "The 65 Lakh Heist".
Repentant dacoits armed with pistols are trainable units available to the player in "".

</doc>
<doc id="9123" url="http://en.wikipedia.org/wiki?curid=9123" title="Davis, California">
Davis, California

Davis is a city in Yolo County, California, United States. It is part of the Sacramento–Arden-Arcade–Roseville Metropolitan Statistical Area. According to estimates published by the US Census Bureau, the city had a total population of 65,622 in 2010 (60,308 in 2000), neither of which includes the on-campus population of UC Davis, which was 5,786 people according to the 2010 United States Census while the official student headcount is 28,475 for the 2011–12 school year. It is the largest city in Yolo County, and the 122nd largest in the state, by population. 
Davis is known for its liberal politics, for having many bicycles and bike paths, and for the campus of the University of California, Davis.
History.
Davis grew around a Southern Pacific Railroad depot built in 1868. It was then known as "Davisville," named after Jerome C. Davis, a prominent local farmer. However, the post office at Davisville shortened the town name simply to "Davis" in 1907. The name stuck, and the city of Davis was incorporated on March 28, 1917.
From its inception as a farming community, Davis has been known for its contributions to agricultural policy along with veterinary care and animal husbandry. Following the passage of the University Farm Bill in 1905 by the California State Legislature, Governor George Pardee selected Davis out of 50 other sites as the future home to the University of California's University Farm, officially opening to students in 1908. The farm, later renamed the Northern Branch of the College of Agriculture in 1922, was upgraded into the seventh UC general campus, the University of California, Davis, in 1959.
Geography and environment.
Location.
Davis is located in Yolo County, California, west of Sacramento, northeast of San Francisco, north of Los Angeles, at the intersection of Interstate 80 and State Route 113. Neighboring towns include Dixon, Winters, and Woodland.
Davis lies in the Sacramento Valley, the northern portion of the Central San Joaquin Valley, in Northern California, at an elevation of about 16 m (52 ft) above sea level.
According to the United States Census Bureau, the city has a total area of . of it is land and of it (0.19%) is water.
The topography is flat, which has helped Davis to become known as a haven for bicyclists.
Climate.
The Davis climate resembles that of nearby Sacramento and is typical of California's Central Valley Mediterranean climate regime: dry, hot summers and cool, rainy, winters. It is classified as a Köppen Csa climate. Average temperatures range from in December and January to in July and August. Thick ground fog called Tule fog settles into Davis during late fall and winter. This fog can be dense with visibility to nearly zero. As in other areas of northern California, the tule fog is a leading cause of accidents in the winter season.
Record temperatures range from a high of on July 17, 1925, to a low of on December 11, 1932.
Neighborhoods.
Davis is internally divided by two freeways (Interstate 80 and State Route 113), a north-south railroad (California Northern), an east-west mainline (Union Pacific) and several major streets. The city is unofficially divided into six main districts made up of smaller neighborhoods (often originally named as housing subdivisions):
The University of California, Davis is located south of Russell Boulevard and west of A Street and then south of 1st Street. The land occupied by the university is not incorporated within the boundaries of the city of Davis and lies within both Yolo and Solano Counties.
Environment.
On November 14, 1984, the Davis City Council declared the city to be a nuclear free zone. In 1998, the City passed a 'Dark Skies' ordinance in an effort to reduce light pollution in the night sky. 
Demographics.
2010.
The 2010 United States Census reported that Davis had a population of 65,622. The population density was 6,615.8 people per square mile (2,554.4/km²). The racial makeup of Davis was 42,571 (64.9%) White, 1,528 (2.3%) African American, 339 (0.5%) Native American, 14,355 (21.9%) Asian, 136 (0.2%) Pacific Islander, 3,121 (4.8%) from other races, and 3,572 (5.4%) from two or more races. Hispanic or Latino of any race were 8,172 persons (12.5%).
In 2006, Davis was ranked as the second most educated city (in terms of the percentage of residents with graduate degrees) in the US by CNN "Money Magazine", after Arlington, Virginia.
Davis' Asian population of 14,355 was apportioned among 1,631 Indian Americans, 6,395 Chinese Americans, 1,560 Korean Americans, 1,185 Vietnamese Americans, 1,033 Filipino Americans, 953 Japanese Americans, and 1,598 other Asian Americans.
Davis' Hispanic and Latino population of 8,172 was apportioned among 5,618 Mexican American, 221 Puerto Rican American, 80 Cuban American, and 2,253 other Hispanic and Latino.
The Census reported that 63,522 people (96.8% of the population) lived in households, 1,823 (2.8%) lived in non-institutionalized group quarters, and 277 (0.4%) were institutionalized.
There were 24,873 households, of which 6,119 (24.6%) had children under the age of 18 living in them, 9,343 (37.6%) were opposite-sex married couples living together, 1,880 (7.6%) had a female householder with no husband present, and 702 (2.8%) had a male householder with no wife present. There were 1,295 (5.2%) unmarried opposite-sex partnerships, and 210 (0.8%) same-sex married couples or partnerships. 5,952 households (23.9%) were made up of individuals and 1,665 (6.7%) had someone living alone who was 65 years of age or older. The average household size was 2.55. There were 11,925 families (47.9% of all households); the average family size was 2.97.
The population age and sex distribution was 10,760 people (16.4%) under the age of 18, 21,757 people (33.2%) aged 18 to 24, 14,823 people (22.6%) aged 25 to 44, 12,685 people (19.3%) aged 45 to 64, and 5,597 people (8.5%) who were 65 years of age or older. The median age was 25.2 years. For every 100 females there were 90.5 males. For every 100 females age 18 and over, there were 88.0 males.
There were 25,869 housing units with an average density of 2,608.0 per square mile (1,007.0/km²), of which 10,699 (43.0%) were owner-occupied, and 14,174 (57.0%) were occupied by renters. The homeowner vacancy rate was 0.9%; the rental vacancy rate was 3.5%. 27,594 people (42.0% of the population) lived in owner-occupied housing units and 35,928 people (54.7%) lived in rental housing units.
2000.
As of the United States 2000 Census, there were 60,308 people, 22,948 households, and 11,290 families residing in the city. The population density was 5,769.2 inhabitants per square mile (2,228.2/km2). There were 23,617 housing units at an average density of 2,259.3 per square mile (872.6/km2). The racial composition of the city was 70.07% White, 2.35% Black or African American, 0.67% Native American, 17.5% Asian, 0.24% Pacific Islander, 4.26% from other races, and 4.87% from two or more races. 9.61% of the population were Hispanic or Latino of any race.
There were 22,948 households of which 26.4% had children under the age of 18 living with them, 38.3% were married couples living together, 8.2% had a female householder with no husband present, and 50.8% were non-families. 25.0% of all households were composed of individuals and 5.2% had someone living alone who was 65 years of age or older. The average household size was 2.50 and the average family size was 3.00.
In the city the population age distribution was 18.6% under the age of 18, 30.9% from 18 to 24, 27.1% from 25 to 44, 16.7% from 45 to 64, and 6.6% who were 65 years of age or older. The median age was 25 years. For every 100 females there were 91.2 males. For every 100 females age 18 and over, there were 87.8 males.
The median income for a household in the city was $42,454, and the median income for a family was $74,051. Males had a median income of $51,189 versus $36,082 for females. The per capita income for the city was $22,937. About 5.4% of families and 24.5% of the population were below the poverty line, including 6.8% of those under age 18 and 2.8% of those age 65 or over.
This city of approximately 62,000 people abuts a university campus of 32,000 students. Although the university's land is not incorporated within the city, many students live off-campus in the city.
Economy.
The California Northern Railroad is based in Davis.
Top employers.
According to the City's 2011 Comprehensive Annual Financial Report, the top employers in the city are:
Bicycling.
Bicycling has been a popular mode of transportation in Davis for decades, particularly among UC Davis students. In 2010, Davis became the new home of the United States Bicycling Hall of Fame.
Bicycle infrastructure became a political issue in the 1960s, culminating in the election of a pro-bicycle majority to the City Council in 1966. By the early 1970s, Davis became a pioneer in the implementation of cycling facilities. As the city expands, new facilities are usually mandated. As a result, Davis residents today enjoy an extensive network of bike lanes, bike paths, and grade-separated bicycle crossings. The flat terrain and temperate climate are also conducive to bicycling.
In 2005 the Bicycle-Friendly Community program of the League of American Bicyclists recognized Davis as the first Platinum Level city in the US In March 2006, "Bicycling Magazine" named Davis the best small town for cycling in its compilation of "America's Best Biking Cities." Bicycling appears to be declining among Davis residents: from 1990 to 2000, the US Census Bureau reported a decline in the fraction of commuters traveling by bicycle, from 22 percent to 15 percent. This resulted in the reestablishment of the city's Bicycle Advisory Commission and creation of advocate groups such as "Davis Bicycles!".
In 1996, 2001, 2006, and 2009 the UC Davis "Cal Aggie Cycling" Team won the national road cycling competition. The team also competes off-road and on the track, and has competed in the national competitions of these disciplines. In 2007, UC Davis also organized a record breaking bicycle parade numbering 822 bicycles.
Sights and culture.
Whole Earth Festival.
A continuous stream of bands, speakers and various workshops occurs throughout Mother's Day weekend on each of Whole Earth Festival's (WEF) three stages and other specialty areas. The WEF is organized entirely by UC Davis students, in association with the Associated Students of UC Davis, the Experimental College, and the university.
Celebrate Davis.
Celebrate Davis is the annual free festival held by the Davis Chamber of Commerce. It features booths by Davis businesses, live music, food vendors, live animals, activities like rock climbing and zip-line. It concludes with fireworks after dark. Parking is problematic, so most people ride their bikes and use the free valet parking.
Picnic Day.
Picnic Day is an annual event at the University of California, Davis and is always held on the third Saturday in April. It is the largest student-run event in the US. Picnic Day starts off with a parade, which features the UC Davis California Aggie Marching Band-uh!, and runs through campus and around downtown Davis and ends with the Battle of the Bands, which lasts until the last band stops playing (sometimes until 2 am). There are over 150 free events and over 50,000 attend every year. Other highlights include: the Dachshund races, aka the Doxie Derby, held in the Pavilion; the Davis Rock Challenge, the Chemistry Magic Show, and the sheep dog trials. Many departments have exhibits and demonstrations, such as the Cole Facility, which until recently showed a fistulated cow (a cow that has been fitted with a plastic portal (a "fistula") into its digestive system to observe digestion processes). Its name was "Hole-y Cow".
Davis Transmedia Art Walk.
The Davis Transmedia Art Walk is a free, self-guided, public art tour includes 23 public murals, 16 sculptures, and 15 galleries and museums all in downtown Davis and the University of Davis campus. A free Davis Art Walk map serves as a detailed guide to the entire collection. The art pieces are all within walking distance of each other. The walk is a roughly circuitous path that can be completed within an hour or two. Every piece of art on the Art Walk has been embedded with an RFID chip. Using a cellphone that supports this technology, you access multimedia files that relate to each work. You can even leave a comment or "burn your own message" for other visitors to see. Artist hosted tours are held on the weekend by appointment only. To pick up a copy of the Davis Art Walk map, visit the Yolo County Visitors Bureau (132 E St., Suite 200; (530) 2978-1900) or the John Natsoulas Center for the Arts (521 1st St.; (530) 756-3938).
Mondavi Center.
The Mondavi Center, located on the UC Davis campus, is one of the biggest non-seasonal attractions to Davis. The Mondavi Center is a theater which hosts many world-class touring acts, including star performers such as Yo-Yo Ma and Cecilia Bartoli, and draws a large audience from Sacramento.
UC Davis Arboretum.
The UC Davis Arboretum is an arboretum and botanical garden. Plants from all over the world grow in different sections of the park. There are notable oak and native plant collections and a small redwood grove. A small waterway spans the arboretum along the bed of the old North Fork of Putah Creek. Occasionally herons, kingfishers, and cormorants can be seen around the waterways, as well as the ever present ducks. Tours of the arboretum led by volunteer naturalists are often held for grade-school children.
The Domes.
The Domes, (AKA Baggins End Innovative Housing), is an on-campus cooperative housing community designed by project manager Ron Swenson and future student-residents in 1972. Consisting of 14 polyurethane foam-insulated fiberglass domes and located in the Sustainable Research Area at the western end of Orchard Road, it is governed by its 26 UCD student residents. It is one of the only student co-housing cooperative communities in the USA, and is an early example of the modern-day growing tiny house movement.
Farmers Market.
The Davis Farmers Market is held every Wednesday evening and Saturday morning. Participants sell a range of fruits and vegetables, baked goods, dairy and meat products (often from certified organic farms), crafts, and plants and flowers. From April to October, the market hosts "Picnic in the Park", with musical events and food sold from restaurant stands.
The Davis Farmers Market won first place in the 2009, and second place in the 2010 "America's Favorite Farmers Markets" held by the American Farmland Trust under the large Farmers market classification.
Media.
Davis has one daily newspaper, "The Davis Enterprise", founded in 1897. UC Davis also has a weekly newspaper called "The California Aggie" which covers campus, local and national news. Davis Media Access, a community media center, is the umbrella organization of television station DCTV. There are also numerous commercial stations broadcasting from nearby Sacramento. Davis has two community radio stations: KDVS 90.3 FM, on the University of California campus, and KDRT 95.7 FM, a subsidiary of Davis Media Access and one of the first low-power FM radio stations in the United States. Davis has the world's largest English-language local wiki, DavisWiki.
Toad Tunnel.
Davis' Toad Tunnel is a wildlife crossing that was constructed in 1995 and has drawn much attention over the years, including a mention on "The Daily Show". Because of the building of an overpass, animal lovers worried about toads being killed by cars commuting from South Davis to North Davis, since the toads hopped from one side of a dirt lot (which the overpass replaced) to the reservoir at the other end. After much controversy, a decision was made to build a toad tunnel, which runs beneath the Pole Line Road overpass which crosses Interstate 80. The project cost $14,000. The tunnel is 21 inches (53 cm) wide and 18 inches (46 cm) high.
The tunnel has created problems of its own. The toads originally refused to use the tunnel and so the tunnel was lit to encourage its use. The toads then died from the heat of the lamps inside the tunnel. Once through the tunnel, the toads also had to contend with birds who grew wise to the toad-producing hole in the ground. The exit to the toad tunnel has been decorated by the Postmaster to resemble a toad town.
Education.
University of California.
The University of California, Davis, or UC Davis, a campus of the University of California, had an 2009 Fall enrollment of 32,153 students. UC Davis has a dominant influence on the social and cultural life of the town.
D-Q University.
Also known as Deganawidah-Quetzalcoatl University and much smaller than UC Davis, D-Q University was a two-year institution located on Road 31 in Yolo County 6.7 miles (11 km) west of State Route 113. This is just west of Davis near the Yolo County Airport. About four miles (6 km) to the west, the Road 31 exit from Interstate 505 is marked with cryptic signage, "DQU." The site is about 100 feet (30 m) above mean sea level (AMSL). NAD83 coordinates for the campus are 
The college closed in 2005. The curriculum was said to include heritage and traditional American Indian ceremonies. The and 5 buildings were formerly a military reservation according to a National Park Service publication, "Five Views." The full name of the school is included here so that readers can accurately identify the topic. According to some tribal members, use of the spelled-out name of the university can be offensive. People who want to be culturally respectful refer to the institution as "D-Q University". Tribal members in appropriate circumstances may use the full name.
Other colleges.
An off-campus branch of Sacramento City College is located in Davis.
Public schools.
Davis' public school system is administrated by the Davis Joint Unified School District.
The city has nine public elementary schools (North Davis, Birch Lane, Pioneer Elementary, Patwin, Cesar Chavez, Robert E. Willett, Marguerite Montgomery, Fred T. Korematsu at Mace Ranch, and Fairfield Elementary (which is outside the city limits but opened in 1866 and is Davis Joint Unified School District's oldest public school)). Davis has one school for independent study (Davis School for Independent Study), three public junior high schools (Ralph Waldo Emerson, Oliver Wendell Holmes, and Frances Harper), one main high school (Davis Senior High School), one alternative high school (Martin Luther King High School), and a small technology-based high school (Leonardo da Vinci High School). Cesar Chavez is a Spanish immersion school, with no English integration until the third grade. The junior high schools contain grades 7 through 9. Due to a decline in the school-age population in Davis, two of the elementary schools in south Davis may have their district boundaries changed, or magnet programs may be moved to equalize enrollment. Valley Oak was closed after the 2007–08 school year, and their campus was granted to Da Vinci High (which had formerly been located in the back of Davis Senior High's campus) and a special-ed preschool.
At one time, Chavez and Willett were incorporated together to provide elementary education K–6 to both English-speaking and Spanish immersion students in West Davis. Cesar Chavez served grades K–3 and was called West Davis Elementary, and Robert E. Willett (named for a long-time teacher at the school, now deceased) served grades 4–6 and was known as West Davis Intermediate. Willett now serves K–6 English-speaking students, and Chavez supports the Spanish immersion program for K–6.
Notable natives and residents.
These are some notable Davis residents, other than UC Davis faculty who were not previously from Davis.
Sister cities.
Davis has seven sister cities

</doc>
<doc id="9128" url="http://en.wikipedia.org/wiki?curid=9128" title="Damon Runyon">
Damon Runyon

Alfred Damon Runyon (October 4, 1880 – December 10, 1946) was an American newspaperman and author.
He was best known for his short stories celebrating the world of Broadway in New York City that grew out of the Prohibition era. To New Yorkers of his generation, a "Damon Runyon character" evoked a distinctive social type from the Brooklyn or Midtown demi-monde. The adjective "Runyonesque" refers to this type of character as well as to the type of situations and dialog that Runyon depicted. He spun humorous and sentimental tales of gamblers, hustlers, actors, and gangsters, few of whom go by "square" names, preferring instead colorful monikers such as "Nathan Detroit," "Benny Southstreet," "Big Jule," "Harry the Horse," "Good Time Charley," "Dave the Dude," or "The Seldom Seen Kid." His distinctive vernacular style is known as "Runyonese": a mixture of formal speech and colorful slang, almost always in present tense, and always devoid of contractions. He is credited with coining the phrase "Hooray Henry", a term now used in British English to describe an upper-class, loud-mouthed, arrogant twit.
Runyon's fictional world is also known to the general public through the musical "Guys and Dolls" based on two of his stories, "The Idyll of Miss Sarah Brown" and "Blood Pressure". The musical additionally borrows characters and story elements from a few other Runyon stories, most notably "Pick The Winner." The film "Little Miss Marker" (and its two remakes, "Sorrowful Jones" and the 1980 "Little Miss Marker") grew from his short story of the same name.
Runyon was also a newspaperman. He wrote the lead article for UP on Franklin Delano Roosevelt's Presidential inauguration in 1933.
Life and work.
Damon Runyon was born Alfred Damon Runyan to a family of newspapermen in Manhattan, Kansas. His grandfather was a newspaper printer from New Jersey who had relocated to Manhattan, Kansas in 1855, and his father was editor of his own newspaper in the town. In 1882 Runyon's father was forced to sell his newspaper, and the family moved westward. The family eventually settled in Pueblo, Colorado in 1887, where Runyon spent the rest of his youth. He began to work in the newspaper trade under his father in Pueblo. In present-day Pueblo, Runyon Field, the Damon Runyon Repertory Theater Company, and Runyon Lake are now named in his honor. He worked for various newspapers in the Rocky Mountain area; at one of those, the spelling of his last name was changed from "Runyan" to "Runyon," a change he let stand.
In 1898 Runyon enlisted in the U.S. Army to fight in the Spanish-American War. While in the service, he was assigned to write for the "Manila Freedom" and "Soldier's Letter".
After a notable failure in trying to organize a Colorado minor baseball league, Runyon moved to New York City in 1910. In his first New York byline, the "American" editor dropped the "Alfred" and the name "Damon Runyon" appeared for the first time. For the next ten years he covered the New York Giants and professional boxing for the "New York American".
He was the Hearst newspapers' baseball columnist for many years, beginning in 1911, and his knack for spotting the eccentric and the unusual, on the field or in the stands, is credited with revolutionizing the way baseball was covered. Perhaps as confirmation, Runyon was inducted into the writers' wing (the J. G. Taylor Spink Award) of the Baseball Hall of Fame in 1967. He is also a member of the International Boxing Hall Of Fame and is known for dubbing heavyweight champion James J. Braddock, the "Cinderella Man". Runyon frequently contributed sports poems to the "American" on boxing and baseball themes, and also wrote numerous short stories and essays. 
One year, while covering spring training in Texas, he met Pancho Villa in a bar and later accompanied the unsuccessful American expedition into Mexico searching for Villa. It was while he was in Mexico that he met the young girl whom he eventually married.
Gambling, particularly on craps or horse races, was a common theme of Runyon's works, and he was a notorious gambler himself. One of his paraphrases from a well-known line in Ecclesiastes ran: "The race is not always to the swift, nor the battle to the strong, but that's how the smart money bets."
A heavy drinker as a young man, he seems to have quit drinking soon after arriving in New York, after his drinking nearly cost him the courtship of the woman who became his first wife, Ellen Egan. He remained a heavy smoker.
His best friend was mobster accountant Otto Berman, and he incorporated Berman into several of his stories under the alias "Regret, the horse player." When Berman was killed in a hit on Berman's boss, Dutch Schultz, Runyon quickly assumed the role of damage control for his deceased friend, correcting erroneous press releases (including one that stated Berman was one of Schultz's gunmen, to which Runyon replied, "Otto would have been as effective a bodyguard as a two-year-old.").
Runyon's marriage to Ellen Egan produced two children (Mary and Damon, Jr.), but broke up in 1928 over rumors that Runyon had become infatuated with Patrice Amati del Grande, a Mexican woman he had first met while covering the Pancho Villa raids in 1916 and discovered once again in New York, when she called the "American" seeking him out. Runyon had promised her in Mexico that if she would complete the education he paid for her, he would find her a dancing job in New York. She became his companion after he separated from his wife. After Ellen Runyon died of the effects of her own drinking problems, Runyon and Patrice married; that marriage ended in 1946 when Patrice left Runyon for a younger man.
Runyon died in New York City from throat cancer in late 1946, at age 66. His body was cremated, and his ashes were illegally scattered from a DC-3 airplane over Broadway in Manhattan by Captain Eddie Rickenbacker on December 18, 1946. The family plot of Damon Runyon is located at Woodlawn Cemetery in The Bronx, New York.
Literary style.
Frank Muir comments that Runyon's plots were, in the manner of O. Henry, neatly constructed with professionally wrought endings, but their distinction lay in the manner of their telling, as the author invented a peculiar argot for his characters to speak. Runyon almost totally avoids the past tense (English humourist E.C. Bentley thought there was only one instance, and was willing to "lay plenty of 6 to 5 that it is nothing but a misprint" but "was" appears in the short stories "The Lily of St Pierre" and "A Piece of Pie"; "had" appears in "The Lily of St Pierre", "Undertaker Song" and "Bloodhounds of Broadway"), and makes little use of the future tense, using the present for both. He also avoided the conditional, using instead the future indicative in situations that would normally require conditional. An example: "Now most any doll on Broadway will be very glad indeed to have Handsome Jack Madigan give her a tumble." ("Guys and Dolls", "Social error"). E. C. Bentley comments that "there is a sort of ungrammatical purity about it [Runyon's resolute avoidance of the past tense], an almost religious exactitude." There is an homage to Runyon that makes use of this peculiarity ("Chronic Offender" by Spider Robinson) which involves a time machine.
He uses many slang terms (which go unexplained in his stories), such as:
There are many recurring composite phrases such as:
E. C. Bentley notes that Runyon's "telling use of the recurrent phrase and fixed epithet" demonstrates a debt to Homer.
Runyon's stories also employ occasional rhyming slang, similar to the cockney variety but native to New York (e.g.: "Miss Missouri Martin makes the following crack one night to her: 'Well, I do not see any Simple Simon on your lean and linger.' This is Miss Missouri Martin's way of saying she sees no diamond on Miss Billy Perry’s finger." (from "Romance in the Roaring Forties")
The comic effect of his style results partly from the juxtaposition of broad slang with mock-pomposity. Women, when not "dolls", "Judies", "pancakes", "tomatoes", or "broads", may be "characters of a female nature", for example. He typically avoided contractions such as "don't" in the example above, which also contributes significantly to the humorously pompous effect. In one sequence, a gangster tells another character to do as he's told, or else "find another world in which to live."
Runyon's short stories are told in the first person by a protagonist who is never named, and whose role is unclear; he knows many gangsters and does not appear to have a job, but he does not admit to any criminal involvement, and seems to be largely a bystander. He describes himself as "being known to one and all as a guy who is just around".
Literary works.
Stories.
There are many collections of Runyon's stories: in particular "Runyon on Broadway" and "Runyon from First to Last" between them provide extensive coverage. The latter is claimed to contain all of Runyon's stories (i.e. fiction) not included in "Runyon on Broadway". In fact, there are two Broadway stories not included in either collection: "Maybe a Queen" and "Leopard's Spots", both collected in "More Guys And Dolls" (1950).
"Runyon on Broadway" contains the following stories, all of which are Broadway stories written in Runyonese:
More Than Somewhat
Furthermore
"Runyon from First to Last" includes the following stories and sketches:
The First Stories (early non-Broadway stories):
Stories à la Carte (Broadway stories written in Runyonese):
The Last Stories (Broadway stories written in Runyonese):
Written in Sickness (sketches):
Film.
Twenty of his stories became motion pictures.
Radio.
"The Damon Runyon Theater" radio series dramatized 52 of Runyon's short stories in weekly broadcasts running from October 1948 to September 1949 (with reruns until 1951). The series was produced by Alan Ladd's Mayfair Transcription Company for syndication to local radio stations. John Brown played the character "Broadway," who doubled as host and narrator. The cast also comprised Alan Reed, Luis Van Rooten, Joseph Du Val, Gerald Mohr, Frank Lovejoy, Herb Vigran, Sheldon Leonard, William Conrad, Jeff Chandler, Lionel Stander, Sidney Miller, Olive Deering and Joe De Santis. Pat O'Brien was initially engaged for the role of "Broadway". The original stories were adapted for the radio by Russell Hughes.
"Broadway's New York had a crisis each week, though the streets had a rose-tinged aura", wrote radio historian John Dunning. "The sad shows then were all the sadder; plays like "For a Pal" had a special poignance. The bulk of Runyon's work had been untapped by radio, and the well was deep."
Television.
"Damon Runyon Theatre" aired on CBS-TV from 1955 to 1956.
Mike McShane told Runyon stories as monologues on British TV in 1994, and an accompanying book was released, both called "Broadway Stories".

</doc>
<doc id="9129" url="http://en.wikipedia.org/wiki?curid=9129" title="Don Tennant">
Don Tennant

Donald G. Tennant (November 23, 1922 – December 8, 2001) was an American advertising agency executive.
He worked at the Leo Burnett agency in Chicago, Illinois. The agency placed anthropomorphic faces of 'critters' on packaged goods. He was the first to draw Tony the Tiger for the Kellogg's Sugar Frosted Flakes advertising campaign in 1952. Tennant was also in charge of the Marlboro account and invented the Marlboro Man.

</doc>
<doc id="9130" url="http://en.wikipedia.org/wiki?curid=9130" title="Devo">
Devo

Devo (, originally ) is an American rock band formed in 1972 consisting of members from Kent and Akron, Ohio. The classic line-up of the band included two sets of brothers, the Mothersbaughs (Mark and Bob) and the Casales (Gerald and Bob), along with Alan Myers. The band had a No. 14 Billboard chart hit in 1980 with the single "Whip It", and has maintained a cult following throughout its existence. Their style over time has shifted between punk, art rock, post-punk and new wave. Their music and stage show mingle kitsch science fiction themes, deadpan surrealist humor, and mordantly satirical social commentary. Their often discordant pop songs feature unusual synthetic instrumentation and time signatures that have proven influential on subsequent popular music, particularly new wave, industrial and alternative rock artists. Devo was also a pioneer of the music video, creating many memorable clips for the LaserDisc format, with "Whip It" getting heavy airplay in the early days of MTV.
History.
Early years.
The name "Devo" comes "from their concept of 'de-evolution' — the idea that instead of continuing to evolve, mankind has actually begun to regress, as evidenced by the dysfunction and herd mentality of American society." This idea was developed as a joke by Kent State University art students Gerald Casale and Bob Lewis as early as the late 1960s. Casale and Lewis created a number of satirical art pieces in a devolution vein. At this time, Casale had also performed with the local band 15-60-75 (The Numbers Band). They met Mark Mothersbaugh around 1970, who introduced them to the pamphlet "Jocko Homo Heavenbound", which includes an illustration of a winged devil labeled "D-EVOLUTION" and would later inspire the song "Jocko Homo". However, the "joke" became serious, following the Kent State shootings of May 4, 1970. This event would be cited multiple times as the impetus for forming the band Devo.
The first form of Devo was the "Sextet Devo" which performed at the 1973 Kent State performing arts festival. It included Casale, Lewis and Mothersbaugh, as well as Gerald's brother Bob Casale on guitar, and friends Rod Reisman and Fred Weber on drums and vocals, respectively. This performance was filmed and a part was included on the home video "The Complete Truth About De-Evolution". This lineup only performed once. Devo returned to perform in the Student Governance Center (featured prominently in the film) at the 1974 Creative Arts Festival with a line-up including the Casale brothers, Bob Lewis, Mark Mothersbaugh, and Jim Mothersbaugh on drums.
Devo later formed as a quartet. They recruited Mark's brothers Bob Mothersbaugh and Jim Mothersbaugh. Bob played electric guitar, and Jim provided percussion using a set of homemade electronic drums. Their first two music videos, "Secret Agent Man" and "Jocko Homo" featured on "The Truth About De-Evolution", were filmed in Akron, and Cuyahoga Falls, Ohio, the hometown of most members. This lineup of Devo lasted until 1976 when Jim left the band. The lineup was occasionally fluid, and Bob Lewis would sometimes play guitar during this period. In concert, Devo would often perform in the guise of theatrical characters, such as Booji Boy, and The Chinaman. Live concerts from this period were often confrontational, and would remain so until 1977. A recording of an early Devo performance from 1975 with the quartet lineup appears on "", ending with the promoters unplugging Devo's equipment.
Following Jim Mothersbaugh's departure, Bob Mothersbaugh found a new drummer in Alan Myers, who played with mechanical precision on a conventional, acoustic drum set. Casale re-recruited his brother Bob Casale, and the popular line-up of Devo was formed. It would endure for nearly ten years.
1975–1980.
Devo gained some fame in 1976 when the short film "The Truth About De-Evolution" by Chuck Statler won a prize at the Ann Arbor Film Festival. This got the attention of Warner Music Group and David Bowie. In 1977 Devo were asked by Neil Young to participate in the making of his film "Human Highway". Released in 1982, the film featured the band as "Nuclear garbagepersons." The band members were asked to write their own parts and Mark Mothersbaugh scored and recorded much of the soundtrack, his first of many.
In 1976 Devo released their first single "Mongoloid" b/w "Jocko Homo", the B-side of which came from the soundtrack to "The Truth About De-Evolution", on their independent label "Booji Boy", followed in 1977 by a cover of the Rolling Stones' """(I Can't Get No) Satisfaction".
In 1978 the "Be Stiff EP"" was released by English independent label Stiff Records, which included the single "Be Stiff" plus two previous Booji Boy releases. "Mechanical Man", a 4 track 7" EP of demos, an apparent bootleg but rumored to be put out by the band themselves, was also released that year.
Devo caught the attention of David Bowie and Iggy Pop, who championed the band and enabled Devo to secure a recording contract with Warner Bros. Records. After Bowie backed out due to previous commitments, their first album, "" was produced by Brian Eno and featured re-recordings of their previous singles "Mongoloid" and "(I Can't Get No) Satisfaction". On October 14, 1978, Devo gained national exposure with an appearance on Saturday Night Live, a week after the Rolling Stones, performing "Satisfaction" and "Jocko Homo."
In 1978, co-founder Bob Lewis asked for credit and compensation for his contributions to the band. The band refused to negotiate, and sued Lewis in Los Angeles Superior Court, seeking a declaratory judgment stating Lewis had no rights to the name or theory of De-evolution. Lewis then filed an action in United States District Court for the Northern District of Ohio, alleging theft of intellectual property. During discovery, Lewis produced articles, promotional materials, documentary evidence and an interview recorded at the Akron Art Institute following the premiere of "In the Beginning was the End" in which Mothersbaugh and other band members credited Lewis with developing the theory of de-evolution, and the band quickly settled for an undisclosed sum.
The band followed up with "Duty Now for the Future" in 1979, which moved the band more towards electronic instrumentation. While not as successful as their first record, it did produce some fan favorites with the songs "Blockhead" and "The Day My Baby Gave Me a Surprize" , as well as a cover of the Johnny Rivers hit "Secret Agent Man". "Secret Agent Man" had been recorded first in 1974 for Devo's first film and performed live as early as 1976. 1979 also brought Devo to Japan for the first time, and a live show from this tour was partially recorded. Devo also appeared on Don Kirshner's Rock Concert in 1979, performing "Blockhead", "Secret Agent Man", "Uncontrollable Urge", and "Mongoloid". Also in 1979 Rhino Records --- in conjunction with LA radio station KROQ --- released "Devotees", a tribute album. It contained a set of covers of DEVO songs interspersed with renditions of popular songs in Devo's style.
Devo actively embraced the Church of the SubGenius. In concert, Devo sometimes performed as their own opening act, pretending to be a Christian soft-rock group called "Dove (the Band of Love)", which is an anagram for "Devo". They appeared as "Dove" in the 1980 televangelism spoof "Pray TV". They also recorded music, later released on the CD "E-Z Listening Disc" (1987), with Muzak style versions of their own songs to play before their concerts.
Devo gained a new level of visibility with 1980's "Freedom of Choice" which included their best-known hit, "Whip It", which quickly became a Top 40 hit. The album moved to an almost completely electronic sound, with the exception of acoustic drums and Bob 1's guitar. The tour for "Freedom of Choice" featured the band performing in front of large custom light boxes which could be laid on their back to form a second, smaller stage during the second half of the set. Other popular songs from "Freedom of Choice" were "Girl U Want," the title track (both of which had popular music videos, along with "Whip It"), and "Gates of Steel". Devo made two appearances on the TV show Fridays in 1980, as well as on Don Kirchner's Rock Concert, American Bandstand, and other shows.
1981–1986.
Devo remained popular in Australia, where the nationally broadcast 1970s–1980s pop TV show "Countdown" was one of the first programs in the world to broadcast their video clips. They were given consistent radio support by Sydney-based noncommercial rock station Double Jay (2JJ) and Brisbane-based independent community station Tripple Zed (4ZzZ), two of the first rock stations outside America to play their recordings. The late-night music program "Nightmoves" aired "The Truth About De-Evolution". This paid off, as in August 1981, they found commercial success in Australia when their Devo Live E.P. spent 3 weeks at the top of the Australian charts. In 1982, they toured Australia and appeared on the TV show "Countdown".
In 1981, Devo contributed a cover of "Working in the Coal Mine," recording during the "Freedom of Choice" sessions, to the film "Heavy Metal". "Coal Mine" would be a pack-in bonus single with their 1981 release, "New Traditionalists". This album brought a new look for Devo, who wore self-described "Utopian Boy Scout uniforms" topped with a plastic "New Traditionalist Pomp," a plastic half-wig modeled on the hairstyle of John F. Kennedy. Among the singles from the album was "Through Being Cool," written as a reaction to their newfound fame from "Whip It," an attack on their new fans that misinterpreted the song—and Devo's—message. The album's accompanying tour featured the band performing an intensely physical show with treadmills and a large Greek temple set. That same year Devo served as Toni Basil's backing band on her debut album, which included versions of three Devo songs, recorded with Basil singing lead.
"Oh, No! It's Devo" followed in 1982. Produced by Roy Thomas Baker, the album featured a darker, more sinister sound than its predecessors. According to Gerald Casale, the album's sound was inspired by reviewers calling them "fascist clowns" in articles. The album's tour featured the band performing seven songs in front of a 12-foot high rear-projection screen with synchronized video, an image recreated using blue screen effects in the album's accompanying music videos. Devo also contributed two songs, "Theme from Doctor Detroit" and "Luv-Luv" to the 1983 Dan Aykroyd film "Doctor Detroit", and produced a music video for "Theme from Doctor Detroit" featuring clips from the film with live action segments.
Devo released their sixth album, "Shout", in 1984 to mixed reviews. The album has been criticized for its overuse of the Fairlight CMI synthesizer, and weak songwriting. However, the band's cover of the Jimi Hendrix classic "'Are You Experienced?" and the accompanying music video received some praise. Following the commercial failure of "Shout", Warner Bros. dropped Devo from their label. Shortly after, claiming to feel creatively uninspired, Alan Myers left the band. This caused the band to abandon the plans for a "Shout" video LP, as well as their tour for the album. During the interim, Mark Mothersbaugh began composing music for the TV show "Pee-Wee's Playhouse", and released an elaborately packaged solo cassette, "Musik for Insomniaks," which was later expanded and released as two CDs in 1988.
1987–1994.
In 1987, Devo reformed with new drummer David Kendrick, formerly of Sparks. Their first project was a soundtrack for the flop horror film "Slaughterhouse Rock", starring Toni Basil. Devo had previously collaborated with Basil on her 1982 album Word of Mouth. The band released "Total Devo" in 1988 on Enigma Records. This album included two songs used in the "Slaughterhouse Rock" soundtrack. The song "Baby Doll" was used in the film "Tapeheads", with newly recorded Swedish lyrics, and was credited to (and shown in a music video by) a fictitious Swedish band called Cube-Squared. Devo followed this up with a world tour, and released the live album "". However, "Total Devo" was not a commercial success, and received poor critical reviews.
In 1989, members of Devo were involved in the project Visiting Kids, releasing a self-titled EP on the New Rose label in 1990. The group featured Mark's then wife Nancye Ferguson, as well as David Kendrick, Bob Mothersbaugh, and his daughter Alex Mothersbaugh. Their record was produced by Bob Casale and Mark Mothersbaugh; Mark also co-wrote some of the songs. Visiting Kids also appeared on the soundtrack to the film "Rockula", as well as on the "Late Show with David Letterman". A promotional video was filmed for the song "Trilobites".
1990 saw the release of "Smooth Noodle Maps", which would be Devo's last album for twenty years. It, too, was not a commercial success. Devo launched a European concert tour, but poor ticket sales caused it to be ended early. The band had a falling out soon after, but played two shows in 1991 before breaking up. Around this time, members of Devo appeared in the film "Spirit of '76", except for Bob Mothersbaugh. Posthumously, two albums of demo recordings from 1974 to 1977—' (1990) and ' (1991)—were released on Rykodisc, as well as an album of early live recordings, "".
Following the split, Mark Mothersbaugh started Mutato Muzika, a commercial music production studio, taking with him Bob Mothersbaugh and Bob Casale. The former works as a composer, and the latter worked as a recording engineer. David Kendrick also worked at Mutato for a period during the early 1990s. Mark has gained considerable success in writing and producing music for television programs (starting with "Pee Wee's Playhouse" and perhaps most famously with "Rugrats"), video games, cartoons, and movies (notably working alongside director Wes Anderson). Gerald Casale began a career as a director of music videos and commercials. He has worked with bands including Rush, Silverchair, and the Foo Fighters. Also, in the wake of Devo's demise, Bob Mothersbaugh attempted to start a solo career with The Bob I Band, recording an album that was never released. The tapes for this are now lost, though a bootleg of the band in concert has surfaced.
1995–2006.
In 1995, Devo reappeared with a new recording of "Girl U Want" on the soundtrack to the movie "Tank Girl". In January 1996, Devo performed a reunion concert at the Sundance Film Festival in Park City, Utah. The band performed on part of the 1996 Lollapalooza tour in the rotating Mystery Spot, with a setlist largely composed of material from their heyday between 1978 and 1982. Also in 1996, Devo also released a multimedia CD-ROM adventure game, The Adventures of the Smart Patrol with Inscape. The game was not a success, but the Lollapalooza tour was received well enough to allow Devo to return in 1997 as a headliner. Devo would perform off and on from 1997 on.
Some of their songs were used in a video game "Interstate '82" developed by Activision and released in 1999 ("Modern Life", "Faster and Faster" and "One Dumb Thing").
While they did not release any albums during this period, Devo recorded a number of songs for various films and compilations since their reunion, including a cover of the Nine Inch Nails hit, "Head Like a Hole" for the film "Supercop". In 2005 Devo recorded a new version of "Whip It" to be used in Swiffer television commercials, a decision they have said they regretted. During an interview with the Dallas Observer, Gerald Casale said, "It's just aesthetically offensive. It's got everything a commercial that turns people off has." The song "Beautiful World" was also used in a re-recorded form for an ad for Target stores. Due to rights issues with their back catalog, Devo often would use re-recorded songs for films and ads.
In 2005, Gerald Casale announced his "solo" project, Jihad Jerry & the Evildoers (the Evildoers themselves including the other members of Devo), and released the first EP, "Army Girls Gone Wild" in 2006. A full length album, "Mine Is Not a Holy War" was released on September 12, 2006 after a several-month delay. It features mostly new material, plus re-recordings of four very obscure Devo songs: "I Need A Chick" and "I Been Refused" (from "Hardcore Devo: Volume Two"), "Find Out" (which appeared on the single and EP of "Peek-A-Boo" in 1982), and "Beehive" (which was recorded by the band in 1974, at which point it was apparently abandoned with the exception of one appearance at a special show in 2001). Devo continued to tour actively in 2005 and 2006, unveiling a new stage show at shows in October 2006, and an appearance of the Jihad Jerry character performing "Beautiful World" as an encore.
Also in 2006, Devo worked on a project with Disney known as Devo 2.0. A band of child performers was assembled and re-recorded Devo songs. A quote from the Akron Beacon Journal elucidates, "...Devo recently finished a new project in cahoots with Disney called Devo 2.0, which features the band playing old songs and two new ones with vocals provided by children. Mothersbaugh doesn't rule out the idea of the band gathering in the studio, eventually, to record a new Devo album." Their debut album, a two disc CD/DVD combo entitled "DEV2.0", was released on March 14, 2006. The lyrics of some of the songs have been changed for family-friendly airplay, which has been claimed by the band to be a play on irony of sorts of the messages of their classic hits.
2007–present.
In an April 2007 interview, Gerald Casale mentioned an upcoming project for a movie about Devo's early days. A script is supposedly being developed, tentatively called "The Beginning Was the End", though the production hasn't been confirmed yet. Casale also stated that there may be some new Devo material as well, but whether this is related to the release of a movie or not is unclear. Devo played their first European tour since 1990 in the summer of 2007, including a performance at Festival Internacional de Benicàssim.
In June 2008 McDonald's released a Happy Meal toy wearing the Devo Energy dome that they named "New Wave Nigel". It was reported by AAP that a band member had initiated legal action against McDonald's as the hamburger chain had copied trademarked elements of the band's look. The following week it was reported a gag order had been placed on the band regarding further public statements on the matter. By July 2008 various blogs referred to "an e-mail" from a colleague of the band's attorney that suggested the issue was "amicably resolved".
In December 2007, Devo released their first new single since 1990, "Watch Us Work It," which was featured in a commercial for Dell. The song features a sample drum track from the "New Traditionalists" song, "The Super Thing". The band has announced in a July 23, 2007, MySpace bulletin that a full length music video for the song was forthcoming, and the song itself is available on iTunes and eMusic. Casale said that this song was chosen from a batch of songs that the band was working on, and that also this is the closest the band has been to a new album.
In a December 5, 2007 article on Mutato Muzika, LA Weekly reported that "After touring sporadically over the past decade but not releasing any new material, Devo are spending December at Mutato trying to create an album’s worth of new material and contemplating a method of dispersal in the post-record-company world." In a recent interview, Mothersbaugh revealed a song title from the in-progress album: "Don't Shoot, I'm a Man". However, in a radio interview on April 17, 2008, Jerry stated that Mark had "killed the project" and that there would be no new Devo album. Casale, however, later stated that "We're going to finish what we started."
Devo's song, "Gut Feeling/Slap Your Mammy", was featured in EA Sports' skateboard video game, "Skate". The songs "Girl U Want" and "Through Being Cool" were released as downloadable content for the video game "Rock Band" on August 19, 2008. The song "Uncontrollable Urge" is featured in the video game "Rock Band 2". All three songs have been rerecorded exclusively for "Rock Band".
Devo played dates in the United States, Japan, Australia, France, and Spain in the summer of 2008. Also in 2008 the band remixed the Attery Squash song 'Devo Was Right About Everything' which was released on the B-side to the Watch Us Work It vinyl 12" single. They also remixed a song by Datarock, "Computer Camp", which can be heard on the band's MySpace page. Datarock routinely cites Devo as an influence. 2008 also saw a Japan exclusive box set containing the band's first six albums, "This is the Devo Box". On October 17, 2008, Devo performed a special concert at the Akron Civic Theater, their first in Akron since 1978, to promote Democratic presidential candidate Barack Obama. They were joined at the concert by fellow Akron-area musicians The Black Keys and Chrissie Hynde.
In an October 2008 interview, Devo confirmed that they would be completing their new album. The Studio Notes section of the November 27 issue of "Rolling Stone" stated that "Devo are working on their first album of new material since 1990's Smooth Noodle Maps. 'We have about 17 songs we're testing out," says frontman Mark Mothersbaugh. 'We've already been contacted by 20 producers - including Snoop Dogg and Fatboy Slim.'"
Devo announced in early 2009 that they would be performing at SXSW on March 20th, with a warmup show in Dallas on March 18th. At these shows, Devo performed a new stage show utilizing synchronized video, similar to the 1982 tour, new costumes, and three new songs: "Don't Shoot, I'm a Man!", "What We Do", and "Fresh", which are tracks from their new album. All of these songs included a video backdrop which the band performed in front of. Devo also confirmed that they would be performing at All Tomorrow's Parties on May 6 and 8, with the May 6th performance featuring the band performing their first album, "", in its entirety. The May 8th performance was a "greatest hits" show, for the ATP "Fans Strike Back" event. In November 2009, Devo toured performances "Q: Are We Not Men? A: We Are Devo!" and "Freedom of Choice" with two-night stands in several cities. Along with the tour, Warner Bros. Records released remastered editions of the two albums.
In April 2009 Devo debuted the music video for "Don't Shoot (I'm a Man)" on their website, through Vimeo. In the June 2009 issue of Rolling Stone, the band noted that the album's release had been pushed back to 2010 to allow for "radical remixing". The album, "Something for Everybody" was eventually released in June 2010, preceded by a 12" single of "Fresh"/"What We Do". Earlier in the year the band had performed at one of the 2010 Vancouver Winter Olympics victory concerts at Whistler Medals Plaza and donated a collection of objects to the Ohio Historical Society including an energy dome, jumpsuit, stickers, and t-shirts.
On September 16, 2009, Warner Bros. and Devo announced a re-release of "" and "Freedom of Choice", with a tour performing both albums.
In March 2010, Devo appeared as the musical guest for an episode of the children's program "Yo Gabba Gabba!", in which they performed an altered version of "Watch Us Work It"
Devo was awarded the first-ever Moog Innovator Award on October 29, 2010 during Moogfest 2010 in Asheville, North Carolina. The Moog Innovator Award "celebrates pioneering artists whose genre-defying work exemplifies the bold, innovative spirit of Bob Moog." Devo was scheduled to perform at MoogFest, but due to Bob Mothersbaugh severely injuring his hand three days prior, the band was forced to cancel. Mark Mothersbaugh and Gerald Casale did collaborate with Austin, Texas band The Octopus Project to perform "Girl U Want" and "Beautiful World."
Devo was also scheduled to play Fun Fun Fun Fest 2010 in Austin, TX but had to cancel due to Bob's injury. The Descendents played FFF in Devo's place.
In an interview on March 3, 2011, Gerald Casale noted that he is "working on a script for a musical, a Devo musical" that would be aimed towards a live Broadway production.
On February 14, 2012, the Devo Facebook page posted a status update that clubdevo.com would soon be launching a brand new "post-Warner Brothers" website that would feature new merchandise. On March 1, a new message from General Boy was added to the website confirming that the band had indeed once again parted ways with Warner Brothers. It added that the new website would offer "new protective gear" and "unreleased material from the archives in vinyl disc format," as well as feature "WORLD-WIDE shipping to anywhere on Planet Earth."
In August 2012, the band released a single called "Don't Roof Rack Me, Bro (Seamus Unleashed)", dedicated to Republican presidential candidate Mitt Romney's former pet dog Seamus. The title relates to the Mitt Romney dog incident, which occurred in 1983 when Romney travelled twelve hours with the dog in a crate on his car's roof rack. Casale has also mentioned plans to release a collection of demos from the sessions of "Something for Everybody", with potential titles being "Devo Opens the Vault", "Gems from the Devo Dumpster," or "Something Else For Everybody."
The band toured the US and Canada in June and July 2014, playing ten dates consisting of their "experimental music" previously composed and recorded from 1974-1978. Billed as the "Hardcore Devo" tour, partial proceeds for the ten shows went to support Bob Casale's family but was planned as a 40th anniversary tour prior to his death.
Deaths.
Alan Myers died of stomach cancer in Los Angeles, California on June 24, 2013. He was 58 years old. News reports at the time of his death incorrectly cited brain cancer as the cause.
Bob Casale died on February 17, 2014 at the age of 61; according to his brother Gerald, it was a "sudden death from conditions that led to heart failure."

</doc>
<doc id="9132" url="http://en.wikipedia.org/wiki?curid=9132" title="Dale Chihuly">
Dale Chihuly

Dale Chihuly (born September 20, 1941), is an American glass sculptor and entrepreneur. His works are considered unique to the field of blown glass, "moving it into the realm of large-scale sculpture," (Museum of Fine Arts, Boston). The technical difficulties of working with glass forms are considerable, yet Chihuly uses it as the primary medium for installations and environmental artwork.
Early life.
Chihuly was born in Tacoma, Washington, to to George and Viola Magnuson Chihuly. At first Chihuly had no interest in continuing his formal education after graduating from high school in 1959. His only brother, George, had died in an accident in 1957, and his father died of a heart attack the next year at the age of 51. 
However, at his mother's urging, after he graduated from Woodrow Wilson High School, he enrolled at the College of the Puget Sound in 1959. A year later, he transferred to the University of Washington in Seattle, where in 1965 he received a bachelor of arts degree in interior design. While at the University of Washington, he was a member of Delta Kappa Epsilon fraternity (Kappa Epsilon chapter).
In the year 1967, he received a Master of Science in sculpture from the University of Wisconsin–Madison, where he studied under Harvey Littleton. In 1968, he studied glass in Venice on a Fulbright Fellowship and received a Master of Fine Arts at the Rhode Island School of Design.
Career.
In 1971, with the support of John Hauberg and Anne Gould Hauberg, Chihuly cofounded the Pilchuck Glass School near Stanwood, Washington. Chihuly also founded the HillTop Artists program in Tacoma, Washington at Jason Lee Middle School and Wilson High School. 
In 1976, while Chihuly was in England, he was involved in a head-on car accident during which he flew through the windshield. His face was severely cut by glass and he was blinded in his left eye. After recovering, he continued to blow glass until he dislocated his right shoulder in a 1979 bodysurfing accident. No longer able to hold the glass blowing pipe, he hired others to do the work. Chihuly explained the change in a 2006 interview, saying "Once I stepped back, I liked the view" and pointed out that it allowed him to see the work from more perspectives and enabled him to anticipate problems faster. Chihuly describes his role as "more choreographer than dancer, more supervisor than participant, more director than actor." "San Diego Union-Tribune" reporter Erin Glass wrote that she "wonders at the vision of not just the artist Chihuly, but the wildly successful entrepreneur Chihuly, whose estimated sales by 2004 was reported by "The Seattle Times" as $29 million." Chihuly and his team of artists were the subjects of the documentary "Chihuly Over Venice." They were also featured in the documentary "Chihuly in the Hotshop," syndicated to public television stations by American Public Television starting on November 1, 2008.
About his work.
Regina Hackett, as the "Seattle Post-Intelligencer" art critic, provided a chronology of his work during the 1970s, 1980s, and 1990s:
For his exhibition in Jerusalem in 2000, in addition to the glass pieces, he had enormous blocks of transparent ice brought in from an Alaskan artesian well and formed a wall, echoing the stones of the nearby Citadel. Lights with color gels were set up behind them for illumination. Chihuly said the melting wall represented the "dissolution of barriers" between people.
Galleries.
Chihuly's largest permanent exhibit can be found at the Oklahoma City Museum of Art. Chihuly maintains two retail stores in partnership with MGM Resorts International. One is located at the Bellagio on the Las Vegas Strip, from the Bellagio hotel/casino website the other at the MGM Grand Casino in Macau. A number of other galleries also carry his pieces. He also has a gallery in Las Vegas in the Crystals in the Las Vegas City Center in Gallery Row. In 1983 Chihuly returned to his native Pacific Northwest where he continued to develop his own work at the Pilchuck Glass School, which he had helped to found in 1971. Throughout the 1970s, influenced by the great glassblowing tradition of Murano, Chihuly experimented with the team approach to glassblowing. Working with a team of master glassblowers and assistants has enabled him to produce architectural glass art of a scale and quantity unimaginable working alone or with only one assistant. In 2010 the Space Needle Corporation submitted a proposal for an exhibition of Chihuly's work at a site in the Seattle Center, in competition with proposals for other uses from several other groups. The project, which sees the new Chihuly exhibition hall occupy the site of the former Fun Forest amusement park in the Seattle Center park and entertainment complex, received the final green light from the Seattle City Council on April 25, 2011. Called Chihuly Garden and Glass, it opened May 21, 2012.
2006 lawsuit.
In 2006, Chihuly filed a lawsuit against his former longtime employee, glassblower Bryan Rubino, and businessman Robert Kaindl, under accusations of copyright and trademark infringement. Kaindl's pieces used titles Chihuly used for his own works, such as Seaforms and Ikebana, and resembled the construction of Chihuly's pieces. Arguments made by legal experts stated influence on art style is not copyright infringement. Chihuly settled the lawsuit independently with Rubino initially, and later Kaindl as well.

</doc>
<doc id="9133" url="http://en.wikipedia.org/wiki?curid=9133" title="Dean Kamen">
Dean Kamen

Dean L. Kamen (born April 5, 1951) is an American entrepreneur and inventor from New Hampshire.
Born on Long Island, New York, he attended Worcester Polytechnic Institute, but dropped out before graduating after five years of private advanced research for the drug infusion pump "AutoSyringe". He is the son of Jack Kamen, an illustrator for "Mad", "Weird Science" and other EC Comics publications.
Career.
Inventions.
Kamen is best known for inventing the product that eventually became known as the Segway PT, an electric, self-balancing human transporter with a computer-controlled gyroscopic stabilization and control system. The device is balanced on two parallel wheels and is controlled by moving body weight. The machine's development was the object of much speculation and hype after segments of a book quoting Steve Jobs and other notable IT visionaries espousing its society-revolutionizing potential were leaked in December 2001.
Kamen has worked extensively on a project involving Stirling engine designs, attempting to create two machines; one that would generate power, and the Slingshot that would serve as a water purification system. He hopes the project will help improve living standards in developing countries. Kamen has a patent issued on his water purifier, , and other patents pending. Kamen states that his company DEKA is now working on solar power inventions.
Kamen is also the co-inventor of a compressed air device that would launch a human into the air in order to quickly launch SWAT teams or other emergency workers to the roofs of tall, inaccessible buildings.
Kamen was already a successful and wealthy inventor, after inventing the first drug infusion pump and starting a company, "AutoSyringe", to market and manufacture the pump. His company DEKA also holds patents for the technology used in portable dialysis machines, an insulin pump (based on the drug infusion pump technology), and an all-terrain electric wheelchair known as the iBOT, using many of the same gyroscopic balancing technologies that later made their way into the Segway.
Kamen and DEKA also developed the DEKA Arm System or "Luke", a prosthetic arm replacement that offers its user much more fine motor control than traditional prosthetic limbs. It was approved for use by the U.S. Food and Drug Administration (FDA) in May 2014, and DEKA is looking for partners to mass-produce the prosthesis.
FIRST.
In 1989, Kamen founded FIRST (For Inspiration and Recognition of Science and Technology), a program for students to get people interested in science, technology, and engineering. One competition started and run by FIRST is the FRC or FIRST Robotics Competition. In 2011, it held 55 regional competitions around the globe, and one international competition in St. Louis, MO. FIRST has gained a great deal of publicity from companies as well as many universities and colleges.
FIRST has many robotic programs, including the Jr.FLL (Junior FIRST Lego League) and the FLL (FIRST Lego League) for younger students, and the FTC (FIRST Tech Challenge) and the FRC (FIRST Robotics Competition) for high school aged students.
Kamen says that the FIRST competition is the invention he is most proud of, and predicts that the 1 million students who have taken part in the contests so far will be responsible for some significant technological advances in years to come.
Awards.
During his career Kamen has won numerous awards. He was elected to the National Academy of Engineering in 1997 for his biomedical devices and for making engineering more popular among high school students. In 1999 he was awarded the 5th Annual Heinz Award in Technology, the Economy and Employment, and in 2000 received the National Medal of Technology from then President Clinton for inventions that have advanced medical care worldwide. In April 2002, Kamen was awarded the Lemelson-MIT Prize for inventors, for his invention of the Segway and of an infusion pump for diabetics. In 2003 his "Project Slingshot," a cheap portable water purification system, was named a runner-up for "coolest invention of 2003" by "Time" magazine. In 2005 he was inducted into the National Inventors Hall of Fame for his invention of the AutoSyringe. In 2006 Kamen was awarded the "Global Humanitarian Action Award" by the United Nations. In 2007 he received the ASME Medal, the highest award from the American Society of Mechanical Engineers, in 2008 he was the recipient of the IRI Achievement Award from the Industrial Research Institute, and in 2011 Kamen was awarded the Benjamin Franklin Medal in Mechanical Engineering of the Franklin Institute.
Kamen received an honorary "Doctor of Engineering" degree from Rensselaer Polytechnic Institute May 17, 1996, a"Doctor of Engineering" degree from Kettering University in 2001, an honorary "Doctor of Science" degree from Clarkson University on May 13, 2001, an honorary "Doctor of Science" degree from the University of Arizona on May 16, 2009, and an honorary doctorate from the Wentworth Institute of Technology when he spoke at the college's centennial celebration in 2004, and other honorary doctorates from North Carolina State University in 2005, Bates College in 2007, the Georgia Institute of Technology in 2008, the Illinois Institute of Technology in 2008 and Plymouth State University in May 2008. Kamen received the prestigious Stevens Honor Award on November 6, 2009, given by the Stevens Institute of Technology and the Stevens Alumni Association. On November 14, 2013, he received the James C. Morgan Global Humanitarian Award.
Personal life.
His residence is a hexagonal, shed style mansion he dubbed Westwind, located in Bedford, New Hampshire, just outside of the larger city of Manchester. The house has at least four different levels and is very eclectically conceived, with such things as hallways resembling mine shafts, 1960s novelty furniture, a collection of vintage wheelchairs, spiral staircases and at least one secret passage, an observation tower, a fully equipped machine shop, and a huge cast-iron steam engine which once belonged to Henry Ford built into the center atrium of the house (which is actually small in comparison), which Kamen is working to convert into a Stirling engine-powered kinetic sculpture. Kamen owns and pilots an Embraer Phenom 300 light jet aircraft and three Enstrom helicopters, including a 280FX, a 480, and a 480B. Kamen regularly commutes to work via his helicopters and had a hangar built into his house.
He is the main subject of "Code Name Ginger: the Story Behind Segway and Dean Kamen's Quest to Invent a New World", a nonfiction narrative book by journalist Steve Kemper published by Harvard Business School Press in 2003 (in paperback as "Reinventing the Wheel").
His company, DEKA, annually creates intricate mechanical presents for him. The company has created a robotic chess player, which is a mechanical arm attached to a chess board, and a vintage-looking computer with antique wood, and a converted typewriter as a keyboard. In addition, DEKA has received funding from DARPA to work on a brain-controlled prosthetic limb called the Luke Arm.
Kamen is also a member of the USA Science and Engineering Festival's Advisory Board.
"Dean of Invention", a TV show on Planet Green starring Kamen and correspondent Joanne Colan, in which they investigate new technologies, premiered on October 22, 2010.

</doc>
<doc id="9135" url="http://en.wikipedia.org/wiki?curid=9135" title="Derivative (finance)">
Derivative (finance)

In finance, a derivative is a special type of contract that "derives" its value from the performance of an underlying entity. This underlying entity can be an asset, index, or interest rate, and is often called the "underlying". Derivatives can be used for a number of purposes - including insuring against price movements (hedging), increasing exposure to price movements for speculation or getting access to otherwise hard to trade assets or markets. 
Some of the more common derivatives include futures, forwards, swaps, options, and variations of these such as caps, floors, collars, and credit default swaps. Most derivatives are traded over-the-counter (off-exchange) or on an exchange such as the Chicago Mercantile Exchange, while most insurance contracts have developed into a separate industry. Derivatives are one of the three main categories of financial instruments, the other two being equities (i.e. stocks or shares) and debt (i.e. bonds and mortgages). 
Basics.
Derivatives are a between two parties that specify conditions (especially the dates, resulting values and definitions of the underlying variables, the parties' contractual obligations, and the notional amount) under which payments are to be made between the parties. The most common underlying assets include commodities, stocks, bonds, interest rates and currencies, but they can also be other derivatives, which adds another layer of complexity to proper valuation. The components of a firm's capital structure, e.g. bonds and stock, can also be considered derivatives, more precisely options, with the underlying being the firm's assets, but this is unusual outside of technical contexts.
There are two groups of derivative contracts: the privately traded over-the-counter (OTC) derivatives such as swaps that do not go through an exchange or other intermediary, and exchange-traded derivatives (ETD) that are traded through specialized derivatives exchanges or other exchanges.
Derivatives are more common in the modern era, but their origins trace back several centuries. One of the oldest derivatives is rice futures, which have been traded on the Dojima Rice Exchange since the eighteenth century. Derivatives are broadly categorized by the relationship between the underlying asset and the derivative (such as forward, option, swap); the type of underlying asset (such as equity derivatives, foreign exchange derivatives, interest rate derivatives, commodity derivatives, or credit derivatives); the market in which they trade (such as exchange-traded or over-the-counter); and their pay-off profile.
Derivatives may broadly be categorized as "lock" or "option" products. Lock products (such as swaps, futures, or forwards) obligate the contractual parties to the terms over the life of the contract. Option products (such as interest rate caps) provide the buyer the right, but not the obligation to enter the contract under the terms specified.
Derivatives can be used either for risk management (i.e. to "hedge" by providing offsetting compensation in case of an undesired event, a kind of "insurance") or for speculation (i.e. making a financial "bet"). This distinction is important because the former is a prudent aspect of operations and financial management for many firms across many industries; the latter offers managers and investors a risky opportunity to increase profit, which may not be properly disclosed to stakeholders.
Along with many other financial products and services, derivatives reform is an element of the Dodd–Frank Wall Street Reform and Consumer Protection Act of 2010. The Act delegated many rule-making details of regulatory oversight to the Commodity Futures Trading Commission and those details are not finalized nor fully implemented as of late 2012.
Size of market.
To give an idea of the size of the derivative market, "The Economist" magazine has reported that as of June 2011, the over-the-counter (OTC) derivatives market amounted to approximately $700 trillion, and the size of the market traded on exchanges totaled an additional $83 trillion. However, these are "notional" values, and some economists say that this value greatly exaggerates the market value and the true credit risk faced by the parties involved. For example, in 2010, while the aggregate of OTC derivatives exceeded $600 trillion, the value of the market was estimated much lower, at $21 trillion. The credit risk equivalent of the derivative contracts was estimated at $3.3 trillion.
Still, even these scaled down figures represent huge amounts of money. For perspective, the budget for total expenditure of the United States Government during 2012 was $3.5 trillion, and the total current value of the US stock market is an estimated $23 trillion. The world annual Gross Domestic Product is about $65 trillion.
And for one type of derivative at least, Credit Default Swaps (CDS), for which the inherent risk is considered high, the higher, nominal value, remains relevant. It was this type of derivative that investment magnate Warren Buffet referred to in his famous 2002 speech in which he warned against "weapons of financial mass destruction." CDS notional value in early 2012 amounted to $25.5 trillion, down from $55 trillion in 2008.
Usage.
Derivatives are used for the following:
Mechanics and Valuation Basics.
Lock products are theoretically valued at zero at the time of execution and thus do not typically require an up-front exchange between the parties. Based upon movements in the underlying asset over time, however, the value of the contract will fluctuate, and the derivative may be either an asset (i.e. "in the money") or a liability (i.e. "out of the money") at different points throughout its life. Importantly, either party is therefore exposed to the credit quality of its counterparty and is interested in protecting itself in an event of default.
Option products have immediate value at the outset because they provide specified protection (intrinsic value) over a given time period (time value). One common form of option product familiar to many consumers is insurance for homes and automobiles. The insured would pay more for a policy with greater liability protections (intrinsic value) and one that extends for a year rather than six months (time value). Because of the immediate option value, the option purchaser typically pays an up front premium. Just like for lock products, movements in the underlying asset will cause the option's intrinsic value to change over time while its time value deteriorates steadily until the contract expires. An important difference between a lock product is that, after the initial exchange, the option purchaser has no further liability to its counterparty; upon maturity, the purchaser will execute the option if it has positive value (i.e. if it is "in the money") or expire at no cost (other than to the initial premium) (i.e. if the option is "out of the money").
Hedging.
Derivatives allow risk related to the price of the underlying asset to be transferred from one party to another. For example, a wheat farmer and a miller could sign a futures contract to exchange a specified amount of cash for a specified amount of wheat in the future. Both parties have reduced a future risk: for the wheat farmer, the uncertainty of the price, and for the miller, the availability of wheat. However, there is still the risk that no wheat will be available because of events unspecified by the contract, such as the weather, or that one party will renege on the contract. Although a third party, called a clearing house, insures a futures contract, not all derivatives are insured against counter-party risk.
From another perspective, the farmer and the miller both reduce a risk and acquire a risk when they sign the futures contract: the farmer reduces the risk that the price of wheat will fall below the price specified in the contract and acquires the risk that the price of wheat will rise above the price specified in the contract (thereby losing additional income that he could have earned). The miller, on the other hand, acquires the risk that the price of wheat will fall below the price specified in the contract (thereby paying more in the future than he otherwise would have) and reduces the risk that the price of wheat will rise above the price specified in the contract. In this sense, one party is the insurer (risk taker) for one type of risk, and the counter-party is the insurer (risk taker) for another type of risk.
Hedging also occurs when an individual or institution buys an asset (such as a commodity, a bond that has coupon payments, a stock that pays dividends, and so on) and sells it using a futures contract. The individual or institution has access to the asset for a specified amount of time, and can then sell it in the future at a specified price according to the futures contract. Of course, this allows the individual or institution the benefit of holding the asset, while reducing the risk that the future selling price will deviate unexpectedly from the market's current assessment of the future value of the asset.
Derivatives trading of this kind may serve the financial interests of certain particular businesses. For example, a corporation borrows a large sum of money at a specific interest rate. The interest rate on the loan reprices every six months. The corporation is concerned that the rate of interest may be much higher in six months. The corporation could buy a forward rate agreement (FRA), which is a contract to pay a fixed rate of interest six months after purchases on a notional amount of money. If the interest rate after six months is above the contract rate, the seller will pay the difference to the corporation, or FRA buyer. If the rate is lower, the corporation will pay the difference to the seller. The purchase of the FRA serves to reduce the uncertainty concerning the rate increase and stabilize earnings.
Speculation and arbitrage.
Derivatives can be used to acquire risk, rather than to hedge against risk. Thus, some individuals and institutions will enter into a derivative contract to speculate on the value of the underlying asset, betting that the party seeking insurance will be wrong about the future value of the underlying asset. Speculators look to buy an asset in the future at a low price according to a derivative contract when the future market price is high, or to sell an asset in the future at a high price according to a derivative contract when the future market price is less.
Individuals and institutions may also look for arbitrage opportunities, as when the current buying price of an asset falls below the price specified in a futures contract to sell the asset.
Speculative trading in derivatives gained a great deal of notoriety in 1995 when Nick Leeson, a trader at Barings Bank, made poor and unauthorized investments in futures contracts. Through a combination of poor judgment, lack of oversight by the bank's management and regulators, and unfortunate events like the Kobe earthquake, Leeson incurred a US$1.3 billion loss that bankrupted the centuries-old institution.
Proportion Used for Hedging and Speculation.
The true proportion of derivatives contracts used for hedging purposes is unknown (and perhaps unknowable), but it appears to be relatively small. Also, derivatives contracts account for only 3–6% of the median firms' total currency and interest rate exposure. Nonetheless, we know that many firms' derivatives activities have at least some speculative component for a variety of reasons.
Types.
OTC and exchange-traded.
In broad terms, there are two groups of derivative contracts, which are distinguished by the way they are traded in the market:
According to the Bank for International Settlements, who first surveyed OTC derivatives in 1995, reported that the "gross market value, which represent the cost of replacing all open contracts at the prevailing market prices, ... increased by 74% since 2004, to $11 trillion at the end of June 2007 (BIS 2007:24)." Positions in the OTC derivatives market increased to $516 trillion at the end of June 2007, 135% higher than the level recorded in 2004. the total outstanding notional amount is US$708 trillion (as of June 2011). Of this total notional amount, 67% are interest rate contracts, 8% are credit default swaps (CDS), 9% are foreign exchange contracts, 2% are commodity contracts, 1% are equity contracts, and 12% are other. Because OTC derivatives are not traded on an exchange, there is no central counter-party. Therefore, they are subject to counterparty risk, like an ordinary contract, since each counter-party relies on the other to perform.
Common derivative contract types.
Some of the common variants of derivative contracts are as follows:
Some common examples of these derivatives are the following:
Economic function of the derivative market.
Some of the salient economic functions of the derivative market include:
In a nutshell, there is a substantial increase in savings and investment in the long run due to augmented activities by derivative Market participant.
Valuation.
Market and arbitrage-free prices.
Two common measures of value are:
Determining the market price.
For exchange-traded derivatives, market price is usually transparent (often published in real time by the exchange, based on all the current bids and offers placed on that particular contract at any one time). Complications can arise with OTC or floor-traded contracts though, as trading is handled manually, making it difficult to automatically broadcast prices. In particular with OTC contracts, there is no central exchange to collate and disseminate prices.
Determining the arbitrage-free price.
The arbitrage-free price for a derivatives contract can be complex, and there are many different variables to consider. Arbitrage-free pricing is a central topic of financial mathematics. For futures/forwards the arbitrage free price is relatively straightforward, involving the price of the underlying together with the cost of carry (income received less interest costs), although there can be complexities.
However, for options and more complex derivatives, pricing involves developing a complex pricing model: understanding the stochastic process of the price of the underlying asset is often crucial. A key equation for the theoretical valuation of options is the Black–Scholes formula, which is based on the assumption that the cash flows from a European stock option can be replicated by a continuous buying and selling strategy using only the stock. A simplified version of this valuation technique is the binomial options model.
OTC represents the biggest challenge in using models to price derivatives. Since these contracts are not publicly traded, no market price is available to validate the theoretical valuation. Most of the model's results are input-dependent (meaning the final price depends heavily on how we derive the pricing inputs).
Therefore it is common that OTC derivatives are priced by Independent Agents that both counterparties involved in the deal designate upfront (when signing the contract).
Criticisms.
Derivatives are often subject to the following criticisms:
Hidden tail risk.
According to Raghuram Rajan, a former chief economist of the International Monetary Fund (IMF), "... it may well be that the managers of these firms [investment funds] have figured out the correlations between the various instruments they hold and believe they are hedged. Yet as Chan and others (2005) point out, the lessons of summer 1998 following the default on Russian government debt is that correlations that are zero or negative in normal times can turn overnight to one — a phenomenon they term "phase lock-in." A hedged position can become unhedged at the worst times, inflicting substantial losses on those who mistakenly believe they are protected."
Risks.
The use of derivatives can result in large losses because of the use of leverage, or borrowing. Derivatives allow investors to earn large returns from small movements in the underlying asset's price. However, investors could lose large amounts if the price of the underlying moves against them significantly. There have been several instances of massive losses in derivative markets, such as the following:
This comes to a staggering $39.5 billion, the majority in the last decade after the Commodity Futures Modernization Act of 2000 was passed.
Counter party risk.
Some derivatives (especially swaps) expose investors to counterparty risk, or risk arising from the other party in a financial transaction. Different types of derivatives have different levels of counter party risk. For example, standardized stock options by law require the party at risk to have a certain amount deposited with the exchange, showing that they can pay for any losses; banks that help businesses swap variable for fixed rates on loans may do credit checks on both parties. However, in private agreements between two companies, for example, there may not be benchmarks for performing due diligence and risk analysis.
Large notional value.
Derivatives typically have a large notional value. As such, there is the danger that their use could result in losses for which the investor would be unable to compensate. The possibility that this could lead to a chain reaction ensuing in an economic crisis was pointed out by famed investor Warren Buffett in Berkshire Hathaway's 2002 annual report. Buffett called them 'financial weapons of mass destruction.' A potential problem with derivatives is that they comprise an increasingly larger notional amount of assets which may lead to distortions in the underlying capital and equities markets themselves. Investors begin to look at the derivatives markets to make a decision to buy or sell securities and so what was originally meant to be a market to transfer risk now becomes a leading indicator.
Financial Reform and Government Regulation.
Under US law and the laws of most other developed countries, derivatives have special legal exemptions that make them a particularly attractive legal form to extend credit. The strong creditor protections afforded to derivatives counterparties, in combination with their complexity and lack of transparency however, can cause capital markets to underprice credit risk. This can contribute to credit booms, and increase systemic risks. Indeed, the use of derivatives to conceal credit risk from third parties while protecting derivative counterparties contributed to the financial crisis of 2008 in the United States.
In the context of a 2010 examination of the ICE Trust, an industry self-regulatory body, Gary Gensler, the chairman of the Commodity Futures Trading Commission which regulates most derivatives, was quoted saying that the derivatives marketplace as it functions now "adds up to higher costs to all Americans." More oversight of the banks in this market is needed, he also said. Additionally, the report said, "[t]he Department of Justice is looking into derivatives, too. The department's antitrust unit is actively investigating 'the possibility of anticompetitive practices in the credit derivatives clearing, trading and information services industries,' according to a department spokeswoman."
For legislators and committees responsible for financial reform related to derivatives in the United States and elsewhere, distinguishing between hedging and speculative derivatives activities has been a nontrivial challenge. The distinction is critical because regulation should help to isolate and curtail speculation with derivatives, especially for "systemically significant" institutions whose default could be large enough to threaten the entire financial system. At the same time, the legislation should allow for responsible parties to hedge risk without unduly tying up working capital as collateral that firms may better employ elsewhere in their operations and investment. 
In this regard, it is important to distinguish between financial (e.g. banks) and non-financial end-users of derivatives (e.g. real estate development companies) because these firms' derivatives usage is inherently different. More importantly, the reasonable collateral that secures these different counterparties can be very different. The distinction between these firms is not always straight forward (e.g. hedge funds or even some private equity firms do not neatly fit either category). Finally, even financial users must be differentiated, as 'large' banks may classified as "systemically significant" whose derivatives activities must be more tightly monitored and restricted than those of smaller, local and regional banks.
Over-the-counter dealing will be less common as the Dodd–Frank Wall Street Reform and Consumer Protection Act comes into effect. The law mandated the clearing of certain swaps at registered exchanges and imposed various restrictions on derivatives. To implement Dodd-Frank, the . The Commission determines which swaps are subject to mandatory clearing and whether a derivatives exchange is eligible to clear a certain type of swap contract.
Nonetheless, the above and other challenges of the rule-making process have delayed full enactment of aspects of the legislation relating to derivatives. The challenges are further complicated by the necessity to orchestrate globalized financial reform among the nations that comprise the world's major financial markets, a primary responsibility of the Financial Stability Board whose progress is ongoing.
In the U.S., by February 2012 the combined effort of the SEC and CFTC had produced over 70 proposed and final derivatives rules. However, both of them had delayed adoption of a number of derivatives regulations because of the burden of other rulemaking, litigation and opposition to the rules, and many core definitions (such as the terms "swap," "security-based swap," "swap dealer," "security-based swap dealer," "major swap participant" and "major security-based swap participant") had still not been adopted. SEC Chairman Mary Schapiro opined: "At the end of the day, it probably does not make sense to harmonize everything [between the SEC and CFTC rules] because some of these products are quite different and certainly the market structures are quite different."
In November 2012, the SEC and regulators from Australia, Brazil, the European Union, Hong Kong, Japan, Ontario, Quebec, Singapore, and Switzerland met to discuss reforming the OTC derivatives market, as had been agreed by leaders at the 2009 G-20 Pittsburgh summit in September 2009. In December 2012, they released a joint statement to the effect that they recognized that the market is a global one and "firmly support the adoption and enforcement of robust and consistent standards in and across jurisdictions", with the goals of mitigating risk, improving transparency, protecting against market abuse, preventing regulatory gaps, reducing the potential for arbitrage opportunities, and fostering a level playing field for market participants. They also agreed on the need to reduce regulatory uncertainty and provide market participants with sufficient clarity on laws and regulations by avoiding, to the extent possible, the application of conflicting rules to the same entities and transactions, and minimizing the application of inconsistent and duplicative rules. At the same time, they noted that "complete harmonization – perfect alignment of rules across jurisdictions" would be difficult, because of jurisdictions' differences in law, policy, markets, implementation timing, and legislative and regulatory processes.
On December 20, 2013 the CFTC provided information on its swaps regulation "comparability" determinations. The release addressed the CFTC's cross-border compliance exceptions. Specifically it addressed which entity level and in some cases transaction-level requirements in six jurisdictions (Australia, Canada, the European Union, Hong Kong, Japan, and Switzerland) it found comparable to its own rules, thus permitting non-US swap dealers, major swap participants, and the foreign branches of US Swap Dealers and major swap participants in these jurisdictions to comply with local rules in lieu of Commission rules.
Reporting.
Mandatory reporting regulations are being finalized in a number of countries, such as Dodd Frank Act in the US, the European Market Infrastructure Regulations (EMIR) in Europe, as well as regulations in Hong Kong, Japan, Singapore, Canada, and other countries. The OTC Derivatives Regulators Forum (ODRF), a group of over 40 world-wide regulators, provided trade repositories with a set of guidelines regarding data access to regulators, and the Financial Stability Board and CPSS IOSCO also made recommendations in with regard to reporting.
DTCC, through its "Global Trade Repository" (GTR) service, manages global trade repositories for interest rates, and commodities, foreign exchange, credit, and equity derivatives. It makes global trade reports to the CFTC in the U.S., and plans to do the same for ESMA in Europe and for regulators in Hong Kong, Japan, and Singapore. It covers cleared and uncleared OTC derivatives products, whether or not a trade is electronically processed or bespoke.

</doc>
<doc id="9136" url="http://en.wikipedia.org/wiki?curid=9136" title="Disney (disambiguation)">
Disney (disambiguation)

Disney usually refers to The Walt Disney Company.
The term may also refer to:

</doc>
<doc id="9137" url="http://en.wikipedia.org/wiki?curid=9137" title="Divine right of kings">
Divine right of kings

The divine right of kings, or divine-right theory of kingship, is a political and religious doctrine of royal and political legitimacy. It asserts that a monarch is subject to no earthly authority, deriving the right to rule directly from the will of God. The king is thus not subject to the will of his people, the aristocracy, or any other estate of the realm, including (in the view of some, especially in Protestant countries) the Church. According to this doctrine, only God can judge an unjust king. The doctrine implies that any attempt to depose the king or to restrict his powers runs contrary to the will of God and may constitute a sacrilegious act. It is often expressed in the phrase "by the Grace of God," attached to the titles of a reigning monarch.
Origins.
The remote origins of the theory are rooted in the medieval idea that God had bestowed earthly power on the king, just as God had given spiritual power and authority to the church, centering on the pope. The immediate author of the theory was Jean Bodin, who based it on the interpretation of Roman law. With the rise of nation-states and the Protestant Reformation, the theory of divine right justified the king's absolute authority in both political and spiritual matters. The theory came to the fore in England under the reign of James I of England (1603–1625, also known as James VI of Scotland 1567–1625). Louis XIV of France (1643–1715) strongly promoted the theory as well.
Decline.
The theory of divine right was abandoned in England during the Glorious Revolution of 1688–89. The American and French Revolutions of the late eighteenth century further weakened the theory's appeal, and by the early twentieth century, it had been virtually abandoned.
Scots texts of James VI of Scotland.
The Scots textbooks of the divine right of kings were written in 1597-98 by James VI of Scotland before his accession to the English throne. His "Basilikon Doron," a manual on the powers of a king, was written to edify his four-year-old son Henry Frederick that a king "acknowledgeth himself ordained for his people, having received from the god a burden of government, whereof he must be countable."
Western conceptions.
The conception of ordination brought with it largely unspoken parallels with the Anglican and Catholic priesthood, but the overriding metaphor in James's handbook was that of a father's relation to his children. "Just as no misconduct on the part of a father can free his children from obedience to the fifth commandment", James also had printed his "Defense of the Right of Kings" in the face of English theories of inalienable popular and clerical rights. The divine right of kings, or divine-right theory of kingship, is a political and religious doctrine of royal and political legitimacy. It asserts that a monarch is subject to no earthly authority, deriving his right to rule directly from the will of God. The king is thus not subject to the will of his people, the aristocracy, or any other estate of the realm, including (in the view of some, especially in Protestant countries) the church. A weaker or more moderate form of this political theory does hold, however, that the king is subject to the church and the pope, although completely irreproachable in other ways; but according to this doctrine in its strong form, only God can judge an unjust king. The doctrine implies that any attempt to depose the king or to restrict his powers runs contrary to the will of God and may constitute a sacrilegious act.
One passage in scripture supporting the idea of divine right of kings was . Martin Luther, when urging the secular authorities to crush the Peasant Rebellion of 1525 in Germany in his "Against the Murderous, Thieving Hordes of Peasants", based his argument on St. Paul's Epistle to the .
It is related to the ancient (but not current) Catholic philosophies regarding monarchy, in which the monarch is God's vicegerent upon the earth and therefore subject to no inferior power. However, in Roman Catholic jurisprudence, the monarch is always subject to natural and divine law, which are regarded as superior to the monarch. The possibility of monarchy declining morally, overturning natural law, and degenerating into a tyranny oppressive of the general welfare was answered theologically with the Catholic concept of extra-legal tyrannicide, ideally ratified by the pope. Until the unification of Italy, the Holy See did, from the time Christianity became the Roman state religion, assert on that ground its primacy over secular princes; however this exercise of power never, even at its zenith, amounted to theocracy, even in jurisdictions where the Bishop of Rome was the temporal authority.
Catholic justified submission.
Catholic thought justified submission to the monarchy by reference to the following:
The French Huguenot nobles and clergy, having rejected the pope and the Catholic Church, were left only with the supreme power of the king who, they taught, could not be gainsaid or judged by anyone. Since there was no longer the countervailing power of the papacy and since the Church of England was a creature of the state and had become subservient to it, this meant that there was nothing to regulate the powers of the king, and he became an absolute power. In theory, divine, natural, customary, and constitutional law still held sway over the king, but, absent a superior spiritual power, it was difficult to see how they could be enforced, since the king could not be tried by any of his own courts.
Some of the symbolism within the coronation ceremony for British monarchs, in which they are anointed with holy oils by the Archbishop of Canterbury, thereby "ordaining" them to monarchy, perpetuates the ancient Roman Catholic monarchical ideas and ceremonial (although few Protestants realize this, the ceremony is nearly entirely based upon that of the Coronation of the Holy Roman Emperor). However, in the UK, the symbolism ends there, since the real governing authority of the monarch was all but extinguished by the Whig revolution of 1688-89 (see Glorious Revolution). The king or queen of the United Kingdom is one of the last monarchs still to be crowned in the traditional Christian ceremonial, which in most other countries has been replaced by an inauguration or other declaration.
The concept of divine right incorporates, but exaggerates, the ancient Christian concept of "royal God-given rights", which teach that "the right to rule is anointed by God", although this idea is found in many other cultures, including Aryan and Egyptian traditions. In pagan religions, the king was often seen as a kind of god and so was an unchallengeable despot. The ancient Roman Catholic tradition overcame this idea with the doctrine of the "Two Swords" and so achieved, for the very first time, a balanced constitution for states. The advent of Protestantism saw something of a return to the idea of a mere unchallengeable despot.
Thomas Aquinas condoned extra-legal tyrannicide in the worst of circumstances:
On the other hand, Aquinas forbade the overthrow of any morally, Christianly and spiritually legitimate king by his subjects. The only human power capable of deposing the king was the pope. The reasoning was that if a subject may overthrow his superior for some bad law, who was to be the judge of whether the law was bad? If the subject could so judge his own superior, then all lawful superior authority could lawfully be overthrown by the arbitrary judgement of an inferior, and thus all law was under constant threat. Towards the end of the Middle Ages, many philosophers, such as Nicholas of Cusa and Francisco Suarez, propounded similar theories. The Church was the final guarantor that Christian kings would follow the laws and constitutional traditions of their ancestors and the laws of the presumptive god and of justice. Similarly, the Chinese concept of Mandate of Heaven required that the emperor properly carry out the proper rituals, consult his ministers, and made it extremely difficult to undo any acts carried out by an ancestor.
The French prelate Bossuet made a classic statement of the doctrine of divine right in a sermon preached before King Louis XIV:
Divine right in Asia.
In early Mesopotamian culture, kings were often regarded as deities after their death. Shulgi of Ur was among the first Mesopotamian rulers to declare himself to be divine. This was the direct precursor to the concept of "Divine Right of kings", as well as in the Egyptian and Roman religions. 
Mandate of Heaven.
In China and East Asia, rulers justified their rule with the philosophy of the Mandate of Heaven, which, although similar to the European concept, bore several key differences. While the divine right of kings granted unconditional legitimacy, the Mandate of Heaven was dependent on the behaviour of the ruler, the Son of Heaven. Heaven would bless the authority of a just ruler, but it could be displeased with a despotic ruler and thus withdraw its mandate, transferring it to a more suitable and righteous person. This withdrawal of mandate also afforded the possibility of revolution as a means to remove the errant ruler; revolt was never legitimate under the European framework of divine right. 
In China, the right of rebellion against an unjust ruler had been a part of the political philosophy ever since the Zhou dynasty, whose rulers had used this philosophy to justify their overthrow of the previous Shang dynasty. Chinese historians interpreted a successful revolt as evidence that the Mandate of Heaven had passed on to the usurper.
In Japan, the Son of Heaven title was less conditional than its Chinese equivalent. There was no divine mandate that punished the emperor for failing to rule justly. The right to rule of the Japanese emperor, descended from the sun goddess Amaterasu, was absolute. The Japanese emperors traditionally wielded little secular power; generally, it was the duty of the sitting emperor to perform rituals and make public appearances, while true power was held by regents, high-ranking ministers or even retired emperors depending in the time period.
Khans of Tarkir.
The medieval Mongols and other steppe people like Turks elected their rulers, then afterwards judged Heaven to have strengthened their rule if they succeeded in war.
Sultans in Southeast Asia.
In the Malay Annals, the rajas and sultans of the Malay States (today Malaysia, Brunei and Philippines) as well as their predecessors, such as the Indonesian kingdom of Majapahit, also claimed divine right to rule. The sultan is mandated by God, and thus is expected to lead his country and people in religious matters, ceremonies as well as prayers. This divine right is called "Daulat", and although the notion of divine right is somewhat obsolete, it is still found in the phrase "Daulat Tuanku" that is used to publicly acclaim the reigning Yang di-Pertuan Agong and the other sultans of Malaysia. The exclamation is similar to the European "Long live the King", and often accompanies pictures of the reigning monarch and his consort on banners during royal occasions. In Indonesia, especially on the island of Java, the sultan's divine right is more commonly known as the "wahyu", or 'revelation', but it is not hereditary, and can be passed on to distant relatives.
South Asian kings.
In Tamil culture, before Brahmanism and especially during the Cankam period, emperors were known as இறையர் ("Iraiyer"), or "those who spill", and kings were called கோ ("Ko") or கோன் ("Kon"). During this time, the distinction between kingship and godhood had not yet occurred, as the caste system had not yet been introduced. Even in Modern Tamil, the word for temple is 'கோயில்', meaning "king's house". Kings were understood to be the "agents of God", as they protected the world like God did. This may well have been continued post-Brahminism in Tamilakam, as the famous Thiruvalangadu inscription states:
Opposition.
In the sixteenth century, both Catholic and Protestant political thinkers began to question the
idea of a monarch's "divine right".
The Spanish Catholic historian Juan de Mariana put forward the argument in his book
"De rege et regis institutione" (1598) that since society was formed by a "pact" among all its members,
"there can be no doubt that they are able to call a king to account". Mariana thus challenged divine right theories by stating in certain circumstances, tyrannicide could be justified. Cardinal Robert Bellarmine also "did not believe that
the institute of monarchy had any divine sanction" and shared Mariana's belief that there were times where Catholics
could lawfully remove a monarch.
Among groups of English Protestant exiles fleeing from Queen Mary I, some of the earliest anti-monarchist publications emerged. "Weaned off uncritical royalism by the actions of Queen Mary… The political thinking of men like Ponet, Knox, Goodman and Hales."
In 1553, Mary I, a Roman Catholic, succeeded her Protestant half-brother, Edward VI, to the English throne. Mary set about trying to restore Roman Catholicism by making sure that: Edward's religious laws were abolished in the Statute of Repeal Act (1553); the Protestant religious laws passed in the time of Henry VIII were repealed; and the Revival of the Heresy Acts were passed in 1554. The Marian Persecutions began soon afterwards. In January 1555, the first of nearly 300 Protestants were burnt at the stake under "Bloody Mary". When Thomas Wyatt the younger instigated what became known as Wyatt's rebellion, John Ponet, the highest-ranking ecclesiastic among the exiles, allegedly participated in the uprising. He escaped to Strasbourg after the Rebellion's defeat and, the following year, he published "A Shorte Treatise of Politike Power", in which he put forward a theory of justified opposition to secular rulers.
"Ponet's treatise comes first in a new wave of anti-monarchical writings… It has never been assessed at its true importance, for it antedates by several years those more brilliantly expressed but less radical Huguenot writings which have usually been taken to represent the Tyrannicide-theories of the Reformation".
Ponet's pamphlet was republished on the eve of King Charles I's execution.
According to U.S. President John Adams, Ponet's work contained "all the essential principles of liberty, which were afterward dilated on by Sidney and Locke", including the idea of a three-branched government.
In due course, opposition to the divine right of kings came from a number of sources, including poet John Milton in his pamphlet "The Tenure of Kings and Magistrates".
Probably the two most famous declarations of a right to revolution against tyranny in the English language are John Locke's "Essay concerning The True Original, Extent, and End of Civil-Government" and Thomas Jefferson's formulation in the United States Declaration of Independence that "all men are created equal".

</doc>
<doc id="9138" url="http://en.wikipedia.org/wiki?curid=9138" title="Davros">
Davros

Davros is a character from the long-running British science fiction television series "Doctor Who". Davros is an archenemy of the Doctor and is the creator of the Doctor's deadliest enemies, the Daleks. Davros was created by screenwriter Terry Nation.
Davros is a scientist from the planet Skaro whose people, the Kaleds, were engaged in a bitter thousand-year war of attrition with their enemies, the Thals. He is horribly scarred and crippled, a condition that various spin-off media attribute to his laboratory being attacked by a Thal shell. He has one functioning hand and one cybernetic eye mounted on his forehead to take the place of his real eyes, which appear to have been fused shut; for much of his existence he depends completely upon a self-designed mobile life-support chair which encloses the lower half of his body. It would become an obvious inspiration for his eventual design of the Dalek. The condition of the lower half of his body is unknown; he is physically incapable of leaving the chair without dying. Davros's voice, like those of the Daleks, is electronically distorted. His manner of speech is generally soft and contemplative, but when angered or excited he is prone to ranting outbursts that resemble the hysterical, staccatissimo speech of the Daleks.
Davros is a megalomaniac who believes that through his creations, the Daleks, he can become the supreme being and ruler of the universe. He is a brilliant, ruthless scientist who has demonstrated mastery of robotics, mechatronics, metallurgy, chemistry, artificial intelligence, cloning, genetic engineering, biology, physics, astronomy, earth science, military tactics, cybernetics and mechanical engineering.
Character history.
The Kaled/Thal Conflict.
Davros first appeared in the 1975 serial "Genesis of the Daleks", written by Terry Nation. Nation, creator of the Dalek concept, had deliberately modelled elements of the Daleks' character on Nazi ideology, and conceived of their creator as a scientist with strong fascist tendencies. The physical appearance of Davros was developed by visual effects designer Peter Day and sculptor John Friedlander, who based Davros' chair on the lower half of a Dalek. Producer Philip Hinchcliffe told Friedlander to consider a design similar to The Mekon from the "Eagle" comic "Dan Dare", with a large dome-like head and a withered body.
Cast in the role of Davros was Michael Wisher, who had previously appeared in several different roles on "Doctor Who" and had provided Dalek voices in the serials "Frontier in Space", "Planet of the Daleks" and "Death to the Daleks". Wisher based his performance as Davros on the philosopher Bertrand Russell. In order to prepare for filming under the heavy mask, Wisher rehearsed wearing a paper bag over his head. Friedlander's mask was cast in hard latex, with only the mouth revealing Wisher's features; make-up artist Sylvia James shaded the mask's tones and blackened Wisher's lips and teeth to hide the transition.
When he first encounters the Fourth Doctor in "Genesis of the Daleks", Davros is the chief scientist of the Kaleds, one of two native sentient races of the planet Skaro. The Kaleds are locked in a bitter thousand-year war of attrition for supremacy of their home-world with a species called the Thals. Davros is the head of the Elite Scientific Division, created to devise new military strategies in order to win the war. Davros realises that contamination from the nuclear and biological weapons used in the war is mutating the Kaled race, and artificially accelerates the process to examine the ultimate evolutionary end product. The mutations are weak and crippled, no more than one-eyed brains with tentacular appendages and with no hope of survival on their own. His solution is to remove all emotions pertaining to weakness (a category in which he groups such emotions as compassion, mercy and kindness) and place the mutants in tank-like "Mark III travel machines" partly based on the design of his wheelchair. He later names these creatures Daleks, an anagram of Kaleds.
Davros quickly becomes obsessed with his creations, considering them to be the ultimate form of life, superior to all others. To stop his own people from shutting down his Dalek project, he arranges for them to be wiped out by the Thals, then blames their destruction on a scientist that opposed him, having the Daleks kill them. The Daleks then exterminate almost all the Thal victors. Davros uses a trick to wipe out those who oppose him, by ordering a vote to decide whether to continue with the project, when the Daleks arrive they kill those who voted against Davros, but ultimately turn on Davros and those with him and apparently kill him at the conclusion of the serial when he tries to halt the Dalek production line.
War with the Movellans.
Davros proved too effective a character to be kept dead and was resurrected four years later in 1979's "Destiny of the Daleks" (played by David Gooderson using the mask Friedlander made for Wisher – the mask had to be split into sections and rejoined to get as good a fit as possible). The Daleks unearth their creator – who had apparently been in suspended animation since his "death" in "Genesis" — to help them break a logical impasse in their war against the android Movellans. However, the Dalek force is destroyed by the Doctor, and Davros is captured and imprisoned by the humans in suspended animation, before being taken to Earth to face trial.
Release.
In the Fifth Doctor story "Resurrection of the Daleks", a small Dalek force aided by human mercenaries and Dalek duplicates liberates Davros (now played by Terry Molloy, with a new mask designed by Stan Mitchell) from his space station prison, needing his expertise to find an antidote for a Movellan-created virus that has all but wiped them out. Believing his creations to be treacherous, Davros begins using mind control on Daleks and humans, ultimately releasing the virus to kill off the Daleks before they can exterminate him. Davros expresses a desire to build a new and improved race of Daleks. However, at the end of the story, he apparently succumbs to the virus himself before he can escape, his physiology being close enough to that of the Daleks for the virus to affect him. The hypothetical creation of a viral weapon that could destroy all living creatures had been the subject of a discussion between the Fourth Doctor and Davros in "Genesis of the Daleks".
The Great Healer.
Davros emerges as "The Great Healer" of the funeral and cryogenic preservation centre Tranquil Repose on the planet Necros in the Sixth Doctor story "Revelation of the Daleks", where he uses frozen bodies to engineer a new variety of Daleks loyal to him, distinguished from the original Daleks by their white and gold livery and slightly changed design. In this story there appear to be two Davroses: one is a head in a tank and apparently a decoy for assassins; the other is in his usual chair (which can now hover), emerging from hiding when the decoy is assassinated. Davros can now fire electric bolts from both his eye and his hand, although the hand is shot off shortly before his original creations arrive to defeat the new Daleks and transport Davros to face trial on Skaro. In this serial Davros was again played by Molloy, his appearance as Davros was the same with very minor changes.
The Dalek Civil War.
Davros appears as the Emperor Dalek in "Remembrance of the Daleks", with his white and gold Daleks now based on Skaro and termed "Imperial Daleks", fighting against the grey "Renegade Dalek" faction. By this time, Davros has been physically transplanted into a customised Dalek casing and is only revealed as the Emperor in the final episode. Both Skaro and the Imperial Dalek mothership are apparently destroyed (in the future) when the Seventh Doctor tricks Davros into using the Time Lord artefact known as the Hand of Omega which makes Skaro's Sun go supernova. However, a Dalek on the bridge of Davros' ship reports that the Emperor's escape pod is being launched and a white light is seen speeding away from the ship moments before its destruction, leaving a clear route to bring Davros back in the future.
Time War.
By the 2005 series, the Daleks and the Time Lords had engaged in a mutually destructive Time War. The concluding episodes of the fourth season, "The Stolen Earth"/"Journey's End", reveal that Davros was thought to have died during the first year of the Time War, when his command ship "flew into the jaws of the Nightmare Child" at the Gates of Elysium, despite the Doctor's failed efforts to save him.
In earlier episodes, Davros is referred to (albeit not by name) twice: first in the episode "Dalek" by the Ninth Doctor, who explains that the Daleks were created by "a genius... a man who was king of his own little world", and again by the Tenth Doctor in the episode "Evolution of the Daleks", where he refers to the Daleks' creator as believing that "removing emotions made a race stronger".
Rescue.
Davros was pulled out of the Time War by Dalek Caan despite it being time-locked, and bred a new Dalek race using cells from his own body, so that he has little skin and flesh left on his chest and his ribcage and internal organs are visible. Though Davros talks about his "new empire", he at one point in time has actually been overthrown by his creatures and is kept prisoner in the Vault, being used for his scientific knowledge. The Doctor taunts him about being the "pet" of the Daleks. Davros now has a mechanical hand which is capable of firing electricity.
Under Davros' guidance, the Daleks steal 27 planets, including Earth, and hide them in the Medusa Cascade, one second out of sync with the rest of the universe. Davros and the Daleks plan to detonate a "reality bomb", a wavelength transmitted by the stolen planets which cancels out the electrical field binding atoms, reducing the whole of creation, even other Universes due to the Medusa Cascade being a rift, to nothingness except for the Daleks and the Crucible, in order to achieve "ultimate victory". However, Davros has been betrayed by Dalek Caan, who having come to the realisation of the evilness of his race after seeing the entirety of time due to his temporal shift, is using his prophecies and influence to cause the Daleks' destruction. He also manipulated events to bring the Doctor and Donna Noble together, who manage to defeat the Daleks. Davros furiously refuses the Doctor's offer to take him to safety, accusing him of being responsible for the destruction, screaming: "Never forget, Doctor, you did this! I name you forever: "You" are the Destroyer of Worlds!", having previously taunted the Doctor for turning his companions into killers and having caused the deaths of countless people out of comparison to himself. Thus the Doctor is forced to leave Davros to his fate as the Crucible self-destructs.
During "Doctor Who Confidential" Russell T Davies explained how he believes Davros to have survived the Crucible's destruction in some way, not specifically showing his death for this reason. He explained that he would not like to be the one to kill off one of the Doctor's greatest enemies, having supposedly killed off the Master in "Last of the Time Lords".
Other appearances.
Comic strips.
"Doctor Who Magazine" printed several comics stories involving Davros. The first, "Nemesis of the Daleks" (#152–155), with the Seventh Doctor, features an appearance of a Dalek Emperor. Speaking with the Emperor, the Doctor addresses him as Davros, but the Emperor responds "Who is Davros?" The Doctor initially assumes Davros's personality has been totally subsumed, but in the later strip "Emperor of the Daleks" (#197–202) this Emperor is shown as a different entity from Davros. Set prior to "Remembrance of the Daleks" in Davros's timeline, but after in the timeline of the Doctor, the latter, accompanied by Bernice Summerfield, together with help from the Sixth Doctor, ensures that Davros will survive the wrath of the Daleks so that he can assume the title of Emperor, allowing history to take its course. "Up Above the Gods" (#227), a vignette following up on this, features the Sixth Doctor and Davros having a conversation in the TARDIS.
Audio plays.
Terry Molloy has reprised his role as Davros in the spin-off audio plays produced by Big Finish Productions, mostly notably "Davros" (taking place during the Sixth Doctor's era), which, through flashbacks, explored the scientist's life prior to his crippling injury, which is attributed to a Thal nuclear attack (an idea that first appeared in Terrance Dicks' novelisation of "Genesis of the Daleks").
"Davros", which does not feature the Daleks, apparently fills in the gaps between "Resurrection of the Daleks" and "Revelation of the Daleks", and has the scientist trying to manipulate the galaxy's economy into a war footing similar to Skaro's. The Sixth Doctor manages to defeat his plans, and Davros is last heard when his ship explodes, an event obliquely mentioned in "Revelation". However the Doctor thinks he has survived. Davros also mentions he will work on a plan to combat famine, tying into Revelation of the Daleks.
"The Davros Mission" is an original audio adventure (without the Doctor) available on the Complete Davros Collection DVD box set. It takes place directly after the television story "Revelation", while leaving the planet Necros and beginning Davros' trial. At the end of "Davros Mission", he turns the tables on the Daleks, forcing them to do his bidding. The Big Finish miniseries I, Davros, also features trial scenes, but mostly explores his early life. In those four stories, his journey is seen from his boyhood, to just before "Genesis of the Daleks".
"The Curse of Davros" begins with Davros and the Daleks working together to try and alter the outcome of the Battle of Waterloo using technology that Davros has created that allows him to swap peoples' minds, with matters becoming more complicated when the Sixth Doctor uses the device to swap bodies with Davros in an attempt to subvert the Daleks' plans from the inside. At the end, Davros is left with an army of Daleks who have had their minds wiped. These Daleks presumably become the "Imperial Daleks", first seen in "Remembrance of the Daleks".
In "The Juggernauts", Davros is on the run from the original Daleks. He hatches a plan to add human tissue to robotic Mechanoids, using them, along with his own Daleks, to destroy the originals. At the end of the story, the self-destruct mechanism of Davros' life-support chair explodes, destroying an entire human colony. It is not clear how Davros survives to become the Dalek Emperor as seen in "Remembrance". However in the DVD, the Davros Connections, director Gary Russell points out that the explosion of Davros' life-support chair leaves the listener to believe there is little of Davros left. This fits chronologically the fact that in "Remembrance" Davros is seen as a head inside the Emperor Dalek.
By the time of the Eighth Doctor audio play "Terror Firma" (set after "Remembrance"), Davros is commanding a Dalek army which has successfully conquered the Earth. His mental instability has grown to the point where "Davros" and "the Emperor" exist within him as different personalities. His Daleks recognise this instability and rebel against Davros. By the story's end the Emperor personality is dominant, and the Daleks agree to follow him and leave Earth.
Novels.
"Terror Firma" may contradict the events of the Eighth Doctor Adventures novel "War of the Daleks" by John Peel, in which an unmerged Davros is placed on trial by the Dalek Prime, a combination of the Dalek Emperor and the Dalek Supreme. In the novel the Dalek Prime claimed that the planet Antalin had been terraformed to resemble Skaro and was destroyed in its place. A subterfuge to destroy Daleks aligned to Davros; both on Skaro (Antalin) and those that remained hidden within Dalek ranks on Skaro (original). Despite finding evidence of threat to Skaro via evidence found on 22nd century earth of Davros's mission to 1960's earth and seeing the event via time-tracking equipment, the Dalek Prime allowed the destruction of Skaro to destroy Daleks allied to Davros. Dalek Prime also claimed that the Dalek/Movellan war (and indeed most of Dalek history before the destruction of "Skaro") was actually faked for Davros's benefit; in fact another ruse designed to bait Davros into giving evidence against himself (as he does in his trial.) Skaro is later seen to be intact and undamaged, and one character notes that it is quite possible the Dalek Prime is lying in order to weaken Davros's claim to leadership of the Daleks, while using foreknowledge of events to destroy and entrap Davros and his allies.
"War of the Daleks", like the comic strips and audio plays, is of uncertain canonicity when it comes to the television series.
At the conclusion of "War", Davros was seemingly disintegrated by a Spider Dalek on the order of the Dalek Prime. However, Davros had previously recruited one of the Spider Daleks as a sleeper agent for just such an eventuality, and even he was not certain in the end if he was being disintegrated or being teleported away to safety, leaving the possibility open for his return.
Short fiction.
Paul Cornell's dark vignette in the "Doctor Who Magazine" Brief Encounters series, "An Incident Concerning the Bombardment of the Phobos Colony" occurs sometime between "Resurrection of the Daleks" and his assumption of the role of Emperor.
Theatre.
In 1993, Michael Wisher, the original Davros, with Peter Miles, who had played his confederate, Nyder, reprised the role in an unlicensed one-off amateur stage production, "The Trial of Davros". The plot of the play involved the Time Lords putting Davros on trial, with Nyder as a witness.
Terry Molloy played Davros in the remounting of the play, again with Peter Miles for another one-off production, mounted in 2005. During the production, specially shot footage portrayed Dalek atrocities.
In 2008, Julian Bleach appeared live as Davros at the Doctor Who Prom, announcing that the Royal Albert Hall would become his new palace, and the audience his "obedient slaves".
Unofficial BBC representation.
The BBC has traditionally created parodies of its own programming to be shown to its staff at Christmas events and parties. The BBC's 1993 Christmas tape parodied the allegedly robotic, dictatorial and ruthless management style of its then Director-General, John Birt, by portraying him as Davros taking over the BBC, carrying out bizarre mergers of departments, awarding himself a bonus and singing a song to the tune of I Wan'na Be Like You describing his plans.
DVD/Big Finish box set.
On 26 November 2007, a Davros boxset was released featuring the following TV stories;
"Genesis of the Daleks"<br>
"Destiny of the Daleks"<br>
"Resurrection of the Daleks"<br>
"Revelation of the Daleks"<br>
"Remembrance of the Daleks" Two Disc Special Edition
And the following Big Finish audios;
"Davros"<br>
"The Juggernauts"<br>
"Terror Firma"<br>
"<br>
"<br>
"<br>
"<br>
"The Davros Mission"

</doc>
<doc id="9140" url="http://en.wikipedia.org/wiki?curid=9140" title="Dalek">
Dalek

The Daleks are a fictional extraterrestrial race of mutants principally portrayed in the British science fiction television programme "Doctor Who". The Daleks were conceived by science-fiction writer Terry Nation and first appeared in the 1963 "Doctor Who" serial "The Daleks", in the shells designed by Raymond Cusick.
Within the programme narrative, Daleks are an extraterrestrial race of cyborgs created by the scientist Davros during the final years of a thousand-year war against the Thals. He genetically modified his race (known as the Kaleds), and integrated them with a tank-like, robotic, mechanical shell. His final modification was to remove their ability to feel pity, compassion, or remorse. The Daleks soon came to view themselves as the supreme race in the universe and began a conquest of universal domination and extermination. Various storylines portray them as having had every emotion removed except hate, leaving them with a desire to purge the Universe of all non-Dalek life. Collectively they are the greatest enemies of the series' protagonist, the Time Lord known as the Doctor. During a conflict with the Time Lords, the Daleks were almost completely killed off. This took place off-screen between the 1996 television movie and the 2005 revived series, and was depicted in the 50th anniversary special "The Day of the Doctor". Their defeat was a plot point in several episodes. They are popularly known for their catchphrase "Exterminate!" and are a well-recognised reference in British popular culture.
Creation.
The Daleks were created by writer Terry Nation and designed by BBC designer Raymond Cusick. They were introduced in December 1963 in the second "Doctor Who" serial, colloquially known as "The Daleks". They became an immediate and huge hit with viewers, featuring in many subsequent serials and two 1960s motion pictures. They have become as synonymous with "Doctor Who" as the Doctor himself, and their behaviour and catchphrases are now part of British popular culture. "Hiding behind the sofa whenever the Daleks appear" has been cited as an element of British cultural identity; and a 2008 survey indicated that nine out of 10 British children were able to identify a Dalek correctly. In 1999 a Dalek appeared on a postage stamp celebrating British popular culture, photographed by Lord Snowdon. In 2010, readers of science fiction magazine SFX voted the Dalek as the all-time greatest monster, beating out competition including Japanese movie monster Godzilla and J. R. R. Tolkien's Gollum, of "The Lord of the Rings".
Entry into popular culture.
The word "Dalek" has entered major dictionaries, including the "Oxford English Dictionary", which, although it is technically a cyborg, defines it as "a type of robot appearing in 'Dr. Who', a B.B.C. Television science-fiction programme; hence used allusively." English-speakers sometimes use the term metaphorically to describe people, usually figures of authority, who act like robots unable to break from their programming; for example, John Birt, the Director-General of the BBC from 1992 to 2000, was publicly called a "croak-voiced Dalek" by playwright Dennis Potter in the MacTaggart Lecture at the 1993 Edinburgh Television Festival.
Physical characteristics.
Externally, Daleks normally resemble human-sized pepper shakers with a single mechanical eyestalk mounted on a rotating dome, a gun mount containing an energy weapon ("gunstick" or "death ray") and a telescopic manipulator arm which is usually tipped by an appendage resembling a sink plunger. Daleks have been seen to be able to use their plungers to interface with technology, crush a man's skull by suction, measure the intelligence of a subject, and extract information from a man's mind. Dalek casings are made of a bonded polycarbide material dubbed "dalekanium" by a member of the human resistance in "The Dalek Invasion of Earth" and by the Cult of Skaro in "Daleks in Manhattan".
The lower half of a Dalek's shell is covered with hemispherical protrusions, or "Dalek bumps," which are shown in the episode "Dalek" to be spheres embedded in the casing. Both the BBC-licensed "Dalek Book" (1964) and "The Doctor Who Technical Manual" (1983) describe these items as being part of a sensory array, whilst in the 2005 series episode "Dalek", they are shown to serve a function in a Dalek's self-destruct mechanism. Their armour has a forcefield that evaporates most bullets and resists most types of energy weapon; this seems to be concentrated around the Dalek's midsection (where the mutant is located), as normally ineffective firepower can be concentrated on the eyestalk to blind a Dalek. Daleks have a very limited field of vision, with no peripheral sight at all, and are relatively easy to hide from in fairly exposed places. Their own energy weapons have also been shown to be capable of destroying them. Their weapons fire a beam that has electrical tendencies, is capable of propagating through water and may be a form of plasma. The eyepiece is a Dalek's most vulnerable spot, and impairing its vision often leads to a blind, panicked firing of its weapon whilst shouting, "My vision is impaired; I cannot see!" Russell T Davies subverted the catchphrase in his 2008 episode "The Stolen Earth", in which a Dalek vaporises a paintball that has blocked its vision while proclaiming "My vision is "not" impaired!".
The creature inside the mechanical casing is depicted as soft and repulsive in appearance and vicious even without its mechanical armour. The first-ever glimpse of a Dalek mutant, in "The Daleks", was a claw peeking out from under a Thal cloak after it had been removed from its casing. The actual appearance of the mutants has varied, but often adheres to the Doctor's description of the species in "Remembrance of the Daleks" as "little green blobs in bonded polycarbide armour". In "Resurrection of the Daleks" a Dalek creature, separated from its casing, attacks and severely injures a human soldier; in "Revelation of the Daleks", there are two Dalek factions (Imperial and Renegade) and the creatures inside have a different appearance in each case, one resembling the amorphous creature from "Resurrection", the other the crab-like creature from the original Dalek serial. As the creature inside is rarely seen on screen, a common misconception exists that Daleks are wholly mechanical robots. In the new series Daleks are retconned to be mollusc-like in appearance, with small tentacles, one or two eyes and an exposed brain.
The voice of a Dalek is electronic; the Dalek creature is apparently unable to make much more than squeaking sounds when out of its casing. Once the mutant is removed, the casing itself can be entered and operated by humanoids; for example, in "The Daleks", Ian Chesterton (William Russell) enters a Dalek shell to masquerade as a guard as part of an escape plan.
For many years it was assumed that, due to their design and gliding motion, Daleks were unable to climb stairs, and that this was a simple way of escaping them. A well known cartoon from "Punch" pictured a group of Daleks at the foot of a flight of stairs with the caption, "Well, this certainly buggers our plan to conquer the Universe". In a scene from the serial "Destiny of the Daleks", the Doctor and companions escape from Dalek pursuers by climbing into a ceiling duct. The Fourth Doctor calls down, "If you're supposed to be the superior race of the universe, why don't you try climbing after us?" The Daleks generally make up for their general lack of mobility with overwhelming firepower; a joke among "Doctor Who" fans goes, "Real Daleks don't climb stairs; they level the building." Dalek mobility has improved over the history of the series: in their first appearance, "The Daleks", they were capable of movement only on the conductive metal floors of their city; in "The Dalek Invasion of Earth" a Dalek emerges from the waters of the River Thames, indicating that they not only had become freely mobile, but are amphibious; "Planet of the Daleks" showed that they could ascend a vertical shaft by means of an external anti-gravity mat placed on the floor, "Revelation of the Daleks" showed Davros in his life-support chair and one of his Daleks hovering and "Remembrance of the Daleks" depicted them as capable of hovering up a flight of stairs. Despite this, journalists covering the series frequently refer to the Daleks' supposed inability to climb stairs; characters escaping up a flight of stairs in the 2005 episode "Dalek" made the same joke, and were shocked when the Dalek began to hover up the stairs after uttering the phrase "ELEVATE", in a similar manner to their normal phrase "EXTERMINATE." The new series depicts the Daleks as fully capable of flight, even space flight.
Prop details.
The non-humanoid shape of the Dalek did much to enhance the creatures' sense of menace. A lack of familiar reference points differentiated them from the traditional "bug-eyed monster" of science fiction, which "Doctor Who" creator Sydney Newman had wanted the show to avoid. The unsettling form of the Daleks, coupled with their alien voices, made many believe that the props were wholly mechanical and operated by remote control.
The Daleks were actually controlled from inside by short operators who had to manipulate their eyestalks, domes and arms, as well as flashing the lights on their heads in sync with the actors supplying their voices. The Dalek cases were built in two pieces; an operator would step into the lower section, and then the top would be secured. The operators looked out between the cylindrical louvres just beneath the dome which were lined with mesh to conceal their faces.
In addition to being hot and cramped the Dalek casings also muffled external sounds, making it difficult for the operators to hear the director's commands or studio dialogue. John Scott Martin, a Dalek operator from the original series, said that Dalek operation was a challenge: "You had to have about six hands: one to do the eyestalk, one to do the lights, one for the gun, another for the smoke canister underneath, yet another for the sink plunger. If you were related to an octopus then it helped."
For "Doctor Who"'s 21st-century revival the Dalek casings retain the same overall shape and dimensional proportions of previous Daleks, although many details have been re-designed to give the Dalek a heavier and more solid look. Changes include a larger, more pointed base; a glowing eyepiece; an all-over metallic brass finish (specified by Davies); thicker, nailed strips on the "neck" section; a housing for the eyestalk pivot; and significantly larger dome lights. The new prop made its on-screen debut in the 2005 episode "Dalek". These Dalek casings use a short operator inside the housing while the 'head' and eyestalk are operated via remote control. A third person, Nicholas Briggs, supplies the voice in their various appearances. In the 2010 season a new and larger model appeared in several colours representing different parts of the Dalek command hierarchy.
Movement.
The original plan of Terry Nation's was for the Daleks to glide across the floor. Early versions of the Daleks rolled on nylon castors, propelled by the operator's feet. Although casters were adequate for the Daleks' debut serial, which was shot entirely at the BBC's Lime Grove Studios, for "The Dalek Invasion of Earth" Terry Nation wanted the Daleks to be filmed on the streets of London. To enable the Daleks to travel smoothly on location, designer Spencer Chapman built the new Dalek shells around miniature tricycles with sturdier wheels, which were hidden by enlarged fenders fitted below the original base. The uneven flagstones of Central London caused the Daleks to rattle as they moved and it was not possible to remove this noise from the final soundtrack. A small parabolic dish was added to the rear of the prop's casing to explain why these Daleks, unlike the ones in their first serial, were not dependent on static electricity drawn up from the floors of the Dalek city for their motive power.
Later versions of the prop had more efficient wheels and were once again simply propelled by the seated operators' feet, but they remained so heavy that when going up ramps they often had to be pushed by stagehands out of camera shot. The difficulty of operating all the prop's parts at once contributed to the occasionally jerky movements of the Dalek. This problem has largely been eradicated with the advent of the "new series" version, as its remotely controlled dome and eyestalk allow the operator to concentrate on the smooth movement of the Dalek and its arms.
Voices.
The staccato delivery, harsh tone and rising inflection of the Dalek voice were initially developed by voice actors Peter Hawkins and David Graham, who would vary the pitch and speed of the lines according to the emotion needed. Their voices were further processed electronically by Brian Hodgson at the BBC Radiophonic Workshop. Although the exact sound-processing devices used have varied, the original 1963 effect used equalisation to boost the mid-range of the actor's voice, then subjected it to ring modulation with a 30 Hz sine wave. The distinctive harsh grating vocal timbre this produced has remained the pattern for all Dalek voices since (with the exception of those in the 1985 serial "Revelation of the Daleks", for which director Graeme Harper deliberately used less distortion).
Besides Hawkins and Graham, notable voice actors for the Daleks have included Roy Skelton, who first voiced the Daleks in the 1967 story "The Evil of the Daleks" and went on to provide voices for five additional Dalek serials including "Planet of the Daleks", and for the one-off anniversary special "The Five Doctors". Michael Wisher, the actor who originated the role of Dalek creator Davros in "Genesis of the Daleks", provided Dalek voices for that same story, as well as for "Frontier in Space", "Planet of the Daleks" and "Death to the Daleks". Other Dalek voice actors include Royce Mills (three stories), Brian Miller (two stories), and Oliver Gilbert and Peter Messaline (one story). John Leeson, who performed the voice of K-9 in several "Doctor Who" stories, and Davros actors Terry Molloy and David Gooderson also contributed supporting voices for various Dalek serials.
Since 2005, the Dalek voice in the television series has been provided by Nicholas Briggs, speaking into a microphone connected to a voice modulator. Briggs had previously provided Dalek and other alien voices for Big Finish Productions audio plays. In a 2006 BBC Radio interview, Briggs said that when the BBC asked him to do the voice for the new television series, they instructed him to bring his own analogue ring modulator that he had used in the audio plays. The BBC's sound department had changed to a digital platform and could not adequately create the distinctive Dalek sound with their modern equipment. Briggs went as far as to bring the voice modulator to the actors' readings of the scripts.
Construction.
Manufacturing the props was expensive. In scenes where many Daleks had to appear, some of them would be represented by wooden replicas ("Destiny of the Daleks") or life-size photographic enlargements in the early black-and-white episodes ("The Daleks", "The Dalek Invasion of Earth" and "The Power of the Daleks"). In stories involving armies of Daleks, the BBC effects team even turned to using commercially available toy Daleks, manufactured by Louis Marx & Co and Herts Plastic Moulders Ltd. Examples of such use can be observed in the serials "The Power of the Daleks", "The Evil of the Daleks" and "Planet of the Daleks". Judicious editing techniques also gave the impression that there were more Dalek props than were actually available, and continue to be used to the present day, such as using split screen in "The Parting of the Ways".
Four fully functioning props were commissioned for the first serial "The Daleks" in 1963, and were constructed from BBC plans by Shawcraft Engineering. These became known in fan circles as "Mk I Daleks". Shawcraft were also commissioned to construct approximately twenty Daleks for the two Dalek movies in 1965 and 1966 (see below). Some of these movie props filtered back to the BBC and were seen in the televised serials, notably "The Chase", which was aired before the first movie's debut. The remaining props not bought by the BBC were either donated to charity or given away as prizes in competitions.
The BBC's own Dalek props were reused many times, with components of the original Shawcraft "Mk I Daleks" surviving right through to their final classic series appearance in 1988. Years of storage and repainting took their toll, however. By the time of the Sixth Doctor's "Revelation of the Daleks" new props were being manufactured out of fibreglass, which were lighter and more affordable to construct than their predecessors. These Daleks were slightly bulkier in appearance around the mid-shoulder section, and also had a slightly redesigned skirt section which was more vertical at the back. Other minor changes were made to the design due to these new construction methods, including alterations to the fender and the incorporation of both the arm boxes, collars and slats into a single fibreglass moulding. These props were repainted in grey for the Seventh Doctor serial "Remembrance of the Daleks" and designated as "Renegade Daleks"; another redesign, painted in cream and gold, became the "Imperial Dalek" faction.
New Dalek props were built for the 21st century version of "Doctor Who". The first, which appeared alone in the 2005 episode "Dalek", was built by modelmaker Mike Tucker. Additional Dalek props based on Tucker's master were subsequently built out of fibreglass by Cardiff-based Specialist Models.
Development.
Wishing to create an alien creature that did not look like a "man in a suit", Terry Nation stated in his script for the first Dalek serial that they should have no legs. He was also inspired by a performance by the Georgian National Ballet, in which dancers in long skirts appeared to glide across the stage. For many of the shows, the Daleks were operated by retired ballet dancers wearing black socks while sitting inside the Dalek. Raymond Cusick (who died on 21 Feb 2013) was given the task of designing the Daleks when Ridley Scott, then a designer for the BBC, proved unavailable after having been initially assigned to their debut serial. An account in Jeremy Bentham's "Doctor Who—The Early Years" (1986) says that after Nation wrote the script, Cusick was given only an hour to come up with the design for the Daleks, and was inspired in his initial sketches by a pepper shaker on a table. Cusick himself, however, states that he based it on a man seated in a chair, and only used the pepper shaker to demonstrate how it might move. 
In 1964 Nation told a "Daily Mirror" reporter that the Dalek name came from a dictionary or encyclopaedia volume, the spine of which read "Dal – Lek" (or, according to another version, "Dal – Eks"). He later admitted that this book and the origin of the Dalek name was completely fictitious, and that anyone bothering to check out his story would have found him out. The name had in reality simply rolled off his typewriter. Later, Nation was pleasantly surprised to discover that in Serbo-Croatian the word "dalek" means "far", or "distant".
Nation grew up during World War II, and remembered the fear caused by German bombings. He consciously based the Daleks on the Nazis, conceiving the species as faceless, authoritarian figures dedicated to conquest and complete conformity. The allusion is most obvious in the Dalek stories penned by Nation, in particular "The Dalek Invasion of Earth" (1964) and "Genesis of the Daleks" (1975).
Prior to writing the first Dalek serial, Nation was chief scriptwriter for comedian Tony Hancock. The two had a falling out, and Nation either resigned or was fired. When Hancock left the BBC, he worked on several series proposals, one of which was called "From Plip to Plop", a comedic history of the world which would have ended with a nuclear apocalypse, the survivors being reduced to living in dustbin-like robot casings and eating radiation to stay alive. According to biographer Cliff Goodwin, when Hancock saw the Daleks, he allegedly shouted at the screen, "That bloody Nation—he's stolen my robots!"
The naming of early "Doctor Who" stories is complex and sometimes controversial. The first Dalek serial is called, variously, "The Survivors" (the pre-production title), "The Mutants" (its official title at the time of production and broadcast, later taken by another unrelated story), "Beyond the Sun" (used on some production documentation), "The Dead Planet" (the on-screen title of the serial's first episode), or simply "The Daleks".
The instant appeal of the Daleks caught the BBC off guard, and transformed "Doctor Who" from a Saturday tea-time children's educational programme to a must-watch national phenomenon. Children were alternately frightened and fascinated by the alien look of the monsters, and the "Doctor Who" production office was inundated by letters and calls asking about the creatures. Newspaper articles focused attention on the series and the Daleks, further enhancing their popularity.
Nation jointly owned the intellectual property rights to the Daleks with the BBC, and the money-making concept proved nearly impossible to sell to anyone else; he was dependent on the BBC wanting to produce stories featuring the creatures. Several attempts to market the Daleks outside of the series were unsuccessful. Since Nation's death in 1997, his share of the rights is now administered by his former agent, Tim Hancock.
Early plans for what eventually became the 1996 "Doctor Who" television movie included radically redesigned Daleks whose cases unfolded like spiders' legs. The concept for these "Spider Daleks" was abandoned, but picked up again in several "Doctor Who" spin-offs.
When the new series was announced, many fans hoped the Daleks would return once more to the programme. The Nation estate however demanded levels of creative control over the Daleks' appearances and scripts that were unacceptable to the BBC. Eventually the Daleks were cleared to appear in the first series.
Fictional history.
Dalek in-universe history has seen many retroactive changes, which have caused continuity problems. When the Daleks first appeared in "The Daleks", they were presented as the descendants of the Dals, mutated after a brief nuclear war between the Dal and Thal races 500 years ago. This race of Daleks is destroyed when their power supply is wrecked. However they reappear in "The Dalek Invasion of Earth", where they have conquered Earth in the 22nd century. Later stories saw them develop time travel and a space empire. In 1975, Terry Nation revised the Daleks' origins in "Genesis of the Daleks", where the Dals were now called Kaleds (of which "Daleks" is an anagram), and the Dalek design was attributed to one man, the crippled Kaled chief scientist and evil genius, Davros. Instead of a short nuclear exchange, the Kaled-Thal war was portrayed as a thousand-year-long war of attrition, fought with nuclear, biological and chemical weapons causing widespread mutations among the Kaled race. Davros experimented on living Kaled cells to find the ultimate mutated form of the Kaled species and placed the subjects in tank-like "travel machines" whose design was based on his own life-support chair.
"Genesis of the Daleks" marked a new era for the depiction of the species, with most of their previous history either forgotten or barely referred to again. Future stories in the original "Doctor Who" series, which followed a rough story arc, would also focus more on Davros, much to the dissatisfaction of some fans who felt that the Daleks should take centre stage rather than merely becoming minions of their creator. Davros made his last televised appearance for 20 years in "Remembrance of the Daleks", which depicted a civil war between two factions of Daleks. One, the "Imperial Daleks", were loyal to Davros, who had become their Emperor, whilst the other, the "Renegade Daleks", followed a black Supreme Dalek. By the end of the story both factions have been wiped out and the Doctor has tricked them into destroying Skaro, though Davros escapes.
A single Dalek appeared in "Dalek", written by Robert Shearman, which was broadcast on BBC One on 30 April 2005. This Dalek appeared to be the sole Dalek survivor of the Time War which had destroyed both the Daleks and the Time Lords. A Dalek Emperor returned at the end of the 2005 series, having rebuilt the Dalek race with genetic material harvested from human subjects. It saw itself as a god, and the new Daleks were shown worshipping it. These Daleks and their fleet were destroyed in "The Parting of the Ways". The 2006 season finale "Army of Ghosts"/"Doomsday" featured a squad of four Dalek survivors from the old Empire, known as the Cult of Skaro, led by a black Dalek known as "Sec", that had survived the Time War by escaping into the Void between dimensions. They emerged, along with the Genesis Ark, a Time Lord prison vessel containing millions of Daleks, at Canary Wharf due to the actions of the Torchwood Institute and Cybermen from a parallel world. This resulted in a Cyberman-Dalek clash in London, which was resolved when the Tenth Doctor caused both groups to be sucked back into the Void. The Cult survived by utilising an "emergency temporal shift" to escape.
These four Daleks, Sec, Jast, Thay and Caan, returned in the two-part story "Daleks in Manhattan"/"Evolution of the Daleks", in which whilst stranded in 1930s New York, they set up a base in the partially built Empire State Building and attempt to rebuild the Dalek race. To this end Dalek Sec merges with a human being to become a Human/Dalek hybrid. The Cult then set about creating "Human Daleks" by "formatting" the brains of a few thousand captured humans, with the intention of producing hybrids which remain fully human in appearance but with Dalek minds. Dalek Sec however starts to become so human, that he changes the DNA to make the hybrids more human. This angers the rest of the Cult, resulting in mutiny and the death of Sec, Thay and Jast as well as the wiping out of all the hybrids. This leaves Dalek Caan as the last Dalek in existence. When the Doctor makes Caan realise that he is the last of his kind, Caan uses emergency temporal shift and escapes once more.
The Daleks returned in the 2008 seasons' two-part finale, "The Stolen Earth"/"Journey's End", accompanied once again by their creator Davros. The story reveals that Caan's temporal shift sent him into the Time War whence he rescued Davros, in the process gaining the ability to see the future at the cost of his own sanity. Davros has created a new race using his own body's cells. The episode depicts a Dalek invasion of Earth which with other planets is taken to the Medusa Cascade, led by a red Supreme Dalek, who has kept Caan and Davros imprisoned in "The Vault", a section of the Dalek flagship, the "Crucible". Davros and the Daleks plan to destroy reality itself with a "reality bomb" for which they need the stolen planets. The plan fails due to the interference of Donna Noble, a companion of the Doctor, and Caan himself, who has been manipulating events to destroy the Daleks after realising the severity of the atrocities they have committed. The Daleks returned in the 2010 episode "Victory of the Daleks", the third episode of the series; Daleks who escaped the destruction of Davros' empire fell back in time and, by chance, managed to retrieve the "Progenitor". This is a tiny apparatus which contains 'original' Dalek DNA. The activation of the Progenitor results in the creation of a "new paradigm" of Daleks. The New Paradigm Daleks deem their creators inferior and exterminate them; their creators make no resistance to this, deeming themselves inferior as well. They are organised into different roles (drone, scientist, strategists, supreme and eternal), which are identifiable with colour-coded armour instead of the identification plates under the eyestalk used by their predecessors. They escape the Doctor at the end of the episode via time travel with the intent to rebuild their Empire.
The Daleks only appeared briefly in subsequent finales "The Pandorica Opens"/"The Big Bang" (2010) as Steven Moffat decided to "give them a rest" and stated "There's a problem with the Daleks. They are the most famous of the Doctor's adversaries and the most frequent, which means they are the most reliably defeatable enemies in the universe." They next appear in "Asylum of the Daleks" (2012), where the Daleks are shown to have greatly increased numbers and have a Parliament; in addition to the traditional "modern" Daleks, several designs from both the original and new series appear. All record of the Doctor is removed from their collective consciousness at the end of the episode. The Daleks then appear in the 50th Anniversary special "The Day of the Doctor", where they are seen being defeated in the Time War. In "The Time of the Doctor", the Daleks are one of the races that travel to Trenzalore and besiege it for centuries to stop the Doctor from releasing the Time Lords. Due to converting Tasha Lem into a Dalek puppet, they regain knowledge of the Doctor. In the end, they are the only enemy left, the others having retreated or been destroyed and nearly kill the near-death Doctor before the Time Lords intervene and grant him a new regeneration cycle. The Doctor then uses his regeneration energy to obliterate the Daleks on the planet.
The Twelfth Doctor's first encounter with the Daleks is in his second full episode, "Into the Dalek". A beleaguered ship of the "Combined Galactic Resistance" has discovered a broken Dalek that has turned "good", desiring to kill all other Daleks. The Doctor, Clara and a team of soldiers are miniaturized and enter the Dalek, which the Doctor nicknames Rusty. They repair the damage, but accidentally restores to its original nature, causing it to go on the rampage and alert the Dalek fleet to the whereabouts of the rebel ship. However, the Doctor manages to return Rusty to its previous state by linking his mind with the Dalek's: Rusty shares the Doctor's view of the universe's beauty, but also his deep hatred of the Daleks. Rusty destroys the other Daleks and departs the ship, determined to track down and bring an end to the Dalek race.
Dalek culture.
Daleks have little, if any, individual personality, ostensibly no emotions other than hatred and anger, and a strict command structure in which they are conditioned to obey superiors' orders without question. Dalek speech is characterised by repeated phrases, and by orders given to themselves and to others. Unlike the stereotypical emotionless robots often found in science fiction, Daleks are often angry; author Kim Newman has described the Daleks as behaving "like toddlers in perpetual hissy fits", gloating when in power and flying into rage when thwarted. They tend to be excitable and will repeat the same word or phrase over and over again in heightened emotional states, most famously "Exterminate! Exterminate!"
In terms of their behaviour, Daleks are extremely aggressive, and seem driven by an instinct to attack. This instinct is so strong that Daleks have been depicted fighting the urge to kill or even attacking when unarmed. The Fifth Doctor characterises this impulse by saying, "However you respond [to Daleks] is seen as an act of provocation." The fundamental feature of Dalek culture and psychology is an unquestioned belief in the superiority of the Dalek race, and their default directive is to destroy all non-Dalek life-forms. Other species are either to be exterminated immediately or enslaved and then exterminated once they are no longer useful.
The Dalek obsession with their own superiority is illustrated by the schism between the Renegade and Imperial Daleks seen in "Revelation of the Daleks" and "Remembrance of the Daleks": the two factions each consider the other to be a perversion despite the relatively minor differences between them. This intolerance of any "contamination" within themselves is also shown in "Dalek," "The Evil of the Daleks" and in the Big Finish Productions audio play "The Mutant Phase". This superiority complex is the basis of the Daleks' ruthlessness and lack of compassion. This is shown in extreme in "Victory of the Daleks," where the new, pure Daleks destroy their creators, impure Daleks, with the latters' consent. It is nearly impossible to negotiate or reason with a Dalek, a single-mindedness that makes them dangerous and not to be underestimated. The Eleventh Doctor (Matt Smith) is later puzzled in the "Asylum of the Daleks" as to why the Daleks don't just kill the sequestered ones that have "gone wrong." Although The Asylum is subsequently obliterated, the Prime Minister of the Daleks explains that "it is offensive to us to destroy such divine hatred," and the Doctor is sickened at the revelation that hatred is actually considered beautiful by the Daleks.
Dalek society is depicted as one of extreme scientific and technological advancement; the Third Doctor states that "it was their inventive genius that made them one of the greatest powers in the universe." However, their reliance on logic and machinery is also a strategic weakness which they recognise, and thus use more emotion-driven species as agents to compensate for these shortcomings.
Although the Daleks are not known for their regard for due process, they have taken at least two enemies back to Skaro for a "trial", rather than killing them immediately. The first was their creator, Davros, in "Revelation of the Daleks", and the second was the renegade Time Lord known as the Master in the 1996 television movie. The reasons for the Master's trial, and why the Doctor would be asked to retrieve the Master's remains, have never been explained on screen. The "Doctor Who Annual 2006" implies that the trial may have been due to a treaty signed between the Time Lords and the Daleks. The framing device for the ' audio plays is a Dalek trial to determine if Davros should be the Daleks' leader once more.
Spin-off novels contain several tongue-in-cheek mentions of Dalek poetry, and an anecdote about an opera based upon it, which was lost to posterity when the entire cast was exterminated on the opening night. Two stanzas are given in the novel "The Also People" by Ben Aaronovitch. In an alternative timeline portrayed in Big Finish Productions audio adventure "The Time of the Daleks", the Daleks show a fondness for the works of Shakespeare. A similar idea was satirised by comedian Frankie Boyle in the BBC comedy quiz programme "Mock the Week"; he gave the fictional Dalek poem "Daffodils; EXTERMINATE DAFFODILS!" as an "unlikely line to hear in "Doctor Who"".
Because the Doctor has defeated the Daleks so often, he has become their collective arch-enemy and they have standing orders to capture or exterminate him on sight. In later fiction, the Daleks know the Doctor as "Ka Faraq Gatri" ("Bringer of Darkness" or "Destroyer of Worlds"), and "The Oncoming Storm". Both the Ninth Doctor (Christopher Eccleston) and Rose Tyler (Billie Piper) suggest that the Doctor is one of the few beings the Daleks fear. In "Doomsday", Rose notes that while the Daleks see the extermination of five million Cybermen as "pest control", "one Doctor" visibly un-nerves them (to the point they physically recoil). To his indignant surprise, in "Asylum of the Daleks", the Eleventh Doctor (Matt Smith) learns that the Daleks have designated him as "The Predator."
As The Doctor escapes The Asylum (with companions Amy and Rory), a Dalek-converted-human (Oswin Oswald) prisoner provides critical assistance, which culminates in completely deleting The Doctor from the Dalek hive-consciousness (the PathWeb), thus wiping the slate entirely blank. However, this was reversed in The Time of the Doctor, when the Daleks regained knowledge of the Doctor through the memory of an old acquaintance of the Doctor, Tasha Lem.
Measurements.
A "rel" is a Dalek and Kaled unit of measurement. It was usually a measurement of time, with a duration of slightly more than one second, as mentioned in "Evolution of the Daleks" and Journey's End, counting down to the ignition of the reality bomb. (One earth minute most likely equals about 50 "rels".) However, in some comic books it was also used as a unit of velocity. Finally, in some cases it was used as a unit of hydroelectric energy (not to be confused with a vep, the unit used to measure artificial sunlight).
The "rel" was first used in the non-canonical feature film , soon after appearing in early Doctor Who comic books.
Licensed appearances.
Two "Doctor Who" movies starring Peter Cushing featured the Daleks as the main villains: "Dr. Who and the Daleks", and "Daleks - Invasion Earth 2150 AD", based on the television serials "The Daleks" and "The Dalek Invasion of Earth", respectively. The movies were not direct remakes; for example, the Doctor in the Cushing films was a human inventor called "Dr. Who" he built a time-travelling device named "Tardis", instead of a mysterious alien who stole a device called "the TARDIS".
Four books focusing on the Daleks were published in the 1960s. "The Dalek Book" (1964, written by Terry Nation and David Whitaker), "The Dalek World" (1965, written by Nation and Whitaker) and "The Dalek Outer Space Book" (1966, by Nation and Brad Ashton) were all hardcover books formatted like annuals, containing text stories and comics about the Daleks, along with fictional information (sometimes based on the television serials, other times made up for the books). Nation also published "The Dalek Pocketbook and Space-Travellers Guide", which collected articles and features treating the Daleks as if they were real. Four more annuals were published in the 1970s by World Distributors under the title "Terry Nation's Dalek Annual" (with cover dates 1976–1979, but published 1975–1978). Two original novels by John Peel, "War of the Daleks" (1997) and "Legacy of the Daleks" (1998), were released as part of the Eighth Doctor Adventures series of "Doctor Who" novels. A novella, "The Dalek Factor" by Simon Clark, was published in 2004, and two books featuring the Daleks and the Tenth Doctor ("I am a Dalek" by Gareth Roberts, 2006, and "Prisoner of the Daleks" by Trevor Baxendale, 2009) have been released as part of the New Series Adventures.
Nation authorised the publication of the comic strip "The Daleks" in the comic "TV Century 21" in 1965. The weekly one-page strip, written by Whitaker but credited to Nation, featured the Daleks as protagonists and "heroes", and continued for two years, from their creation of the mechanised Daleks by the humanoid Dalek scientist, Yarvelling, to their eventual discovery in the ruins of a crashed space-liner of the co-ordinates for Earth, which they proposed to invade. Although much of the material in these strips directly contradicted what was shown on television, some concepts like the Daleks using humanoid duplicates and the design of the Dalek Emperor did show up later on in the programme.
At the same time, a "Doctor Who" strip was also being published in "TV Comic". Initially, the strip did not have the rights to use the Daleks, so the First Doctor battled the "Trods" instead, cone-shaped robotic creatures that ran on static electricity. By the time the Second Doctor appeared in the strip in 1967 the rights issues had been resolved, and the Daleks began making appearances starting in "The Trodos Ambush" (TVC #788-#791), where they massacred the Trods. The Daleks also made appearances in the Third Doctor-era "Dr. Who" comic strip that featured in the combined "Countdown/TV Action" comic during the early 1970s.
Other licensed appearances have included a number of stage plays (see Stage plays below) and television adverts for Wall's "Sky Ray" ice lollies (1966), Weetabix breakfast cereal (1977), Kit Kat chocolate bars (2001), and the ANZ Bank (2005). In 2003, Daleks also appeared in UK billboard ads for Energizer batteries, alongside the slogan "Are You Power Mad?"
Other appearances.
Non-"Doctor Who" television and film.
Daleks have made cameo appearances in television programmes and films unrelated to "Doctor Who" from the 1960s to the present day.
Music.
Daleks have been referred to or associated in many musical compositions.
Dalek is also mentioned and pictured in the Doc Brown vs Doctor Who. Epic Rap Battles of History Season 2, being summoned by Brown to exterminate the Tenth Doctor.
Samples of Dalek voices uttering the phrases "the prisoners have escaped" and "exterminate them" appear in the song "Shakespeare's Tacklebox" by the Australian band Spiderbait on their 1993 debut LP "ShaShaVaGlava".
Politics.
At the 1966 Conservative Party conference in Blackpool, delegate Hugh Dykes publicly compared the Labour government's Defence Secretary Denis Healey to the creatures. "Mr. Healey is the Dalek of defence, pointing a metal finger at the armed forces and saying 'I will eliminate you'."
In a British Government Parliamentary Debate in the House of Commons on 12 February 1968, the then Minister of Technology Tony Benn mentioned the Daleks during a reply to a question from the Labour MP Hugh Jenkins concerning the Concorde aircraft project. In the context of the dangers of solar flares, he said, "Because we are exploring the frontiers of technology, some people think Concorde will be avoiding solar flares like Dr. Who avoiding Daleks. It is not like this at all."
Australian Labor Party luminary Robert Ray described his right wing Labor Unity faction successor, Victorian Senator Stephen Conroy, and his Socialist Left faction counterpart, Kim Carr, as "factional Daleks" during a 2006 Australian Fabian Society lunch in Sydney.
Daleks have been used in political cartoons to caricature: Douglas Hurd, as the 'Douglek', in Private Eye's Dan Dire – Pilot of the Future, Tony Benn John Birt, Tony Blair (also portrayed as Davros), Alastair Campbell, Alec Douglas-Home, Charles de Gaulle, Peter Mandelson, Mark Thompson.
Magazine covers.
Daleks have appeared on magazine covers promoting "Doctor Who" since the "Dalekmania" fad of the 1960s. "Radio Times" has featured the Daleks on its cover several times, beginning with the 21–27 November 1964 issue which promoted "The Dalek Invasion of Earth". Other magazines also used Daleks to attract readers' attention, including "Girl Illustrated".
In April 2005, "Radio Times" created a special cover to commemorate both the return of the Daleks to the screen in "Dalek" and the forthcoming general election. This cover recreated a scene from "The Dalek Invasion of Earth" in which the Daleks were seen crossing Westminster Bridge, with the Houses of Parliament in the background. The cover text read "VOTE DALEK!" In a 2008 contest sponsored by the Periodical Publishers Association, this cover was voted the best British magazine cover of all time. In 2013 it was voted "Cover of the century" by the Professional Publishers Association. The 2010 UK general election campaign also prompted a collector's set of three near-identical covers of the "Radio Times" on 17 April with exactly the same headline but with the newly redesigned Daleks in their primary colours representing the three main political parties, Red being Labour, Blue as Conservative and Yellow as Liberal Democrats.
Parodies.
Daleks have been the subject of many parodies, including Spike Milligan's "Pakistani Dalek" sketch in his comedy series "Q", and Victor Lewis-Smith's "Gay Daleks". Occasionally the BBC has used the Daleks to parody other subjects: in 2002, BBC Worldwide published the "Dalek Survival Guide", a parody of "The Worst-Case Scenario Survival Handbooks". Comedian Eddie Izzard has an extended stand-up routine about Daleks, which was included in his 1993 stand-up show "Live at the Ambassadors". The Daleks made two brief appearances in a pantomime version of "Aladdin" at the Birmingham Hippodrome which starred "Torchwood" star John Barrowman in the lead role. A joke telling robot, possessing a Dalek-like boom, and loosely modelled after the Dalek also appeared in the "South Park" episode "Funnybot" even spouting out "exterminate". A Dalek can also be seen in the background at timepoints 1:13 and 1:17 in the Sam and Max animated series episode The Trouble With Gary. In the "Community" parody of "Doctor Who" called "Inspector Spacetime", they are referred to as Blorgons.
Merchandising.
The BBC approached Walter Tuckwell, a New Zealand-born entrepreneur who was handling product merchandising for other BBC shows, and asked him to do the same for the Daleks and "Doctor Who". Tuckwell created a glossy sales brochure that sparked off a Dalek craze, dubbed "Dalekmania" by the press, which peaked in 1965.
Toys and models.
The first Dalek toys were released in 1965 as part of the "Dalekmania" craze. These included battery-operated, friction drive and "Rolykins" Daleks from Louis Marx & Co., as well as models from Cherilea, Herts Plastic Moulders Ltd and Cowan, de Groot Ltd, and "Bendy" Daleks made by Newfeld Ltd. At the height of the Daleks' popularity, in addition to toy replicas, there were Dalek board games and activity sets, slide projectors for children and even Dalek playsuits made from PVC. Collectible cards, stickers, toy guns, music singles, punching bags and many other items were also produced in this period. Dalek toys released in the 1970s included a new version of Louis Marx's battery-operated Dalek (1974), a "talking Dalek" from Palitoy (1975) and a Dalek board game (1975) and Dalek action figure (1977), both from Denys Fisher. From 1988 to 2002, Dapol released a line of Dalek toys in conjunction with its "Doctor Who" action figure series.
In 1984, Sevans Models released a self-assembly model kit for a one-fifth scale Dalek, which "Doctor Who" historian David Howe has described as "the most accurate model of a Dalek ever to be released". Comet Miniatures released two Dalek self-assembly model kits in the 1990s.
In 1992, Bally released a Doctor Who pinball machine which prominently featured the Daleks both as a primary playfield feature and as a motorised toy in the topper.
Bluebird Toys produced a Dalek-themed "Doctor Who" playset in 1998.
Beginning in 2000, Product Enterprise (who later operated under the names "Iconic Replicas" and "Sixteen 12 Collectibles") produced various Dalek toys. These included Dalek "Rolykins" (based on the Louis Marx toy from 1965); push-along "talking" Daleks; Dalek "Rollamatics" with a pull back and release mechanism; and a remote control Dalek.
In 2005 Character Options was granted the "Master Toy License" for the revived "Doctor Who" series, including the Daleks. Their product lines have included static/push-along and radio controlled Daleks, radio controlled versions and radio controlled / 1:3 scale variants. The 12-inch remote control Dalek won the 2005 award for Best Electronic Toy of the Year from the Toy Retailers Association. Some versions of the 18-inch model included semi-autonomous and voice command-features. In 2008, the company acquired a license to produce Daleks of the various "classic series" variants. For the fifth revived series, both Ironside (Post-Time war Daleks in camouflage khaki), Drone (new, red) and, later, Strategist Daleks (new, blue) were released as both RC Infrared Battle Daleks and action figures.
Video games.
Licensed "Doctor Who" games featuring Daleks include 1984's "The Key to Time", a text adventure game for the Sinclair ZX Spectrum. Daleks also appeared in minor roles or as thinly disguised versions in other, minor games throughout the 80s, but did not feature as central adversaries in a licensed game until 1992, when Admiral Software published "Dalek Attack". The game allowed the player to play various Doctors or companions, running them through several environments to defeat the Daleks. In 1997 the BBC released a PC game entitled "Destiny of the Doctors" which also featured the Daleks, among other adversaries.
Unauthorized games featuring Daleks continued to appear through the 1990s and 2000s, including Dalek-based modifications of "Dark Forces", "Quake", and "Half-Life", and even more recently, a mod of ""; many of these can be found online, including an Adobe Flash game, "Dalek:Dissolution Earth". In 1998 "QWho", a modification for "Quake", featured the Daleks as adversaries. This also formed the basis of "TimeQuake", a total conversion written in 2000 which included other "Doctor Who" monsters such as Sontarans. Another unauthorised game is "DalekTron", a based on written to coincide with the 2005 series.
One authorised online game is "The Last Dalek", a Flash game created by New Media Collective for the BBC. It is based on the 2005 episode "Dalek" and can be played at the official BBC "Doctor Who" website. The "Doctor Who" website also features another game, "Daleks vs Cybermen" (also known as "Cyber Troop Control Interface"), based on the 2006 episode "Doomsday"; in this game, the player controls troops of Cybermen which must fight Daleks as well as Torchwood Institute members.
On 5 June 2010, the BBC released the first of four official computer games on its website, 'Doctor Who: The Adventure Games', which are intended as part of the official TV series adventures. In the first of these, 'The City of the Daleks', the Doctor in his 11th incarnation and Amy Pond must stop the Daleks re-writing time and reviving Skaro, their homeland.
They also appear in the Nintendo DS and Wii games and .
Several Daleks appear in the iOS game as rare enemies the player faces, appearing only in the first and final levels.
Full-size reproductions.
Dalek fans have been building life-size reproduction Daleks for many years. The BBC and Terry Nation estate officially disapprove of self-built Daleks, but usually intervene only if attempts are made to trade unlicensed Daleks and Dalek components commercially, or if it is considered that actual or intended use may damage the BBC's reputation or the Doctor Who/Dalek brand. The Crewe, Cheshire-based company "This Planet Earth" is the only business which has been licensed by the BBC and the Terry Nation Estate to produce full-size TV Dalek replicas, and by Canal+ Image UK Ltd. to produce full size Movie Dalek replicas commercially.

</doc>
<doc id="9141" url="http://en.wikipedia.org/wiki?curid=9141" title="Davy Jones (musician)">
Davy Jones (musician)

David Thomas "Davy" Jones (30 December 194529 February 2012) was an English singer-songwriter, musician, actor and businessman best known as one of the Monkees four man pop rock group and co-star of the TV series of the same name. His acting credits include a Tony-nominated role as the Artful Dodger in the original London and Broadway productions of "Oliver!" as well as a starring cameo role in a hallmark episode of "The Brady Bunch" television show and later reprised parody film; "Love, American Style"; and "My Two Dads". Jones is considered one of the great teen idols.
Early life.
Davy Jones was born at 20 Leamington Street, Openshaw, Manchester, Lancashire, England, on 30 December 1945. His television acting debut was on the British television soap opera "Coronation Street". He portrayed Colin Lomax, Ena Sharples' grandson, for one episode on 6 March 1961. He also appeared in the BBC police series "Z-Cars". After the death of his mother from emphysema when he was 14 years old, Jones rejected acting in favour of a career as a jockey, apprenticing with Newmarket trainer Basil Foster. He dropped out of secondary school to begin his career in that field. This career was short-lived however. Even though Foster believed Jones would be successful as a jockey, he encouraged his young protégé to take a role as the Artful Dodger a production of "Oliver!" in London's West End, a move which changed Jones' life forever. In turn, Jones cared for Foster in his later years, bringing him to the United States and providing him financial support.
Early acting and recording career.
Foster was approached by a friend who worked in a theatre in the West End of London during casting for the musical "Oliver!". Foster replied, "I've got the kid." Jones was cast and appeared to great acclaim as the Artful Dodger. He played the role in London and then on Broadway and was nominated for a Tony Award. On 9 February 1964, he appeared on "The Ed Sullivan Show" with Georgia Brown who was playing Nancy in the Broadway production of "Oliver!". This was the same episode of the show in which the Beatles made their first appearance. Jones said of that night, "I watched the Beatles from the side of the stage, I saw the girls going crazy, and I said to myself, this is it, I want a piece of that."
Following his "Ed Sullivan" appearance, Jones signed a contract with Ward Sylvester of Screen Gems (then the television division of Columbia Pictures). A pair of American television appearances followed, as Jones received screen time in episodes of "Ben Casey" and "The Farmer's Daughter".
Jones debuted on the Hot 100 in the week of 14 August 1965, with the single "What Are We Going To Do?" The 19-year-old singer was signed to Colpix Records, a label owned by Columbia. His debut album "David Jones", on the same label, followed soon after (CP493). In 1967 the album was issued in the UK, in mono only, on the Pye Records label (NPL 18178).
The Monkees.
From 1966 to 1971, Jones was a member of the Monkees, a pop-rock group formed expressly for a television show of the same name. With Screen Gems producing the series, Jones was shortlisted for auditions, as he was the only Monkee who was signed to a deal with the studio, but still had to meet producers Bob Rafelson's and Bert Schneider's standards. Jones sang lead vocals on many of the Monkees' recordings, including "I Wanna Be Free" and "Daydream Believer".
The NBC television series the Monkees was popular, and remained so in syndication. After the group disbanded in 1971, Jones reunited with Micky Dolenz as well as Monkees songwriters Tommy Boyce and Bobby Hart in 1974 as a short-lived group called Dolenz, Jones, Boyce & Hart.
A Monkees television show marathon ("Pleasant Valley Sunday") broadcast on 23 February 1986 by MTV resulted in a wave of Monkeemania not seen since the group's heyday. Jones reunited with Dolenz and Peter Tork from 1986 to 1989 to celebrate the band's renewed success and promote the 20th anniversary of the group. A new top 20 hit, "That Was Then, This Is Now" was released (though Jones did not perform on the song) as well as an album, "Pool It!".
Monkees activity ceased until 1996 when Jones reunited with Dolenz, Tork and Michael Nesmith to celebrate the 30th anniversary of the band. The group released a new album entitled "Justus", the first album since 1967's "Headquarters" that featured the band members performing all instrumental duties. It was the last time all four Monkees performed together.
In February 2011, Jones confirmed rumours of another Monkees reunion. "There's even talk of putting the Monkees back together again in the next year or so for a U.S. and UK tour," he told Disney's Backstage Pass newsletter. "You're always hearing all those great songs on the radio, in commercials, movies, almost everywhere." The tour (Jones's last) came to fruition entitled, "."
Post-Monkees career.
In 1967, Jones opened his first store, called Zilch, at 217 Thompson Street in the Greenwich Village section of New York City. The store sold "hip" clothing and accessories and also allowed customers to design their own clothes.
After the Monkees officially disbanded in 1971, Jones kept himself busy by establishing a New York City-style street market in Los Angeles, called "The Street" which cost approximately $40,000. He also collaborated with musical director Doug Trevor on a one-hour ABC television special entitled "Pop Goes Davy Jones", which featured new artists the Jackson Five and the Osmonds.
Bell Records, then having a string of hits with "The Partridge Family", signed Jones to a somewhat inflexible solo record contract in 1971. Jones was not allowed to choose his songs or producer, resulting in several lackluster and aimless records. His second solo album, "Davy Jones" (1971) was notable for the song "Rainy Jane", which reached No.52 in the "Billboard" charts. To promote the album, Jones performed "Girl" on an episode of "The Brady Bunch" entitled "Getting Davy Jones". Although the single sold poorly, the popularity of Jones's appearance on the show resulted in "Girl" becoming his best remembered solo hit, even though it was not included in the album. The final single, "I'll Believe In You"/"Road to Love," was poorly received. Jones also continued acting after the Monkees, either as himself or another character. He appeared in an episode of "Here Come the Brides", as well as two episodes each of "Love, American Style" and "My Two Dads". Jones also appeared in animated form as himself in 1972 in an hour-long episode of "The New Scooby-Doo Movies". Other television appearances include "Sledge Hammer!", "Boy Meets World", "Hey Arnold!", "The Single Guy" (where he is mistaken for Dudley Moore) and "Sabrina, the Teenage Witch" in which he sang "Daydream Believer" to Sabrina (Melissa Joan Hart). In 2009, Jones made a cameo appearance as himself in the "SpongeBob SquarePants" episode "SpongeBob vs. The Big One" (his appearance was meant to be a pun on Davy Jones' Locker).
Jones also returned to theatre several times after the Monkees. He appeared in several productions of "Oliver!" as the Artful Dodger, and in 1989 toured the US portraying Fagin. Jones also co-starred with Micky Dolenz in Harry Nilsson's play "The Point" at the Mermaid Theatre, London in 1978.
Despite his initial high profile after the end of the Monkees, Jones struggled to establish himself as a solo artist. Glenn A. Baker, author of "Monkeemania: The True Story of the Monkees", commented in 1986 that "for an artist as versatile and confident as (Davy) Jones, the relative failure of his post-Monkees activities is puzzling. For all his cocky predictions to the press about his future plans, Davy fell into a directionless heap when left to his own devices."
The continued popularity of his 1971 "Brady Bunch" appearance led to his being cast as himself in "The Brady Bunch Movie". Jones sang his signature solo hit "Girl", with a grunge band providing backing, this time with middle-aged women swooning over him. Micky Dolenz and Peter Tork also appeared alongside Jones as judges.
In 2001, Jones released "Just Me", an album of his own songs, some written for the album and others originally on Monkees releases. In the early 2000s he was performing in the Flower Power Concert Series during Epcot's Flower and Garden Festival, a yearly gig he would continue until his death.
In April 2006, Jones recorded the single "Your Personal Penguin", written by children's author Sandra Boynton, as a companion piece to her new board book of the same title. On 1 November 2007, the Boynton book and CD titled "Blue Moo" was released and Jones is featured in both the book and CD, singing "Your Personal Penguin". In 2009, Jones released a collection of classics and standards from the 1940s through the 1970s entitled "She".
In December 2008, "Yahoo! Music" named Jones the "Number 1 teen idol of all time". In 2009, Jones was rated second in a list of 10 best teen idols compiled by Fox News.
Personal life.
Family life.
Jones was married three times. In December 1968 he married Dixie Linda Haines, with whom he had been living. Their relationship had been kept out of the public eye until after the birth of their first child in October 1968; when it was finally made public, it caused a considerable backlash for Jones from his fans. Jones later stated, in "Tiger Beat" magazine, "I kept my marriage a secret because I believe stars should be allowed a private life." Jones and Haines had two daughters: Talia Elizabeth (born 2 October 1968) and Sarah Lee (born 3 July 1971). The marriage ended in 1975.
Jones married his second wife, Anita Pollinger, on 24 January 1981, and also had two daughters with her: Jessica Lillian (born 4 September 1981) and Annabel Charlotte (born 26 June 1988). They divorced in 1996, during the Monkees' 30th Anniversary reunion tour. Jones married for a third time, on 30 August 2009, to Jessica Pacheco, 32 years his junior. This was Pacheco's third marriage as well. On 28 July 2011, Pacheco filed to divorce Jones in Miami-Dade County, Florida, but dropped the suit in October. They were still married when he died in February 2012. Pacheco was omitted from Jones's will, which he made before their marriage. His oldest daughter, Talia, whom he named his executor, was granted by the court the unusual request that her father's will be sealed, on the basis that “planning documents and financial affairs as public opinion could have a material effect on his copyrights, royalties and ongoing goodwill.”
Horse racing.
In addition to his career as an entertainer, Jones' other first love was horses. Training as a jockey in his teens, Jones later said, "I made one huge mistake. When the Monkees finished in 1969–70, I should have got away from Hollywood and got back into the racing game. Instead I waited another 10 years. Everyone makes mistakes in life and for me that was the biggest." He held an amateur rider's licence and rode in his first race at Newbury in Berkshire, England for trainer Toby Balding.
On 1 February 1996, he won his first race, on Digpast, in the one-mile Ontario Amateur Riders Handicap at Lingfield in Surrey. Jones also had horse ownership interests in both the U.S. and the U.K., and served as a commercial spokesman for Colonial Downs racetrack in Virginia. In tribute to Jones, Lingfield Park announced that the first two races on the card for 3 March 2012 would be renamed the "Hey Hey We're The Monkees Handicap" and the "In Memory of Davy Jones Selling Stakes" with successful horses in those races accompanied into the Winners' Enclosure by some of the Monkees' biggest hits. Plans were also announced to erect a plaque to commemorate Jones next to a Monkey Puzzle tree on the course.
Death.
On the morning of 29 February 2012, Jones went to tend his 14 horses at a farm in Indiantown, Florida. After riding one of his favourite horses around the track, he complained of chest pains and difficulty breathing and was rushed to Martin Memorial South Hospital in Stuart, Florida, where he was pronounced dead of a severe heart attack due to atherosclerosis.
On Wednesday, 7 March 2012, a private funeral service was held at Holy Cross Catholic parish in Indiantown, Florida. The three surviving Monkees did not attend in order not to draw more attention to the grieving family. Instead, the group attended memorial services in New York City as well as organising their own private memorial in Los Angeles along with David's family and close friends. Additionally, a public memorial service was held on 10 March 2012 in Beavertown, Pennsylvania, near a church Jones had purchased for future renovation.
On Monday 12 March, a private memorial service was held in Openshaw, Manchester at Lees Street Congregational Church where Jones performed as a child in church plays. Jones' wife and daughters travelled to Britain to join his British-based relatives for the service, and also placed his ashes on his parents graves for a little while.
Books.
"Mutant Monkees Meet the Masters of the Multi-Media Manipulation Machine!" [Paperback] Samuel French Trade (June 1992)
ISBN 978-0-9631235-0-3

</doc>
<doc id="9142" url="http://en.wikipedia.org/wiki?curid=9142" title="Discharge">
Discharge

Discharge in the context to expel or to "let go" may refer to:
Discharge in the context of a flow may refer to:
Other uses of discharge include:

</doc>
<doc id="9146" url="http://en.wikipedia.org/wiki?curid=9146" title="Dolly (sheep)">
Dolly (sheep)

Dolly (5 July 1996 – 14 February 2003) was a female domestic sheep, and the first mammal to be cloned from an adult somatic cell, using the process of nuclear transfer. She was cloned by Ian Wilmut, Keith Campbell and colleagues at the Roslin Institute, part of the University of Edinburgh, Scotland, and the biotechnology company PPL Therapeutics, based near Edinburgh. The funding for Dolly's cloning was provided by PPL Therapeutics and the Ministry of Agriculture. She was born on 5 July 1996 and died from a progressive lung disease 5 months before her seventh birthday. She has been called "the world's most famous sheep" by sources including BBC News and "Scientific American".
The cell used as the donor for the cloning of Dolly was taken from a mammary gland, and the production of a healthy clone therefore proved that a cell taken from a specific part of the body could recreate a whole individual. On Dolly's name, Wilmut stated "Dolly is derived from a mammary gland cell and we couldn't think of a more impressive pair of glands than Dolly Parton's".
Birth.
Dolly was born on 5 July 1996 and had three mothers (one provided the egg, another the DNA and a third carried the cloned embryo to term). She was created using the technique of somatic cell nuclear transfer, where the cell nucleus from an adult cell is transferred into an unfertilised oocyte (developing egg cell) that has had its cell nucleus removed. The hybrid cell is then stimulated to divide by an electric shock, and when it develops into a blastocyst it is implanted in a surrogate mother. Dolly was the first clone produced from a cell taken from an adult mammal. The production of Dolly showed that genes in the nucleus of such a mature differentiated somatic cell are still capable of reverting to an embryonic totipotent state, creating a cell that can then go on to develop into any part of an animal. Dolly's existence was announced to the public on 22 February 1997. It gained much attention in the media. A commercial with Scottish scientists playing with sheep was aired on TV, and a special report in TIME Magazine featured Dolly the sheep. "Science" featured Dolly as the breakthrough of the year. Even though Dolly was not the first animal to be cloned, she gained this attention in the media because she was the first to be cloned from an adult cell.
Life.
Dolly lived her entire life at the Roslin Institute in Edinburgh. There she was bred with a Welsh Mountain ram and produced six lambs in total. Her first lamb, named Bonnie, was born in April 1998. The next year Dolly produced twin lambs Sally and Rosie, and she gave birth to triplets Lucy, Darcy and Cotton in the year after that. In late 2001, at the age of four, Dolly developed arthritis and began to walk stiffly. This was treated with anti-inflammatory drugs.
Death.
On 14 February 2003, Dolly was euthanised because she had a progressive lung disease and severe arthritis. A Finn Dorset such as Dolly has a life expectancy of around 11 to 12 years, but Dolly lived to be 6.5 years old. A post-mortem examination showed she had a form of lung cancer called Jaagsiekte, which is a fairly common disease of sheep and is caused by the retrovirus JSRV. Roslin scientists stated that they did not think there was a connection with Dolly being a clone, and that other sheep in the same flock had died of the same disease. Such lung diseases are a particular danger for sheep kept indoors, and Dolly had to sleep inside for security reasons.
Some in the press speculated that a contributing factor to Dolly's death was that she could have been born with a genetic age of six years, the same age as the sheep from which she was cloned. One basis for this idea was the finding that Dolly's telomeres were short, which is typically a result of the ageing process. The Roslin Institute have stated that intensive health screening did not reveal any abnormalities in Dolly that could have come from advanced aging.
Legacy.
After cloning was successfully demonstrated through the production of Dolly, many other large mammals were cloned, including pigs, deer, horses and bulls. The attempt to clone argali (mountain sheep) did not produce viable embryos. The attempt to clone a banteng bull was more successful, as were the attempts to clone mouflon (a form of wild sheep), both resulting in viable offspring. The reprogramming process cells need to go through during cloning is not perfect and embryos produced by nuclear transfer often show abnormal development. Making cloned mammals was highly inefficient (Dolly was the only lamb that survived to adulthood from 277 attempts - although by 2014 Chinese scientists were reported to have 70–80% success rates cloning pigs) Wilmut, who led the team that created Dolly, announced in 2007 that the nuclear transfer technique may never be sufficiently efficient for use in humans.
Cloning may have uses in preserving endangered species and may become a viable tool for reviving extinct species. In January 2009, scientists from the Centre of Food Technology and Research of Aragon, in Zaragoza, northern Spain announced the cloning of the Pyrenean ibex, a form of wild mountain goat, which was officially declared extinct in 2000. Although the newborn ibex died shortly after birth due to physical defects in its lungs it is the first time an extinct animal has been cloned, and may open doors for saving endangered and newly extinct species by resurrecting them from frozen tissue.

</doc>
<doc id="9156" url="http://en.wikipedia.org/wiki?curid=9156" title="Dolores Fuller">
Dolores Fuller

Dolores Agnes Fuller (née Eble; March 10, 1923 – May 9, 2011) was an American actress and songwriter best known as the one-time girlfriend of the low-budget film director Ed Wood. She played the protagonist's girlfriend in "Glen or Glenda", co-starred in Wood's "Jail Bait", and had a minor role in his "Bride of the Monster". Later, Elvis Presley recorded a number of her songs written for his films.
Film career.
Her first screen appearance was at the age of 10, when she appeared briefly in Frank Capra’s "It Happened One Night". According to Fuller, the female lead in "Bride of the Monster" was written for her but Wood gave it to Loretta King instead.
In August 1954, Fuller was cast in Wood's "The Vampire's Tomb", intended to star Bela Lugosi. Frank Yaconelli was named as her co-star and 'comic killer'. The film was never made. She ended up making an appearance in "Bride of the Monster" (1956), also with Lugosi. Fuller hosted a benefit for Lugosi which preceded the showing of "Bride of the Atom" (early working title of "Bride of the Monster") on May 11, 1955. A cocktail party was held at the Gardens Restaurant at 4311 Magnolia Avenue in Burbank, California. Vampira attended and was escorted by Paul Marco. A single screening of the film was presented at the Hollywood Paramount.
According to Fuller, as quoted in Wood biography "Nightmare of Ecstasy" (1992), she first met Ed Wood when she attended a casting call with a friend for a movie he was supposed to direct called "Behind Locked Doors"; it has also been stated that they met in a restaurant.
She became his girlfriend shortly thereafter and began acting in his films. Her movie career included a bit part in "It Happened One Night" (1934) and roles in "Outlaw Women" (1952), "Glen or Glenda" (1953), "Body Beautiful" (1953), "The Blue Gardenia" (1953), "Count the Hours" (1953), "Mesa of Lost Women" (1953), "College Capers" (1954), "Jail Bait" (1954), "The Raid" (1954), "This Is My Love" (1954), "The Opposite Sex" (1956), "The Ironbound Vampire" (1997), and "Dimensions in Fear" (1998).
Television performer and songwriter.
Fuller had already had earlier experience on television in "Queen for a Day" and "The Dinah Shore Show". As Fuller remembered, she was the one "putting bread on the table." Another quote from her: "I had a size four and a half foot, so I modeled the slippers in an artist's short smock."
She lost her job on "The Dinah Shore Show" when, as she said, "We were shooting all night, and into the next day, and time just got away from me, and I didn't realize that I was supposed to be on the set working as Dinah's double on her show, Chevy Theatre. I completely messed up my job, I was what they called a no show." She also appeared on an episode of "It's a Great Life" as "the blonde in the mink coat."
Fuller's ability as a songwriter manifested itself through the intervention of her friend, producer Hal Wallis; Fuller had wanted to get an acting role in the Elvis Presley movie "Blue Hawaii", which Wallis was producing, but instead he put her in touch with Hill & Range, the publisher that provided Presley with songs. Fuller went into a collaborative partnership with composer Ben Weisman and co-wrote one song, "Rock-A-Hula Baby", for the film. It was a beginning that eventually led to Elvis Presley recording a dozen of her songs, including "I Got Lucky" and "Spinout". Fuller also had her music recorded by Nat 'King' Cole, Peggy Lee, and other leading talents of the time. 
Private life.
Grey quotes Fuller as saying of the period before her success, "He (Ed Wood) begged me to marry him. I loved him in a way, but I couldn't handle the transvestism. I'm a very normal person. It's hard for me to deviate! I wanted a man that was all man. After we broke up, he would stand outside my home in Burbank and cry. "Let me in, I love you!" What good would I have done if I had married him? We would have starved together. I bettered myself. I had to uplift myself." She has also been quoted as saying that "His dressing up didn’t bother me — we all have our little queer habits" and giving Wood's drinking as the reason for their breakup.
Fuller's autobiography, "A Fuller Life: Hollywood, Ed Wood and Me", co-authored by Winnipeg writer Stone Wallace and her husband Philip Chamberlin, was published in 2008.
Portrayal in "Ed Wood".
Fuller was portrayed by Sarah Jessica Parker in Tim Burton's 1994 Wood biographical film "Ed Wood", a portrayal of which she disapproved due to the image of her smoking in the film. Fuller says she never smoked. She also complained that she was only portrayed as "sort of as an actress" and did not feel she was given credit for her other accomplishments and contributions towards Wood's career. However, she stated that she liked the film overall, praising Johnny Depp's performance in the title role.
Discography.
Songs recorded by Elvis Presley with lyrics by Dolores Fuller:
According to AllMusic, other songs co-written by her include "I'll Touch a Star" by Terry Stafford, "Lost Summer Love" by Shelley Fabares and "Someone to Tell It To" by Nat King Cole.

</doc>
<doc id="9160" url="http://en.wikipedia.org/wiki?curid=9160" title="De jure">
De jure

De jure (, ; Classical Latin de iúre ) is an expression that means "concerning law", as contrasted with "de facto", which means "concerning fact". The terms "de jure" and "de facto" are used instead of "in law" and "in practice", respectively, when one is describing political or legal situations.
In a legal context, "de jure" is also translated as "concerning law". A practice may exist de facto, where, for example, the people obey a contract as though there were a law enforcing it, yet there is no such law. A process known as "desuetude" may allow (de facto) practices to replace (de jure) laws that have fallen out of favor, locally. 
Examples.
It is possible to have multiple simultaneous conflicting (de jure) legalities, possibly none of which is in force ("de facto"). After seizing power in 1526, Ahmad ibn Ibrahim al-Ghazi made his brother, Umar Din, the lawful ("de jure") Sultan of Adal. Ahmad, however, was in practice ("de facto") the actual Sultan, and his brother was a figurehead. Between 1805 and 1914, the ruling dynasty of Egypt ruled as "de jure" viceroys of the Ottoman Empire, but acted as "de facto" independent rulers who maintained a polite fiction of Ottoman suzerainty. However, from about 1882, the rulers had only "de jure" rule over Egypt, as it had by then become a British puppet state. Thus, Egypt was by Ottoman law "de jure" a province of the Ottoman Empire, but "de facto" was part of the British Empire.
In American law, particularly after "Brown v. Board of Education" (1954), the difference between "de facto" segregation (segregation that existed because of the voluntary associations and neighborhoods) and "de jure" segregation (segregation that existed because of local laws that mandated the segregation), became important distinctions for court-mandated remedial purposes.
In Canada, cannabis is illegal in law, but there is widespread use in practice, especially in British Columbia. It is strikingly similar to the existence of speakeasies during prohibition, wherein enforcement of laws departed from the letter, because of widespread and popular practice.

</doc>
<doc id="9163" url="http://en.wikipedia.org/wiki?curid=9163" title="Des Moines, Iowa">
Des Moines, Iowa

Des Moines is the capital and the most populous city in the U.S. state of Iowa. It is also the county seat of Polk County. A small portion of the city extends into Warren County. It was incorporated on September 22, 1851, as Fort Des Moines which was shortened to "Des Moines" in 1857. It is named after the Des Moines River, which may have been adapted from the French "Rivière des Moines", meaning "River of the Monks." The city's population was 203,433 as of the 2010 census. The five-county metropolitan area is ranked 88th in terms of population in the United States with 594,600 residents according to the 2014 estimate by the United States Census Bureau.
Des Moines is a major center of the U.S. insurance industry and has a sizable financial services and publishing business base. In fact, Des Moines was credited as the "number one spot for U.S. insurance companies" in a Business Wire article and named the third largest "insurance capital" of the world. The city is the headquarters for the Principal Financial Group, Athene USA insurance, the Meredith Corporation, Ruan Transportation, EMC Insurance Companies, and Wellmark Blue Cross Blue Shield. Other major corporations such as Wells Fargo, Voya Financial, Nationwide Mutual Insurance Company, ACE Limited, Marsh, Monsanto and Pioneer Hi-Bred have large operations in or near the metro area. In recent years Microsoft, Hewlett Packard and Facebook have established data processing and logistical facilities in the Des Moines metro. "Forbes" magazine ranked Des Moines as the "Best Place for Business" in both 2010 and 2013. In 2014, NBC ranked Des Moines as the "Wealthiest City in America" according to its criteria.
Des Moines is an important city in U.S. presidential politics because it is the capital of Iowa, which is home to the first caucuses of the presidential primary cycle. Hence, many presidential candidates set up campaign headquarters in Des Moines. A 2007 article in "The New York Times" stated "if you have any desire to witness presidential candidates in the most close-up and intimate of settings, there is arguably no better place to go than Des Moines."
Etymology.
Des Moines takes its name from Fort Des Moines (1843–46), which was named for the Des Moines River. The French "des Moines" (pronounced ) translates literally to either "from the monks" or "of the monks".
The historian Virgil Vogel claimed that "Moingona" was derived from the Algonquian clan name "Loon".
Some historians and researchers lacking linguistic or Algonquianist training concluded that "Moingona" meant "people by the portage" or something similar, a reference to the Des Moines Rapids, where the earliest meetings between the Moingona and European explorers took place.
One popular interpretation of "Des Moines" ignores Vogel's research, and concludes that "Des Moines" refers to French Trappist monks, who lived in huts on top of what is now known as Monks Mound near St. Louis some from the Des Moines River.
A controversial recent hypothesis using a study of Miami-Illinois tribal names concludes the word "Moingona" comes from "mooyiiinkweena", a derogatory name which translates roughly to "the excrement-faces." The name was apparently given to Marquette and Joliet by a tribal leader to dissuade them from doing business with a neighboring tribe.
Prehistory.
Prehistoric inhabitants of early Des Moines.
The juncture of the Des Moines and Raccoon rivers has attracted humans for at least 7,000 years. Several prehistoric occupation areas have been identified in downtown Des Moines by archaeologists. Discovered in December 2010, the "Palace" is an expansive 7,000-year-old site located at the new wastewater treatment plant in southeastern Des Moines. It contains well-preserved house deposits and numerous graves. State of Iowa archaeologist John Doershuk was assisted by the University of Iowa archaeologists at this dig.
At least three Late Prehistoric villages stood in or near downtown Des Moines, dating from about AD 1300 to 1700. In addition, 15 to 18 prehistoric American Indian mounds were observed in downtown Des Moines by early settlers. All have been destroyed.
History.
Origin of Fort Des Moines.
The City of Des Moines traces its origins to May 1843, when Captain James Allen supervised the construction of a fort on the site where the Des Moines and Raccoon Rivers merge. Allen wanted to use the name Fort Raccoon; however, the U.S. War Department told him to name it Fort Des Moines. The fort was built to control the Sauk and Meskwaki Indians, who had been transplanted to the area from their traditional lands in eastern Iowa. The fort was abandoned in 1846 after the Sauk and Meskwaki were removed from the state.
The Sauk and Meskwaki did not fare well in Des Moines. The illegal whiskey trade, combined with the destruction of traditional lifeways, led to severe problems. One newspaper reported: "It is a fact that the location of Fort Des Moines among the Sac and Fox Indians (under its present commander) for the last two years, had corrupted them more and lowered them deeper in the scale of vice and degradation, than all their intercourse with the whites for the ten years previous."
Even after official removal, the Meskwaki continued to return to Des Moines until ca. 1857. Archaeological excavations have demonstrated that many fort-related features survived under what is now Martin Luther King, Jr. Parkway and First Street. Soldiers stationed at Fort Des Moines opened the first coal mines in the area, mining coal from the riverbank for the fort's blacksmith.
Early settlement.
Settlers occupied the abandoned fort and nearby areas. On May 25, 1846, Fort Des Moines became the seat of Polk County. Arozina Perkins, a school teacher who spent the winter of 1850–1851 in the town of Fort Des Moines, was not favorably impressed:
This is one of the strangest looking "cities" I ever saw... This town is at the juncture of the Des Moines and Raccoon rivers. It is mostly a level prairie with a few swells or hills around it. We have a court house of "brick", and one church, a plain, framed building belonging to the Methodists. There are two taverns here, one of which has a most important little bell that rings together some fifty boarders. I cannot tell you how many dwellings there are, for I have not counted them; some are of logs, some of brick, some framed, and some are the remains of the old dragoon houses...The people support two papers and there are several dry goods shops. I have been into but four of them... Society is as varied as the buildings are. There are people from nearly every state, and Dutch, Swedes, etc.
In May 1851, much of the town was destroyed during the Flood of 1851. "The Des Moines and Raccoon rivers rose to an unprecedented height, inundating the entire country east of the Des Moines river. Crops were utterly destroyed, houses and fences swept away." This flood provided a clean slate for the city to grow on.
Era of growth.
On September 22, 1851, Des Moines was incorporated as a city, the charter was approved by voters on October 18. In 1857, the name "Fort Des Moines" was shortened to "Des Moines" and the state capital was moved here from Iowa City.
Growth was slow during the Civil War period, but the city exploded in size and importance after a railroad link was completed in 1866.
In 1864, the Des Moines Coal Company was organized to begin the first systematic mining in the region. Its first mine, north of town on the west side of the river, was exhausted by 1873. The Black Diamond mine, near the south end of the West Seventh Street Bridge, sunk a mine shaft to reach a coal bed. By 1876, this mine employed 150 men and shipped 20 carloads of coal per day. By 1885, there were numerous mine shafts within the city limits, and mining began to spread into the surrounding countryside. By
1893, there were 23 mines in the region. By 1908, the coal resources of Des Moines were largely exhausted. Nonetheless, in 1912, Des Moines was home to eight locals of the United Mine Workers union representing a total of 1,410 miners. This represents about 1.7 percent of the city's population in 1910.
By 1900, Des Moines was Iowa's largest city with a population of 62,139. In 1910, the Census Bureau reported Des Moines' population as 97.3% white and 2.7% black.
"City Beautiful", industrial decline, and rebirth.
At the turn of the 20th century, Des Moines undertook a "City Beautiful" project in which large Beaux Arts public buildings and fountains were constructed along the Des Moines River. The former Des Moines Public Library building (now the home of the World Food Prize), the central Post Office (now Polk County Administrative Building with a newer addition attached), and the City Hall are surviving examples of the 1900–1910 buildings. They form the Civic Center Historic District.
The ornate riverfront balustrade that still line the Des Moines and Raccoon River were built by the federal Civilian Conservation Corps in the mid-1930s. The ornamental fountains that once stood along the riverbank were buried in the 1950s, when the city began a post-industrial decline which lasted until the late 1980s. The city has since rebounded, transforming from a blue-collar industrial city to a white-collar professional city.
In 1907, the city adopted a city commission government known as the Des Moines Plan, comprising an elected mayor and four commissioners who were responsible for public works, public property, public safety, and finance. This form of government was scrapped in 1950 in favor of a council-manager government, and further changed in 1967 so that four of the seven city council members were elected by ward rather than at-large. As with many major urban areas, the city core began losing population to the suburbs in the 1960s (the peak population of 208,982 was recorded in 1960). The population was 198,682 in 2000 and grew slightly to 200,538 in 2009. However, the growth of the outlying suburbs has been a constant and the overall metro area population is over 560,000 today.
During the Great Flood of 1993, heavy rains throughout June and early July caused the Des Moines and Raccoon Rivers to rise above flood stage levels. The Des Moines Water Works was submerged by flood waters during the early morning hours of July 11, 1993 leaving an estimated 250,000 people without running water for 12 days and without drinking water for 20 days. Des Moines suffered major flooding again in June 2008 with a major levee breach. The Des Moines river is controlled upstream by Saylorville Reservoir, but in both 1993 and 2008 the river overtopped the reservoir spillway.
Today, Des Moines is a member of ICLEI Local Governments for Sustainability USA. Through ICLEI, Des Moines has implemented "The Tomorrow Plan", a regional plan focused on developing central Iowa in a sustainable fashion, centrally planning growth and resource consumption to manage the local population.
Cityscape.
The skyline of Des Moines changed during the 1970s and 1980s as several new skyscrapers were built. Before then, the 19-story Equitable Building, from 1924, was the tallest building in the city and the tallest building in Iowa. The 25-story Financial Center was completed in 1973 and the 36-story Ruan Center was completed in 1974. They were later joined by the 33-story Des Moines Marriott Hotel (1981), the 25-story HUB Tower and 25-story Plaza Building (1985), Iowa's tallest building, Principal Financial Group's 45-story tower at 801 Grand (1991), and the 19-story EMC Insurance Building (1997).
This time period also saw the opening of the Civic Center of Greater Des Moines (1979) which hosts Broadway shows and special events, the Greater Des Moines Botanical Garden (1979) which is a large city botanical garden/greenhouse on the east side of the river, the Polk County Convention Complex (1985), and the State of Iowa Historical Museum (1987). The Des Moines skywalk system also began to take shape during the 1980s. The skywalk system is long and connects many downtown buildings.
The city is in the midst of major construction in the downtown area. The new Science Center of Iowa and Blank IMAX Dome Theater and the Iowa Events Center opened in 2005, while the new central branch of the Des Moines Public Library, designed by renowned architect David Chipperfield of London, opened on April 8, 2006.
The World Food Prize Foundation, which is based in Des Moines, completed restoration of the former Des Moines Public Library building in October 2011. The former library now serves as the home and headquarters of the Dr. Norman Borlaug/World Food Prize Hall of Laureates.
In 2002, the Principal Financial Group and the city announced plans for the Principal Riverwalk, which will feature trails, pedestrian bridges across the river, a fountain and skating plaza, and a "civic garden" in front of the City Hall. Multiple existing downtown buildings are being converted to loft apartments and condominiums. This trend is highlighted by the success of the East Village district of shops, studios, and housing between the Capitol district and the Des Moines River.
Geography.
Des Moines is located at (41.590939, −93.620866). According to the United States Census Bureau, the city has a total area of , of which, is land and is water. It is 850 feet above sea level at the confluence of the Racoon and Des Moines Rivers.
In November 2005, Des Moines voters approved a measure that allowed the city to involuntarily annex certain parcels of land in the northeast, southeast, and southern corners of Des Moines, particularly areas bordering the Iowa Highway 5/U.S. 65 bypass. The annexations became official on June 26, 2009, as 5,174 acres (9.27 square miles) and approximately 868 new residents were added to the city of Des Moines. An additional 759 acres (1.18 square miles) were voluntarily annexed to the city over that same period of time.
Metropolitan area.
The Des Moines-West Des Moines Metropolitan Statistical Area consists of five central Iowa counties: Polk, Dallas, Warren, Madison, and Guthrie. The area had a 2000 census population of 481,394 and an estimated 2009 population of 562,906. The Des Moines-Newton-Pella Combined Statistical Area consists of those five counties plus Jasper and Marion counties; the 2000 census population of this area was 550,659, and the estimated 2009 population was 631,805. Des Moines' suburban communities include Altoona, Ankeny, Bondurant, Carlisle, Clive, Grimes, Johnston, Norwalk, Pleasant Hill, Urbandale, Waukee, West Des Moines, and Windsor Heights.
Climate.
Being located near the center of North America, far removed from a large body of water, the Des Moines area has a hot summer type humid continental climate (Köppen "Dfa"), with hot, humid summers and cold, snowy winters. Summer temperatures can often climb into the range, occasionally reaching . Humidity can be high in spring and summer, with frequent afternoon thunderstorms. Fall brings pleasant temperatures and colorful fall foliage. Winters vary from moderately cold to bitterly cold, with low temperatures venturing below quite often. Snowfall averages per season, and annual precipitation averages , with a peak in the warmer months.
Demographics.
2010 census.
As of the census of 2010, there were 203,433 people, 81,369 households, and 47,491 families residing in the city. The population density was . There were 88,729 housing units at an average density of . The racial makeup of the city was 76.4% White, 10.2% African American, 0.5% Native American, 4.4% Asian (1.2% Vietnamese, 0.9% Laotian, 0.4% Burmese, 0.3% Asian Indian, 0.3% Thai, 0.2% Chinese, 0.2% Cambodian, 0.2% Filipino, 0.1% Hmong, 0.1% Korean, 0.1% Nepalese), 0.1% Pacific Islander, 5.0% from other races, and 3.4% from two or more races. People of Hispanic or Latino origin, of any race, formed 12.0% of the population (9.4% Mexican, 0.7% Salvadoran, 0.3% Guatemalan, 0.3% Puerto Rican, 0.1% Honduran, 0.1% Ecuadorian, 0.1% Cuban, 0.1% Spaniard, 0.1% Spanish). Non-Hispanic Whites were 70.5% of the population in 2010, compared to 87.8% in 1990.
There were 81,369 households of which 31.6% had children under the age of 18 living with them, 38.9% were married couples living together, 14.2% had a female householder with no husband present, 5.3% had a male householder with no wife present, and 41.6% were non-families. 32.5% of all households were made up of individuals and 9.4% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.11.
The median age in the city was 33.5 years. 24.8% of residents were under the age of 18; 10.9% were between the ages of 18 and 24; 29.4% were from 25 to 44; 23.9% were from 45 to 64; and 11% were 65 years of age or older. The gender makeup of the city was 48.9% male and 51.1% female.
2000 census.
As of the census of 2000, there were 198,682 people, 80,504 households, and 48,704 families residing in the city. The population density was 2,621.3 people per square mile (1,012.0/km²). There were 85,067 housing units at an average density of 1,122.3 per square mile (433.3/km²). The racial makeup of the city was 82.3% white, 8.07% Black, 0.35% American Indian, 3.50% Asian, 0.05% Pacific Islander, 3.52% from other races, and 2.23% from two or more races. 6.61% of the population were Hispanic or Latino of any race. 20.9% were of German, 10.3% Irish, 9.1% "American" and 8.0% English ancestry, according to Census 2000.
There were 80,504 households out of which 29.5% had children under the age of 18 living with them, 43.7% were married couples living together, 12.6% had a female householder with no husband present, and 39.5% were non-families. 31.9% of all households were made up of individuals and 10.2% had someone living alone who was 65 years of age or older. The average household size was 2.39 and the average family size was 3.04.
Age spread: 24.8% under the age of 18, 10.6% from 18 to 24, 31.8% from 25 to 44, 20.4% from 45 to 64, and 12.4% who were 65 years of age or older. The median age was 34 years. For every 100 females, there were 93.8 males. For every 100 females age 18 and over, there were 90.5 males.
The median income for a household in the city was $38,408, and the median income for a family was $46,590. Males had a median income of $31,712 versus $25,832 for females. The per capita income for the city was $19,467. About 7.9% of families and 11.4% of the population were below the poverty line, including 14.9% of those under age 18 and 7.6% of those ages 65 or over.
Economy.
Many insurance companies are headquartered in Des Moines, including the Principal Financial Group, EMC Insurance Group, Allied Insurance, Wellmark Blue Cross and Blue Shield of Iowa, Athene, Holmes Murphy, and American Republic Insurance Company. Des Moines has been referred to as the "Hartford of the West" because of this. The Principal is one of two Fortune 500 companies having headquarters in Iowa, ranking 273rd on the magazine's list in 2009.
As a center of financial and insurance services, other major corporations headquartered outside of Iowa have established a presence in the Des Moines Metro area, including Wells Fargo, Voya Financial, and Electronic Data Systems. The Meredith Corporation, a leading publishing and marketing company, is also based in Des Moines. Meredith publishes "Better Homes and Gardens", one of the most widely circulated publications in the United States.
Other major employers in Des Moines include UnityPoint Health, Mercy Medical Center, MidAmerican Energy Company, CDS Global, UPS, Firestone Agricultural Tire Company, EDS, Dahl's Foods, Drake University, Titan Tire, "The Des Moines Register", Anderson Erickson, Dee Zee and EMCO.
In 2010, "Forbes" magazine ranked the Des Moines metropolitan area first on its list of "Best Places For Business And Careers," based on factors such as the cost of doing business, cost of living, educational attainment, and crime rate.
Culture.
Arts and theatre.
 The City of Des Moines is a cultural center for Iowa and home to several art and history museums and performing arts groups. The Des Moines Performing Arts routinely hosts Broadway shows and other live professional theater. Its president and CEO, Jeff Chelsvig, is a member of the League of American Theatres and Producers, Inc. The Temple for Performing Arts and Des Moines Playhouse are other venues for live theatre, comedy, and performance arts.
The Des Moines Metro Opera has been a respected cultural resource in Des Moines since 1973. The Opera offers award-winning educational and outreach programs and is one of the largest performing arts organizations in the state. Ballet Des Moines was established in 2002. Currently performing three productions each year, the Ballet also provides opportunities for education and outreach.
The Des Moines Symphony performs frequently at different venues. In addition to performing seven pairs of classical concerts each season, the Symphony also entertains with New Year's Eve Pops and its annual Yankee Doodle Pops concerts.
The Metro Arts Alliance produces ' every year, that offers free jazz shows daily at various venues throughout the city during the entire month of July.
Wells Fargo Arena is the Des Moines area's primary venue for sporting events and concerts since its opening in 2005. Named for title sponsor Wells Fargo Financial Services, Wells Fargo Arena holds 16,980 and books large, national touring acts for arena concert performances, while several smaller venues host local, regional, and national bands. It is the home of the Iowa Energy of the NBA Development League, the Iowa Wild of the American Hockey League, and the Iowa Barnstormers of the Indoor Football League.
The Simon Estes Riverfront Amphitheater is an outdoor concert venue located on the east bank of the Des Moines River which hosts music events such as the Alive Concert Series.
The Des Moines Art Center, with a wing designed by architect I.M. Pei, presents art exhibitions and educational programs as well as hands-on studio art classes. The Center houses an internationally renowned collection of artwork from the 19th century to the present. An extension of the world-renowned art center is located downtown in an energetic urban museum space, featuring three or four exciting and fresh exhibitions each year. A Museum shop offers unique gifts, jewelry, cards, and books.
For the first time in Iowa history, a play was performed at the Iowa State Capitol on August 23, 2014. 'Lincoln's Last Interview', written and directed by Emmy nominee Brent Roske, was performed in the House of Representatives chamber with the audience seated at the desks. Iowa news host Elizabeth Klinge played a reporter interviewing Abraham and Mary Lincoln, played by Matthew McIver and Mary Bricker.
Dedicated September 27, 2009, the Pappajohn Sculpture Park located in Western Gateway Park from 10th to 15th Streets and between Grand Avenue and Locust Street, showcases a collection of 24 world class sculptures valued at more than $40 million donated by Des Moines philanthropists John and Mary Pappajohn. Resting on of green space, the sculpture park is designed as an outdoor art museum. Nearby is the beautifully restored and historic Temple for Performing Arts, reborn as a cultural center for the city. Next to the Temple is the Central Library, with its ultramodern, freeform architecture and "organic" roof designed by renowned English architect David Chipperfield.
Salisbury House and Gardens is a 42-room historic house museum located on of woodlands in the South of Grand neighborhood of Des Moines. It is named after—and loosely inspired by—King's House in Salisbury, England. Built in the 1920s by cosmetics magnate Carl Weeks and his wife, Edith, the Salisbury House contains authentic 16th-century English oak and rafters dating to Shakespeare's days, numerous other architectural features re-purposed from other historic English homes, and an internationally significant collection of original fine art, tapestries, decorative art, furniture, musical instruments, and rare books and documents. The Salisbury House is listed on the National Register of Historic Places, and has been featured on A&E's "America's Castles" and PBS's "Antiques Roadshow." Prominent artists in the Salisbury House collection include Joseph Stella, Lillian Genth, Anthony van Dyck and Lawrence Alma-Tadema.
Built in 1877 by prominent pioneer businessman Hoyt Sherman, Hoyt Sherman Place mansion was Des Moines' first public art gallery and houses a distinctive collection of 19th and 20th century artwork. Its restored 1,250-seat theater features an intricate rococo plaster ceiling and excellent acoustics and is used for a variety of cultural performances and entertainment.
Attractions.
Arising in the east and facing westward toward downtown, the Iowa State Capitol building with its , 23-karat gold leafed dome towering above the city is a favorite of sightseers. Four smaller domes flank the main dome. The Capitol houses the governor's offices, legislature, and the old Supreme Court Chambers. The ornate interior also features a grand staircase, mural "Westward", five-story law library, scale model of the "USS Iowa", and collection of first lady dolls. Guided tours are available. The Capitol grounds include a World War II memorial with sculpture and Wall of Memories. Other monuments include the 1894 Soldiers and Sailors Monument of the Civil War and memorials honoring those who served in the Spanish-American, Korean, and Vietnam Wars.
The West Capitol Terrace provides a stunning entrance from the west to the state's grandest building, the State Capitol Building. With its picturesque views, the lush, "people's park" at the foot of the Capitol complex includes a promenade and landscaped gardens, in addition to providing public space for rallies and special events. A granite map of Iowa depicting all 99 counties rests at the base of the terrace and has become a popular attraction for in-state visitors, many of whom can be seen walking over the map to find their home county.
Iowa's history lives on in the State of Iowa Historical Museum. This modern granite and glass structure at the foot of the State Capitol Building houses permanent and temporary exhibits exploring the people, places, events, and issues of Iowa's past. The showcase includes native wildlife, American Indian and pioneer artifacts, and political and military items. The museum features a genealogy and Iowa history library, museum gift shop, and cafe.
Terrace Hill, a National Historic Landmark and Iowa Governor's Residence, is among the best examples of American Victorian Second Empire architecture. This opulent 1869 home was built by Iowa's first millionaire, Benjamin F. Allen, and restored to the late 19th century period. It overlooks downtown Des Moines and is situated on with a re-created Victorian formal garden. Tours are conducted Tuesdays through Saturdays from March through December.
The Science Center of Iowa and Blank IMAX Dome Theater offers seven interactive learning areas, live programs, and hands-on activities encouraging learning and fun for all ages. Among its three theaters include the 216-seat Blank IMAX Dome Theater, 175-seat John Deere Adventure Theater featuring live performances, and a domed Star Theater.
 The Greater Des Moines Botanical Garden is an indoor conservatory of over 15,000 exotic plants, one of the largest collections of tropical, subtropical, and desert-growing plants in the Midwest. The Center blooms with thousands of flowers year-round. Beautiful and extensive exterior gardens are also located here. Nearby are the Robert D. Ray Asian Gardens and Pavilion, named in honor of the former governor whose influence helped relocate thousands of Vietnamese refugees to Iowa homes in the 1970s and 1980s. Developed by the city's Asian community, the Gardens include a three-story Chinese pavilion, bonsai landscaping, and granite sculptures to highlight the importance of diversity and recognize Asian American contributions in Iowa.
Blank Park Zoo is a beautifully landscaped zoological park located on the south side. Among the exhibits include a tropical rain forest, Australian Outback, and Africa. The Zoo offers education classes, tours, and rental facilities.
The Great Ape Trust of Iowa was established as a scientific research facility with a campus housing bonobos and orangutans for the noninvasive interdisciplinary study of their cognitive and communicative capabilities. The Trust offers small public tours on a seasonal basis and only by reservation.
 The East Village, located on the east side of the Des Moines River, begins at the river and extends about five blocks east to the State Capitol Building, offering an eclectic blend of historic buildings, hip eateries, boutiques, art galleries, and a wide variety of other retail establishments mixed with residences.
Adventureland Park is an amusement park in neighboring Altoona, just northeast of Des Moines. The park boasts more than 100 rides, shows, and attractions, including three great roller coasters. A hotel and campground is located just outside the park. Also in Altoona is Prairie Meadows Racetrack and Casino, a popular entertainment venue for gambling and horse racing enthusiasts. Open 24 hours a day, year-round, the racetrack and casino features live racing, plus over 1,750 slot machines, table games, and concert and show entertainment.
Living History Farms in suburban Urbandale tells the story of Midwestern agriculture and rural life in a open-air museum with interpreters dressed in period costume who recreate the daily routines of early Iowans. Open daily from May through October, the Living History Farms include a 1700 Ioway Indian village, 1850 pioneer farm, 1875 frontier town, 1900 horse-powered farm, and a modern crop center.
Wallace House was the home of the first Henry Wallace, a national leader in agriculture and conservation and the first editor of "Wallaces' Farmer" farm journal. This restored 1883 Italianate Victorian houses exhibits, artifacts, and information covering four generations of Henry Wallaces and other family members.
Historic Jordan House in West Des Moines is a stately Victorian home built in 1850 and added to in 1870 by the first white settler in West Des Moines, James C. Jordan. Completely refurbished, this mansion was once part of the Underground Railroad and today houses 16 period rooms, a railroad museum, West Des Moines community history, and a museum dedicated to the Underground Railroad in Iowa. In 1893 Jordan’s daughter Eda was sliding down the banister when she fell off and broke her neck. She died two days later, and her ghost is reputed to haunt the house.
"The Chicago Tribune" wrote that Iowa's capital city has "walker-friendly downtown streets and enough outdoor sculpture, sleek buildings, storefronts and cafes to delight the most jaded stroller."
Festivals and events.
Des Moines plays host to a growing number of nationally acclaimed cultural events, including the annual Des Moines Arts Festival in June, , Iowa State Fair in August, and the World Food Festival in October. On Saturdays from May through October, the popular Downtown Farmers' Market draws visitors from across the state.
The 80/35 Music Festival debuted in 2008 in Western Gateway Park.
 is a monthly craft event which sets up with "the best of the best" of indie crafters in downtown Des Moines from May through November. It is the first monthly indie craft event of its kind.
Other annual festivals and events include: ArtFest Midwest, Blue Ribbon Bacon Fest, Celebrasian Heritage Festival, Des Moines Pride Festival, Des Moines Renaissance Faire, Festa Italiana, Festival of Trees and Lights, World Food and Music Festival, Interrobang Film Festival, Latino Heritage Festival, Rib America Festival, Winefest, and Wild Rose Film Festival.
Government.
Des Moines currently operates under a council-manager form of government. The council consists of a mayor (who, as of 2014, is Frank Cownie) and is elected in citywide vote, two at-large members, and four members representing each of the city's four wards.
A plan to merge the governments of Des Moines and Polk County was rejected by voters during the November 2, 2004, election. The consolidated city-county government would have had a full-time mayor and a 15-member council that would have been divided among the city and its suburbs. Each suburb would have still retained its individual government but had the option to join the consolidated government at any time. Although a full merger was soundly rejected, many city and county departments and programs have been consolidated.
Transportation.
Des Moines has an extensive skywalk system within its downtown core. With over four miles of enclosed walkway, it is one of the largest of such systems in the United States. The Des Moines Skywalk System has been criticised for hurting street-level business, though a recent initiative has been made to make street-level Skywalk entrances more visible.
Interstate 235 cuts through the city, and Interstate 35 and Interstate 80 both pass through the Des Moines metropolitan area. On the northern side of the Des Moines metropolitan area, Interstates 35 and 80 converge into a long concurrency while Interstate 235 takes a direct route through Des Moines, Windsor Heights, and West Des Moines before meeting up with Interstates 35 and 80 on the western edge of the metro.
Des Moines' freeway design makes it easy for travelers. A motorist who misses an exit at the interstate
"mixmasters" at the eastern and western edges of the metro eventually ends up at the convergence of the same interstates at the opposite side of the metro. These Interstates include I-35, I-80, and I-235. I-235, which takes the brunt of most congestion, is six lanes throughout the entire length and expands to eight and ten lanes near the downtown area. The remainder of traffic congestion in the area occurs near the Northeast and West Mixmasters on either side of Des Moines. Interstate 35 south of the West Mixmaster is currently being widened to six lanes all the way to the IA 5 bypass interchanges.
Due to increasing traffic congestion and prevalence of accidents along Interstate 235, the city of Des Moines implemented a speeding camera program to curb excessive speeding through the most densely populated parts of Greater Des Moines. The northeast Mixmaster has also undergone a redesign with wider lanes and redesign of bridges allowing easier traffic flow in all directions along Interstates 35, 80, and 235. U.S. Highway 65 and Iowa Highway 5 form a freeway loop to the east and south of the city, providing an alternative route around the metropolitan area. U.S. Highways 6 and 69 and Iowa Highways 28, 141, 163, and 415 are also important routes to and within the city.
There have been proposals to convert Iowa Highway 5 and U.S. Highway 65 into what would be renamed Interstate 335, giving the Des Moines Metropolitan Area an interstate running from the northeast suburb of Altoona to the southeastern suburb of Carlisle and then connect with Interstate 35 in the southern part of West Des Moines.
Des Moines's public transit system, operated by DART (Des Moines Area Regional Transit), which was the Des Moines Metropolitan Transit Authority until October 2006, consists entirely of buses, including regular in-city routes and express and commuter buses to outlying suburban areas. A light rail tram system has been proposed, but is years away and dependent on finding funding.
Burlington Trailways, Jefferson Lines, and Megabus run long-distance, inter-city bus routes through Des Moines. The nearest Amtrak train station is in Osceola, about south of Des Moines. Trains on the route that passes through Osceola, the California Zephyr, go east to Chicago, Illinois, and west to Emeryville, California.
The Des Moines International Airport (DSM), located in the southern part of Des Moines, on Fleur Drive, offers nonstop service to destinations within the United States. Currently the only international service is cargo service, but there have been discussions about adding an international terminal.
Education.
The Des Moines Public Schools district is the largest community school district in Iowa with 32,062 enrolled students as of the 2012–2013 school year. The district consists of 63 schools: 38 elementary schools, eleven middle schools, five high schools (East, Hoover, Lincoln, North, and Roosevelt), and ten special schools and programs. Small parts of the city are instead served by Carlisle Community Schools, Johnston Community School District, the Southeast Polk Community School District and the Saydel School District Grandview Park Baptist School is the only private school in the city, although Des Moines Christian School (located in Des Moines from 1947 to 2006) in Urbandale, Iowa Christian Academy and Dowling Catholic High School in West Des Moines, and Ankeny Christian Academy on the north side of the metro area serve some city residents.
Des Moines is also home to the main campuses of two four-year private colleges: Drake University and Grand View University. Simpson College, Upper Iowa University, and William Penn University also have classroom facilities in the area. For-profit colleges with classrooms in the area include the University of Phoenix. Des Moines Area Community College is the area's community college with campuses in Ankeny, downtown Des Moines, and West Des Moines. Other institutions of higher learning in Des Moines include AIB College of Business and Des Moines University, an osteopathic medical school.
Media.
The Des Moines market, which originally consisted of Polk, Dallas, Story, and Warren counties, was ranked 91st by Arbitron as of the fall of 2007 with a population of 512,000 aged 12 and older. But in June 2011 it was moved up to 72nd with the addition of Boone, Clarke, Greene, Guthrie, Jasper, Lucas, Madison and Marion counties.
Radio.
Commercial stations.
Most of Des Moines' commercial radio stations are owned by one of four companies. Clear Channel Communications owns five radio stations in the area, including WHO 1040 am, a 50,000-watt AM news/talk station that has the highest ratings in the area and once employed future President Ronald Reagan as a sportscaster. In addition to WHO, Clear Channel owns KDRB 100.3 FM (adult hits), KKDM 107.5 FM (contemporary hits), KDXA 106.3 FM (alternative rock), and KXNO 1460 am (sports radio). (They also own news/talk station KASI 1430 am and hot adult contemporary station KCYZ 105.1 FM, both of which broadcast from Ames.) Citadel Broadcasting owns five stations that broadcast from facilities in Urbandale: KBGG 1700 am (sports), KGGO 94.9 FM (classic rock), KHKI 97.3 FM (country music), KJJY 92.5 FM (country music), and KWQW 98.3 FM (talk radio). Saga Communications owns seven stations in the area: KAZR 103.3 FM (rock), KIOA 93.3 FM (oldies), KIOA-HD2 99.9FM & 93.3 HD2 (Rhythmic Top 40), KMYR 104.1 FM (soft adult contemporary), KPSZ 940 am (contemporary Christian music), KRNT 1350 am (adult standards), and KSTZ 102.5 FM (adult contemporary hits). Connoisseur Communications is a relative newcomer with a pair of move-ins, KZHC 96.3 (Country) and KICP (Rhythmic Top 40), both broadcasting from studios in Indianola. Other stations in the Des Moines area include religious stations KNWI 107.1 FM, KWKY 1150 am, and KPUL 99.5 FM.
Non-commercial stations.
Non-commercial radio stations in the Des Moines area include KDPS 88.1 FM, a station operated by the Des Moines Public Schools; KWDM 88.7 FM, a station operated by Valley High School; KJMC 89.3 FM, an urban contemporary station; and KDFR 91.3 FM, operated by Family Radio. WOI 640 am and WOI-FM 90.1 are both based out of Iowa State University in Ames and serve as the area's National Public Radio outlets.
Low-power FM stations include Drake University's KDRA-LP and Grand View University's KGVC-LP, which share the 94.1 frequency, and KFMG-LP 99.1, a community radio station broadcasting from the Hotel Fort Des Moines and also webstreamed.
Television.
The Des Moines-Ames media market consists of 35 central Iowa counties: Adair, Adams, Appanoose, Audubon, Boone, Calhoun, Carroll, Clarke, Dallas, Decatur, Franklin, Greene, Guthrie, Hamilton, Hardin, Humboldt, Jasper, Kossuth, Lucas, Madison, Mahaska, Marion, Marshall, Monroe, Pocahontas, Polk, Poweshiek, Ringgold, Story, Taylor, Union, Warren, Wayne, Webster, and Wright. It is ranked 71st by Nielsen Media Research for the 2008–2009 television season with 432,410 television households.
Commercial television stations serving Des Moines include KCCI channel 8, a CBS affiliate; WHO-TV channel 13, an NBC affiliate; KDSM-TV channel 17, a Fox affiliate; ABC affiliate WOI-TV channel 5 broadcasts from studios in West Des Moines; KDMI channel 56, a MyNetworkTV affiliate and broadcasts from Ankeny, Iowa. KCWI-TV channel 23, the local CW affiliate, is licensed to Ames but broadcasts from studios in Ankeny. KFPX-TV channel 39, the local ION affiliate, is licensed to Newton. KDIN channel 11 is the local PBS member station and flagship of the Iowa Public Television network. Mediacom is the Des Moines area's cable television provider.
Print.
"The Des Moines Register" is the city's primary daily newspaper. As of March 31, 2007, the "Register" ranked 71st in circulation among daily newspapers in the United States according to the Audit Bureau of Circulations with 146,050 daily and 233,229 Sunday subscribers. Weekly newspapers include "Juice", a publication aimed at the 25–34 demographic published by the "Register" on Wednesdays; "Cityview", an alternative weekly published on Thursdays; and the "Des Moines Business Record", a business journal published on Sundays, along with the West Des Moines Register, the Johnston Register, and the Waukee Register on Tuesdays, Wednesdays, or Thursdays depending on the address of the subscriber. Additionally, magazine publisher Meredith Corporation is based in Des Moines.
Sports and recreation.
Sports.
Des Moines is home to the Iowa Cubs baseball team of the Pacific Coast League. The I-Cubs, which are the Class AAA team of the major league Chicago Cubs, play their home games at Principal Park near the confluence of the Des Moines and Raccoon Rivers.
Wells Fargo Arena of the Iowa Events Center is home to the Iowa Wild of the American Hockey League, the Iowa Energy of the NBA Development League, and the Iowa Barnstormers of the Indoor Football League. The Wild, the AHL affiliate of the National Hockey League's Minnesota Wild have played at Wells Fargo Arena since 2013; previously, the Iowa Chops played four seasons in Des Moines (known as the Iowa Stars for three of those seasons. The Barnstormers relaunched as an af2 club in 2008 before joining a relaunched Arena Football League in 2010 and the Indoor Football League in 2015; the Barnstormers had previously played in the Arena Football League from 1994 to 2000 (famously featuring future NFL and Super Bowl MVP Kurt Warner) before relocating to New York. The Energy began play in the D-League in 2007 and serve as an affiliate for the Memphis Grizzlies.
Three other sports teams play in suburban Des Moines despite carrying the Des Moines brand. The Des Moines Buccaneers of the United States Hockey League play at Buccaneer Arena in Urbandale, the Des Moines Menace soccer team play at Valley Stadium in West Des Moines, and the Des Moines Derby Dames of the Women's Flat Track Derby Association play at the 7 Flags Event Center in Clive.
Des Moines is also home to the Bulldogs of Drake University, an NCAA Division I member of the Missouri Valley Conference, primarily playing northwest of downtown at the on-campus Drake Stadium and Knapp Center. Drake Stadium is home to the famed Drake Relays each April. In addition to the Drake Relays, Drake Stadium has hosted multiple NCAA Outdoor Track and Field Championships and USA Outdoor Track and Field Championships.
The Principal Charity Classic, a Champions Tour golf event, is held at Wakonda Club on the city's south side in late May or early June. The Hy-Vee Triathlon, which debuted in 2007, is held every September and served as a qualifier to the Olympic Games in 2008. The IMT Des Moines Marathon is held throughout the city each October.
See also: List of Des Moines sports teams
Parks and recreation.
Des Moines has 76 city parks and three golf courses, as well as three family aquatic centers, five community centers and three swimming pools. The city has of trails.
The Principal Riverwalk is a riverwalk park district currently being constructed along the banks of the Des Moines River in the downtown. Primarily funded by the Principal Financial Group, the Riverwalk is a multi-year jointly funded project also funded by the city and state. Upon completion, it will feature a recreational trail connecting the east and west sides of downtown via two pedestrian bridges. A landscaped promenade along the street level is planned. The Riverwalk includes the downtown Brenton Skating Plaza, open from November through March.
Gray's Lake, part of the of Gray's Lake Park, features a boat rental facility, fishing pier, floating boardwalks, and a park resource center. Located just south of the downtown, the centerpiece of the park is a lighted Kruidenier Trail, encircling it entirely.
From downtown Des Moines primarily along the east bank of the Des Moines River, the Neil Smith and John Pat Dorrian Trails are paved recreational trails that connect Gray's Lake northward to the east shore of Saylorville Lake, Big Creek State Park, and the recreational trails of Ankeny including the High Trestle Trail. These trails are near several recreational facilities including the Pete Crivaro Park, Principal Park, the Principal Riverwalk, the Greater Des Moines Botanical Garden, Union Park and its Heritage Carousel of Des Moines, Birdland Park and the Birdland Marina/Boatramp on the Des Moines River, Riverview Park, McHenry Park, and River Drive Park. Although outside of Des Moines, Jester Park has of land along the western shore of Saylorville Lake and can be reached from the Neil Smith Trail over the Saylorville Dam.
Just west of Gray's Lake are the of the Des Moines Water Works Park. The Water Works Park is located along the banks of the Raccoon River immediately upstream from where the Raccoon River empties into the Des Moines River. The Des Moines Water Works Facility, which obtains the city's drinking water from the Raccoon River, is located entirely within the Water Works Park. A bridge in the park crosses the Raccoon River. The Water Works Park recreational trails link to downtown Des Moines by travelling past Gray's Lake and back across the Raccoon River via either along the Meredith Trail near Principal Park, or along the Martin Luther King Jr. Parkway. The Water Works Park trails connect westward to Valley Junction and the recreational trails of the western suburbs: Windsor Heights, Urbandale, Clive, and Waukee. Also originating from Water Works Park, the Great Western Trail is an journey southward from Des Moines to Martensdale through the Willow Creek Golf Course, Orilla, and Cumming. Often, the location for summer music festivals and concerts, Water Works Park will be an overnight campground for thousands of bicyclists on Tuesday, July 23, 2013, during RAGBRAI XLI.
Sister cities.
The Greater Des Moines Sister City Commission, with members from the City of Des Moines and the suburbs of West Des Moines, Windsor Heights, Johnston, and Ankeny, maintains sister city relationships with five world communities:

</doc>
<doc id="9164" url="http://en.wikipedia.org/wiki?curid=9164" title="Donald Campbell">
Donald Campbell

Donald Malcolm Campbell (23 March 1921 – 4 January 1967) was a British speed record breaker who broke eight absolute world speed records on water and on land in the 1950s and 1960s. He remains the only person to set both world land and water speed records in the same year (1964).
Family and personal life.
Donald Campbell was born at Canbury House, Kingston upon Thames, Surrey, the son of Malcolm, later Sir Malcolm Campbell, holder of 13 world speed records in the 1920s and 30s in the famous "Bluebird" cars and boats, and his second wife, Dorothy Evelyn née Whittall. This background would shape Donald Campbell's entire character, and indeed his life.
Campbell attended St Peter's School, Seaford and Uppingham School. At the outbreak of World War II he volunteered for the Royal Air Force, but was unable to serve because of a case of childhood rheumatic fever. He joined Briggs Motor Bodies Ltd in West Thurrock, where he became a maintenance engineer. Subsequently, he was a shareholder in a small engineering company called Kine engineering, producing machine tools. Following his father's death on New Year's Eve, 31 December 1948 and aided by Malcolm's chief engineer, Leo Villa, the younger Campbell strove to set speed records first on water and then land.
He married three times: to Daphne Harvey in 1945, producing daughter Georgina (Gina) Campbell, born on 19 September 1946; to Dorothy McKegg in 1952; and to Tonia Bern in December 1958, which lasted until his death in 1967. Campbell was intensely superstitious, hating the colour green, the number thirteen, and believing nothing good ever happened on a Friday. He apparently also had some interest in the paranormal, which he nurtured as a member of the Ghost Club.
Campbell loved life and certainly lived it well. Despite being a qualified engineer, a successful businessman, a multiple record-breaker in his own right and a highly effective advocate of his own cause, Campbell was a restless man and seemed driven to emulate, if not surpass, his father's achievements. He was generally light-hearted and was generally, at least until his 1960 crash at the Bonneville salt flats, optimistic in his outlook. Behind the public façade of speed king, he was a complex character – proud and vulnerable, increasingly anxious about his place in the world. He had effectively given himself an impossible task – carrying on his father's role in an age when logic must have told him everything was against it. Campbell was a great patriot. The way he viewed it, his achievements were not for himself, but for the greater good of Britain.
Water Speed Records.
Campbell began his speed record attempts in the summer of 1949, using his father's old boat, "Bluebird K4". His initial attempts that summer were unsuccessful, although he did come close to raising his father's existing record. The team returned to Coniston Water, Lancashire in 1950 for further trials. Whilst there, they heard that an American, Stanley Sayers had raised the record from , beyond K4's capabilities without substantial modification. Over the winter 1950/51, Bluebird K4 was modified to make it a 'prop-rider' as opposed to her original immersed propeller configuration. This greatly reduced hydrodynamic drag as the third planing point would now be the propeller hub, meaning one of the two propeller blades was always out of the water at high speed. She now sported two cockpits, the second one being for Leo Villa. Bluebird K4 now had a chance of exceeding Sayers record and also enjoyed success as a circuit racer, winning the Oltranza Cup in Italy in the spring of that year. Returning to Coniston in September, they finally got Bluebird up to 170 mph after further trials, only to suffer a structural failure at which wrecked the boat. Sayers raised the record the following year to in Slo-Mo-Shun IV. Along with Donald Campbell, Britain had another potential contender for Water Speed Record honours - John Cobb. He had commissioned the world's first purpose-built turbo jet Hydroplane, "Crusader", with a target speed of over , and began trials on Loch Ness in autumn 1952. Cobb was killed later that year, when Crusader broke up, during an attempt on the record. Campbell was devastated at Cobb's loss, but his determination soon reasserted itself, and he resolved to build a new Bluebird boat to bring the WSR back to Britain.
In early 1953, Campbell began development of his own advanced all-metal jet-powered "Bluebird K7" hydroplane to challenge the record, by now held by the American prop rider hydroplane Slo-Mo-Shun IV.[1] Designed by Ken and Lew Norris, the K7 was a steel framed, aluminium bodied, three-point hydroplane with a Metropolitan-Vickers Beryl axial-flow turbojet engine, producing 3500 pound-force (16 kN) of thrust. Like Slo-Mo-Shun, but unlike Cobb's tricycle Crusader, the three planing points were arranged with two forward, on outrigged sponsons and one aft, in a "pickle-fork" layout, prompting Bluebird's early comparison to a blue lobster. K7 was of very advanced design and construction, and its load bearing steel space frame ultra rigid and stressed to 25g (exceeding contemporary military jet aircraft). It had a design speed of and remained the only successful jet-boat in the world until the late 1960s.
The designation "K7" was derived from its Lloyd's unlimited rating registration. It was carried on a prominent white roundel on each sponson, underneath an infinity symbol. Bluebird K7 was the seventh boat registered at Lloyds in the 'Unlimited' series.
Campbell set seven world water speed records in K7 between July 1955 and December 1964. The first of these marks was set at Ullswater on 23 July 1955, where he achieved a speed of but only after many months of trials and a major redesign of Bluebird's forward sponson attachments points. Campbell achieved a steady series of subsequent speed-record increases with the boat during the rest of the decade, beginning with a mark of in 1955 on Lake Mead in Nevada. Subsequently, four new marks were registered on Coniston Water, where Campbell and Bluebird became an annual fixture in the later half of the fifties, enjoying significant sponsorship from the Mobil oil company and then subsequently BP. Campbell also an unsuccessful attempt in 1957 at Canandaigua in New York state in the summer of 1957, which failed due to lack of suitable calm water conditions. Bluebird K7 became a well known and popular attraction, and as well as her annual Coniston appearances, K7 was displayed extensively in the UK, USA, Canada and Europe, and then subsequently in Australia during Campbell's prolonged attempt on the land speed record (LSR) in 1963 - 64.
In order to extract more speed, and endow the boat with greater high speed stability, in both pitch and yaw, K7 was subtly modified in the second half of the 1950s to incorporate more effective streamlining with a blown Perspex cockpit canopy and fluting to the lower part of the main hull. In 1958, a small wedge shaped tail fin, housing an arrester parachute, modified sponson fairings, that gave a significant reduction in forward aerodynamic lift, and a fixed hydrodynamic stabilising fin, attached to the transom to aid directional stability, and exert a marginal down-force on the nose were incorporated into the design to increase the safe operating envelope of the hydroplane. Thus she reached in 1956, where an unprecedented peak speed of was achieved on one run, in 1957, in 1958 and in 1959.
Campbell was awarded the CBE in January 1957 for his water speed record breaking, and in particular his record at Lake Mead in the USA which earned him and Britain very positive acclaim.
Land Speed Record Attempt.
It was after the Lake Mead WSR success in 1955 that the seeds of Donald's ambition to hold the Land Speed Record as well were planted. The following year, the serious planning was under way - to build a car to break the land speed record, which then stood at and had been set by John Cobb in 1947. The Norris brothers designed "Bluebird-Proteus CN7" with in mind. The brothers were even more enthusiastic about the car than the boat and like all of his projects, Donald wanted Bluebird CN7, to be the best of its type, a showcase of British engineering skill. The British motor industry in the guise of Dunlop, BP, Smiths Industries and Lucas, as well as many others, became heavily involved in the project to build the most advanced car the world had yet seen. CN7 was powered by a specially modified Bristol-Siddeley Proteus free-turbine engine of . driving all four wheels. Bluebird CN7 was designed to achieve 475–500 mph and was completed by the spring of 1960.
Following low-speed tests conducted at the Goodwood motor racing circuit in Sussex, in July, the "CN7" was taken to the Bonneville Salt Flats in Utah, USA, scene of his father's last LSR triumph, some 25 years earlier in September 1935. The trials initially went well, and various adjustments were made to the car. On the sixth run in CN7, Campbell lost control at over 360 mph and crashed. It was the car's enormous structural integrity that saved his life. He was hospitalised with a fractured skull and a burst eardrum, as well as minor cuts and bruises."CN7" was a write off. Almost immediately, Campbell announced he was determined to have another go. Sir Alfred Owen, whose Rubery Owen industrial group had built CN7, offered to rebuild it for him. That single decision was to have a profound influence on the rest of Donald Campbell's life. His original plan had been to break the LSR at over 400 mph in 1960, return to Bonneville the following year to really bump up the speed to something near to 500 mph, get his seventh WSR with K7 and then retire, as undisputed champion of speed and perhaps just as important, secure in the knowledge that he was worthy of his father's legacy.
Campbell decided not to go back to Utah for the new trials. He felt the Bonneville course was too short at and the salt surface was in poor condition. BP offered to find another venue and eventually after a long search, Lake Eyre, in South Australia, was chosen. It hadn't rained there for nine years and the vast dry bed of the salt lake offered a course of up to . By the summer of 1962, Bluebird CN7 was rebuilt, some nine months later than Campbell had hoped. It was essentially the same car, but with the addition of a large stabilising tail fin and a reinforced fibreglass cockpit cover. At the end of 1962, CN7 was shipped out to Australia ready for the new attempt. Low-speed runs had just started when the rains came. The course was compromised and further rain meant, that by May 1963, Lake Eyre was flooded to a depth of 3 inches, causing the attempt to be abandoned. Donald was heavily criticised in the press for alleged time wasting and mismanagement of the project, despite the fact that he could hardly be held responsible for the unprecedented weather.
To make matters worse for Campbell, American Craig Breedlove drove his pure thrust jet car 'Spirit of America' to a speed of at Bonneville in July 1963. Although the 'car' did not conform to FIA (Federation Internationale de L'Automobile) regulations, that stipulated it had to be wheel-driven and have a minimum of four wheels, in the eyes of the world, Breedlove was now the fastest man on earth.
Campbell returned to Australia in early spring 1964, but the Lake Eyre course failed to fulfil the early promise it had shown in 1962 and there were further spells of rain. BP pulled out as his main sponsor after a dispute, but he was able to secure backing from Australian oil company Ampol.
The track never properly dried out and Campbell was forced to make the best of the conditions. Finally, in July 1964, he was able to post some speeds that approached the record. On the 17th of that month, he took advantage of a break in the weather and made two courageous runs along the shortened and still damp track, posting a new LSR of . 
Campbell was bitterly disappointed with the record as the vehicle had been designed for much higher speeds. "CN7" covered the final third of the measured mile at an average of , peaking as it left the measured distance at over . He resented the fact that it had all been so difficult. 'We've made it – we got the bastard at last,' was his reaction to the success. Campbell's 403.1 mph represented the official Land Speed Record.
In 1969, after Campbell's fatal accident, his widow, Tonia Bern-Campbell negotiated a deal with Lynn Garrison, President of Craig Breedlove and Associates, that would see Craig Breedlove run Bluebird on Bonneville's Salt Flats. This concept was cancelled when the parallel Spirit of America supersonic car project failed to find support.
The Double.
Donald now planned to go after the Water Speed Record one more time with Bluebird K7 - to do what he had aimed for so many years ago, during the initial planning stages of CN7 – break both records in the same year. After more delays, he finally achieved his seventh WSR at Lake Dumbleyung near Perth, Western Australia, on the last day of 1964, at a speed of . He had become the first, and so far only, person to set both land and water speed records in the same year. Campbell's LSR was short-lived, because FIA rule changes meant that pure jet cars would be eligible to set records from October 1964. Campbell's speed on his final Lake Eyre run remained the highest speed achieved by a wheel-driven car until 2001; "Bluebird CN7" is now on display at the National Motor Museum in Hampshire, England, her potential only partly realised.
Rocket Car Plans & The Final Water Speed Record Attempt.
Bluebird Mach 1.1.
 Donald Campbell decided a massive jump in speed was called for following his successful 1964 LSR attempt in Bluebird CN7. His vision was of a supersonic rocket car with a potential maximum speed of . Norris Brothers were requested to undertake a design study Bluebird Mach 1.1 was a design for a rocket-powered supersonic land speed record car.
Campbell chose a lucky date to hold a press conference at the Charing Cross Hotel on 7 July 1965 to announce his future record breaking plans:
Bluebird Mach 1.1 was to be rocket-powered. Ken Norris had calculated using rocket motors would result in a vehicle with very low frontal area, greater density, and lighter weight than if he went down the jet engine route. Bluebird Mach 1.1 would also be a relatively compact and simple design. Norris specified two off-the-shelf Bristol Siddeley BS.605 rocket engines. The 605 had been developed as a take-off assist rocket engine for military aircraft and was fuelled with kerosene, using hydrogen peroxide as the oxidizer. Each engine was rated at thrust. In Bluebird Mach 1.1 application, the combined thrust would be equivalent of 36,000 bhp (27,000 kW; 36,000 PS) at .
The Final Record Attempt.
In order to increase publicity for his rocket car venture, in the spring of 1966, Campbell decided to try once more for a water speed record. This time the target was . "Bluebird K7" was fitted with a lighter and more powerful Bristol Orpheus engine, taken from a Folland Gnat jet aircraft, which developed of thrust. The modified boat was taken back to Coniston in the first week of November 1966. The trials did not go well. The weather was appalling, and "K7" suffered an engine failure when her air intakes collapsed and debris was drawn into the engine. By the middle of December, some high-speed runs were made, in excess of but still well below Campbell's existing record. Problems with "Bluebird"'s fuel system meant that the engine could not reach full rpm, and so would not develop maximum power. Eventually, by the end of December, after further modifications to her fuel system, and the replacement of a fuel pump, the fuel starvation problem was fixed, and Campbell awaited better weather to mount an attempt.
Death.
On 4 January 1967, weather conditions were finally suitable for an attempt. Campbell commenced the first run of his last record attempt at just after 8.45 am. Bluebird moved slowly out towards the middle of the lake, where she paused briefly as Donald lined her up. With a deafening blast of power, Campbell now applied full throttle and Bluebird began to surge forward. Clouds of spray issued from the jet-pipe, water poured over the rear spar and after a few hundred yards, at 70 mph, Bluebird unstuck from the surface and rocketed off towards the southern end of the lake, producing her characteristic comet's tail of spray.
She entered the measured kilometre at 8:46. Leo Villa witnessed her passing the first marker buoy at about in perfect steady planing trim, her nose slightly down, still accelerating.
7.525 seconds later, Keith Harrison saw her leave the measured kilometre at a speed of over . The average speed for the first run was . Campbell lifted his foot from the throttle about 3/10 of a second before passing the southern kilometre marker. As Bluebird left the measured kilometre, Keith Harrison and Eric Shaw in a course boat at the southern end of the measured kilo both noticed that she was very light around the bows, riding on her front stabilising fins. Her planing trim was no worse than she had exhibited when equipped with the Beryl engine, but it was markedly different from that observed by Leo Villa at the northern end of the kilometre, when she was under full acceleration. Campbell had made his usual commentary throughout the run.
Campbell's words on his first run were, via radio intercom:
Instead of refuelling and waiting for the wash of this run to subside, Campbell decided to make the return run immediately. This was not an unprecedented diversion from normal practice, as Campbell had used the advantage presented i.e. no encroachment of water disturbances on the measured kilometre by the quick turn-a-round, in many previous runs. The second run was even faster once severe tramping subsided on the run-up from Peel Island (caused by the water-brake disturbance). Once smooth water was reached some 700 metres or so from the start of the kilometre, K7 demonstrated cycles of 'ground' effect hovering before accelerating hard at 0.63g to a peak speed of some 200 metres or so from the southern marker buoy. Bluebird was now experiencing bouncing episodes of the starboard sponson with increasing ferocity. At the peak speed, the most intense and long-lasting bounce precipitated a severe decelerating episode (328 mph - 296 mph, -1.86g) as K7 dropped back onto the water. Engine flame-out then occurred and, shorn of thrust nose-down momentum, K7 experienced a gliding episode in strong ground effect with increasing angle-of-attack (AoA), before completely leaving the water at her static stability pitch-up limit of 5.2°. Bluebird then executed an almost complete somersault (~ 320° and slightly off-axis) before plunging into the water (port sponson marginally in advance of the starboard), approximately 230 metres from the end of the measured kilometre. The boat then cartwheeled across the water before coming to rest. The impact broke K7 forward of the air intakes (where Donald was sitting) and the main hull sank shortly afterwards. Campbell had been killed instantly. Mr Whoppit, Campbell's teddy bear mascot, was found among the floating debris and the pilot's helmet was recovered. Royal Navy divers made efforts to find and recover the body but, although the wreck of K7 was found, they called off the search, after two weeks, without locating his body.
Campbell's last words, during a 31 second transmission, on his final run were, via radio intercom:
The cause of the crash has been variously attributed to Campbell not waiting to refuel after doing a first run of and hence the boat being lighter, or to the wash caused by his first run and made much worse by the use of the water brake. These factors have since been found to be not particularly important: The water brake was used well to the south of the measured distance, and only from approx. . The area in the centre of the course where Bluebird was travelling at peak speed on her return run was flat calm, and not disturbed by the wash from the first run, which had not had time to be reflected back on the course. Campbell knew this and, as discussed previously, adopted his well-practiced, 'quick turn-a-round' strategy.
The cause of the crash can be put down to Bluebird exceeding its aerodynamic static stability limit, complicated by the additional destabilizing influences of loss of engine thrust, damage to the port spar fairing, and, the hitherto unappreciated contribution of ground effect lift enhancement. There is also evidence to point to the fact that K7s dynamic stability limit had been exceeded. The cause(s) of the engine flame-out cannot be established unequivocally. It could have been due to fuel starvation, damage to some ancillary structural element associated with engine function (following the worst bouncing episode), disturbance of the airstream into the intakes during the pitching episodes, or indeed a combination of all three. Further evidence of lost engine thrust may be seen in both cinematographic and still film recordings of the latter part of the run - as Bluebird left the water, jet exhaust from a functioning engine would have severely disturbed the water surface; no such disturbance or accompanying spray is evident. Also, close examination of such records show no evidence to the effect that the water brake was deployed.
Despite extensive efforts by a team of Royal Navy divers, although Bluebird's wreckage was located on 5 January, on the lake bed, Donald Campbell's body was not located until 2001.
On 28 January 1967 Campbell was posthumously awarded the Queen's Commendation for Brave Conduct "For courage and determination in attacking the world water speed record."
Recovery of Bluebird K7 and the body of Donald Campbell.
The wreckage of Campbell's craft was recovered by the Bluebird Project between October 2000, when the first sections were raised, and May 2001, when Donald Campbell's body was recovered. The largest section comprising approximately two thirds of the centre hull was raised on 8 March 2001. The project began when diver Bill Smith was inspired to look for the wreck after hearing the Marillion song "Out of This World" (from the album "Afraid of Sunlight"), which was written about Campbell and "Bluebird".
The recovered wreck revealed that the water brake had deployed after the accident as a result of stored accumulator pressure; Campbell would not have had time to deploy the relatively slow-moving brake as the boat flipped out of control. The boat still contained fuel in the engine fuel lines, discounting the fuel-starvation theory. The wreckage all evidenced an impact from left to right, wiping the whole front of the boat off in that direction. Campbell's lower harness mounts had failed and were found to be effectively useless. Further dives recovered various parts of K7, which had separated from the main hull when it broke up on impact.
Campbell's body was finally located just over two months later and recovered from the lake on 28 May 2001, still wearing his blue nylon overalls.
Campbell was interred in Coniston cemetery on 12 September 2001 after his coffin was carried down the lake, and through the measured kilometre, on a launch, one last time. A funeral service was then held at St Andrew's Church in Coniston, after an earlier, and positive DNA examination had been carried out. The funeral was attended by his widow Tonia, daughter Gina, other members of his family, members of his former team, and admirers. The funeral was overshadowed in the media due to coverage of the 9/11 attacks in the United States. Jean Wales (Donald Campbell's sister) had, however, been against the recovery of her brother's body out of respect for his stated wish that, in the event of something going wrong, "Skipper and boat stay together". When Donald Campbell was buried in Coniston cemetery on 12 September 2001 she did not attend the service. Steve Hogarth, lead singer for Marillion, was also present at the funeral and performed the song "Out of This World" solo.
Legacy.
Between them, Donald Campbell and his father had set eleven speed records on water and ten on land.
The story of Donald Campbell's last attempt at the water speed record on Coniston Water was told in the BBC television film "Across the Lake" in 1988, with Anthony Hopkins as Donald. Nine years earlier, Robert Hardy had played Donald's father, Sir Malcolm Campbell, in the BBC2 Playhouse television drama "Speed King" - both were written by Roger Milner and produced by Innes Lloyd. In 2003, the BBC showed a documentary reconstruction of Campbell's fateful water-speed record attempt in an episode of "Days That Shook the World". It featured a mixture of modern reconstruction and original film footage. All of the original colour clips were taken from a film capturing the event, "Campbell at Coniston" by John Lomax, a local amateur filmmaker from Wallasey, England. Lomax's film won awards worldwide in the late 1960s for recording the final weeks of Campbell's life.
In 1956, he was surprised by Eamonn Andrews for the seventh episode of the new television show "This Is Your Life".
An English Heritage blue plaque commemorates Campbell and his father at Canbury School, Kingston Hill, Kingston upon Thames, where they lived.
In the village of Coniston, the Ruskin Museum has a display of Donald Campbell memorabilia, and the Bristol Orpheus engine recovered in 2001 is also displayed. The engine's casing is mostly missing, having acted as a sacrificial anode in its time underwater but the internals are remarkably preserved. Donald Campbell's helmet from the ill fated run is also on display.
On Thursday 7 December 2006, Gina Campbell, Donald's daughter, formally gave Bluebird K7 to the Ruskin Museum in Coniston on behalf of the Campbell Family Heritage Trust. In agreement with the Campbell Family Heritage Trust and the museum, Bill Smith is to organize the restoration of the boat, which is now under way. Now the property of the Ruskin Museum, the intention is to rebuild K7 back to running order circa 4 January 1967. Bill Smith has said that this will take an undisclosed number of years to accomplish. Gina Campbell commented: "I've decided to secure the future of Bluebird for the people of Coniston, the Ruskin Museum and the people of the world". Museum Director Vicky Slowe spoke of Gina's generosity and then said: "Bill Smith has assured us he can get Bluebird fully conserved and reconfigured at no cost to the museum. As of 2008, K7 is being fully restored by The Bluebird Project, to a very high standard of working condition in North Shields, Tyne and Wear, using a significant proportion of her original fabric, but with a new BS Orpheus engine of the same type albeit incorporating many original components."
As of May 2009 permission has been given for a one off set of proving trials of Bluebird on Coniston Water, where she will be tested to a safe speed for demonstration purposes only. There is no fixed date for completion of Bluebird K7 or the trials. K7 will be housed in her own purpose built wing at the Ruskin Museum in Coniston, while remaining in the care of The Bluebird Project.

</doc>
<doc id="9165" url="http://en.wikipedia.org/wiki?curid=9165" title="Directed set">
Directed set

In mathematics, a directed set (or a directed preorder or a filtered set) is a nonempty set "A" together with a reflexive and transitive binary relation ≤ (that is, a preorder), with the additional property that every pair of elements has an upper bound: In other words, for any "a" and "b" in "A" there must exist a "c" in "A" with "a" ≤ "c" and "b" ≤ "c".
Directed sets are a generalization of nonempty totally ordered sets, that is, all totally ordered sets are directed sets (contrast "partially" ordered sets which need not be directed). In topology, directed sets are used to define nets, which generalize sequences and unite the various notions of limit used in analysis. Directed sets also give rise to direct limits in abstract algebra and (more generally) category theory.
Equivalent definition.
In addition to the definition above, there is an equivalent definition. A directed set is a set "A" with a preorder such that every finite subset of "A" has an upper bound. In this definition, the existence of an upper bound of the empty subset implies that "A" is nonempty.
Examples.
Examples of directed sets include:
Contrast with semilattices.
Directed sets are a more general concept than (join) semilattices: every join semilattice is a directed set, as the join or least upper bound of two elements is the desired "c". The converse does not hold however, witness the directed set {1000,0001,1101,1011,1111} ordered bitwise (e.g. 1000 ≤ 1011 holds, but 0001 ≤ 1000 does not, since in the last bit 1 > 0), where {1000,0001} has three upper bounds but no "least" upper bound, cf. picture.
Directed subsets.
The order relation in a directed sets is not required to be antisymmetric, and therefore directed sets are not always partial orders. However, the term "directed set" is also used frequently in the context of posets. In this setting, a subset "A" of a partially ordered set ("P",≤) is called a directed subset if it is a directed set according to the same partial order: in other words, it is not the empty set, and every pair of elements has an upper bound. Here the order relation on the elements of "A" is inherited from "P"; for this reason, reflexivity and transitivity need not be required explicitly. 
A directed subset of a poset is not required to be downward closed; a subset of a poset is directed if and only if its downward closure is an ideal. While the definition of a directed set is for an "upward-directed" set (every pair of elements has an upper bound), it is also possible to define a downward-directed set in which every pair of elements has a common lower bound. A subset of a poset is downward-directed if and only if its upper closure is a filter.
Directed subsets are used in domain theory, which studies directed complete partial orders. These are posets in which every upward-directed set is required to have a least upper bound. In this context, directed subsets again provide a generalization of convergent sequences.

</doc>
<doc id="9209" url="http://en.wikipedia.org/wiki?curid=9209" title="Edward Bellamy">
Edward Bellamy

Edward Bellamy (March 26, 1850 – May 22, 1898) was an American author and socialist, most famous for his utopian novel, "Looking Backward", a Rip Van Winkle-like tale set in the distant future of the year 2000. Bellamy's vision of a harmonious future world inspired the formation of over 160 "Nationalist Clubs" dedicated to the propagation of Bellamy's political ideas and working to make them a practical reality.
Biography.
Early years.
Edward Bellamy was born in Chicopee, Massachusetts. His father was Rufus King Bellamy (1816–1886), a Baptist minister and a descendant of Joseph Bellamy. His mother, Maria Louisa Putnam Bellamy, was herself the daughter of a Baptist minister named Benjamin Putnam, a man forced to withdraw from the ministry in Salem, Massachusetts, following objections to his becoming a Freemason.
Bellamy attended public school at Chicopee Falls before leaving for Union College of Schenectady, New York, where he studied for just two semesters. Upon leaving school, Bellamy made his way to Europe for a year, spending extensive time in Germany. Bellamy briefly studied law but abandoned that field without ever having practiced as a lawyer, instead entering the world of journalism. In this capacity Bellamy briefly served on the staff of the "New York Post" before returning to his native Massachusetts to take a position at the "Springfield Union."
At the age of 25, Bellamy developed tuberculosis, the disease that would ultimately kill him. He suffered with its effects throughout his adult life. In an effort to regain his health, Bellamy spent a year in the Hawaiian Islands (1877 to 1878). Returning to the United States, Bellamy decided to abandon the daily grind of journalism in favor of literary work, which put fewer demands upon his time and his health.
Bellamy married Emma Augusta Sanderson in 1882. The couple had two children.
Literary career.
Bellamy's early novels, including "Six to One" (1877), "Dr. Heidenhoff's Process" (1880), and "Miss Ludington's Sister" (1884) were unremarkable works, making use of standard psychological plots. A turn to utopian science fiction with "Looking Backward, 2000–1887," published in January 1888, captured the public imagination and catapulted Bellamy to literary fame. The publisher of the book could scarcely keep up with demand. Within a year the book had sold some 200,000 copies and by the end of the 19th century it had sold more copies than any other book published in America outside of "Uncle Tom's Cabin" by Harriet Beecher Stowe.
Although Bellamy claimed he did not write "Looking Backward" as a blueprint for political action, but rather sought to write "a literary fantasy, a fairy tale of social felicity," the book inspired legions of inspired readers to establish so-called Nationalist Clubs, beginning in Boston late in 1888. Bellamy's vision of a country relieved of its social ills through abandonment of the principle of competition and establishment of state ownership of industry proved an appealing panacea to a generation of intellectuals alienated from the dark side of Gilded Age America. By 1891 it was reported that no fewer than 162 Nationalist Clubs were in existence.
Bellamy himself came to actively participate in the political movement which emerged around his book, particularly after 1891 when he founded his own magazine, "The New Nation," and began to promote united action between the various Nationalist Clubs and the emerging People's Party. For the next three and a half years, Bellamy gave his all to politics, publishing his magazine, working to influence the platform of the People's Party, and publicizing the Nationalist movement in the popular press. This phase of Bellamy's life came to an end in 1894, when "The New Nation" was forced to suspend publication owing to financial difficulties.
With the key activists of the Nationalist Clubs largely absorbed into the apparatus of the People's Party (although a Nationalist Party did run candidates for office in Wisconsin as late as 1896), Bellamy abandoned politics for a return to literature. He set to work on a sequel to "Looking Backward" titled "Equality," attempting to deal with the ideal society of the post-revolutionary future in greater detail. The book saw print in 1897 and would prove to be Bellamy's final creation.
Death and legacy.
Edward Bellamy died of tuberculosis in Chicopee Falls, Massachusetts. He was 48 years old at the time of his death.
His lifelong home in Chicopee was designated a National Historic Landmark in 1971.
Bellamy was the cousin of Francis Bellamy, famous for creation of the Pledge of Allegiance.

</doc>
<doc id="9222" url="http://en.wikipedia.org/wiki?curid=9222" title="E">
E

E (named "e" , plural "ees") is the fifth letter and a vowel in the ISO basic Latin alphabet. It is the most commonly used letter in many languages, including: Czech, Danish, Dutch, English, French, German, Hungarian, Latin, Norwegian, Spanish, and Swedish.
History.
The Latin letter 'E' differs little from its derivational source, the Greek letter epsilon, 'Ε'. In etymology, the Semitic "hê"  has been suggested to have started as a praying or calling human figure ("hillul" 'jubilation'), and was probably based on a similar Egyptian hieroglyph that indicated a different pronunciation. In Semitic, the letter represented (and in foreign words), in Greek "hê" became epsilon with the value . Etruscans and Romans followed this usage. Although Middle English spelling used 'e' to represent long and short , the Great Vowel Shift changed long (as in 'me' or 'bee') to while short (as in 'met' or 'bed') remained a mid vowel.
Use in other languages.
In the International Phonetic Alphabet, /e/ represents the close-mid front unrounded vowel. In the orthography of many languages it represents either this or //, or some variation (such as a nasalized version) of these sounds, often with diacritics (as: ) to indicate contrasts. Less commonly, as in Saanich, E represents a mid-central vowel /ə/. Digraphs with 'e' are common to indicate diphthongs and monophthongs, such as 'ea' or 'ee' for or in English, 'ei' for in German, and 'eu' for in French or in German.
Most common letter.
'E' is the most common (or highest-frequency) letter in the English alphabet (starting off the typographer's phrase ETAOIN SHRDLU) and several other European languages, which has implications in both cryptography and data compression. In the story The Gold Bug by Edgar Allan Poe, a character figures out a random character code by remembering that the most used letter in English is E. This makes it a hard and popular letter to use when writing lipograms. Ernest Vincent Wright's "Gadsby" (1939) is considered a "dreadful" novel, and that "at least part of Wright's narrative issues were caused by language limitations imposed by the lack of "E"." Both Georges Perec's novel "A Void" ("La Disparition") (1969) and its English translation by Gilbert Adair omit 'e' and are considered better works.
Other representations.
In British Sign Language (BSL), the letter 'e' is signed by extending the index finger of the right hand touching the tip of index on the left hand, with all fingers of left hand open.

</doc>
<doc id="9223" url="http://en.wikipedia.org/wiki?curid=9223" title="Economics">
Economics

 
Economics is the social science that studies economic activity to gain an understanding of the processes that govern the production, distribution and consumption of goods and services in an economy.
The term "economics" comes from the Ancient Greek from (', "house") and (', "custom" or "law"), hence "rules of the house (hold for good management)". 'Political economy' was the earlier name for the subject, but economists in the late 19th century suggested "economics" as a shorter term for "economic science" to establish itself as a separate discipline outside of political science and other social sciences.
Economics focuses on the behavior and interactions of economic agents and how economies work. Consistent with this focus, primary textbooks often distinguish between microeconomics and macroeconomics. Microeconomics examines the behavior of basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the entire economy (meaning aggregated production, consumption, savings, and investment) and issues affecting it, including unemployment of resources (labor, capital, and land), inflation, economic growth, and the public policies that address these issues (monetary, fiscal, and other policies).
Other broad distinctions within economics include those between positive economics, describing "what is," and normative economics, advocating "what ought to be"; between economic theory and applied economics; between rational and behavioral economics; and between mainstream economics (more "orthodox" and dealing with the "rationality-individualism-equilibrium nexus") and heterodox economics (more "radical" and dealing with the "institutions-history-social structure nexus").
Besides the traditional concern in production, distribution, and consumption in an economy, economic analysis may be applied throughout society, as "in" business, finance, health care, and government. Economic analyses may also be applied "to" such diverse subjects as crime, education, the family, law, politics, religion, social institutions, war, and science; by considering the economic aspects of these subjects. Education, for example, requires time, effort, and expenses, plus the foregone income and experience, yet these losses can be weighted against future benefits education may bring to the agent or the economy. At the turn of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.
Definitions.
There are a variety of modern definitions of economics. Some of the differences may reflect evolving views of the subject or different views among economists. Scottish philosopher Adam Smith (1776) defined what was then called political economy as "an inquiry into the nature and causes of the wealth of nations", in particular as:
J.-B. Say (1803), distinguishing the subject from its public-policy uses, defines it as the science "of" production, distribution, and consumption of wealth. On the satirical side, Thomas Carlyle (1849) coined "the dismal science" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798). John Stuart Mill (1844) defines the subject in a social context as:
Alfred Marshall provides a still widely cited definition in his textbook "Principles of Economics" (1890) that extends analysis beyond wealth and from the societal to the microeconomic level:
Lionel Robbins (1932) developed implications of what has been termed "[p]erhaps the most commonly accepted current definition of the subject":
Robbins describes the definition as not "classificatory" in "pick[ing] out certain "kinds" of behaviour" but rather "analytical" in "focus[ing] attention on a particular "aspect" of behaviour, the form imposed by the influence of scarcity." He affirmed that previous economist have usually centered their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow. But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. We cannot define economics as the science that study wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that study a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).
Some subsequent comments criticized the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behavior and rational-choice modeling expanded the domain of the subject to areas previously treated in other fields. There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.
Gary Becker, a contributor to the expansion of economics into new areas, describes the approach he favors as "combin[ing the] assumptions of maximizing behavior, stable preferences, and market equilibrium, used relentlessly and unflinchingly." One commentary characterizes the remark as making economics an approach rather than a subject matter but with great specificity as to the "choice process and the type of social interaction that [such] analysis involves." The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.
Microeconomics.
Markets.
Microeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradeable units and government regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment.
In theory, in a free market the aggregates (sum of) of "quantity demanded" by buyers and "quantity supplied" by sellers will be equal and reach economic equilibrium over time in reaction to price changes; in practice, various issues may prevent equilibrium, and any equilibrium reached may not necessarily be morally equitable. For example, if the supply of healthcare services is limited by external factors, the equilibrium price may be unaffordable for many who desire it but cannot pay for it.
Various market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a "price taker" as no participant influences the price of a product. In the real world, markets often experience imperfect competition.
Forms include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Unlike perfect competition, imperfect competition invariably means market power is unequally distributed. Firms under imperfect competition have the potential to be "price makers", which means that, by holding a disproportionately high share of market power, they can influence the prices of their products.
Microeconomics studies individual markets by simplifying the economic system by assuming that activity in the market being analysed does not affect other markets. This method of analysis is known as partial-equilibrium analysis (supply and demand). This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across "all" markets. This method studies both changes in markets and their interactions leading towards equilibrium.
Production, cost, and efficiency.
In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defense, smallpox vaccinations, etc.) or private goods (new computers, bananas, etc.), and "guns" vs. "butter".
Opportunity cost refers to the economic cost of production: the value of the next best opportunity foregone. Choices must be made between desirable yet mutually exclusive actions. It has been described as expressing "the basic relationship between scarcity and choice.". The opportunity cost of an activity is an element in ensuring that scarce resources are used efficiently, such that the cost is weighed against the value of that activity in deciding on more or less of it. Opportunity costs are not restricted to monetary or financial costs but could be measured by the real cost of output forgone, leisure, or anything else that provides the alternative benefit (utility).
Inputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car.
Economic efficiency describes how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs, or in other words, the amount of "waste" is reduced. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off.
The production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say "guns" and "butter"). The PPF is a table or graph (as at the right) showing the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good.
Scarcity is represented in the figure by people being willing but unable in the aggregate to consume "beyond the PPF" (such as at "X") and by the negative slope of the curve. If production of one good "increases" along the curve, production of the other good "decreases", an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter.
The slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a "real opportunity cost". Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. "Along the PPF", scarcity implies that choosing "more" of one good in the aggregate entails doing with "less" of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents.
By construction, each point on the curve shows "productive efficiency" in maximizing output for given total inputs. A point "inside" the curve (as at "A"), is feasible but represents "production inefficiency" (wasteful use of inputs), in that output of "one or both goods" could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organization of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points.
Much applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organize society for the most efficient use of resources has been described as the "essence of economics", where the subject "makes its unique contribution."
Specialization.
Specialization is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus "relatively" cheaper, input.
Even if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialize in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else.
It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialization in similar but differentiated product lines, to the overall benefit of respective trading parties or regions.
The general theory of specialization applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding "division of labour" with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses.
An example that combines features above is a country that specializes in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products.
Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialization of production creates opportunities for "gains from trade" whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the "increased income levels" that trade may facilitate.
Supply and demand.
Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.
For a given market of a commodity, "demand" is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is "constrained utility maximization" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesized relation of each individual consumer for ranking different commodity bundles as more or less preferred.
The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy of it (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.
"Supply" is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesized to be "profit-maximizers", meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a directly proportional relation between price and quantity supplied (other things unchanged).
That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The "Law of Supply" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.
Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilize at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.
For a given quantity of a consumer good, the point on the demand curve indicates the value, or marginal utility, to consumers for that unit. It measures what the consumer would be prepared to pay for that unit. The corresponding point on the supply curve measures marginal cost, the increase in total cost to the supplier for the corresponding unit of the good. The price in equilibrium is determined by supply and demand. In a perfectly competitive market, supply and demand equate marginal cost and marginal utility at equilibrium.
On the supply side of the market, some factors of production are described as (relatively) "variable" in the short run, which affects the cost of changing output levels. Their usage rates can be changed easily, such as electrical power, raw-material inputs, and over-time and temp work. Other inputs are relatively "fixed", such as plant and equipment and key personnel. In the long run, all inputs may be adjusted by management. These distinctions translate to differences in the elasticity (responsiveness) of the supply curve in the short and long runs and corresponding differences in the price-quantity change from a shift on the supply or demand side of the market.
Marginalist theory, such as above, describes the consumers as attempting to reach most-preferred positions, subject to income and wealth constraints while producers attempt to maximize profits subject to their own constraints, including demand for goods produced, technology, and the price of inputs. For the consumer, that point comes where marginal utility of a good, net of price, reaches zero, leaving no net gain from further consumption increases. Analogously, the producer compares marginal revenue (identical to price for the perfect competitor) against the marginal cost of a good, with "marginal profit" the difference. At the point where marginal profit reaches zero, further increases in production of the good stop. For movement to market equilibrium and for changes in equilibrium, price and quantity also change "at the margin": more-or-less of something, rather than necessarily all-or-nothing.
Other applications of demand and supply include the distribution of income among the factors of production, including labour and capital, through factor markets. In a competitive labour market for example the quantity of labour employed and the price of labour (the wage rate) depends on the demand for labour (from employers for production) and supply of labour (from potential workers). Labour economics examines the interaction of workers and employers through such markets to explain patterns and changes of wages and other labour income, labour mobility, and (un)employment, productivity through human capital, and related public-policy issues.
Demand-and-supply analysis is used to explain the behavior of perfectly competitive markets, but as a standard of comparison it can be extended to any type of market. It can also be generalized to explain variables across the economy, for example, total output (estimated as real GDP) and the general price level, as studied in macroeconomics. Tracing the qualitative and quantitative effects of variables that change supply and demand, whether in the short or long run, is a standard exercise in applied economics. Economic theory may also specify conditions such that supply and demand through the market is an efficient mechanism for allocating resources.
Firms.
People frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through "firms". The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase people begin to organise their production in firms when the costs of doing business becomes lower than doing it on the market. Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading.
In perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organization generalizes from that special case to study the strategic behavior of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly.
Managerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimize business decisions, including unit-cost minimization and profit maximization, given the firm's objectives and constraints imposed by technology and market conditions.
Uncertainty and game theory.
Uncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behavior would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.
Game theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organization, discussed above, to model different types of firm behavior, for example in an oligopolistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. As a method heavily used in behavioral economics, it postulates that agents choose strategies to maximize their payoffs, given the strategies of other agents with at least partially conflicting interests.
In this, it generalizes maximization approaches developed to analyze market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic "Theory of Games and Economic Behavior" by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as formulation of nuclear strategies, ethics, political science, and evolutionary biology.
Risk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyzes the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets, financial crises, and related government policy or regulation.
Some market organizations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's "Market for Lemons" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a "lemon" depress its price below what a quality second-hand car would be. Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behavior (say more reckless driving).
Both problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market ("incomplete markets"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care. Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure.
Market failure.
The term "market failure" encompasses several problems which may undermine standard economic assumptions. Although economists categorise market failures differently, the following categories emerge in the main texts.
Information asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above.
Natural monopoly, or the overlapping concepts of "practical" and "technical" monopoly, is an extreme case of "failure of competition" as a restraint on producers. Extreme economies of scale are one possible cause.
Public goods are goods which are undersupplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time.
Externalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidize or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities. Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply.
In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesized long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition.
Some specialised fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or "public bads".
Policy options include regulations that reflect cost-benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights.
Public sector.
Public finance is the field of economics that deals with budgeting the revenues and expenditures of a public sector entity, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost-benefit analysis of government programs, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behavior analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats.
Much of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies "ought" to be like.
Welfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society.
Macroeconomics.
Macroeconomics examines the economy as a whole to explain broad aggregates and their interactions "top down", that is, using a simplified form of general-equilibrium theory. Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy.
Since at least the 1960s, macroeconomics has been characterized by further integration as to micro-based modeling of sectors, including rationality of players, efficient use of market information, and imperfect competition. This has addressed a long-standing concern about inconsistent developments of the same subject.
Macroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth.
Growth.
"Growth economics" studies factors that explain economic growth – the increase in output per capita of a country over a long period of time. The same factors are used to explain differences in the "level" of output per capita "between" countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth.
Much-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting.
Business cycle.
The economics of a depression were the spur for the creation of "macroeconomics" as a separate discipline field of study. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled "The General Theory of Employment, Interest and Money" outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output.
He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government to stabilize output over the business cycle.
Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of "The General Theory".
Over the years, understanding of the business cycle has branched into various research programs, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with neoclassical economics, stating that Keynesianism is correct in the short run but qualified by neoclassical-like considerations in the intermediate and long run.
New classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and "rational expectations" theory, lead by Robert Lucas, and real business cycle theory.
In contrast, the new Keynesian approach retains the rational expectations assumption, however it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are "sticky", which means they do not adjust instantaneously to changes in economic conditions.
Thus, the new classicals assume that prices and wages adjust automatically to attain full employment, whereas the new Keynesians see full employment as being automatically achieved only in the long run, and hence government and central-bank policies are needed because the "long run" may be very long.
Unemployment.
The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labor force. Unemployment can be generally broken down into several types that are related to different causes.
Classical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Wages may be too high because of minimum wage laws or union activity. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment.
Structural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process.
While some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.
Inflation and monetary policy.
Money is a "means of final payment" for goods in most price system economies and the unit of account in which prices are typically stated. An apt statement by Francis Amasa Walker, a well-known economist is, "Money is what money does." Money has a general acceptability, a relative consistency in value, divisibility, durability, portability, elastic in supply and survives with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others.
As a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialized producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces.
At the level of an economy, theory and evidence are consistent with a positive relationship running from the total money supply to the nominal value of total output and to the general price level. For this reason, management of the money supply is a key aspect of monetary policy.
Fiscal policy.
Governments implement fiscal policy by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government.
For example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity.
The effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government use resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed.
Skeptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from fiscal policy will be offset by the increased savings rate intended to pay for future higher taxes.
International economics.
International trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalization.
The distinct field of "development economics" examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors.
Economic systems is the of economics that studies the methods and institutions by which societies determine the ownership, direction, and allocation of economic resources. An "economic system" of a society is the unit of analysis.
Among contemporary systems at different ends of the organizational spectrum are socialist systems and capitalist systems, in which most production occurs in respectively state-run and private enterprises. In between are mixed economies. A common element is the interaction of economic and political influences, broadly described as political economy. "Comparative economic systems" studies the relative performance and behavior of different economies or systems.
Practice.
Contemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science. Professional economists are expected to be familiar with these tools, while a minority specialize in econometrics and mathematical methods.
Theory.
Mainstream economic theory relies upon a priori quantitative economic models, which employ a variety of concepts. Theory typically proceeds with an assumption of "ceteris paribus", which means holding constant explanatory variables other than the one under consideration. When creating theories, the objective is to find ones which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories.
In microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm. Early macroeconomic models focused on modeling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models in microfoundations.
The aforementioned microeconomic concepts play a major part in macroeconomic models – for instance, in monetary theory, the quantity theory of money predicts that increases in the money supply increase inflation, and inflation is assumed to be influenced by rational expectations. In development economics, slower growth in developed nations has been sometimes predicted because of the declining marginal returns of investment and capital, and this has been observed in the Four Asian Tigers. Sometimes an economic hypothesis is only "qualitative", not "quantitative".
Expositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, Paul Samuelson's treatise "Foundations of Economic Analysis" (1947) used mathematical methods to represent the theory, particularly as to maximizing behavioral relations of agents reaching equilibrium. The book focused on examining the class of statements called "operationally meaningful theorems" in economics, which are theorems that can conceivably be refuted by empirical data.
Empirical investigation.
Economic theories are frequently tested empirically, largely through the use of econometrics using economic data. The controlled experiments common to the physical sciences are difficult and uncommon in economics, and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments.
Statistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance ("signal strength") of the hypothesized relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs.
Criticism based on professional standards and non-replicability of results serve as further checks against bias, errors, and over-generalization, although much economic research has been accused of being non-replicable, and prestigious journals have been accused of not facilitating replication through the provision of the code and data. Like theories, uses of test statistics are themselves open to critical analysis, although critical commentary on papers in economics in prestigious journals such as the "American Economic Review" has declined precipitously in the past 40 years. This has been attributed to journals' incentives to maximize citations in order to rank higher on the Social Science Citation Index (SSCI).
In applied economics, input-output models employing linear programming methods are quite common. Large amounts of data are run through computer programs to analyze the impact of certain policies; IMPLAN is one well-known example.
Experimental economics has promoted the use of scientifically controlled experiments. This has reduced long-noted distinction of economics from natural sciences allowed direct tests of what were previously taken as axioms. In some cases these have found that the axioms are not entirely correct; for example, the ultimatum game has revealed that people reject unequal offers.
In behavioral economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences. These techniques have led some to argue that economics is a "genuine science."
Profession.
The professionalization of economics, reflected in the growth of graduate programs on the subject, has been described as "the main change in economics since around 1900". Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study.
In the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national Treasury, Central Bank or Bureau of Statistics.
The Nobel Memorial Prize in Economic Sciences (commonly known as the Nobel Prize in Economics) is a prize awarded to economists each year for outstanding intellectual contributions in the field.
Related subjects.
Economics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, , family economics and institutional economics.
Law and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be. A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities.
Political economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behavior, and externalities should impact government policy. Historians have employed "political economy" to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests.
Energy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics.
The sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity). Classic works include Max Weber's "The Protestant Ethic and the Spirit of Capitalism" (1905) and Georg Simmel's "The Philosophy of Money" (1900). More recently, the works of Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field.
History.
Economic writings date from earlier Mesopotamian, Greek, Roman, Indian subcontinent, Chinese, Persian, and Arab civilizations. Notable writers from antiquity through to the 14th century include Aristotle, Xenophon, Chanakya (also known as Kautilya), Qin Shi Huang, Thomas Aquinas, and Ibn Khaldun. Joseph Schumpeter described Aquinas as "coming nearer than any other group to being the 'founders' of scientific economics" as to monetary, interest, and value theory within a natural-law perspective.
Two groups, later called "mercantilists" and "physiocrats", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing cheap raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies.
Physiocrats, a group of 18th century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth. Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire, which called for minimal government intervention in the economy.
Adam Smith (1723–1790) was an early economic theorist. Smith was harshly critical of the mercantilists but described the physiocratic system "with all its imperfections" as "perhaps the purest approximation to the truth that has yet been published" on the subject.
Classical political economy.
The publication of Adam Smith's "The Wealth of Nations" in 1776, has been described as "the effective birth of economics as a separate discipline." The book identified land, labor, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the Physiocratic idea that only agriculture was productive.
Smith discusses potential benefits of specialization by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries. His "theorem" that "the division of labor is limited by the extent of the market" has been described as the "core of a theory of the functions of firm and industry" and a "fundamental principle of economic organization." To Smith has also been ascribed "the most important substantive proposition in all of economics" and foundation of resource-allocation theory – that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment).
In an argument that includes "one of the most famous passages in all economics," Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society, and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce. In this:
Economists have linked Smith's invisible-hand concept to his concern for the common man and woman through economic growth and development, enabling higher levels of consumption, which Smith describes as "the sole end and purpose of all production."
He embeds the "invisible hand" in a framework that includes limiting restrictions on competition and foreign trade by government and industry in the same chapter and elsewhere regulation of banking and the interest rate, provision of a "natural system of liberty" — national defence, an egalitarian justice and legal system, and certain institutions and public works with general benefits to the whole society that might otherwise be unprofitable to produce, such as education and roads, canals, and the like. An influential introductory textbook includes parallel discussion and this assessment: "Above all, it is Adam Smith's vision of a self-regulating invisible hand that is his enduring contribution to modern economics."
The Rev. Thomas Robert Malthus (1798) used the idea of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level. Economist Julian Lincoln Simon has criticised Malthus's conclusions.
While Adam Smith emphasized the production of income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was the first to state and prove the principle of comparative advantage, according to which each country should specialize in producing and exporting goods in that it has a lower "relative" cost of production, rather relying only on its own production. It has been termed a "fundamental analytical explanation" for gains from trade.
Coming at the end of the Classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene.
Value theory was important in classical theory. Smith wrote that the "real price of every thing ... is the toil and trouble of acquiring it" as influenced by its scarcity. Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity. Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of markets to move to long-run equilibrium.
Marxism.
Marxist (later, Marxian) economics descends from classical economics. It derives from the work of Karl Marx. The first volume of Marx's major work, "Das Kapital", was published in German in 1867. In it, Marx focused on the labour theory of value and the theory of surplus value which, he believed, explained the exploitation of labour by capital. The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production and the theory of surplus value demonstrated how the workers only got paid a proportion of the value their work had created. The U.S. Export-Import Bank defines a Marxist-Lenninist state as having a centrally planned economy. They are now rare, examples can still be seen in Cuba, North Korea and Laos.
Neoclassical economics.
At the dawn as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his "Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth" (1803). These three items are considered by the science only in relation to the increase or diminution of wealth, and not in reference to their processes of execution. Say's definition has prevailed up to our time, saved by substituting the word "wealth" for "goods and services" meaning that wealth may include non material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed, because many economists were making theoretical and philosophical inroads in other areas of human activity. In his "Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of a particular aspect of human behavior, the one that falls under the influence of scarcity, which forces people to choose, allocate scarce resources to competing ends, and economize (seeking the greatest welfare while avoiding the wasting of scarce resources). For Robbins, the insufficiency was solved, and his definition allows us to proclaim, with an easy conscience, education economics, safety and security economics, health economics, war economics, and of course, production, distribution and consumption economics as valid subjects of the economic science.
Citing Robbins: "Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses". After discussing it for decades, Robbins' definition became widely accepted by mainstream economists, and it has opened way into current textbooks. Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition. Due to the lack of strong consensus, and that production, distribution and consumption of goods and services is the prime area of study of economics, the old definition still stands in many quarters.
A body of theory later termed "neoclassical economics" or "marginalism" formed from about 1870 to 1910. The term "economics" was popularized by such neoclassical economists as Alfred Marshall as a concise synonym for 'economic science' and a substitute for the earlier "political economy". This corresponded to the influence on the subject of mathematical methods used in the natural sciences.
Neoclassical economics systematized supply and demand as joint determinants of price and quantity in market equilibrium, affecting both the allocation of output and the distribution of income. It dispensed with the labour theory of value inherited from classical economics in favor of a marginal utility theory of value on the demand side and a more general theory of costs on the supply side. In the 20th century, neoclassical theorists moved away from an earlier notion suggesting that total utility for a society could be measured in favor of ordinal utility, which hypothesizes merely behavior-based relations across persons.
In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded. In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics.
Neoclassical economics is occasionally referred as "orthodox economics" whether by its critics or sympathizers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalize earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analyzing long-run variables affecting national income.
Neoclassical economics studies the behavior of individuals, households, and organizations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more resource-controlling players to attain the best possible outcome under bounded rational conditions. In other words, resource-controlling agents maximize value subject to the constraints imposed by the information the agents have, their cognitive limitations, and the finite amount of time they have to make and execute a decision. Economic science centers on the activities of the economic agents that comprise society. They are the focus of economic analysis.
An approach to understanding these processes, through the study of agent behavior under scarcity, may go as follows:
The continuous interplay (exchange or trade) done by economic actors in all markets sets the prices for all goods and services which, in turn, make the rational managing of scarce resources possible. At the same time, the decisions (choices) made by the same actors, while they are pursuing their own interest, determine the level of output (production), consumption, savings, and investment, in an economy, as well as the remuneration (distribution) paid to the owners of labor (in the form of wages), capital (in the form of profits) and land (in the form of rent). Each period, as if they were in a giant feedback system, economic players influence the pricing processes and the economy, and are in turn influenced by them until a steady state (equilibrium) of all variables involved is reached or until an external shock throws the system toward a new equilibrium point. Because of the autonomous actions of rational interacting agents, the economy is a complex adaptive system.
Keynesian economics.
Keynesian economics derives from John Maynard Keynes, in particular his book "The General Theory of Employment, Interest and Money" (1936), which ushered in contemporary macroeconomics as a distinct field. The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low "effective demand" and why even price flexibility and monetary policy might be unavailing. The term "revolutionary" has been applied to the book in its impact on economic analysis.
Keynesian economics has two successors. Post-Keynesian economics also concentrates on macroeconomic rigidities and adjustment processes. Research on micro foundations for their models is represented as based on real-life practices rather than simple optimizing models. It is generally associated with the University of Cambridge and the work of Joan Robinson.
New-Keynesian economics is also associated with developments in the Keynesian fashion. Within this group researchers tend to share with other economists the emphasis on models employing micro foundations and optimizing behavior but with a narrower focus on standard Keynesian themes such as price and wage rigidity. These are usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones.
Chicago school of economics.
The Chicago School of economics is best known for its free market advocacy and monetarist ideas. According to Milton Friedman and monetarists, market economies are inherently stable if the money supply does not greatly expand or contract. Ben Bernanke, former Chairman of the Federal Reserve, is among the economists today generally accepting Friedman's analysis of the causes of the Great Depression.
Milton Friedman effectively took many of the basic principles set forth by Adam Smith and the classical economists and modernized them. One example of this is his article in the September 1970 issue of The New York Times Magazine, where he claims that the social responsibility of business should be "to use its resources and engage in activities designed to increase its profits ... (through) open and free competition without deception or fraud."
Other schools and approaches.
Other well-known schools or trends of thought referring to a particular style of economics practiced at and disseminated from well-defined groups of academicians that have become known worldwide, include the Austrian School, the Freiburg School, the School of Lausanne, post-Keynesian economics and the Stockholm school. Contemporary mainstream economics is sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago-school approach.
Within macroeconomics there is, in general order of their appearance in the literature; classical economics, Keynesian economics, the neoclassical synthesis, post-Keynesian economics, monetarism, new classical economics, and supply-side economics. Alternative developments include ecological economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, feminist economics and biophysical economics.
Criticisms.
General criticisms.
"The dismal science" is a derogatory alternative name for economics devised by the Victorian historian Thomas Carlyle in the 19th century. It is often stated that Carlyle gave economics the nickname "the dismal science" as a response to the late 18th century writings of The Reverend Thomas Robert Malthus, who grimly predicted that starvation would result, as projected population growth exceeded the rate of increase in the food supply. However, the actual phrase was coined by Carlyle in the context of a debate with John Stuart Mill on slavery, in which Carlyle argued for slavery, while Mill opposed it.
Some economists, like John Stuart Mill or Léon Walras, have maintained that the production of wealth should not be tied to its distribution.
In "The Wealth of Nations", Adam Smith addressed many issues that are currently also the subject of debate and dispute. Smith repeatedly attacks groups of politically aligned individuals who attempt to use their collective influence to manipulate a government into doing their bidding. In Smith's day, these were referred to as factions, but are now more commonly called special interests, a term which can comprise international bankers, corporate conglomerations, outright oligopolies, monopolies, trade unions and other groups.
Economics per se, as a social science, is independent of the political acts of any government or other decision-making organization, however, many policymakers or individuals holding highly ranked positions that can influence other people's lives are known for arbitrarily using a plethora of economic concepts and rhetoric as vehicles to legitimize agendas and value systems, and do not limit their remarks to matters relevant to their responsibilities. The close relation of economic theory and practice with politics is a focus of contention that may shade or distort the most unpretentious original tenets of economics, and is often confused with specific social agendas and value systems.
Notwithstanding, economics legitimately has a role in informing government policy. It is, indeed, in some ways an outgrowth of the older field of political economy. Some academic economic journals are currently focusing increased efforts on gauging the consensus of economists regarding certain policy issues in hopes of effecting a more informed political environment. Currently, there exists a low approval rate from professional economists regarding many public policies. Policy issues featured in a recent survey of AEA economists include trade restrictions, social insurance for those put out of work by international competition, genetically modified foods, curbside recycling, health insurance (several questions), medical malpractice, barriers to entering the medical profession, organ donations, unhealthy foods, mortgage deductions, taxing internet sales, Wal-Mart, casinos, ethanol subsidies, and inflation targeting.
In "Steady State Economics" 1977, Herman Daly argues that there exist logical inconsistencies between the emphasis placed on economic growth and the limited availability of natural resources.
Issues like central bank independence, central bank policies and rhetoric in central bank governors discourse or the premises of macroeconomic policies (monetary and fiscal policy) of the state, are focus of contention and criticism.
Deirdre McCloskey has argued that many empirical economic studies are poorly reported, and she and Stephen Ziliak argue that although her critique has been well-received, practice has not improved. This latter contention is controversial.
A 2002 International Monetary Fund study looked at "consensus forecasts" (the forecasts of large groups of economists) that were made in advance of 60 different national recessions in the 1990s: in 97% of the cases the economists did not predict the contraction a year in advance. On those rare occasions when economists did successfully predict recessions, they significantly underestimated their severity.
Criticisms of assumptions.
Economics has been subject to criticism that it relies on unrealistic, unverifiable, or highly simplified assumptions, in some cases because these assumptions simplify the proofs of desired conclusions. Examples of such assumptions include perfect information, profit maximization and rational choices. The field of information economics includes both mathematical-economical research and also behavioral economics, akin to studies in behavioral psychology.
Nevertheless, prominent mainstream economists such as Keynes and Joskow have observed that much of economics is conceptual rather than quantitative, and difficult to model and formalize quantitatively. In a discussion on oligopoly research, Paul Joskow pointed out in 1975 that in practice, serious students of actual economies tended to use "informal models" based upon qualitative factors specific to particular industries. Joskow had a strong feeling that the important work in oligopoly was done through informal observations while formal models were "trotted out "ex post"". He argued that formal models were largely not important in the empirical work, either, and that the fundamental factor behind the theory of the firm, behavior, was neglected.
In recent years, feminist critiques of neoclassical economic models gained prominence, leading to the formation of feminist economics. Contrary to common conceptions of economics as a positive and objective science, feminist economists call attention to the social construction of economics and highlight the ways in which its models and methods reflect masculine preferences. Primary criticisms focus on failures to account for: the selfish nature of actors (homo economicus); exogenous tastes; the impossibility of utility comparisons; the exclusion of unpaid work; and the exclusion of class and gender considerations. Feminist economics developed to address these concerns, and the field now includes critical examinations of many areas of economics including paid and unpaid work, economic epistemology and history, globalization, household economics and the care economy. In 1988, Marilyn Waring published the book "If Women Counted", in which she argues that the discipline of economics ignores women's unpaid work and the value of nature; according to Julie A. Nelson, "If Women Counted" "showed exactly how the unpaid work traditionally done by women has been made invisible within national accounting systems" and "issued a wake-up call to issues of ecological sustainability." Bjørnholt and McKay argue that the financial crisis of 2007–08 and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession. They argue that such a reshaping should include new advances within feminist economics that take as their starting point the socially responsible, sensible and accountable subject in creating an economy and economic theories that fully acknowledge care for each other as well as the planet.
Philip Mirowski observes that
In a series of peer-reviewed journal and conference papers and books published over a period of several decades, John McMurtry has provided extensive criticism of what he terms the "unexamined assumptions and implications [of economics], and their consequent cost to people's lives."
Nassim Nicholas Taleb and Michael Perelman are two additional scholars who criticized conventional or mainstream economics. Taleb opposes most economic theorizing, which in his view suffers acutely from the problem of overuse of Plato's Theory of Forms, and calls for cancellation of the Nobel Memorial Prize in Economics, saying that the damage from economic theories can be devastating. Michael Perelman provides extensive criticism of economics and its assumptions in all his books (and especially his books published from 2000 to date), papers and interviews.
Despite these concerns, mainstream graduate programs have become increasingly technical and mathematical.
See also.
General:

</doc>
<doc id="9225" url="http://en.wikipedia.org/wiki?curid=9225" title="Electronic paper">
Electronic paper

Electronic paper, e-paper and electronic ink are display technologies that mimic the appearance of ordinary ink on paper. Unlike conventional backlit flat panel displays that emit light, electronic paper displays reflect light like paper. This may make them more comfortable to read, and provide a wider viewing angle than most light-emitting displays. The contrast ratio in electronic displays available as of 2008 approaches newspaper, and newly developed displays are slightly better. An ideal e-paper display can be read in direct sunlight without the image appearing to fade.
Many electronic paper technologies hold static text and images indefinitely without electricity. Flexible electronic paper uses plastic substrates and plastic electronics for the display backplane. There is ongoing competition among manufacturers to provide full-color ability.
Applications of electronic visual displays include electronic pricing labels in retail shops, and digital signage, time tables at bus stations, electronic billboards, mobile phone displays, and e-readers able to display digital versions of books and e-paper magazines.
Technologies.
Gyricon.
Electronic paper was fir"s"t developed in the 1970s by Nick Sheridon at Xerox's Palo Alto Research Center. The first electronic paper, called Gyricon, consisted of polyethylene spheres between 75 and 106 micrometers across. Each sphere is a janus particle composed of negatively charged black plastic on one side and positively charged white plastic on the other (each bead is thus a dipole). The spheres are embedded in a transparent silicone sheet, with each sphere suspended in a bubble of oil so that they can rotate freely. The polarity of the voltage applied to each pair of electrodes then determines whether the white or black side is face-up, thus giving the pixel a white or black appearance.
At the FPD 2008 exhibition, Japanese company Soken demonstrated a wall with electronic wall-paper using this technology. From 2007 Estonian company Visitret Displays is developing this kind of displays using PVDF as material for spheres dramatically improving the video speed and decreasing the control voltage.
Electrophoretic.
In the simplest implementation of an electrophoretic display, titanium dioxide (titania) particles approximately one micrometer in diameter are dispersed in a hydrocarbon oil. A dark-colored dye is also added to the oil, along with surfactants and charging agents that cause the particles to take on an electric charge. This mixture is placed between two parallel, conductive plates separated by a gap of 10 to 100 micrometres. When a voltage is applied across the two plates, the particles migrate electrophoretically to the plate that bears the opposite charge from that on the particles. When the particles are located at the front (viewing) side of the display, it appears white, because light is scattered back to the viewer by the high-index titania particles. When the particles are located at the rear side of the display, it appears dark, because the incident light is absorbed by the colored dye. If the rear electrode is divided into a number of small picture elements (pixels), then an image can be formed by applying the appropriate voltage to each region of the display to create a pattern of reflecting and absorbing regions.
Electrophoretic displays are considered prime examples of the electronic paper category, because of their paper-like appearance and low power consumption.
Examples of commercial electrophoretic displays include the high-resolution active matrix displays used in the Amazon Kindle, Barnes & Noble Nook, Sony Librie, Sony Reader, Kobo eReader and iRex iLiad e-readers. These displays are constructed from an electrophoretic imaging film manufactured by E Ink Corporation. Another early commercial product to use the technology is the Motorola MOTOFONE.
Electrophoretic Display technology has also been developed by Sipix and Bridgestone/Delta.
SiPix Imaging Inc. is now part of E Ink. The Sipix design uses a flexible 0.15mm Microcup architecture, instead of E Ink's 0.04mm diameter microcapsules.
Bridgestone Corp.'s Advanced Materials Division cooperated with Delta Optoelectronics Inc. in developing the Quick Response Liquid Powder Display (QR-LPD) technology. 
Electrophoretic displays can be manufactured using the Electronics on Plastic by Laser Release (EPLaR) process developed by Philips Research to enable existing AM-LCD manufacturing plants to create flexible plastic displays.
Electrophoretic display.
An electrophoretic display forms images by rearranging charged pigment particles with an applied electric field.
In the 1990s another type of electronic paper was invented by Joseph Jacobson, who later co-founded the E Ink Corporation, which formed a partnership with Philips Components two years later to develop and market the technology. In 2005, Philips sold the electronic paper business as well as its related patents to Prime View International. This used tiny microcapsules filled with electrically charged white particles suspended in a colored oil. In early versions, the underlying circuitry controlled whether the white particles were at the top of the capsule (so it looked white to the viewer) or at the bottom of the capsule (so the viewer saw the color of the oil). This was essentially a reintroduction of the well-known electrophoretic display technology, but microcapsules meant the display could be made on flexible plastic sheets instead of glass.
One early version of electronic paper consists of a sheet of very small transparent capsules, each about 40 micrometres across. Each capsule contains an oily solution containing black dye (the electronic ink), with numerous white titanium dioxide particles suspended within. The particles are slightly negatively charged, and each one is naturally white.
The screen holds microcapsules in a layer of liquid polymer, sandwiched between two arrays of electrodes, the upper of which is transparent. The two arrays are aligned to divide the sheet into pixels, and each pixel corresponds to a pair of electrodes situated either side of the sheet. The sheet is laminated with transparent plastic for protection, resulting in an overall thickness of 80 micrometres, or twice that of ordinary paper.
The network of electrodes connects to display circuitry, which turns the electronic ink 'on' and 'off' at specific pixels by applying a voltage to specific electrode pairs. A negative charge to the surface electrode repels the particles to the bottom of local capsules, forcing the black dye to the surface and turning the pixel black. Reversing the voltage has the opposite effect. It forces the particles to the surface, turning the pixel white. A more recent implementation of this concept requires only one layer of electrodes beneath the microcapsules.
Electrowetting.
Electro-wetting display (EWD) is based on controlling the shape of a confined water/oil interface by an applied voltage. With no voltage applied, the (coloured) oil forms a flat film between the water and a hydrophobic (water-repellent) insulating coating of an electrode, resulting in a coloured pixel.
When a voltage is applied between the electrode and the water, the interfacial tension between the water and the coating changes. As a result the stacked state is no longer stable, causing the water to move the oil aside.
This makes a partly transparent pixel, or, if a reflective white surface is under the switchable element, a white pixel. Because of the small pixel size, the user only experiences the average reflection, which provides a high-brightness, high-contrast switchable element.
Displays based on electro-wetting provide several attractive features. The switching between white and coloured reflection is fast enough to display video content.
It's a low-power and low-voltage technology, and displays based on the effect can be made flat and thin. The reflectivity and contrast are better than or equal to other reflective display types and approach the visual qualities of paper.
In addition, the technology offers a unique path toward high-brightness full-colour displays, leading to displays that are four times brighter than reflective LCDs and twice as bright as other emerging technologies.
Instead of using red, green and blue (RGB) filters or alternating segments of the three primary colours, which effectively result in only one third of the display reflecting light in the desired colour, electro-wetting allows for a system in which one sub-pixel can switch two different colours independently.
This results in the availability of two thirds of the display area to reflect light in any desired colour. This is achieved by building up a pixel with a stack of two independently controllable coloured oil films plus a colour filter.
The colours are cyan, magenta and yellow, which is a subtractive system, comparable to the principle used in inkjet printing for example. Compared to LCD another factor two in brightness is gained because no polarisers are required.
Examples of commercial electrowetting displays include Liquavista, ITRI, PVI and ADT.
Miortech’s 2nd generation electrowetting display technology solves a number of issues of 1st generation electrowetting display technology and large-area devices are easy to manufacture since the pixel walls act as spacers. Miortech develops rearview mirrors using its 2nd generation EWD technology.
Electrofluidic.
Electrofluidic displays are a variation of an electrowetting display. Electrofluidic displays place an aqueous pigment dispersion inside a tiny reservoir. The reservoir comprises <5-10% of the viewable pixel area and therefore the pigment is substantially hidden from view. Voltage is used to electromechanically pull the pigment out of the reservoir and spread it as a film directly behind the viewing substrate. As a result, the display takes on color and brightness similar to that of conventional pigments printed on paper. When voltage is removed liquid surface tension causes the pigment dispersion to rapidly recoil into the reservoir. As reported in the May 2009 Issue of Nature Photonics, the technology can potentially provide >85% white state reflectance for electronic paper.
The core technology was invented at the Novel Devices Laboratory at the University of Cincinnati. The technology is currently being commercialized by Gamma Dynamics.
Interferometric modulator (Mirasol).
Technology used in electronic visual displays that can create various colors via interference of reflected light. The color is selected with an electrically switched light modulator comprising a microscopic cavity that is switched on and off using driver integrated circuits similar to those used to address liquid crystal displays (LCD).
Other technologies.
Other research efforts into e-paper have involved using organic transistors embedded into flexible substrates, including attempts to build them into conventional paper.
Simple color e-paper consists of a thin colored optical filter added to the monochrome technology described above. The array of pixels is divided into triads, typically consisting of the standard cyan, magenta and yellow, in the same way as CRT monitors (although using subtractive primary colors as opposed to additive primary colors). The display is then controlled like any other electronic color display.
Examples of electrochromic displays include Acreo, Ajjer, Aveso and Ntera.
Disadvantages.
Electronic paper technologies have a very low refresh rate compared to other low-power display technologies, such as LCD. This prevents producers from implementing sophisticated interactive applications (using fast moving menus, mouse pointers or scrolling) like those possible on mobile devices. An example of this limit is that a document cannot be smoothly zoomed without either extreme blurring during the transition or a very slow zoom.
Another limit is that a shadow of an image may be visible after refreshing parts of the screen. Such shadows are termed "ghost images", and the effect is termed "ghosting". This effect is reminiscent of screen burn-in but, unlike it, is solved after the screen is refreshed several times. Turning every pixel white, then black, then white, helps normalize the contrast of the pixels. This is why several devices with this technology "flash" the entire screen white and black when loading a new image.
No company has yet successfully brought a full color display to market.
Electronic paper is still a topic in the R&D community and remains under development for manufacturability, marketability, and reliability considerations. 
Applications.
Several companies are simultaneously developing electronic paper and ink. While the technologies used by each company provide many of the same features, each has its own distinct technological advantages. All electronic paper technologies face the following general challenges:
Electronic ink can be applied to flexible or rigid materials. For flexible displays, the base requires a thin, flexible material tough enough to withstand considerable wear, such as extremely thin plastic. The method of how the inks are encapsulated and then applied to the substrate is what distinguishes each company from others. These processes are complex and are carefully guarded industry secrets. Nevertheless, making electronic paper is less complex and costly than LCDs.
There are many approaches to electronic paper, with many companies developing technology in this area. Other technologies being applied to electronic paper include modifications of liquid crystal displays, electrochromic displays, and the electronic equivalent of an Etch A Sketch at Kyushu University. Advantages of electronic paper includes low power usage (power is only drawn when the display is updated), flexibility and better readability than most displays. Electronic ink can be printed on any surface, including walls, billboards, product labels and T-shirts. The ink's flexibility would also make it possible to develop rollable displays for electronic devices.
Wristwatches.
In December 2005 Seiko released the first electronic ink based watch called the Spectrum SVRD001 wristwatch, which has a flexible electrophoretic display and in March 2010 Seiko released a second generation of this famous e-ink watch with an active matrix display. The Pebble smart watch (2013) uses a low-power memory LCD manufactured by Sharp for its e-paper display.
e-Books.
In 2004 Sony released Librié EBR-1000EP in Japan, the first e-book reader with an electronic paper display. In September 2006 Sony released the PRS-500 Sony Reader e-book reader in the USA. On October 2, 2007, Sony announced the PRS-505, an updated version of the Reader. In November 2008, Sony released the PRS-700BC, which incorporated a backlight and a touchscreen.
In late 2007, Amazon began producing and marketing the Amazon Kindle, an e-book reader with an e-paper display. In February 2009, Amazon released the Kindle 2 and in May 2009 the larger Kindle DX was announced. In July 2010 the third generation Kindle was announced, with notable design changes. The fourth generation of Kindles were announced in September 2011. This generation was unique as it marked the Kindle's first departure from keyboards in favor of touchscreens. In September 2012, Amazon announced the fifth generation of the Kindle, which incorporates a LED frontlight and a higher contrast display.
In November 2009 Barnes and Noble launched the Barnes & Noble Nook, running an Android operating system. It differs from other big name readers in having a replaceable battery, and a separate touch-screen color LCD below the main electronic paper reading screen.
Newspapers.
In February 2006, the Flemish daily "De Tijd" distributed an electronic version of the paper to select subscribers in a limited marketing study, using a pre-release version of the iRex iLiad. This was the first recorded application of electronic ink to newspaper publishing.
The French daily "Les Échos" announced the official launch of an electronic version of the paper on a subscription basis, in September 2007. Two offers were available, combining a one year subscription and a reading device. The offer included either a light (176g) reading device (adapted for Les Echos by Ganaxa) or the iRex iLiad. Two different processing platforms were used to deliver readable information of the daily, one based on the newly developed GPP electronic ink platform from "Ganaxa", and the other one developed internally by Les Echos.
Displays embedded in smart cards.
Flexible display cards enable financial payment cardholders to generate a one-time password to reduce online banking and transaction fraud. Electronic paper offers a flat and thin alternative to existing key fob tokens for data security. The world’s first ISO compliant smart card with an embedded display was developed by Innovative Card Technologies and nCryptone in 2005. The cards were manufactured by Nagra ID.
Status displays.
Some devices, like USB flash drives, have used electronic paper to display status information, such as available storage space. Once the image on the electronic paper has been set, it requires no power to maintain, so the readout can be seen even when the flash drive is not plugged in.
Mobile phones.
Motorola's low-cost mobile phone, the Motorola F3, uses an alphanumeric black-and-white electrophoretic display.
The Samsung Alias 2 mobile phone incorporates electronic ink from E Ink into the keypad, which allows the keypad to change character sets and orientation while in different display modes.
Electronic shelf labels.
E-Paper based electronic shelf labels (ESL) are used to digitally display the prices at retail stores. Electronic paper based labels are updated via two-way infrared or radio technology.
Other.
Other proposed applications include clothes, digital photo frames, information boards and keyboards. Keyboards with dynamically changeable keys are useful for less represented languages, non-standard keyboard layouts such as Dvorak, or for special non-alphabetical applications such as video editing or games.

</doc>
<doc id="9228" url="http://en.wikipedia.org/wiki?curid=9228" title="Earth">
Earth

Earth, also known as the world, Terra, or Gaia, is the third planet from the Sun, the densest planet in the Solar System, the largest of the Solar System's four terrestrial planets, and the only celestial body known to accommodate life. It is home to about 8.74 million species. There are billions of humans who depend upon its biosphere and minerals. The Earth's human population is divided among about two hundred independent states that interact through diplomacy, conflict, travel, trade, and media.
According to evidence from sources such as radiometric dating, Earth was formed around four and a half billion years ago. Within its first billion years, life appeared in its oceans and began to affect its atmosphere and surface, promoting the proliferation of aerobic as well as anaerobic organisms and causing the formation of the atmosphere's ozone layer. This layer and Earth's magnetic field block the most life-threatening parts of the Sun's radiation, so life was able to flourish on land as well as in water. Since then, Earth's position in the Solar System, its physical properties and its geological history have allowed life to persist.
Earth's lithosphere is divided into several rigid segments, or tectonic plates, that migrate across the surface over periods of many millions of years. Over 70% percent of Earth's surface is covered with water, with the remainder consisting of continents and islands which together have many lakes and other sources of water that contribute to the hydrosphere. Earth's poles are mostly covered with ice that is the solid ice of the Antarctic ice sheet and the sea ice that is the polar ice packs. The planet's interior remains active, with a solid iron inner core, a liquid outer core that generates the magnetic field, and a thick layer of relatively solid mantle.
Earth gravitationally interacts with other objects in space, especially the Sun and the Moon. During one orbit around the Sun, the Earth rotates about its own axis 366.26 times, creating 365.26 solar days, or one sidereal year. The Earth's axis of rotation is tilted 23.4° away from the perpendicular of its orbital plane, producing seasonal variations on the planet's surface with a period of one tropical year (365.24 solar days). The Moon is Earth's only natural satellite. It began orbiting the Earth about . The Moon's gravitational interaction with Earth stimulates ocean tides, stabilizes the axial tilt, and gradually slows the planet's rotation.
Name and etymology.
The modern English "Earth" developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled '. It has cognates in every Germanic language and their proto-Germanic root has been reconstructed as *"erþō". In its earliest appearances, "eorðe" was already being used to translate the many senses of Latin ' and Greek ("gē"): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus among the devotees of Nerthus and later Norse mythology included Jörð, a giantess often given as the mother of Thor.
Originally, "earth" was written in lowercase and, from early Middle English, its definite sense as "the globe" was expressed as "the earth". By early Modern English, many nouns were capitalized and "the earth" became (and often remained) "the Earth", particularly when referenced along with other heavenly bodies. More recently, the name is simply given as "Earth", by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes Earth when appearing as a name (e.g., "Earth's atmosphere") but writes it in lowercase when preceded by "the" (e.g., "the atmosphere of the earth"). It almost always appears in lowercase in colloquial expressions such as "what on earth are you doing?"
Composition and structure.
Earth is a terrestrial planet, meaning that it is a rocky body, rather than a gas giant like Jupiter. It is the largest of the four terrestrial planets in size and mass. Of these four planets, Earth also has the highest density, the highest surface gravity, the strongest magnetic field, and fastest rotation, and is probably the only one with active plate tectonics.
Shape.
The shape of the Earth approximates an oblate spheroid, a sphere flattened along the axis from pole to pole such that there is a bulge around the equator. This bulge results from the rotation of the Earth, and causes the diameter at the equator to be (kilometer) larger than the pole-to-pole diameter. Thus the furthest point on the surface from the Earth's center of mass is the Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is about , which is approximately 40,000 km/π, as the meter was originally defined as 1/10,000,000 of the distance from the equator to the North Pole through Paris, France.
Local topography deviates from this idealized spheroid, although on a global scale, these deviations are small: Earth has a tolerance of about one part in about 584, or 0.17%, from the reference spheroid, which is less than the 0.22% tolerance allowed in billiard balls. The largest local deviations in the rocky surface of the Earth are Mount Everest (8,848 m above local sea level) and the Mariana Trench ( below local sea level). Due to the equatorial bulge, the surface locations farthest from the center of the Earth are the summits of Mount Chimborazo in Ecuador and Huascarán in Peru.
Chemical composition.
The mass of the Earth is approximately . It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.
The geochemist F. W. Clarke calculated that a little more than 47% of the Earth's crust consists of oxygen. The more common rock constituents of the Earth's crust are nearly all oxides; chlorine, sulfur and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. The principal oxides are silica, alumina, iron oxides, lime, magnesia, potash and soda. The silica functions principally as an acid, forming silicates, and all the commonest minerals of igneous rocks are of this nature. From a computation based on 1,672 analyses of all kinds of rocks, Clarke deduced that 99.22% were composed of 11 oxides (see the table at right), with the other constituents occurring in minute quantities.
Internal structure.
The interior of the Earth, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties, but unlike the other terrestrial planets, it has a distinct outer and inner core. The outer layer of the Earth is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity, and the thickness of the crust varies: averaging (kilometers) under the oceans and 30- on the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are comprised. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at 410 and below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The inner core may rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year.
Heat.
Earth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes in Earth are potassium-40, uranium-238, uranium-235, and thorium-232. At the center, the temperature may be up to , and the pressure could reach 360 GPa. Because much of the heat is provided by radioactive decay, scientists believe that early in Earth's history, before isotopes with short half-lives had been depleted, Earth's heat production would have been much higher. This extra heat production, twice present-day at approximately , would have increased temperature gradients within Earth, increasing the rates of mantle convection and plate tectonics, and allowing the production of igneous rocks such as komatiites that are not formed today.
The mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes; a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs in the oceans because the crust there is much thinner than that of the continents.
Tectonic plates.
The mechanically rigid outer layer of the Earth, the lithosphere, is broken into pieces called tectonic plates. These plates are rigid segments that move in relation to one another at one of three types of plate boundaries: Convergent boundaries, at which two plates come together, Divergent boundaries, at which two plates are pulled apart, and Transform boundaries, in which two plates slide past one another laterally. Earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur along these plate boundaries. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates, and their motion is strongly coupled with convection patterns inside the Earth's mantle.
As the tectonic plates migrate across the planet, the ocean floor is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes continually recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old in age. The oldest oceanic crust is located in the Western Pacific, and has an estimated age of about . By comparison, the oldest dated continental crust is .
The seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between 50 and . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of 75 mm/year and the Pacific Plate moving 52–69 mm/year. At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of about 21 mm/year.
Surface.
The Earth's terrain varies greatly from place to place. About 70.8% of the surface is covered by water, with much of the continental shelf below sea level. This equates to (139.43 million sq mi). The submerged surface has mountainous features, including a globe-spanning mid-ocean ridge system, as well as undersea volcanoes, oceanic trenches, submarine canyons, oceanic plateaus and abyssal plains. The remaining 29.2% (, or 57.51 million sq mi) not covered by water consists of mountains, deserts, plains, plateaus, and other geomorphologies.
The planetary surface undergoes reshaping over geological time periods due to tectonics and erosion. The surface features built up or deformed through plate tectonics are subject to steady weathering from precipitation, thermal cycles, and chemical effects. Glaciation, coastal erosion, the build-up of coral reefs, and large meteorite impacts also act to reshape the landscape.
The continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on the Earth's surface include quartz, the feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.
The pedosphere is the outermost layer of the Earth that is composed of soil and subject to soil formation processes. It exists at the interface of the lithosphere, atmosphere, hydrosphere and biosphere. The total arable land is 13.31% of the land surface, with 4.71% supporting permanent crops. Close to 40% of the Earth's land surface is used for cropland and pasture, or an estimated 1.3 km2 of cropland and 3.4 km2 of pastureland.
The elevation of the land surface of the Earth varies from the low point of −418 m at the Dead Sea, to a 2005-estimated maximum altitude of 8,848 m at the top of Mount Everest. The mean height of land above sea level is 840 m.
Besides being divided logically into Northern and Southern Hemispheres centered on the earths poles, the earth has been divided arbitrarily into Eastern and Western Hemispheres. The surface of the Earth is traditionally divided into seven continents and various seas. As people settled and organized the planet, nearly all the land was divided into nations. As of 2013, there are about 196 recognized nations. An example of how major geographical regions can be broken down is Africa, America, Antarctica, Asia, Australia, and Europe.
Hydrosphere.
The abundance of water on Earth's surface is a unique feature that distinguishes the "Blue Planet" from others in the Solar System. The Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of 2,000 m. The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of 10,911.4 m.
The mass of the oceans is approximately 1.35 metric tons, or about 1/4400 of the total mass of the Earth. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all the land on Earth were spread evenly, water would rise to an altitude of more than 2.7 km. About 97.5% of the water is saline, while the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is ice.
The average salinity of the Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool, igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño-Southern Oscillation.
Atmosphere.
The atmospheric pressure on the surface of the Earth averages 101.325 kPa, with a scale height of about 8.5 km. It is 78% nitrogen and 21% oxygen, with trace amounts of water vapor, carbon dioxide and other gaseous molecules. The height of the troposphere varies with latitude, ranging between 8 km at the poles to 17 km at the equator, with some variation resulting from weather and seasonal factors.
Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms as well as the formation of the ozone layer which blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life on Earth include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane and ozone are the primary greenhouse gases in the Earth's atmosphere. Without this heat-retention effect, the average surface would be −18 °C, in contrast to the current +15 °C, and life would likely not exist.
Weather and climate.
The Earth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first 11 km of the planet's surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises, and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.
The primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.
Water vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and settles to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land, and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topological features and temperature differences determine the average precipitation that falls in each region.
The amount of solar energy reaching the Earth's decreases with increasing latitude. At higher latitudes the sunlight reaches the surface at lower angles and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about 0.4 °C per degree of latitude away from the equator. The Earth can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates. Climate can also be classified based on the temperature and precipitation, with the climate regions characterized by fairly uniform air masses. The commonly used Köppen climate classification system (as modified by Wladimir Köppen's student Rudolph Geiger) has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes.
Upper atmosphere.
Above the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the Earth's magnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above the Earth's surface, is a working definition for the boundary between atmosphere and space.
Thermal energy causes some of the molecules at the outer edge of the Earth's atmosphere to increase their velocity to the point where they can escape from the planet's gravity. This causes a slow but steady leakage of the atmosphere into space. Because unfixed hydrogen has a low molecular weight, it can achieve escape velocity more readily and it leaks into outer space at a greater rate than other gasses. The leakage of hydrogen into space contributes to the pushing of the Earth from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is believed to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the Earth's atmosphere may have influenced the nature of life that developed on the planet. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.
Magnetic field.
The main part of the Earth's magnetic field is generated in the Earth's core, the site of a dynamo process that converts kinetic energy of fluid convective motion into electromagnetic energy. The field extends outwards from the core, through the mantle, and up to the Earth's surface, where it is, to rough approximation, a dipole. The poles of the dipole are located close to the Earth's geographic poles. At the equator of the magnetic field, the magnetic field strength at the planet's surface is , with global magnetic dipole moment of . The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.
Magnetosphere.
The field forms the magnetosphere, which deflects particles in the solar wind. The sunward edge of the bow shock is located at about 13 times the radius of the Earth. The collision between the magnetic field and the solar wind forms the Van Allen radiation belts, a pair of concentric, torus-shaped regions of energetic charged particles. When the plasma enters the Earth's atmosphere at the magnetic poles, it forms the aurora.
Orbit and rotation.
Rotation.
Earth's rotation period relative to the Sun—its mean solar day—is 86,400 seconds of mean solar time (86,400.0025 SI seconds). As the Earth's solar day is now slightly longer than it was during the 19th century due to tidal acceleration, each day varies between 0 and 2 SI ms longer.
Earth's rotation period relative to the fixed stars, called its "stellar day" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its "sidereal day", is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.
Apart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in the Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or Moon every two minutes; from the planet's surface, the apparent sizes of the Sun and the Moon are approximately the same.
Orbit.
Earth orbits the Sun at an average distance of about 150 million kilometers every 365.2564 mean solar days, or one sidereal year. From Earth, this gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of the Earth averages about 29.8 km/s (107,000 km/h), which is fast enough to travel a distance equal to the planet's diameter, about 12,742 km, in seven minutes, and the distance to the Moon, 384,000 km, in about 3.5 hours.
The Moon revolves with the Earth around a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common revolution around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and the Earth, the Earth revolves in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.4 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.
The Hill sphere, or gravitational sphere of influence, of the Earth is about 1.5 Gm or 1,500,000 km in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.
Earth, along with the Solar System, is situated in the Milky Way galaxy and orbits about 28,000 light years from the center of the galaxy. It is about 20 light years above the galactic plane in the Orion spiral arm.
Axial tilt and seasons.
Due to the axial tilt of the Earth, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes seasonal change in climate, with summer in the northern hemisphere occurring when the North Pole is pointing toward the Sun, and winter taking place when the pole is pointed away. During the summer, the day lasts longer and the Sun climbs higher in the sky. In winter, the climate becomes generally cooler and the days shorter. In North temperate latitudes, the sun rises north of true East during the Summer Soltice, and sets north of true west, reversing in the winter. The sun rises south of true east in the summer for the Southern Temperate Zone, and sets south of true west.
Above the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the southern hemisphere the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.
By astronomical convention, the four seasons are determined by the solstices—the point in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the northern hemisphere, Winter Solstice occurs on about December 21, Summer Solstice is near June 21, Spring Equinox is around March 20 and Autumnal Equinox is about September 23. In the Southern hemisphere, the situation is reversed, with the Summer and Winter Solstices exchanged and the Spring and Autumnal Equinox dates switched.
The angle of the Earth's tilt is relatively stable over long periods of time. The tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of the Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and Moon on the Earth's equatorial bulge. From the perspective of the Earth, the poles also migrate a few meters across the surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. The rotational velocity of the Earth also varies in a phenomenon known as length of day variation.
In modern times, Earth's perihelion occurs around January 3, and the aphelion around July 4. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching the Earth at perihelion relative to aphelion. Since the southern hemisphere is tilted toward the Sun at about the same time that the Earth reaches the closest approach to the Sun, the southern hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the southern hemisphere.
Habitability.
A planet that can sustain life is termed habitable, even if life did not originate there. The Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of the Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere and protective magnetic field all contribute to the current climatic conditions at the surface.
Biosphere.
A planet's life forms are sometimes said to form a "biosphere". The Earth's biosphere is generally believed to have begun evolving about . The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.
Evolution of life.
Highly energetic chemistry is thought to have produced a self-replicating molecule around and half a billion years later the last common ancestor of all life existed. The development of photosynthesis allowed the Sun's energy to be harvested directly by life forms; the resultant oxygen accumulated in the atmosphere and formed a layer of ozone (a form of molecular oxygen [O3]) in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized the surface of Earth. The earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia.
Since the 1960s, it has been hypothesized that severe glacial action between 750 and , during the Neoproterozoic, covered much of the planet in a sheet of ice. This hypothesis has been termed "Snowball Earth", and is of particular interest because it preceded the Cambrian explosion, when multicellular life forms began to proliferate.
Following the Cambrian explosion, about , there have been five major mass extinctions. The most recent such event was , when an asteroid impact triggered the extinction of the (non-avian) dinosaurs and other large reptiles, but spared some small animals such as mammals, which then resembled shrews. Over the past , mammalian life has diversified, and several million years ago an African ape-like animal such as "Orrorin tugenensis" gained the ability to stand upright. This enabled tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which allowed the evolution of the human race. The development of agriculture, and then civilization, allowed humans to influence the Earth in a short time span as no other life form had, affecting both the nature and quantity of other life forms.
Natural resources and land use.
The Earth provides resources that are exploitable by humans for useful purposes. Some of these are non-renewable resources, such as mineral fuels, that are difficult to replenish on a short time scale.
Large deposits of fossil fuels are obtained from the Earth's crust, consisting of coal, petroleum, natural gas and methane clathrate. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed in Earth's crust through a process of ore genesis, resulting from actions of erosion and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.
The Earth's biosphere produces many useful biological products for humans, including (but far from limited to) food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, 5,053 Mha (50.53 million km2) of the Earth's land surface consisted of forest and woodlands, 6,788 Mha (67.88 million km2) was grasslands and pasture, and 1,501 Mha (15.01 million km2) was cultivated as croplands. The estimated amount of irrigated land in 1993 was . Humans also live on the land by using building materials to construct shelters.
Natural and environmental hazards.
Large areas of the Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.
Many localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion, erosion, and introduction of invasive species.
According to the United Nations, a scientific consensus exists linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.
Human geography.
Cartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting the Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.
Earth has reached approximately seven billion human inhabitants as of October 31, 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.
It is estimated that one-eighth of the surface of the Earth is suitable for humans to live on: three-quarters is covered by oceans, while half of the land area is either desert (14%), high mountains (27%), or other unsuitable terrain. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen-Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)
In 2000, 90% of all humans lived in the Northern Hemisphere. Half lived north of 27° N latitude. An estimated 86% of all people live in the Eastern Hemisphere.
Independent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica and the odd unclaimed area of Bir Tawil between Egypt and Sudan. As of 2013, there are 206 sovereign states, including the 193 United Nations member states. In addition, there are 59 dependent territories, and a number of autonomous areas, territories under dispute and other entities. Historically, Earth has never had a sovereign government with authority over the entire globe, although a number of nation-states have striven for world domination and failed.
The United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.
The first human to orbit the Earth was Yuri Gagarin on April 12, 1961. In total, about 487 people have visited outer space and reached Earth orbit as of July 30, 2010, and, of these, twelve have walked on the Moon. Normally the only humans in space are those on the International Space Station. The station's crew, six people, is usually replaced every six months. The furthest humans have travelled from Earth is 400,171 km, achieved during the Apollo 13 mission in 1970.
Cultural and historical viewpoint.
The standard astronomical symbol of the Earth consists of a cross circumscribed by a circle, .
Unlike the rest of the planets in the Solar System, humankind did not begin to view the Earth as a moving object in orbit around the Sun until the 16th century. Earth has often been personified as a deity, in particular a goddess. In many cultures a mother goddess is also portrayed as a fertility deity. Creation myths in many religions recall a story involving the creation of the Earth by a supernatural deity or deities. A variety of religious groups, often associated with fundamentalist branches of Protestantism or Islam, assert that their interpretations of these creation myths in sacred texts are literal truth and should be considered alongside or replace conventional scientific accounts of the formation of the Earth and the origin and development of life. Such assertions are opposed by the scientific community and by other religious groups. A prominent example is the creation–evolution controversy.
In the past, there were varying levels of belief in a flat Earth, but this was displaced by spherical Earth, a concept that has been credited to Pythagoras (6th century BC). Human cultures have developed many views of the planet, including its personification as a planetary deity, its shape as flat, its position as the center of the universe, and in the modern Gaia Principle, as a single, self-regulating organism in its own right.
Chronology.
Formation.
The earliest material found in the Solar System is dated to (bya); therefore, it is inferred that the Earth must have been formed by accretion around this time. By the primordial Earth had formed. The formation and evolution of the Solar System bodies occurred in tandem with the Sun. In theory a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that in tandem with the star. A nebula contains gas, ice grains and dust (including primordial nuclides). In nebular theory planetesimals commence forming as particulate accrues by cohesive clumping and then by gravity. The assembly of the primordial Earth proceeded for 10–. The Moon formed shortly thereafter, about .
The formation of the Moon remains a topic of debate. The working hypothesis is that it formed by accretion from material loosed from the Earth after a Mars-sized object, named Theia, impacted with Earth. The model, however, is not self-consistent. In this scenario, the mass of Theia is 10% of the Earth's mass, it impacts with the Earth in a glancing blow, and some of its mass merges with the Earth. Between approximately 3.8 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon, and by inference, to the Earth.
Geological history.
Earth's atmosphere and oceans formed by volcanic activity and outgassing that included water vapor. The origin of the world's oceans was condensation augmented by water and ice delivered by asteroids, proto-planets, and comets. In this model, atmospheric "greenhouse gases" kept the oceans from freezing while the newly forming Sun was at 70% luminosity. By , the Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.
A crust formed when the molten outer layer of the planet Earth cooled to form a solid as the accumulated water vapor began to act in the atmosphere. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from the earth's interior. On time scales lasting hundreds of millions of years, the supercontinents have formed and broken up three times. Roughly (million years ago), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–, then finally Pangaea, which also broke apart .
The present pattern of ice ages began about and then intensified during the Pleistocene about . High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating every 40–. The last continental glaciation ended 10,000 years ago.
Predicted future.
Estimates on how much longer the planet will be able to continue to support life range from , to as long as . The future of the planet is closely tied to that of the Sun. As a result of the steady accumulation of helium at the Sun's core, the star's total luminosity will slowly increase. The luminosity of the Sun will grow by 10% over the next and by 40% over the next . Climate models indicate that the rise in radiation reaching the Earth is likely to have dire consequences, including the loss of the planet's oceans.
The Earth's increasing surface temperature will accelerate the inorganic CO2 cycle, reducing its concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately 500-. The lack of vegetation will result in the loss of oxygen in the atmosphere, so animal life will become extinct within several million more years. After another billion years all surface water will have disappeared and the mean global temperature will reach (). The Earth is expected to be effectively habitable for about another from that point, although this may be extended up to if the nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.
The Sun, as part of its evolution, will become a red giant in about . Models predict that the Sun will expand to roughly , which is about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, the Earth will move to an orbit from the Sun, when the star reaches its maximum radius. The planet was, therefore, initially expected to escape envelopment by the expanded Sun's sparse outer atmosphere, though most, if not all, remaining life would have been destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that the Earth's orbit will decay due to tidal effects and drag, causing it to enter the red giant Sun's atmosphere and be vaporized. After that, the Sun's core will collapse into a white dwarf, as its outer layers are ejected into space as a planetary nebula. The matter that once made up the Earth will be released into interstellar space, where it may one day become incorporated into a new generation of planets and other celestial bodies.
Moon.
The Moon is a relatively large, terrestrial, planet-like satellite, with a diameter about one-quarter of the Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites orbiting other planets are called "moons" after Earth's Moon.
The gravitational attraction between the Earth and Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit the Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.
Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately 38 mm a year. Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs a year—add up to significant changes. During the Devonian period, for example, (approximately ) there were 400 days in a year, with each day lasting 21.8 hours.
The Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists believe that without this stabilization against the torques applied by the Sun and planets to the Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.
Viewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.
The most widely accepted theory of the Moon's origin, the giant impact theory, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements, and the fact that its composition is nearly identical to that of the Earth's crust.
Asteroids and artificial satellites.
Earth has at least five co-orbital asteroids, including 3753 Cruithne and . A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, of Earth in Earth's orbit around the Sun.
, there are 931 operational, man-made satellites orbiting the Earth. There are also inoperative satellites and over 300,000 pieces of space debris. Earth's largest artificial satellite is the International Space Station.
Notes.
</math>, where "m" is the mass of the Earth, "a" is an Astronomical Unit, and "M" is the mass of the Sun. So the radius in A.U. is about formula_1.</ref>

</doc>
<doc id="9230" url="http://en.wikipedia.org/wiki?curid=9230" title="English Channel">
English Channel

The English Channel (; ), often referred to simply as the Channel (), is an arm of the Atlantic Ocean that separates southern England from northern France, and joins the North Sea to the Atlantic. It is about long and varies in width from at its widest to in the Strait of Dover. It is the smallest of the shallow seas around the continental shelf of Europe, covering an area of some .
Geography.
The International Hydrographic Organization defines the limits of the English Channel as follows:
The IHO defines the southwestern limit of the North Sea as "a line joining the Walde Lighthouse (France, 1°55'E) and Leathercoat Point (England, 51°10'N)". The Walde Lighthouse is 6 km east of Calais (), and Leathercoat Point is at the north end of St Margaret's Bay, Kent ().
The Strait of Dover (French: "Pas de Calais"), at the Channel's eastern end is its narrowest point, while its widest point lies between Lyme Bay and the Gulf of Saint Malo near its midpoint. It is relatively shallow, with an average depth of about at its widest part, reducing to a depth of about between Dover and Calais. From there eastwards the adjoining North Sea continues to shallow to about in the Broad Fourteens where it lies over the watershed of the former land bridge between East Anglia and the Low Countries. It reaches a maximum depth of in the submerged valley of Hurds Deep, west-northwest of Guernsey.
The eastern region along the French coast between Cherbourg and the mouth of the Seine river at Le Havre is frequently referred to as the "Bay of the Seine ()".
There are several major islands in the Channel, the most notable being the Isle of Wight off the English coast, and the Channel Islands, British Crown Dependencies off the coast of France. The Isles of Scilly off the far southwest coast of England are not generally counted as being in the Channel. The coastline, particularly on the French shore, is deeply indented; several small islands close to the coastline, including Chausey and Mont Saint-Michel, are within French jurisdiction. The Cotentin Peninsula in France juts out into the Channel, and the Isle of Wight creates a small parallel channel known as the Solent in English waters. The Celtic Sea is to the west of the Channel.
The Channel is of geologically recent origins, having been dry land for most of the Pleistocene period. It is thought to have been created between 450,000 and 180,000 years ago by two catastrophic glacial lake outburst floods caused by the breaching of the Weald–Artois anticline, a ridge that held back a large proglacial lake in the Doggerland region, now submerged under the North Sea. The flood would have lasted for several months, releasing as much as one million cubic metres of water per second. The cause of the breach is not known but may have been an earthquake or the build-up of water pressure in the lake. The flood carved a large bedrock-floored valley down the length of the Channel, leaving behind streamlined islands and longitudinal erosional grooves characteristic of catastrophic megaflood events. It destroyed the isthmus that connected Britain to continental Europe, although a land bridge across the southern North Sea would have existed intermittently at later times after periods of glaciation resulted in lowering of sea levels.
The Channel acts as a funnel that amplifies the tidal range from less than a metre as observed at sea to more than 6 metres as observed in the Channel Islands, the west coast of the Cotentin Peninsula and the north cost of Britanny. The time difference of about 6 hours between high water at the eastern and western limits of the Channel are indicative of the tidal range being amplified further by resonance.
For the UK Shipping Forecast the Channel is divided into the following areas, from the west:
Name.
The name "English Channel" has been widely used since the early 18th century, possibly originating from the designation in Dutch sea maps from the 16th century onwards. In modern Dutch however, it is known as (with no reference to the word "English"). Historically, it has also been known as the "British Channel" or the "British Sea" having been called the by the 2nd-century geographer Ptolemy. The same name is used on an Italian map of about 1450, which gives the alternative name of —possibly the first recorded use of the "Channel" designation.
The French name has been in use since at least the 17th century. The name is usually said to refer to the Channel's sleeve () shape. However, it is sometimes claimed to derive from a Celtic word meaning "channel" that is also the source of the name for the Minch in Scotland.
In Spain and most Spanish-speaking countries the Channel is referred to as . In Portuguese it is known as . This is not a translation from French: in Portuguese and Spanish, means "stain", while the word for sleeve is – which suggests either a phonetic borrowing from French or a common source. Other languages also use this name, such as Greek () and Italian (). The German name is , literally "sleeve-channel".
The name in Breton ("Mor Breizh") means "Breton Sea", and its Cornish name ("Mor Bretannek") means "British Sea".
History.
Before the end of the Devensian glaciation (the most recent ice age that ended around 10,000 years ago), the British Isles were part of continental Europe, linked by an unbroken Weald-Artois Anticline, which acted as a natural dam that held back a large freshwater pro-glacial lake in the Doggerland region, now submerged under the North Sea. During this period the North Sea and almost all of the British Isles were covered with ice. The lake was fed by meltwater from the Baltic and from the Caledonian and Scandinavian ice sheets that joined to the north, blocking its exit. The sea level was about lower than it is today. Then, more than 200,000 yBP a single catastrophic glacial lake outburst flood overtopped the Weald-Artois Anticline and scoured a channel through an expanse of low-lying tundra, right down to the underlying chalk bedrock. In a study published in 2007 high-resolution sonar revealed the unexpectedly well-preserved scourmarks and the telltale lenticular island forms characteristic of torrential flood. Through the scoured channel passed a river which now drained the combined Rhine and Thames towards the Atlantic to the west. As the ice sheet melted, a large freshwater lake formed in the southern part of what is now the North Sea. As the meltwater could still not escape to the north (as the northern North Sea was still frozen) the outflow channel from the lake entered the Atlantic Ocean in the region of Dover and Calais.
The Channel, which delayed human reoccupation of Great Britain for more than 100,000 years, has in historic times been both an easy entry for seafaring people and a key natural defence, halting invading armies while in conjunction with control of the North Sea allowing Britain to blockade the continent. The most significant failed invasion threats came when the Dutch and Belgian ports were held by a major continental power, e.g. from the Spanish Armada in 1588, Napoleon during the Napoleonic Wars, and Nazi Germany during World War II. Successful invasions include the Roman conquest of Britain, the Norman Conquest in 1066 and the invasion by the Dutch in 1688, while the concentration of excellent harbours in the Western Channel on Britain's south coast made possible the largest invasion of all time, the Normandy Landings in 1944. Channel naval battles include the Battle of the Downs (1639), Battle of Goodwin Sands (1652), the Battle of Portland (1653), the Battle of La Hougue (1692) and the engagement between USS "Kearsarge" and CSS "Alabama" (1864).
In more peaceful times the Channel served as a link joining shared cultures and political structures, particularly the huge Angevin Empire from 1135 to 1217. For nearly a thousand years, the Channel also provided a link between the Modern Celtic regions and languages of Cornwall and Brittany. Brittany was founded by Britons who fled Cornwall and Devon after Anglo-Saxon encroachment. In Brittany, there is a region known as "Cornouaille" (Cornwall) in French and "Kernev" in Breton In ancient times there was also a "Domnonia" (Devon) in Brittany as well.
In February 1684 , ice formed on the sea in a belt wide off the coast of Kent and wide on the French side.
Route to the British Isles.
Diodorus Siculus and Pliny both suggest trade between the rebel Celtic tribes of Armorica and Iron Age Britain flourished. In 55 BC Julius Caesar invaded, claiming that the Britons had aided the Veneti against him the previous year. He was more successful in 54 BC, but Britain was not fully established as part of the Roman Empire until completion of the invasion by Aulus Plautius in 43 AD. A brisk and regular trade began between ports in Roman Gaul and those in Britain. This traffic continued until the end of Roman rule in Britain in 410 AD, after which the early Anglo-Saxons left less clear historical records.
In the power vacuum left by the retreating Romans, the Germanic Angles, Saxons, and Jutes began the next great migration across the North Sea. Having already been used as mercenaries in Britain by the Romans, many people from these tribes crossed during the Migration Period, conquering and perhaps displacing the native Celtic populations.
Norsemen and Normans.
The attack on Lindisfarne in 793 is generally considered the beginning of the Viking Age. For the next 250 years the Scandinavian raiders of Norway, Sweden, and Denmark dominated the North Sea, raiding monasteries, homes, and towns along the coast and along the rivers that ran inland. According to the "Anglo-Saxon Chronicle" they began to settle in Britain in 851. They continued to settle in the British Isles and the continent until around 1050.
The fiefdom of Normandy was created for the Viking leader Rollo (also known as Robert of Normandy). Rollo had besieged Paris but in 911 entered vassalage to the king of the West Franks Charles the Simple through the Treaty of St.-Claire-sur-Epte. In exchange for his homage and fealty, Rollo legally gained the territory he and his Viking allies had previously conquered. The name "Normandy" reflects Rollo's Viking (i.e. "Northman") origins.
The descendants of Rollo and his followers adopted the local Gallo-Romance language and intermarried with the area's inhabitants and became the Normans – a Norman French-speaking mixture of Scandinavians, Hiberno-Norse, Orcadians, Anglo-Danish, and indigenous Franks and Gauls.
Rollo's descendant William, Duke of Normandy became king of England in 1066 in the Norman Conquest beginning with the Battle of Hastings, while retaining the fiefdom of Normandy for himself and his descendants. In 1204, during the reign of King John, mainland Normandy was taken from England by France under Philip II, while insular Normandy (the Channel Islands) remained under English control. In 1259, Henry III of England recognised the legality of French possession of mainland Normandy under the Treaty of Paris. His successors, however, often fought to regain control of mainland Normandy.
With the rise of William the Conqueror the North Sea and Channel began to lose some of their importance. The new order oriented most of England and Scandinavia's trade south, toward the Mediterranean and the Orient.
Although the British surrendered claims to mainland Normandy and other French possessions in 1801, the monarch of the United Kingdom retains the title Duke of Normandy in respect to the Channel Islands. The Channel Islands (except for Chausey) are Crown dependencies of the British Crown. Thus the Loyal toast in the Channel Islands is "La Reine, notre Duc" ("The Queen, our Duke"). The British monarch is understood to "not" be the Duke of Normandy in regards of the French region of Normandy described herein, by virtue of the Treaty of Paris of 1259, the surrender of French possessions in 1801, and the belief that the rights of succession to that title are subject to Salic Law which excludes inheritance through female heirs.
French Normandy was occupied by English forces during the Hundred Years' War in 1346–1360 and again in 1415–1450.
England and Britain: Naval superpower.
From the reign of Elizabeth I, English foreign policy concentrated on preventing invasion across the Channel by ensuring no major European power controlled the potential Dutch and Flemish invasion ports. Her climb to the pre-eminent sea power of the world began in 1588 as the attempted invasion of the Spanish Armada was defeated by the combination of outstanding naval tactics by the English and the Dutch under command of Charles Howard, 1st Earl of Nottingham with Sir Francis Drake second in command, and the following stormy weather. Over the centuries the Royal Navy slowly grew to be the most powerful in the world.
The building of the British Empire was possible only because the Royal Navy eventually managed to exercise unquestioned control over the seas around Europe, especially the Channel and the North Sea. During the Seven Years' War, France attempted to launch an invasion of Britain. To achieve this France needed to gain control of the Channel for several weeks, but was thwarted following the British naval victory at the Battle of Quiberon Bay in 1759.
Another significant challenge to British domination of the seas came during the Napoleonic Wars. The Battle of Trafalgar took place off the coast of Spain against a combined French and Spanish fleet and was won by Admiral Horatio Nelson, ending Napoleon's plans for a cross-Channel invasion and securing British dominance of the seas for over a century.
First World War.
The exceptional strategic importance of the Channel as a tool for blockade was recognised by the First Sea Lord Admiral Fisher in the years before World War I. "Five keys lock up the world! Singapore, the Cape, Alexandria, Gibraltar, Dover." However on 25 July 1909 Louis Blériot successfully made the first Channel crossing from Calais to Dover in an aeroplane. Blériot's crossing signalled the end of the Channel as a barrier-moat for England against foreign enemies.
Because the Kaiserliche Marine surface fleet could not match the British Grand Fleet, the Germans developed submarine warfare, which was to become a far greater threat to Britain. The Dover Patrol was set up just before war started to escort cross-Channel troopships and to prevent submarines from accessing the Channel, thereby obliging them to travel to the Atlantic via the much longer route around Scotland.
On land, the German army attempted to capture Channel ports (see "Race to the Sea"), but although the trenches are often said to have stretched "from the frontier of Switzerland to the English Channel", they reached the coast at the North Sea. Much of the British war effort in Flanders was a bloody but successful strategy to prevent the Germans reaching the Channel coast.
At the outset of the war, an attempt was made to block the path of U-boats through the Dover Strait with naval minefields. By February 1915, this had been augmented by a 25 kilometre stretch of light steel netting called the Dover Barrage, which it was hoped would ensnare submerged submarines. After initial success, the Germans learned how to pass through the barrage, aided by the unreliability of British mines. 31 January 1917, the Germans restarted unrestricted submarine warfare leading to dire Admiralty predictions that submarines would defeat Britain by November, the most dangerous situation Britain faced in either World War.
The Battle of Passchendaele in 1917 was fought to reduce the threat by capturing the submarine bases on the Belgian coast, though it was the introduction of convoys and not capture of the bases that averted defeat. In April 1918 the Dover Patrol carried out the famous Zeebrugge Raid against the U-boat bases. During 1917, the Dover Barrage was re-sited with improved mines and more effective nets, aided by regular patrols by small warships equipped with powerful searchlights. A German attack on these vessels resulted in the Battle of Dover Strait in 1917. A much more ambitious attempt to improve the barrage by installing eight massive concrete towers across the strait was called the Admiralty M–N Scheme, but only two towers were nearing completion at the end of the war and the project was abandoned.
The naval blockade in the Channel and North Sea was one of the decisive factors in the German defeat in 1918.
Second World War.
During the Second World War, naval activity in the European theatre was primarily limited to the Atlantic. During the Battle of France in May 1940, the Germans succeeded in capturing both Boulogne and Calais, thereby threatening the line of retreat for the British Expeditionary Force. By a combination of hard fighting and German indecision, the port of Dunkirk was kept open allowing 338,000 Allied troops to be evacuated in Operation Dynamo. More than 11,000 were evacuated from Le Havre during Operation Cycle and a further 192,000 were evacuated from ports further down the coast in Operation Ariel in June 1940. The early stages of the Battle of Britain featured air attacks on Channel shipping and ports, and until the Normandy Landings (with the exception of the Channel Dash) the narrow waters were too dangerous for major warships. Despite these early successes against shipping, the Germans did not win the air supremacy necessary for Operation Sealion, the projected cross-Channel invasion.
The Channel subsequently became the stage for an intensive coastal war, featuring submarines, minesweepers, and Fast Attack Craft.
Dieppe was the site of an ill-fated raid by Canadian and British armed forces. More successful was the later Operation Overlord (D-Day), a massive invasion of German-occupied France by Allied troops. Caen, Cherbourg, Carentan, Falaise and other Norman towns endured many casualties in the fight for the province, which continued until the closing of the so-called Falaise gap between Chambois and Montormel, then liberation of Le Havre.
The Channel Islands were the only part of the British Commonwealth occupied by Germany (excepting the part of Egypt occupied by the Afrika Korps at the time of the Second Battle of El Alamein, which was a protectorate and not part of the Commonwealth). The German occupation of 1940–1945 was harsh, with some island residents being taken for slave labour on the Continent; native Jews sent to concentration camps; partisan resistance and retribution; accusations of collaboration; and slave labour (primarily Russians and eastern Europeans) being brought to the islands to build fortifications. The Royal Navy blockaded the islands from time to time, particularly following the liberation of mainland Normandy in 1944. Intense negotiations resulted in some Red Cross humanitarian aid, but there was considerable hunger and privation during the occupation, particularly in the final months, when the population was close to starvation. The German troops on the islands surrendered on 9 May 1945, a few days after the final surrender in mainland Europe.
Population.
The English Channel is far more densely populated on the English shore. The most significant towns and cities along both the English and French sides of the Channel (each with more than 20,000 inhabitants, ranked in descending order; populations are the urban area populations from the 1999 French census, 2001 UK census, and 2001 Jersey census) are as follows:
Shipping.
The Channel has traffic on both the UK-Europe and North Sea-Atlantic routes, and is the world's busiest seaway, with over 500 ships per day. Following an accident in January 1971 and a series of disastrous collisions with wreckage in February, the Dover Traffic Separation System (TSS) the world's first radar controlled TSS was set up by the International Maritime Organization. The scheme mandates that vessels travelling north must use the French side, travelling south the English side. There is a separation zone between the two lanes.
In December 2002 the MV "Tricolor", carrying £30m of luxury cars sank northwest of Dunkirk after collision in fog with the container ship "Kariba". The cargo ship "Nicola" ran into the wreckage the next day. There was no loss of life.
The shore-based long range traffic control system was updated in 2003 and there is a series of Traffic Separation Systems in operation. Though the system is inherently incapable of reaching the levels of safety obtained from aviation systems such as the Traffic Collision Avoidance System, it has reduced accidents to one or two per year.
Marine GPS systems allow ships to be preprogrammed to follow navigational channels accurately and automatically, further avoiding risk of running aground, but following the fatal collision between Dutch Aquamarine and Ash in October 2001, Britain's Marine Accident Investigation Branch (MAIB) issued a safety bulletin saying it believed that in these most unusual circumstances GPS use had actually contributed to the collision. The ships were maintaining a very precise automated course, one directly behind the other, rather than making use of the full width of the traffic lanes as a human navigator would.
A combination of radar difficulties in monitoring areas near cliffs, a failure of a CCTV system, incorrect operation of the anchor, the inability of the crew to follow standard procedures of using a GPS to provide early warning of the ship dragging the anchor and reluctance to admit the mistake and start the engine led to the MV "Willy" running aground in Cawsand bay, Cornwall in January 2002. The MAIB report makes it clear that the harbour controllers were informed of impending disaster by shore observers before the crew were themselves aware. The village of Kingsand was evacuated for three days because of the risk of explosion, and the ship was stranded for 11 days.
Ecology.
As a busy shipping lane, the Channel experiences environmental problems following accidents involving ships with toxic cargo and oil spills. Indeed over 40% of the UK incidents threatening pollution occur in or very near the Channel. One of the most infamous was the MSC "Napoli", which with nearly 1700 tonnes of dangerous cargo was controversially beached in Lyme Bay, a protected World Heritage Site coastline. The ship had been damaged and was en route to Portland Harbour when much nearer harbours were available.
Transport.
Ferry.
The number of ferry routes crossing the Strait of Dover has reduced since the Channel Tunnel opened. Current cross-channel ferry routes are:
Channel Tunnel.
Many travellers cross beneath the Channel using the Channel Tunnel, first proposed in the early 19th century and finally opened in 1994, connecting the UK and France by rail. It is now routine to travel between Paris or Brussels and London on the Eurostar train. Cars can also be carried on special trains between Folkestone and Calais.
Economy.
Tourism.
The coastal resorts of the Channel, such as Brighton and Deauville, inaugurated an era of aristocratic tourism in the early 19th century, which developed into the seaside tourism that has shaped resorts around the world. Short trips across the Channel for leisure purposes are often referred to as Channel Hopping.
Culture and languages.
The two dominant cultures are English on the north shore of the Channel, French on the south. However, there are also a number of minority languages that are or were found on the shores and islands of the English Channel, which are listed here, with the Channel's name following them.
Dutch previously had a larger range, and extended into parts of modern-day France. For more information, please see French Flemish.
Most other languages tend towards variants of the French and English forms, but notably Welsh has "Môr Udd".
Channel crossings.
As one of the narrowest but most famous international waterways lacking dangerous currents, the Channel has been the first objective of numerous innovative sea, air, and human powered crossing technologies.
By boat.
Pierre Andriel crossed the English Channel aboard the "Élise", ex the Scottish p.s. "Margery" in March 1816, one of the earliest seagoing voyages by steam ship.
The paddle steamer "Defiance", Captain William Wager, was the first steamer to cross the Channel to Holland, arriving there on 9 May 1816.
On 10 June 1821, English-built paddle steamer "Rob Roy" was the first passenger ferry to cross channel. The steamer was purchased subsequently by the French postal administration and renamed "Henri IV" and put into regular passenger service a year later. It was able to make the journey across the Straits of Dover in around three hours.
In June 1843, because of difficulties with Dover harbour, the South Eastern Railway company developed the Boulogne-sur-Mer-Folkestone route as an alternative to Calais-Dover. The first ferry crossed under the command of Captain Hayward.
The Mountbatten class hovercraft (MCH) entered commercial service in August 1968, initially between Dover and Boulogne but later also Ramsgate (Pegwell Bay) to Calais. The journey time Dover to Boulogne was roughly 35 minutes, with six trips per day at peak times. The fastest crossing of the English Channel by a commercial car-carrying hovercraft was 22 minutes, recorded by the "Princess Anne" MCH SR-N4 Mk3 on 14 September 1995, for the 10:00 am service .
The youngest recorded sailors to cross the Channel were a team of eight with ages ranging from 7 to 16 on 18 July 2010. They sailed their single-handed RS Tera dinghies from Dover to Boulogne in 5 hours and were tracked by Dover Coastguard Radar, who retain a record of the passage. They were from three UK clubs: Castle Cove SC, Dabchicks SC and Downs SC.
By swimming.
The sport of Channel swimming traces its origins to the latter part of the 19th century when Captain Matthew Webb made the first observed and unassisted swim across the Strait of Dover, swimming from England to France on 24–25 August 1875 in 21 hours 45 minutes.
In 1927, at a time when fewer than ten swimmers had managed to emulate the feat and many dubious claims were being made, the Channel Swimming Association (CSA) was founded to authenticate and ratify swimmers' claims to have swum the Channel and to verify crossing times. The CSA was dissolved in 1999 and was succeeded by two separate organisations: CSA (Ltd) and the Channel Swimming and Piloting Federation (CSPF). Both observe and authenticate cross-Channel swims in the Strait of Dover. The Channel Crossing Association was set up at about this time to cater for unorthodox crossings.
The team with the most number of Channel swims to its credit is the International Sri Chinmoy Marathon Team, with 35 crossings by 25 members (by 2005).
By the end of 2005, 811 people had completed 1,185 verified crossings under the rules of the CSA, the CSA (Ltd), the CSPF and Butlins.
The number of swims conducted under and ratified by the Channel Swimming Association to 2005 was 982 by 665 people. This includes 24 two-way crossings and three three-way crossings.
The number of ratified swims to 2004 was 948 by 675 people (456 men, 214 women). There have been 16 two-way crossings (9 by men and 7 by women). There have been three three-way crossings (2 by men and 1 by a woman). (It is unclear whether this last set of data is comprehensive or CSA only.)
The Strait of Dover is the busiest stretch of water in the world. It is governed by International Law as described in this document. It states: "However, in exceptional cases the French Maritime Authorities may grant authority for unorthodox craft to cross French territorial waters within the Traffic Separation Scheme when these craft set off from the British coast, on condition that the request for authorisation is sent to them with the opinion of the British Maritime Authorities." It is therefore theoretically possible to hire a non CSA or CS&PF pilot boat when swimming the channel, although it would be difficult to convince the MCA to endorse the trip.
The CCA, CSA, and CS&PF are the organisations escorting channel swims, because their pilots have the experience, qualifications, and equipment to guarantee the safety of the swimmers they escort.
The fastest verified swim of the Channel was by the Australian Trent Grimsey on 8 September 2012, in 6 hours 55 minutes, beating the previous record set in 2007 by Bulgarian swimmer Petar Stoychev.
By car.
On 16 September 1965, two Amphicars crossed from Dover to Calais. One was crewed by two British Army officers, Captain Mike Bailey REME and Captain Peter Tappenden RAOC, the other by Tim Dill-Russell and Sgt Joe Minto RASC. The crossing took 7 hours 20 minutes, with mid-Channel wind conditions reaching force 5 on the Beaufort scale. The cars went on to the Frankfurt Motor Show that year, where they were put on display.
In 2007, the presenters of the BBC programme "Top Gear" (Jeremy Clarkson, Richard Hammond and James May) "drove" across the Channel from England to France. They did it by designing "amphibious cars" that could be driven on land and also operate in water.
After four attempts – twice failing to leave Dover Harbour – they reached the coast of France in a Nissan pick-up with an outboard motor and oil drums attached to the back to aid stability in open water. The other two vehicles that attempted the crossing (a Triumph Herald with a sail and a Volkswagen Campervan with a propeller attached to the flywheel) both sank. Clarkson believed it might be possible to break the world record for crossing the Channel in this manner, but the team was unsuccessful. The "Daily Mail" claimed that the BBC received criticism from a coastguard who claimed that they had not been told that the stunt was going to take place, and allegedly branded it "completely irresponsible"; however this was not reported by any other media sources and the aired episode showed the full co-operation of the coastguard.

</doc>
<doc id="9232" url="http://en.wikipedia.org/wiki?curid=9232" title="Eiffel Tower">
Eiffel Tower

The Eiffel Tower (, ) is an iron lattice tower located on the Champ de Mars in Paris. It was named after the engineer Gustave Eiffel, whose company designed and built the tower. Erected in 1889 as the entrance arch to the 1889 World's Fair, it was initially criticised by some of France's leading artists and intellectuals for its design, but has become both a global cultural icon of France and one of the most recognizable structures in the world. The tower is the tallest structure in Paris and the most-visited paid monument in the world; 6.98 million people ascended it in 2011. The tower received its 250 millionth visitor in 2010.
The tower is tall, about the same height as an 81- building. During its construction, the Eiffel Tower surpassed the Washington Monument to assume the title of the tallest man-made structure in the world, a title it held for 41 years, until the Chrysler Building in New York City was built in 1930. Because of the addition of the aerial atop the Eiffel Tower in 1957, it is now taller than the Chrysler Building by . Not including broadcast aerials, it is the second-tallest structure in France, after the Millau Viaduct.
The tower has three levels for visitors, with restaurants on the first and second. The third level observatory's upper platform is above the ground, the highest accessible to the public in the European Union. Tickets can be purchased to ascend by stairs or lift (elevator) to the first and second levels. The climb from ground level to the first level is over 300 steps, as is the walk from the first to the second level. Although there are stairs to the third and highest level, these are usually closed to the public and it is generally only accessible by lift.
History.
Origin.
The design of the Eiffel Tower was originated by Maurice Koechlin and Émile Nouguier, two senior engineers who worked for the Compagnie des Établissements Eiffel, after discussion about a suitable centrepiece for the proposed 1889 Exposition Universelle, a World's Fair which would celebrate the centennial of the French Revolution. In May 1884 Koechlin, working at home, made an outline drawing of their scheme, described by him as "a great pylon, consisting of four lattice girders standing apart at the base and coming together at the top, joined together by metal trusses at regular intervals". Initially Eiffel himself showed little enthusiasm, but he did sanction further study of the project, and the two engineers then asked Stephen Sauvestre, the head of company's architectural department, to contribute to the design. Sauvestre added decorative arches to the base, a glass pavilion to the first level, and other embellishments. This enhanced version gained Eiffel's support: he bought the rights to the patent on the design which Koechlin, Nougier, and Sauvestre had taken out, and the design was exhibited at the Exhibition of Decorative Arts in the autumn of 1884 under the company name. On 30 March 1885 Eiffel presented a paper on the project to the "Société des Ingiénieurs Civils"; after discussing the technical problems and emphasising the practical uses of the tower, he finished his talk by saying that the tower would symbolise
Little happened until the beginning of 1886, when Jules Grévy was re-elected as President and Édouard Lockroy was appointed as Minister for Trade. A budget for the Exposition was passed and on 1 May Lockroy announced an alteration to the terms of the open competition which was being held for a centerpiece for the exposition, which effectively made the choice of Eiffel's design a foregone conclusion: all entries had to include a study for a 300 m (980 ft) four-sided metal tower on the Champ de Mars. On 12 May a commission was set up to examine Eiffel's scheme and its rivals and on 12 June it presented its decision, which was that all the proposals except Eiffel's were either impractical or insufficiently worked out. After some debate about the exact site for the tower, a contract was finally signed on 8 January 1887. This was signed by Eiffel acting in his own capacity rather than as the representative of his company, and granted him 1.5 million francs toward the construction costs: less than a quarter of the estimated 6.5 million francs. Eiffel was to receive all income from the commercial exploitation of the tower during the exhibition and for the following twenty years. Eiffel later established a separate company to manage the tower, putting up half the necessary capital himself.
The "Artists Protest".
The projected tower had been a subject of some controversy, attracting criticism from both those who did not believe that it was feasible and those who objected on artistic grounds, whose objections were an expression of a longstanding debate about the relationship between architecture and engineering. This came to a head as work began at the Champ de Mars: A "Committee of Three Hundred" (one member for each metre of the tower's height) was formed, led by the prominent architect Charles Garnier and including some of the most important figures of the French arts establishment, including Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet: a petition was sent to Charles Alphand, the Minister of Works and Commissioner for the Exposition, and was published by "Le Temps" on 14 February 1887. 
Gustave Eiffel responded to these criticisms by comparing his tower to the Egyptian Pyramids: "My tower will be the tallest edifice ever erected by man. Will it not also be grandiose in its way? And why would something admirable in Egypt become hideous and ridiculous in Paris?" These criticisms were also masterfully dealt with by Édouard Lockroy in a letter of support written to Alphand, ironically saying "Judging by the stately swell of the rhythms, the beauty of the metaphors, the elegance of its delicate and precise style, one can tell that…this protest is the result of collaboration of the most famous writers and poets of our time", and going on to point out that the protest was irrelevant since the project had been decided upon months before and was already under construction. Indeed, Garnier had been a member of the Tower Commission that had assessed the various proposals, and had raised no objection. Eiffel was similarly unworried, pointing out to a journalist that it was premature to judge the effect of the tower solely on the basis of the drawings, that the Champ de Mars was distant enough from the monuments mentioned in the protest for there to be little risk of the tower overwhelming them, and putting the aesthetic argument for the Tower: "Do not the laws of natural forces always conform to the secret laws of harmony?"
Some of the protestors were to change their minds when the tower was built: others remained unconvinced. Guy de Maupassant supposedly ate lunch in the tower's restaurant every day because it was the one place in Paris where the tower was not visible. 
Today, the tower is widely considered to be a striking piece of structural art. It is often featured in films and literature. Already before 1918 it had become a symbol for Paris and for France, when Guillaume Apollinaire made a nationalist poem in the shape of the tower (a calligram) to express his feelings about the war against Germany.
Construction.
Work on the foundations started on 28 January 1887. Those for the east and south legs were straightforward, each leg resting on four concrete slabs, one for each of the principal girders of each leg but the other two, being closer to the river Seine, were more complicated: each slab needed two piles installed by using compressed-air caissons long and in diameter driven to a depth of to support the concrete slabs, which were thick. Each of these slabs supported a block built of limestone each with an inclined top to bear a supporting shoe for the ironwork. Each shoe was anchored into the stonework by a pair of bolts 10 cm (4 in) in diameter and long. The foundations were complete by 30 June and the erection of the ironwork began. The very visible work on-site was complemented by the enormous amount of exacting preparatory work that was entailed: the drawing office produced 1,700 general drawings and 3,629 detailed drawings of the 18,038 different parts needed. The task of drawing the components was complicated by the complex angles involved in the design and the degree of precision required: the position of rivet holes was specified to within 0.1 mm (0.04 in) and angles worked out to one second of arc. The finished components, some already riveted together into sub-assemblies, arrived on horse-drawn carts from the factory in the nearby Parisian suburb of Levallois-Perret and were first bolted together, the bolts being replaced by rivets as construction progressed. No drilling or shaping was done on site: if any part did not fit it was sent back to the factory for alteration. In all there were 18,038 pieces joined by two and a half million rivets.
At first the legs were constructed as cantilevers but about halfway to the first level construction was paused in order to construct a substantial timber scaffold. This caused a renewal of the concerns about the structural soundness of the project, and sensational headlines such as "Eiffel Suicide!" and "Gustave Eiffel has gone mad: he has been confined in an Asylum" appeared in the popular press. At this stage a small "creeper" crane was installed in each leg, designed to move up the tower as construction progressed and making use of the guides for the lifts which were to be fitted in each leg. The critical stage of joining the four legs at the first level was complete by the end of March 1888. Although the metalwork had been prepared with the utmost precision, provision had been made to carry out small adjustments in order to precisely align the legs: hydraulic jacks were fitted to the shoes at the base of each leg, each capable of exerting a force of 800 tonnes, and in addition the legs had been intentionally constructed at a slightly steeper angle than necessary, being supported by sandboxes on the scaffold. Although construction involved 300 on-site employees, only one person died thanks to Eiffel's stringent safety precautions and use of movable stagings, guard-rails, and screens.
Lifts.
Equipping the Tower with adequate and safe passenger lifts was a major concern of the government commission overseeing the Exposition. Although some visitors could be expected to climb to the first or even the second stage, the main means of ascent clearly had to be lifts.
Constructing lifts to reach the first platform was relatively straightforward: the legs of the lower section were wide enough and so nearly straight that they could contain a straight track, and a contract was given to the French company Roux, Combaluzier and Lepape for two lifts to be fitted in the east and west legs. Roux, Combaluzier and Lepape used a pair of endless chains with rigid, articulated links to which the car was attached. Lead weights on some links of the chains’ upper or return sections counterbalanced most of the car’s weight. The car was pushed up by the links below, not drawn by those above: to prevent the chain buckling it was enclosed in a conduit. At the bottom of the run the chains passed around 3.9 m (12 ft 10 in) diameter sprockets. Smaller sprockets at the top guided the chains.
The lifts to the second platform presented a more complex problem, because a straight track was not possible. No French company was willing to undertake the work. The European branch of Otis Brothers & Company submitted a proposal but this was rejected: the fair’s charter ruled out the use of any foreign material in the construction of the Tower. The deadline for bids was extended, but still no French companies put themselves forward, and eventually the contract was given to Otis in July 1887. Otis had been confident that they would eventually be given the contract and had already started design studies. The car was divided into two superimposed compartments, each holding 25 passengers, with the lift operator occupying an exterior platform on the lower level. Motive power was provided by an inclined hydraulic ram, 12.67 m (36 ft) long 96.5 cm (38 in) diameter 10.83 m 35 ft 6 in stroke in the tower leg: this moved a carriage carrying six sheaves. Five fixed sheaves were mounted higher up the leg, producing an arrangement similar to a block and tackle but acting in reverse, multiplying the stroke of the piston rather than the force generated. The hydraulic pressure in the driving cylinder was produced by a large open reservoir on the second platform. After being exhausted from the cylinder, the water was pumped back up to the reservoir by two pumps in the machinery room at the base of the south leg. This reservoir also provided power to the lifts to the first level.
The original lifts from the second to the third floor were supplied by Léon Edoux. A pair of hydraulic rams were mounted on the second level, reaching nearly halfway up to the third level. One lift car was mounted on top of these rams: cables ran from the top of this car up to sheaves on the third level and then back down to a second car.
Each car only travelled half the distance between the second and third levels and passengers were required to change lifts halfway by means of a short gangway. The ten-ton cars held 65 passengers each.
Inauguration and the 1889 Exposition.
The main structural work was completed at the end of March 1889 and on the 31st Eiffel celebrated this by leading a group of government officials, accompanied by representatives of the press, to the top of the tower. Since the lifts were not yet in operation, the ascent was made by foot, and took over an hour, Eiffel frequently stopping to make explanations of various features. Most of the party chose to stop at the lower levels, but a few, including Nouguier, Compagnon, the President of the City Council and reporters from "Le Figaro" and "Le Monde Illustré" completed the climb. At 2:35 Eiffel hoisted a large French flag, to the accompaniment of a 25-gun salute fired from the lower level. There was still work to be done, particularly on the lifts and the fitting out of the facilities for visitors, and the tower was not opened to the public until nine days after the opening of the Exposition on 6 May: even then the lifts had not been completed. The tower was an immediate success with the public, and nearly 30,000 visitors made the 1,710-step climb to the top using the stairs before the lifts entered service on 26 May.
Tickets cost 2 francs for the first level, 3 for the second, and 5 for the top, with half-price admission on Sundays, and by the end of the exhibition there had been 1,896,987 visitors.
After dark the tower was lit by hundreds of gas lamps and a beacon sending out three beams of red, white and blue light. Two searchlights were mounted on a circular rail, and were used to illuminate various features of the Exposition. The opening and closing of the Exposition were announced every day by a cannon fired from the top.
On the second level, the French newspaper "Le Figaro" had an office and a printing press, where a special souvenir edition, "Le Figaro de la Tour", was produced. There was also a pâtisserie.
On the third level there was a post office, where visitors could send letters or postcards as a memento of their visit. Graffitists were also catered for: sheets of paper were mounted on the walls for visitors to record their impressions: these were replaced daily. Gustave Eiffel describes some of the responses as "vraiment curieuse".
Famous visitors to the tower included The Prince of Wales, Sarah Bernhardt, "Buffalo Bill Cody (his Wild West show was an attraction at the Exposition) and Thomas Edison. Edison was invited by Eiffel to his private apartment at the top of the tower, where Edison presented him with one of his phonographs: this invention was one of the sensations of the Exposition. Edison signed the guestbook with the following message—
Eiffel had a permit for the tower to stand for 20 years; it was to be dismantled in 1909, when its ownership would revert to the City of Paris. The City had planned to tear it down (part of the original contest rules for designing a tower was that it should be easy to demolish) but as the tower proved valuable for communication purposes it was allowed to remain after the expiry of the permit.
Eiffel made use of his apartment at the top level of the tower to carry out meteorological observations, and also made use of the tower to perform experiments on the action of air resistance on falling bodies.
Design of the tower.
Material.
The puddled iron (wrought iron) structure of the Eiffel Tower weighs tonnes, while the entire structure, including non-metal components, is approximately  tonnes. As a demonstration of the economy of design, if the 7,300 tonnes of the metal structure were melted down it would fill the 125-metre-square base to a depth of only 6.25 cm (2.5 in), assuming the density of the metal to be 7.8 tonnes per cubic metre. Depending on the ambient temperature, the top of the tower may shift away from the sun by up to 18 cm (7.1 in) because of thermal expansion of the metal on the side facing the sun.
Wind considerations.
At the time the tower was built many people were shocked by its daring shape. Eiffel was accused of trying to create something artistic without regard to engineering. However, Eiffel and his engineers, as experienced bridge builders, understood the importance of wind forces and knew that if they were going to build the tallest structure in the world they had to be certain it would withstand them. In an interview with the newspaper "Le Temps (Paris)" of 14 February 1887, Eiffel said:
Eiffel used empirical and graphical methods accounting for the effects of wind rather than a specific mathematical formula. Careful examination of the tower shows a basically exponential shape (actually two different exponentials, the lower section overdesigned to ensure resistance to wind forces. Several mathematical explanations have been proposed over the years for the success of the design; the most recent is described as a nonlinear integral equation based on counterbalancing the wind pressure on any point on the tower with the tension between the construction elements at that point. As proof of the tower's effectiveness in wind resistance, it sways only 6–7 cm (2–3 in) in the wind.
Accommodation.
When built, the first level contained three restaurants (one French, one Russian and one Flemish) and an "Anglo-American Bar". After the exposition closed the Flemish restaurant was converted to a 250 seat theatre. A promenade ran around the outside.
On the third level there were laboratories for various experiments and a small apartment reserved for Gustave Eiffel to entertain guests. This is now visible to the public, complete with period decorations and lifelike models of Gustave and some guests.
Passenger lifts.
As described the arrangement of the lifts has been changed several times during the course of the Tower's history.
Owing to the elasticity of the cables and the time taken to get the cars level with the landings, each lift in normal service takes an average of 8 minutes and 50 seconds to do the round trip, spending an average of 1 minute and 15 seconds at each floor. The average journey time between floors is just 1 minute.
The 1899 east and west hydraulic mechanism works are on display to the public in a small museum in the base of the east and west towers, which is somewhat hidden from public view. Because the massive mechanism requires frequent lubrication and attention, public access is often restricted. The rope mechanism of the north tower is visible to visitors as they exit from the lift.
Engraved names.
Gustave Eiffel engraved on the tower seventy-two names of French scientists, engineers, and mathematicians in recognition of their contributions. Eiffel chose this "invocation of science" because of his concern over the artists' protests against the tower. This engraving was painted over at the beginning of the twentieth century but restored in 1986–1987 by the "Société Nouvelle d'exploitation de la Tour Eiffel", a company contracted to operate business related to the Tower.
Maintenance.
Maintenance of the tower includes applying of paint every seven years to protect it from rust.
The height of the Eiffel Tower varies by due to temperature.
Aesthetic considerations.
In order to give the appearance of uniform colour the paint used is graduated in tone to counteract the effect of atmospheric perspective, and is lighter at the bottom, getting darker toward the top. Periodically the colour of the paint is changed; as of 2013 it is bronze coloured. On the first floor there are interactive consoles hosting a poll for the colour to use for the next repaint.
The only non-structural elements are the four decorative grillwork arches, added in Sauvestre's sketches, which served to make the structure look more substantial, and to make a more impressive entrance to the Exposition.
One of the great Hollywood movie clichés is that the view from a Parisian window always includes the tower. In reality, since zoning restrictions limit the height of most buildings in Paris to seven storeys high, only a small number of taller buildings have a clear view of the tower.
Tourism.
Transport.
The nearest Paris Métro station is Bir-Hakeim and the nearest RER station is Pont de l'Alma. The tower itself is located at the intersection of the quai Branly and the Pont d'Iéna.
Popularity.
More than 250 million people have visited the tower since its construction in 1889: in 2012 there were 6,180,000 visitors. The tower is the most-visited paid monument in the world.
Restaurants.
The tower has two restaurants: "Le 58 tour Eiffel", on the first floors and the "Le Jules Verne", a gourmet restaurant on the second floor, with a private lift. This restaurant has one star in the Michelin Red Guide. It is run by the multi-Michelin star chef Alain Ducasse.
Replicas.
As one of the most iconic structures in the world, the Eiffel Tower has been the inspiration for the creation of over 30 duplicates and similar towers around the world.
An early example is the Blackpool Tower in England. Mayor Sir John Bickerstaffe was so impressed on seeing the Eiffel Tower at the 1889 Exhibition that he invested in a similar tower for his own city.
Communications.
The tower has been used for radio transmission since the beginning of the 20th century. Until the 1950s, sets of aerial wires ran from the summit to anchors on the Avenue de Suffren and Champ de Mars. These were connected to long-wave transmitters in small bunkers; in 1909 a permanent underground radio centre was built near the south pillar, which still exists today. On 20 November 1913 the Paris Observatory, using the Eiffel Tower as an aerial, exchanged sustained wireless signals with the United States Naval Observatory which used an aerial in Arlington, Virginia. The object of the transmissions was to measure the difference in longitude between Paris and Washington, D.C. Today, both radio and television stations broadcast their signals from the top of the Eiffel Tower.
Television.
Analogue.
Analogue television signals ceased from the Eiffel Tower on 8 March 2011.
Image copyright claims.
The tower and its representations have long been in the public domain. However, in June 1990 a French court ruled that a special lighting display on the tower in 1989 (the tower's 100th anniversary) was an "original visual creation" protected by copyright. The Court of Cassation, France's judicial court of last resort, upheld the ruling in March 1992. The Société d'Exploitation de la Tour Eiffel (SETE) now considers any illumination of the tower to be under copyright. As a result, it is no longer legal to publish contemporary photographs of the tower at night without permission in France and some other countries.
The imposition of copyright has been controversial. The Director of Documentation for what was then the Société nouvelle d'exploitation de la tour Eiffel (SNTE), Stéphane Dieu, commented in January 2005, "It is really just a way to manage commercial use of the image, so that it isn't used in ways we don't approve." However, it also could be used to prohibit tourist photographs of the tower at night from being published, as well as hindering non-profit and semi-commercial publication of images of the tower. French doctrine and jurisprudence traditionally allow pictures incorporating a copyrighted work as long as their presence is incidental or accessory to the main represented subject, a reasoning akin to the "de minimis" rule. Thus, SETE could not claim copyright on, for example, photographs or panoramas of Paris including the lit tower.
In popular culture.
As a global landmark, the Eiffel Tower is featured in media including films, video games, and television shows.
In a commitment ceremony in 2007, Erika Eiffel, an American woman, "married" the Eiffel Tower. Her relationship with the tower has been the subject of extensive global publicity.
Taller structures.
Although it was the world's tallest structure when completed in 1889, the Eiffel Tower has since lost its standing both as the tallest lattice tower and as the tallest structure in France.
External links.
 

</doc>
<doc id="9235" url="http://en.wikipedia.org/wiki?curid=9235" title="Ethical egoism">
Ethical egoism

Ethical egoism is the normative ethical position that moral agents ought to do what is in their own self-interest. It differs from psychological egoism, which claims that people can only act in their self-interest. Ethical egoism also differs from rational egoism, which holds that it is rational to act in one's self-interest.
Ethical egoism contrasts with ethical altruism, which holds that moral agents have an obligation to help others. Egoism and altruism both contrast with ethical utilitarianism, which holds that a moral agent should treat one's self (also known as the subject) with no higher regard than one has for others (as egoism does, by elevating self-interests and "the self" to a status not granted to others). But it also holds that one should not (as altruism does) sacrifice one's own interests to help others' interests, so long as one's own interests (i.e. one's own desires or well-being) are substantially equivalent to the others' interests and well-being. Egoism, utilitarianism, and altruism are all forms of consequentialism, but egoism and altruism contrast with utilitarianism, in that egoism and altruism are both agent-focused forms of consequentialism (i.e. subject-focused or subjective). However, utilitarianism is held to be agent-neutral (i.e. objective and impartial): it does not treat the subject's (i.e. the self's, i.e. the moral "agent's") own interests as being more or less important than the interests, desires, or well-being of others.
Ethical egoism does not, however, require moral agents to harm the interests and well-being of others when making moral deliberation; e.g. what is in an agent's self-interest may be incidentally detrimental, beneficial, or neutral in its effect on others. Individualism allows for others' interest and well-being to be disregarded or not, as long as what is chosen is efficacious in satisfying the self-interest of the agent. Nor does ethical egoism necessarily entail that, in pursuing self-interest, one ought always to do what one wants to do; e.g. in the long term, the fulfillment of short-term desires may prove detrimental to the self. Fleeting pleasure, then, takes a back seat to protracted eudaimonia. In the words of James Rachels, "Ethical egoism [...] endorses selfishness, but it doesn't endorse foolishness."
Ethical egoism is often used as the philosophical basis for support of right-libertarianism and individualist anarchism. These are political positions based partly on a belief that individuals should not coercively prevent others from exercising freedom of action.
Forms of ethical egoism.
Ethical egoism can be broadly divided into three categories: individual, personal, and universal. An "individual ethical egoist" would hold that all people should do whatever benefits "my" ("the individual")" "self-interest; a "personal ethical egoist" would hold that he or she should act in "his or her" self-interest, but would make no claims about what anyone else ought to do; a "universal ethical egoist" would argue that everyone should act in ways that are in their self-interest.
History.
Ethical egoism, as a category of moral philosophies, was introduced by the philosopher Henry Sidgwick in his "The Methods of Ethics", written in 1874. Sidgwick compared egoism to the philosophy of utilitarianism, writing that whereas utilitarianism sought to maximize overall pleasure, egoism focused only on maximizing individual pleasure.
Since the introduction of the term, ethical egoism has been retroactively applied to philosophers before Sidgwick. The philosophy of Yang Zhu (4th century B.C.), Yangism, is considered to be egoist. Yangism views "wei wo", or "everything for myself", as the only virtue necessary for self-cultivation. Although ancient Greek philosophers believed in individual virtue ethics, philosophers like "Plato, Aristotle, and the Stoics [did] not accept the formal principle that whatever the good is, we should seek only our own good, or prefer it to the good of others." The beliefs of the Cyrenaics, however, have been referred to as a "form of egoistic hedonism," unlike the hedonistic virtue ethics of the Epicureans.
Justifications.
Philosopher James Rachels, in an essay that takes as its title the theory's name, outlines the three arguments most commonly touted in its favor:
Notable proponents.
The term "ethical egoism" has been applied retroactively to philosophers such as Bernard de Mandeville and to many other materialists of his generation, although none of them declared themselves to be egoists. Note that materialism does not necessarily imply egoism, as indicated by Karl Marx, and the many other materialists who espoused forms of collectivism. It has been argued that ethical egoism can lend itself to individualist anarchism such as that of Benjamin Tucker, or the combined anarcho-communism and egoism of Emma Goldman, both of whom were proponents of many egoist ideas put forward by Max Stirner. In this context, egoism is another way of describing the sense that the common good should be enjoyed by all. However, most notable anarchists in history have been less radical, retaining altruism and a sense of the importance of the individual that is appreciable but does not go as far as egoism. Recent trends to greater appreciation of egoism within anarchism tend to come from less classical directions such as post-left anarchy or Situationism (e.g. Raoul Vaneigem). Egoism has also been referenced by anarcho-capitalists, such as Murray Rothbard.
Philosopher Max Stirner, in his book "The Ego and Its Own", was the first philosopher to call himself an egoist, though his writing makes clear that he desired not a new idea of morality (ethical egoism), but rather a rejection of morality (amoralism), as a nonexistent and limiting “spook”; for this, Stirner has been described as the first individualist anarchist. Other philosophers, such as Thomas Hobbes and David Gauthier, have argued that the conflicts which arise when people each pursue their own ends can be resolved for the best of each individual only if they all voluntarily forgo some of their aims — that is, one's self-interest is often best pursued by allowing others to pursue their self-interest as well so that liberty is equal among individuals. Sacrificing one's short-term self-interest to maximize one's long-term self-interest is one form of "rational self-interest" which is the idea behind most philosophers' advocacy of ethical egoism. Egoists have also argued that one's actual interests are not immediately obvious, and that the pursuit of self-interest involves more than merely the acquisition of some good, but the "maximizing" of one's chances of survival and/or happiness.
Philosopher Friedrich Nietzsche suggested that egoistic or "life-affirming" behavior stimulates jealousy or "ressentiment" in others, and that this is the psychological motive for the altruism in Christianity. Sociologist Helmut Schoeck similarly considered envy the motive of collective efforts by society to reduce the disproportionate gains of successful individuals through moral or legal constraints, with altruism being primary among these. In addition, Nietzsche (in "Beyond Good and Evil") and Alasdair MacIntyre (in "After Virtue") have pointed out that the ancient Greeks did not associate morality with altruism in the way that post-Christian Western civilization has done.
Aristotle's view is that we have duties to ourselves as well as to other people (e.g. friends) and to the "polis" as a whole. The same is true for Thomas Aquinas, Christian Wolff and Immanuel Kant, who claim that there are duties to ourselves as Aristotle did, although it has been argued that, for Aristotle, the duty to one's self is primary.
Ayn Rand argued that there is a positive harmony of interests among free, rational humans, such that no moral agent can rationally coerce another person consistently with his own long-term self-interest. Rand argued that other people are an enormous value to an individual's well-being (through education, trade and affection), but also that this value could be fully realized only under conditions of political and economic freedom. According to Rand, voluntary trade alone can assure that human interaction is "mutually" beneficial. Rand's student, Leonard Peikoff has argued that the identification of one's interests itself is impossible absent the use of principles, and that self-interest cannot be consistently pursued absent a consistent adherence to certain ethical principles. Recently, Rand's position has also been defended by such writers as Tara Smith, Tibor Machan, Allan Gotthelf, David Kelley, Douglas Rasmussen, Nathaniel Branden, Harry Binswanger, Andrew Bernstein, and Craig Biddle.
Philosopher David L. Norton identified himself an "ethical individualist," and, like Rand, saw a harmony between an individual's fidelity to his own self-actualization, or "personal destiny," and the achievement of society's well being.
Criticisms.
According to amoralism, there is nothing wrong with egoism, but there is also nothing ethical about it; one can adopt rational egoism and drop morality as a superfluous attribute of the egoism.
Ethical egoism has been alleged as the basis for immorality. Egoism has also been alleged as being outside the scope of moral philosophy. Thomas Jefferson writes in an 1814 letter to Thomas Law:
In contrast, Rand saw ethics as a necessity for human survival and well-being, and argued that the "social" implications of morality, including natural rights, were simply a subset of the wider field of ethics. Thus, for Rand, "virtue" included productiveness, honesty with oneself, and scrupulousness of thought. Although she greatly admired Jefferson, she also wrote:
In "The Moral Point of View", Kurt Baier objects that ethical egoism provides no moral basis for the resolution of conflicts of interest, which, in his opinion, form the only vindication for a moral code. Were this an ideal world, one in which interests and purposes never jarred, its inhabitants would have no need of a specified set of ethics, according to Baier. This, however, is not an "ideal world." Baier believes that ethical egoism fails to provide the moral guidance and arbitration that it necessitates. Far from resolving conflicts of interest, claimed Baier, ethical egoism all too often spawns them. To this, as Rachels has shown, the ethical egoist may object that he cannot admit a construct of morality whose aim is merely to forestall conflicts of interest. "On his view," he writes, "the moralist is not like a courtroom judge, who resolves disputes. Instead, he is like the Commissioner of Boxing, who urges each fighter to do his best."
Baiers is also part of a team of philosophers who hold that ethical egoism is paradoxical, implying that to do what is in one's best interests can be both wrong and right in ethical terms. Although a successful pursuit of self-interest may be viewed as a moral victory, it could also be dubbed immoral if it prevents another person from executing what is in "his" best interests. Again, however, the ethical egoists have responded by assuming the guise of the Commissioner of Boxing. His philosophy precludes empathy for the interests of others, so forestalling them is perfectly acceptable. "Regardless of whether we think this is a correct view," adds Rachels, "it is, at the very least, a "consistent" view, and so this attempt to convict the egoist of self-contradiction fails."
Finally, it has been averred that ethical egoism is no better than bigotry in that, like racism, it divides people into two types — themselves and others — and discriminates against one type on the basis of some arbitrary disparity. This, to Rachels's mind, is probably the best objection to ethical egoism, for it provides the soundest reason why the interests of others ought to concern the interests of the self. "What," he asks, "is the difference between myself and others that justifies placing myself in this special category? Am I more intelligent? Do I enjoy my life more? Are my accomplishments greater? Do I have needs or abilities that are so different from the needs and abilities of others? "What is it that makes me so special"? Failing an answer, it turns out that Ethical Egoism is an arbitrary doctrine, in the same way that racism is arbitrary. [...] We should care about the interests of other people "for the very same reason we care about our own interests"; for their needs and desires are comparable to our own."

</doc>
<doc id="9236" url="http://en.wikipedia.org/wiki?curid=9236" title="Evolution">
Evolution

Evolution is the change in the inherited characteristics of biological populations over successive generations. Evolutionary processes give rise to diversity at every level of biological organisation, including species, individual organisms and molecules such as DNA and proteins.
All life on Earth is descended from a last universal ancestor that lived approximately 3.8 billion years ago. Repeated speciation and the divergence of life can be inferred from shared sets of biochemical and morphological traits, or by shared DNA sequences. These homologous traits and sequences are more similar among species that share a more recent common ancestor, and can be used to reconstruct evolutionary histories, using both existing species and the fossil record. Existing patterns of biodiversity have been shaped both by speciation and by extinction.
Charles Darwin was the first to formulate a scientific argument for the theory of evolution by means of natural selection. Evolution by natural selection is a process inferred from three facts about populations: 1) more offspring are produced than can possibly survive, 2) traits vary among individuals, leading to different rates of survival and reproduction, and 3) trait differences are heritable. Thus, when members of a population die they are replaced by the progeny of parents better adapted to survive and reproduce in the environment in which natural selection takes place. This process creates and preserves traits that are seemingly fitted for the functional roles they perform. Natural selection is the only known cause of adaptation, but not the only known cause of evolution. Other, nonadaptive causes of evolution include mutation and genetic drift.
In the early 20th century, genetics was integrated with Darwin's theory of evolution by natural selection through the discipline of population genetics. The importance of natural selection as a cause of evolution was accepted into other branches of biology. Moreover, previously held notions about evolution, such as orthogenesis and "progress" became obsolete. Scientists continue to study various aspects of evolution by forming and testing hypotheses, constructing scientific theories, using observational data, and performing experiments in both the field and the laboratory. Biologists agree that descent with modification is one of the most reliably established facts in science. Discoveries in evolutionary biology have made a significant impact not just within the traditional branches of biology, but also in other academic disciplines (e.g., anthropology and psychology) and on society at large.
History of evolutionary thought.
The proposal that one type of animal could descend from an animal of another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork "De Rerum Natura". In contrast to these materialistic views, Aristotle understood all natural things, not only living things, as being imperfect actualisations of different fixed natural possibilities, known as "forms", "ideas", or (in Latin translations) "species". This was part of his teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages, and were integrated into Christian learning, but Aristotle did not demand that real types of animals always corresponded one-for-one with exact metaphysical forms, and specifically gave examples of how new types of living things could come to be.
In the 17th century the new method of modern science rejected Aristotle's approach, and sought explanations of natural phenomena in terms of physical laws which were the same for all visible things, and did not need to assume any fixed natural categories, nor any divine cosmic order. But this new approach was slow to take root in the biological sciences, which became the last bastion of the concept of fixed natural types. John Ray used one of the previously more general terms for fixed natural types, "species", to apply to animal and plant types, but he strictly identified each type of living thing as a species, and proposed that each species can be defined by the features that perpetuate themselves each generation. These species were designed by God, but showing differences caused by local conditions. The biological classification introduced by Carolus Linnaeus in 1735 also viewed species as fixed according to a divine plan.
Other naturalists of this time speculated on evolutionary change of species over time according to natural laws. Maupertuis wrote in 1751 of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Buffon suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single micro-organism (or "filament"). The first full-fledged evolutionary scheme was Lamarck's "transmutation" theory of 1809 which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and that on a local level these lineages adapted to the environment by inheriting changes caused by use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the "Natural Theology or Evidences of the Existence and Attributes of the Deity" (1802) which proposed complex adaptations as evidence of divine design, and was admired by Charles Darwin.
The critical break from the concept of fixed species in biology began with the theory of evolution by natural selection, which was formulated by Charles Darwin. Partly influenced by "An Essay on the Principle of Population" by Thomas Robert Malthus, Darwin noted that population growth would lead to a "struggle for existence" where favorable variations could prevail as others perished. Each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of animals and plants from a common ancestry through the working of natural laws working the same for all types of thing. Darwin was developing his theory of "natural selection" from 1838 onwards until Alfred Russel Wallace sent him a similar theory in 1858. Both men presented their separate papers to the Linnean Society of London. At the end of 1859, Darwin's publication of "On the Origin of Species" explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwinian evolution. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.
Precise mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865 Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells (sperm and eggs) and somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cells structure. De Vries was also one of the researchers who made Mendel's work well-known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, De Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. At the turn of the 20th century, pioneers in the field of population genetics, such as J.B.S. Haldane, Sewall Wright, and Ronald Fisher, set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.
In the 1920s and 1930s a modern evolutionary synthesis connected natural selection, mutation theory, and Mendelian inheritance into a unified theory that applied generally to any branch of biology. The modern synthesis was able to explain patterns observed across species in populations, through fossil transitions in palaeontology, and even complex cellular mechanisms in developmental biology. The publication of the structure of DNA by James Watson and Francis Crick in 1953 demonstrated a physical basis for inheritance. Molecular biology improved our understanding of the relationship between genotype and phenotype. Advancements were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that "nothing in biology makes sense except in the light of evolution", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.
Since then, the modern synthesis has been further extended to explain biological phenomena across the full and integrative scale of the biological hierarchy, from genes to species. This extension has been dubbed "evo-devo".
Heredity.
Evolution in organisms occurs through changes in heritable traits – particular characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the "brown-eye trait" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome is called its genotype.
The complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. These traits come from the interaction of its genotype with the environment. As a result, many aspects of an organism's phenotype are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. However, some people tan more easily than others, due to differences in their genotype; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.
Heritable traits are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long polymer composed of four types of bases. The sequence of bases along a particular DNA molecule specify the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, the long strands of DNA form condensed structures called chromosomes. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are more complex and are controlled by multiple interacting genes.
Recent findings have confirmed important examples of heritable changes that cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalization. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Descendants inherit genes plus environmental characteristics generated by the ecological actions of ancestors. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.
Variation.
An individual organism's phenotype results from both its genotype and the influence from the environment it has lived in. A substantial part of the variation in phenotypes in a population is caused by the differences between their genotypes. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation — when it either disappears from the population or replaces the ancestral allele entirely.
Natural selection will only cause evolution if there is enough genetic variation in a population. Before the discovery of Mendelian genetics, one common hypothesis was blending inheritance. But with blending inheritance, genetic variance would be rapidly lost, making evolution by natural selection implausible. The "Hardy-Weinberg principle" provides the solution to how variation is maintained in a population with Mendelian inheritance. The frequencies of alleles (variations in a gene) will remain constant in the absence of selection, mutation, migration and genetic drift.
Variation comes from mutations in genetic material, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is identical in all individuals of that species. However, even relatively small differences in genotype can lead to dramatic differences in phenotype: for example, chimpanzees and humans differ in only about 5% of their genomes.
Mutation.
Mutations are changes in the DNA sequence of a cell's genome. When mutations occur, they can either have no effect, alter the product of a gene, or prevent the gene from functioning. Based on studies in the fly "Drosophila melanogaster", it has been suggested that if a mutation changes a protein produced by a gene, this will probably be harmful, with about 70% of these mutations having damaging effects, and the remainder being either neutral or weakly beneficial.
Mutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.
New genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA.
The generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions. When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to one hundred independent domains that each catalyse one step in the overall process, like a step in an assembly line.
Sex and recombination.
In asexual organisms, genes are inherited together, or "linked", as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.
Gene flow.
Gene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy metal tolerant and heavy metal sensitive populations of grasses.
Gene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast "Saccharomyces cerevisiae" and the adzuki bean beetle "Callosobruchus chinensis" has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.
Large-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.
Mechanisms.
From a Neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms. For example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, genetic hitchhiking, mutation and gene flow.
Natural selection.
Evolution by means of natural selection is the process by which genetic mutations that enhance reproduction become and remain more common in successive generations of a population. It has often been called a "self-evident" mechanism because it necessarily follows from three simple facts:
These conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits on to the next generation than those with traits that do not confer an advantage.
The central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.
If an allele increases fitness more than the other alleles of that gene, then with each generation this allele will become more common within the population. These traits are said to be "selected "for"". Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele becoming rarer — they are "selected "against"". Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form (see Dollo's law).
Natural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time — for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilizing selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to slowly become all the same height.
A special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard to fake, sexually selected traits.
Natural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. "Nature" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: "Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity and material cycles (ie: exchange of materials between living and nonliving parts) within the system." Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.
Natural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of co-operation, as discussed below.
Biased mutation.
In addition to being a major source of variation, mutation may also function as a mechanism of evolution when there are different probabilities at the molecular level for different mutations to occur, a process known as mutation bias. If two genotypes, for example one with the nucleotide G and another with the nucleotide A in the same position, have the same fitness, but mutation from G to A happens more often than mutation from A to G, then genotypes with A will tend to evolve. Different insertion vs. deletion mutation biases in different taxa can lead to the evolution of different genome sizes. Developmental or mutational biases have also been observed in morphological evolution. For example, according to the phenotype-first theory of evolution, mutations can eventually cause the genetic assimilation of traits that were previously induced by the environment.
Mutation bias effects are superimposed on other processes. If selection would favor either one out of two mutations, but there is no extra advantage to having both, then the mutation that occurs the most frequently is the one that is most likely to become fixed in a population. Mutations leading to the loss of function of a gene are much more common than mutations that produce a new, fully functional gene. Most loss of function mutations are selected against. But when selection is weak, mutation bias towards loss of function can affect evolution. For example, pigments are no longer useful when animals live in the darkness of caves, and tend to be lost. This kind of loss of function can occur because of mutation bias, and/or because the function had a cost, and once the benefit of the function disappeared, natural selection leads to the loss. Loss of sporulation ability in a bacterium during laboratory evolution appears to have been caused by mutation bias, rather than natural selection against the cost of maintaining sporulation ability. When there is no selection for loss of function, the speed at which loss evolves depends more on the mutation rate than it does on the effective population size, indicating that it is driven more by mutation bias than by genetic drift.
Genetic drift.
Genetic drift is the change in allele frequency from one generation to the next that occurs because alleles are subject to sampling error. As a result, when selective forces are absent or relatively weak, allele frequencies tend to "drift" upward or downward randomly (in a random walk). This drift halts when an allele eventually becomes fixed, either by disappearing from the population, or replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that began with the same genetic structure to drift apart into two divergent populations with different sets of alleles.
It is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.
The neutral theory of molecular evolution proposed that most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. Hence, in this model, most genetic changes in a population are the result of constant mutation pressure and genetic drift. This form of the neutral theory is now largely abandoned, since it does not seem to fit the genetic variation seen in nature. However, a more recent and better-supported version of this model is the nearly neutral theory, where a mutation that would be neutral in a small population is not necessarily neutral in a large population. Other alternative theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft.
The time for a neutral allele to become fixed by genetic drift depends on population size, with fixation occurring more rapidly in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.
Genetic hitchhiking.
Recombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.
Gene flow.
Gene flow involves the exchange of genes between populations and between species. The presence or absence of gene flow fundamentally changes the course of evolution. Due to the complexity of organisms, any two completely isolated populations will eventually evolve genetic incompatibilities through neutral processes, as in the Bateson-Dobzhansky-Muller model, even if both populations remain essentially identical in terms of their adaptation to the environment.
If genetic differentiation between populations develops, gene flow between populations can introduce traits or alleles which are disadvantageous in the local population and this may lead to organisms within these populations evolving mechanisms that prevent mating with genetically distant populations, eventually resulting in the appearance of new species. Thus, exchange of genetic information between individuals is fundamentally important for the development of the biological species concept (BSC).
During the development of the modern synthesis, Sewall Wright developed his shifting balance theory, which regarded gene flow between partially isolated populations as an important aspect of adaptive evolution. However, recently there has been substantial criticism of the importance of the shifting balance theory.
Outcomes.
Evolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by co-operating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed.
These outcomes of evolution are sometimes divided into macroevolution, which is evolution that occurs at or above the level of species, such as extinction and speciation and microevolution, which is smaller evolutionary changes, such as adaptations, within a species or population. In general, macroevolution is regarded as the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one – the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels – with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.
A common misconception is that evolution has goals or long-term plans; realistically however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size, and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to modern evolutionary research, since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.
Adaptation.
Adaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term "adaptation" for the evolutionary process and "adaptive trait" for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky.
Adaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria "Escherichia coli" evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, "Flavobacterium" evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium "Sphingobium" evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).
Adaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.
During evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, and the presence of hip bones in whales and snakes. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.
However, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard "Holaspis guentheri", which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.
An area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.
Co-evolution.
Interactions between organisms can produce both conflict and co-operation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called co-evolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.
Co-operation.
Not all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.
Coalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.
Such cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the "helping" individual contains alleles which promote the helping activity, it is likely that its kin will "also" contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.
Speciation.
Speciation is the process where a species diverges into two or more descendant species.
There are multiple ways to define the concept of "species". The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The biological species concept (BSC) is a classic example of the interbreeding approach. Defined by Ernst Mayr in 1942, the BSC states that "species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups". Despite its wide and long-term use, the BSC like others is not without controversy, for example because these concepts cannot be applied to prokaryotes, and this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.
Barriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.
Speciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four mechanisms for speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.
The second mechanism of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.
The third mechanism of speciation is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass "Anthoxanthum odoratum", which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause "reinforcement", which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.
Finally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and non-random mating, to allow reproductive isolation to evolve.
One type of sympatric speciation involves cross-breeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species "Arabidopsis thaliana" and "Arabidopsis arenosa" cross-bred to give the new species "Arabidopsis suecica". This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.
Speciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short "bursts" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.
Extinction.
Extinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs went extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future.
The role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous "low-level" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (competitive exclusion). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.
Evolutionary history of life.
Origin of life.
Highly energetic chemistry is thought to have produced a self-replicating molecule around ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.
Common descent.
All organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits and finally, that organisms can be classified using these similarities into a hierarchy of nested groups – similar to a family tree. However, modern research has suggested that, due to horizontal gene transfer, this "tree of life" may be more complicated than a simple branching tree since some genes have spread independently between distantly related species.
Past species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, paleontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.
More recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.
Evolution of life.
Prokaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6 – 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent co-evolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.
The history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria.
Soon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.
About 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from "reptile"-like lineages), mammals around 129 million years ago, homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.
Applications.
Concepts and models used in evolutionary biology, such as natural selection, have many applications.
Artificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.
Understanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.
Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.
In computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.
Social and cultural responses.
In the 19th century, particularly after the publication of "On the Origin of Species" in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.
While various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of "Vestiges of the Natural History of Creation" in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation-evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.
The teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The "Scopes Trial" decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 "Epperson v. Arkansas" decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design, to be excluded once again in the 2005 "Kitzmiller v. Dover Area School District" case.
Further reading.
Introductory reading
Advanced reading

</doc>
<doc id="9238" url="http://en.wikipedia.org/wiki?curid=9238" title="Ernst Mayr">
Ernst Mayr

Ernst Walter Mayr (; July 5, 1904 – February 3, 2005) was one of the 20th century's leading evolutionary biologists. He was also a renowned taxonomist, tropical explorer, ornithologist, and historian of science. His work contributed to the conceptual revolution that led to the modern evolutionary synthesis of Mendelian genetics, systematics, and Darwinian evolution, and to the development of the biological species concept.
Although Charles Darwin and others posited that multiple species could evolve from a single common ancestor, the mechanism by which this occurred was not understood, creating the "species problem". Ernst Mayr approached the problem with a new definition for species. In his book "Systematics and the Origin of Species" (1942) he wrote that a species is not just a group of morphologically similar individuals, but a group that can breed only among themselves, excluding all others. When populations within a species become isolated by geography, feeding strategy, mate selection, or other means, they may start to differ from other populations through genetic drift and natural selection, and over time may evolve into new species. The most significant and rapid genetic reorganization occurs in extremely small populations that have been isolated (as on islands).
His theory of peripatric speciation (a more precise form of allopatric speciation which he advanced), based on his work on birds, is still considered a leading mode of speciation, and was the theoretical underpinning for the theory of punctuated equilibrium, proposed by Niles Eldredge and Stephen Jay Gould. Mayr is sometimes credited with inventing modern philosophy of biology, particularly the part related to evolutionary biology, which he distinguished from physics due to its introduction of (natural) history into science.
Biography.
Mayr was the second son of Helene Pusinelli and Dr. Otto Mayr. His father was a jurist (District Prosecuting
Attorney at Würzburg) but took an interest in natural history and took the children out on field trips. He learnt all the local birds in Würzburg from his elder brother Otto. He also had access to a natural history magazine for amateurs, "Kosmos". His father died just before he was thirteen. The family then moved to Dresden and he studied at the Staatsgymnasium (“Royal Gymnasium” until 1918) in Dresden-Neustadt and completed his high school education there. In April 1922, while still in high school, he joined the newly founded Saxony Ornithologists’ Association. Here he met Rudolf Zimmermann, who became his ornithological mentor. In February 1923, Mayr passed his high school examination (Abitur) and his mother rewarded him with a pair of binoculars.
On March 23, 1923 on the lakes of Moritzburg, the Frauenteich, he spotted what he identified as a Red-crested Pochard. The species had not been seen in Saxony since 1845 and the local club argued about the identity. Raimund Schelcher (1891–1979) of the club then suggested that Mayr visit his classmate Erwin Stresemann on his way to Greifswald, where Mayr was to begin his medical studies. After a tough interrogation, Stresemann accepted and published the sighting as authentic. Stresemann was very impressed and suggested that, between semesters, Mayr could work as a volunteer in the ornithological section of the museum. Mayr wrote about this event, "It was as if someone had given me the key to heaven." He entered the University of Greifswald in 1923 and, according to Mayr himself, "took the medical curriculum (to satisfy a family tradition) but after only a year, he decided to leave medicine and enrolled at the Faculty of Biological Sciences." Mayr was endlessly interested in ornithology and "chose Greifswald at the Baltic for my studies for no other reason than that...it was situated in the ornithologically most interesting area." Although he ostensibly planned to become a physician, he was "first and foremost an ornithologist." During the first semester break Stresemann gave him a test to identify treecreepers and Mayr was able to identify most of the specimens correctly. Stresemann declared that Mayr 'was a born systematist'. In 1925 Stresemann suggested that he give up his medical studies, in fact he leaves faculty of medicine and enroll into faculty of Biology and then join the Berlin Museum with the prospect of bird-collecting trips to the tropics on the condition that he completed his doctoral studies in 16 months. Mayr completed his doctorate in ornithology at the University of Berlin under Dr. Carl Zimmer, who was a full professor (Ordentlicher Professor), on June 24, 1926 at the age of 21. On July 1 he accepted the position offered to him at the Museum for a monthly salary of 330.54 Reichsmark.
At the International Zoological Congress at Budapest in 1927, Mayr was introduced by Stresemann to banker and naturalist Walter Rothschild, who asked him to undertake an expedition to New Guinea on behalf of himself and the American Museum of Natural History in New York. In New Guinea, Mayr collected several thousand bird skins (he named 26 new bird species during his lifetime) and, in the process also named 38 new orchid species. During his stay in New Guinea, he was invited to accompany the Whitney South Seas Expedition to the Solomon Islands. Also, while in New Guinea, he visited the Lutheran missionaries Otto Thiele and Christian Keyser, in the Finschhafen district; there, while in conversation with his hosts, he uncovered the discrepancies in Hermann Detzner's popular book, "Four Years among the Cannibals in German Guinea from 1914 to the Truce," in which Detzner claimed to have seen the interior, discovered several species of flora and fauna, while remaining only steps ahead of the Australian patrols sent to capture him.
He returned to Germany in 1930 and in 1931 he accepted a curatorial position at the American Museum of Natural History, where he played the important role of brokering and acquiring the Walter Rothschild collection of bird skins, which was being sold in order to pay off a blackmailer. During his time at the museum he produced numerous publications on bird taxonomy, and in 1942 his first book, "Systematics and the Origin of Species", which completed the evolutionary synthesis started by Darwin.
After Mayr was appointed at the American Museum of Natural History, he influenced American ornithological research by mentoring young birdwatchers. Mayr was surprised at the differences between American and German birding societies. He noted that the German society was "far more scientific, far more interested in life histories and breeding bird species, as well as in reports on recent literature."
Mayr organized a monthly seminar under the auspices of the Linnean Society of New York. Under the influence of J. A. Allen, Frank Chapman, and Jonathan Dwight, the society concentrated on taxonomy and later became a clearing house for bird banding and sight records.
Mayr encouraged his Linnaean Society seminar participants to take up a specific research project of their own. Under Mayr's influence one of them, Joseph Hickey, went on to write "A Guide to Birdwatching" (1943). Hickey remembered later, "Mayr was our age and invited on all our field trips. The heckling of this German foreigner was tremendous, but he gave tit for tat, and any modern picture of Dr E. Mayr as a very formal person does not square with my memory of the 1930s. He held his own." A group of eight young birdwatchers from The Bronx later became the Bronx County Bird Club, led by Ludlow Griscom. "Everyone should have a problem" was the way one Bronx County Bird Club member recalled Mayr's refrain.
Mayr said of his own involvement with the local birdwatchers: "In those early years in New York when I was a stranger in a big city, it was the companionship and later friendship which I was offered in the Linnean Society that was the most important thing in my life."
Mayr also greatly influenced the American ornithologist Margaret Morse Nice. Mayr encouraged her to correspond with European ornithologists and helped her in her landmark study on song sparrows. Nice wrote to Joseph Grinnell in 1932, trying to get foreign literature reviewed in the "Condor": "Too many American ornithologists have despised the study of the living bird; the magazines and books that deal with the subject abound in careless statements, anthropomorphic interpretations, repetition of ancient errors, and sweeping conclusions from a pitiful array of facts. ... in Europe the study of the living bird is taken seriously. We could learn a great deal from their writing." Mayr ensured that Nice could publish her two-volume "Studies in the Life History of the Song Sparrow". He found her a publisher, and her book was reviewed by Aldo Leopold, Joseph Grinnell, and Jean Delacour. Nice dedicated her book to "My Friend Ernst Mayr."
Mayr joined the faculty of Harvard University in 1953, where he also served as director of the Museum of Comparative Zoology from 1961 to 1970. He retired in 1975 as emeritus professor of zoology, showered with honors. Following his retirement, he went on to publish more than 200 articles, in a variety of journals—more than some reputable scientists publish in their entire careers; 14 of his 25 books were published after he was 65. Even as a centenarian, he continued to write books. On his 100th birthday, he was interviewed by "Scientific American" magazine. Mayr died on 3 February 2005 in his retirement home in Bedford, Massachusetts after a short illness. His wife, Margarete, died in 1990. He was survived by two daughters, five grandchildren and 10 great-grandchildren.
The awards that Mayr received include the National Medal of Science, the Balzan Prize, the Sarton Medal of the History of Science Society, the International Prize for Biology, the Loye and Alden Miller Research Award, and the Lewis Thomas Prize for Writing about Science. In 1939 he was elected a Corresponding Member of the Royal Australasian Ornithologists Union. He was awarded the Linnean Society of London's prestigious Darwin-Wallace Medal in 1958 and the Linnaean Society of New York's inaugural Eisenmann Medal in 1983. For his work, "Animal Species and Evolution", he was awarded the Daniel Giraud Elliot Medal from the National Academy of Sciences in 1967. In 1995 he received the Benjamin Franklin Medal for Distinguished Achievement in the Sciences of the American Philosophical Society.
Mayr never won a Nobel Prize, but he noted that there is no prize for evolutionary biology and that Darwin would not have received one, either. (In fact, there is no Nobel Prize for biology.) Mayr did win a 1999 Crafoord Prize. It honors basic research in fields that do not qualify for Nobel Prizes and is administered by the same organization as the Nobel Prize.
Mayr was co-author of six global reviews of bird species new to science (listed below).
Mayr said he was an atheist towards "the idea of a personal God" because "there is nothing that supports [it]" 
Mayr's ideas.
As a traditionally trained biologist with little mathematical experience, Mayr was often highly critical of early mathematical approaches to evolution such as those of J.B.S. Haldane, famously calling such approaches "beanbag genetics" in 1959. He maintained that factors such as reproductive isolation had to be taken into account. In a similar fashion, Mayr was also quite critical of molecular evolutionary studies such as those of Carl Woese.
In many of his writings, Mayr rejected reductionism in evolutionary biology, arguing that evolutionary pressures act on the whole organism, not on single genes, and that genes can have different effects depending on the other genes present. He advocated a study of the whole genome rather than of isolated genes only. Current molecular studies in evolution and speciation indicate that although allopatric speciation seems to be the norm in groups (such as in many invertebrates—especially in the insects), there are numerous cases of sympatric speciation in groups with greater mobility (such as the birds).
After articulating the biological species concept in 1942, Mayr played a central role in the species problem debate over what was the best species concept. He staunchly defended the biological species concept against the many definitions of "species" that others proposed.
Mayr was an outspoken defender of the scientific method, and one known to sharply critique science on the edge. As a notable example, in 1995, he criticized the Search for Extra-Terrestrial Intelligence (SETI) as conducted by fellow Harvard professor Paul Horowitz as being a waste of university and student resources, for its inability to address and answer a scientific question. Carl Sagan provided a strong rebuttal to the criticism, and pointed out that many eminent biologists and biochemists had endorsed SETI with the statement: 
Mayr rejected the idea of a gene-centered view of evolution and starkly but politely criticized Richard Dawkins' ideas:
Mayr insisted throughout his career that the gene as the target of selection cannot and should not be considered a valid idea in modern evolutionary thought.

</doc>
<doc id="9239" url="http://en.wikipedia.org/wiki?curid=9239" title="Europe">
Europe

Europe ( or ) is a continent that comprises the westernmost peninsula of Eurasia. It is generally divided from Asia by the watershed divides of the Ural and Caucasus Mountains, the Ural River, the Caspian and Black Seas, and the waterways connecting the Black and Aegean Seas.
Europe is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, the Mediterranean Sea to the south, and the Black Sea and connected waterways to the southeast. Yet the borders of Europe—a concept dating back to classical antiquity—are arbitrary, as the primarily physiographic term "continent" also incorporates cultural and political elements.
Europe is the world's second-smallest continent by surface area, covering about or 2% of the Earth's surface and about 6.8% of its land area. Of Europe's approximately 50 countries, Russia is by far the largest by both area and population, taking up 40% of the continent (although the country has territory in both Europe and Asia), while Vatican City is the smallest. Europe is the third-most populous continent after Asia and Africa, with a population of 739-743 million or about 11% of the world's population. The most commonly used currency is the euro.
Europe, in particular ancient Greece, is the birthplace of Western culture. It played a predominant role in global affairs from the 15th century onwards, especially after the beginning of colonialism. Between the 16th and 20th centuries, European nations controlled at various times the Americas, most of Africa, Oceania, and the overwhelming majority of Asia. The Industrial Revolution, which began in Great Britain around the end of the 18th century, gave rise to radical economic, cultural, and social change in Western Europe, and eventually the wider world. Demographic growth meant that, by 1900, Europe's share of the world's population was 25%.
Both world wars were largely focused upon Europe, greatly contributing to a decline in Western European dominance in world affairs by the mid-20th century as the United States and Soviet Union took prominence. During the Cold War, Europe was divided along the Iron Curtain between NATO in the west and the Warsaw Pact in the east. European integration led to the formation of the Council of Europe and the European Union in Western Europe, both of which have been expanding eastward since the revolutions of 1989 and the fall of the Soviet Union in 1991. The European Union nowadays has growing influence over its member countries. Many European countries are members of the Schengen Area, which abolishes border and immigration controls among its members.
Definition.
The use of the term "Europe" has developed gradually throughout history. In antiquity, the Greek historian Herodotus mentioned that the world had been divided by unknown persons into three parts, Europe, Asia, and Libya (Africa), with the Nile and the River Phasis forming their boundaries—though he also states that some considered the River Don, rather than the Phasis, as the boundary between Europe and Asia. Europe's eastern frontier was defined in the 1st century by geographer Strabo at the River Don. The "Book of Jubilees" described the continents as the lands given by Noah to his three sons; Europe was defined as stretching from the Pillars of Hercules at the Strait of Gibraltar, separating it from North Africa, to the Don, separating it from Asia.
A cultural definition of Europe as the lands of Latin Christendom coalesced in the 8th century, signifying the new cultural condominium created through the confluence of Germanic traditions and Christian-Latin culture, defined partly in contrast with Byzantium and Islam, and limited to northern Iberia, the British Isles, France, Christianized western Germany, the Alpine regions and northern and central Italy. The concept is one of the lasting legacies of the Carolingian Renaissance: "Europa" often figures in the letters of Charlemagne's court scholar, Alcuin. This division—as much cultural as geographical—was used until the Late Middle Ages, when it was challenged by the Age of Discovery. The problem of redefining Europe was finally resolved in 1730 when, instead of waterways, the Swedish geographer and cartographer von Strahlenberg proposed the Ural Mountains as the most significant eastern boundary, a suggestion that found favour in Russia and throughout Europe.
Europe is now generally defined by geographers as the westernmost peninsula of Eurasia, with its boundaries marked by large bodies of water to the north, west and south; Europe's limits to the far east are usually taken to be the Urals, the Ural River, and the Caspian Sea; to the southeast, including the Caucasus Mountains, the Black Sea and the waterways connecting the Black Sea to the Mediterranean Sea.
Islands are generally grouped with the nearest continental landmass, hence Iceland is generally considered to be part of Europe, while the nearby island of Greenland is usually assigned to North America. Nevertheless, there are some exceptions based on sociopolitical and cultural differences. Cyprus is closest to Anatolia (or Asia Minor), but is usually considered part of Europe both culturally and politically and currently is a member state of the EU. Malta was considered an island of North Africa for centuries.
Sometimes, the word 'Europe' is used in a geopolitically limiting way to refer only to the European Union or, even more exclusively, a culturally defined core. On the other hand, the Council of Europe has 47 member countries, and only 28 member states are in the EU. In addition, people in the British Isles may refer to "continental" or "mainland" Europe as Europe.
Etymology.
In ancient Greek mythology, Europa was a Phoenician princess whom Zeus abducted after assuming the form of a dazzling white bull. He took her to the island of Crete where she gave birth to Minos, Rhadamanthus, and Sarpedon. For Homer, Europe (, ""; see also List of Greek place names) was a mythological queen of Crete, not a geographical designation.
The etymology of "Europe" is uncertain. One theory suggests that it is derived from the Greek εὐρύς ("eurus"), meaning "wide, broad" and ὤψ/ὠπ-/ὀπτ- ("ōps"/"ōp"-/"opt-"), meaning "eye, face, countenance", hence ', "wide-gazing", "broad of aspect" (compare with "glaukōpis" (γλαυκῶπις 'grey-eyed') Athena or "boōp"'is" (βοὠπις 'ox-eyed') Hera). "Broad" has been an epithet of Earth itself in the reconstructed Proto-Indo-European religion. Another theory suggests that it is based on a Semitic word such as the Akkadian "erebu" meaning "to go down, set" (in reference to the sun), cognate to Phoenician " 'ereb" "evening; west" and Arabic Maghreb, Hebrew "ma'arav" (see also "Erebus", PIE "*h1regʷos", "darkness"). However, Martin Litchfield West states that "phonologically, the match between Europa's name and any form of the Semitic word is very poor". 
Whatever the origin of the name of the mythological figure, Εὐρώπη is first used as a geographical term in the 6th century BC, by Greek geographers such as Anaximander and Hecataeus. Anaximander placed the boundary between Asia and Europe along the Phasis River (the modern Rioni) in the Caucasus, a convention still followed by Herodotus in the 5th century BC. But the convention received by the Middle Ages and surviving into modern usage is that of the Roman era used by Roman era authors such as Posidonius, Strabo and Ptolemy,
who took the Tanais (the modern Don River) as the boundary. 
The term "Europe" is first used for a cultural sphere in the Carolingian Renaissance of the 9th century. From that time, the term designated the sphere of influence of the Western Church, as opposed to both the Eastern Orthodox churches and to the Islamic world. The modern convention, enlarging the area of "Europe" somewhat to the east and the southeast, develops in the 19th century.
Most major world languages use words derived from "Europa" to refer to the "continent" (peninsula). Chinese, for example, uses the word (歐洲); a similar Chinese-derived term is also sometimes used in Japanese such as in the Japanese name of the European Union, , despite the katakana being more commonly used. However, in some Turkic languages the originally Persian name "Frangistan" (land of the Franks) is used casually in referring to much of Europe, besides official names such as "Avrupa" or "Evropa".
History.
Prehistory.
"Homo erectus georgicus", which lived roughly 1.8 million years ago in Georgia, is the earliest hominid to have been discovered in Europe. Other hominid remains, dating back roughly 1 million years, have been discovered in Atapuerca, Spain. Neanderthal man (named after the Neandertal valley in Germany) appeared in Europe 150,000 years ago and disappeared from the fossil record about 28,000 BC, with this extinction probably due to climate change, and their final refuge being present-day Portugal. The Neanderthals were supplanted by modern humans (Cro-Magnons), who appeared in Europe around 43 to 40 thousand years ago.
The European Neolithic period—marked by the cultivation of crops and the raising of livestock, increased numbers of settlements and the widespread use of pottery—began around 7000 BC in Greece and the Balkans, probably influenced by earlier farming practices in Anatolia and the Near East. It spread from the Balkans along the valleys of the Danube and the Rhine (Linear Pottery culture) and along the Mediterranean coast (Cardial culture). Between 4500 and 3000 BC, these central European neolithic cultures developed further to the west and the north, transmitting newly acquired skills in producing copper artefacts. In Western Europe the Neolithic period was characterised not by large agricultural settlements but by field monuments, such as causewayed enclosures, burial mounds and megalithic tombs. The Corded Ware cultural horizon flourished at the transition from the Neolithic to the Chalcolithic. During this period giant megalithic monuments, such as the Megalithic Temples of Malta and Stonehenge, were constructed throughout Western and Southern Europe. The European Bronze Age began c. 3200 BC in Greece.
The European Iron Age began around 1200 BC. Iron Age colonisation by the Greeks and Phoenicians gave rise to early Mediterranean cities. Early Iron Age Italy and Greece from around the 8th century BC gradually gave rise to historical Classical antiquity.
Classical antiquity.
Ancient Greece had a profound influence on Western civilisation. Western democratic and individualistic culture are often attributed to Ancient Greece. The Greeks invented the polis, or city-state, which played a fundamental role in their concept of identity. These Greek political ideals were rediscovered in the late 18th century by European philosophers and idealists. Greece also generated many cultural contributions: in philosophy, humanism and rationalism under Aristotle, Socrates and Plato; in history with Herodotus and Thucydides; in dramatic and narrative verse, starting with the epic poems of Homer; in medicine with Hippocrates and Galen; and in science with Pythagoras, Euclid and Archimedes.
Another major influence came on Europe that would impact Western civilisation from the Roman Empire which left its mark on law, politics, language, engineering, architecture, government and many more aspects in western civilisation. During the "pax romana", the Roman Empire expanded to encompass the entire Mediterranean Basin and much of Europe.
Stoicism influenced Roman emperors such as Hadrian, Antoninus Pius, and Marcus Aurelius, who all spent time on the Empire's northern border fighting Germanic, Pictish and Scottish tribes. Christianity was eventually legitimised by Constantine I after three centuries of imperial persecution.
Early Middle Ages.
During the decline of the Roman Empire, Europe entered a long period of change arising from what historians call the "Age of Migrations". There were numerous invasions and migrations amongst the Ostrogoths, Visigoths, Goths, Vandals, Huns, Franks, Angles, Saxons, Slavs, Avars, Bulgars and, later on, the Vikings, Pechenegs, Cumans and Magyars. Renaissance thinkers such as Petrarch would later refer to this as the "Dark Ages". Isolated monastic communities were the only places to safeguard and compile written knowledge accumulated previously; apart from this very few written records survive and much literature, philosophy, mathematics, and other thinking from the classical period disappeared from Europe.
From the 7th century, Byzantine history was greatly affected by the rise of Islam and the Caliphates. Muslim Arabs first invaded historically Roman territory under Abū Bakr, first Caliph of the Rashidun Caliphate, who entered Roman Syria and Roman Mesopotamia. Under Umar, the second Caliph, the Muslims decisively conquered Syria and Mesopotamia, as well as Roman Palestine, Roman Egypt, and parts of Asia Minor and Roman North Africa. This trend continued under Umar's successors and under the Umayyad Caliphate, which conquered the rest of Mediterranean North Africa and most of the Iberian Peninsula. Over the next centuries Muslim forces were able to take further European territory, including Cyprus, Malta, Crete, Sicily and parts of southern Italy. In the East, Volga Bulgaria became an Islamic state in the 10th century.
The Muslim conquest of Hispania began when the Moors (mostly Berbers with some Arabs) invaded the Christian Visigothic kingdom of Iberia in the year 711, under their Berber leader Tariq ibn Ziyad. They landed at Gibraltar on 30 April and worked their way northward. Tariq's forces were joined the next year by those of his superior, Musa ibn Nusair. During the eight-year campaign most of the Iberian Peninsula was brought under Muslim rule — save for small areas in the northwest (Asturias) and largely Basque regions in the Pyrenees, with the exception of the Basque Banu Qasi Muslim dynasty. This territory, under the Arabic name Al-Andalus, became part of the expanding Umayyad empire.
The unsuccessful second siege of Constantinople (717) weakened the Umayyad dynasty and reduced their prestige. After their success in over-running Iberia, the conquerors moved northeast across the Pyrenees, but were defeated by the Frankish leader Charles Martel at the Battle of Poitiers in 732, though they continued to raid and capture cities as far as Avignon. The Umayyads were overthrown in 750 by the 'Abbāsids and most of the Umayyad clan massacred.
A surviving Umayyad prince, Abd al-Rahman I, escaped to Iberia and founded a new Umayyad dynasty in the Emirate of Cordoba, (756). Charles Martel's son, Pippin the Short retook Narbonne, and his grandson Charlemagne established the Marca Hispanica across the Pyrenees in part of what today is Catalonia, reconquering Girona in 785 and Barcelona in 801. The Umayyads in Iberia proclaimed themselves caliphs in 929.
During the Dark Ages, the Western Roman Empire fell under the control of various tribes. The Germanic and Slav tribes established their domains over Western and Eastern Europe respectively. Eventually the Frankish tribes were united under Clovis I. Charlemagne, a Frankish king of the Carolingian dynasty who had conquered most of Western Europe, was anointed "Holy Roman Emperor" by the Pope in 800. This led in 962 to the founding of the Holy Roman Empire, which eventually became centred in the German principalities of central Europe.
East Central Europe saw the creation of Slavic states and the adoption of Christianity (circa 1000 AD). Powerful West Slavic state of Great Moravia spread its territory all the way south to the Balkan Slavs. Moravia reached its largest territorial extent under Svatopluk I and caused a series of armed conflicts with East Francia. Further south, placed between the Frankish Empire and the Byzantines, the first South Slavic states emerged in the late 7th and 8th century: First Bulgarian Empire, Serbian Principality (later Kingdom and Empire) and Duchy of Croatia (later Kingdom of Croatia).
The predominantly Greek speaking Eastern Roman Empire retroactively became known in the West as the Byzantine Empire. Its capital was Constantinople. Emperor Justinian I presided over Constantinople's first golden age: he established a legal code, funded the construction of the Hagia Sophia and brought the Christian church under state control. Fatally weakened by the sack of Constantinople in 1204, during the Fourth Crusade, Byzantium fell in 1453 when it was conquered by the Ottoman Empire.
Middle Ages.
The economic growth of Europe around the year 1000, together with the lack of safety on the mainland trading routes, made possible the development of major commercial routes along the coast of the Mediterranean Sea. In this context, the growing independence acquired by some coastal cities gave the Maritime Republics a leading role in the European scene.
The Middle Ages on the mainland were dominated by the two upper echelons of the social structure: the nobility and the clergy. Feudalism developed in France in the Early Middle Ages and soon spread throughout Europe. A struggle for influence between the nobility and the monarchy in England led to the writing of the Magna Carta and the establishment of a parliament. The primary source of culture in this period came from the Roman Catholic Church. Through monasteries and cathedral schools, the Church was responsible for education in much of Europe.
The Papacy reached the height of its power during the High Middle Ages. An East-West Schism in 1054 split the former Roman Empire religiously, with the Eastern Orthodox Church in the Byzantine Empire and the Roman Catholic Church in the former Western Roman Empire. In 1095 Pope Urban II called for a crusade against Muslims occupying Jerusalem and the Holy Land. In Europe itself, the Church organised the Inquisition against heretics. In Spain, the Reconquista concluded with the fall of Granada in 1492, ending over seven centuries of Islamic rule in the Iberian Peninsula.
In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Pechenegs and the Cuman-Kipchaks, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north and temporarily halted the expansion of the Rus' state to the south and east. Like many other parts of Eurasia, these territories were overrun by the Mongols. The invaders, who became known as Tatars, were mostly Turkic-speaking peoples under Mongol suzerainty. They established the state of the Golden Horde with headquarters in Crimea, which later adopted Islam as a religion and ruled over modern-day southern and central Russia for more than three centuries. After the collapse of Mongol dominions, the first Romanian states (principalities) emerged in the 14th century: Moldova and Walachia. Previously, these territories were under the successive control of Pechenegs and Cumans.
The Great Famine of 1315–1317 was the first crisis that would strike Europe in the late Middle Ages. The period between 1348 and 1420 witnessed the heaviest loss. The population of France was reduced by half. Medieval Britain was afflicted by 95 famines, and France suffered the effects of 75 or more in the same period. Europe was devastated in the mid-14th century by the Black Death, one of the most deadly pandemics in human history which killed an estimated 25 million people in Europe alone—a third of the European population at the time.
The plague had a devastating effect on Europe's social structure; it induced people to live for the moment as illustrated by Giovanni Boccaccio in "The Decameron" (1353). It was a serious blow to the Roman Catholic Church and led to increased persecution of Jews, foreigners, beggars and lepers. The plague is thought to have returned every generation with varying virulence and mortalities until the 18th century. During this period, more than 100 plague epidemics swept across Europe.
Early modern period.
The Renaissance was a period of cultural change originating in Florence and later spreading to the rest of Europe. in the 14th century. The rise of a new humanism was accompanied by the recovery of forgotten classical Greek and Arabic knowledge from monastic libraries, often translated from Arabic into Latin. The Renaissance spread across Europe between the 14th and 16th centuries: it saw the flowering of art, philosophy, music, and the sciences, under the joint patronage of royalty, the nobility, the Roman Catholic Church, and an emerging merchant class. Patrons in Italy, including the Medici family of Florentine bankers and the Popes in Rome, funded prolific quattrocento and cinquecento artists such as Raphael, Michelangelo, and Leonardo da Vinci.
Political intrigue within the Church in the mid-14th century caused the Western Schism. During this forty-year period, two popes—one in Avignon and one in Rome—claimed rulership over the Church. Although the schism was eventually healed in 1417, the papacy's spiritual authority had suffered greatly.
The Church's power was further weakened by the Protestant Reformation (1517–1648), initially sparked by the works of German theologian Martin Luther, a result of the lack of reform within the Church. The Reformation also damaged the Holy Roman Empire's power, as German princes became divided between Protestant and Roman Catholic faiths. This eventually led to the Thirty Years War (1618–1648), which crippled the Holy Roman Empire and devastated much of Germany, killing between 25 and 40 percent of its population. In the aftermath of the Peace of Westphalia, France rose to predominance within Europe.
The 17th century in southern, central and eastern Europe was a period of general decline. Central and Eastern Europe experienced more than 150 famines in a 200-year period between 1501 to 1700. From the 15th to 18th centuries, when the disintegrating khanates of the Golden Horde were conquered by Russia, Tatars from the Crimean Khanate frequently raided Eastern Slavic lands to capture slaves. The Battle of Vienna in 1683 broke the advance of the Ottoman Turks into Europe, and marked the political hegemony of the Habsburg dynasty in central Europe. The Nogai Horde and Kazakh Khanate had frequently raided the Slavic-speaking areas of Russia, Ukraine and Poland for at least a hundred years until the Russian expansion and conquest of most of northern Eurasia (i.e. Eastern Europe, Central Asia and Siberia).
The Renaissance and the New Monarchs marked the start of an Age of Discovery, a period of exploration, invention, and scientific development. Among the great figures of the Western scientific revolution of the 16th and 17th centuries were Copernicus, Kepler, Galileo, and Isaac Newton. According to Peter Barrett, "It is widely accepted that 'modern science' arose in the Europe of the 17th century (towards the end of the Renaissance), introducing a new understanding of the natural world." In the 15th century, Portugal and Spain, two of the greatest naval powers of the time, took the lead in exploring the world. Christopher Columbus reached the New World in 1492 and Vasco da Gama opened the ocean route to the East in 1498, and soon after the Spanish and Portuguese began establishing colonial empires in the Americas and Asia. France, the Netherlands and England soon followed in building large colonial empires with vast holdings in Africa, the Americas, and Asia.
18th and 19th centuries.
The Age of Enlightenment was a powerful intellectual movement during the 18th century promoting scientific and reason-based thoughts. Discontent with the aristocracy and clergy's monopoly on political power in France resulted in the French Revolution and the establishment of the First Republic as a result of which the monarchy and many of the nobility perished during the initial reign of terror. Napoleon Bonaparte rose to power in the aftermath of the French Revolution and established the First French Empire that, during the Napoleonic Wars, grew to encompass large parts of Europe before collapsing in 1815 with the Battle of Waterloo. Napoleonic rule resulted in the further dissemination of the ideals of the French Revolution, including that of the nation-state, as well as the widespread adoption of the French models of administration, law, and education. The Congress of Vienna, convened after Napoleon's downfall, established a new balance of power in Europe centred on the five "Great Powers": the UK, France, Prussia, Austria, and Russia. This balance would remain in place until the Revolutions of 1848, during which liberal uprisings affected all of Europe except for Russia and the UK. These revolutions were eventually put down by conservative elements and few reforms resulted. The year 1859 saw the unification of Romania, as a nation-state, from smaller principalities. In 1867, the Austro-Hungarian empire was formed; and 1871 saw the unifications of both Italy and Germany as nation-states from smaller principalities.
In parallel, the Eastern Question grew more complex ever since the Ottoman defeat in the Russo-Turkish War (1768–1774). As the dissolution of the Ottoman Empire seemed imminent, the Great Powers struggled to safeguard their strategic and commercial interests in the Ottoman domains. The Russian Empire stood to benefit from the decline, whereas the Habsburg Empire and Britain perceived the preservation of the Ottoman Empire to be in their best interests. Meanwhile, the Serbian revolution and Greek War of Independence marked the birth of nationalism in the Balkans. Formal recognition of the "de facto" independent principalities of Montenegro, Serbia and Romania ensued at the Congress of Berlin in 1878.
The Industrial Revolution started in Great Britain in the last part of the 18th century and spread throughout Europe. The invention and implementation of new technologies resulted in rapid urban growth, mass employment, and the rise of a new working class. Reforms in social and economic spheres followed, including the first laws on child labour, the legalisation of trade unions, and the abolition of slavery. In Britain, the Public Health Act of 1875 was passed, which significantly improved living conditions in many British cities. Europe's population increased from about 100 million in 1700 to 400 million by 1900. The last major famine recorded in Western Europe, the Irish Potato Famine, caused death and mass emigration of millions of Irish people. In the 19th century, 70 million people left Europe in migrations to various European colonies abroad and to the United States.
20th century to the present.
Two World Wars and an economic depression dominated the first half of the 20th century. World War I was fought between 1914 and 1918. It started when Archduke Franz Ferdinand of Austria was assassinated by the Yugoslav nationalist Gavrilo Princip. Most European nations were drawn into the war, which was fought between the Entente Powers (France, Belgium, Serbia, Portugal, Russia, the United Kingdom, and later Italy, Greece, Romania, and the United States) and the Central Powers (Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire). The War left more than 16 million civilians and military dead. Over 60 million European soldiers were mobilised from 1914 to 1918.
Partly as a result of its defeat Russia was plunged into the Russian Revolution, which threw down the Tsarist monarchy and replaced it with the communist Soviet Union. Austria-Hungary and the Ottoman Empire collapsed and broke up into separate nations, and many other nations had their borders redrawn. The Treaty of Versailles, which officially ended World War I in 1919, was harsh towards Germany, upon whom it placed full responsibility for the war and imposed heavy sanctions.
Excess deaths in Russia over the course of World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million. In 1932–1933, under Stalin's leadership, confiscations of grain by the Soviet authorities contributed to the second Soviet famine which caused millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Stalin was also responsible for the Great Purge of 1937–38 in which the NKVD executed 681,692 people; millions of people were deported and exiled to remote areas of the Soviet Union.
Economic instability, caused in part by debts incurred in the First World War and 'loans' to Germany played havoc in Europe in the late 1920s and 1930s. This and the Wall Street Crash of 1929 brought about the worldwide Great Depression. Helped by the economic crisis, social instability and the threat of communism, fascist movements developed throughout Europe placing Adolf Hitler of Nazi Germany, Francisco Franco of Spain and Benito Mussolini of Italy in power.
In 1933, Hitler became the leader of Germany and began to work towards his goal of building Greater Germany. Germany re-expanded and took back the Saarland and Rhineland in 1935 and 1936. In 1938, Austria became a part of Germany following the Anschluss. Later that year, following the Munich Agreement signed by Germany, France, the United Kingdom and Italy, Germany annexed the Sudetenland, which was a part of Czechoslovakia inhabited by ethnic Germans, and in early 1939, the remainder of Czechoslovakia was split into the Protectorate of Bohemia and Moravia, controlled by Germany, and the Slovak Republic. At the time, Britain and France preferred a policy of appeasement.
With tensions mounting between Germany and Poland over the future of Danzig, the Germans turned to the Soviets, and signed the Molotov–Ribbentrop Pact, which allowed the Soviets to invade the Baltic states and parts of Poland and Romania. Germany invaded Poland on 1 September 1939, prompting France and the United Kingdom to declare war on Germany on 3 September, opening the European Theatre of World War II. The Soviet invasion of Poland started on 17 September and Poland fell soon thereafter. On 24 September, the Soviet Union attacked the Baltic countries and later, Finland. The British hoped to land at Narvik and send troops to aid Finland, but their primary objective in the landing was to encircle Germany and cut the Germans off from Scandinavian resources. Around the same time, Germany moved troops into Denmark. The Phoney War continued.
In May 1940, Germany attacked France through the Low Countries. France capitulated in June 1940. By August Germany began a bombing offensive on Britain, but failed to convince the Britons to give up. In 1941, Germany invaded the Soviet Union in the Operation Barbarossa. On 7 December 1941 Japan's attack on Pearl Harbor drew the United States into the conflict as allies of the British Empire and other allied forces.
After the staggering Battle of Stalingrad in 1943, the German offensive in the Soviet Union turned into a continual fallback. The Battle of Kursk, which involved the largest tank battle in history, was the last major German offensive on the Eastern Front. In 1944, British and American forces invaded France in the D-Day landings, opening a new front against Germany. Berlin finally fell in 1945, ending World War II in Europe. The war was the largest and most destructive in human history, with 60 million dead across the world. More than 40 million people in Europe had died as a result of World War II, including between 11 and 17 million people who perished during the Holocaust. The Soviet Union lost around 27 million people (mostly civilians) during the war, about half of all World War II casualties. By the end of World War II, Europe had more than 40 million refugees. Several post-war expulsions in Central and Eastern Europe displaced a total of about 20 million people.
World War I and especially World War II diminished the eminence of Western Europe in world affairs. After World War II the map of Europe was redrawn at the Yalta Conference and divided into two blocs, the Western countries and the communist Eastern bloc, separated by what was later called by Winston Churchill an "Iron Curtain". The United States and Western Europe
established the NATO alliance and later the Soviet Union and Central Europe established the Warsaw Pact.
The two new superpowers, the United States and the Soviet Union, became locked in a fifty-year long Cold War, centred on nuclear proliferation. At the same time decolonisation, which had already started after World War I, gradually resulted in the independence of most of the European colonies in Asia and Africa.
In the 1980s the reforms of Mikhail Gorbachev and the Solidarity movement in Poland accelerated the collapse of the Eastern bloc and the end of the Cold War. Germany was reunited, after the symbolic fall of the Berlin Wall in 1989, and the maps of Central and Eastern Europe were redrawn once more.
European integration also grew after World War II. The Treaty of Rome in 1957 established the European Economic Community between six Western European states with the goal of a unified economic policy and common market. In 1967 the EEC, European Coal and Steel Community and Euratom formed the European Community, which in 1993 became the European Union. The EU established a parliament, court and central bank and introduced the euro as a unified currency. In 2004 and 2007, more Central and Eastern European countries began joining, expanding the EU to its current size of 28 European countries, and once more making Europe a major economical and political centre of power.
Geography.
Europe is a peninsula that makes up the western fifth of the Eurasian landmass. It has a higher ratio of coast to landmass than any other continent or subcontinent. Its maritime borders are made up of the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean, Black, and Caspian Seas to the south
Land relief in Europe shows great variation within relatively small areas. The southern regions are more mountainous, while moving north the terrain descends from the high Alps, Pyrenees, and Carpathians, through hilly uplands, into broad, low northern plains, which are vast in the east. This extended lowland is known as the Great European Plain, and at its heart lies the North German Plain. An arc of uplands also exists along the north-western seaboard, which begins in the western parts of the islands of Britain and Ireland, and then continues along the mountainous, fjord-cut spine of Norway.
This description is simplified. Sub-regions such as the Iberian Peninsula and the Italian Peninsula contain their own complex features, as does mainland Central Europe itself, where the relief contains many plateaus, river valleys and basins that complicate the general trend. Sub-regions like Iceland, Britain, and Ireland are special cases. The former is a land unto itself in the northern ocean which is counted as part of Europe, while the latter are upland areas that were once joined to the mainland until rising sea levels cut them off.
Climate.
Europe lies mainly in the temperate climate zones, being subjected to prevailing westerlies.
The climate is milder in comparison to other areas of the same latitude around the globe due to the influence of the Gulf Stream. The Gulf Stream is nicknamed "Europe's central heating", because it makes Europe's climate warmer and wetter than it would otherwise be. The Gulf Stream not only carries warm water to Europe's coast but also warms up the prevailing westerly winds that blow across the continent from the Atlantic Ocean.
Therefore the average temperature throughout the year of Naples is 16 °C (60.8 °F), while it is only 12 °C (53.6 °F) in New York City which is almost on the same latitude. Berlin, Germany; Calgary, Canada; and Irkutsk, in the Asian part of Russia, lie on around the same latitude; January temperatures in Berlin average around 8 °C (15 °F) higher than those in Calgary, and they are almost 22 °C (40 °F) higher than average temperatures in Irkutsk.
Geology.
The Geology of Europe is hugely varied and complex, and gives rise to the wide variety of landscapes found across the continent, from the Scottish Highlands to the rolling plains of Hungary.
Europe's most significant feature is the dichotomy between highland and mountainous Southern Europe and a vast, partially underwater, northern plain ranging from Ireland in the west to the Ural Mountains in the east. These two halves are separated by the mountain chains of the Pyrenees and Alps/Carpathians. The northern plains are delimited in the west by the Scandinavian Mountains and the mountainous parts of the British Isles. Major shallow water bodies submerging parts of the northern plains are the Celtic Sea, the North Sea, the Baltic Sea complex and Barents Sea.
The northern plain contains the old geological continent of Baltica, and so may be regarded geologically as the "main continent", while peripheral highlands and mountainous regions in the south and west constitute fragments from various other geological continents. Most of the older geology of western Europe existed as part of the ancient microcontinent Avalonia.
Geological history.
The geological history of Europe traces back to the formation of the Baltic Shield (Fennoscandia) and the Sarmatian craton, both around 2.25 billion years ago, followed by the Volgo-Uralia shield, the three together leading to the East European craton (≈ Baltica) which became a part of the supercontinent Columbia. Around 1.1 billion years ago, Baltica and Arctica (as part of the Laurentia block) became joined to Rodinia, later resplitting around 550 million years ago to reform as Baltica. Around 440 million years ago Euramerica was formed from Baltica and Laurentia; a further joining with Gondwana then leading to the formation of Pangea. Around 190 million years ago, Gondwana and Laurasia split apart due to the widening of the Atlantic Ocean. Finally, and very soon afterwards, Laurasia itself split up again, into Laurentia (North America) and the Eurasian continent. The land connection between the two persisted for a considerable time, via Greenland, leading to interchange of animal species. From around 50 million years ago, rising and falling sea levels have determined the actual shape of Europe, and its connections with continents such as Asia. Europe's present shape dates to the late Tertiary period about five million years ago.
Biodiversity.
Having lived side-by-side with agricultural peoples for millennia, Europe's animals and plants have been profoundly affected by the presence and activities of man. With the exception of Fennoscandia and northern Russia, few areas of untouched wilderness are currently found in Europe, except for various national parks.
The main natural vegetation cover in Europe is mixed forest. The conditions for growth are very favourable. In the north, the Gulf Stream and North Atlantic Drift warm the continent. Southern Europe could be described as having a warm, but mild climate. There are frequent summer droughts in this region. Mountain ridges also affect the conditions. Some of these (Alps, Pyrenees) are oriented east-west and allow the wind to carry large masses of water from the ocean in the interior. Others are oriented south-north (Scandinavian Mountains, Dinarides, Carpathians, Apennines) and because the rain falls primarily on the side of mountains that is oriented towards the sea, forests grow well on this side, while on the other side, the conditions are much less favourable. Few corners of mainland Europe have not been grazed by livestock at some point in time, and the cutting down of the pre-agricultural forest habitat caused disruption to the original plant and animal ecosystems.
Probably 80 to 90 percent of Europe was once covered by forest. It stretched from the Mediterranean Sea to the Arctic Ocean. Though over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European forest dwelling species which require a mixture of tree species and diverse forest structure. The amount of natural forest in Western Europe is just 2–3% or less, in European Russia 5–10%. The country with the smallest percentage of forested area is Iceland (1%), while the most forested country is Finland (77%).
In temperate Europe, mixed forest with both broadleaf and coniferous trees dominate. The most important species in central and western Europe are beech and oak. In the north, the taiga is a mixed spruce–pine–birch forest; further north within Russia and extreme northern Scandinavia, the taiga gives way to tundra as the Arctic is approached. In the Mediterranean, many olive trees have been planted, which are very well adapted to its arid climate; Mediterranean Cypress is also widely planted in southern Europe. The semi-arid Mediterranean region hosts much scrub forest. A narrow east-west tongue of Eurasian grassland (the steppe) extends eastwards from Ukraine and southern Russia and ends in Hungary and traverses into taiga to the north.
Glaciation during the most recent ice age and the presence of man affected the distribution of European fauna. As for the animals, in many parts of Europe most large animals and top predator species have been hunted to extinction. The woolly mammoth was extinct before the end of the Neolithic period. Today wolves (carnivores) and bears (omnivores) are endangered. Once they were found in most parts of Europe. However, deforestation and hunting caused these animals to withdraw further and further. By the Middle Ages the bears' habitats were limited to more or less inaccessible mountains with sufficient forest cover. Today, the brown bear lives primarily in the Balkan peninsula, Scandinavia, and Russia; a small number also persist in other countries across Europe (Austria, Pyrenees etc.), but in these areas brown bear populations are fragmented and marginalised because of the destruction of their habitat. In addition, polar bears may be found on Svalbard, a Norwegian archipelago far north of Scandinavia. The wolf, the second largest predator in Europe after the brown bear, can be found primarily in Central and Eastern Europe and in the Balkans, with a handful of packs in pockets of Western Europe (Scandinavia, Spain, etc.).
European wild cat, foxes (especially the red fox), jackal and different species of martens, hedgehogs, different species of reptiles (like snakes such as vipers and grass snakes) and amphibians, different birds (owls, hawks and other birds of prey).
Important European herbivores are snails, larvae, fish, different birds, and mammals, like rodents, deer and roe deer, boars, and living in the mountains, marmots, steinbocks, chamois among others. A number of insects, such as the small tortoiseshell butterfly, add to the biodiversity.
The extinction of the dwarf hippos and dwarf elephants has been linked to the earliest arrival of humans on the islands of the Mediterranean.
Sea creatures are also an important part of European flora and fauna. The sea flora is mainly phytoplankton. Important animals that live in European seas are zooplankton, molluscs, echinoderms, different crustaceans, squids and octopuses, fish, dolphins, and whales.
Biodiversity is protected in Europe through the Council of Europe's Bern Convention, which has also been signed by the European Community as well as non-European states.
Political geography.
The list below includes all entities falling even partially under any of the various common definitions of Europe, geographic or political. The data displayed are per sources in cross-referenced articles.
Within the above-mentioned states are several de facto independent countries with limited to no international recognition. None of them are members of the UN:
Several dependencies and similar territories with broad autonomy are also found in Europe. Note that the list does not include the constituent countries of the United Kingdom, federal states of Germany and Austria, and autonomous territories of Spain and the post-Soviet republics as well as the republic of Serbia.
Integration.
European integration is the process of political, legal, economic (and in some cases social and cultural) integration of states wholly or partially in Europe. In the present day, European integration has primarily come about through the Council of Europe and European Union in Western and Central Europe and Commonwealth of Independent States in Central and Eastern Europe and most of former Soviet countries.
Economy.
As a continent, the economy of Europe is currently the largest on Earth and it is the richest region as measured by assets under management with over $32.7 trillion compared to North America's $27.1 trillion in 2008. In 2009 Europe remained the wealthiest region. Its $37.1 trillion in assets under management represented one-third of the world's wealth. It was one of several regions where wealth surpassed its precrisis year-end peak. As with other continents, Europe has a large variation of wealth among its countries. The richer states tend to be in the West; some of the Central and Eastern European economies are still emerging from the collapse of the Soviet Union and Yugoslavia.
The European Union, an intergovernmental body composed of 28 European states, comprises the largest single economic area in the world. 18 EU countries share the euro as a common currency.
Five European countries rank in the top ten of the world's largest national economies in GDP (PPP). This includes (ranks according to the CIA): Germany (5), the UK (6), Russia (7), France (8), and Italy (10).
There is huge disparity between many European countries in terms of their income. The richest in terms of GDP per capita is Monaco with its US$172,676 per capita (2009) and the poorest is Moldova with its GDP per capita of US$1,631 (2010). Monaco is the richest country in terms of GDP per capita in the world according to the World Bank report.
Pre–1945: Industrial growth.
Capitalism has been dominant in the Western world since the end of feudalism. From Britain, it gradually spread throughout Europe. The Industrial Revolution started in Europe, specifically the United Kingdom in the late 18th century, and the 19th century saw Western Europe industrialise. Economies were disrupted by World War I but by the beginning of World War II they had recovered and were having to compete with the growing economic strength of the United States. World War II, again, damaged much of Europe's industries.
1945–1990: The Cold War.
After World War II the economy of the UK was in a state of ruin, and continued to suffer relative economic decline in the following decades. Italy was also in a poor economic condition but regained a high level of growth by the 1950s. West Germany recovered quickly and had doubled production from pre-war levels by the 1950s. France also staged a remarkable comeback enjoying rapid growth and modernisation; later on Spain, under the leadership of Franco, also recovered, and the nation recorded huge unprecedented economic growth beginning in the 1960s in what is called the Spanish miracle. The majority of Central and Eastern European states came under the control of the Soviet Union and thus were members of the Council for Mutual Economic Assistance (COMECON).
The states which retained a free-market system were given a large amount of aid by the United States under the Marshall Plan. The western states moved to link their economies together, providing the basis for the EU and increasing cross border trade. This helped them to enjoy rapidly improving economies, while those states in COMECON were struggling in a large part due to the cost of the Cold War. Until 1990, the European Community was expanded from 6 founding members to 12. The emphasis placed on resurrecting the West German economy led to it overtaking the UK as Europe's largest economy.
1991–2007: Integration and reunification.
With the fall of communism in Central and Eastern Europe in 1991, the post-socialist states began free market reforms: Poland, Hungary, and Slovenia adopted them reasonably quickly, while Ukraine and Russia are still in the process of doing so.
After East and West Germany were reunited in 1990, the economy of West Germany struggled as it had to support and largely rebuild the infrastructure of East Germany.
By the millennium change, the EU dominated the economy of Europe comprising the five largest European economies of the time namely Germany, the United Kingdom, France, Italy, and Spain. In 1999, 12 of the 15 members of the EU joined the Eurozone replacing their former national currencies by the common euro. The three who chose to remain outside the Eurozone were: the United Kingdom, Denmark, and Sweden.
The European Union is now the largest economy in the world.
2008–2010: Recession.
Figures released by Eurostat in January 2009 confirmed that the Eurozone had gone into recession in the third quarter of 2008. It impacted much of the region. In early 2010, fears of a sovereign debt crisis developed concerning some countries in Europe, especially Greece, Ireland, Spain, and Portugal. As a result, measures were taken, especially for Greece, by the leading countries of the Eurozone.
The EU-27 unemployment rate was 10.3% in April 2012. Recent university graduates have been unable to find work. In April 2012, the unemployment rate in the EU27 for those aged 15–24 was 22.4%.
Demographics.
Since the Renaissance, Europe has had a major influence in culture, economics and social movements in the world. The most significant inventions had their origins in the Western world, primarily Europe and the United States. Approximately 70 million Europeans died through war, violence and famine between 1914 and 1945. Some current and past issues in European demographics have included religious emigration, race relations, economic immigration, a declining birth rate and an ageing population.
In some countries, such as Ireland and Poland, access to abortion is limited. It remains illegal on the island of Malta. Furthermore, three European countries (the Netherlands, Belgium, and Switzerland) and the Autonomous Community of Andalusia (Spain) have allowed a limited form of voluntary euthanasia for some terminally ill people.
In 2005, the population of Europe was estimated to be 731 million according to the United Nations, which is slightly more than one-ninth of the world's population. A century ago, Europe had nearly a quarter of the world's population. The population of Europe has grown in the past century, but in other areas of the world (in particular Africa and Asia) the population has grown far more quickly. Among the continents, Europe has a relatively high population density, second only to Asia. The most densely populated country in Europe (and in the world) is Monaco. Pan and Pfeil (2004) count 87 distinct "peoples of Europe", of which 33 form the majority population in at least one sovereign state, while the remaining 54 constitute ethnic minorities.
According to UN population projection, Europe's population may fall to about 7% of world population by 2050, or 653 million people (medium variant, 556 to 777 million in low and high variants, respectively). Within this context, significant disparities exist between regions in relation to fertility rates. The average number of children per female of child bearing age is 1.52. According to some sources, this rate is higher among Muslims in Europe. The UN predicts a steady population decline in Central and Eastern Europe as a result of emigration and low birth rates.
Europe is home to the highest number of migrants of all global regions at 70.6 million people, the IOM's report said. In 2005, the EU had an overall net gain from immigration of 1.8 million people. This accounted for almost 85% of Europe's total population growth. The European Union plans to open the job centres for legal migrant workers from Africa. In 2008, 696,000 persons were given citizenship of an EU27 member state, a decrease from 707,000 the previous year.
Emigration from Europe began with Spanish and Portuguese settlers in the 16th century, and French and English settlers in the 17th century. But numbers remained relatively small until waves of mass emigration in the 19th century, when millions of poor families left Europe.
Today, large populations of European descent are found on every continent. European ancestry predominates in North America, and to a lesser degree in South America (particularly in Uruguay, Argentina, Chile and Brazil, while most of the other Latin American countries also have a considerable population of European origins). Australia and New Zealand have large European derived populations. Africa has no countries with European-derived majorities (or with the exception of Cape Verde and probably São Tomé and Príncipe, depending on context), but there are significant minorities, such as the White South Africans. In Asia, European-derived populations predominate in Northern Asia (specifically Russians), some parts of Northern Kazakhstan and Israel. Additionally, transcontinental or geographically Asian countries such as Georgia, Armenia, Azerbaijan, Cyprus and Turkey have populations historically closely related to Europeans, with considerable genetic and cultural affinity.
Language.
European languages mostly fall within three Indo-European language groups: the Romance languages, derived from the Latin of the Roman Empire; the Germanic languages, whose ancestor language came from southern Scandinavia; and the Slavic languages.
Slavic languages are most spoken by the number of native speakers in Europe, they are spoken in Central, Eastern, and Southeastern Europe. Romance languages are spoken primarily in south-western Europe as well as in Romania and Moldova, in Central or Eastern Europe. Germanic languages are spoken in Northern Europe, the British Isles and some parts of Central Europe.
Many other languages outside the three main groups exist in Europe. Other Indo-European languages include the Baltic group (that is, Latvian and Lithuanian), the Celtic group (that is, Irish, Scottish Gaelic, Manx, Welsh, Cornish, and Breton), Greek, Armenian, and Albanian. In addition, a distinct group of Uralic languages (Estonian, Finnish, and Hungarian) is spoken mainly in Estonia, Finland, and Hungary, while Kartvelian languages (Georgian, Mingrelian, and Svan), are spoken primarily in Georgia, and two other language families reside in the North Caucasus (termed Northeast Caucasian, most notably including Chechen, Avar and Lezgin and Northwest Caucasian, notably including Adyghe). Maltese is the only Semitic language that is official within the EU, while Basque is the only European language isolate. Turkic languages include Azerbaijani and Turkish, in addition to the languages of minority nations in Russia.
Multilingualism and the protection of regional and minority languages are recognised political goals in Europe today. The Council of Europe Framework Convention for the Protection of National Minorities and the Council of Europe's European Charter for Regional or Minority Languages set up a legal framework for language rights in Europe.
Religion.
Historically, religion in Europe has been a major influence on European art, culture, philosophy and law. The largest religion in Europe is Christianity, with 76.2% of Europeans considering themselves Christians, including Catholic, Eastern Orthodox and Protestant Churches. Following these is Islam concentrated mainly in the Balkans and eastern Europe (Bosnia and Herzegovina, Albania, Kosovo, Kazakhstan, North Cyprus, Turkey, Azerbaijan, North Caucasus, and the Volga-Ural region). Other religions, including Judaism, Hinduism, and Buddhism are minority religions (though Tibetan Buddhism is the majority religion of Russia's Republic of Kalmykia). The 20th century saw the revival of Neopaganism through movements such as Wicca and Druidry.
Europe has become a relatively secular continent, with an increasing number and proportion of irreligious, atheist and agnostic people, actually the largest in the Western world. There are a particularly high number of self-described non-religious people in the Czech Republic, Estonia, Sweden, Germany (East), and France.
Culture.
The culture of Europe can be described as a series of overlapping cultures; cultural mixes exist across the continent. There are cultural innovations and movements, sometimes at odds with each other. Thus the question of "common culture" or "common values" is complex.
According to historian Hilaire Belloc, for several centuries the peoples of Europe based their self-identification on the remaining traces of the Roman culture and on the concept of Christendom, because many European-wide military alliances were of religious nature: the Crusades (1095–1291), the Reconquista (711-1492), the Battle of Lepanto (1571).

</doc>
<doc id="9240" url="http://en.wikipedia.org/wiki?curid=9240" title="Europa">
Europa

Europa commonly refers to:
Europa may also refer to:

</doc>
<doc id="9241" url="http://en.wikipedia.org/wiki?curid=9241" title="Euglenozoa">
Euglenozoa

The Euglenozoa are a large group of flagellate protozoa. They include a variety of common free-living species, as well as a few important parasites, some of which infect humans. There are two main subgroups, the euglenids and kinetoplastids. Euglenozoa are unicellular, mostly around 15-40 µm in size, although some euglenids get up to 500 µm long. 
Structure.
The two main sub-groups of micro-organisms classified to be Excavata-Euglenozoa as listed above are incorrect. The two correct sub-groups as should be are: euglenoids and kinetoplasmids. Take note that euglenoids and euglenids are different. Most euglenozoa have two flagella, which are inserted parallel to one another in an apical or subapical pocket. In some these are associated with a cytostome or mouth, used to ingest bacteria or other small organisms. This is supported by one of three sets of microtubules that arise from the flagellar bases; the other two support the dorsal and ventral surfaces of the cell.
Some other euglenozoa feed through absorption, and many euglenids possess chloroplasts and so obtain energy through photosynthesis. These chloroplasts are surrounded by three membranes and contain chlorophylls "A" and "B", along with other pigments, so are probably derived from a captured green alga. Reproduction occurs exclusively through cell division. During mitosis, the nuclear membrane remains intact, and the spindle microtubules form inside of it.
The group is characterized by the ultrastructure of the flagella. In addition to the normal supporting microtubules or axoneme, each contains a rod (called "paraxonemal"), which has a tubular structure in one flagellum and a latticed structure in the other. Based on this, two smaller groups have been included here: the diplonemids and "Postgaardi".
Classification.
The euglenozoa are generally accepted as monophyletic. They are related to Percolozoa; the two share mitochondria with disk-shaped cristae, which only occurs in a few other groups.
Both probably belong to a larger group of eukaryotes called the excavates. This grouping, though, has been challenged.

</doc>
<doc id="9247" url="http://en.wikipedia.org/wiki?curid=9247" title="Epistemology">
Epistemology

Epistemology ("ἐπιστήμη",episteme|knowledge, understanding|| "λόγος", logos|study of) is the branch of philosophy concerned with the nature and scope of knowledge and is also referred to as "theory of knowledge". It questions what knowledge is and how it can be acquired, and the extent to which knowledge pertinent to any given subject or entity can be acquired. Much of the debate in this field has focused on the philosophical analysis of the nature of knowledge and how it relates to connected notions such as truth, belief, and justification. The term "epistemology" was introduced by the Scottish philosopher James Frederick Ferrier (1808–1864).
Background and meaning.
Epistemology, derived from the Greek "epistēmē" meaning "knowledge" and "logos" meaning "study of". It translates the German concept Wissenschaftslehre, which was used by Fichte and Bolzano for different projects before it was taken up again by Husserl. J.F. Ferrier coined the word on the model of 'ontology', to designate that branch of philosophy – afﬁrmed to be the latter's 'true beginning' – to discover the meaning of knowledge. The term passed into French as épistémologie, with, however, a generally narrower meaning than the original, the import of which is covered by 'theory of knowledge [théorie de la connaissance].' Thus Émile Meyerson opened his "Identity and Reality", written in 1908, with the remark that the word 'is becoming current' as equivalent to 'the philosophy of the sciences [philosophie des sciences].'
Knowledge.
Knowledge that, knowledge how, and knowledge by acquaintance.
In epistemology in general, the kind of knowledge usually discussed is propositional knowledge, also known as "knowledge that." This is distinguished from "knowledge how" and "acquaintance-knowledge". For example: in mathematics, it is known "that" 2 + 2 = 4, but there is also knowing "how" to add two numbers and knowing a "person" (e.g., oneself), "place" (e.g., one's hometown), "thing" (e.g., cars), or "activity" (e.g., addition). Some philosophers think there is an important distinction between "knowing that," "knowing how," and "acquaintance-knowledge," with epistemology being primarily concerned with the first of these.
In his paper "On Denoting" and his later book "Problems of Philosophy" Bertrand Russell stressed the distinction between "knowledge by description" and "knowledge by acquaintance". Gilbert Ryle is also credited with stressing the distinction between knowing how and knowing that in "The Concept of Mind." In "Personal Knowledge," Michael Polanyi argues for the epistemological relevance of knowledge how and knowledge that; using the example of the act of balance involved in riding a bicycle, he suggests that the theoretical knowledge of the physics involved in maintaining a state of balance cannot substitute for the practical knowledge of how to ride, and that it is important to understand how both are established and grounded. This position is essentially Ryle's, who argued that a failure to acknowledge the distinction between knowledge that and knowledge how leads to infinite regress.
In recent times, some epistemologists (Sosa, Greco, Kvanvig, Zagzebski) and Duncan Pritchard have argued that epistemology should evaluate people's "properties" (i.e., intellectual virtues) and not just the properties of propositions or of propositional mental attitudes.
Belief.
In common speech, a "statement of belief" is typically an expression of faith and/or trust in a person, power or other entity — a paradigmatic example of such a statement of belief would be a declaration or affirmation of religious faith (as in, e.g., the Nicene Creed). While it addresses belief of "this" kind, epistemology is also concerned with belief in a very much broader sense of the word. In this broader sense "belief" simply means the acceptance as true of any cognitive content. To believe is to accept as true.
Truth.
Whether someone's belief is true is not a prerequisite for (its) belief. On the other hand, if something is actually "known", then it categorically cannot be false. For example, if a person believes that a bridge is safe enough to support him, and attempts to cross it, but the bridge then collapses under his weight, it could be said that he "believed" that the bridge was safe but that his belief was mistaken. It would "not" be accurate to say that he "knew" that the bridge was safe, because plainly it was not. By contrast, if the bridge actually supported his weight, then he might say that he had believed that the bridge was safe, whereas now, after proving it to himself (by crossing it), he "knows" it was safe.
Epistemologists argue over whether belief is the proper truth-bearer. Some would rather describe knowledge as a system of justified true propositions, and others as a system of justified true sentences. Plato, in his Gorgias, argues that belief is the most commonly invoked truth-bearer.
Justification.
In many of Plato's dialogues, such as the "Meno" and, in particular, the "Theaetetus", Socrates considers a number of theories as to what knowledge is, the last being that knowledge is true belief that has been "given an account of" (meaning explained or defined in some way). According to the theory that knowledge is justified true belief, in order to know that a given proposition is true, one must not only believe the relevant true proposition, but one must also have a good reason for doing so. One implication of this would be that no one would gain knowledge just by believing something that happened to be true. For example, an ill person with no medical training, but with a generally optimistic attitude, might believe that he will recover from his illness quickly. Nevertheless, even if this belief turned out to be true, the patient would not have "known" that he would get well since his belief lacked justification.
The definition of knowledge as justified true belief was widely accepted until the 1960s. At this time, a paper written by the American philosopher Edmund Gettier provoked major widespread discussion. (See theories of justification for other views on the idea.)
The Gettier problem.
Edmund Gettier is best known for a short paper entitled 'Is Justified True Belief Knowledge?' published in 1963, which called into question the theory of knowledge that had been dominant among philosophers for thousands of years. In a few pages, Gettier argued that there are situations in which one's belief may be justified and true, yet fail to count as knowledge. That is, Gettier contended that while justified belief in a true proposition is necessary for that proposition to be known, it is not sufficient. As in the diagram, a true proposition can be believed by an individual (purple region) but still not fall within the "knowledge" category (yellow region).
According to Gettier, there are certain circumstances in which one does not have knowledge, even when all of the above conditions are met. Gettier proposed two thought experiments, which have come to be known as "Gettier cases," as counterexamples to the classical account of knowledge. One of the cases involves two men, Smith and Jones, who are awaiting the results of their applications for the same job. Each man has ten coins in his pocket. Smith has excellent reasons to believe that Jones will get the job and, furthermore, knows that Jones has ten coins in his pocket (he recently counted them). From this Smith infers, "the man who will get the job has ten coins in his pocket." However, Smith is unaware that he also has ten coins in his own pocket. Furthermore, Smith, not Jones, is going to get the job. While Smith has strong evidence to believe that Jones will get the job, he is wrong. Smith has a justified true belief that a man with ten coins in his pocket will get the job; however, according to Gettier, Smith does not "know" that a man with ten coins in his pocket will get the job, because Smith's belief is "...true by virtue of the number of coins in "Jones's" pocket, while Smith does not know how many coins are in Smith's pocket, and bases his belief...on a count of the coins in Jones's pocket, whom he falsely believes to be the man who will get the job." (see p. 122.) These cases fail to be knowledge because the subject's belief is justified, but only happens to be true by virtue of luck. In other words, he made the correct choice (in this case predicting an outcome) for the wrong reasons. This example is similar to those often given when discussing belief and truth, wherein a person's belief of what will happen can coincidentally be correct without his or her having the actual knowledge to base it on.
Responses to Gettier.
The responses to Gettier have been varied. Usually, they have involved substantial attempts to provide a definition of knowledge different from the classical one, either by recasting knowledge as justified true belief with some additional fourth condition, or as something else altogether.
Infallibilism, indefeasibility.
In one response to Gettier, the American philosopher Richard Kirkham has argued that the only definition of knowledge that could ever be immune to all counterexamples is the infallibilist one. To qualify as an item of knowledge, goes the theory, a belief must not only be true and justified, the justification of the belief must "necessitate" its truth. In other words, the justification for the belief must be infallible.
Yet another possible candidate for the fourth condition of knowledge is "indefeasibility." Defeasibility theory maintains that there should be no overriding or defeating truths for the reasons that justify one's belief. For example, suppose that person "S" believes he saw Tom Grabit steal a book from the library and uses this to justify the claim that Tom Grabit stole a book from the library. A possible defeater or overriding proposition for such a claim could be a true proposition like, "Tom Grabit's identical twin Sam is currently in the same town as Tom." When no defeaters of one's justification exist, a subject would be epistemically justified.
The Indian philosopher B K Matilal has drawn on the Navya-Nyāya fallibilism tradition to respond to the Gettier problem. Nyaya theory distinguishes between "know p" and "know that one knows p" – these are different events, with different causal conditions. The second level is a sort of implicit inference that usually follows immediately the episode of knowing p (knowledge "simpliciter"). The Gettier case is examined by referring to a view of Gangesha Upadhyaya (late 12th century), who takes any true belief to be knowledge; thus a true belief acquired through a wrong route may just be regarded as knowledge simpliciter on this view. The question of justification arises only at the second level, when one considers the knowledgehood of the acquired belief. Initially, there is lack of uncertainty, so it becomes a true belief. But at the very next moment, when the hearer is about to embark upon the venture of "knowing whether he knows p", doubts may arise. "If, in
some Gettier-like cases, I am wrong in my inference about the knowledgehood of the given occurrent belief (for the evidence may be pseudo-evidence), then I am mistaken about the truth of my belief – and this is in accordance with Nyaya fallibilism: not all knowledge-claims can be sustained."
Reliabilism.
Reliabilism has been a significant line of response to the Gettier problem among philosophers, originating with work by Alvin Goldman in the 1960s. According to reliabilism, a belief is justified (or otherwise supported in such a way as to count towards knowledge) only if it is produced by processes that typically yield a sufficiently high ratio of true to false beliefs. In other words, this theory states that a true belief counts as knowledge only if it is produced by a reliable belief-forming process.
Reliabilism has been challenged by Gettier cases. Another argument that challenges reliabilism, like the Gettier cases (although it was not presented in the same short article as the Gettier cases), is the case of Henry and the barn façades. In the thought experiment, a man, Henry, is driving along and sees a number of buildings that resemble barns. Based on his perception of one of these, he concludes that he has just seen barns. While he has seen one, and the perception he based his belief that the one he saw was of a real barn, all the other barn-like buildings he saw were façades. Theoretically, Henry does not know that he has seen a barn, despite both his belief that he has seen one being true and his belief being formed on the basis of a reliable process (i.e. his vision), since he only acquired his true belief by accident.
Other responses.
Robert Nozick has offered the following definition of knowledge:
"S" knows that "P" if and only if:
Nozick argues that the third of these conditions serves to address cases of the sort described by Gettier. Nozick further claims this condition addresses a case of the sort described by D. M. Armstrong: A father believes his daughter innocent of committing a particular crime, both because of faith in his baby girl and (now) because he has seen presented in the courtroom a conclusive demonstration of his daughter's innocence. His belief via the method of the courtroom satisfies the four subjunctive conditions, but his faith-based belief does not. If his daughter were guilty, he would still believe her innocent, on the basis of faith in his daughter; this would violate the third condition.
The British philosopher Simon Blackburn has criticized this formulation by suggesting that we do not want to accept as knowledge beliefs, which, while they "track the truth" (as Nozick's account requires), are not held for appropriate reasons. He says that "we do not want to award the title of knowing something to someone who is only meeting the conditions through a defect, flaw, or failure, compared with someone else who is not meeting the conditions." In addition to this, externalist accounts of knowledge, such as Nozick's, are often forced to reject closure in cases where it is intuitively valid.
Timothy Williamson has advanced a theory of knowledge according to which knowledge is not justified true belief plus some extra condition(s). In his book "Knowledge and its Limits," Williamson argues that the concept of knowledge cannot be broken down into a set of other concepts through analysis—instead, it is "sui generis." Thus, though knowledge requires justification, truth, and belief, the word "knowledge" can't be, according to Williamson's theory, accurately regarded as simply shorthand for "justified true belief."
Alvin Goldman writes in his Causal Theory of Knowing that in order for knowledge to truly exist there must be a causal chain between the proposition and the belief of that proposition.
Externalism and internalism.
Part of the debate over the nature of knowledge is a debate between epistemological externalists on the one hand, and epistemological internalists on the other.
Externalists hold that factors deemed "external", meaning outside of the psychological states of those who gain knowledge, can be conditions of knowledge. For example, an externalist response to the Gettier problem is to say that, in order for a justified true belief to count as knowledge, there must be a link or dependency between the belief and the state of the external world. Usually this is understood to be a causal link. Such causation, to the extent that it is "outside" the mind, would count as an external, knowledge-yielding condition. Internalists, on the other hand, assert that all knowledge-yielding conditions are within the psychological states of those who gain knowledge.
Though unfamiliar with the internalist/externalist debate himself, many point to René Descartes as an early example of the internalist path to justification. He wrote that, because the only method by which we perceive the external world is through our senses, and that, because the senses are not infallible, we should not consider our concept of knowledge to be infallible. The only way to find anything that could be described as "indubitably true," he advocates, would be to see things "clearly and distinctly". He argued that if there is an omnipotent, good being who made the world, then it's reasonable to believe that people are made with the ability to know. However, this does not mean that man's ability to know is perfect. God gave man the ability to know, but not omniscience. Descartes said that man must use his capacities for knowledge correctly and carefully through methodological doubt. The dictum "Cogito ergo sum" (I think, therefore I am) is also commonly associated with Descartes' theory, because in his own methodological doubt, doubting everything he previously knew in order to start from a blank slate, the first thing that he could not logically bring himself to doubt was his own existence: "I do not exist" would be a contradiction in terms; the act of saying that one does not exist assumes that someone must be making the statement in the first place. Though Descartes could doubt his senses, his body and the world around him, he could not deny his own existence, because he was able to doubt and must exist in order to do so. Even if some "evil genius" were to be deceiving him, he would have to exist in order to be deceived. This one sure point provided him with what he would call his Archimedean point, in order to further develop his foundation for knowledge. Simply put, Descartes' epistemological justification depended upon his indubitable belief in his own existence and his clear and distinct knowledge of God.
The Value problem.
A formulation of the value problem in epistemology first occurs in Plato's Meno. The problem is to identify what is it about knowledge (if anything) that makes it more valuable than mere true belief, or that makes knowledge more valuable than a more minimal conjunction of its components on a particular analysis of knowledge. The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link with the concept of value in ethics.
The value problem has been presented as an argument against epistemic reliabilism by philosophers including Linda Zagzebski, Wayne Riggs and Richard Swinburne. Zagzebski gives a thought experiment to illustrate the unimportance of the belief being produced by a reliable process: imagine you go to a coffee machine and attempt to have it produce you a cup of coffee. The machine you use might reliably produce coffee, or it might not. Imagine one machine had a 90% chance of producing you coffee while another only had a 40% chance. If you happen to choose the 40% chance machine and it produces you a cup of coffee, the fact that it does not "reliably" produce coffee does not change the value that the coffee has to you. Similarly, if you have a true belief achieved through an unreliable process, Zagzebski argues that there's no particular reason that has "less" value than one produced through a reliable process. Advocates of virtue epistemology have argued that the value of knowledge comes from an internal relationship between the knower and the mental state of believing.
One of the more influential responses to the problem is that knowledge is not particularly valuable and is not what ought to be the main focus of epistemology. Instead, epistemologists ought to focus on other mental states, such as understanding.
Acquiring knowledge.
"A priori" and "a posteriori" knowledge.
The nature of this distinction has been disputed by various philosophers; however, the terms may be roughly defined as follows:
Evolutionary psychology takes a novel approach to the problem. It says that there is an innate predisposition for certain types of learning. "Only small parts of the brain resemble a tabula rasa; this is true even for human beings. The remainder is more like an exposed negative waiting to be dipped into a developer fluid"
Analytic–synthetic distinction.
Immanuel Kant, in his "Critique of Pure Reason", drew a distinction between "analytic" and "synthetic" propositions. He contended that some propositions are such that we can know them to be true just by understanding their meaning. For example, consider, "My father's brother is my uncle." We can know it to be true solely by virtue of our understanding what its terms mean. Philosophers call such propositions "analytic." Synthetic propositions, on the other hand, have distinct subjects and predicates. An example of a synthetic proposition would be, "My father's brother has black hair." Kant stated that all mathematical and scientific statements are synthetic a priori propositions because they are necessarily true but our knowledge about the attributes of the mathematical or physical subjects we can only get by logical inference.
The American philosopher W. V. O. Quine, in his "Two Dogmas of Empiricism", famously challenged the distinction, arguing that the two have a blurry boundary. Some contemporary philosophers have offered more sustainable accounts of the distinction.
Branches or 'tendencies' within epistemology.
Historical.
The historical study of philosophical epistemology is the historical study of efforts to gain philosophical understanding or knowledge of the nature and scope of human knowledge. Since efforts to get that kind of understanding have a history, the questions philosophical epistemology asks today about human knowledge are not necessarily the same as they once were. But that does not mean that philosophical epistemology is itself a historical subject, or that it pursues only or even primarily historical understanding.
Empiricism.
In philosophy, empiricism is generally a theory of knowledge focusing on the role of experience, especially experience based on perceptual observations by the senses. Certain forms treat all knowledge as empirical, while some regard disciplines such as mathematics and logic as exceptions.
There are many variants of empiricism, positivism and realism being among the most commonly expounded but central to all empiricist epistemologies is the notion of the epistemologically privileged status of sense data.
Idealism.
Many idealists believe that knowledge is primarily (at least in some areas) acquired by "a priori" processes or is innate—for example, in the form of concepts not derived from experience. The relevant theoretical processes often go by the name "intuition". The relevant theoretical concepts may purportedly be part of the structure of the human mind (as in Kant's theory of transcendental idealism), or they may be said to exist independently of the mind (as in Plato's theory of Forms).
Rationalism.
By contrast with empiricism and idealism, which centres around the epistemologically privileged status of sense data (empirical) and the primacy of Reason (theoretical) respectively, modern rationalism adds a third 'system of thinking', (as Gaston Bachelard has termed these areas) and holds that all three are of equal importance: The empirical, the theoretical and the "abstract". For Bachelard, rationalism makes equal reference to all three systems of thinking.
Constructivism.
Constructivism is a view in philosophy according to which all "knowledge is a compilation of human-made constructions", "not the neutral discovery of an objective truth". Whereas objectivism is concerned with the "object of our knowledge", constructivism emphasises "how we construct knowledge". Constructivism proposes new definitions for knowledge and truth that form a new paradigm, based on inter-subjectivity instead of the classical objectivity, and on viability instead of truth. Piagetian constructivism, however, believes in objectivity—constructs can be validated through experimentation. The constructivist point of view is pragmatic; as Vico said: "The norm of the truth is to have made it."
The regress problem.
"... to justify a belief one must appeal to a further justified belief. This means that one of two things can be the case. Either there are some [epistemologically basic] beliefs that we can be justified for holding, without being able to justify them on the basis of any other belief, or else for each justified belief there is an infinite regress of (potential) justification [the nebula theory]. On this theory there is no rock bottom of justification. Justification just meanders in and out through our network of beliefs, stopping nowhere." The apparent impossibility of completing an infinite chain of reasoning is thought by some to support skepticism. Socrates said, "The only true wisdom is in knowing you know nothing."
Response to the regress problem.
Many epistemologists studying justification have attempted to argue for various types of chains of reasoning that can escape the regress problem.
Infinitism.
It is not impossible for an infinite justificatory series to exist. This position is known as "infinitism". Infinitists typically take the infinite series to be merely potential, in the sense that an individual may have indefinitely many reasons available to him, without having consciously thought through all of these reasons when the need arises. This position is motivated in part by the desire to avoid what is seen as the arbitrariness and circularity of its chief competitors, foundationalism and coherentism. In mathematics, an infinite series will sometimes converge – (this is the basis of calculus) – one can therefore have an infinite series of logical arguments and analyze it for a convergent (or non-convergent) solution.
Foundationalism.
Foundationalists respond to the regress problem by asserting that certain "foundations" or "basic beliefs" support other beliefs but do not themselves require justification from other beliefs. These beliefs might be justified because they are self-evident, infallible, or derive from reliable cognitive mechanisms. Perception, memory, and a priori intuition are often considered to be possible examples of basic beliefs.
The chief criticism of foundationalism is that if a belief is not supported by other beliefs, accepting it may be arbitrary or unjustified, though foundationalism is based upon the principle that these beliefs are infallible enough to be recognised as such in practice.
Coherentism.
Another response to the regress problem is coherentism, which is the rejection of the assumption that the regress proceeds according to a pattern of linear justification. To avoid the charge of circularity, coherentists hold that an individual belief is justified circularly by the way it fits together (coheres) with the rest of the belief system of which it is a part. This theory has the advantage of avoiding the infinite regress without claiming special, possibly arbitrary status for some particular class of beliefs. Yet, since a system can be coherent while also being wrong, coherentists face the difficulty of ensuring that the whole system corresponds to reality. Additionally, most logicians agree that any argument that is circular is trivially valid. That is, to be illuminating, arguments must be linear with conclusions that follow from stated premises.
However, Warburton writes in 'Thinking from A to Z,' "Circular arguments are not invalid; in other words, from a logical point of view there is nothing intrinsically wrong with them. However, they are, when viciously circular, spectacularly uninformative.(Warburton 1996)."
Foundherentism.
A position known as "foundherentism", advanced by Susan Haack, is meant to be a unification of foundationalism and coherentism. One component of this theory is what is called the "analogy of the crossword puzzle." Whereas, for example, infinitists regard the regress of reasons as "shaped" like a single line, Susan Haack has argued that it is more like a crossword puzzle, with multiple lines mutually supporting each other.
What do people know?
At the heart of the question is skepticism, with many approaches involved trying to disprove some particular form of it.
Skepticism.
Skepticism is related to the question of whether a certain knowledge is possible. If point B cannot be proven before point A, and if in order to prove point A it must be established with absolute certainty, then skepticism argues that it is difficult to prove any point at all. Skeptics argue that the belief in something does not necessarily justify an assertion of knowledge of it. In this skeptics oppose foundationalism, which states that there have to be some basic beliefs that are justified without reference to others. The skeptical response to this can take several approaches. First, claiming that "basic beliefs" must exist, amounts to the logical fallacy of argument from ignorance combined with the slippery slope. While a foundationalist would use Münchhausen trilemma as a justification for demanding the validity of basic beliefs, a skeptic would see no problem with admitting the result.
Developments from skepticism.
Early in the 20th century, the notion that belief had to be justified as such to count as knowledge lost favour. Fallibilism is the view that knowing something does not entail certainty regarding it. Charles Sanders Peirce was a fallibilist and the most developed form of fallibilism can be traced to Karl Popper (1902–1994) whose first book "Logik Der Forschung" ("The Logic of Scientific Discovery"), 1934 introduced a "conjectural turn" into the philosophy of science and epistemology at large. He adumbrated a school of thought that is known as Critical Rationalism with a central tenet being the rejection of the idea that knowledge can ever be justified in the strong form that is sought by most schools of thought. His two most helpful exponents are the late William W Bartley and David Miller, recently retired from the University of Warwick. 
Epistemic culture.
Epistemic culture distinguishes between various settings of knowledge production and stresses their contextual aspects. Coined by Karin Knorr-Cetina in her book "Epistemic Cultures"; she defines epistemic cultures as an "amalgam of arrangements and mechanisms—bonded through affinity, necessity and historical coincidence—which in a given field, make up how we know what we know". The term provides the conceptual framework used to demonstrate that different laboratories do not share the same "scientific" knowledge production model, but rather each is endowed with a different epistemic culture prescribing what is adequate knowledge and how it is obtained. Since its introduction, the term has been picked up and used by various researchers engaging in science and technology studies.
Practical applications.
Epistemology is particularly commonly employed in issues of law where proof of guilt or innocence may be required, or when it must be determined whether a person knew a particular fact before taking a specific action (e.g., whether an action was premeditated). Another practical application is to the design of user interfaces. For example, the skills, rules, and knowledge taxonomy of human behavior has been used by designers to develop systems that are compatible with multiple "ways of knowing": abstract analytic reasoning, experience-based 'gut feelings', and 'craft' sensorimotor skills.
Other common applications of epistemology include:
External links.
"Stanford Encyclopedia of Philosophy" articles:
Other links:

</doc>
<doc id="9248" url="http://en.wikipedia.org/wiki?curid=9248" title="Esperanto">
Esperanto

Esperanto ( or ; ) is a constructed international auxiliary language. It is the most widely spoken constructed language in the world. Its name derives from "Doktoro Esperanto" ("Esperanto" translates as "one who hopes"), the pseudonym under which L. L. Zamenhof published the first book detailing Esperanto, the "Unua Libro," on July 26, 1887. Zamenhof's goal was to create an easy-to-learn, politically neutral language that would transcend nationality and foster peace and international understanding between people with different languages.
Between 100,000 and 2,000,000 people worldwide fluently or actively speak Esperanto, including perhaps 1,000 native speakers who learned Esperanto from birth. Esperanto has a notable presence in 112 countries. Its usage is highest in Europe, East Asia, and South America.
The first World Congress of Esperanto was organized in France in 1905. Since then, congresses have been held in various countries every year, with the exceptions of years during the world wars. Although no country has adopted Esperanto officially, Esperanto was recommended by the French Academy of Sciences in 1921 and recognized by UNESCO in 1954, which recommended it to its member states in 1985. Esperanto was the 32nd language accepted as adhering to the "Common European Framework of Reference for Languages" in 2007.
Esperanto is currently the language of instruction of the International Academy of Sciences in San Marino. There is evidence that learning Esperanto may provide a superior foundation for learning languages in general, and some primary schools teach it as preparation for learning other foreign languages.
Esperanto has a notable online presence. lernu!, the most popular online learning platform of Esperanto, reported 150,000 registered users in 2013, and sees between 150,000 and 200,000 visitors each month. With about articles, Esperanto Wikipedia is the 32nd-largest Wikipedia as measured by the number of articles, and the largest Wikipedia in a constructed language. On February 22, 2012, Google Translate added Esperanto as its 64th language. Duolingo started the development of Esperanto on September 2014 as a language that can be learnt, making it the first invented language ever on Duolingo.
Currently, Esperanto is seen by many of its speakers as an alternative or addition to the growing use of English throughout the world, offering a language that is easier to learn than English.
History.
Creation.
Esperanto was created in the late 1870s and early 1880s by Ludwig Lazarus Zamenhof, an ophthalmologist from Białystok, then part of the Russian Empire. According to Zamenhof, he created the language to foster harmony between people from different countries. His feelings and the situation in Białystok may be gleaned from an extract from his letter to Nikolai Borovko:
After some ten years of development, which Zamenhof spent translating literature into Esperanto as well as writing original prose and verse, the first book of Esperanto grammar was published in Warsaw on the 26th of July 1887. The number of speakers grew rapidly over the next few decades, at first primarily in the Russian Empire and Central Europe, then in other parts of Europe, the Americas, China, and Japan. In the early years, speakers of Esperanto kept in contact primarily through correspondence and periodicals, but in 1905 the first world congress of Esperanto speakers was held in Boulogne-sur-Mer, France. Since then world congresses have been held in different countries every year, except during the two World Wars. Since the Second World War, they have been attended by an average of more than 2,000 people and up to 6,000 people.
Zamenhof's name for the language was simply "Internacia Lingvo" ("International Language").
Early proposals.
The autonomous territory of Neutral Moresnet, between what is today Belgium and Germany, had a sizable proportion of Esperanto-speakers among its small and multiethnic population. There was a proposal to make Esperanto its official language.
However, time was running out for the tiny territory. Neither Belgium nor Prussia (now within the German Empire) had ever surrendered its original claim to it. Around 1900, Germany in particular was taking a more aggressive stance towards the territory and was accused of sabotage and of obstructing the administrative process in order to force the issue. It was the First World War, however, that was the catalyst that brought about the end of neutrality. On August 4, 1914, Germany invaded Belgium, leaving Moresnet at first "an oasis in a desert of destruction". In 1915, the territory was annexed by the Kingdom of Prussia, without international recognition.
After the Great War, there was a proposal for the League of Nations to accept Esperanto as their working language, following a report by Nitobe Inazō, an official delegate of League of Nations during the 13th World Congress of Esperanto in Prague. Ten delegates accepted the proposal with only one voice against, the French delegate, Gabriel Hanotaux. Hanotaux did not like how the French language was losing its position as the international language and saw Esperanto as a threat, effectively wielding his veto power to block the decision. However, two years later, the League recommended that its member states include Esperanto in their educational curricula. For this reason, many people see the 1920s as the heyday of the Esperanto movement. Anarchism as a political movement was very supportive during this time of anationalism as well as of the esperato language.
Responses of 20th-century totalitarian regimes to Esperanto.
As a potential vehicle for international understanding, Esperanto attracted the suspicion of many totalitarian states. The situation was especially pronounced in Nazi Germany, Francoist Spain, and the Soviet Union under Joseph Stalin.
In Germany, there was additional motivation to persecute Esperanto because Zamenhof was Jewish. In his work, "Mein Kampf," Adolf Hitler specifically mentioned Esperanto as an example of a language that could be used by an international Jewish conspiracy once they achieved world domination. Esperantists were killed during the Holocaust, with Zamenhof's family in particular singled out for murder. The efforts of some Esperantists to expel Jewish colleagues and align themselves with the Reich were finally futile and Esperanto was forbidden in 1936. Esperantists in German concentration camps taught the language to fellow prisoners, telling guards they were teaching Italian, the language of one of Germany's Axis allies.
In Imperial Japan, the left-wing of the Japanese Esperanto movement was persecuted, but its leaders were careful enough not to give the impression to the government that the Esperantists were revolutionaries, which proved a successful strategy.
In the early years of the Soviet Union, Esperanto was given a measure of government support, and the Soviet Esperanto Association was an officially recognized organization. The degree of support possibly existed because Stalin himself had studied Esperanto. However, in 1937, Stalin reversed this policy. He denounced Esperanto as "the language of spies" and had Esperantists exiled or executed. The use of Esperanto was effectively banned until 1956.
Fascist Italy, however, allowed the use of Esperanto finding its phonology similar to that of Italian and publishing some tourist material in the language.
After the Spanish Civil War, Francoist Spain persecuted anarchists and Catalan nationalists, among whom the use of Esperanto was extensive, but in the 1950s the Esperanto movement was tolerated again.
Official use.
Esperanto has never been a secondary official language of any recognized country. However, there were plans at the beginning of the 20th century to establish Neutral Moresnet as the world's first Esperanto state. In addition, the self-proclaimed artificial island micronation of Rose Island used Esperanto as its official language in 1968. In February 2013 an Avaaz petition was created to make Esperanto one of the official languages of the European Union.
The US Army has published military phrase books in Esperanto, to be used in war games by mock enemy forces. In the summer of 1924, the American Radio Relay League adopted Esperanto as its official international auxiliary language, and hoped that the language would be used by radio amateurs in international communications, but its actual use for radio communications was negligible.
Esperanto is the working language of several non-profit international organizations such as the "Sennacieca Asocio Tutmonda", a left-wing cultural association, or Education@Internet, which has developed from an Esperanto organization; most others are specifically Esperanto organizations. The largest of these, the World Esperanto Association, has an official consultative relationship with the United Nations and UNESCO, which recognized Esperanto as a medium for international understanding in 1954. Esperanto is also the first language of teaching and administration of one university, the International Academy of Sciences San Marino.
All the personal documents issued by the World Service Authority, including the World Passport, are written in Esperanto, together with English, French, Spanish, Russian, Arabic, and Chinese.
Linguistic properties.
Classification.
As a constructed language, most scholars would say Esperanto is not genealogically related to any natural language. The phonology, grammar, vocabulary, and semantics are based on the Indo-European languages spoken in Europe. The sound inventory is essentially Slavic, as is much of the semantics, whereas the vocabulary derives primarily from the Romance languages, with a lesser contribution from Germanic languages and minor contributions from Slavic languages and Greek. Pragmatics and other aspects of the language not specified by Zamenhof's original documents were influenced by the native languages of early authors, primarily Russian, Polish, German, and French. However, Paul Wexler proposes that Esperanto is relexified Yiddish, which in turn he claims is a relexified Slavic language.
Esperanto has been described as "a language lexically predominantly Romanic, morphologically intensively agglutinative, and to a certain degree isolating in character". Typologically, Esperanto has prepositions and a pragmatic word order that by default is "subject–verb–object." Adjectives can be freely placed before or after the nouns they modify, though placing them before the noun is more common. New words are formed through extensive prefixing and suffixing.
Phonology.
Esperanto has 23 consonants, 5 vowels, and 2 semivowels that combine with the vowels to form 6 diphthongs. (The consonant and semivowel are both written "j", and the uncommon consonant is written with the digraph "dz", which is the only consonant that doesn't have its own letter.) Tone is not used to distinguish meanings of words. Stress is always on the second-last vowel in fully Esperanto words unless a final vowel "o" is elided, which occurs mostly in poetry. For example, "familio" "family" is , with the stress on the second "i," but when the word is used without the final "o (famili’)," the stress remains on the second "i": .
Consonants.
The 23 consonants are:
The sound is usually trilled , but may be tapped . The is normally pronounced like English "v," but may be pronounced (between English "v" and "w") or , depending on the language background of the speaker. A semivowel normally occurs only in diphthongs after the vowels and , not as a consonant . Common, if debated, assimilation includes the pronunciation of "nk" as and "kz" as .
A large number of consonant clusters can occur, up to three in initial position (as in "stranga", "strange") and four in medial position (as in "instrui", "teach"). Final clusters are uncommon except in foreign names, poetic elision of final "o," and a very few basic words such as "cent" "hundred" and "post" "after".
Vowels.
Esperanto has the five cardinal vowels found in such languages as Spanish, Swahili, Tagalog, Modern Hebrew, and Modern Greek:
There are also two semivowels, and , which combine with the cardinal vowels to form six falling diphthongs: "aj, ej, oj, uj, aŭ," and "eŭ".
Since there are only five vowels, a good deal of variation in pronunciation is tolerated. For instance, "e" commonly ranges from (French "é") to (French "è"). These details often depend on the speaker's native language. A glottal stop may occur between adjacent vowels in some people's speech, especially when the two vowels are the same, as in "heroo" "hero" ( or ) and "praavo" "great-grandfather" ( or ).
Alphabet.
The Esperanto alphabet is based on the Latin script, using a one-sound-one-letter principle. It includes six letters with diacritics: ĉ, ĝ, ĥ, ĵ, ŝ (with circumflex), and ŭ (with breve). The alphabet does not include the letters "q, w, x," or "y", which are only used when writing unassimilated foreign terms or proper names.
The 28-letter alphabet is:
All unaccented letters are pronounced approximately as in the IPA, with the exception of "c". Esperanto "j" and "c" are used in a way familiar to speakers of many European languages, but which is largely unfamiliar to English speakers: "j" has a "y" sound, as in yellow" and "boy," and "c" has a "ts" sound, as in "hits or the "zz" in "pizza". The accented letters are a bit like "h"-digraphs in English: "Ĉ" is pronounced like English "ch", and "ŝ" like "sh". "Ĝ" is the "g" in gem", "ĵ" a "zh" sound, as in "fusion" or French Jacques", and the rare "ĥ" is like the German "Bach, or older Scottish English "loch".
Writing diacritics.
Until the widespread adoption of Unicode, the letters with diacritics (found in the "Latin-Extended A" section of the Unicode Standard) caused problems with printing and computing. This was particularly true of the five letters with circumflexes, as they do not occur in any other language. These problems have abated, and are now normally seen only with computing applications that are limited to ASCII characters (typically internet chat systems and databases).
There are two principal workarounds to this problem, which substitute digraphs for the accented letters. Zamenhof, the inventor of Esperanto, created an "h-convention", which replaces "ĉ, ĝ, ĥ, ĵ, ŝ," and "ŭ" with "ch, gh, hh, jh, sh," and "u," respectively. If used in a database, a program in principle could not determine whether to render, for example, "ch" as "c" followed by "h" or as "ĉ", and would fail to render, for example, the word "senchava" properly. A more recent "x-convention" has gained ground since the advent of computing. This system replaces each diacritic with an "x" (not part of the Esperanto alphabet) after the letter, producing the six digraphs "cx, gx, hx, jx, sx," and "ux."
There are computer keyboard layouts that support the Esperanto alphabet, and some systems use software that automatically replaces x- or h-convention digraphs with the corresponding diacritic letters (EK for Microsoft Windows and Esperanta Klavaro for Windows Phone are Examples). Another example is the Esperanto Wikipedia, which accepts the x-convention for input: when a contributor types "cx" when editing an article, it will appear as the correct "ĉ" in the article text. (The input pane also accepts "ĉ"; when the page is saved, it will be changed to "cx", so that the x-convention applies uniformly in the wikitext.)
Grammar.
Esperanto words are derived by stringing together prefixes, roots, and suffixes. This process is regular, so that people can create new words as they speak and be understood. Compound words are formed with a modifier-first, head-final order, as in English (compare "birdsong" and "songbird," and likewise, "birdokanto" and "kantobirdo").
The different parts of speech are marked by their own suffixes: all common nouns end in "-o", all adjectives in "-a", all derived adverbs in "-e", and all verbs in one of six tense and mood suffixes, such as the present tense "-as".
Plural nouns used as grammatical subjects end in "-oj" (pronounced like English "oy"), whereas their singular direct object forms end in "-on". Plural direct objects end with the combination "-ojn" (rhymes with "coin"); "-o-" indicates that the word is a noun, "-j-" indicates the plural, and "-n" indicates the accusative. Adjectives agree with their nouns; their endings are plural "-aj" (pronounced "eye"), accusative "-an", and plural accusative "-ajn" (rhymes with "fine").
The suffix "-n", besides indicating the direct object, is used to indicate movement and a few other things as well.
The six verb inflections consist of three tenses and three moods. They are present tense "-as", future tense "-os", past tense "-is", infinitive mood "-i", conditional mood "-us" and jussive mood "-u" (used for wishes and commands). Verbs are not marked for person or number. Thus, "kanti" means "to sing", "mi kantas" means "I sing", "vi kantas" means "you sing", and "ili kantas" means "they sing".
Word order is comparatively free. Adjectives may precede or follow nouns; subjects, verbs and objects may occur in any order. However, the article "la" "the", demonstratives such as "tiu" "that" and prepositions (such as "ĉe" "at") must come before their related nouns. Similarly, the negative "ne" "not" and conjunctions such as "kaj" "and" and "ke" "that" must precede the phrase or clause that they introduce. In copular (A = B) clauses, word order is just as important as in English: "people are animals" is distinguished from "animals are people".
Vocabulary.
The core vocabulary of Esperanto was defined by "Lingvo internacia", published by Zamenhof in 1887. This book listed 900 roots; these could be expanded into tens of thousands of words using prefixes, suffixes, and compounding. In 1894, Zamenhof published the first Esperanto dictionary, "Universala Vortaro", which had a larger set of roots. The rules of the language allowed speakers to borrow new roots as needed; it was recommended, however, that speakers use most international forms and then derive related meanings from these.
Since then, many words have been borrowed, primarily (but not solely) from the European languages. Not all proposed borrowings become widespread, but many do, especially technical and scientific terms. Terms for everyday use, on the other hand, are more likely to be derived from existing roots; "komputilo" "computer", for instance, is formed from the verb "komputi" "compute" and the suffix "-ilo" "tool". Words are also calqued; that is, words acquire new meanings based on usage in other languages. For example, the word "muso" "mouse" has acquired the meaning of a computer mouse from its usage in English. Esperanto speakers often debate about whether a particular borrowing is justified or whether meaning can be expressed by deriving from or extending the meaning of existing words.
Some compounds and formed words in Esperanto are not entirely straightforward; for example, "eldoni", literally "give out", means "publish", paralleling the usage of certain European languages (such as German). In addition, the suffix "-um-" has no defined meaning; words using the suffix must be learned separately (such as "dekstren" "to the right" and "dekstrumen" "clockwise").
There are not many idiomatic or slang words in Esperanto, as these forms of speech tend to make international communication difficult—working against Esperanto's main goal.
Useful phrases.
Below are listed some useful Esperanto words and phrases along with transcriptions:
Sample text.
The following short extract gives an idea of the character of Esperanto. (Pronunciation is covered above; the Esperanto letter "j" is pronounced like English "y".)
Education.
The majority of Esperanto speakers learn the language through self-directed study, online tutorials, and correspondence courses taught by volunteers. In more recent years, free teaching websites, like "lernu!", have become popular.
Esperanto instruction is occasionally available at schools, including four primary schools in a pilot project under the supervision of the University of Manchester, and by one count at 69 universities. However, outside China and Hungary, these mostly involve informal arrangements rather than dedicated departments or state sponsorship. Eötvös Loránd University in Budapest had a department of Interlinguistics and Esperanto from 1966 to 2004, after which time instruction moved to vocational colleges; there are state examinations for Esperanto instructors. Additionally, Adam Mickiewicz University in Poland offers a diploma in Interlinguistics. The Senate of Brazil passed a bill in 2009 that would make Esperanto an optional part of the curriculum in public schools, although mandatory if there is demand for it. As of 2012 the bill is still under consideration by the Chamber of Deputies.
Various educators have estimated that Esperanto can be learned in anywhere from one quarter to one twentieth the amount of time required for other languages. Claude Piron, a psychologist formerly at the University of Geneva and Chinese–English–Russian–Spanish translator for the United Nations, argued that Esperanto is far more intuitive than many ethnic languages. "Esperanto relies entirely on innate reflexes [and] differs from all other languages in that you can always trust your natural tendency to generalize patterns. [...] The same neuropsychological law [—called by] Jean Piaget "generalizing assimilation"—applies to word formation as well as to grammar."
The Institute of Cybernetic Pedagogy at Paderborn (Germany) has compared the length of study time it takes natively French-speaking high-school students to obtain comparable 'standard' levels in Esperanto, English, German, and Italian. The results were:
Language acquisition.
Four primary schools in Britain, with some 230 pupils, are currently following a course in "propaedeutic Esperanto"—that is, instruction in Esperanto to raise language awareness and accelerate subsequent learning of foreign languages—under the supervision of the University of Manchester. As they put it,
Studies have been conducted in New Zealand, United States, Germany, Italy and Australia. The results of these studies were favorable and demonstrated that studying Esperanto before another foreign language expedites the acquisition of the other, natural, language. This appears to be because learning subsequent foreign languages is easier than learning one's first foreign language, whereas the use of a grammatically simple and culturally flexible auxiliary language like Esperanto lessens the first-language learning hurdle. In one study, a group of European secondary school students studied Esperanto for one year, then French for three years, and ended up with a significantly better command of French than a control group, who studied French for all four years. Similar results have been found for other combinations of native and second languages, as well as for arrangements in which the course of study was reduced to two years, of which six months is spent learning Esperanto.
Community.
Geography and demography.
Esperanto is by far the most widely spoken constructed language in the world. Speakers are most numerous in Europe and East Asia, especially in urban areas, where they often form Esperanto clubs. Esperanto is particularly prevalent in the northern and central countries of Europe; in China, Korea, Japan, and Iran within Asia; in Brazil, Argentina, and Mexico in the Americas; and in Togo in Africa.
Number of speakers.
An estimate of the number of Esperanto speakers was made by Sidney S. Culbert, a retired psychology professor at the University of Washington and a longtime Esperantist, who tracked down and tested Esperanto speakers in sample areas in dozens of countries over a period of twenty years. Culbert concluded that between one and two million people speak Esperanto at Foreign Service Level 3, "professionally proficient" (able to communicate moderately complex ideas without hesitation, and to follow speeches, radio broadcasts, etc.). Culbert's estimate was not made for Esperanto alone, but formed part of his listing of estimates for all languages of more than one million speakers, published annually in the World Almanac and Book of Facts. Culbert's most detailed account of his methodology is found in a 1989 letter to David Wolff. Since Culbert never published detailed intermediate results for particular countries and regions, it is difficult to independently gauge the accuracy of his results.
In the Almanac, his estimates for numbers of language speakers were rounded to the nearest million, thus the number for Esperanto speakers is shown as two million. This latter figure appears in "Ethnologue". Assuming that this figure is accurate, that means that about 0.03% of the world's population speaks the language. Although it is not Zamenhof's goal of a universal language, it still represents a level of popularity unmatched by any other constructed language.
Marcus Sikosek (now Ziko van Dijk) has challenged this figure of 1.6 million as exaggerated. He estimated that even if Esperanto speakers were evenly distributed, assuming one million Esperanto speakers worldwide would lead one to expect about 180 in the city of Cologne. Van Dijk finds only 30 fluent speakers in that city, and similarly smaller-than-expected figures in several other places thought to have a larger-than-average concentration of Esperanto speakers. He also notes that there are a total of about 20,000 members of the various Esperanto organizations (other estimates are higher). Though there are undoubtedly many Esperanto speakers who are not members of any Esperanto organization, he thinks it unlikely that there are fifty times more speakers than organization members.
Finnish linguist Jouko Lindstedt, an expert on native-born Esperanto speakers, presented the following scheme to show the overall proportions of language capabilities within the Esperanto community:
In the absence of Dr. Culbert's detailed sampling data, or any other census data, it is impossible to state the number of speakers with certainty. According to the website of the World Esperanto Association:
In 2009 Lu Wunsch-Rolshoven used 2001 year census data from Hungary and Lithuania as a base for an estimate, resulting in approximately 160,000 to 300,000 to speak the language actively or fluently throughout the world, with about 80,000 to 150,000 of these being in the European Union.
Native speakers.
Native Esperanto speakers, "denaskuloj," have learned the language from birth from Esperanto-speaking parents. This usually happens when Esperanto is the chief or only common language in an international family, but sometimes occurs in a family of devoted Esperantists. The 15th edition of "Ethnologue" cited estimates that there were 200 to 2,000 native speakers in 1996, but these figures were removed from the 16th and 17th editions.
Culture.
Esperantists can access an international culture, including a large body of original as well as translated literature. There are more than 25,000 Esperanto books, both originals and translations, as well as several regularly distributed Esperanto magazines. In 2013 a museum about Esperanto opened in China. Esperantists use the language for free accommodations with Esperantists in 92 countries using the Pasporta Servo or to develop pen pal friendships abroad through the Esperanto Pen Pal Service.
Every year, 1,500–3,000 Esperantists meet for the World Congress of Esperanto "(Universala Kongreso de Esperanto)".
Historically, much Esperanto music, such as "Kaj Tiel Plu", has been in various folk traditions. There is also a variety of classical and semi-classical choral music, both original and translated, as well as large ensemble music that includes voices singing Esperanto texts. Lou Harrison, who incorporated styles and instruments from many world cultures in his music, used Esperanto titles and/or texts in several of his works, most notably "La Koro-Sutro" (1973). David Gaines used Esperanto poems as well as an excerpt from a speech by Dr. Zamenhof for his "Symphony No. 1 (Esperanto)" for mezzo-soprano and orchestra (1994–98). He wrote original Esperanto text for his "Povas plori mi ne plu" ("I Can Cry No Longer") for unaccompanied SATB choir (1994).
There are also shared traditions, such as Zamenhof Day, and shared behaviour patterns. Esperantists speak primarily in Esperanto at international Esperanto meetings.
Detractors of Esperanto occasionally criticize it as "having no culture". Proponents, such as Prof. Humphrey Tonkin of the University of Hartford, observe that Esperanto is "culturally neutral by design, as it was intended to be a facilitator between cultures, not to be the carrier of any one national culture". The late Scottish Esperanto author William Auld wrote extensively on the subject, arguing that Esperanto is "the expression of a common human culture, unencumbered by national frontiers. Thus it is considered a culture on its own."
Noted authors in Esperanto.
Some authors of works in Esperanto are:
Popular culture.
Esperanto has been used in a number of films and novels. Typically, this is done either to add the exotic flavour of a foreign language without representing any particular ethnicity, or to avoid going to the trouble of inventing a new language. The Charlie Chaplin film "The Great Dictator" (1940) showed Jewish ghetto shop signs in Esperanto. Two full-length feature films have been produced with dialogue entirely in Esperanto: "Angoroj," in 1964, and "Incubus," a 1965 B-movie horror film. Other amateur productions have been made, such as a dramatisation of the novel "Gerda Malaperis" (Gerda Has Disappeared). A number of "mainstream" films in national languages have used Esperanto in some way.
Esperanto is used as the universal language in the far future of Harry Harrison's "Stainless Steel Rat" and "Deathworld" stories. Poul Anderson's story "High Treason" takes place in a future where Earth became united politically but was still divided into many languages and cultures, and Esperanto became the language of its space armed forces, fighting wars with various extraterrestrial races.
The opening song to the popular video game "Final Fantasy XI", 'Memoro de la Ŝtono', was written in Esperanto. It was the first game in the series that was played online, and would have players from both Japan and North America (official European support was added after the North American launch) playing together on the same servers, using an auto-translate tool to communicate. The composer, Nobuo Uematsu, felt that Esperanto was a good language to symbolize worldwide unity.
Esperanto is also found in "Saga (comic book)" as the language Blue, spoken by the inhabitants of Wreath. It is rendered in blue-colored text. Blue is generally only spoken by inhabitants of Wreath, while most other cultures use a universal language that appears to be simply named "Language." Some Wreath inhabitants use translator rings to communicate with those who don't speak Blue. Magic seems to be activated via the linguistic medium of blue. The Esperanto in Saga is sometimes badly translated, and may have been generated with translation software, such as Google Translator.
In the television show "Red Dwarf", the bulk of which takes place more than three million years in the future, crewman Arnold Rimmer constantly spends his time trying to learn Esperanto and failing, even compared to his bunkmate Dave Lister who only maintains a casual interest. Additionally many of the signs around the ship 'Red Dwarf' are written in both English and Esperanto. The novel "Infinity Welcomes Careful Drivers" states that, although not required, it is widely expected that officers in the Space Corps be fluent in the language, hence Rimmer's interest.
Esperanto is referenced in Season 25, Episode 12 of "The Simpsons", when Skinner expresses hope for the growth of Springfield Esperanto Society and speaks two sentences in Esperanto.
Science.
In 1921 the French Academy of Sciences recommended using Esperanto for international scientific communication. A few scientists and mathematicians, such as Maurice Fréchet (mathematics), John C. Wells (linguistics), Helmar Frank (pedagogy and cybernetics), and Nobel laureate Reinhard Selten (economics) have published part of their work in Esperanto. Frank and Selten were among the founders of the International Academy of Sciences in San Marino, sometimes called the "Esperanto University", where Esperanto is the primary language of teaching and administration.
A message in Esperanto was recorded and included in "Voyager 1"'s Golden Record.
Commerce and trade.
Esperanto business groups have been active for many years. The French Chamber of Commerce did research in the 1920s and reported in "The New York Times" in 1921 that Esperanto seemed to be the best business language.
Goals of the movement.
Zamenhof's intention was to create an easy-to-learn language to foster international understanding. It was to serve as an international auxiliary language, that is, as a universal second language, not to replace ethnic languages. This goal was widely shared among Esperanto speakers in the early decades of the movement. Later, Esperanto speakers began to see the language and the culture that had grown up around it as ends in themselves, even if Esperanto is never adopted by the United Nations or other international organizations.
Those Esperanto speakers who want to see Esperanto adopted officially or on a large scale worldwide are commonly called "finvenkistoj", from "fina venko", meaning "final victory", or "pracelistoj", from "pracelo", meaning "original goal". Those who focus on the intrinsic value of the language are commonly called "raŭmistoj", from Rauma, Finland, where a declaration on the near-term unlikelihood of the "fina venko" and the value of Esperanto culture was made at the International Youth Congress in 1980.
The Prague Manifesto (1996) presents the views of the mainstream of the Esperanto movement and of its main organisation, the World Esperanto Association (UEA).
Symbols and flags.
The earliest flag, and the one most commonly used today, features a green five-pointed star against a white canton, upon a field of green. It was proposed to Zamenhof by Irishman Richard Geoghegan, author of the first Esperanto textbook for English speakers, in 1887. The flag was approved in 1905 by delegates to the first conference of Esperantists at Boulogne-sur-Mer. A version with an "E" superimposed over the green star is sometimes seen. Other variants include that for Christian Esperantists, with a white Christian cross superimposed upon the green star, and that for Leftists, with the color of the field changed from green to red.
In 1987, a second flag design was chosen in a contest organized by the UEA celebrating the first centennial of the language. It featured a white background with two stylised curved "E"s facing each other. Dubbed the "jubilea simbolo" (jubilee symbol), it attracted criticism from some Esperantists, who dubbed it the "melono" (melon) because of the design's elliptical shape. It is still in use, though to a lesser degree than the traditional symbol, known as the "verda stelo" (green star).
Politics.
Esperanto has been placed in many proposed political situations. The most popular of these is the Europe – Democracy – Esperanto, which aims to establish Esperanto as the official language of the European Union. Grin's Report, published in 2005 by François Grin found that the use of English as the lingua franca within the European Union costs billions annually and significantly benefits English-speaking countries financially. The report considered a scenario where Esperanto would be the lingua franca and found that it would have many advantages, particularly economically speaking, as well as ideologically.
Religion.
Esperanto has served an important role in several religions, such as Oomoto from Japan and the Baha'i Faith from Iran, and has been encouraged by others, like some Spiritist movements.
Oomoto.
The Oomoto religion encourages the use of Esperanto among its followers and includes Zamenhof as one of its deified spirits.
Bahá'í Faith.
The Bahá'í Faith encourages the use of an auxiliary international language. The Baha'i's believe that it will not be the language of the future, although it has great potential in this role, as it has not been chosen by the people. L. L. Zamenhof's daughter Lidja became a Bahá'í, and various volumes of the Bahá'í literatures and other Baha'i books have been translated into Esperanto. In 1973, the Bahá'í Esperanto-League for active Bahá'í supporters of Esperanto was founded.
Spiritism.
In 1908, spiritist Camilo Chaigneau wrote an article named "Spiritism and Esperanto" in the periodic "La Vie d'Outre-Tombe" recommending the use of Esperanto in a "central magazine" for all spiritists and esperantists. Esperanto then became actively promoted by spiritists, at least in Brazil, initially by Ismael Gomes Braga and František Lorenz; the latter is known in Brazil as Francisco Valdomiro Lorenz, and was a pioneer of both spiritist and Esperantist movements in this country.
The Brazilian Spiritist Federation publishes Esperanto coursebooks, translations of Spiritism's basic books, and encourages Spiritists to become Esperantists.
Bible translations.
The first translation of the Bible into Esperanto was a translation of the Tanakh or Old Testament done by L. L. Zamenhof. The translation was reviewed and compared with other languages' translations by a group of British clergy and scholars before its publication at the British and Foreign Bible Society in 1910. In 1926 this was published along with a New Testament translation, in an edition commonly called the "Londona Biblio". In the 1960s, the "Internacia Asocio de Bibliistoj kaj Orientalistoj" tried to organize a new, ecumenical Esperanto Bible version. Since then, the Dutch Remonstrant pastor Gerrit Berveling has translated the Deuterocanonical or apocryphal books in addition to new translations of the Gospels, some of the New Testament epistles, and some books of the Tanakh or Old Testament. These have been published in various separate booklets, or serialized in "Dia Regno", but the Deuterocanonical books have appeared in recent editions of the Londona Biblio.
Christianity.
Christian Esperanto organizations include two that were formed early in the history of Esperanto:
Individual churches using Esperanto include:
Chick Publications, publisher of Protestant fundamentalist themed evangelistic tracts, has published a number of comic book style tracts by Jack T. Chick translated into Esperanto, including "This Was Your Life!" ("Jen Via Tuta Vivo!")
Islam.
Ayatollah Khomeini of Iran called on Muslims to learn Esperanto and praised its use as a medium for better understanding among peoples of different religious backgrounds. After he suggested that Esperanto replace English as an international lingua franca, it began to be used in the seminaries of Qom. An Esperanto translation of the Qur'an was published by the state shortly thereafter. In 1981, its usage became less popular when it became apparent that followers of the Bahá'í Faith were interested in it.
Criticism.
Esperanto was conceived as a language of international communication, more precisely as a universal second language. Since publication, there has been debate over whether it is possible for Esperanto to attain this position, and whether it would be an improvement for international communication were it to do so; Esperanto proponents have also been criticized for diverting public funds to encourage its study over more "useful national languages".
Since Esperanto is a planned language, there have been many criticisms of minor points. An example is Zamenhof's choice of the word "edzo" over something like "spozo" for "husband, spouse", or his choice of the Classic Greek and Old Latin singular and plural endings "-o, -oj, -a, -aj" over their Medieval contractions "-o, -i, -a, -e." (Both these changes were adopted by the Ido reform, though Ido dispensed with adjectival agreement altogether.) Some more common examples of general criticism include the following:
Modifications.
Though Esperanto itself has changed little since the publication of the "Fundamento de Esperanto" (Foundation of Esperanto), a number of reform projects have been proposed over the years, starting with Zamenhof's proposals in 1894 and Ido in 1907. Several later constructed languages, such as Universal, were based on Esperanto.
In modern times, attempts have been made to eliminate perceived sexism in the language. One example of this is Riism. However, as Esperanto has become a living language, changes are as difficult to implement as in ethnic languages.
Eponymous entities.
There are many geographical and astronomical features named after Esperanto, or after its creator L. L. Zamenhof. These include Esperanto Island in Zed Islands off Livingston Island, and the asteroids 1421 Esperanto and 1462 Zamenhof discovered by Finnish astronomer and Esperantist Yrjö Väisälä.

</doc>
<doc id="9251" url="http://en.wikipedia.org/wiki?curid=9251" title="Engineering">
Engineering

Engineering (from Latin "ingenium", meaning "cleverness" and "ingeniare", meaning "to contrive, devise") is the application of scientific, economic, social, and practical knowledge in order to invent, design, build, maintain, and improve structures, machines, devices, systems, materials and processes. The discipline of engineering is extremely broad, and encompasses a range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied science, technology and types of application.
The American Engineers' Council for Professional Development (ECPD, the predecessor of ABET) has defined "engineering" as:
The creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation or safety to life and property.
One who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Designated Engineering Representative, Chartered Engineer, Incorporated Engineer, Ingenieur or European Engineer.
History.
Engineering has existed since ancient times as humans devised fundamental inventions such as the wedge, lever, wheel, and pulley. Each of these inventions is consistent with the modern definition of engineering, exploiting basic mechanical principles to develop useful tools and objects.
The term "engineering" itself has a much more recent etymology, deriving from the word "engineer", which itself dates back to 1300, when an "engine'er" (literally, one who operates an "engine") originally referred to "a constructor of military engines." In this context, now obsolete, an "engine" referred to a military machine, "i.e.", a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, "e.g.", the U.S. Army Corps of Engineers.
The word "engine" itself is of even older origin, ultimately deriving from the Latin "ingenium" (c. 1250), meaning "innate quality, especially mental power, hence a clever invention."
Later, as the design of civilian structures such as bridges and buildings matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the older discipline of military engineering.
Ancient era.
The Pharos of Alexandria, the pyramids in Egypt, the Hanging Gardens of Babylon, the Acropolis and the Parthenon in Greece, the Roman aqueducts, Via Appia and the Colosseum, Teotihuacán and the cities and pyramids of the Mayan, Inca and Aztec Empires, the Great Wall of China, the Brihadeeswarar Temple of Thanjavur and tombs of India, among many others, stand as a testament to the ingenuity and skill of the ancient civil and military engineers.
The earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630-2611 BC.
Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, the first known mechanical computer, and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.
Chinese, Greek and Roman armies employed complex military machines and inventions such as artillery which was developed by the Greeks around the 4th century B.C., the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.
Renaissance era.
William Gilbertis considered to be the first electrical engineer with his 1600 publication of De Magnete. He coined the term "electricity".
The first steam engine was built in 1698 by mechanical engineer Thomas Savery. The development of this device gave rise to the Industrial Revolution in the coming decades, allowing for the beginnings of mass production.
With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering the fields then known as the mechanic arts became incorporated into engineering.
Modern era.
The early stages of electrical engineering included the experiments of Alessandro Volta in the 1800s, the experiments of Michael Faraday, Georg Ohm and others and the invention of the electric motor in 1872. The work of James Maxwell and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.
The inventions of Thomas Savery and the Scottish engineer James Watt gave rise to modern mechanical engineering. The development of specialized machines and their maintenance tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace Britain and abroad.
John Smeaton was the first self-proclaimed civil engineer, and is often regarded as the "father" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbours and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. His lighthouse remained in use until 1877 and was dismantled and partially rebuilt at Plymouth Hoe where it is known as Smeaton's Tower. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain "hydraulicity" in lime; work which led ultimately to the invention of Portland cement.
Chemical engineering, like its counterpart mechanical engineering, developed in the nineteenth century during the Industrial Revolution. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes.
Aeronautical engineering deals with aircraft design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.
The first PhD in engineering (technically, "applied science and engineering") awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.
Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I . Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.
In 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage.
Main branches of engineering.
Engineering is a broad discipline which is often broken down into several sub-disciplines. These disciplines concern themselves with differing areas of engineering work. Although initially an engineer will usually be trained in a specific discipline, throughout an engineer's career the engineer may become multi-disciplined, having worked in several of the outlined areas. Engineering is often characterized as having four main branches:
Beyond these four, sources vary on other main branches. Historically, naval engineering and mining engineering were major branches. Modern fields sometimes included as major branches include manufacturing engineering, acoustical engineering, corrosion engineering, Instrumentation and control, aerospace, automotive, computer, electronic, petroleum, systems, audio, software, architectural, agricultural, biosystems, biomedical, geological, textile, industrial, materials, and nuclear engineering. These and other branches of engineering are represented in the 36 institutions forming the membership of the UK Engineering Council.
New specialties sometimes combine with the traditional fields and form new branches - for example Earth Systems Engineering and Management involves a wide range of subject areas including anthropology, engineering, environmental science, ethics and philosophy. A new or emerging area of application will commonly be defined temporarily as a permutation or subset of existing disciplines; there is often gray area as to when a given sub-field becomes large and/or prominent enough to warrant classification as a new "branch." One key indicator of such emergence is when major universities start establishing departments and programs in the new field.
For each of these fields there exists considerable overlap, especially in the areas of the application of sciences to their disciplines such as physics, chemistry and mathematics.
Methodology.
Engineers apply mathematics and sciences such as physics to find suitable solutions to problems or to make improvements to the status quo. More than ever, engineers are now required to have knowledge of relevant sciences for their design projects. As a result, they may keep on learning new material throughout their career.
If multiple options exist, engineers weigh different design choices on their merits and choose the solution that best matches the requirements. The crucial and unique task of the engineer is to identify, understand, and interpret the constraints on a design in order to produce a successful result. It is usually not enough to build a technically successful product; it must also meet further requirements.
Constraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productibility, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.
Problem solving.
Engineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem allows them to analyze it (sometimes definitively), and to test potential solutions.
Usually multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of "low-level" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.
Engineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.
Engineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. However, the greater the safety factor, the less efficient the design may be.
The study of failed products is known as forensic engineering, and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure.
Computer use.
As with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.
One of the most widely used design tools in the profession is computer-aided design (CAD) software like Autodesk Inventor, DSS SolidWorks, or Pro Engineer which enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.
These allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.
There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and AEC software for civil engineering.
In recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).
Social context.
Engineering as a subject ranges from large collaborations to small individual projects. Almost all engineering projects are beholden to some sort of financing agency: a company, a set of investors, or a government. The few types of engineering that are minimally constrained by such issues are "pro bono" engineering and open-design engineering.
By its very nature engineering has interconnections with society and human behavior. Every product or construction used by modern society will have been influenced by engineering. Engineering is a very powerful tool to make changes to environment, society and economies, and its application brings with it a great responsibility. Many engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large.
Engineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.
Engineering is a key driver of human development. Sub-Saharan Africa in particular has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid. The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.
All overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:
Engineering companies in many established economies are facing significant challenges ahead with regard to the number of skilled engineers being trained, compared with the number retiring. This problem is very prominent in the UK. There are many economic and political issues that this can cause, as well as ethical issues It is widely agreed that engineering faces an "image crisis", rather than it being fundamentally an unattractive career. Much work is needed to avoid huge problems in the UK and well as the USA and other western economies.
Relationships with other disciplines.
Science.
There exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.
Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists.
In the book "What Engineers Know and How They Know It", Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics and/or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.
Examples are the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of Miner's rule to calculate fatigue damage. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.
As stated by Fung "et al." in the revision to the classic engineering text "Foundations of Solid Mechanics":
"Engineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what is existing. Since a design has to be concrete, it must have its geometry, dimensions, and characteristic numbers. Almost all engineers working on new designs find that they do not have all the needed information. Most often, they are limited by insufficient scientific knowledge. Thus they study mathematics, physics, chemistry, biology and mechanics. Often they have to add to the sciences relevant to their profession. Thus engineering sciences are born."
Although engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability and constructability or ease of fabrication, as well as legal considerations such as patent infringement or liability in the case of failure of the solution.
Medicine and biology.
The study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, enhance and even replace functions of the human body, if necessary, through the use of technology.
Modern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers. The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.
Conversely, some engineering disciplines view the human body as a biological machine worth studying, and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.
Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.
Medicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.
The heart for example functions much like a pump, the skeleton is like a linked structure with levers, the brain produces electrical signals etc. These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.
Newly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.
Art.
There are connections between engineering and art;
they are direct in some fields, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering); and indirect in others.
The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design. Robert Maillart's bridge design is perceived by some to have been deliberately artistic. At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.
Among famous historical figures Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.
Other fields.
In political science the term "engineering" has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term.

</doc>
<doc id="9252" url="http://en.wikipedia.org/wiki?curid=9252" title="Education">
Education

Education in its general sense is a form of learning in which the knowledge, skills, and habits of a group of people are transferred from one generation to the next through teaching, training, or research. Education frequently takes place under the guidance of others, but may also be autodidactic. Any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational. Education is commonly divided into stages such as preschool, primary school, secondary school and then college, university or apprenticeship.
A right to education has been recognized by some governments. At the global level, Article 13 of the United Nations' 1966 International Covenant on Economic, Social and Cultural Rights recognizes the right of everyone to an education. Although education is compulsory in most places up to a certain age, attendance at school often isn't, and a minority of parents choose home-schooling, e-learning or similar for their children.
Etymology.
Etymologically, the word "education" is derived from the Latin "ēducātiō" ("A breeding, a bringing up, a rearing") from "ēdūcō" ("I educate, I train") which is related to the homonym "ēdūcō" ("I lead forth, I take out; I raise up, I erect") from "ē-" ("from, out of") and "dūcō ("I lead, I conduct").
Education can take place in formal or informal educational settings.
It is the combination of Awareness and knowledge.
History.
Education began in the earliest prehistory, as adults trained the young of their society in the knowledge and skills they would need to master and eventually pass on. In pre-literate societies this was achieved orally and through imitation. Story-telling continued from one generation to the next. As cultures began to extend their knowledge beyond skills that could be readily learned through imitation, formal education developed. Schools existed in Egypt at the time of the Middle Kingdom.
Plato founded the Academy in Athens, the first institution of higher learning in Europe. The city of Alexandria in Egypt, founded in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece. There mathematician Euclid and anatomist Herophilus; constructed the great Library of Alexandria and translated the Hebrew Bible into Greek. European civilizations suffered a collapse of literacy and organization following the fall of Rome in AD 476.
In China, Confucius (551-479 BCE), of the State of Lu, was China's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbours like Korea, Japan and Vietnam. He gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.
After the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centers of advanced education. Some of these ultimately evolved into medieval universities and forebears of many of Europe's modern universities. During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of enquiry and produced a great variety of fine scholars and natural philosophers, including Thomas Aquinas of the University of Naples, Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation; and Saint Albert the Great, a pioneer of biological field research. The University of Bologne is considered the oldest continually operating university.
Elsewhere during the Middle Ages, Islamic science and mathematics flourished under the Islamic caliphate established across the Middle East, extending from the Iberian Peninsula in the west to the Indus in the east and to the Almoravid Dynasty and Mali Empire in the south.
The Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. The European Age of Empires saw European ideas of education in philosophy, religion, arts and sciences spread out across the globe. Missionaries and scholars also brought back new ideas from other civilisations — as with the Jesuit China missions who played a significant role in the transmission of knowledge, science, and culture between China and Europe, translating works from Europe like Euclid's Elements for Chinese scholars and the thoughts of Confucius for European audiences. The Enlightenment saw the emergence of a more secular educational outlook in Europe.
In most countries today full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.
Formal education.
Systems of schooling involve institutionalized teaching and learning in relation to a curriculum, which itself is established according to a predetermined purpose of the schools in the system. School systems are sometimes also based on religions, giving them different curricula.
Preschool.
Preschools provide education up to the age of between 4 and 8 when children enter primary education. Also known as nursery schools and as kindergarten, except in the USA, where kindergarten is a term used for primary education.
Preschool education is important because it can give a child the edge in a competitive world and education climate. While children who do not receive the fundamentals during their preschool years will be taught the alphabet, counting, shapes and colors and designs when they begin their formal education they will be behind the children who already possess that knowledge. The true purpose behind kindergarten is "to provide a child-centered, preschool curriculum for three to seven year old children that aimed at unfolding the child's physical, intellectual, and moral nature with balanced emphasis on each of them."
This period of education is very important in the formative years of the child. Teachers with special skills and training are needed at this time to nurture the children to develop their potentials.
Primary.
Primary (or elementary) education consists of the first 5–7 years of formal, structured education. In general, primary education consists of six or eight years of schooling starting at the age of five or six, although this varies between, and sometimes within, countries. Globally, around 89% of primary-age children are enrolled in primary education, and this proportion is rising. Under the Education For All programs driven by UNESCO, most countries have committed to achieving universal enrollment in primary education by 2015, and in many countries, it is compulsory for children to receive primary education. The division between primary and secondary education is somewhat arbitrary, but it generally occurs at about eleven or twelve years of age. Some education systems have separate middle schools, with the transition to the final stage of secondary education taking place at around the age of fourteen. Schools that provide primary education, are mostly referred to as "primary schools". Primary schools in these countries are often subdivided into infant schools and junior school.
In India, compulsory education spans over twelve years, out of which children receive elementary education for 8 years. Elementary schooling consists of five years of primary schooling and 3 years of upper primary schooling. Various states in the republic of India provide 12 years of compulsory school education based on a national curriculum framework designed by the National Council of Educational Research and Training.
Secondary.
In most contemporary educational systems of the world, secondary education comprises the formal education that occurs during adolescence. It is characterized by transition from the typically compulsory, comprehensive primary education for minors, to the optional, selective tertiary, "post-secondary", or "higher" education (e.g. university, vocational school) for adults. Depending on the system, schools for this period, or a part of it, may be called secondary or high schools, gymnasiums, lyceums, middle schools, colleges, or vocational schools. The exact meaning of any of these terms varies from one system to another. The exact boundary between primary and secondary education also varies from country to country and even within them, but is generally around the seventh to the tenth year of schooling. Secondary education occurs mainly during the teenage years. In the United States, Canada and Australia primary and secondary education together are sometimes referred to as K-12 education, and in New Zealand Year 1–13 is used. The purpose of secondary education can be to give common knowledge, to prepare for higher education or to train directly in a profession.
The emergence of secondary education in the United States did not happen until 1910, caused by the rise in big businesses and technological advances in factories (for instance, the emergence of electrification), that required skilled workers. In order to meet this new job demand, high schools were created, with a curriculum focused on practical job skills that would better prepare students for white collar or skilled blue collar work. This proved to be beneficial for both employers and employees, for the improvement in human capital caused employees to become more efficient, which lowered costs for the employer, and skilled employees received a higher wage than employees with just primary educational attainment.
In Europe, grammar schools or academies date from as early as the 16th century, in the form of public schools, fee-paying schools, or charitable educational foundations, which themselves have an even longer history.
Community colleges offer nonresidential junior college offering courses to people living in a particular area.
Tertiary (higher).
Higher education, also called tertiary, third stage, or post secondary education, is the non-compulsory educational level that follows the completion of a school providing a secondary education, such as a high school or secondary school. Tertiary education is normally taken to include undergraduate and postgraduate education, as well as vocational education and training. Colleges and universities are the main institutions that provide tertiary education. Collectively, these are sometimes known as tertiary institutions. Tertiary education generally results in the receipt of certificates, diplomas, or academic degrees.
Higher education generally involves work towards a degree-level or foundation degree qualification. In most developed countries a high proportion of the population (up to 50%) now enter higher education at some time in their lives. Higher education is therefore very important to national economies, both as a significant industry in its own right, and as a source of trained and educated personnel for the rest of the economy.
University education includes teaching, research, and social services activities, and it includes both the "undergraduate" level (sometimes referred to as tertiary education) and the "graduate" (or "postgraduate") level (sometimes referred to as graduate school). Universities are generally composed of several colleges. In the United States, universities can be private and independent like Yale University; public and state-governed like the Pennsylvania State System of Higher Education; or independent but state-funded like the University of Virginia. A number of career specific courses are now available to students through the Internet.
A liberal arts institution can be defined as a "college or university curriculum aimed at imparting broad general knowledge and developing general intellectual capacities, in contrast to a professional, vocational, or technical curriculum." Although what is known today as the liberal arts college began in Europe, the term is more commonly associated with universities in the United States.
Vocational.
Vocational education is a form of education focused on direct and practical training for a specific trade or craft. Vocational education may come in the form of an apprenticeship or internship as well as institutions teaching courses such as carpentry, agriculture, engineering, medicine, architecture and the arts.
Special.
In the past, those who were disabled were often not eligible for public education. Children with disabilities were often educated by physicians or special tutors. These early physicians (people like Itard, Seguin, Howe, Gallaudet) set the foundation for special education today. They focused on individualized instruction and functional skills. Special education was only provided to people with severe disabilities in its early years, but more recently it has been opened to anyone who has experienced difficulty learning.
Other educational forms.
Alternative.
While considered "alternative" today, most alternative systems have existed since ancient times. After the public school system was widely developed beginning in the 19th century, some parents found reasons to be discontented with the new system. Alternative education developed in part as a reaction to perceived limitations and failings of traditional education. A broad range of educational approaches emerged, including alternative schools, self learning, homeschooling and unschooling. Example alternative schools include Montessori schools, Waldorf schools (or Steiner schools), Friends schools, Sands School, Summerhill School, The Peepal Grove School, Sudbury Valley School, Krishnamurti schools, and open classroom schools.
In time, some ideas from these experiments and paradigm challenges may be adopted as the norm in education, just as Friedrich Fröbel's approach to early childhood education in 19th century Germany has been incorporated into contemporary kindergarten classrooms. Other influential writers and thinkers have included the Swiss humanitarian Johann Heinrich Pestalozzi; the American transcendentalists Amos Bronson Alcott, Ralph Waldo Emerson, and Henry David Thoreau; the founders of progressive education, John Dewey and Francis Parker; and educational pioneers such as Maria Montessori and Rudolf Steiner, and more recently John Caldwell Holt, Paul Goodman, Frederick Mayer, George Dennison and Ivan Illich.
Indigenous.
Indigenous education refers to the inclusion of indigenous knowledge, models, methods and content within formal and non-formal educational systems. Often in a post-colonial context, the growing recognition and use of indigenous education methods can be a response to the erosion and loss of indigenous knowledge and language through the processes of colonialism. Furthermore, it can enable indigenous communities to "reclaim and revalue their languages and cultures, and in so doing, improve the educational success of indigenous students."
Informal learning.
Informal learning is one of three forms of learning defined by the Organisation for Economic Co-operation and Development (OECD). Informal learning occurs in a variety of places, such as at home, work, and through daily interactions and shared relationships among members of society. For many learners this includes language acquisition, cultural norms and manners. Informal learning for young people is an ongoing process that also occurs in a variety of places, such as out of school time, in youth programs at community centers and media labs.
Informal learning usually takes place outside educational establishments, does not follow a specified curriculum and may originate accidentally, sporadically, in association with certain occasions, from changing practical requirements. It is not necessarily planned to be pedagogically conscious, systematic and according to subjects, but rather unconsciously incidental, holistically problem-related, and related to situation management and fitness for life. It is experienced directly in its "natural" function of everyday life and is often spontaneous.
The concept of 'education through recreation' was applied to childhood development in the 19th century. In the early 20th century, the concept was broadened to include young adults but the emphasis was on physical activities. L.P. Jacks, also an early proponent of lifelong learning, described education through recreation: "A master in the art of living draws no sharp distinction between his work and his play, his labour and his leisure, his mind and his body, his education and his recreation. He hardly knows which is which. He simply pursues his vision of excellence through whatever he is doing and leaves others to determine whether he is working or playing. To himself he always seems to be doing both. Enough for him that he does it well." Education through recreation is the opportunity to learn in a seamless fashion through all of life's activities. The concept has been revived by the University of Western Ontario to teach anatomy to medical students.
Self-directed learning.
Autodidacticism (also autodidactism) is a contemplative, absorbing process, of "learning on your own" or "by yourself", or as a self-teacher. Some autodidacts spend a great deal of time reviewing the resources of libraries and educational websites. One may become an autodidact at nearly any point in one's life. While some may have been informed in a conventional manner in a particular field, they may choose to inform themselves in other, often unrelated areas. Notable autodidacts include Abraham Lincoln (U.S. president), Srinivasa Ramanujan (mathematician), Michael Faraday (chemist and physicist), Charles Darwin (naturalist), Thomas Alva Edison (inventor), Tadao Ando (architect), George Bernard Shaw (playwright), Frank Zappa (composer, recording engineer, film director), and Leonardo da Vinci (engineer, scientist, mathematician).
Open education and e-learning.
"Main articles: Open education" and "E-learning"
In 2012, e-learning had grown at 14 times the rate of traditional learning. Open education is fast growing to become the dominant form of education, for many reasons such as its efficiency and results compared to traditional methods. Cost of education has been an issue throughout history, and a major political issue in most countries today. Open education is generally significantly cheaper than traditional campus based learning and in many cases even free. Many large university institutions are now starting to offer free or almost free full courses such as Harvard, MIT and Berkeley teaming up to form edX. Other universities offering open education are Stanford, Princeton, Duke, Johns Hopkins, Edinburgh, U. Penn, U. Michigan, U. Virginia, U. Washington, and Caltech. It has been called the biggest change in the way we learn since the printing press. Many people despite favorable studies on effectiveness may still desire to choose traditional campus education for social and cultural reasons.
The conventional merit-system degree is currently not as common in open education as it is in campus universities, although some open universities do already offer conventional degrees such as the Open University in the United Kingdom. Presently, many of the major open education sources offer their own form of certificate. Due to the popularity of open education, these new kind of academic certificates are gaining more respect and equal "academic value" to traditional degrees. Many open universities are working to have the ability to offer students standardized testing and traditional degrees and credentials.
There has been a culture forming around distance learning for people who are looking to enjoy the shared social aspects that many people value in traditional on-campus education, which is not often directly offered from open education. Examples of this are people in open education forming study groups, meetups and movements such as UnCollege.
Development goals.
Since 1909, the ratio of children in the developing world going to school has increased. Before then, a small minority of boys attended school. By the start of the 21st century, the majority of all children in most regions of the world attended school.
Universal Primary Education is one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain. Securing charitable funding from prospective donors is one particularly persistent problem. Researchers at the Overseas Development Institute have indicated that the main obstacles to receiving more funding for education include conflicting donor priorities, an immature aid architecture, and a lack of evidence and advocacy for the issue. Additionally, Transparency International has identified corruption in the education sector as a major stumbling block to achieving Universal Primary Education in Africa. Furthermore, demand in the developing world for improved educational access is not as high as foreigners have expected. Indigenous governments are reluctant to take on the recurrent costs involved. There is economic pressure from those parents who prefer their children to earn money in the short term rather than work towards the long-term benefits of education.
A study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole. Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:
Internationalization.
Nearly every country now has Universal Primary Education.
Similarities — in systems or even in ideas — that schools share internationally have led to an increase in international student exchanges. The European Socrates-Erasmus Program facilitates exchanges across European universities. The Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Programs such as the International Baccalaureate have contributed to the internationalization of education. The global campus online, led by American universities, allows free access to class materials and lecture files recorded during the actual classes.
Education and technology in developing countries.
Technology plays an increasingly significant role in improving access to education for people living in impoverished areas and developing countries. There are charities dedicated to providing infrastructures through which the disadvantaged may access educational materials, for example, the One Laptop per Child project.
The OLPC foundation, a group out of MIT Media Lab and supported by several major corporations, has a stated mission to develop a $100 laptop for delivering educational software. The laptops were widely available as of 2008. They are sold at cost or given away based on donations.
In Africa, the New Partnership for Africa's Development (NEPAD) has launched an "e-school program" to provide all 600,000 primary and high schools with computer equipment, learning materials and internet access within 10 years. An International Development Agency project called nabuur.com, started with the support of former American President Bill Clinton, uses the Internet to allow co-operation by individuals on issues of social development.
India is developing technologies that will bypass land-based telephone and Internet infrastructure to deliver distance learning directly to its students. In 2004, the Indian Space Research Organization launched EDUSAT, a communications satellite providing access to educational materials that can reach more of the country's population at a greatly reduced cost.
Private v public funding in developing countries.
Research into LCPSs (low cost private schools) found that over 5 years to July 2013, debate around LCPSs to achieving Education for All (EFA) objectives was polarised and finding growing coverage in international policy. The polarisation was due to disputes around whether the schools are affordable for the poor, reaching disadvantaged groups, provide quality education, supporting or undermining equality, and are financially sustainable.
The report examined the main challenges that development organisations which support LCPSs have encountered. Surveys suggest these types of schools are expanding across Africa and Asia and is attributed to excess demand. These surveys also found concern for:
The report said there were some cases of successful voucher and subsidy programmes; evaluations of international support to the sector are not widespread. Addressing regulatory ineffectiveness is a key challenge. Emerging approaches stress the importance of understanding the political economy of the market for LCPSs, specifically how relationships of power and accountability between users, government and private providers can produce better education outcomes for the poor.
Educational theory.
Purpose of schools.
Individual purposes for pursuing education can vary. The understanding of the goals and means of educational socialization processes may also differ according to the sociological paradigm used.
In the early years of schooling, the focus is generally around developing basic interpersonal communication and literacy skills in order to further ability to learn more complex skills and subjects. After acquiring these basic abilities, education is commonly focused towards individuals gaining necessary knowledge and skills to improve ability to create value and a livelihood for themselves. Satisfying personal curiosities (education for the sake of itself) and desire for personal development, to "better oneself" without career based reasons for doing so are also common reasons why people pursue education and use schools.
Education is often understood to be a means of overcoming handicaps, achieving greater equality and acquiring wealth and status for all (Sargent 1994). Learners can also be motivated by their interest in the subject area or specific skill they are trying to learn. Learner-responsibility education models are driven by the interest of the learner in the topic to be studied.
Education is often perceived as a place where children can develop according to their unique needs and potentialities with the purpose of developing every individual to their full potential.
Educational psychology.
Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Although the terms "educational psychology" and "school psychology" are often used interchangeably, researchers and theorists are likely to be identified as , whereas practitioners in schools or school-related settings are identified as school psychologists. Educational psychology is concerned with the processes of educational attainment in the general population and in sub-populations such as gifted children and those with specific disabilities.
Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. Educational psychology in turn informs a wide range of specialties within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks (Lucas, Blazek, & Raley, 2006).
Learning modalities.
There has been much interest in learning modalities and styles over the last two decades. The most commonly employed learning modalities are:
Other commonly employed modalities include musical, interpersonal, verbal, logical, and intrapersonal.
Dunn and Dunn focused on identifying relevant stimuli that may influence learning and manipulating the school environment, at about the same time as Joseph Renzulli recommended varying teaching strategies. Howard Gardner identified a wide range of modalities in his Multiple Intelligences theories. The Myers-Briggs Type Indicator and Keirsey Temperament Sorter, based on the works of Jung, focus on understanding how people's personality affects the way they interact personally, and how this affects the way individuals respond to each other within the learning environment. The work of David Kolb and Anthony Gregorc's Type Delineator follows a similar but more simplified approach.
Some theories propose that all individuals benefit from a variety of learning modalities, while others suggest that individuals may have preferred learning styles, learning more easily through visual or kinesthetic experiences. A consequence of the latter theory is that effective teaching should present a variety of teaching methods which cover all three learning modalities so that different students have equal opportunities to learn in a way that is effective for them. Guy Claxton has questioned the extent that learning styles such as VAK are helpful, particularly as they can have a tendency to label children and therefore restrict learning. Recent research has argued "there is no adequate evidence base to justify incorporating learning styles assessments into general educational practice."
Philosophy.
As an academic field, philosophy of education is "the philosophical study of education and its problems (...) its central subject matter is education, and its methods are those of philosophy". "The philosophy of education may be either the philosophy of the process of education or the philosophy of the discipline of education. That is, it may be part of the discipline in the sense of being concerned with the aims, forms, methods, or results of the process of educating or being educated; or it may be metadisciplinary in the sense of being concerned with the concepts, aims, and methods of the discipline." As such, it is both part of the field of education and a field of applied philosophy, drawing from fields of metaphysics, epistemology, axiology and the philosophical approaches (speculative, prescriptive, and/or analytic) to address questions in and about pedagogy, education policy, and curriculum, as well as the process of learning, to name a few. For example, it might study what constitutes upbringing and education, the values and norms revealed through upbringing and educational practices, the limits and legitimization of education as an academic discipline, and the relation between education theory and practice.
Curriculum.
In formal education, a curriculum is the set of courses and their content offered at a school or university. As an idea, curriculum stems from the Latin word for "race course", referring to the course of deeds and experiences through which children grow to become mature adults. A curriculum is prescriptive, and is based on a more general syllabus which merely specifies what topics must be understood and to what level to achieve a particular grade or standard.
An academic discipline is a branch of knowledge which is formally taught, either at the university–or via some other such method. Each discipline usually has several sub-disciplines or branches, and distinguishing lines are often both arbitrary and ambiguous. Examples of broad areas of academic disciplines include the natural sciences, mathematics, computer science, social sciences, humanities and applied sciences.
Educational institutions may incorporate fine arts as part of K-12 grade curricula or within majors at colleges and universities as electives. The various types of fine arts are music, dance, and theater.
Instruction.
Instruction is the facilitation of another's learning. Instructors in primary and secondary institutions are often called teachers, and they direct the education of students and might draw on many subjects like reading, writing, mathematics, science and history. Instructors in post-secondary institutions might be called teachers, instructors, or professors, depending on the type of institution; and they primarily teach only their specific discipline. Studies from the United States suggest that the quality of teachers is the single most important factor affecting student performance, and that countries which score highly on international tests have multiple policies in place to ensure that the teachers they employ are as effective as possible. With the passing of NCLB in the United States (No Child Left Behind), teachers must be highly qualified. A popular way to gauge teaching performance is to use student evaluations of teachers (SETS), but these evaluations have been criticized for being counterproductive to learning and inaccurate due to student bias.
Economics.
It has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth. Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested by rich countries. However, technology transfer requires knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation. Therefore, a country's ability to learn from the leader is a function of its stock of "human capital". Recent study of the determinants of aggregate economic growth have stressed the importance of fundamental economic institutions and the role of cognitive skills.
At the level of the individual, there is a large literature, generally related to the work of Jacob Mincer, on how earnings are related to the schooling and other human capital. This work has motivated a large number of studies, but is also controversial. The chief controversies revolve around how to interpret the impact of schooling. Some students who have indicated a high potential for learning, by testing with a high intelligence quotient, may not achieve their full academic potential, due to financial difficulties.
Economists Samuel Bowles and Herbert Gintis argued in 1976 that there was a fundamental conflict in American schooling between the egalitarian goal of democratic participation and the inequalities implied by the continued profitability of capitalist production on the other.

</doc>
<doc id="9253" url="http://en.wikipedia.org/wiki?curid=9253" title="Encyclopedia">
Encyclopedia

An encyclopedia or encyclopaedia (also spelled encyclopædia, see spelling differences) is a type of reference work or compendium holding a comprehensive summary of information from either all branches of knowledge or a particular branch of knowledge.
Encyclopedias are divided into articles or entries, which are usually accessed alphabetically by article name. Encyclopedia entries are longer and more detailed than those in most dictionaries. Generally speaking, unlike dictionary entries, which focus on linguistic information about words, encyclopedia articles focus on factual information to cover the thing or concept for which the article name stands.
Encyclopedias have existed for around 2,000 years; the oldest still in existence, "Naturalis Historia", was written in ca. AD 77 by Pliny the Elder. The modern encyclopedia evolved out of dictionaries around the 17th century. Historically, some encyclopedias were contained in one volume, but some, such as the "Encyclopædia Britannica", the "Enciclopedia Italiana" (62 volumes, 56.000 pages) or the world's largest "Enciclopedia universal ilustrada europeo-americana" (118 volumes, 105.000 pages), became huge multi-volume works. Some modern encyclopedias, such as Wikipedia, are electronic and are often freely available.
Etymology.
The word "encyclopedia" comes from the Koine Greek , transliterated "enkyklios paideia", meaning "general education" from "enkyklios" (ἐγκύκλιος), meaning "circular, recurrent, required regularly, general" and "paideia" (παιδεία), meaning "education, rearing of a child"; it was reduced to a single word due to an error by copyists of Latin manuscripts. Together, the phrase literally translates as "complete instruction" or "complete knowledge".
Copyists of Latin manuscripts took this phrase to be a single Greek word, "enkyklopaidia", with the same meaning, and this spurious Greek word became the New Latin word "encyclopaedia", which in turn came into English. Though the notion of a compendium of knowledge dates back thousands of years, the term was first used in the title of a book in 1517 by Johannes Aventinus: "Encyclopedia orbisque doctrinarum, hoc est omnium artium, scientiarum, ipsius philosophiae index ac divisio", and in 1538 by Joachimus Fortius Ringelbergius, "Lucubrationes vel potius absolutissima kyklopaideia" (Basel, 1538). The word "encyclopaedia" was first used as a noun in the title of his book by the Croatian encyclopedist Pavao Skalić in his "Encyclopaedia seu orbis disciplinarum tam sacrarum quam prophanarum epistemon" (Encyclopaedia, or Knowledge of the World of Disciplines, Basel, 1559). One of the oldest vernacular uses was by François Rabelais in his "Pantagruel" in 1532. Several encyclopedias have names that include the suffix "-p(a)edia", e.g., Banglapedia (on matters relevant for Bengal).
In British usage, the spellings "encyclopedia" and "encyclopaedia" are both current. In American usage, only the former is commonly used. The spelling "encyclopædia"—with the "æ" ligature—was frequently used in the 19th century and is increasingly rare, although it is retained in product titles such as "Encyclopædia Britannica" and others. The "Oxford English Dictionary" (1989) records "encyclopædia" and "encyclopaedia" as equal alternatives (in that order), and notes the "æ" would be obsolete except that it is preserved in works that have Latin titles. "Webster's Third New International Dictionary" (1997–2002) features "encyclopedia" as the main headword and "encyclopaedia" as a minor variant. In addition, "cyclopedia" and "cyclopaedia" are now rarely used shortened forms of the word originating in the 17th century.
Characteristics.
The modern encyclopedia was developed from the dictionary in the 18th century. Historically, both encyclopedias and dictionaries have been researched and written by well-educated, well-informed content experts, but they are significantly different in structure. A dictionary is a linguistic work which primarily focuses on alphabetical listing of words and their definitions. Synonymous words and those related by the subject matter are to be found scattered around the dictionary, giving no obvious place for in-depth treatment. Thus, a dictionary typically provides limited information, analysis or background for the word defined. While it may offer a definition, it may leave the reader lacking in understanding the meaning, significance or limitations of a term, and how the term relates to a broader field of knowledge. An encyclopedia is, allegedly, not written in order to convince, although one of its goals is indeed to convince its reader about its own veracity. In the terms of Aristotle's Modes of persuasion, a dictionary should persuade the reader through "logos" (conveying only appropriate emotions); it will be expected to have a lack of pathos (it should not stir up irrelevant emotions), and to have little ethos except that of the dictionary itself.
To address those needs, an encyclopedia article is typically not limited to simple definitions, and is not limited to defining an individual word, but provides a more extensive meaning for a "subject or discipline". In addition to defining and listing synonymous terms for the topic, the article is able to treat the topic's more extensive meaning in more depth and convey the most relevant accumulated knowledge on that subject. An encyclopedia article also often includes many maps and illustrations, as well as bibliography and statistics.
Four major elements define an encyclopedia: its subject matter, its scope, its method of organization, and its method of production:
Some works entitled "dictionaries" are actually similar to encyclopedias, especially those concerned with a particular field (such as the "Dictionary of the Middle Ages", the "Dictionary of American Naval Fighting Ships", and "Black's Law Dictionary"). The "Macquarie Dictionary," Australia's national dictionary, became an encyclopedic dictionary after its first edition in recognition of the use of proper nouns in common communication, and the words derived from such proper nouns.
There are some broad differences between encyclopedias and dictionaries. Most noticeably, encyclopedia articles are longer, fuller and more thorough than entries in most general-purpose dictionaries. There are differences in content as well. Generally speaking, dictionaries provide linguistic information about words themselves, while encyclopedias focus more on the thing for which those words stand. Thus, while dictionary entries are inextricably fixed to the word described, encyclopedia articles can be given a different entry name. As such, dictionary entries are not fully translatable into other languages, but encyclopedia articles can be.
In practice, however, the distinction is not concrete, as there is no clear-cut difference between factual, "encyclopedic" information and linguistic information such as appears in dictionaries. Thus encyclopedias may contain material that is also found in dictionaries, and vice versa. In particular, dictionary entries often contain factual information about the thing named by the word.
History.
Encyclopedias have progressed from the beginning of history in written form, through medieval and modern times in print, and most recently, displayed on computer and distributed via computer networks.
Ancient times.
One of the earliest encyclopedic works to have survived to modern times is the "Naturalis Historia" of Pliny the Elder, a Roman statesman living in the 1st century AD. He compiled a work of 37 chapters covering natural history, architecture, medicine, geography, geology, and all aspects of the world around him. He stated in the preface that he had compiled 20,000 facts from 2000 works by over 200 authors, and added many others from his own experience. The work was published around AD 77-79, although he probably never finished proofing the work before his death in the eruption of Vesuvius in AD 79.
Middle Ages.
Saint Isidore of Seville, one of the greatest scholars of the early Middle Ages, is widely recognized as being the author of the first known encyclopedia of the Middle Ages, the "Etymologiae" or "Origines" (around 630), in which he compiled a sizable portion of the learning available at his time, both ancient and modern. The encyclopedia has 448 chapters in 20 volumes, and is valuable because of the quotes and fragments of texts by other authors that would have been lost had they not been collected by Saint Isidore.
The most popular encyclopedia of the Carolingian Age was the "De universo" or "De rerum naturis" by Rabanus Maurus, written about 830, which was based on "Etymologiae".
The early Muslim compilations of knowledge in the Middle Ages included many comprehensive works. Around year 960, the Brethren of Purity of Basra were engaged in their Encyclopedia of the Brethren of Purity. Notable works include Abu Bakr al-Razi's encyclopedia of science, the Mutazilite Al-Kindi's prolific output of 270 books, and Ibn Sina's medical encyclopedia, which was a standard reference work for centuries. Also notable are works of universal history (or sociology) from Asharites, al-Tabri, al-Masudi, Tabari's "History of the Prophets and Kings", Ibn Rustah, al-Athir, and Ibn Khaldun, whose Muqadimmah contains cautions regarding trust in written records that remain wholly applicable today.
The enormous encyclopedic work in China of the "Four Great Books of Song", compiled by the 11th century AD during the early Song Dynasty (960–1279), was a massive literary undertaking for the time. The last encyclopedia of the four, the "Prime Tortoise of the Record Bureau", amounted to 9.4 million Chinese characters in 1,000 written volumes.
Renaissance.
These works were all hand copied and thus rarely available, beyond wealthy patrons or monastic men of learning: they were expensive, and usually written for those extending knowledge rather than those using it.
During Renaissance the creation of printing allowed a wider diffusion of encyclopedias and every scholar could have his or her own copy. The "De expetendis et fugiendis rebus" by Giorgio Valla was posthumously printed in 1501 by Aldo Manuzio in Venice. This work followed the traditional scheme of liberal arts. However, Valla added the translation of ancient Greek works on mathematics (firstly by Archimedes), newly discovered and translated. The "Margarita Philosophica" by Gregor Reisch, printed in 1503, was a complete encyclopedia explaining the seven liberal arts.
The term encyclopaedia was coined by 16th century humanists who misread copies of their texts of Pliny and Quintilian, and combined the two Greek words "enkyklios paideia" into one word, έγκυκλοπαιδεία. The phrase "enkyklios paideia" (ἐγκύκλιος παιδεία) was used by Plutarch and the Latin word Encyclopedia came from him.
The first work titled in this way was the "Encyclopedia orbisque doctrinarum, hoc est omnium artium, scientiarum, ipsius philosophiae index ac divisio" written by Johannes Aventinus in 1517.
The English physician and philosopher, Sir Thomas Browne used the word 'encyclopaedia' in 1646 in the preface to define his "Pseudodoxia Epidemica", a major work of the 17th-century scientific revolution. Browne structured his encyclopaedia upon the time-honoured schemata of the Renaissance, the so-called 'scale of creation' which ascends through the mineral, vegetable, animal, human, planetary and cosmological worlds. "Pseudodoxia Epidemica" was a European best-seller, translated into French, Dutch and German as well as Latin it went through no less than five editions, each revised and augmented, the last edition appearing in 1672.
18th–19th centuries.
The beginnings of the modern idea of the general-purpose, widely distributed printed encyclopedia precede the 18th century encyclopedists. However, Chambers' "Cyclopaedia, or Universal Dictionary of Arts and Sciences" (1728), and the "Encyclopédie" of Denis Diderot and Jean le Rond d'Alembert (1751 onwards), as well as "Encyclopædia Britannica" and the "Conversations-Lexikon", were the first to realize the form we would recognize today, with a comprehensive scope of topics, discussed in depth and organized in an accessible, systematic method. Chambers, in 1728, followed the earlier lead of John Harris's "Lexicon Technicum" of 1704 and later editions (see also below); this work was by its title and content "A Universal English Dictionary of Arts and Sciences: Explaining not only the Terms of Art, but the Arts Themselves".
During the 19th and early 20th century, many smaller or less developed languages saw their first encyclopedias, using French, German, and English role models. While encyclopedias in larger languages, having large markets that could support a large editorial staff, churned out new 20-volume works in a few years and new editions with brief intervals, such publication plans often spanned a decade or more in smaller language.
20th century.
Popular and affordable encyclopedias such as Harmsworth's Universal Encyclopaedia and the Children's Encyclopaedia appeared in the early 1920s.
In the United States, the 1950s and 1960s saw the introduction of several large popular encyclopedias, often sold on installment plans. The best known of these were "World Book" and "Funk and Wagnalls".
The second half of the 20th century also saw the proliferation of specialized encyclopedias that compiled topics in specific fields. This trend has continued. Encyclopedias of at least one volume in size now exist for most if not all academic disciplines, including such narrow topics such as bioethics and African American history.
By the late 20th century, encyclopedias were being published on CD-ROMs for use with personal computers. Microsoft's "Encarta", launched in 1993, was a landmark example as it had no printed equivalent. Articles were supplemented with both video and audio files as well as numerous high-quality images. After sixteen years, Microsoft discontinued the Encarta line of products in 2009.
21st century.
In 2001, Jimmy Wales and Larry Sanger launched Wikipedia, a collaboratively edited, multilingual, open-source, free Internet encyclopedia that is supported by the non-profit Wikimedia Foundation.

</doc>
<doc id="9256" url="http://en.wikipedia.org/wiki?curid=9256" title="Enigma machine">
Enigma machine

An Enigma machine was any of a family of related electro-mechanical rotor cipher machines used in the twentieth century for enciphering and deciphering secret messages. Enigma was invented by the German engineer Arthur Scherbius at the end of World War I. Early models were used commercially from the early 1920s, and adopted by military and government services of several countries—most notably by Nazi Germany before and during World War II. Several different Enigma models were produced, but the German military models are the most commonly discussed.
German military texts enciphered on the Enigma machine were first broken by the Polish Cipher Bureau, beginning in December 1932. This success was a result of efforts by three Polish cryptologists, Marian Rejewski, Jerzy Różycki and Henryk Zygalski, working for Polish military intelligence. Rejewski "reverse-engineered" the device, using theoretical mathematics and material supplied by French military intelligence. Subsequently the three mathematicians designed mechanical devices for breaking Enigma ciphers, including the cryptologic bomb. From 1938 onwards, additional complexity was repeatedly added to the Enigma machines, making decryption more difficult and necessitating larger numbers of equipment and personnel—more than the Poles could readily produce.
On 25 July 1939, in Warsaw, the Poles initiated French and British military intelligence representatives into their Enigma-decryption techniques and equipment, including Zygalski sheets and the cryptologic bomb, and promised each delegation a Polish-reconstructed Enigma. The demonstration represented a vital basis for the later British continuation and effort. During the war, British cryptologists decrypted a vast number of messages enciphered on Enigma. The intelligence gleaned from this source, codenamed "Ultra" by the British, was a substantial aid to the Allied war effort.
Though Enigma had some cryptographic weaknesses, in practice it was German procedural flaws, operator mistakes, laziness, failure to systematically introduce changes in encipherment procedures, and Allied capture of key tables and hardware that, during the war, enabled Allied cryptologists to succeed.
The exact influence of Ultra on the course of the war is debated; an oft-repeated assessment is that decryption of German ciphers advanced the end of the European war by two years. Winston Churchill told the United Kingdom's King George VI after World War II: "It was thanks to Ultra that we won the war."
Design.
Like other rotor machines, the Enigma machine is a combination of mechanical and electrical subsystems. The mechanical subsystem consists of a keyboard; a set of rotating disks called "rotors" arranged adjacently along a spindle; and one of various stepping components to turn one or more rotor with each key press.
Electrical pathway.
The mechanical parts act in such a way as to form a varying electrical circuit. When a key is pressed, a circuit is completed. Current flows through various components in their current configuration, ultimately lighting one display lamp, revealing an output letter. For example, when encrypting a message starting "ANX...", the operator would first press the "A" key, and the "Z" lamp might light, so "Z" would be the first letter of the ciphertext. The operator would next press "N", and then "X" in the same fashion, and so on.
The detailed operation of Enigma is shown in the wiring diagram to the left. To simplify the example, only four components of a complete Enigma machine are shown. In reality, there are 26 lamps and keys, rotor wirings inside the rotors (of which there are either three or four) and between six and ten plug leads.
Current flowed from the battery (1) through a depressed bi-directional keyboard switch (2) to the plugboard (3). Next, it passed through the (unused in this instance, so shown closed) plug "A" (3) via the entry wheel (4), through the wiring of the three (Wehrmacht Enigma) or four ("Kriegsmarine" M4 and "Abwehr" variants) installed rotors (5), and entered the reflector (6). The reflector returned the current, via an entirely different path, back through the rotors (5) and entry wheel (4), proceeding through plug "S" (7) connected with a cable (8) to plug "D", and another bi-directional switch (9) to light the appropriate lamp.
The repeated changes of electrical path through an Enigma scrambler implemented a polyalphabetic substitution cipher that provided Enigma's security. The diagram on the right shows how the electrical pathway changed with each key depression, which caused rotation of at least the right-hand rotor. Current passed into the set of rotors, into and back out of the reflector, and out through the rotors again. The greyed-out lines are other possible paths within each rotor; these are hard-wired from one side of each rotor to the other. The letter "A" encrypts differently with consecutive key presses, first to "G", and then to "C". This is because the right-hand rotor has stepped, sending the signal on a completely different route. Eventually other rotors step with a key press.
Rotors.
The rotors (alternatively "wheels" or "drums", "Walzen" in German) formed the heart of an Enigma machine. Each rotor was a disc approximately in diameter made from hard rubber or bakelite with brass spring-loaded pins on one face arranged in a circle; on the other side are a corresponding number of circular electrical contacts. The pins and contacts represent the alphabet—typically the 26 letters A–Z (this will be assumed for the rest of this description). When the rotors were mounted side-by-side on the spindle, the pins of one rotor rested against the contacts of the neighbouring rotor, forming an electrical connection. Inside the body of the rotor, 26 wires connected each pin on one side to a contact on the other in a complex pattern. Most of the rotors were identified by Roman numerals, and each issued copy of rotor I was wired identically to all others. The same was true for the special thin beta and gamma rotors used in the M4 naval variant.
By itself, a rotor performs only a very simple type of encryption—a simple substitution cipher. For example, the pin corresponding to the letter "E" might be wired to the contact for letter "T" on the opposite face, and so on. Enigma's security came from using several rotors in series (usually three or four) and the regular stepping movement of the rotors, thus implementing a polyalphabetic substitution cipher.
When placed in an Enigma, each rotor can be set to one of 26 possible positions. When inserted, it can be turned by hand using the grooved finger-wheel, which protrudes from the internal Enigma cover when closed. So that the operator can know the rotor's position, each had an "alphabet tyre" (or letter ring) attached to the outside of the rotor disk, with 26 characters (typically letters); one of these could be seen through the window, thus indicating the rotational position of the rotor. In early models, the alphabet ring was fixed to the rotor disk. A later improvement was the ability to adjust the alphabet ring relative to the rotor disk. The position of the ring was known as the "Ringstellung" ("ring setting"), and was a part of the initial setting prior to an operating session. In modern terms it was a part of the initialization vector.
Each rotor contained a notch (or more than one) that controlled rotor stepping. In the military variants, the notches are located on the alphabet ring.
The Army and Air Force Enigmas were used with several rotors, initially three. On 15 December 1938, this changed to five, from which three were chosen for a given session. Rotors were marked with Roman numerals to distinguish them: I, II, III, IV and V, all with single notches located at different points on the alphabet ring. This variation was probably intended as a security measure, but ultimately allowed the Polish Clock Method and British Banburismus attacks.
The Naval version of the "Wehrmacht" Enigma had always been issued with more rotors than the other services: at first six, then seven, and finally eight. The additional rotors were marked VI, VII and VIII, all with different wiring, and had two notches, resulting in more frequent turnover. The four-rotor Naval Enigma (M4) machine accommodated an extra rotor in the same space as the three-rotor version. This was accomplished by replacing the original reflector with a thinner one and by adding a thin fourth rotor. That fourth rotor was one of two types, "Beta" or "Gamma", and never stepped, but could be manually set to any of 26 positions. One of the 26 made the machine perform identically to the three-rotor machine.
Stepping.
To avoid merely implementing a simple (and easily breakable) substitution cipher, every key press caused one or more rotors to step by one twenty-sixth of a full rotation, before the electrical connections were made. This changed the substitution alphabet used for encryption, ensuring that the cryptographic substitution was different at each new rotor position, producing a more formidable polyalphabetic substitution cipher. The stepping mechanism varied slightly from model to model. The right-hand rotor stepped once with each keystroke, and other rotors stepped less frequently.
Turnover.
The advancement of a rotor other than the left-hand one was called a "turnover" by the British. This was achieved by a ratchet and pawl mechanism. Each rotor had a ratchet with 26 teeth and every time a key was pressed, the set of spring-loaded pawls moved forward in unison, trying to engage with a ratchet. The alphabet ring of the rotor to the right normally prevented this. As this ring rotated with its rotor, a notch machined into it would eventually align itself with the pawl, allowing it to engage with the ratchet, and advance the rotor on its left. The right-hand pawl, having no rotor and ring to its right, stepped its rotor with every key depression. For a single-notch rotor in the right-hand position, the middle rotor stepped once for every 26 steps of the right-hand rotor. Similarly for rotors two and three. For a two-notch rotor, the rotor to its left would turn over twice for each rotation.
The first five rotors to be introduced (I–V) contained one notch each, while the additional naval rotors VI, VII and VIII each had two notches. The position of the notch on each rotor was determined by the letter ring which could be adjusted in relation to the core containing the interconnections. The points on the rings at which they caused the next wheel to move were as follows.
The design also included a feature known as "double-stepping". This occurred when each pawl aligned with both the ratchet of its rotor and the rotating notched ring of the neighbouring rotor. If a pawl engaged with a ratchet through alignment with a notch, as it moved forward it pushed against both the ratchet and the notch, advancing both rotors. In a three-rotor machine, double-stepping affected rotor two only. If in moving forward the ratchet of rotor three was engaged, rotor two would move again on the subsequent keystroke, resulting in two consecutive steps. Rotor two also pushes rotor one forward after 26 steps, but since rotor one moves forward with every keystroke anyway, there is no double-stepping. This double-stepping caused the rotors to deviate from odometer-style regular motion.
With three wheels and only single notches in the first and second wheels, the machine had a period of 26 × 25 × 26 = 16,900 (not 26 × 26 × 26, because of double-stepping). Historically, messages were limited to a few hundred letters, and so there was no chance of repeating any combined rotor position during a single session, denying cryptanalysts valuable clues.
To make room for the Naval fourth rotors, the reflector was made much thinner. The fourth rotor fitted into the space made available. No other changes were made, which eased the changeover. Since there were only three pawls, the fourth rotor never stepped, but could be manually set into one of 26 possible positions.
A device that was designed, but not implemented before the war's end, was the "Lückenfüllerwalze" (gap-fill wheel) that implemented irregular stepping. It allowed field configuration of notches in all 26 positions. If the number of notches was a relative prime of 26 and the number of notches were different for each wheel, the stepping would be more unpredictable. Like the Umkehrwalze-D it also allowed the internal wiring to be reconfigured.
Entry wheel.
The current entry wheel ("Eintrittswalze" in German), or entry stator, connects the plugboard to the rotor assembly. If the plugboard is not present, the entry wheel instead connects the keyboard and lampboard to the rotor assembly. While the exact wiring used is of comparatively little importance to security, it proved an obstacle to Rejewski's progress during his study of the rotor wirings. The commercial Enigma connects the keys in the order of their sequence on the keyboard: "Q"formula_1"A", "W"formula_1"B", "E"formula_1"C" and so on. However, the military Enigma connects them in straight alphabetical order: "A"formula_1"A", "B"formula_1"B", "C"formula_1"C", and so on. It took inspired guesswork for Rejewski to penetrate the modification.
Reflector.
With the exception of models "A" and "B", the last rotor came before a 'reflector' (German: "Umkehrwalze", meaning 'reversal rotor'), a patented feature unique to Enigma among the period's various rotor machines. The reflector connected outputs of the last rotor in pairs, redirecting current back through the rotors by a different route. The reflector ensured that Enigma is self-reciprocal: conveniently, encryption was the same as decryption. However, the reflector also gave Enigma the property that no letter ever encrypted to itself. This was a severe conceptual flaw and a cryptological mistake subsequently exploited by codebreakers.
In Model 'C', the reflector could be inserted in one of two different positions. In Model 'D', the reflector could be set in 26 possible positions, although it did not move during encryption. In the "Abwehr" Enigma, the reflector stepped during encryption in a manner like the other wheels.
In the German Army and Air Force Enigma, the reflector was fixed and did not rotate; there were four versions. The original version was marked 'A', and was replaced by "Umkehrwalze B" on 1 November 1937. A third version, "Umkehrwalze C" was used briefly in 1940, possibly by mistake, and was solved by Hut 6. The fourth version, first observed on 2 January 1944, had a rewireable reflector, called "Umkehrwalze D", allowing the Enigma operator to alter the connections as part of the key settings.
Plugboard.
The plugboard (Steckerbrett in German) permitted variable wiring that could be reconfigured by the operator (visible on the front panel of Figure 1; some of the patch cords can be seen in the lid). It was introduced on German Army versions in 1930, and was soon adopted by the Navy. The plugboard contributed more cryptographic strength than an extra rotor. Enigma without a plugboard (known as "unsteckered Enigma") can be solved relatively straightforwardly using hand methods; these techniques are generally defeated by the plugboard, driving Allied cryptanalysts to special machines to solve it.
A cable placed onto the plugboard connected letters in pairs; for example, "E" and "Q" might be a steckered pair. The effect was to swap those letters before and after the main rotor scrambling unit. For example, when an operator presses "E", the signal was diverted to "Q" before entering the rotors. Up to 13 steckered pairs might be used at one time, although only 10 were normally used.
Current flowed from the keyboard through the plugboard, and proceeded to the entry-rotor or "Eintrittswalze". Each letter on the plugboard had two jacks. Inserting a plug disconnected the upper jack (from the keyboard) and the lower jack (to the entry-rotor) of that letter. The plug at the other end of the crosswired cable was inserted into another letter's jacks, thus switching the connections of the two letters.
Accessories.
Other features made various Enigma machines more secure or more convenient.
Some M4 Enigmas used the "Schreibmax", a small printer that could print the 26 letters on a narrow paper ribbon. This eliminated the need for a second operator to read the lamps and transcribe the letters. The "Schreibmax" was placed on top of the Enigma machine and was connected to the lamp panel. To install the printer, the lamp cover and light bulbs had to be removed. It improved both convenience and operational security; the printer could be installed remotely such that the signal officer operating the machine no longer had to see the decrypted plaintext.
Another accessory was the remote lamp panel "Fernlesegerät". For machines equipped with the extra panel, the wooden case of the Enigma was wider and could store the extra panel. A lamp panel version could be connected afterwards, but that required, as with the "Schreibmax", that the lamp panel and lightbulbs be removed. The remote panel made it possible for a person to read the decrypted plaintext without the operator seeing it.
In 1944, the "Luftwaffe" introduced a plugboard switch, called the "Uhr" (clock), a small box containing a switch with 40 positions. It replaced the standard plugs. After connecting the plugs, as determined in the daily key sheet, the operator turned the switch into one of the 40 positions, each producing a different combination of plug wiring. Most of these plug connections were, unlike the default plugs, not pair-wise. In one switch position, the "Uhr" did not swap letters, but simply emulated the 13 stecker wires with plugs.
Mathematical analysis.
The Enigma transformation for each letter can be specified mathematically as a product of permutations. Assuming a three-rotor German Army/Air Force Enigma, let formula_7 denote the plugboard transformation, formula_8 denote that of the reflector, and formula_9 denote those of the left, middle and right rotors respectively. Then the encryption formula_10 can be expressed as
After each key press, the rotors turn, changing the transformation. For example, if the right-hand rotor formula_12 is rotated formula_13 positions, the transformation becomes formula_14, where formula_15 is the cyclic permutation mapping "A" to "B", "B" to "C", and so forth. Similarly, the middle and left-hand rotors can be represented as formula_16 and formula_17 rotations of formula_18 and formula_19. The encryption transformation can then be described as
Combining three rotors from a set of five, the rotor settings with 26 positions, and the plugboard with ten pairs of letters connected, the military Enigma has 158,962,555,217,826,360,000 (158 quintillion) different settings.
Operation.
In use, the Enigma required a list of daily key settings and auxiliary documents. The procedures for German Naval Enigma were more elaborate and more secure than those in other services. Navy codebooks were printed in red, water-soluble ink on pink paper so that they could easily be destroyed if they were endangered.
In German military practice, communications were divided into separate networks, each using different settings. These communication nets were termed "keys" at Bletchley Park, and were assigned code names, such as "Red", "Chaffinch", and "Shark". Each unit operating in a network was assigned a settings list for its Enigma for a period of time. For a message to be correctly encrypted and decrypted, both sender and receiver had to configure their Enigma in the same way; rotor selection and order, starting position and plugboard connections must be identical. All these settings (together the key in modern terms) were established beforehand, distributed in codebooks.
An Enigma machine's initial state, the cryptographic key, has several aspects:
Note that although the "ringstellung" was a required part of the setup, they did not affect encryption because the rotors were positioned independently of the rings. The ring settings were only necessary to determine the initial rotor position based on the "message setting" that was transmitted at the beginning of a message, as described in the "Indicators" section, below. Once the receiver's rotors were set to the indicated positions, the ring settings no longer played any role.
In modern cryptographic language, the ring settings did not actually contribute entropy to the key used for encrypting the message. Rather, the ring settings were part of a separate key (along with the rest of the setup such as wheel order and plug settings) used to encrypt an "initialization vector" for the message. The session key consisted of the complete setup "except for" the ring settings, plus the initial rotor positions chosen arbitrarily by the sender (the "message setting"). The important part of this session key was the rotor positions, not the ring positions. However, by "encoding" the rotor position into the ring position using the ring settings, additional variability was added to the encryption of the initialization vector.
Enigma was designed to be secure even if the rotor wiring was known to an opponent, although in practice considerable effort protected the wiring configuration. If the wiring is secret, the total number of possible configurations has been calculated to be around 10114 (approximately 380 bits); with known wiring and other operational constraints, this is reduced to around 1023 (76 bits). Users of Enigma were confident of its security because of the large number of possibilities; it was not then feasible for an adversary to even begin to try a brute force attack.
Indicator.
Most of the key was kept constant for a set time period, typically a day. However, a different initial rotor position was used for each message, a concept similar to an initialisation vector in modern cryptography. The reason is that encrypting many messages with identical or near-identical settings (termed in cryptanalysis as being "in depth"), would enable an attack using a statistical procedure such as Friedman's Index of coincidence. The starting position for the rotors was transmitted just before the ciphertext, usually after having been enciphered. The exact method used was termed the "indicator procedure". Design weakness and operator sloppiness in these indicator procedures were two of the main weakness that made cracking Enigma possible.
One of the earliest "indicator procedures" was used by Polish cryptanalysts to make the initial breaks into the Enigma. The procedure was for the operator to set up his machine in accordance with his settings list, which included a global initial position for the rotors (the "Grundstellung", meaning "ground setting"), say, "AOH". The operator turned his rotors until "AOH" was visible through the rotor windows. At that point, the operator chose his own arbitrary starting position for that particular message. An operator might select "EIN", and these became the "message settings" for that encryption session. The operator then typed "EIN" into the machine, twice, to allow for detection of transmission errors. The results were an encrypted indicator—the "EIN" typed twice might turn into "XHTLOA", which would be transmitted along with the message. Finally, the operator then spun the rotors to his message settings, "EIN" in this example, and typed the plaintext of the message.
At the receiving end, the operation was reversed. The operator set the machine to the initial settings and typed in the first six letters of the message ("XHTLOA"). In this example, "EINEIN" emerged on the lamps. After moving his rotors to "EIN", the receiving operator then typed in the rest of the ciphertext, deciphering the message.
The weakness in this indicator scheme came from two factors. First, use of a global ground setting—this was later changed so the operator selected his initial position to encrypt the indicator, and sent the initial position in the clear. The second problem was the repetition of the indicator, which was a serious security flaw. The message setting was encoded twice, resulting in a relation between first and fourth, second and fifth, and third and sixth character. This security problem enabled the Polish Cipher Bureau to break into the pre-war Enigma system as early as 1932. However, from 1940 on, the Germans changed procedure.
During World War II, codebooks were only used each day to set up the rotors, their ring settings and the plugboard. For each message, the operator selected a random start position, let's say "WZA", and a random message key, perhaps "SXT". He moved the rotors to the "WZA" start position and encoded the message key "SXT". Assume the result was "UHL". He then set up the message key, "SXT", as the start position and encrypted the message. Next, he transmitted the start position, "WZA", the encoded message key, "UHL", and then the ciphertext. The receiver set up the start position according to the first trigram, "WZA", and decoded the second trigram, "UHL", to obtain the "SXT" message setting. Next, he used this "SXT" message setting as the start position to decrypt the message. This way, each ground setting was different and the new procedure avoided the security flaw of double encoded message settings.
This procedure was used by "Wehrmacht" and "Luftwaffe" only. The "Kriegsmarine" procedures on sending messages with the Enigma were far more complex and elaborate. Prior to encryption the message was encoded using the "Kurzsignalheft" code book. The "Kurzsignalheft" contained tables to convert sentences into four-letter groups. A great many choices were included, for example, logistic matters such as refueling and rendezvous with supply ships, positions and grid lists, harbor names, countries, weapons, weather conditions, enemy positions and ships, date and time tables. Another codebook contained the "Kenngruppen" and "Spruchschlüssel": the key identification and message key.
Additional details.
The Army Enigma machine used only the 26 alphabet characters. Signs were replaced with rare character combinations. A space was omitted or replaced with an X. The X was generally used as point or full-stop.
Some signs were different in other parts of the armed forces. The "Wehrmacht" replaced a comma with ZZ and the question sign with FRAGE or FRAQ.
The "Kriegsmarine" replaced the comma with Y and the question sign with UD. The combination CH, as in ""Acht" (eight) or "Richtung"" (direction), was replaced with Q (AQT, RIQTUNG). Two, three and four zeros were replaced with CENTA, MILLE and MYRIA.
The "Wehrmacht" and the "Luftwaffe" transmitted messages in groups of five characters.
The "Kriegsmarine", using the four rotor Enigma, had four-character groups. Frequently used names or words were varied as much as possible. Words like "Minensuchboot" (minesweeper) could be written as MINENSUCHBOOT, MINBOOT, MMMBOOT or MMM354. To make cryptanalysis harder, messages were limited to 250 characters. Longer messages were divided into several parts, each using a different message key.
History.
The Enigma family included multiple designs. The earliest were commercial models dating from the early 1920s. Starting in the mid-1920s, the German military began to use Enigma, making a number of security-related changes. Various nations either adopted or adapted the design for their own cipher machines.
Commercial Enigma.
On 23 February 1918, German engineer Arthur Scherbius applied for a patent for a cipher machine using rotors and, with E. Richard Ritter, founded the firm of Scherbius & Ritter. They approached the German Navy and Foreign Office with their design, but neither was interested. They then assigned the patent rights to Gewerkschaft Securitas, who founded the "Chiffriermaschinen Aktien-Gesellschaft" (Cipher Machines Stock Corporation) on 9 July 1923; Scherbius and Ritter were on the board of directors.
Chiffriermaschinen AG began advertising a rotor machine—"Enigma model A"—which was exhibited at the Congress of the International Postal Union in 1923–1924. The machine was heavy and bulky, incorporating a typewriter. It measured 65×45×35 cm and weighed about .
In 1925 Enigma "model B" was introduced, and was of a similar construction. While bearing the Enigma name, both models "A" and "B" were quite unlike later versions: they differed in physical size and shape, but also cryptographically, in that they lacked the reflector.
The reflector—suggested by Scherbius's colleague Willi Korn—was introduced in "Enigma C" (1926).
"Model C" was smaller and more portable than its predecessors. It lacked a typewriter, relying on the operator; hence the informal name of "glowlamp Enigma" to distinguish it from models "A" and "B".
The "Enigma C" quickly gave way to "Enigma D" (1927). This version was widely used, with shipments to Sweden, the Netherlands, United Kingdom, Japan, Italy, Spain, United States and Poland.
Military Enigma.
The Navy was the first military branch to adopt Enigma. This version, named "Funkschlüssel C" ("Radio cipher C"), had been put into production by 1925 and was introduced into service in 1926.
The keyboard and lampboard contained 29 letters—A-Z, Ä, Ö and Ü—which were arranged alphabetically, as opposed to the QWERTZU ordering. The rotors had 28 contacts, with the letter "X" wired to bypass the rotors unencrypted.
Three rotors were chosen from a set of five and the reflector could be inserted in one of four different positions, denoted α, β, γ and δ. The machine was revised slightly in July 1933.
By 15 July 1928, the German Army ("Reichswehr") had introduced their own version of the Enigma—the "Enigma G", revised to the "Enigma I" by June 1930. Enigma I is also known as the "Wehrmacht", or "Services" Enigma, and was used extensively by German military services and other government organisations (such as the railways), before and during World War II.
The major difference between "Enigma I" and commercial Enigma models was the addition of a plugboard to swap pairs of letters, greatly increasing cryptographic strength. Other differences included the use of a fixed reflector and the relocation of the stepping notches from the rotor body to the movable letter rings. The machine measured 28×34×15 cm (11 in×13.5 in×6 in) and weighed around .
By 1930, the Army had suggested that the Navy adopt their machine, citing the benefits of increased security (with the plugboard) and easier interservice communications. The Navy eventually agreed and in 1934 brought into service the Navy version of the Army Enigma, designated "Funkschlüssel" ' or "M3". While the Army used only three rotors at that time, the Navy specified a choice of three from a possible five.
In December 1938, the Army issued two extra rotors so that the three rotors were chosen from a set of five. In 1938, the Navy added two more rotors, and then another in 1939 to allow a choice of three rotors from a set of eight. In August 1935, the Air Force introduced the Wehrmacht Enigma for their communications.
A four-rotor Enigma was introduced by the Navy for U-boat traffic on 1 February 1942, called "M4" (the network was known as "Triton", or "Shark" to the Allies). The extra rotor was fitted in the same space by splitting the reflector into a combination of a thin reflector and a thin fourth rotor.
There was also a large, eight-rotor printing model, the "Enigma II". In 1933 the Polish Cipher Bureau detected that it was in use for high-level military communications, but that it was soon withdrawn, as it was unreliable and jammed frequently.
The "Abwehr" used the "Enigma G" (the "Abwehr" Enigma). This Enigma variant was a four-wheel unsteckered machine with multiple notches on the rotors. This model was equipped with a counter which incremented upon each key press, and so is also known as the "counter machine" or the "Zählwerk" Enigma.
Other countries.
Other countries used Enigma machines. The Italian Navy adopted the commercial Enigma as "Navy Cipher D". The Spanish also used commercial Enigma during their Civil War. British codebreakers succeeded in breaking these machines, which lacked a plugboard. Not only militaries used the Enigma, they were also used by diplomatic services.
The Swiss used a version of Enigma called "model K" or "Swiss K" for military and diplomatic use, which was very similar to commercial Enigma D. The machine was cracked by Poland, France, the United Kingdom and the United States (the latter codenamed it INDIGO). An "Enigma T" model (codenamed "Tirpitz") was used by Japan.
An estimated 100,000 Enigma machines were constructed. After the end of World War II, the Allies sold captured Enigma machines, still widely considered secure, to developing countries. As these countries did not know that the machine had been broken, their supposedly secure communications were being read regularly by the major Western intelligence agencies.
Surviving machines.
The effort to break the Enigma was not disclosed until the 1970s. Since then, interest in the Enigma machine has grown. Enigmas are on public display in museums around the world.
The Deutsches Museum in Munich has both the three- and four-rotor German military variants, as well as several civilian versions. Enigma machines are exhibited at the National Codes Centre in Bletchley Park, the Government Communications Headquarters, the Science Museum in London, the Polish Institute and Sikorski Museum in London, the Polish Army Museum in Warsaw, the "Armémuseum" (Swedish Army Museum) in Stockholm, the National Signals Museum in Finland, and at the Australian War Memorial and in the foyer of the Defence Signals Directorate, both in Canberra, Australia.
In the United States, Enigma machines can be seen at the Computer History Museum in Mountain View, California, and at the National Security Agency's National Cryptologic Museum in Fort Meade, Maryland, where visitors can try their hand at enciphering and deciphering messages. Two machines that were acquired after the capture of during World War II are on display at the Museum of Science and Industry in Chicago, Illinois. The San Diego State University Library has a machine. A four rotor device is on display in the ANZUS Corridor of the The Pentagon on the second floor, A ring, between corridors 9 and 10. This machine is on loan from Australia.
In Canada, a Swiss Army issue Enigma-K, is in Calgary, Alberta. It is on permanent display at the Naval Museum of Alberta inside the Military Museums of Calgary. A 3-rotor Enigma machine is on display at the Military Communications and Electronics Museum at Canadian Forces Base (CFB) Kingston in Kingston, Ontario.
Occasionally, Enigma machines are sold at auction; prices have in recent years ranged from US$40,000 to US$203,000 in 2011. Replicas are available in various forms, including an exact reconstructed copy of the Naval M4 model, an Enigma implemented in electronics (Enigma-E), various simulators and paper-and-scissors analogues.
A rare "Abwehr" Enigma machine, designated G312, was stolen from the Bletchley Park museum on 1 April 2000. In September, a man identifying himself as "The Master" sent a note demanding £25,000 and threatening to destroy the machine if the ransom were not paid. In early October 2000, Bletchley Park officials announced that they would pay the ransom, but the stated deadline passed with no word from the blackmailer. Shortly afterward, the machine was sent anonymously to BBC journalist Jeremy Paxman, missing three rotors.
In November 2000, an antiques dealer named Dennis Yates was arrested after telephoning "The Sunday Times" to arrange the return of the missing parts. The Enigma machine was returned to Bletchley Park after the incident. In October 2001, Yates was sentenced to 10 months in prison and served three months.
In October 2008, the Spanish daily newspaper "El País" reported that 28 Enigma machines had been discovered by chance in an attic of Army headquarters in Madrid. These 4-rotor commercial machines had helped Franco's Nationalists win the Spanish Civil War because, though the British cryptologist Alfred Dilwyn Knox in 1937 broke the cipher generated by Franco's Enigma machines, this was not disclosed to the Republicans, who failed to break the cipher. The Nationalist government continued using its 50 Enigmas into the 1950s. Some machines have gone on display in Spanish military museums, including one at the National Museum of Science and Technology (MUNCYT) in A Coruña. Two have been given to Britain's GCHQ.
The Bulgarian military used Enigma machines with a Cyrillic keyboard; one is on display in the National Museum of Military History in Sofia.
Derivatives.
The Enigma was influential in the field of cipher machine design, spinning off other rotor machines. The British Typex was originally derived from the Enigma patents; Typex even includes features from the patent descriptions that were omitted from the actual Enigma machine. The British paid no royalties for the use of the patents, to protect secrecy. The Typex implementation is not the same as that found in German or other Axis versions.
A Japanese Enigma clone was codenamed GREEN by American cryptographers. Little used, it contained four rotors mounted vertically. In the U.S., cryptologist William Friedman designed the M-325, a machine logically similar, although not in construction.
A unique rotor machine was constructed in 2002 by Netherlands-based Tatjana van Vark. This device makes use of 40-point rotors, allowing letters, numbers and some punctuation to be used; each rotor contains 509 parts.
Machines like the SIGABA, NEMA, Typex and so forth, are deliberately not considered to be Enigma derivatives as their internal ciphering functions are not mathematically identical to the Enigma transform.
Several software implementations exist, but not all exactly match Enigma behavior. The most commonly used software derivative (that is not compliant with any hardware implementation of the Enigma) is at EnigmaCo.de. Many Java applet Enigmas only accept single letter entry, complicating use even if the applet is Enigma compliant. Technically, Enigma@home is the largest scale deployment of a software Enigma, but the decoding software does not implement encipherment making it a derivative (as all original machines could cipher and decipher).
A user-friendly 3-rotor simulator, where users can select rotors, use the plugboard and define new settings for the rotors and reflectors is available. The output appears in separate windows which can be independently made "invisible" to hide decryption. Another includes an "autotyping" function which takes plaintext from a clipboard and converts it to cyphertext (or vice-versa) at one of four speeds. The "very fast" option produces 26 characters in less than one second.

</doc>
<doc id="9257" url="http://en.wikipedia.org/wiki?curid=9257" title="Enzyme">
Enzyme

Enzymes are macromolecular biological catalysts. They are responsible for thousands of metabolic processes that sustain life. Enzymes are highly selective catalysts, greatly accelerating both the rate and specificity of metabolic reactions, from the digestion of food to the synthesis of DNA. Most enzymes are proteins, although some catalytic RNA molecules have been identified. Enzymes adopt a specific three-dimensional structure, and may employ organic (e.g. biotin) and inorganic (e.g. magnesium ion) cofactors to assist in catalysis.
Enzymes act by converting starting molecules (substrates) into different molecules (products). Almost all chemical reactions in a biological cell need enzymes in order to occur at rates sufficient for life. Since enzymes are selective for their substrates and speed up only a few reactions from among many possibilities, the set of enzymes made in a cell determines which metabolic pathways occur in that cell, tissue and organ. Organelles are also differentially enriched in sets of enzymes to compartmentalise function within the cell.
Like all catalysts, enzymes increase the rate of a reaction by lowering its activation energy ("E"a‡). As a result, products are formed faster and reactions reach their equilibrium state more rapidly. Most enzyme reaction rates are millions of times faster than those of comparable un-catalyzed reactions and some are so fast that they are diffusion limited. As with all catalysts, enzymes are not consumed by the reactions they catalyze, nor do they alter the equilibrium of these reactions. However, enzymes do differ from most other catalysts in that they are highly specific for their substrates. Enzymes are known to catalyze about 4,000 biochemical reactions. A few RNA molecules called ribozymes also catalyze reactions, with an important example being some parts of the ribosome. Synthetic molecules called artificial enzymes also display enzyme-like catalysis.
Enzyme activity can be affected by other molecules: decreased by inhibitors or increased by activators. Many drugs and poisons are enzyme inhibitors. Activity is also affected by temperature, pressure, chemical environment (e.g., pH), and the concentration of substrate. Some enzymes are used commercially, for example, in the synthesis of antibiotics. In addition, some household products use enzymes to speed up biochemical reactions (e.g., enzymes in biological washing powders break down protein or fat stains on clothes; enzymes in meat tenderizers break down proteins into smaller molecules, making the meat easier to chew). The study of enzymes is called "enzymology".
Etymology and history.
As early as the late 17th and early 18th centuries, the digestion of meat by stomach secretions and the conversion of starch to sugars by plant extracts and saliva were known. However, the mechanism by which this occurred had not been identified.
In 1833, French chemist Anselme Payen discovered the first enzyme, diastase. A few decades later, when studying the fermentation of sugar to alcohol by yeast, Louis Pasteur came to the conclusion that this fermentation was catalyzed by a vital force contained within the yeast cells called "ferments", which were thought to function only within living organisms. He wrote that "alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells."
In 1877, German physiologist Wilhelm Kühne (1837–1900) first used the term "enzyme", which comes from Greek ἔνζυμον, "leavened", to describe this process. The word "enzyme" was used later to refer to nonliving substances such as pepsin, and the word "ferment" was used to refer to chemical activity produced by living organisms.
In 1897, Eduard Buchner submitted his first paper on the ability of yeast extracts that lacked any living yeast cells to ferment sugar. In a series of experiments at the University of Berlin, he found that the sugar was fermented even when there were no living yeast cells in the mixture. He named the enzyme that brought about the fermentation of sucrose "zymase". In 1907, he received the Nobel Prize in Chemistry "for his biochemical research and his discovery of cell-free fermentation". Following Buchner's example, enzymes are usually named according to the reaction they carry out. Typically, to generate the name of an enzyme, the suffix "-ase" is added to the name of its substrate (e.g., lactase is the enzyme that cleaves lactose) or the type of reaction (e.g., DNA polymerase forms DNA polymers).
Having shown that enzymes could function outside a living cell, the next step was to determine their biochemical nature. Many early workers noted that enzymatic activity was associated with proteins, but several scientists (such as Nobel laureate Richard Willstätter) argued that proteins were merely carriers for the true enzymes and that proteins "per se" were incapable of catalysis. However, in 1926, James B. Sumner showed that the enzyme urease was a pure protein and crystallized it; Sumner did likewise for the enzyme catalase in 1937. The conclusion that pure proteins can be enzymes was definitively proved by Northrop and Stanley, who worked on the digestive enzymes pepsin (1930), trypsin and chymotrypsin. These three scientists were awarded the 1946 Nobel Prize in Chemistry.
This discovery that enzymes could be crystallized eventually allowed their structures to be solved by x-ray crystallography. This was first done for lysozyme, an enzyme found in tears, saliva and egg whites that digests the coating of some bacteria; the structure was solved by a group led by David Chilton Phillips and published in 1965. This high-resolution structure of lysozyme marked the beginning of the field of structural biology and the effort to understand how enzymes work at an atomic level of detail.
Structures and mechanisms.
Enzymes are in general globular proteins and range from just 62 amino acid residues in size, for the monomer of 4-oxalocrotonate tautomerase, to over 2,500 residues in the animal fatty acid synthase. A small number of RNA-based biological catalysts exist, with the most common being the ribosome; these are referred to as either RNA-enzymes or ribozymes. The activities of enzymes are determined by their three-dimensional structure. However, although structure does determine function, predicting a novel enzyme's activity just from its structure is a very difficult problem that has not yet been solved.
Most enzymes are much larger than the substrates they act on, and only a small portion of the enzyme (around 2–4 amino acids) is directly involved in catalysis. The region that contains these catalytic residues, binds the substrate, and then carries out the reaction is known as the active site. Enzymes can also contain sites that bind cofactors, which are needed for catalysis. Some enzymes also have binding sites for small molecules, which are often direct or indirect products or substrates of the reaction catalyzed. This binding can serve to increase or decrease the enzyme's activity, providing a means for feedback regulation.
Like all proteins, enzymes are long, linear chains of amino acids that fold to produce a three-dimensional product. Each unique amino acid sequence produces a specific structure, which has unique properties. Individual protein chains may sometimes group together to form a protein complex. Most enzymes can be denatured—that is, unfolded and inactivated—by heating or chemical denaturants, which disrupt the three-dimensional structure of the protein. Depending on the enzyme, denaturation may be reversible or irreversible.
Structures of enzymes with substrates or substrate analogs during a reaction may be obtained using Time resolved crystallography methods.
Specificity.
Enzymes are usually very specific as to which reactions they catalyze and the substrates that are involved in these reactions. Complementary shape, charge and hydrophilic/hydrophobic characteristics of enzymes and substrates are responsible for this specificity. Enzymes can also show impressive levels of stereospecificity, regioselectivity and chemoselectivity.
Some of the enzymes showing the highest specificity and accuracy are involved in the copying and expression of the genome. These enzymes have "proof-reading" mechanisms. Here, an enzyme such as DNA polymerase catalyzes a reaction in a first step and then checks that the product is correct in a second step. This two-step process results in average error rates of less than 1 error in 100 million reactions in high-fidelity mammalian polymerases. Similar proofreading mechanisms are also found in RNA polymerase, aminoacyl tRNA synthetases and ribosomes.
Whereas some enzymes have broad-specificity, as they can act on a relatively broad range of different physiologically relevant substrates, many enzymes possess small side activities which arose fortuitously (i.e. neutrally), which may be the starting point for the evolutionary selection of a new function; this phenomenon is known as enzyme promiscuity.
"Lock and key" model.
Enzymes are very specific, and it was suggested by Emil Fischer in 1894 that this was because both the enzyme and the substrate possess specific complementary geometric shapes that fit exactly into one another. This is often referred to as "the lock and key" model. However, while this model explains enzyme specificity, it fails to explain the stabilization of the transition state that enzymes achieve.
In 1958, Daniel Koshland suggested a modification to the lock and key model: since enzymes are rather flexible structures, the active site is continuously reshaped by interactions with the substrate as the substrate interacts with the enzyme. As a result, the substrate does not simply bind to a rigid active site; the amino acid side-chains that make up the active site are molded into the precise positions that enable the enzyme to perform its catalytic function. In some cases, such as glycosidases, the substrate molecule also changes shape slightly as it enters the active site. The active site continues to change until the substrate is completely bound, at which point the final shape and charge is determined.
Induced fit may enhance the fidelity of molecular recognition in the presence of competition and noise via the conformational proofreading mechanism.
Based on Fischer's lock-and-key model and Koshland’s induced fit theory, the Chou’s distorted key theory for peptide drugs was proposed
 to develop peptide drugs against HIV/AIDS and SARS.
Mechanisms.
Enzymes can act in several ways, all of which lower ΔG‡ (Gibbs energy):
It is interesting that this entropic effect involves destabilization of the ground state, and its contribution to catalysis is relatively small.
Transition state stabilization.
The understanding of the origin of the reduction of ΔG‡ requires one to find out how the enzymes can stabilize its transition state more than the transition state of the uncatalyzed reaction. It seems that the most effective way for reaching large stabilization is the use of electrostatic effects, in particular, when having a relatively fixed polar environment that is oriented toward the charge distribution of the transition state. Such an environment does not exist in the uncatalyzed reaction in water.
Dynamics and function.
The internal dynamics of enzymes has been suggested to be linked with their mechanism of catalysis.
Internal dynamics are the movement of parts of the enzyme's structure, such as individual amino acid residues, a group of amino acids, or even an entire protein domain. These movements occur at various time-scales ranging from femtoseconds to seconds. Networks of protein residues throughout an enzyme's structure can contribute to catalysis through dynamic motions. This is simply seen in the kinetic scheme of the combined process, enzymatic activity and dynamics; this scheme can have several independent Michaelis-Menten-like reaction pathways that are connected through fluctuation rates.
Protein motions are vital to many enzymes, but whether small and fast vibrations, or larger and slower conformational movements are more important depends on the type of reaction involved. However, although these movements are important in binding and releasing substrates and products, it is not clear if protein movements help to accelerate the chemical steps in enzymatic reactions. These new insights also have implications in understanding allosteric effects and developing new medicines.
Allosteric modulation.
Allosteric sites are sites on the enzyme that bind to molecules in the cellular environment. The sites form weak, noncovalent bonds with these molecules, causing a change in the conformation of the enzyme. This change in conformation translates to the active site, which then affects the reaction rate of the enzyme. Allosteric interactions can both inhibit and activate enzymes and are a common way that enzymes are controlled in the body.
Cofactors and coenzymes.
Cofactors.
Some enzymes do not need any additional components to show full activity. However, others require non-protein molecules called cofactors to be bound for activity. Cofactors can be either inorganic (e.g., metal ions and iron-sulfur clusters) or organic compounds (e.g., flavin and heme). Organic cofactors can be either prosthetic groups, which are tightly bound to an enzyme, or coenzymes, which are released from the enzyme's active site during the reaction. Coenzymes include NADH, NADPH and adenosine triphosphate. These molecules transfer chemical groups between enzymes.
An example of an enzyme that contains a cofactor is carbonic anhydrase, and is shown in the ribbon diagram above with a zinc cofactor bound as part of its active site. These tightly bound molecules are usually found in the active site and are involved in catalysis. For example, flavin and heme cofactors are often involved in redox reactions.
Enzymes that require a cofactor but do not have one bound are called "apoenzymes" or "apoproteins". An apoenzyme together with its cofactor(s) is called a "holoenzyme" (this is the active form). Most cofactors are not covalently attached to an enzyme, but are very tightly bound. However, organic prosthetic groups can be covalently bound (e.g., biotin in the enzyme pyruvate carboxylase). The term "holoenzyme" can also be applied to enzymes that contain multiple protein subunits, such as the DNA polymerases; here the holoenzyme is the complete complex containing all the subunits needed for activity.
Coenzymes.
Coenzymes are small organic molecules that can be loosely or tightly bound to an enzyme. Tightly bound coenzymes can be called prosthetic groups. Coenzymes transport chemical groups from one enzyme to another. Some of these chemicals such as riboflavin, thiamine and folic acid are vitamins (compounds that cannot be synthesized by the body and must be acquired from the diet). The chemical groups carried include the hydride ion (H-) carried by NAD or NADP+, the phosphate group carried by adenosine triphosphate, the acetyl group carried by coenzyme A, formyl, methenyl or methyl groups carried by folic acid and the methyl group carried by S-adenosylmethionine.
Since coenzymes are chemically changed as a consequence of enzyme action, it is useful to consider coenzymes to be a special class of substrates, or second substrates, which are common to many different enzymes. For example, about 700 enzymes are known to use the coenzyme NADH.
Coenzymes are usually continuously regenerated and their concentrations maintained at a steady level inside the cell: for example, NADPH is regenerated through the pentose phosphate pathway and "S"-adenosylmethionine by methionine adenosyltransferase. This continuous regeneration means that even small amounts of coenzymes are used very intensively. For example, the human body turns over its own weight in ATP each day.
Thermodynamics.
As all catalysts, enzymes do not alter the position of the chemical equilibrium of the reaction. Usually, in the presence of an enzyme, the reaction runs in the same direction as it would without the enzyme, just more quickly. However, in the absence of the enzyme, other possible uncatalyzed, "spontaneous" reactions might lead to different products, because in those conditions this different product is formed faster.
Furthermore, enzymes can couple two or more reactions, so that a thermodynamically favorable reaction can be used to "drive" a thermodynamically unfavorable one. For example, the hydrolysis of ATP is often used to drive other chemical reactions.
Enzymes catalyze the forward and backward reactions equally. They do not alter the equilibrium itself, but only the speed at which it is reached. For example, carbonic anhydrase catalyzes its reaction in either direction depending on the concentration of its reactants.
Nevertheless, if the equilibrium is greatly displaced in one direction, that is, in a very exergonic reaction, the reaction is in effect irreversible. Under these conditions, the enzyme will, in fact, catalyze the reaction only in the thermodynamically allowed direction.
Kinetics.
Enzyme kinetics is the investigation of how enzymes bind substrates and turn them into products. The rate data used in kinetic analyses are commonly obtained from enzyme assays, where since the 90s, the dynamics of many enzymes are studied on the level of individual molecules.
In 1902 Victor Henri proposed a quantitative theory of enzyme kinetics, but his experimental data were not useful because the significance of the hydrogen ion concentration was not yet appreciated. After Peter Lauritz Sørensen had defined the logarithmic pH-scale and introduced the concept of buffering in 1909 the German chemist Leonor Michaelis and his Canadian postdoc Maud Leonora Menten repeated Henri's experiments and confirmed his equation, which is referred to as Henri-Michaelis-Menten kinetics (termed also Michaelis-Menten kinetics). Their work was further developed by G. E. Briggs and J. B. S. Haldane, who derived kinetic equations that are still widely considered today a starting point in solving enzymatic activity.
The major contribution of Henri was to think of enzyme reactions in two stages. In the first, the substrate binds reversibly to the enzyme, forming the enzyme-substrate complex. This is sometimes called the Michaelis complex. The enzyme then catalyzes the chemical step in the reaction and releases the product. Note that the simple Michaelis Menten mechanism for the enzymatic activity is considered today a basic idea, where many examples show that the enzymatic activity involves structural dynamics. This is incorporated in the enzymatic mechanism while introducing several Michaelis Menten pathways that are connected with fluctuating rates. Nevertheless, there is a mathematical relation connecting the behavior obtained from the basic Michaelis Menten mechanism (that was indeed proved correct in many experiments) with the generalized Michaelis Menten mechanisms involving dynamics and activity;
 this means that the measured activity of enzymes on the level of many enzymes may be explained with the simple Michaelis-Menten equation, yet, the actual activity of enzymes is richer and involves structural dynamics.
Enzymes can catalyze up to several million reactions per second. For example, the uncatalyzed decarboxylation of orotidine 5'-monophosphate has a half life of 78 million years. However, when the enzyme orotidine 5'-phosphate decarboxylase is added, the same process takes just 25 milliseconds. Enzyme rates depend on solution conditions and substrate concentration. Conditions that denature the protein abolish enzyme activity, such as high temperatures, extremes of pH or high salt concentrations, while raising substrate concentration tends to increase activity when [S] is low. To find the maximum speed of an enzymatic reaction, the substrate concentration is increased until a constant rate of product formation is seen. This is shown in the saturation curve on the right. Saturation happens because, as substrate concentration increases, more and more of the free enzyme is converted into the substrate-bound ES form. At the maximum reaction rate ("V"max) of the enzyme, all the enzyme active sites are bound to substrate, and the amount of ES complex is the same as the total amount of enzyme.
However, "V"max is only one kinetic constant of enzymes. The amount of substrate needed to achieve a given rate of reaction is also important. This is given by the Michaelis-Menten constant ("K"m), which is the substrate concentration required for an enzyme to reach one-half its maximum reaction rate; generally, each enzyme has a characteristic "K"m for a given substrate. Another useful constant is "k"cat, which is the rate of product formation handled by one active site and is generally given in units of inverse seconds.
The efficiency of an enzyme can be expressed in terms of "k"cat/"K"m. This is also called the specificity constant and incorporates the rate constants for all steps in the reaction up to and including the first irreversible step. Because the specificity constant reflects both affinity and catalytic ability, it is useful for comparing different enzymes against each other, or the same enzyme with different substrates. The theoretical maximum for the specificity constant is called the diffusion limit and is about 108 to 109 (M−1 s−1). At this point every collision of the enzyme with its substrate will result in catalysis, and the rate of product formation is not limited by the reaction rate but by the diffusion rate. Enzymes with this property are called "catalytically perfect" or "kinetically perfect". Example of such enzymes are triose-phosphate isomerase, carbonic anhydrase, acetylcholinesterase, catalase, fumarase, β-lactamase, and superoxide dismutase.
Michaelis-Menten kinetics relies on the law of mass action, which is derived from the assumptions of free diffusion and thermodynamically driven random collision. However, many biochemical or cellular processes deviate significantly from these conditions, because of macromolecular crowding, phase-separation of the enzyme/substrate/product, or one or two-dimensional molecular movement. In these situations, a fractal Michaelis-Menten kinetics may be applied.
Some enzymes operate with kinetics, which are faster than diffusion rates, which would seem to be impossible. Several mechanisms have been invoked to explain this phenomenon. Some proteins are believed to accelerate catalysis by drawing their substrate in and pre-orienting them by using dipolar electric fields. Other models invoke a quantum-mechanical tunneling explanation, whereby a proton or an electron can tunnel through activation barriers, although for proton tunneling this model remains somewhat controversial. Quantum tunneling for protons has been observed in tryptamine. This suggests that enzyme catalysis may be more accurately characterized as "through the barrier" rather than the traditional model, which requires substrates to go "over" a lowered energy barrier.
Inhibition.
Enzyme reaction rates can be decreased by various types of enzyme inhibitors.
In competitive inhibition, the inhibitor and substrate compete for the enzyme (i.e., they can not bind at the same time). Often competitive inhibitors strongly resemble the real substrate of the enzyme. For example, methotrexate is a competitive inhibitor of the enzyme dihydrofolate reductase, which catalyzes the reduction of dihydrofolate to tetrahydrofolate. The similarity between the structures of folic acid and this drug are shown in the figure to the "right" bottom. In some cases, the inhibitor can bind to a site other than the binding-site of the usual substrate and exert an allosteric effect to change the shape of the usual binding-site. For example, strychnine acts as an allosteric inhibitor of the glycine receptor in the mammalian spinal cord and brain stem. Glycine is a major post-synaptic inhibitory neurotransmitter with a specific receptor site. Strychnine binds to an alternate site that reduces the affinity of the glycine receptor for glycine, resulting in convulsions due to lessened inhibition by the glycine. In competitive inhibition the maximal rate of the reaction is not changed, but higher substrate concentrations are required to reach a given maximum rate, increasing the apparent Km.
In uncompetitive inhibition, the inhibitor cannot bind to the free enzyme, only to the ES-complex. The EIS-complex thus formed is enzymatically inactive. This type of inhibition is rare, but may occur in multimeric enzymes.
Non-competitive inhibitors can bind to the enzyme at the binding site at the same time as the substrate,but not to the active site. Both the EI and EIS complexes are enzymatically inactive. Because the inhibitor can not be driven from the enzyme by higher substrate concentration (in contrast to competitive inhibition), the apparent Vmax changes. But because the substrate can still bind to the enzyme, the Km stays the same.
This type of inhibition resembles the non-competitive, except that the EIS-complex has residual enzymatic activity.This type of inhibitor does not follow Michaelis-Menten equation.
In many organisms, inhibitors may act as part of a feedback mechanism. If an enzyme produces too much of one substance in the organism, that substance may act as an inhibitor for the enzyme at the beginning of the pathway that produces it, causing production of the substance to slow down or stop when there is sufficient amount. This is a form of negative feedback. Enzymes that are subject to this form of regulation are often multimeric and have allosteric binding sites for regulatory substances. Their substrate/velocity plots are not hyperbolar, but sigmoidal (S-shaped).
Irreversible inhibitors react with the enzyme and form a covalent adduct with the protein. The inactivation is irreversible. These compounds include eflornithine a drug used to treat the parasitic disease sleeping sickness. Penicillin and Aspirin also act in this manner. With these drugs, the compound is bound in the active site and the enzyme then converts the inhibitor into an activated form that reacts irreversibly with one or more amino acid residues.
Since inhibitors modulate the function of enzymes they are often used as drugs. A common example of an inhibitor that is used as a drug is aspirin, which inhibits the COX-1 and COX-2 enzymes that produce the inflammation messenger prostaglandin, thus suppressing pain and inflammation. However, other enzyme inhibitors are poisons. For example, the poison cyanide is an irreversible enzyme inhibitor that combines with the copper and iron in the active site of the enzyme cytochrome c oxidase and blocks cellular respiration.
Biological function.
Enzymes serve a wide variety of functions inside living organisms. They are indispensable for signal transduction and cell regulation, often via kinases and phosphatases. They also generate movement, with myosin hydrolyzing ATP to generate muscle contraction and also moving cargo around the cell as part of the cytoskeleton. Other ATPases in the cell membrane are ion pumps involved in active transport. Enzymes are also involved in more exotic functions, such as luciferase generating light in fireflies. Viruses can also contain enzymes for infecting cells, such as the HIV integrase and reverse transcriptase, or for viral release from cells, like the influenza virus neuraminidase.
An important function of enzymes is in the digestive systems of animals. Enzymes such as amylases and proteases break down large molecules (starch or proteins, respectively) into smaller ones, so they can be absorbed by the intestines. Starch molecules, for example, are too large to be absorbed from the intestine, but enzymes hydrolyze the starch chains into smaller molecules such as maltose and eventually glucose, which can then be absorbed. Different enzymes digest different food substances. In ruminants, which have herbivorous diets, microorganisms in the gut produce another enzyme, cellulase, to break down the cellulose cell walls of plant fiber.
Several enzymes can work together in a specific order, creating metabolic pathways. In a metabolic pathway, one enzyme takes the product of another enzyme as a substrate. After the catalytic reaction, the product is then passed on to another enzyme. Sometimes more than one enzyme can catalyze the same reaction in parallel; this can allow more complex regulation: with, for example, a low constant activity provided by one enzyme but an inducible high activity from a second enzyme.
Enzymes determine what steps occur in these pathways. Without enzymes, metabolism would neither progress through the same steps nor be fast enough to serve the needs of the cell. Indeed, a metabolic pathway such as glycolysis could not exist independently of enzymes. Glucose, for example, can react directly with ATP to become phosphorylated at one or more of its carbons. In the absence of enzymes, this occurs so slowly as to be insignificant. However, if hexokinase is added, these slow reactions continue to take place except that phosphorylation at carbon 6 occurs so rapidly that, if the mixture is tested a short time later, glucose-6-phosphate is found to be the only significant product. As a consequence, the network of metabolic pathways within each cell depends on the set of functional enzymes that are present.
Control of activity.
There are five main ways that enzyme activity is controlled in the cell.
Involvement in disease.
Since the tight control of enzyme activity is essential for homeostasis, any malfunction (mutation, overproduction, underproduction or deletion) of a single critical enzyme can lead to a genetic disease. The importance of enzymes is shown by the fact that a lethal illness can be caused by the malfunction of just one type of enzyme out of the thousands of types present in our bodies.
One example is the most common type of phenylketonuria. A mutation of a single amino acid in the enzyme phenylalanine hydroxylase, which catalyzes the first step in the degradation of phenylalanine, results in build-up of phenylalanine and related products. This can lead to intellectual disability if the disease is untreated. Another example of enzyme deficiency is pseudocholinesterase, in which the body's ability to break down choline ester drugs is impaired.
A further example is when germline mutations in genes coding for DNA repair enzymes cause hereditary cancer syndromes such as xeroderma pigmentosum. Defects in these enzymes cause cancer since the body is less able to repair mutations in the genome. This causes a slow accumulation of mutations and results in the development of many types of cancer in the sufferer.
Oral administration of enzymes can be used to treat several diseases (e.g. pancreatic insufficiency and lactose intolerance). Since enzymes are proteins themselves they are potentially subject to inactivation and digestion in the gastrointestinal environment. Therefore a non-invasive imaging assay was developed to monitor gastrointestinal activity of exogenous enzymes (prolyl endopeptidase as potential adjuvant therapy for celiac disease) in vivo.
Naming conventions.
An enzyme's name is often derived from its substrate or the chemical reaction it catalyzes, with the word ending in -ase. Examples are lactase, alcohol dehydrogenase and DNA polymerase. This may result in different enzymes, called isozymes, with the same function having the same basic name. Isoenzymes have a different amino acid sequence and might be distinguished by their optimal pH, kinetic properties or immunologically. Isoenzyme and isozyme are homologous proteins. Furthermore, the normal physiological reaction an enzyme catalyzes may not be the same as under artificial conditions. This can result in the same enzyme being identified with two different names. For example, glucose isomerase, which is used industrially to convert glucose into the sweetener fructose, is a xylose isomerase "in vivo" (within the body).
The International Union of Biochemistry and Molecular Biology have developed a nomenclature for enzymes, the EC numbers; each enzyme is described by a sequence of four numbers preceded by "EC".
The first number broadly classifies the enzyme based on its mechanism.
The top-level classification is
According to the naming conventions, enzymes are generally classified into six main family classes and many sub-family classes. Some web-servers, e.g.,
and bioinformatics tools have been developed to predict which main family class
and sub-family class
an enzyme molecule belongs to according to its sequence information alone via the pseudo amino acid composition.
Industrial applications.
Enzymes are used in the chemical industry and other industrial applications when extremely specific catalysts are required. However, enzymes in general are limited in the number of reactions they have evolved to catalyze and also by their lack of stability in organic solvents and at high temperatures. As a consequence, protein engineering is an active area of research and involves attempts to create new enzymes with novel properties, either through rational design or "in vitro" evolution. These efforts have begun to be successful, and a few enzymes have now been designed "from scratch" to catalyze reactions that do not occur in nature.
Further reading.
Etymology and history
Enzyme structure and mechanism
Thermodynamics
Kinetics and inhibition
Function and control of enzymes in the cell
Enzyme-naming conventions
Industrial applications

</doc>
