<doc id="7426" url="http://en.wikipedia.org/wiki?curid=7426" title="Charles I of England">
Charles I of England

Charles I (19 November 1600 – 30 January 1649) was monarch of the three kingdoms of England, Scotland, and Ireland from 27 March 1625 until his execution in 1649.
Charles was the second son of King James VI of Scotland, but after his father inherited the English throne in 1603, he moved to England, where he spent much of the rest of his life. He became heir apparent to the English, Irish and Scottish thrones on the death of his elder brother, Henry Frederick, Prince of Wales, in 1612. An unsuccessful and unpopular attempt to marry him to a Spanish Habsburg princess culminated in an eight-month visit to Spain in 1623 that demonstrated the futility of the marriage negotiations. Two years later he married the Bourbon princess Henrietta Maria of France instead.
After his succession, Charles quarrelled with the Parliament of England, which sought to curb his royal prerogative. Charles believed in the divine right of kings and thought he could govern according to his own conscience. Many of his subjects opposed his policies, in particular the levying of taxes without parliamentary consent, and perceived his actions as those of a tyrannical absolute monarch. His religious policies, coupled with his marriage to a Roman Catholic, generated the antipathy and mistrust of reformed groups such as the Puritans and Calvinists, who thought his views too Catholic. He supported high church ecclesiastics, such as Richard Montagu and William Laud, and failed to successfully aid Protestant forces during the Thirty Years' War. His attempts to force the Church of Scotland to adopt high Anglican practices led to the Bishops' Wars, strengthened the position of the English and Scottish parliaments and helped precipitate his own downfall.
From 1642, Charles fought the armies of the English and Scottish parliaments in the English Civil War. After his defeat in 1645, he surrendered to a Scottish force that eventually handed him over to the English Parliament. Charles refused to accept his captors' demands for a constitutional monarchy, and temporarily escaped captivity in November 1647. Re-imprisoned on the Isle of Wight, Charles forged an alliance with Scotland, but by the end of 1648 Oliver Cromwell's New Model Army had consolidated its control over England. Charles was tried, convicted, and executed for high treason in January 1649. The monarchy was abolished and a republic called the Commonwealth of England was declared. In 1660, the English Interregnum ended when the monarchy was restored to Charles's son, Charles II.
Early life.
The second son of King James VI of Scotland and Anne of Denmark, Charles was born in Dunfermline Palace, Fife, on 19 November 1600. At a Protestant ceremony in the Chapel Royal at Holyrood Palace in Edinburgh on 23 December 1600, he was baptised by David Lindsay, Bishop of Ross, and created Duke of Albany, the traditional title of the second son of the King of Scotland, with the subsidiary titles of Marquess of Ormond, Earl of Ross and Lord Ardmannoch.
James VI was the first cousin twice removed of Queen Elizabeth I of England, and when she died childless in March 1603, he became King of England as James I. Charles was a weak and sickly infant, and while his parents and older siblings left for England in April and early June that year, due to his fragile health, he remained in Scotland with his father's friend Lord Fyvie, appointed as his guardian.
By 1604, Charles was three and a half and as he was able to walk the length of the great hall at Dunfermline Palace without assistance, it was decided that he was strong enough to make the journey to England to be reunited with his family. In mid-July 1604, Charles left Dunfermline for England where he was to spend most of the rest of his life. In England, Charles was placed under the charge of Elizabeth, Lady Carey, the wife of courtier Sir Robert Carey, who put him in boots made of Spanish leather and brass to help strengthen his weak ankles. His speech development was also slow, and he retained a stammer, or hesitant speech, for the rest of his life.
In January 1605, Charles was created Duke of York, as is customary in the case of the English sovereign's second son, and made a Knight of the Bath. Thomas Murray, a Presbyterian Scot, was appointed as a tutor. Charles learnt the usual subjects of classics, languages, mathematics and religion. In 1611, he was made a Knight of the Garter.
Eventually, Charles apparently conquered his physical infirmity, which might have been caused by rickets. He became an adept horseman and marksman, and took up fencing. Even so, his public profile remained low in contrast to that of his physically stronger and taller elder brother, Henry Frederick, Prince of Wales, whom Charles adored and attempted to emulate. However, in early November 1612, Henry died at the age of 18 of what is suspected to have been typhoid (or possibly porphyria). Charles, who turned 12 two weeks later, became heir apparent. As the eldest surviving son of the sovereign, Charles automatically gained several titles (including Duke of Cornwall and Duke of Rothesay). Four years later, in November 1616, he was created Prince of Wales and Earl of Chester.
Heir apparent.
In 1613, his sister Elizabeth married Frederick V, Elector Palatine, and moved to Heidelberg. In 1617, the Habsburg Archduke Ferdinand of Austria, a Catholic, was elected king of Bohemia. The following year, the Bohemians rebelled, defenestrating the Catholic governors. In August 1619, the Bohemian diet chose as their monarch Frederick V, who was leader of the Protestant Union, while Ferdinand was elected Holy Roman Emperor in the imperial election. Frederick's acceptance of the Bohemian crown in defiance of the emperor marked the beginning of the turmoil that would develop into the Thirty Years' War. The conflict, originally confined to Bohemia, spiralled into a wider European war, which the English Parliament and public quickly grew to see as a polarised continental struggle between Catholics and Protestants. In 1620, Charles's brother-in-law, Frederick V, was defeated at the Battle of White Mountain near Prague and his hereditary lands in the Electoral Palatinate were invaded by a Habsburg force from the Spanish Netherlands. James, however, had been seeking marriage between the new Prince of Wales and Ferdinand's niece, Habsburg princess Infanta Maria of Spain, and began to see the Spanish match as a possible diplomatic means of achieving peace in Europe.
Unfortunately for James, negotiation with Spain proved generally unpopular, both with the public and with James's court. The English Parliament was actively hostile towards Spain and Catholicism, and thus, when called by James in 1621, the members hoped for an enforcement of recusancy laws, a naval campaign against Spain, and a Protestant marriage for the Prince of Wales. James's Lord Chancellor, Francis Bacon was impeached before the House of Lords for corruption. The impeachment was the first since 1459 without the king's official sanction in the form of a bill of attainder. The incident set an important precedent as the process of impeachment would later be used against Charles and his supporters: the Duke of Buckingham, Archbishop Laud, and the Earl of Strafford. James insisted that the House of Commons be concerned exclusively with domestic affairs, while the members protested that they had the privilege of free speech within the Commons' walls, demanding war with Spain and a Protestant Princess of Wales. Charles, like his father, considered the discussion of his marriage in the Commons impertinent and an infringement of his father's royal prerogative. In January 1622, James dissolved Parliament, angry at what he perceived as the members' impudence and intransigence.
Charles and the Duke of Buckingham, James's favourite and a man who had great influence over the prince, travelled incognito to Spain in February 1623 to try to reach agreement on the long-pending Spanish match. In the end, however, the trip was an embarrassing failure. The Infanta thought Charles was little more than an infidel, and the Spanish at first demanded that he convert to Roman Catholicism as a condition of the match. The Spanish insisted on toleration of Catholics in England and the repeal of the penal laws, which Charles knew would never be agreed by Parliament, and that the Infanta remain in Spain for a year after any wedding to ensure that England complied with all the terms of the treaty. A personal quarrel erupted between Buckingham and the Count of Olivares, the Spanish chief minister, and so Charles conducted the ultimately futile negotiations personally. When Charles returned to London in October, without a bride and to a rapturous and relieved public welcome, he and Buckingham pushed a reluctant King James to declare war on Spain.
With the encouragement of his Protestant advisers, James summoned the English Parliament in 1624 so that he could request subsidies for a war. Charles and Buckingham supported the impeachment of the Lord Treasurer, Lionel Cranfield, 1st Earl of Middlesex, who opposed war on grounds of cost and who quickly fell in much the same manner as Bacon had. James told Buckingham he was a fool, and presciently warned his son that he would live to regret the revival of impeachment as a parliamentary tool. An under-funded makeshift army under Ernst von Mansfeld set off to recover the Palatinate, but it was so poorly provisioned that it never advanced beyond the Dutch coast.
By 1624, James was growing ill, and as a result was finding it difficult to control Parliament. By the time of his death in March 1625, Charles and the Duke of Buckingham had already assumed "de facto" control of the kingdom.
Early reign.
With the failure of the Spanish match, Charles and Buckingham turned their attention to France. On 1 May 1625 Charles was married by proxy to the fifteen-year-old French princess Henrietta Maria in front of the doors of the Notre Dame de Paris. Charles had seen Henrietta Maria in Paris while en route to Spain. The couple married in person on 13 June 1625 in Canterbury. Charles delayed the opening of his first Parliament until after the second ceremony, to forestall any opposition. Many members of the Commons were opposed to the king's marriage to a Roman Catholic, fearing that Charles would lift restrictions on Catholic recusants and undermine the official establishment of the reformed Church of England. Although he told Parliament that he would not relax religious restrictions, he promised to do exactly that in a secret marriage treaty with Louis XIII of France. Moreover, the treaty placed under French command an English naval force that would be used to suppress the Protestant Huguenots at La Rochelle. Charles was crowned on 2 February 1626 at Westminster Abbey, but without his wife at his side because she refused to participate in a Protestant religious ceremony.
Distrust of Charles's religious policies increased with his support of a controversial anti-Calvinist ecclesiastic, Richard Montagu, who was in disrepute among the Puritans. In his pamphlet "A New Gag for an Old Goose" (1624), a reply to the Catholic pamphlet "A New Gag for the New Gospel", Montagu argued against Calvinist predestination, the doctrine that salvation and damnation were preordained by God. Anti-Calvinists – known as Arminians – believed that human beings could influence their own fate through the exercise of free will. Arminian divines had been one of the few sources of support for Charles's proposed Spanish marriage. With the support of King James, Montagu produced another pamphlet, entitled "Appello Caesarem", in 1625 shortly after the old king's death and Charles's accession. To protect Montagu from the stricture of Puritan members of Parliament, Charles made the cleric one of his royal chaplains, increasing many Puritans' suspicions that Charles favoured Arminianism as a clandestine attempt to aid the resurgence of Catholicism.
Rather than direct involvement in the European land war, the English Parliament preferred a relatively inexpensive naval attack on Spanish colonies in the New World, hoping for the capture of the Spanish treasure fleets. Parliament voted to grant a subsidy of £140,000, which was an insufficient sum for Charles's war plans. Moreover, the House of Commons limited its authorisation for royal collection of tonnage and poundage (two varieties of customs duties) to a period of one year, although previous sovereigns since Henry VI of England had been granted the right for life. In this manner, Parliament could delay approval of the rates until after a full-scale review of customs revenue. The bill made no progress in the House of Lords past its first reading. Although no Parliamentary Act for the levy of tonnage and poundage was obtained, Charles continued to collect the duties.
A poorly conceived and executed naval expedition against Spain under the leadership of Buckingham went badly, and the House of Commons began proceedings for the impeachment of the duke. In May 1626, Charles nominated Buckingham as Chancellor of Cambridge University in a show of support, and had two members who had spoken against Buckingham – Dudley Digges and Sir John Eliot – arrested at the door of the House. The Commons was outraged by the imprisonment of two of their members, and after about a week in custody, both were released. On 12 June 1626, the Commons launched a direct protestation attacking Buckingham, stating,
"We protest before your Majesty and the whole world that until this great person be removed from intermeddling with the great affairs of state, we are out of hope of any good success; and do fear that any money we shall or can give will, through his misemployment, be turned rather to the hurt and prejudice of this your kingdom than otherwise, as by lamentable experience we have found those large supplies formerly and lately given." Despite Parliament's protests, however, Charles refused to dismiss his friend, dismissing Parliament instead.
Meanwhile, domestic quarrels between Charles and Henrietta Maria were souring the early years of their marriage. Disputes over her jointure, appointments to her household, and the practice of her religion culminated in the king expelling the vast majority of her French attendants in August 1626. Despite Charles's agreement to provide the French with English ships as a condition of marrying Henrietta Maria, in 1627 he launched an attack on the French coast to defend the Huguenots at La Rochelle. The action, led by Buckingham, was ultimately unsuccessful. Buckingham's failure to protect the Huguenots – and his retreat from Saint-Martin-de-Ré – spurred Louis XIII's siege of La Rochelle and furthered the English Parliament's and people's detestation of the duke.
Charles provoked further unrest by trying to raise money for the war through a "forced loan": a tax levied without parliamentary consent. In November 1627, the test case in the King's Bench, the "Five Knights' Case", found that the king had a prerogative right to imprison without trial those who refused to pay the forced loan. Summoned again in March 1628, on 26 May Parliament adopted a Petition of Right, calling upon the king to acknowledge that he could not levy taxes without Parliament's consent, not impose martial law on civilians, not imprison them without due process, and not quarter troops in their homes. Charles assented to the petition on 7 June, but by the end of the month he had prorogued Parliament and re-asserted his right to collect customs duties without authorisation from Parliament.
On 23 August 1628, Buckingham was assassinated. Charles was deeply distressed. According to Edward Hyde, 1st Earl of Clarendon, he "threw himself upon his bed, lamenting with much passion and with abundance of tears". He remained grieving in his room for two days. In contrast, the public rejoiced at Buckingham's death, which accentuated the gulf between the court and the nation, and between the Crown and the Commons. Although the death of Buckingham effectively ended the war with Spain and eliminated his leadership as an issue, it did not end the conflicts between Charles and Parliament. It did, however, coincide with an improvement in Charles's relationship with his wife, and by November 1628 their old quarrels were at an end. Perhaps Charles's emotional ties were transferred from Buckingham to Henrietta Maria. She became pregnant for the first time, and the bond between them grew ever stronger. Together, they embodied an image of virtue and family life, and their court became a model of formality and morality.
Personal rule.
Parliament prorogued.
In January 1629 Charles opened the second session of the English Parliament, which had been prorogued in June 1628, with a moderate speech on the tonnage and poundage issue. Members of the House of Commons began to voice opposition to Charles's policies in light of the case of John Rolle, a Member of Parliament whose goods had been confiscated for failing to pay tonnage and poundage. Many MPs viewed the imposition of the tax as a breach of the Petition of Right. When Charles ordered a parliamentary adjournment on 2 March, members held the Speaker, Sir John Finch, down in his chair so that the ending of the session could be delayed long enough for resolutions against Catholicism, Arminianism and tonnage and poundage to be read out and acclaimed by the chamber. The provocation was too much for Charles, who dissolved Parliament and had nine parliamentary leaders, including Sir John Eliot, imprisoned over the matter, thereby turning the men into martyrs, and giving popular cause to their protest.
Shortly after the prorogation, without the means in the foreseeable future to raise funds from Parliament for a European war, or the influence of Buckingham, Charles made peace with France and Spain. The following eleven years, during which Charles ruled England without a Parliament, are referred to as the personal rule or the "eleven years' tyranny". Ruling without Parliament was not exceptional, and was supported by precedent. Only Parliament, however, could legally raise taxes, and without it Charles's capacity to acquire funds for his treasury was limited to his customary rights and prerogatives.
Finances.
A large fiscal deficit had arisen in the reigns of Elizabeth I and James I. Notwithstanding Buckingham's short lived campaigns against both Spain and France, there was little financial capacity for Charles to wage wars overseas. Throughout his reign Charles was obliged to rely primarily on volunteer forces for defence and on diplomatic efforts to support his sister, Elizabeth, and his foreign policy objective for the restoration of the Palatinate. England was still the least taxed country in Europe, with no official excise and no regular direct taxation. To raise revenue without reconvening Parliament, Charles resurrected an all-but-forgotten law called the "Distraint of Knighthood", in abeyance for over a century, which required any man who earned £40 or more from land each year to present himself at the king's coronation to be knighted. Relying on this old statute, Charles fined individuals who had failed to attend his coronation in 1626.
The chief tax imposed by Charles was a feudal levy known as ship money, which proved even more unpopular, and lucrative, than poundage and tonnage before it. Previously, collection of ship money had been authorised only during wars, and only on coastal regions. Charles, however, argued that there was no legal bar to collecting the tax for defence during peacetime and throughout the whole of the kingdom. Ship money, paid directly to the Treasury of the Navy, provided between £150,000 to £200,000 annually between 1634 and 1638, after which yields declined. Opposition to ship money steadily grew, but the 12 common law judges of England declared that the tax was within the king's prerogative, though some of them had reservations. The prosecution of John Hampden for non-payment in 1637–38 provided a platform for popular protest, and the judges only found against Hampden by the narrow margin of 7–5.
The king also derived money through the granting of monopolies, despite a statute forbidding such action, which, though inefficient, raised an estimated £100,000 a year in the late 1630s. Charles also raised funds from the Scottish nobility, at the price of considerable acrimony, by the Act of Revocation (1625), whereby all gifts of royal or church land made to the nobility since 1540 were revoked, with continued ownership being subject to an annual rent. In addition, the boundaries of the royal forests in England were extended to their ancient limits as part of a scheme to maximise income by exploiting the land and fining land users within the re-asserted boundaries for encroachment.
Religious conflicts.
Throughout Charles's reign, the issue of how far the English Reformation should progress was constantly brought to the forefront of political debate. Arminian theology emphasised clerical authority and the individual's ability to reject or accept salvation, and was consequently viewed as heretical and a potential vehicle for the reintroduction of Roman Catholicism by its Calvinist opponents. Charles's sympathy to the teachings of Arminianism, and specifically his wish to move the Church of England away from Calvinism in a more traditional and sacramental direction, were perceived by Puritans as irreligious tendencies. In addition, Charles's subjects followed news of the European war closely and grew increasingly dismayed by Charles's diplomacy with Spain and his failure to support the Protestant cause abroad effectively.
In 1633, Charles appointed William Laud as Archbishop of Canterbury. Together, they began a series of anti-Calvinist reforms that attempted to ensure religious uniformity by restricting non-conformist preachers, insisting that the liturgy be celebrated as prescribed in the Book of Common Prayer, organising the internal architecture of English churches so as to emphasise the sacrament of the altar, and re-issuing King James's Declaration of Sports, which permitted secular activities on the sabbath. The Feoffees for Impropriations, an organisation that bought benefices and advowsons so that Puritans could be appointed to them, was dissolved. To prosecute those who opposed his reforms, Laud used the two most powerful courts in the land, the Court of High Commission and the Court of Star Chamber. The courts became feared for their censorship of opposing religious views, and became unpopular among the propertied classes for inflicting degrading punishments on gentlemen. For example, in 1637 William Prynne, Henry Burton and John Bastwick were pilloried, whipped and mutilated by cropping and imprisoned indefinitely for publishing anti-episcopal pamphlets.
When Charles attempted to impose his religious policies in Scotland he faced numerous difficulties. Although born in Scotland, Charles had become estranged from his northern kingdom; his first visit since early childhood was for his Scottish coronation in 1633. To the dismay of the Scots, who had removed many traditional rituals from their liturgical practice, Charles insisted that the coronation be conducted in the Anglican rite. In 1637, the king ordered the use of a new prayer book in Scotland that was almost identical to the English Book of Common Prayer, without consulting either the Scottish Parliament or the Kirk. Although written, under Charles's direction, by Scottish bishops, many Scots resisted it, seeing the new prayer book as a vehicle for introducing Anglicanism to Scotland. On 23 July, riots erupted in Edinburgh upon the first Sunday of the prayer book's usage, and unrest spread throughout the Kirk. The public began to mobilise around a re-affirmation of the National Covenant, whose signatories pledged to uphold the reformed religion of Scotland and reject any innovations that were not authorised by Kirk and Parliament. When the General Assembly of the Church of Scotland met in November 1638, it condemned the new prayer book, abolished episcopal church government by bishops, and adopted Presbyterian government by elders and deacons.
Bishops' Wars.
Charles perceived the unrest in Scotland as a rebellion against his authority, precipitating the First Bishops' War in 1639. Charles did not seek subsidies from the English Parliament to wage war, but instead raised an army without parliamentary aid and marched to Berwick-upon-Tweed, on the border of Scotland. Charles's army did not engage the Covenanters as the king feared the defeat of his forces, whom he believed to be significantly outnumbered by the Scots. In the Treaty of Berwick, Charles regained custody of his Scottish fortresses and secured the dissolution of the Covenanters' interim government, albeit at the decisive concession that both the Scottish Parliament and General Assembly of the Scottish Church were called.
Charles's military failure in the First Bishops' War caused a financial and diplomatic crisis for Charles that deepened when his efforts to raise finance from Spain, while simultaneously continuing his support for his Palatine relatives, led to the public humiliation of the Battle of the Downs, where the Dutch destroyed a Spanish bullion fleet off the coast of Kent in sight of the impotent English navy.
Charles continued peace negotiations with the Scots in a bid to gain time before launching a new military campaign. Because of his financial weakness, he was forced to call Parliament into session in an attempt to raise funds for such a venture. Both English and Irish parliaments were summoned in the early months of 1640. In March 1640, the Irish Parliament duly voted in a subsidy of £180,000 with the promise to raise an army 9,000 strong by the end of May. In the English general election in March, however, court candidates fared badly, and Charles's dealings with the English Parliament in April quickly reached stalemate. The earls of Northumberland and Strafford attempted to broker a compromise whereby the king would agree to forfeit ship money in exchange for £650,000 (although the cost of the coming war was estimated at around £1 million). Nevertheless, this alone was insufficient to produce consensus in the Commons. The Parliamentarians' calls for further reforms were ignored by Charles, who still retained the support of the House of Lords. Despite the protests of Northumberland, the Short Parliament (as it came to be known) was dissolved in May 1640, less than a month after it assembled.
By this stage Strafford, Lord Deputy of Ireland since 1632, had emerged as Charles's right-hand man and together with Laud, pursued a policy of "Thorough" that aimed to make central royal authority more efficient and effective at the expense of local or anti-government interests. Although originally a critic of the king, Strafford defected to royal service in 1628 (due in part to Buckingham's persuasion), and had since emerged, alongside Laud, as the most influential of Charles's ministers.
Bolstered by the failure of the English Short Parliament, the Scottish Parliament declared itself capable of governing without the king's consent and, in August 1640, the Covenanter army moved into the English county of Northumberland. Following the illness of the earl of Northumberland, who was the king's commander-in-chief, Charles and Strafford went north to command the English forces, despite Strafford being ill himself with a combination of gout and dysentery. The Scottish soldiery, many of whom were veterans of the Thirty Years' War, had far greater morale and training compared to their English counterparts, and met virtually no resistance until reaching Newcastle upon Tyne where, at the Battle of Newburn, they defeated the English forces and occupied the city, as well as the neighbouring county of Durham.
As demands for a parliament grew, Charles took the unusual step of summoning a great council of peers. By the time it met, on 24 September at York, Charles had resolved to follow the almost universal advice to call a parliament. After informing the peers that a parliament would convene in November, he asked them to consider how he could acquire funds to maintain his army against the Scots in the meantime. They recommended making peace. A cessation of arms, although not a final settlement, was negotiated in the humiliating Treaty of Ripon, signed in October 1640. The treaty stated that the Scots would continue to occupy Northumberland and Durham and be paid £850 per day, until peace was restored and the English Parliament recalled, which would be required to raise sufficient funds to pay the Scottish forces.
Consequently, in November Charles summoned what later became known as the Long Parliament. Once again, Charles's supporters fared badly at the polls. Of the 493 members of the Commons, over 350 were opposed to the king.
Long Parliament.
Tensions escalate.
The Long Parliament proved just as difficult for Charles as had the Short Parliament. It assembled on 3 November 1640 and quickly began proceedings to impeach the king's leading counsellors of high treason. Strafford was taken into custody on 10 November; Laud was impeached on 18 December; Lord Keeper Finch was impeached the following day, and he consequently fled to the Hague with Charles's permission on 21 December. To prevent the king from dissolving it at will, Parliament passed the Triennial Act, which required Parliament to be summoned at least once every three years, and permitted the Lord Keeper of the Great Seal and 12 peers to summon Parliament if the king failed to do so. The Act was coupled with a subsidy bill, and so to secure the latter, Charles grudgingly granted royal assent in February 1641.
Strafford had become the principal target of the Parliamentarians, particularly John Pym, and he went on trial for high treason on 22 March 1641. However, the key allegation by Sir Henry Vane that Strafford had threatened to use the Irish army to subdue England was not corroborated and on 10 April Pym's case collapsed. Pym and his allies immediately launched a bill of attainder, which simply declared Strafford guilty and pronounced the sentence of death.
Charles assured Strafford that "upon the word of a king you shall not suffer in life, honour or fortune", and the attainder could not succeed if Charles withheld assent. Furthermore, many members and most peers were opposed to the attainder, not wishing, in the words of one, to "commit murder with the sword of justice". However, increased tensions and an attempted coup by royalist army officers in support of Strafford and in which Charles was involved began to sway the issue. The Commons passed the bill on 20 April by a large margin (204 in favour, 59 opposed, and 230 abstained), and the Lords acquiesced (by 26 votes to 19, with 79 absent) in May. Charles, fearing for the safety of his family in the face of unrest, assented reluctantly on 9 May after consulting his judges and bishops. Strafford was beheaded three days later.
On 3 May, Parliament's Protestation had attacked the "wicked counsels" of Charles's "arbitrary and tyrannical government"; while those who signed the petition undertook to defend the king's "person, honour and estate", they also swore to preserve "the true reformed religion", parliament, and the "rights and liberties of the subjects". Within a week, Charles had assented to an unprecedented Act, which forbade the dissolution of the English Parliament without Parliament's consent. In the following months, ship money, fines in distraint of knighthood and excise without parliamentary consent were declared unlawful, and the Courts of Star Chamber and High Commission were abolished. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. The House of Commons also launched bills attacking bishops and episcopacy, but these failed in the Lords.
Charles had made important concessions in England, and temporarily improved his position in Scotland by securing the favour of the Scots on a visit from August to November 1641 during which he conceded to the official establishment of Presbyterianism. However, following an attempted royalist coup in Scotland, known as "The Incident", Charles's credibility was significantly undermined.
Irish rebellion.
In Ireland, the population was split into three main socio-political groups: the Gaelic Irish, who were Catholic; the Old English, who were descended from medieval Normans and were also predominantly Catholic; and the New English, who were Protestant settlers from England and Scotland aligned with the English Parliament and the Covenanters. Strafford's administration had improved the Irish economy and boosted tax revenue, but had done so by heavy-handedly imposing order. He had trained up a large Catholic army in support of the king and had weakened the authority of the Irish Parliament, while continuing to confiscate land from Catholics for Protestant settlement at the same time as promoting a Laudian Anglicanism that was anathema to Presbyterians. As a result, all three groups had become disaffected. Strafford's impeachment provided a new departure for Irish politics whereby all sides joined together to present evidence against him. In a similar manner to the English Parliament, the Old English members of the Irish Parliament argued that while opposed to Strafford they remained loyal to Charles. They argued that the king had been led astray by malign counsellors, and that, moreover, a viceroy such as Strafford could emerge as a despotic figure instead of ensuring that the king was directly involved in governance. Strafford's fall from power weakened Charles's influence in Ireland. The dissolution of the Irish army was unsuccessfully demanded three times by the English Commons during Strafford's imprisonment, until Charles was eventually forced through lack of money to disband the army at the end of Strafford's trial. Disputes concerning the transfer of land ownership from native Catholic to settler Protestant, particularly in relation to the plantation of Ulster, coupled with resentment at moves to ensure the Irish Parliament was subordinate to the Parliament of England, sowed the seeds of rebellion. When armed conflict arose between the Gaelic Irish and New English, in late October 1641, the Old English sided with the Gaelic Irish while simultaneously professing their loyalty to the king.
In November 1641, the House of Commons passed the Grand Remonstrance, a long list of grievances against actions by Charles's ministers committed since the beginning of his reign (that were asserted to be part of a grand Catholic conspiracy of which the king was an unwitting member), but it was in many ways a step too far by Pym and passed by only 11 votes – 159 to 148. Furthermore, the Remonstrance had very little support in the House of Lords, which the Remonstrance attacked. The tension was heightened by news of the Irish rebellion, coupled with inaccurate rumours of Charles's complicity. Throughout November, a series of alarmist pamphlets published stories of atrocities in Ireland, which included massacres of New English settlers by the native Irish who could not be controlled by the Old English lords. Rumours of "papist" conspiracies in England circulated the kingdom, and English anti-Catholic opinion was strengthened, damaging Charles's reputation and authority.
The English Parliament distrusted Charles's motivations when he called for funds to put down the Irish rebellion; many members of the Commons suspected that forces raised by Charles might later be used against Parliament itself. Pym's Militia Bill was intended to wrest control of the army from the king, but it did not have the support of the Lords, let alone Charles. Instead, the Commons passed the bill as an ordinance, which they claimed did not require royal assent. The Militia Ordinance appears to have prompted more members of the Lords to support the king. In an attempt to strengthen his position, Charles generated great antipathy in London, which was already fast falling into anarchy, when he placed the Tower of London under the command of Colonel Thomas Lunsford, an infamous, albeit efficient, career officer. When rumours reached Charles that Parliament intended to impeach his wife for supposedly conspiring with the Irish rebels, the king decided to take drastic action.
Five members.
Charles suspected, probably correctly, that some members of the English Parliament had colluded with the invading Scots. On 3 January, Charles directed Parliament to give up five members of the Commons – Pym, John Hampden, Denzil Holles, William Strode and Sir Arthur Haselrig – and one peer – Lord Mandeville – on the grounds of high treason. When Parliament refused, it was possibly Henrietta Maria who persuaded Charles to arrest the five members by force, which Charles intended to carry out personally. However, news of the warrant reached Parliament ahead of him, and the wanted men slipped away by boat shortly before Charles entered the House of Commons with an armed guard on 4 January 1642. Having displaced the Speaker, William Lenthall, from his chair, the king asked him where the MPs had fled. Lenthall, on his knees, famously replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." Charles abjectly declared "all my birds have flown", and was forced to retire, empty-handed.
The botched arrest attempt was politically disastrous for Charles. No English sovereign had ever entered the House of Commons, and his unprecedented invasion of the chamber to arrest its members was considered a grave breach of parliamentary privilege. In one stroke Charles destroyed his supporters' efforts to portray him as a defence against innovation and disorder.
Parliament quickly seized London, and Charles fled the capital for Hampton Court Palace on 10 January 1642, moving two days later to Windsor Castle. After sending his wife and eldest daughter to safety abroad in February, he travelled northwards, hoping to seize the military arsenal at Hull. To his dismay, he was rebuffed by the town's Parliamentary governor, Sir John Hotham, who refused him entry in April, and Charles was forced to withdraw.
English Civil War.
In mid-1642, both sides began to arm. Charles raised an army using the medieval method of commission of array, and Parliament called for volunteers for its militia. Following futile negotiations, Charles raised the royal standard in Nottingham on 22 August 1642. At the start of the First English Civil War, Charles's forces controlled roughly the Midlands, Wales, the West Country and northern England. He set up his court at Oxford. Parliament controlled London, the south-east and East Anglia, as well as the English navy.
After a few skirmishes, the opposing forces met in earnest at Edgehill, on 23 October 1642. Charles's nephew Prince Rupert of the Rhine disagreed with the battle strategy of the royalist commander Lord Lindsey, and Charles sided with Rupert. Lindsey resigned, leaving Charles to assume overall command assisted by Lord Forth. Rupert's cavalry successfully charged through the parliamentary ranks, but instead of swiftly returning to the field, rode off to plunder the parliamentary baggage train. Lindsey, acting as a colonel, was wounded and bled to death without medical attention. The battle ended inconclusively as the daylight faded.
In his own words, the experience of battle had left Charles "exceedingly and deeply grieved". He regrouped at Oxford, turning down Rupert's suggestion of an immediate attack on London. After a week, he set out for the capital on 3 November, capturing Brentford on the way while simultaneously continuing to negotiate with civic and parliamentary delegations. At Turnham Green on the outskirts of London, the royalist army met resistance from the city militia, and faced with a numerically superior force, Charles ordered a retreat. He over-wintered in Oxford, strengthening the city's defences and preparing for the next season's campaign. Peace talks between the two sides collapsed in April.
The war continued indecisively through 1643 and 1644, and Henrietta Maria returned to Britain for 17 months from February 1643. After Rupert captured Bristol in July 1643, Charles visited the port city and lay siege to Gloucester, further up the river Severn. His plan to undermine the city walls failed due to heavy rain, and on the approach of a parliamentary relief force, Charles lifted the siege and withdrew to Sudeley Castle. The parliamentary army turned back towards London, and Charles set off in pursuit. The two armies met at Newbury, Berkshire, on 20 September. Just as at Edgehill, the battle stalemated at nightfall, and the armies disengaged. In January 1644, Charles summoned a Parliament at Oxford, which was attended by about 40 peers and 118 members of the Commons; all told, the Oxford Parliament, which sat until March 1645, was supported by the majority of peers and about a third of the Commons. Charles became disillusioned by the assembly's ineffectiveness, calling it a "mongrel" in private letters to his wife.
In 1644, Charles remained in the southern half of England while Rupert rode north to relieve Newark and York, which were under threat from parliamentary and Scottish Covenanter armies. Charles was victorious at the battle of Cropredy Bridge in late June, but the royalists in the north were defeated at the battle of Marston Moor just a few days later. The king continued his campaign in the south, encircling and disarming the parliamentary army of the Earl of Essex. Returning northwards to his base at Oxford, he fought at Newbury for a second time before the winter closed in; the battle ended indecisively. Attempts to negotiate a settlement over the winter, while both sides re-armed and re-organised, were again unsuccessful.
At the battle of Naseby on 14 June 1645, Rupert's horsemen again mounted a successful charge, against the flank of Parliament's New Model Army, but Charles's troops elsewhere on the field were pushed back by the opposing forces. Charles, attempting to rally his men, rode forward but as he did so, Lord Carnwath seized his bridle and pulled him back, fearing for the king's safety. Carnwath's action was misinterpreted by the royalist soldiers as a signal to move back, leading to a collapse of their position. The military balance tipped decisively in favour of Parliament. There followed a series of defeats for the royalists, and then the Siege of Oxford, from which Charles escaped (disguised as a servant) in April 1646. He put himself into the hands of the Scottish Presbyterian army besieging Newark, and was taken northwards to Newcastle upon Tyne. After nine months of negotiations, the Scots finally arrived at an agreement with the English Parliament: in exchange for £100,000, and the promise of more money in the future, the Scots withdrew from Newcastle and delivered Charles to the parliamentary commissioners in January 1647.
Captivity.
Parliament held Charles under house arrest at Holdenby House in Northamptonshire, until Cornet George Joyce took him by threat of force from Holdenby on 3 June in the name of the New Model Army. By this time, mutual suspicion had developed between Parliament, which favoured army disbandment and Presbyterianism, and the New Model Army, which was primarily officered by Independent non-conformists who sought a greater political role. Charles was eager to exploit the widening divisions, and apparently viewed Joyce's actions as an opportunity rather than a threat. He was taken first to Newmarket, at his own suggestion, and then transferred to Oatlands and subsequently Hampton Court, while more ultimately fruitless negotiations took place. By November, he determined that it would be in his best interests to escape – perhaps to France, Southern England or to Berwick-upon-Tweed, near the Scottish border. He fled Hampton Court on 11 November, and from the shores of Southampton Water made contact with Colonel Robert Hammond, Parliamentary Governor of the Isle of Wight, whom he apparently believed to be sympathetic. Hammond, however, confined Charles in Carisbrooke Castle and informed Parliament that Charles was in his custody.
From Carisbrooke, Charles continued to try to bargain with the various parties. In direct contrast to his previous conflict with the Scottish Kirk, on 26 December 1647 he signed a secret treaty with the Scots. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles's behalf and restore him to the throne on condition that Presbyterianism be established in England for three years.
The royalists rose in May 1648, igniting the Second Civil War, and as agreed with Charles, the Scots invaded England. Uprisings in Kent, Essex, and Cumberland, and a rebellion in South Wales, were put down by the New Model Army, and with the defeat of the Scots at the Battle of Preston in August 1648, the royalists lost any chance of winning the war.
Charles's only recourse was to return to negotiations, which were held at Newport on the Isle of Wight. On 5 December 1648, Parliament voted by 129 to 83 to continue negotiating with the king, but Oliver Cromwell and the army opposed any further talks with someone they viewed as a bloody tyrant and were already taking action to consolidate their power. Hammond was replaced as Governor of the Isle of Wight on 27 November, and placed in the custody of the army the following day. In Pride's Purge on 6 and 7 December, the members of Parliament out of sympathy with the military were arrested or excluded by Colonel Thomas Pride, while others stayed away voluntarily. The remaining members formed the Rump Parliament. It was effectively a military coup.
Trial.
Charles was moved to Hurst Castle at the end of 1648, and thereafter to Windsor Castle. In January 1649, the Rump House of Commons indicted him on a charge of treason, which was rejected by the House of Lords. The idea of trying a king was a novel one. The Chief Justices of the three common law courts of England – Henry Rolle, Oliver St John and John Wilde – all opposed the indictment as unlawful. The Rump Commons declared itself capable of legislating alone, passed a bill creating a separate court for Charles's trial, and declared the bill an act without the need for royal assent. The High Court of Justice established by the Act consisted of 135 commissioners, but many either refused to serve or chose to stay away. Only 68 (all firm Parliamentarians) attended Charles's trial on charges of high treason and "other high crimes" that began on 20 January 1649 in Westminster Hall. John Bradshaw acted as President of the Court, and the prosecution was led by the Solicitor General, John Cook.
Charles was accused of treason against England by using his power to pursue his personal interest rather than the good of the country. The charge stated that he, "for accomplishment of such his designs, and for the protecting of himself and his adherents in his and their wicked practices, to the same ends hath traitorously and maliciously levied war against the present Parliament, and the people therein represented", and that the "wicked designs, wars, and evil practices of him, the said Charles Stuart, have been, and are carried on for the advancement and upholding of a personal interest of will, power, and pretended prerogative to himself and his family, against the public interest, common right, liberty, justice, and peace of the people of this nation." Reflecting the modern concept of command responsibility, the indictment held him "guilty of all the treasons, murders, rapines, burnings, spoils, desolations, damages and mischiefs to this nation, acted and committed in the said wars, or occasioned thereby." An estimated 300,000 people, or 6% of the population, died during the war.
Over the first three days of the trial, whenever Charles was asked to plead, he refused, stating his objection with the words: "I would know by what power I am called hither, by what lawful authority...?" He claimed that no court had jurisdiction over a monarch, that his own authority to rule had been given to him by God and by the traditional laws of England, and that the power wielded by those trying him was only that of force of arms. Charles insisted that the trial was illegal, explaining that, The court, by contrast, challenged the doctrine of sovereign immunity, and proposed that "the King of England was not a person, but an office whose every occupant was entrusted with a limited power to govern 'by and according to the laws of the land and not otherwise'."
At the end of the third day, Charles was removed from the court, which then heard over 30 witnesses against the king in his absence over the next two days, and on 26 January condemned him to death. The following day, the king was brought before a public session of the commission, declared guilty and sentenced. Fifty-nine of the commissioners signed Charles's death warrant.
Execution.
Charles's decapitation was scheduled for Tuesday, 30 January 1649. Two of his children remained in England under the control of the Parliamentarians: Elizabeth and Henry. They were permitted to visit him on 29 January, and he bid them a tearful farewell. The following morning, he called for two shirts to prevent the cold weather causing any noticeable shivers that the crowd could have mistaken for fear:
He walked under guard from St James's Palace, where he had been confined, to the Palace of Whitehall, where an execution scaffold was erected in front of the Banqueting House. Charles was separated from spectators by large ranks of soldiers, and his last speech reached only those with him on the scaffold. He blamed his fate on his failure to prevent the execution of his loyal servant Strafford: "An unjust sentence that I suffered to take effect, is punished now by an unjust sentence on me." He declared that he had desired the liberty and freedom of the people as much as any, "but I must tell you that their liberty and freedom consists in having government ... It is not their having a share in the government; that is nothing appertaining unto them. A subject and a sovereign are clean different things." He continued, "I shall go from a corruptible to an incorruptible Crown, where no disturbance can be."
At about 2 p.m., Charles put his head on the block after saying a prayer and signalled the executioner when he was ready by stretching out his hands; he was then beheaded with one clean stroke. According to observer Philip Henry, a moan "as I never heard before and desire I may never hear again" rose from the assembled crowd, some of whom then dipped their handkerchiefs in the king's blood as a memento.
The executioner was masked and disguised, and there is debate over his identity. The commissioners approached Richard Brandon, the common hangman of London, but he refused, at least at first, despite being offered £200. It is possible he relented and undertook the commission after being threatened with death, but there are others who have been named as potential candidates, including George Joyce, William Hulet and Hugh Peters. The clean strike, confirmed by an examination of the king's body at Windsor in 1813, suggests that the execution was carried out by an experienced headsman.
It was common practice for the severed head of a traitor to be held up and exhibited to the crowd with the words "Behold the head of a traitor!" Although Charles's head was exhibited, the words were not used, possibly because the executioner did not want his voice recognised. On the day after the execution, the king's head was sewn back onto his body, which was then embalmed and placed in a lead coffin.
The commission refused to allow Charles's burial at Westminster Abbey, so his body was conveyed to Windsor on the night of 7 February. He was buried in private in the Henry VIII vault alongside the coffins of Henry VIII and Henry's third wife, Jane Seymour, in St George's Chapel, Windsor Castle, on 9 February 1649. The king's son, Charles II, later planned for an elaborate royal mausoleum to be erected in Hyde Park, London, but it was never built.
Legacy.
Ten days after Charles's execution, on the day of his interment, a memoir purporting to be written by the king appeared for sale. This book, the "Eikon Basilike" (Greek: the "Royal Portrait"), contained an "apologia" for royal policies, and it proved an effective piece of royalist propaganda. John Milton wrote a Parliamentary rejoinder, the "Eikonoklastes" ("The Iconoclast"), but the response made little headway against the pathos of the royalist book. Anglicans and royalists fashioned an image of martyrdom, and Charles was recognised as a martyr king by his followers. From the latter half of the seventeenth century, high Anglicans commemorated his martyrdom on the anniversary of his death and churches, such as those at Falmouth and Tunbridge Wells, were founded in his honour.
Partly inspired by his visit to the Spanish court in 1623, Charles became a passionate and knowledgeable art collector, amassing one of the finest art collections ever assembled. His intimate courtiers including the Duke of Buckingham and the Earl of Arundel shared his interest and have been dubbed the Whitehall group. In Spain, he sat for a sketch by Velázquez, and acquired works by Titian and Correggio, among others. In England, his commissions included the ceiling of the Banqueting House, Whitehall, by Rubens and paintings by other artists from the Low Countries such as van Honthorst, Mytens, and van Dyck. In 1627 and 1628, he purchased the entire collection of the Duke of Mantua, which included work by Titian, Correggio, Raphael, Caravaggio, del Sarto and Mantegna. Charles's collection grew further to encompass Bernini, Breughel, da Vinci, Holbein, Hollar, Tintoretto and Veronese, and self-portraits by both Dürer and Rembrandt. By Charles's death, there were an estimated 1760 paintings, most of which were sold and dispersed by Parliament.
With the monarchy overthrown, England became a republic or "Commonwealth". The House of Lords was abolished by the Rump Commons, and executive power was assumed by a Council of State. All significant military opposition in Britain and Ireland was extinguished by the forces of Oliver Cromwell in the Third English Civil War and the Cromwellian conquest of Ireland. Cromwell forcibly disbanded the Rump Parliament in 1653, thereby establishing The Protectorate with himself as Lord Protector. Upon his death in 1658, he was briefly succeeded by his ineffective son, Richard. Parliament was reinstated, and the monarchy was restored to Charles I's eldest son, Charles II, in 1660.
Assessments.
In the words of John Philipps Kenyon, "Charles Stuart is a man of contradictions and controversy". Revered by high Tories who considered him a saintly martyr, he was condemned by Whig historians, such as Samuel Rawson Gardiner, who thought him duplicitous and delusional. In recent decades, most historians have criticised him, the main exception being Kevin Sharpe who offered a more sympathetic view of Charles that has not been widely adopted. While Sharpe argued that the king was a dynamic man of conscience, Professor Barry Coward thought Charles "was the most incompetent monarch of England since Henry VI", a view shared by Ronald Hutton, who called him "the worst king we have had since the Middle Ages".
Archbishop William Laud, who was beheaded by Parliament during the war, described Charles as "A mild and gracious prince who knew not how to be, or how to be made, great." Charles was more sober and refined than his father, but he was intransigent and deliberately pursued unpopular policies that ultimately brought ruin on himself. Both Charles and James were advocates of the divine right of kings, but while James's ambitions concerning absolute prerogative were tempered by compromise and consensus with his subjects, Charles believed that he had no need to compromise or even to explain his actions. He thought that he was answerable only to God. "Princes are not bound to give account of their actions," he wrote, "but to God alone".
Titles, styles, honours and arms.
Titles and styles.
The official style of Charles I as king was "Charles, by the Grace of God, King of England, Scotland, France and Ireland, Defender of the Faith, etc." The style "of France" was only nominal, and was used by every English monarch from Edward III to George III, regardless of the amount of French territory actually controlled. The authors of his death warrant referred to him as "Charles Stuart, King of England".
Arms.
As Duke of York, Charles bore the royal arms of the kingdom differenced by a label Argent of three points, each bearing three torteaux Gules. The Prince of Wales bore the royal arms differenced by a plain label Argent of three points. As king, Charles bore the royal arms undifferenced: Quarterly, I and IV Grandquarterly, Azure three fleurs-de-lis Or (for France) and Gules three lions passant guardant in pale Or (for England); II Or a lion rampant within a tressure flory-counter-flory Gules (for Scotland); III Azure a harp Or stringed Argent (for Ireland). In Scotland, the Scottish arms were placed in the first and fourth quarters with the English and French arms in the second quarter.
Issue.
Charles had nine children, two of whom eventually succeeded as king, and two of whom died at or shortly after birth.

</doc>
<doc id="7431" url="http://en.wikipedia.org/wiki?curid=7431" title="Counter-Strike">
Counter-Strike

Counter-Strike is a first-person shooter video game developed by Valve Corporation. It was initially developed and released as a "Half-Life" modification by Minh "Gooseman" Le and Jess "Cliffe" Cliffe in 1999, before Le and Cliffe were hired and the game's intellectual property acquired. "Counter-Strike" was first released by Valve on the Microsoft Windows platform in 2000. The game later spawned a franchise, and is the first installment in the "Counter-Strike" series. Several remakes and Ports of "Counter-Strike" have been released on the Xbox console, as well as OS X and Linux.
Set in various locations around the globe, players assume the roles of members of combating teams that include counter-terrorists and terrorists. During each round of gameplay, the two teams are tasked with defeating the other by the means of either achieving the map's objectives, or else killing all of the enemy combatants. Each player may customize their arsenal of weapons and accessories at the beginning of every match, with the currency earned after each round.
As of August 2011, the "Counter-Strike" franchise has sold over 25 million units.
Gameplay.
"Counter-Strike" is a first-person shooter game in which players join either the terrorist team, the counter-terrorist team, or become spectators. Each team attempts to complete their mission objective and/or eliminate the opposing team. Each round starts with the two teams spawning simultaneously.
The objectives vary depending on the type of map, and these are the most usual ones:
A player can choose to play as one of eight different default character models (four for each side, although "" added two extra models, bringing the total to ten). Players are generally given a few seconds before the round begins (known as "freeze time") to prepare and buy equipment, during which they cannot attack or move. They can return to the buy area within a set amount of time to buy more equipment (some custom maps included neutral "buy zones" that could be used by both teams). Once the round has ended, surviving players retain their equipment for use in the next round; players who were killed begin the next round with the basic default starting equipment.
Standard monetary bonuses are awarded for winning a round, losing a round, killing an enemy, being the first to instruct a hostage to follow, rescuing a hostage, planting the bomb (Terrorist) or defusing the bomb (Counter Terrorist).
The scoreboard displays team scores in addition to statistics for each player: name, kills, deaths, and ping (in milliseconds). The scoreboard also indicates whether a player is dead, carrying the bomb (on bomb maps), or is the VIP (on assassination maps), although information on players on the opposing team is hidden from a player until his/her death, as this information can be important.
Killed players become "spectators" for the duration of the round; they cannot change their names before their next spawn, text chat cannot be sent to or received from live players, and voice chat can only be received from live players and not sent to them. Spectators are generally able to watch the rest of the round from multiple selectable views, although some servers disable some of these views to prevent dead players from relaying information about living players to their teammates through alternative media (most notably voice in the case of Internet cafes and Voice over IP programs such as TeamSpeak or Ventrilo). This form of cheating is known as "ghosting."
Development.
"Counter-Strike" is itself a mod, and it has developed its own community of script writers and mod creators. Some mods add bots, while others remove features of the game, and others create different modes of play. Some mods, often called "admin plugins", give server administrators more flexible and efficient control over his or her server. There are some mods which affect gameplay heavily, such as Gun Game, where players start with a basic pistol and must score kills to receive better weapons, and Zombie Mod, where one team consists of zombies and must "spread the infection" by killing the other team (using only the knife). There are also the Superhero and mods which mix the first-person gameplay of "Counter-Strike" with an experience system, allowing a player to become more powerful as they continue to play. The game is also highly customizable on the player's end, allowing the user to install or even create their own custom skins, HUDs, spray graphics, sprites, and sound effects, given the proper tools.
Cheating.
"Counter-Strike" has been a big target for exploitation by cheaters since its release. In-game, cheating is often referred to as "hacking" in reference to programs or "hacks" executed by the client.
Valve Anti-Cheat.
Valve has implemented an anti-cheat system called Valve Anti-Cheat (VAC). Players cheating on a VAC-enabled server risk having their account permanently banned from all VAC-secured servers.
With the first version of VAC, a ban took hold almost instantly after being detected and the cheater had to wait 2 years to have the account unbanned. Since VAC's second version, cheaters are not banned automatically. With the second version, Valve instituted a policy of 'delayed bans,' the theory being that if a new hack is developed which circumvents the VAC system, it will spread amongst the 'cheating' community. By delaying the initial ban, Valve hopes to identify (and ban) as many cheaters as possible. Like any software detection system, some cheats are not detected by VAC. To remedy this, some servers implement a voting system, in which case players can call for a vote to kick or ban the accused cheater. VAC's success at identifying cheats and banning those who use them has also provided a boost in the purchasing of private cheats. These cheats are updated frequently to minimize the risk of detection, and are generally only available to a trusted list of recipients who collectively promise not to reveal the underlying design. Even with private cheats however, some servers have alternative anticheats to coincide with VAC itself. This can help with detecting "some" cheaters, but most paid for cheats are designed to bypass these alternative server-based anticheats.
Release.
When "Counter-Strike" was published by Sierra Entertainment/Vivendi Universal Games, it was bundled with "Team Fortress Classic", "" multiplayer, and the "Wanted", "Half-Life: Absolute Redemption" and "Firearms" mods."
On March 24, 1999, Planet Half-Life opened its "Counter-Strike" section. Within two weeks, the site had received 10,000 hits. On June 19, 1999, the first public beta of "Counter-Strike" was released, followed by numerous further "beta" releases. On April 12, 2000, Valve announced that the "Counter-Strike" developers and Valve had teamed up. In January 2013, Valve began testing a version of "Counter-Strike" for OS X and Linux, eventually releasing the update to all users in April 2013.
Reception.
Upon its retail release, "Counter-Strike" received highly favorable reviews. The New York Times reported that E-Sports Entertainment ESEA League started the first professional fantasy e-sports league in 2004 with the game "Counter-Strike". Some credit the move into professional competitive team play with prizes as a major factor in "Counter-Strike"'s longevity and success.
Controversy.
"Counter-Strike" faced controversy in April 2007 when Jack Thompson, a now-disbarred attorney from Florida, speculated that the perpetrator of the Virginia Tech Massacre had been trained to kill in the game, well before Seung-Hui Cho (the shooter) was identified. News sources originally stated that Seung-Hui Cho only played the game in high school, however no video games whatsoever were found in the gunman's dorm room, and there is no evidence that he ever played "Counter-Strike". Thompson also blamed "Counter-Strike" for the February 14, 2008 Northern Illinois University shooting perpetrated by Steven Kazmierczak on the day after the shooting. It is reported that Kazmierczak did play "Counter-Strike" in college.
On January 17, 2008, a Brazilian federal court order prohibiting all sales of "Counter-Strike" and "EverQuest" and imposing the immediate withdrawal of these from all stores began to be enforced. The federal Brazilian judge Carlos Alberto Simões de Tomaz, of the Minas Gerais judiciary section, ordered the ban in October 2007 because, according to him, the games "bring imminent stimulus to the subversion of the social order, attempting against the democratic and rightful state and against the public safety." The move has been described by media as a publicity stunt on the regulation of video game violence and sexually explicit content, and also as a hasty decision that ignored much more violent games. As all versions of "Counter-Strike" were very popular in Brazil at the time, the decision was met with considerable uproar by the Brazilian gaming community. Some media have reported that the game tested by the judge contained mods likening the scenario to Rio de Janeiro's favelas and adding Brazilian Military Police uniforms to player models, which might have worsened the case for "Counter-Strike". The game's developer Valve did not comment on the episode. As of June 18, 2009, a regional federal court order lifting the prohibition on the sale of "Counter-Strike" was published. The game is now being sold again in Brazil.
Legacy.
Following the success of the first "Counter-Strike", Valve went on to make multiple sequels to the game. ', a game using "Counter-Strike's" GoldSrc engine, was released in 2004. ', a remake of the original "Counter-Strike" game, was the first in the series to use Valve's Source engine and was also released in 2004, only eight months after the release of "Counter-Strike: Condition Zero". The next game in the "Counter-Strike" series to be developed primarily by Valve Corporation was "", released for Windows, OS X, Linux, PlayStation 3, and Xbox 360 in 2012.
The game also spawned multiple spin-offs in the form of arcade games developed by Nexon Corporation and targeted primarily at Asian gaming markets. Three "Counter-Strike" games have been developed and released by Nexon Corporation thus far, "Counter-Strike Neo", "Counter-Strike Online", and "Counter-Strike Online 2".

</doc>
<doc id="7434" url="http://en.wikipedia.org/wiki?curid=7434" title="Camille Pissarro">
Camille Pissarro

Camille Pissarro (; 10 July 1830 – 13 November 1903) was a Danish-French Impressionist and Neo-Impressionist painter born on the island of St Thomas (now in the US Virgin Islands, but then in the Danish West Indies). His importance resides in his contributions to both Impressionism and Post-Impressionism. Pissarro studied from great forerunners, including Gustave Courbet and Jean-Baptiste-Camille Corot. He later studied and worked alongside Georges Seurat and Paul Signac when he took on the Neo-Impressionist style at the age of 54.
In 1873 he helped establish a collective society of fifteen aspiring artists, becoming the "pivotal" figure in holding the group together and encouraging the other members. Art historian John Rewald called Pissarro the "dean of the Impressionist painters", not only because he was the oldest of the group, but also "by virtue of his wisdom and his balanced, kind, and warmhearted personality". Cézanne said "he was a father for me. A man to consult and a little like the good Lord," and he was also one of Gauguin's masters. Renoir referred to his work as "revolutionary", through his artistic portrayals of the "common man", as Pissarro insisted on painting individuals in natural settings without "artifice or grandeur".
Pissarro is the only artist to have shown his work at all eight Paris Impressionist exhibitions, from 1874 to 1886. He "acted as a father figure not only to the Impressionists" but to all four of the major Post-Impressionists, including Georges Seurat, Paul Cézanne, Vincent van Gogh and Paul Gauguin.
Early years.
Jacob Abraham Camille Pissarro was born on 10 July 1830 on the island of St. Thomas to Frederick and Rachel Manzano de Pissarro. His father, who was of Portuguese Jewish descent, held French nationality and his mother was native Creole. His father was a merchant who came to the island from France to deal with the business affairs of a deceased uncle, and married his widow. The marriage, however, caused a stir within St. Thomas' small Jewish community, either because Rachel was outside the faith or because she was previously married to Frederick's uncle, and in subsequent years his four children were forced to attend the all-black primary school. Upon his death, his will specified that his estate be split equally between the synagogue and St. Thomas' Protestant church.
When Camille was twelve his father sent him to boarding school in France. He studied at the Savary Academy in Passy near Paris. While a young student, he developed an early appreciation of the French art masters. Monsieur Savary himself gave him a strong grounding in drawing and painting and suggested he draw from nature when he returned to St. Thomas, which he did when he was seventeen. However, his father preferred he work in his business, giving him a job working as a cargo clerk. He took every opportunity during those next five years at the job to practice drawing during breaks and after work.
When he turned twenty-one, Danish artist Fritz Melbye, then living on St. Thomas, inspired Pissarro to take on painting as a full-time profession, becoming his teacher and friend. Pissarro then chose to leave his family and job and live in Venezuela, where he and Melbye spent the next two years working as artists in Caracas and La Guaira. He drew everything he could, including landscapes, village scenes, and numerous sketches, enough to fill up multiple sketchbooks. In 1855 he moved back to Paris where he began working as assistant to Anton Melbye, Fritz Melbye's brother.
Life in France.
In Paris he worked as assistant to Danish painter Anton Melbye. He also studied paintings by other artists whose style impressed him: Courbet, Charles-François Daubigny, Jean-François Millet, and Corot. He also enrolled in various classes taught by masters, at schools such as École des Beaux-Arts and Académie Suisse. But Pissarro eventually found their teaching methods "stifling," states art historian John Rewald. This prompted him to search for alternative instruction, which he requested and received from Corot.
Paris Salon and Corot's influence.
His initial paintings were in accord with the standards at the time to be displayed at the Paris Salon, the official body whose academic traditions dictated the kind of art that was acceptable. The Salon's annual exhibition was essentially the only marketplace for young artists to gain exposure. As a result, Pissarro worked in the traditional and prescribed manner to satisfy the tastes of its official committee.
In 1859 his first painting was accepted and exhibited. His other paintings during that period were influenced by Camille Corot, who tutored him. He and Corot both shared a love of rural scenes painted from nature. It was by Corot that Pissarro was inspired to paint outdoors, also called "plein air" painting. Pissarro found Corot, along with the work of Gustave Courbet, to be "statements of pictorial truth," writes Rewald. He discussed their work often. Jean-François Millet was another whose work he admired, especially his "sentimental renditions of rural life".
Use of natural outdoor settings.
During this period Pissarro began to understand and appreciate the importance of expressing on canvas the beauties of nature without adulteration. After a year in Paris, he therefore began to leave the city and paint scenes in the countryside to capture the daily reality of village life. He found the French countryside to be "picturesque," and worthy of being painted. It was still mostly agricultural and sometimes called the "golden age of the peasantry". Pissarro later explained the technique of painting outdoors to a student:
Corot, however, would complete his own scenic paintings back in his studio where they would often be revised to his preconceptions. Pissarro, on the other hand, preferred to finish his paintings outdoors, often at one sitting, which gave his work a more realistic feel. As a result, his art was sometimes criticised as being "vulgar," because he painted what he saw: "rutted and edged hodgepodge of bushes, mounds of earth, and trees in various stages of development." According to one source, details such as those were equivalent to today's art showing garbage cans or beer bottles on the side of a street scene. This difference in style created disagreements between Pissarro and Corot.
With Monet, Cézanne, and Guillaumin.
In 1859, while attending the free school, the Académie Suisse, Pissarro became friends with a number of younger artists who likewise chose to paint in the more realistic style. Among them were Claude Monet, Armand Guillaumin and Paul Cézanne. What they shared in common was their dissatisfaction with the dictates of the Salon. Cézanne's work had been mocked at the time by the others in the school, and, writes Rewald, in his later years Cézanne "never forgot the sympathy and understanding with which Pissarro encouraged him." As a part of the group, Pissarro was comforted from knowing he was not alone, and that others similarly struggled with their art.
Pissarro agreed with the group about the importance of portraying individuals in natural settings, and expressed his dislike of any artifice or grandeur in his works, despite what the Salon demanded for its exhibits. In 1863 almost all of the group's paintings were rejected by the Salon, and French Emperor Napoleon III instead decided to place their paintings in a separate exhibit hall, the Salon des Refusés. However, only works of Pissarro and Cézanne were included, and the separate exhibit brought a hostile response from both the officials of the Salon and the public.
In subsequent Salon exhibits of 1865 and 1866, Pissarro acknowledged his influences from Melbye and Corot, whom he listed as his masters in the catalogue. But in the exhibition of 1868 he no longer credited other artists as an influence, in effect declaring his independence as a painter. This was noted at the time by art critic and author Émile Zola, who offered his opinion:
Another writer tries to describe elements of Pissarro's style:
And though, on orders from the hanging Committee and the Marquis de Chennevières, Pissarro's paintings of Pontoise for example had been skyed, hung near the ceiling, this did not prevent Jules-Antoine Castagnary from noting that the qualities of his paintings had been observed by art lovers. At the age of thirty-eight, Pissarro had begun to win himself a reputation as a landscapist to rival Corot and Daubigny.
In the late 1860s or early 1870s, Pissarro became fascinated with Japanese prints, which influenced his desire to experiment in new compositions. He described the art to his son Lucien:
Marriage and children.
In 1871 he married his mother's maid, Julie Vellay, a vineyard grower's daughter, with whom he would later have seven children. They lived outside of Paris in Pontoise and later in Louveciennes, both of which places inspired many of his paintings including scenes of village life, along with rivers, woods, and people at work. He also kept in touch with the other artists of his earlier group, especially Monet, Renoir, Cézanne, and Frédéric Bazille.
The London years.
After the outbreak of the Franco-Prussian War of 1870–71, having only Danish nationality and being unable to join the army, he moved his family to Norwood, then a village on the edge of London. However, his style of painting, which was a forerunner of what was later called "Impressionism", did not do well. He wrote to his friend, Theodore Duret, that "my painting doesn't catch on, not at all ..."
Pissarro met the Paris art dealer Paul Durand-Ruel, in London, who became the dealer who helped sell his art for most of his life. Durand-Ruel put him in touch with Monet who was likewise in London during this period. They both viewed the work of British landscape artists John Constable and J. M. W. Turner, which confirmed their belief that their style of open air painting gave the truest depiction of light and atmosphere, an effect that they felt could not be achieved in the studio alone. Pissarro's paintings also began to take on a more spontaneous look, with loosely blended brushstrokes and areas of impasto, giving more depth to the work.
Through the paintings Pissarro completed at this time, he records Sydenham and the Norwoods at a time when they were just recently connected by railways, but prior to the expansion of suburbia. One of the largest of these paintings is a view of "St. Bartholomew's Church" at Lawrie Park Avenue, commonly known as , in the collection of the London National Gallery. Twelve oil paintings date from his stay in Upper Norwood and are listed and illustrated in the catalogue raisonné prepared jointly by his fifth child Ludovic-Rodolphe Pissarro and Lionello Venturi and published in 1939. These paintings include "Norwood Under the Snow", and "Lordship Lane Station", views of The Crystal Palace relocated from Hyde Park, "Dulwich College", "Sydenham Hill", "All Saints Church Upper Norwood", and a lost painting of St. Stephen's Church.
Returning to France, in 1890 Pissarro again visited England and painted some ten scenes of central London. He came back again in 1892, painting in Kew Gardens and Kew Green, and also in 1897, when he produced several oils described as being of Bedford Park, Chiswick, but in fact all being of the nearby Stamford Brook area except for one of Bath Road, which runs from Stamford Brook along the south edge of Bedford Park.
French Impressionism.
When Pissarro returned to his home in France after the war, he discovered that of the 1,500 paintings he had done over 20 years, which he was forced to leave behind when he moved to London, only 40 remained. The rest had been damaged or destroyed by the soldiers, who often used them as floor mats outside in the mud to keep their boots clean. It is assumed that many of those lost were done in the Impressionist style he was then developing, thereby "documenting the birth of Impressionism." Armand Silvestre, a critic, went so far as to call Pissarro "basically the inventor of this [Impressionist] painting"; however, Pissarro's role in the Impressionist movement was "less that of the great man of ideas than that of the good counselor and appeaser ..." "Monet ... could be seen as the guiding force."
He soon reestablished his friendships with the other Impressionist artists of his earlier group, including Cézanne, Monet, Manet, Renoir, and Degas. Pissarro now expressed his opinion to the group that he wanted an alternative to the Salon so their group could display their own unique styles.
To assist in that endeavour, in 1873 he helped establish a separate collective, called the "Société Anonyme des Artistes, Peintres, Sculpteurs et Graveurs," which included fifteen artists. Pissarro created the group's first charter and became the "pivotal" figure in establishing and holding the group together. One writer noted that with his prematurely grey beard, the forty-three-year-old Pissarro was regarded as a "wise elder and father figure" by the group. Yet he was able to work alongside the other artists on equal terms due to his youthful temperament and creativity. Another writer said of him that "he has unchanging spiritual youth and the look of an ancestor who remained a young man".
Impressionist exhibitions that shocked the critics.
The following year, in 1874, the group held their first 'Impressionist' Exhibition, which shocked and "horrified" the critics, who primarily appreciated only scenes portraying religious, historical, or mythological settings. They found fault with the Impressionist paintings on many grounds:
A "revolutionary" style.
Pissarro showed five of his paintings, all landscapes, at the exhibit, and again Émile Zola praised his art and that of the others. In the Impressionist exhibit of 1876; however, art critic Albert Wolff complained in his review, "Try to make M. Pissarro understand that trees are not violet, that sky is not the color of fresh butter ..." Journalist and art critic Octave Mirbeau on the other hand, writes, "Camille Pissarro has been a revolutionary through the revitalized working methods with which he has endowed painting".
According to Rewald, Pissarro had taken on an attitude more simple and natural than the other artists. He writes:
In later years, Cézanne also recalled this period and referred to Pissarro as "the first Impressionist". In 1906, a few years after Pissarro's death, Cézanne, then 67 and a role model for the new generation of artists, paid Pissarro a debt of gratitude by having himself listed in an exhibition catalogue as "Paul Cézanne, pupil of Pissarro".
Pissarro, Degas, and American impressionist Mary Cassatt planned a journal of their original prints in the late 1870s, a project that nevertheless came to nothing when Degas withdrew. Art historian and the artist's great-grandson Joachim Pissarro notes that they "professed a passionate disdain for the Salons and refused to exhibit at them." Together they shared an "almost militant resolution" against the Salon, and through their later correspondences it is clear that their mutual admiration "was based on a kinship of ethical as well as aesthetic concerns".
Cassatt had befriended Degas and Pissarro years earlier when she joined Pissarro's newly formed French Impressionist group and gave up opportunities to exhibit in the United States. She and Pissarro were often treated as "two outsiders" by the Salon since neither were French or had become French citizens. However, she was "fired up with the cause" of promoting Impressionism and looked forward to exhibiting "out of solidarity with her new friends". Towards the end of the 1890s she began to distance herself from the Impressionists, avoiding Degas at times she did not have the strength to defend herself against his "wicked tongue". Instead, she came to prefer the company of "the gentle Camille Pissarro", with whom she could speak frankly about the changing attitudes toward art. She once described him as a teacher "that could have taught the stones to draw correctly."
Neo-Impressionist period.
By the 1880s, Pissarro began to explore new themes and methods of painting to break out of what he felt was an artistic "mire". As a result, Pissarro went back to his earlier themes by painting the life of country people, which he had done in Venezuela in his youth. Degas described Pissarro's subjects as "peasants working to make a living".
However, this period also marked the end of the Impressionist period due to Pissarro's leaving the movement. As Joachim Pissarro points out, "Once such a die-hard Impressionist as Pissarro had turned his back on Impressionism, it was apparent that Impressionism had no chance of surviving ..."
It was Pissarro's intention during this period to help "educate the public" by painting people at work or at home in realistic settings, without idealising their lives. Renoir, in 1882, referred to Pissarro's work during this period as "revolutionary," in his attempt to portray the "common man." Pissarro himself did not use his art to overtly preach any kind of political message, however, although his preference for painting humble subjects was intended to be seen and purchased by his upper class clientele. He also began painting with a more unified brushwork along with pure strokes of color.
Studying with Seurat and Signac.
In 1885 he met Georges Seurat and Paul Signac, both of whom relied on a more "scientific" theory of painting by using very small patches of pure colours to create the illusion of blended colours and shading when viewed from a distance. Pissarro then spent the years from 1885 to 1888 practising this more time-consuming and laborious technique, referred to as pointillism. The paintings that resulted were distinctly different from his Impressionist works, and were on display in the 1886 Impressionist Exhibition, but under a separate section, along with works by Seurat, Signac, and his son Lucien.
All four works were considered an "exception" to the eighth exhibition. Joachim Pissarro notes that virtually every reviewer who commented on Pissarro's work noted "his extraordinary capacity to change his art, revise his position and take on new challenges." One critic writes:
Pissarro explained the new art form as a "phase in the logical march of Impressionism", but he was alone among the other Impressionists with this attitude, however. Joachim Pissarro states that Pissarro thereby became the "only artist who went from Impressionism to Neo-Impressionism".
In 1884, art dealer Theo van Gogh asked Pissarro if he would take in his older brother, Vincent, as a boarder in his home. Lucien Pissarro wrote that his father was impressed by Van Gogh's work and had "foreseen the power of this artist", who was 23 years younger. Although Van Gogh never boarded with him, Pissarro did explain to him the various ways of finding and expressing light and color, ideas which he later used in his paintings, notes Lucien.
Abandoning Neo-Impressionism.
Pissarro eventually turned away from Neo-Impressionism, claiming its system was too artificial. He explains in a letter to a friend:
However, after reverting to his earlier style, his work became, according to Rewald, "more subtle, his color scheme more refined, his drawing firmer ... So it was that Pissarro approached old age with an increased mastery."
But the change also added to Pissarro's continual financial hardship which he felt until his 60s. His "headstrong courage and a tenacity to undertake and sustain the career of an artist", writes Joachim Pissarro, was due to his "lack of fear of the immediate repercussions" of his stylistic decisions. In addition, his work was strong enough to "bolster his morale and keep him going", he writes. His Impressionist contemporaries, however, continued to view his independence as a "mark of integrity", and they turned to him for advice, referring to him as "Père Pissarro" (father Pissarro).
Later years.
In his older age Pissarro suffered from a recurring eye infection that prevented him from working outdoors except in warm weather. As a result of this disability, he began painting outdoor scenes while sitting by the window of hotel rooms. He often chose hotel rooms on upper levels to get a broader view. He moved around northern France and painted from hotels in Rouen, Paris, Le Havre and Dieppe. On his visits to London, he would do the same.
Pissarro died in Paris on 13 November 1903 and was buried in Père Lachaise Cemetery.
Legacy and influence.
During the period Pissarro exhibited his works, art critic Armand Silvestre had called Pissarro the "most real and most naive member" of the Impressionist group. His work has also been described by art historian Diane Kelder as expressing "the same quiet dignity, sincerity, and durability that distinguished his person." She adds that "no member of the group did more to mediate the internecine disputes that threatened at times to break it apart, and no one was a more diligent proselytizer of the new painting."
According to Pissarro's son, Lucien, his father painted regularly with Cézanne beginning in 1872. He recalls that Cézanne walked a few miles to join Pissarro at various settings in Pontoise. While they shared ideas during their work, the younger Cézanne wanted to study the countryside through Pissarro's eyes, as he admired Pissarro's landscapes from the 1860s. Cézanne, although only nine years younger than Pissarro, said that "he was a father for me. A man to consult and a little like the good Lord."
Lucien Pissarro was taught painting by his father, and described him as a "splendid teacher, never imposing his personality on his pupil." Gauguin, who also studied under him, referred to Pissarro "as a force with which future artists would have to reckon". Art historian Diane Kelder notes that it was Pissarro who introduced Gauguin, who was then a young stockbroker studying to become an artist, to Degas and Cézanne. Gauguin, near the end of his career, wrote a letter to a friend in 1902, shortly before Pissarro's death:
The American impressionist Mary Cassatt, who at one point lived in Paris to study art, and joined his Impressionist group, noted that he was "such a teacher that he could have taught the stones to draw correctly."
Lost and found paintings.
During the early 1930s throughout Europe, Jewish owners of numerous fine art masterpieces found themselves forced to give up or sell off their collections for minimal prices due to anti-Jewish laws created by the new Nazi regime. Many Jews were forced to flee Germany. When those forced into exile owned valuables, including artwork, they were often seized by officials for personal gain. In the decades after World War II, many art masterpieces were found on display in various galleries and museums in Europe and the United States. Some, as a result of legal action, were later returned to the families of the original owners. Many of the recovered paintings were then donated to the same or other museums as a gift.
One such lost piece, Pissarro's 1897 oil painting, "Rue St. Honoré, Apres Midi, Effet de Pluie", was discovered hanging at Madrid's government-owned museum, the Museo Thyssen-Bornemisza. In January 2011 the Spanish government denied a request by the US ambassador to return the painting. At the subsequent trial in Los Angeles, the court ruled that the Thyssen-Bornemisza Collection Foundation was the rightful owner. Pissarro's "Le Quai Malaquais, Printemps" is said to have been similarly stolen, while in 1999, Pissarro's 1897 "Le Boulevard de Montmartre, Matinée de Printemps" appeared in the Israel Museum in Jerusalem, its donor having been unaware of its pre-war provenance. In January 2012, "Le Marché aux Poissons" (The Fish Market), a color monotype, was returned after 30 years.
During his lifetime, Camille Pissarro sold few of his paintings. By the 21st century, however, his paintings were selling for millions. An auction record for the artist was set on 6 November 2007 at Christie's in New York, where a group of four paintings, "Les Quatre Saisons" (the Four Seasons), sold for $14,601,000 (estimate $12,000,000 – $18,000,000). In November 2009 "Le Pont Boieldieu et la Gare d'Orléans, Rouen, Soleil" sold for $7,026,500 at Sotheby's in New York. In February 2014 the 1897 "Le Boulevard de Montmartre, Matinée de Printemps", originally owned by the German industrialist and Holocaust victim Max Silberberg (), sold at Sotheby's in London for £19.9M, nearly five times the previous record.
A family of painters.
Camille's son Lucien was an Impressionist and Neo-impressionist painter. Lucien's daughter Orovida Pissarro was also a painter. Camille's great-grandson, Joachim Pissarro, became Head Curator of Drawing and Painting at the Museum of Modern Art in New York City and a professor in Hunter College's Art Department. From the only daughter of Camille, Jeanne Pissarro, other painters include Henri Bonin-Pissarro (1918–2003) and Claude Bonin-Pissarro (born 1921), who is the father of the Abstract artist Frédéric Bonin-Pissarro (born 1964).
References.
"Critical Catalogue of Paintings".
In June 2006 publishers Skira/Wildenstein released "Pissarro: Critical Catalogue of Paintings", compiled by Joachim Pissarro (descendant of the painter) and Claire Durand-Ruel Snollaerts (descendant of the French art dealer Paul Durand-Ruel). The 1,500-page, three-volume work is the most comprehensive collection of Pissarro paintings to date, and contains accompanying images of drawings and studies, as well as photographs of Pissarro and his family that had not previously been published. ISBN 88-7624-525-1

</doc>
<doc id="7435" url="http://en.wikipedia.org/wiki?curid=7435" title="Cardiology diagnostic tests and procedures">
Cardiology diagnostic tests and procedures

The diagnostic tests in cardiology are methods of identifying heart conditions associated with healthy vs. unhealthy, pathologic, heart function.
Bedside.
History.
Obtaining a medical history is always the first "test", part of understanding the likelihood of significant disease, as detectable within the current limitations of clinical medicine. Yet heart problems often produce no symptoms until very advanced, and many symptoms, such as palpitations and sensations of extra or missing heart beats correlate poorly with relative heart health "vs" disease. Hence, a history alone is rarely sufficient to diagnose a heart condition.
Auscultation.
"Auscultation" employs a stethoscope to more easily hear various normal and abnormal sounds, such as normal heart beat sounds and the usual heart beat sound changes associated with breathing versus heart murmurs.
Laboratory.
Blood tests.
A variety of "blood tests" are available for analyzing cholesterol transport behavior, HDL, LDL, triglycerides, lipoprotein little a, homocysteine, C-reactive protein, blood sugar control: fasting, after eating or averages using glycosylated albumen or hemoglobin, myoglobin, creatine kinase, troponin, brain-type natriuretic peptide, etc. to assess the evolution of coronary artery disease and evidence of existing damage. A great many more physiologic markers related to atherosclerosis and heart function are used and being developed and evaluated in research.
(*) due to the high cost, LDL is usually calculated instead of being measured directly<br>
source: Beyond Cholesterol, Julius Torelli MD, 2005 ISBN 0-312-34863-0
Electrophysiology.
Electrocardiogram.
"Electrocardiography" (ECG/EKG in German vernacular. Elektrokardiogram) monitors electrical activity of the heart, primarily as recorded from the skin surface. A 12 lead recording, recording the electrical activity in three planes, anterior, posterior, and lateral is the most commonly used form. The ECG allows observation of the heart electrical activity by visualizing waveform beat origin (typically from the sinoatrial or SA node) following down the bundle of HIS and ultimately stimulating the ventricles to contract forcing blood through the body. Much can be learned by observing the QRS morphology (named for the respective portions of the polarization/repolarization waveform of the wave, P,Q,R,S,T wave). Rhythm abnormalities can also be visualized as in slow heart rate bradycardia, or fast heart rate tachycardia.
Holter monitor.
A "Holter monitor" records a continuous EKG rhythm pattern (rarely a full EKG) for 24 hours or more. These monitors are used for suspected frequent rhythm abnormalities, especially ones the wearer may not recognize by symptoms. They are more expensive than event monitors.
Event monitor.
An "Event monitor" records short term EKG rhythm patterns, generally storing the last 2 to 5 minutes, adding in new and discarding old data, for 1 to 2 weeks or more. There are several different types with different capabilities. When the wearer presses a button on the monitor, it quits discarding old and continues recording for a short additional period. The wearer then plays the recording, via a standard phone connection, to a center with compatible receiving and rhythm printing equipment, after which the monitor is ready to record again. These monitors are used for suspected infrequent rhythm abnormalities, especially ones the wearer does recognize by symptoms. They are less expensive than Holter monitors.
Cardiac stress testing.
"Cardiac stress testing" is used to determine to assess cardiac function and to disclose evidence of exertion-related cardiac hypoxia. Radionuclide testing using thallium or technetium can be used tos of perfusion abnormalities. With a maximal stress test the level of exercise is increased until the patient heart rate will not increase any higher, despite increased exercise. A fairly accurate estimate of the target heart rate, based on extensive clinical research, can be estimated by the formula 220 beats per minute minus patient's age. This linear relation is accurate up to about age 30, after which it mildly underestimates typical maximum attainable heart rates achievable by healthy individuals. Other formulas exist, such as that by Miller (217 - (0.85 × Age)) and others . Achieving a high enough heart rate at the end of exercise is critical to improving the sensitivity of the test to detect high grade heart artery stenosis.
Electrophysiology study.
The electrophysiology study or EP study is the end all of electrophysiological tests of the heart. It involves a catheter with electrodes probing the endocardium, the inside of the heart, and testing the conduction pathways and electrical activity of individual areas of the heart.
Medical imaging.
Cardiac imaging techniques include Coronary catheterization, echocardiogram, and Intravascular ultrasound.

</doc>
<doc id="7437" url="http://en.wikipedia.org/wiki?curid=7437" title="Carlo Collodi">
Carlo Collodi

Carlo Lorenzini, better known by the pen name Carlo Collodi (; November 24, 1826 – October 26, 1890), was an Italian children's writer known for the world-renowned fairy tale novel, "The Adventures of Pinocchio".
Early life.
Collodi was born in Florence.
Career.
During the Italian wars of Independence in 1848 and 1860 Collodi served as a volunteer with the Tuscan army. His active interest in political matters may be seen in his earliest literary works as well as in the founding of the satirical newspaper "Il Lampione". This newspaper was censored by order of the Grand Duke of Tuscany in 1849 but re-emerged in May 1860.
Lorenzini had won fame as early as 1856 with his novel "In vapore" and had also begun intense activity on other political newspapers such as "Il Fanfulla"; at the same time he was employed by the Censorship Commission for the Theatre. During this period he composed various satirical sketches and stories (sometimes simply by collating earlier articles), including "Macchiette" (1880), "Occhi e nasi" (1881), "Storie allegre" (1887).
In 1875, he entered the domain of children's literature with "Racconti delle fate", a translation of French fairy tales by Perrault. In 1876 Lorenzini wrote "Giannettino" (inspired by Alessandro Luigi Parravicini's "Giannetto"), the "Minuzzolo", and "Il viaggio per l'Italia di Giannettino", a series which explored the re-unification of Italy through the ironic thoughts and actions of the character Giannettino.
Lorenzini became fascinated by the idea of using an amiable, rascally character as a means of expressing his own convictions through allegory. In 1880 he began writing "Storia di un burattino" ("The story of a marionette"), also called "Le avventure di Pinocchio", which was published weekly in "Il Giornale per i Bambini" (the first Italian newspaper for children).
Death and legacy.
Lorenzini died in Florence in 1890, unaware of the fame and popularity that awaited his work: as in the allegory of the story, Pinocchio eventually went on to lead his own independent life, distinct from that of the author.
Lorenzini is buried at San Miniato al Monte Basilica.

</doc>
<doc id="7439" url="http://en.wikipedia.org/wiki?curid=7439" title="Constructible number">
Constructible number

A point in the Euclidean plane is a constructible point if, given a fixed coordinate system (or a fixed line segment of unit length), the point can be constructed with unruled straightedge and compass. A complex number is a constructible number if its corresponding point in the Euclidean plane is constructible from the usual "x"- and "y"-coordinate axes.
It can then be shown that a real number "r" is constructible if and only if, given a line segment of unit length, a line segment of length |"r" | can be constructed with compass and straightedge. It can also be shown that a complex number is constructible if and only if its real and imaginary parts are constructible.
The set of constructible numbers can be completely characterized in the language of field theory: the constructible numbers form the quadratic closure of the rational numbers: the smallest field extension that is closed under square root and complex conjugation. This has the effect of transforming geometric questions about compass and straightedge constructions into algebra. This transformation leads to the solutions of many famous mathematical problems, which defied centuries of attack.
Geometric definitions.
The geometric definition of a constructible point is as follows. First, for any two distinct points "P" and "Q" in the plane, let "L"("P", "Q" ) denote the unique line through "P" and "Q", and let "C" ("P", "Q" ) denote the unique circle with center "P", passing through "Q". (Note that the order of "P" and "Q" matters for the circle.) By convention, "L"("P", "P" ) = "C" ("P", "P" ) = {"P" }. Then a point "Z" is "constructible from E, F, G and H" if either
Since the order of "E", "F", "G", and "H" in the above definition is irrelevant, the four letters may be permuted in any way. Put simply, "Z" is constructible from "E", "F", "G" and "H" if it lies in the intersection of any two distinct lines, or of any two distinct circles, or of a line and a circle, where these lines and/or circles can be determined by "E", "F", "G", and "H", in the above sense.
Now, let "A" and "A"′ be any two distinct fixed points in the plane. A point "Z" is "constructible" if either
Put simply, "Z" is constructible if it is either "A" or "A"′, or if it is obtainable from a finite sequence of points starting with "A" and "A"′, where each new point is constructible from previous points in the sequence.
For example, the center point of "A" and "A"′ is defined as follows. The circles "C" ("A", "A"′) and "C" ("A"′, "A") intersect in two distinct points; these points determine a unique line, and the center is defined to be the intersection of this line with "L"("A", "A"′).
Transformation into algebra.
All rational numbers are constructible, and all constructible numbers are algebraic numbers. Also, if "a" and "b" are constructible numbers with "b" ≠ 0, then and "a"/"b" are constructible. Thus, the set "K" of all constructible complex numbers forms a field, a subfield of the field of algebraic numbers.
Furthermore, "K" is closed under square roots and complex conjugation. These facts can be used to characterize the field of constructible numbers, because, in essence, the equations defining lines and circles are no worse than quadratic. The characterization is the following: a complex number is constructible if and only if it lies in a field at the top of a finite tower of quadratic extensions, starting with the rational field Q. More precisely, "z" is constructible if and only if there exists a tower of fields
formula_1
where "z" is in "K""n" and for all 0 ≤ "j" < "n", the dimension ["K""j" + 1 : "K""j" ] = 2.
Impossible constructions.
The algebraic characterization of constructible numbers provides an important "necessary" condition for constructibility: if "z" is constructible, then it is algebraic, and its minimal irreducible polynomial has degree a power of 2, or equivalently, the field extension Q("z")/Q has dimension a power of 2. One should note that it is true, (but not obvious to show) that the converse is false — this is not a "sufficient" condition for constructibility. However, this defect can be remedied by considering the normal closure of Q("z")/Q.
The non-constructibility of certain numbers proves the impossibility of certain problems attempted by the philosophers of ancient Greece. In the following chart, each row represents a specific ancient construction problem. The left column gives the name of the problem. The second column gives an equivalent algebraic formulation of the problem. In other words, the solution to the problem is affirmative if and only if each number in the given set of numbers is constructible. Finally, the last column provides the simplest known counterexample. In other words, the number in the last column is an element of the set in the same row, but is not constructible.

</doc>
<doc id="7441" url="http://en.wikipedia.org/wiki?curid=7441" title="Carson City, Nevada">
Carson City, Nevada

Carson City, officially the Consolidated Municipality of Carson City, is an independent city and the capital of the state of Nevada, named for the mountain man Kit Carson. As of the 2010 census, the population was 55,274. The majority of the population of the town lives in Eagle Valley, on the eastern edge of the Carson Range, a branch of the Sierra Nevada. Carson City is about south of Reno and originated as a stopover for California bound emigrants, but developed into a city with the Comstock Lode, a silver strike in the mountains to the northeast. The city has served as the capital of Nevada since statehood in 1864 and for much of its history was a hub for the Virginia and Truckee Railroad, although the tracks were removed in the 1950s. Prior to 1969, Carson City was also the county seat of Ormsby County. In 1969, the county was abolished, and its territory was merged with Carson City to form the Consolidated Municipality of Carson City. With the consolidation, the city limits today extend west across the Sierra Nevada to the California state line in the middle of Lake Tahoe. Like other independent cities in the United States, it is treated as a county-equivalent for census purposes.
History.
The first European Americans to arrive in what is known as Eagle Valley were John C. Fremont and his exploration party in January 1843. Fremont named the river flowing through the valley Carson River in honor of Christopher "Kit" Carson, the mountain man and scout he had hired for his expedition. Prior to the Fremont expedition, the Washoe people inhabited the valley and surrounding areas. Settlers named the area Washoe in reference to the tribe.
By 1851 the Eagle Station ranch located along the Carson River served as a trading post and stopover for travelers on the California Trail's Carson Branch which ran through Eagle Valley. The valley and trading post received their name from a bald eagle that was hunted and killed by one of the early settlers and was featured on a wall inside the post.
As the area was part of the Utah Territory, it was governed from Salt Lake City, where the territorial government was headquartered. Early settlers bristled at the control exerted by Mormon-influenced officials and desired the creation of the Nevada territory. A vigilante group of influential settlers, headed by Abraham Curry, sought a site for a capital city for the envisioned territory. In 1858, Abraham Curry bought Eagle Station and thereafter renamed the settlement Carson City. As Curry and several other partners had Eagle Valley surveyed for development. Curry had decided for himself that Carson City would someday serve as the capital city and left a plot open in the center of town for a future capitol building.
Following the discovery of gold and silver in 1859 on the nearby Comstock Lode, Carson City's population began to rise. Curry built the Warm Springs Hotel a mile to the east of the center of town. When the territorial governor James W. Nye traveled to Nevada, he chose Carson City as the territorial capital, influenced by Carson City lawyer William Stewart, who escorted him from San Francisco to Nevada. As such, Carson City bested Virginia City and American Flat. Curry loaned the Warm Springs Hotel to the territorial Legislature as a meeting hall. The Legislature named Carson City to be the seat of Ormsby County and selected the hotel as the territorial prison with Curry serving as its first warden. Today the property still serves as part of the state prison.
When Nevada became a state in 1864 during the Civil War, Carson City was confirmed as Nevada's permanent capital. Carson City's development was no longer dependent on the mining industry and instead became a thriving commercial center. The Virginia & Truckee Railroad was built between Virginia City and Carson City. A wooden flume was also built from the Sierra Nevadas into Carson City. The current capitol building was constructed from 1870 to 1871. The United States Mint operated a branch mint in Carson City between the years 1870 and 1893, which struck gold and silver coins. People came from China during that time, many of them to work on the railroad. Some of them owned businesses and taught school. By 1880, almost a thousand Chinese people, "one for every five Caucasians," lived in Carson City.
Carson City's population and transportation traffic decreased when the Central Pacific Railroad built a line through Donner Pass, too far to the north to benefit Carson City. The city was slightly revitalized with the mining booms in Tonopah and Goldfield. The US federal building (now renamed the Paul Laxalt Building) was completed in 1890 as was the Stewart Indian School. Carson City resigned itself to small city status, advertising itself as "America's smallest capital." The city slowly grew; by 1960 it had reached its 1880, boom-time population.
20th-century revitalization and growth.
As early as the late 1940s, discussions began about merging Ormsby County and Carson City. By this time, the county was little more than Carson City and a few hamlets to the west. However, the effort never got beyond the planning stages until 1966, when a statewide referendum formally approved the merger. The required constitutional amendment was passed in 1968. On April 1, 1969; Ormsby County and Carson City officially merged, and Carson City took over all municipal services under an independent city status. With this consolidation, Carson City absorbed former town sites such as Empire City, which had grown up in the 1860s as a milling center along the Carson River and current US 50. Carson City could now advertise itself as one of America's largest state capitals with its of city limits.
In 1991, the city adopted a downtown master plan, specifying that no building within of the capitol would surpass it in height. This plan prohibited future high-rise development in the center of downtown. The Ormsby House is currently the tallest building in downtown Carson City, at a height of 117 feet. The structure was completed in 1972.
Demographics.
Carson City is the smallest of the United States' 366 Metropolitan Statistical Areas.
As of the 2010 census there are 55,274 people, 20,171 households, and 13,252 families residing in the city. The population density is 366 people per square mile (141/km2). There are 21,283 housing units at an average density of 148/sq mi (57/km2). The racial makeup of the city is 81.1% White, 1.9% Black or African American, 2.4% Native American, 2.1% Asian, 0.2% Pacific Islander, 9.4% from other races, and 2.9% from two or more races. 21% of the population are Hispanic or Latino of any race.
As of the 2000 census, there are 20,171 households, out of which 29.8% have children under the age of 18 living with them, 50.0% are married couples living together, 11.0% have a female householder with no husband present, and 34.3% are non-families. 27.8% of all households are made up of individuals and 11.00% have someone living alone who is 65 years of age or older. The average household size is 2.44 and the average family size is 2.97. The city's age distribution is: 23.4% under the age of 18, 7.9% from 18 to 24, 28.9% from 25 to 44, 24.9% from 45 to 64, and 14.9% who are 65 years of age or older. The median age is 39 years. For every 100 females there are 106.90 males. For every 100 females age 18 and over, there are 108.20 males.
Data from the 2000 census indicates that the median income for a household in the city is $41,809, and the median income for a family is $49,570. Males have a median income of $35,296 versus $27,418 for females. The per capita income for the city is $20,943. 10.0% of the population and 6.9% of families are below the poverty line. Out of the total population, 13.7% of those under the age of 18 and 5.8% of those 65 and older are living below the poverty line.
Languages.
As of 2010, 82.31% (42,697) of Carson City residents age 5 and older spoke English at home as a primary language, while 14.12% (7,325) spoke Spanish, 0.61% (318) French, and numerous Indic languages was spoken as a main language by 0.50% (261) of the population over the age of five. In total, 17.69% (9,174) of Carson City's population age 5 and older spoke a mother language other than English.
Economy.
The following is a list of the top employers in Carson City from the fourth quarter of 2012:
1,500 - 1,999 Employees
1,000 - 1,499 Employees
500 - 999 Employees
200 - 499 Employees
100-199 Employees
Government and politics.
Carson City is governed via the mayor-council system. The mayor is elected in a citywide vote to a four-year term. The city council is called the Board of Supervisors and has four members. Members are elected from single member wards. Nevada's capital is generally considered a Republican stronghold, often voting for Republicans by wide margins. In 2004, George Bush defeated John Kerry 57-40%. In 2008 however Barack Obama became the first Democrat since 1964 to win Ormsby County/Carson City, defeating John McCain 49% to 48%, by 204 votes, a margin of under 1%.
Carson City, being the state capital, is home to many political protests and demonstrations at any given time. Weekly protests have included Save the Virginia Range Horses, End the Wars in Iraq and Afghanistan, Occupy Carson City and Nevada ANTI-Corruption Movement that features a 150-foot-long by 4-foot-tall Crime Scene banner in front of the Nevada Attorney General Catherine Cortez Masto and Governor Brian Sandoval offices, the State Legislative buildings and the Carson City courthouse. Other large protests include the Tea Party protest in 2009 and AFSCME local 4041 protests related to State employee collective bargaining efforts and other issues. Perhaps the biggest protest second to the 2009 Tea Party protest was the 2009 demonstration against education budget cuts by Governor Brian Sandoval.
In an attempt to either make proposed spent nuclear fuel storage facility at Yucca Mountain too expensive (by raising property tax rates to the maximum allowed) or to allow the state to collect the potential federal payments of property taxes on the facility, the State of Nevada in 1987 created a new county with no residents called Bullfrog County, carved Yucca Mountain out of Nye County, and made Carson City the county seat of Bullfrog County, even though it is not located in Bullfrog County and is more than 100 miles from Yucca Mountain. A state judge found the process unconstitutional, Yucca Mountain was absorbed back into Nye County, and Bullfrog County was dissolved in 1989.
Climate.
Carson City features a semi-arid climate (Köppen "BSk") with cool but not inordinately cold winters and hot summers. The city is situated in a high desert river valley approximately above sea level. There are four fairly distinct seasons, all of which are relatively mild compared to many parts of the country and to what one may expect given its elevation. Winters see typically light to moderate snowfall, with a median of . Most precipitation occurs in winter and spring, with summer and fall being fairly dry, drier than neighboring California. There are 37 days of + highs annually, with + temperatures occurring in some years.
The Carson River flows from Douglas County through the southwestern edge of Carson City.
Also notably, Carson City has warmed the most than any other city in the nation in the last 30 years. 
Education.
The Carson City School District operates ten schools in Carson City. The six elementary schools are Bordewich-Bray Elementary School, Empire Elementary School, Fremont Elementary School, Fritsch Elementary School, Mark Twain Elementary School, and Al Seeliger Elementary School. The two middle schools are Carson Middle School and Eagle Valley Middle School. Carson High School and the alternative Pioneer High School serve high school students. Carson High is on Saliman Road.
Western Nevada College (WNC) is a regionally accredited, two-year and four-year institution which is part of the Nevada System of Higher Education. It has an education program. The school also offers associate of arts, associate of science.
Sports and recreation.
Carson City has never hosted any professional team sports. However, a variety of sports are offered a parks and recreation. Many neighborhood parks offers a wide variety of features, including picnic tables, beaches, restrooms, fishing, softball, basketball, pond, tennis, and volleyball. The largest park is Mills Park, which has a total land area of and includes the narrow gauge Carson & Mills Park Railroad.
While there are no ski slopes within Carson City, the city is located close to Heavenly Mountain Resort, Diamond Peak and Mount Rose skiing areas.
Popular culture.
Films.
The following is a list of movies with scenes filmed in Carson City
The following is a list of films with scenes set in Carson City but filmed elsewhere
Notable people.
Carson City has served as one of the state’s centers for politics and business. Every state governor since Denver S. Dickerson has resided in the Governor's Mansion located in Carson City. "See also: List of Governors of Nevada." The following personalities took up a residence in Carson City at some point in their lives.
Transportation.
There are two highways in the city US Route 395 and US Route 50. Carson City is home to one under-construction freeway Interstate 580. Phase 1 of the Carson City Freeway Project from US 395, just north of the city, to US 50 was completed in February 2006 and Phase 2A, extending from Rt. 50 to Fairview Drive, was officially opened on September 24, 2009. Phase 2B, Fairview Drive to Rt. 50, awaits funding and, according to Director Martinovich at NDOT, completion is anticipated for the fall of 2017 Prior to 2012, Carson City was one of only five state capitals not directly served by an Interstate highway; the city lost this distinction when I-580 was extended into the city limits.
Carson City's first modern bus system, Jump Around Carson, or JAC, opened to the public. JAC uses a smaller urban bus that is ideal for Carson City. However, there is virtually no ground public transportation to other destinations. Passenger trains haven't served Carson City since 1950, when the Virginia and Truckee Railroad was shut down. Greyhound Lines stopped their bus services to the town in 2006 and Amtrak discontinued their connecting thruway bus to Sacramento in 2008. There is now only a limited Monday – Friday RTC bus service to Reno which is still served by both Greyhound and Amtrak.
Carson City is also served by the Carson Airport, which is a regional airport in the northern part of the city. Reno-Tahoe International Airport, which is away, handles domestic commercial flights.

</doc>
<doc id="7442" url="http://en.wikipedia.org/wiki?curid=7442" title="Clark Kent">
Clark Kent

Clark Kent is an American fictional character, a superhero created by Jerry Siegel and Joe Shuster. Appearing regularly in stories published by DC Comics, he debuted in "Action Comics" #1 (June 1938) and serves as the civilian and secret identity of the superhero Superman.
Over the decades there has been considerable debate as to which personality the character identifies with most. From his first introduction in 1938 to the mid-1980s, "Clark Kent" was seen mostly as a disguise for Superman, enabling him to mix with ordinary people. This was the view in most comics and other media such as movie serials and TV (e.g., in "Atom Man vs. Superman" starring Kirk Alyn and "The Adventures of Superman" starring George Reeves) and radio. In 1986, during John Byrne's revamping of the character, Clark Kent became more emphasized. Different takes persist in the present.
Overview.
As Superman's alter ego, the personality, concept, and name of Clark Kent have become ingrained in popular culture as well, becoming synonymous with secret identities and innocuous fronts for ulterior motives and activities. In 1992, Superman co-creator Joe Shuster told the "Toronto Star" that the name derived from 1930s cinematic leading men Clark Gable and Kent Taylor, but the persona from bespectacled silent film comic Harold Lloyd and himself. Another, perhaps more likely possibility, is that Jerry Siegel pulled from his own love of pulp heroes Doc Clark Savage and The Shadow alias Kent Allard. This idea was notably stated in the book "Men of Tomorrow: Geeks, Gangsters, and the Rise of the American Comic Book".
Clark's middle name is given variously as either Joseph, Jerome or Jonathan, all being allusions to creators Jerry Siegel and Joe Shuster.
Beginnings.
In the earliest "Superman" comics, Clark Kent's primary purpose was to fulfill the perceived dramatic requirement that a costumed superhero cannot remain on full duty all the time. Clark thus acted as little more than a front for Superman's activities. Although his name and history were taken from his early life with his adoptive Earth parents, everything about Clark was staged for the benefit of his alternate identity: as a reporter for the "Daily Planet", he receives late-breaking news before the general public, has a plausible reason to be present at crime scenes, and need not strictly account for his whereabouts as long as he makes his story deadlines. He sees his job as a journalist as an extension of his Superman responsibilities—bringing truth to the forefront and fighting for the little guy. He believes that everybody has the right to know what is going on in the world, regardless of who is involved.
To deflect suspicion that he is Superman, Clark Kent adopted a largely passive and introverted personality with conservative mannerisms, a higher-pitched voice, and a slight slouch. This personality is typically described as "mild-mannered," perhaps most famously by the opening narration of Max Fleischer's "Superman" animated theatrical shorts. These traits extended into Clark's wardrobe, which typically consists of a bland-colored business suit, a red necktie, black-rimmed glasses (which in Pre-Crisis stories had lenses of Kryptonian material that would not be damaged when he fired his heat vision through them), combed-back hair, and occasionally a fedora.
Fellow reporter Lois Lane became the object of Clark's/Superman's romantic affection. Lois's affection for Superman and her rejection of Clark's clumsy advances have been a recurring theme in Superman comics and movies and on television.
Transitions.
Clark wears his Superman costume underneath his street clothes, allowing easy changes between the two personae and the dramatic gesture of ripping open his shirt to reveal the familiar "S" emblem when called into action. Superman usually stores his Clark Kent clothing compressed in a secret pouch within his cape, though some stories have shown him leaving his clothes in some covert location (such as the "Daily Planet" storeroom) for later retrieval.
In the Pre-Crisis comic book title "Superman Family", Clark is featured in a series of stories called "The Private Life of Clark Kent" wherein he solves problems subtly without changing into Superman.
Adoption.
Adopted by Jonathan and Martha Kent from the Kansas town of Smallville, Clark (and thus Superman) was raised with the values of a typical rural American town, including attending the local Methodist Church (though it is debated by comic fans if Superman is a Methodist).
Most continuities state that the Kents had been unable to have biological children. In the Golden and Silver Age versions of his origin, after the Kents retrieved Clark from his rocket, they brought him to the Smallville Orphanage and returned a few days later to formally adopt the orphan, giving him as a first name Martha's maiden name, "Clark." In John Byrne's 1986 origin version "The Man of Steel," instead of adopting him through an orphanage, the Kents passed Clark off as their own child after their farm was isolated for months by a series of snowstorms that took place shortly after they found his rocket, using their past medical history of various miscarriages to account for their reasons for keeping Martha's pregnancy secret.
Silver Age.
In the Silver Age comics continuity, Clark's superpowers manifested upon his landing on Earth and he gradually learned to master them, adopting the superhero identity of Superboy at the age of eight. He subsequently developed Clark's timid demeanor as a means of ensuring that no one would suspect any connection between the two alter-egos.
Modern Age retroactive continuity.
In the wake of John Byrne's reboot of Superman continuity in "The Man of Steel", many traditional aspects of Clark Kent were dropped in favor of giving him a more aggressive and extroverted personality (although not as strong as Lois's), including such aspects as making Clark a top football player in high school along with being a successful author and Pulitzer Prize-winning writer, which includes at least two original novels, "The Janus Contract", and "Under a Yellow Sun". Furthermore, Clark's motivations for his professional writing were deepened as both a love for the art that "contributes at least as much social good as his Superman activities" and as a matter of personal fulfillment in an intellectual field in which his abilities give no unfair competition to his colleagues beyond typing extraordinarily fast. Following "One Year Later", Clark adopts some tricks to account for his absences, such as feigning illness or offering to call the police. These, as well as his slouching posture, are references to his earlier mild-mannered Pre-Crisis versions, but he still maintains a sense of authority and his assertive self. Feeling that Clark is the real person and that Clark is not afraid to be himself in his civilian identity, John Byrne has stated in interviews that he took inspiration for this portrayal from the George Reeves version of Superman.
Clark's favorite movie is "To Kill a Mockingbird" (in which Gregory Peck wears glasses not unlike Kent's). According to the "DC Comics Official Guide to Superman," Clark enjoys peanut butter and jelly sandwiches, football games, and the smell of Kansas in the springtime. His favorite baseball team is the Metropolis Monarchs and his favorite football team is the Metropolis Sharks. As of "One Year Later", Clark is in his mid-thirties, stands at , and weighs about . Unlike in the Silver Age, his powers developed over several years, only coming to their peak when he was an adult.
Secret identity.
Superman's secret identity as Clark Kent is one of the DC Universe's greatest secrets. Only a few trusted people are aware of it, such as Batman and other members of the Justice League, Superman's cousin Supergirl, and Clark's childhood friend Lana Lang (In pre-Crisis stories, Lana did not know, but their friend Pete Ross did, unbeknownst to anyone, including Clark). Lex Luthor, other supervillains, and various civilians have learned the secret identity several times, though their knowledge is usually removed through various means (the boxer Muhammad Ali is one of the very few to deduce the identity and retain the knowledge).
Traditionally, Lois Lane (and sometimes others) would often suspect Superman of truly being Clark Kent; this was particularly prominent in Silver Age stories, including those in the series "Superman's Girl Friend Lois Lane". More recent stories (post-Crisis) often feature the general public assuming that Superman has no secret identity owing to the fact that he, unlike most heroes, doesn't wear a mask. In "The Secret Revealed," a supercomputer constructed by Lex Luthor calculated Superman's true identity from information that had been assembled by his staff, but Lex dismissed the idea because he could not believe that someone so powerful would want another, weaker identity. In post-"Crisis" continuity, Lois Lane, feeling that someone like Clark could not be Superman, never suspected the dual identity beyond one isolated incident before Clark finally revealed it to her. In "Visitor," Lois finds Superman at the Kent farm with Lana Lang and asks him point-blank if he is Clark Kent. Before he can answer, the Kents tell her that they raised Superman alongside Clark like a brother. In the 2009 retcon of the mythos, Lois Lane is fully aware from the beginning, along with Perry White, that the meek, pudgy, and bumbling Clark Kent deliberately holds himself back: however, still far from associating him with Superman, they simply believe he's hiding his qualities as a good reporter. In the current continuity established by DC's New 52 relaunch in 2011, Lois Lane remains unaware that Clark is Superman.
In the future of the Legion of Super-Heroes, his secret identity is historical fact, with exhibits at a Superman Museum depicting the hero and his friends' and family's adventures.
Security of identity.
Various explanations over the decades have been offered for why people have never suspected Superman and Clark Kent of being one and the same:
Identity change.
When crises arise, Clark quickly changes into Superman. Originally during his appearances in "Action Comics" and later in his own magazine, the Man of Steel would strip to his costume and stand revealed as Superman, often with the transformation having already been completed. But within a short time, Joe Shuster and his ghost artists began depicting Clark Kent ripping open his shirt to reveal the "S" insignia on his chest—an image that became so iconic that other superheroes, during the Golden Age and later periods, would copy the same type of change during transformations.
In the Fleischer theatrical cartoons released by Paramount, the mild-mannered reporter often ducked into a telephone booth or stockroom to make the transformation. Since the shorts were produced during the rise of film noir in cinema, the change was usually represented as a stylized sequence: Clark Kent's silhouette is clearly seen behind a closed door's pebble glass window (or a shadow thrown across a wall) as he strips to his Superman costume. Then, the superhero emerges having transformed from his meek disguise to his true self. In the comic books and in the George Reeves television series, he favors the "Daily Planet"s storeroom for his changes of identities (the heroic change between identities within the storeroom is almost always seen in the comics, but never viewed in the Reeves series).
The CBS Saturday morning series "The New Adventures of Superman" produced by Filmation Studios—as well as "The Adventures of Superboy" from the same animation house—featured the iconic "shirt rip" to reveal the "S" or Clark Kent removing his unbuttoned white dress shirt in a secluded spot, usually thanks to stock animation which was reused over dozens of episodes, to reveal his costume underneath while uttering his famed line "This is a job for Superman!"
As a dramatic plot device, Clark often has to quickly improvise in order to find a way to change unnoticed. For example, in "Superman" (1978), Clark, unable to use a newer, open-kiosk pay phone (and getting a nice laugh from the theater audience), runs down the street and rips open his shirt to reveal his costume underneath. He quickly enters a revolving door, spinning through it at incredible speed while changing clothes. Thus made invisible, he appears to have entered the building as Clark Kent and exited seconds later as Superman. Later in the film, when the need to change is more urgent (as he believes the city is about to be poisoned by Lex Luthor), he simply jumps out a window of the Daily Planet offices, changing at super-speed as he falls (the film merely shows the falling Kent blurring into a falling Superman) and flies off. Further films in the series continued this tradition, with Clark blurring into Superman, changing at super-speed while he runs.
In "Lois & Clark," Clark's usual method of changing was to either "suddenly" remember something urgent that required his immediate attention or leave the room/area under the pretense of contacting a source, summoning the police, heading to a breaking story's location, etc. The change would then frequently occur off-screen, although the shirt-rip reveal was a prominently used move well-associated with the show. Clark also developed a method of rapidly spinning into his costume at super speed which became a trademark change, especially during the third and fourth seasons of the series, and extremely popular with the show's fans.
In one scene of "", Clark becomes aware of an emergency while talking with Bruce Wayne and, in the next panel, he has flown out of his Kent clothing and glasses so quickly that they have had no time to fall.
In Season 8 of "Smallville," Clark begins to show a bit more of his double identity. He starts slowing down his superspeed enough for surveillance cameras to see his iconic red and blue streak. This reveals to the citizens of Metropolis that a superhero is among them and the name "The Red-Blue Blur" is coined. When Jimmy Olsen becomes suspicious, Clark decides to reserve his usual red-and-blue for saving people. He carries a backpack with him to work every day, containing his change of clothes. He begins to practice his speed change at home and at the "Daily Planet." He changes in a superspeed spin in the "Daily Planet"s phone booth and once even in his office chair. The last minute of the last episode of "Smallville" had Clark responding to an emergency, rushing to the top of the Daily Planet, and then using the familiar shirt-rip while the camera zoomed in on the familiar S-logo to the original John Williams fanfare.
Debate over true identity.
A relatively recent debate is which of the two identities (Superman or Clark Kent) is the real person and which is the façade. Fans and Superman scholars follow one of three interpretations:
Clark Kent has also been depicted without the Superman alter ego. In the Elseworlds stories starting with "", he is the son of Jonathan Kent, who saves his son from the destruction of the Earth. Clark ends up on Krypton, where he is adopted by Jor-El and becomes the planet's Green Lantern.
In other media.
"The Adventures of Superman" radio series (1940-1951).
In the early "Adventures of Superman" radio episodes, Kal-El landed on Earth as an adult. He saved a man and his son and they gave him the idea of living as a normal person. They gave him the name of Clark Kent, and he later got a job as a newspaper reporter under that name. In that role he adopted a higher voice and a more introverted personality – clearly establishing that Kent is the secret identity and Superman is the true person.
Later episodes shifted to the usual origin story, in which Kal-El landed on Earth as a baby and was raised by the Kent family.
Clayton "Bud" Collyer voiced both Clark Kent and Superman, until Michael Fitzmaurice replaced him in the final episodes.
Kirk Alyn film serials (1948-1950).
In the film serials "Superman" (1948) and "Atom Man vs. Superman" (1950), Kirk Alyn portrays Clark as a mild-mannered reporter who comes to Metropolis and secures a job at the "Daily Planet", following the death of his foster parents. While he quickly gains the respect of "Planet" editor Perry White, he is forced to contend with rival reporter Lois Lane, who often uses trickery to prevent Clark from pursuing a lead (giving her the chance to scoop him). Nevertheless, his journalistic skills are useful as he pursues stories on the crime boss known as the Spider Lady, and the criminal scientist Luthor (who had yet to receive his first name, Lex).
"Adventures of Superman" TV series (1952-1958).
In the 1950s George Reeves series, Clark Kent is portrayed as a cerebral character who is the crime reporter for the "Daily Planet" and who as Kent uses his intelligence and powers of deduction to solve crimes (often before Inspector Henderson does) before catching the villain as Superman. Examples include the episodes "Mystery of the Broken Statues", "A Ghost for Scotland Yard", "The Man in the Lead Mask", and "The Golden Vulture". George Reeves' Kent/Superman is also established as a champion of justice for the oppressed in episodes like "The Unknown People" and "The Birthday Letter". Although Kent is described in the show introduction as "mild-mannered", he can be very assertive, often giving orders to people and taking authoritative command of situations, though, as in the Pre-Crisis Superman stories at that time, Clark is still considered the secret identity. He gets people to trust his judgment very easily and has a good, often wisecracking, sense of humor. Reeves, who first appeared as the character in the 1951 film "Superman and the Mole Men", was older than subsequent Superman actors.
Christopher Reeve films (1978-1987) and "Superman Returns" (2006).
In 1978, the first of four Superman films was made in which Clark Kent and Superman were portrayed by Christopher Reeve (with teenage Kent played by Jeff East in the first film). This was followed nearly two decades later by a fifth film called "Superman Returns" with Brandon Routh giving a performance very similar to Reeve's. In contrast to George Reeves' intellectual Clark Kent, Reeve's version is much more of an awkward fumbler and bungler, although Reeve is also an especially athletic, dashing and debonair Superman. Clark Kent's hair is always absolutely flat, while Superman's hair has a slight wave and is parted on the opposite side as Kent's. These films leave the impression that Clark Kent is really a "secret" identity that is used to enable Superman to serve humanity better, rather than just a role to help him assimilate into the human community.
A great deal of emphasis is placed on his origins on the planet Krypton with exotic crystalline sets designed by John Barry, effectively giving Superman a third persona as Kal-El. The first film is in three sections: Kal-El's infancy on Krypton (shot in London on the 007 stage), Clark Kent's teen years in Smallville, and Kent/Superman's adult life in Metropolis (shot in New York City). In earlier sections of the film, Reeve's Kent interacts with both his earthly parents and the spirit of his Kryptonian father through a special crystal, in a way George Reeves never did. The film has a fair amount of quasi-Biblical imagery suggestive of Superman as a sort of Christ-figure sent by Jor-El "to show humans the way." (See also Superman (1978 film)#Themes). In "Superman II" Reeve's Superman has to sacrifice his powers (effectively becoming just Clark Kent) in order to have a love relationship with Lois Lane, a choice he eventually abrogates to protect the world.
The relationship between Superman and Kent came to actual physical blows in "Superman III". Superman is given a piece of manufactured Kryptonite, but instead of weakening or killing him it drives him crazy, depressed, angry, and casually destructive, committing crimes which range from petty acts of vandalism to environmental disasters, like causing an oil spillage in order to bed a lusty woman by the name of Lorelei in league with the villains. Driven alcoholic, Superman, his outfit dirty and neglected, eventually goes to a car wrecking yard where Kent, in a proper business suit and glasses, suddenly emerges from within him. A fight ensues in which the "evil" Superman tries to dispose of the "good" Kent, but the latter fights back, "kills" the evil side to his nature and, reclaiming the Superman mantle, sets off to repair the damage and capture the villains. Les Daniels comments in his book, "DC Comics: A Celebration of the World's Favourite Comic Book Heroes", "The 'good' Superman, ultimately triumphant, (is) dressed as Clark, thus implying that he is the more valid personality (as well as the one Lana loves)" and expresses annoyance that "Something could have been made of this, but sadly nothing was".
The indirect "Christianization" of Superman in the Reeve films (admitted by film producer Pierre Spengler on the DVD commentaries) has provoked comment on the Jewish origins of Superman. Rabbi Simcha Weinstein's book "Up, Up and Oy Vey: How Jewish History, Culture and Values Shaped the Comic Book Superhero" says that Superman is both a pillar of society and one whose cape conceals a "nebbish," saying, "He's a bumbling, nebbish Jewish stereotype. He's Woody Allen."
 Ironically, it is also in the Reeve films that Clark Kent's persona has the greatest resemblance to Woody Allen, though his conscious model was Cary Grant's character in "Bringing up Baby." This same theme is pursued about '40s superheroes generally in "Disguised as Clark Kent: Jews, Comics, and the Creation of the Superhero" by Danny Fingeroth.
"Lois & Clark: The New Adventures of Superman" (1993-1997).
Clark Kent's character is given one of its heaviest emphases in the 1990s series "". It is made very clear during the series, even discussed directly by the characters, that Clark Kent is who he really is, rather than his superheroic alter-ego.
In "Lois and Clark", Kent (Dean Cain) is a stereotypical wide-eyed farm kid from Kansas with the charm, grace and humor of George Reeves, but without the awkward geekiness of Christopher Reeve. Emphasis is laid on the comic elements of his dual relationship with Lois Lane (Teri Hatcher). The ban on Christopher Reeve's Superman having a relationship with a human while retaining his superpowers is entirely absent in the world of "Lois and Clark." In the final season, Clark Kent marries Lois Lane (a few years after her almost-marriage to his arch-enemy Lex Luthor, whom she refused at the altar), finding love, happiness, and completeness in this relationship which does not jeopardize his Superman persona.
Superman's secret identity was discovered by a number of villains during the series. In some cases, like that of Lex Luthor, the villain died before he could share the discovery. In two cases, the claim was discredited by having Superman and Clark appear together in public, using a hologram in the first case and a Clark Kent from a parallel universe in the second (in the first case, there was also footage filmed of Superman uniforms in Kent's closet, but that was explained by stating Superman simply needs a place to store them). In one case, Superman destroyed the evidence (a time traveler's journal), and stated that the villain's unsupported words will be ignored.
"Smallville" TV series (2001-2011).
"Smallville" was adapted to television in 2001, by Alfred Gough and Miles Millar. Clark Kent is played by Tom Welling, with others portraying Clark as an infant. Throughout the series, Clark never officially adopted a costume until around the eighth season, but prior to this was seen wearing Superman's traditional colors of red and blue, more often as the series progresses (more commonly a blue shirt underneath a red jacket, reflecting Superman's uniform and cape colors). He is going through a process of character formation, making many mistakes in his youth, over time forming better and better judgment, while always self-consciously aware of his status as an alien from another planet who is different from other people. In season eight, he begins a fight against evil, hoping to be a source of inspiration and hope to others. A modest amount of religious imagery is seen occasionally in the series, but to a lesser degree than in the Christopher Reeve series.
"Smallville"'s Kent is particularly inwardly conflicted as he attempts to live the life of a normal human being, while keeping the secret of his alien heritage from his friends. Throughout the first seven seasons of the series he has a complicated relationship with Lana Lang, as well as his self-perceived guilt over the fact that the meteor shower that killed Lana's parents and created most of the superhumans he fought in the show's first few years was caused by his rocket coming to Earth and dragging pieces of Krypton with it. Clark's powers appear over time. He is not aware of all of his powers at the start of the show; for instance, his heat vision and super breath do not develop until seasons two and six, respectively, and his power of flight did not emerge until the series finale, up until that point the power appeared only in a few rare cases, such as when he was temporarily 're-programmed' to assume a Kryptonian persona or when he was trapped in a virtual reality.
Clark Kent starts out best friends with Lex Luthor, whom he meets after saving the latter's life. (Boyhood friendship with Lex Luthor had been the basis of a "Superboy" adventure published in 1960).
Clark and Lex remain entangled for most of the series. Lex Luthor's father, Lionel Luthor, is an unscrupulous industrialist with whom Lex has a troubled relationship. Lex would like to transcend his family background and be a better person than his father, but after multiple setbacks he slowly slips into evil, becoming convinced that only he can 'protect' the world from the perceived alien threats by taking control of it, regardless of the cost to others. In turn, Clark Kent has a slightly dark side with which he comes to grips over time, made even worse by his experiences with Red Kryptonite, which causes him to lose his morals and act solely on impulse while under its influence. In different ways to Luthor, Clark also does not have fully ideal relationship either with his adoptive father, Jonathan, nor with an A.I. based on Jor-El that was sent by the original to guide him, Jonathan occasionally having trouble relating to Clark while Jor-El's lack of his template's emotions causes him to treat Clark too harshly at times. The younger Luthor slightly envies Clark's 'clean-cut' and wholesome parents (who disapprove of Clark's friendship with Luthor), while Clark is impressed with Luthor's wealth while failing to understand some of the manipulations he carries out in his interactions with others. Even in his better days, Luthor is highly ambitious for power and wealth, at one time noting that he shares his name with Alexander the Great. Clark Kent, on the other hand, has no idea what he is going to do with his life while bewildered by his powers, and his uncertainty as to why he was sent to Earth.
In season eight of "Smallville", Clark Kent begins to work as a reporter at the "Daily Planet". Shortly after he begins to save lives as an anonymous superhero crimefighter, which becomes known as the "Red-Blue Blur" after a photograph is taken of one of his rescues.
In season nine, Clark unintentionally begins to formalize his dual identity to protect his secret and also privately introduces the well-known glasses to Lois Lane. Additionally, during the opening scene of the season nine finale, Clark finds a gift from his mother containing his Superman suit (Although the suit is subsequently taken by Jor-El until Clark is ready for it).
In season ten, for the first time in public Clark begins to formulate a bumbling/stuttering Kent with glasses akin to the Christoper Reeve/Brandon Routh portrayal of the character. In the season ten finale of the series he fully adopts the Superman identity, when he takes action to save Earth from Darkseid, who was drawn to Earth by Clark's actions and sought to take the hero as a host.
"Smallville"s Kent has also appeared in various literature (including comics and over a dozen young adult novels) based on the television series.
Animated series.
In the 1940s Superman shorts, Clark is shown to have a wisecracking sense of humor and he and Lois are good friends. At the near end of each short, Clark gives out a smile and a wink to the audience (that was carried over to the 1966 Superman animated series).
In the "" of the mid to late 1990s, Clark Kent is shown as a mild-mannered but competent reporter and is shown exposing various criminals through his reporter identity. In this identity, Clark and Lois are good friends (with Lois frequently calling him "Smallville" in a teasing but good-natured way) but do not share romantic feelings; instead, it is Superman and Lois who have a romantic relationship. Lang Lang on the other hand knows of Clark's identity as Superman but seems more interested in Clark because she knew him as that first.
"Man of Steel "(2013).
In the Superman reboot film "Man of Steel", Clark Kent is portrayed by Henry Cavill, with Dylan Sprayberry and Cooper Timberline portraying younger versions of the character. 
In this film, Kal-El is Krypton's first natural birth in centuries, a birth without using Krypton's genesis chamber. In order to save Krypton's future and stop Zod's coup, his biological father Jor-El steals Krypton's DNA template (Codex), bonds it to Kal-El's cells, and sends him to Earth before Krypton explodes. Kal-El's ship lands in a small Kansas town. He is raised as the adoptive son of Jonathan and Martha Kent, who name him Clark Kent. As a boy, Clark is a conflicted and lonely person who questions his place and purpose in the world. At a very young age, he learns of his superhuman abilities including superhearing, heat vision, X-ray vision, superhuman strength, and invulnerability. Despite being ridiculed throughout his childhood and adolescence, he uses his abilities to help others. However, he was depicted as being an angry individual, who is forced to show restraint on his temptations to bring harm to those who attempt to do so to him; a trait that follows him into adulthood. When he learns about his alien background as a boy, he is frightened and confused. 
After Jonathan's death, an adult Clark spends several years living a nomadic lifestyle, working different jobs under false identities while saving people in secret, as well as struggling to cope with the loss of his adoptive father. He eventually infiltrates a U.S. military investigation of a Kryptonian scout spaceship in the Arctic. Clark enters the alien ship and communicates with the preserved consciousness of Jor-El in the form of a hologram. Jor-El reveals Clark's origins and the extinction of his race, and tells Clark that he was sent to Earth to bring hope to mankind. Lois Lane, a journalist from the Daily Planet sent to write a story on the discovery, sneaks inside the ship while following Clark and is rescued by him when she is injured. Lois's editor, Perry White, rejects her story of a "superhuman" rescuer, so she traces Clark back to Kansas with the intention of writing an exposé. After hearing his story, she decides not to reveal his secret. After the discovery of his background and purpose, he is shown to be less confused and a little more joyful, as evidenced by his discussion with his adoptive mother Martha. 
When Zod arrives to transform Earth into a new Krypton, Lois helps Clark/Superman stop Zod. By film's end, to create an alias that gives him access to dangerous situations without arousing suspicion, Clark takes a job as a reporter at the Daily Planet and adopts a modernized version of his "mild-mannered" look from the comics.
It is worth noting that, as a nod to many comics, Clark is implied to have an interest in football, as evidenced when he is seen watching a game while drinking beer just before Zod's arrival and ultimatum.

</doc>
<doc id="7445" url="http://en.wikipedia.org/wiki?curid=7445" title="Classification of finite simple groups">
Classification of finite simple groups

In mathematics, the classification of the finite simple groups is a theorem stating that every finite simple group belongs to one of four categories described below. These groups can be seen as the basic building blocks of all finite groups, in a way reminiscent of the way the prime numbers are the basic building blocks of the natural numbers. The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference with respect to the case of integer factorization is that such "building blocks" do not necessarily determine uniquely a group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.
The proof of the classification theorem consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.
Statement of the classification theorem.
Theorem. Every finite simple group is isomorphic to one of the following groups:
The classification theorem has applications in many branches of mathematics, as questions about the structure of finite groups (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.
Daniel Gorenstein announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of quasithin groups. The completed proof of the classification was announced by after Aschbacher and Smith published a 1221 page proof for the missing quasithin case.
Overview of the proof of the classification theorem.
 wrote two volumes outlining the low rank and odd characteristic part of the proof, and 
wrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:
Groups of small 2-rank.
The simple groups of low 2-rank are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.
The simple groups of small 2-rank include:
The classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.
All groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the balance theorem implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the signalizer functor theorem only work for groups with elementary abelian subgroups of rank at least 3.)
Groups of component type.
A group is said to be of component type if for some centralizer "C" of an involution, "C"/"O"("C") has a component (where "O"("C") is the core of "C", the maximal normal subgroup of odd order).
These are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.
A major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the B-theorem, which states that every component of "C"/"O"("C") is the image of a component of "C".
The idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.
Groups of characteristic 2 type.
A group is of characteristic 2 type if the generalized Fitting subgroup "F"*("Y") of every 2-local subgroup "Y" is a 2-group.
As the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.
The rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious quasithin groups, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.
Groups of rank at least 3 are further subdivided into 3 classes by the trichotomy theorem, proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.
The three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of "standard type" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.
The general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.
Existence and uniqueness of the simple groups.
The main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the monster totaled about 200 pages, and the identification of the Ree groups by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.
History of the proof.
Gorenstein's program.
In 1972 announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:
Timeline of the proof.
Many of the items in the list below are taken from . The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the "wrong" order.
Second-generation classification.
The proof of the theorem, as it stood around 1985 or so, can be called "first generation". Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a second-generation classification proof. This effort, called "revisionism", was originally led by Daniel Gorenstein.
As of 2005, six volumes of the second generation proof have been published , with most of the balance of the proof in manuscript. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from second generation proof being written in a more relaxed style.) Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.
Gorenstein and his collaborators have given several reasons why a simpler proof is possible.
 has called the work on the classification problem by Ulrich Meierfrankenfeld, Bernd Stellmacher, Gernot Stroth, and a few others, a third generation program. One goal of this is to treat all groups in characteristic 2 uniformly using the amalgam method.
Why is the proof so long?
Gorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of compact Lie groups. 

</doc>
<doc id="7446" url="http://en.wikipedia.org/wiki?curid=7446" title="Chalcolithic">
Chalcolithic

The Chalcolithic (; "khalkós", "copper" and "líthos", "stone") period or Copper Age, also known as the Eneolithic or Æneolithic (from Latin "aeneus" "of bronze"), is a phase of the Bronze Age before it was discovered that adding tin to copper formed the harder bronze. The Copper Age was originally defined as a transition between the Neolithic and the Bronze Age. However, because it is characterized by the use of metals, the Copper Age is considered a part of the Bronze Age rather than the Stone Age.
The archaeological site of Belovode on the Rudnik mountain in Serbia contains the world's oldest securely dated evidence of copper making at high temperature, from 5,000 BCE.
Origin of name.
The multiple names result from multiple recognitions of the period. Originally the term "Bronze Age" meant that either copper or bronze was being used as the chief hard substance for the manufacture of tools and weapons. In 1881, John Evans, recognizing that the use of copper often preceded the use of bronze, distinguished between a transitional Copper Age and the Bronze Age proper. He did not include this transitional period in the tripartite system of Early, Middle and Late Bronze Age but placed it at the beginning outside of it. He did not, however, present it as a fourth age, but chose to retain the traditional three-age system.
In 1884, Gaetano Chierici, perhaps following the lead of Evans, renamed it in Italian as the "Eneo-litica", or "Bronze-stone" transition. This phrase was never intended to mean that the period was one in which both bronze and stone were used. The Copper Age features the use of copper, excluding bronze; moreover, stone continued to be used throughout both the Bronze Age and the Iron Age. "Litica" simply names the Stone Age as the point from which the transition began and is not another -lithic age. The Eneolithic was never part of the Stone Age, which ended conclusively the moment the first smelter succeeded in obtaining copper from copper ore for the first time.
Subsequently British scholars used either Evans's "Copper Age" or the term "Eneolithic" (or Aeneolithic), a translation of Chierici's "eneo-litica". After several years, a number of complaints appeared in the literature that "Eneolithic" seemed to the untrained eye to be produced from e-neolithic, "outside the Neolithic," clearly not a definitive characterization of the Copper Age. About the year 1900, many writers began to substitute "Chalcolithic" for Eneolithic, to avoid the false segmentation. It was at this time that the misunderstanding began among those who had not understood the Italian. The -lithic was seen as a new -lithic age, a part of the Stone Age in which copper was used, which may appear paradoxical. Today Copper Age, Eneolithic and Chalcolithic are used synonymously to mean Evans's original definition of Copper Age.
The period is a transitional one, but does not stand outside the traditional three-age system. It appears that copper was not widely exploited at first, and that efforts in alloying it with tin and other metals began quite soon, making it difficult to distinguish the distinct Chalcolithic cultures from later periods. The boundary between the Copper and Bronze Ages is indistinct, since alloys sputtered in and out of use due to the erratic supply of tin.
The emergence of metallurgy may have occurred first in the Fertile Crescent, where it gave rise to the Bronze Age in the 4th millennium BCE (the traditional view), though finds from the Vinča culture in Europe have now been securely dated to slightly earlier than those of the Fertile Crescent. There was an independent invention of copper and bronze smelting first by Andean civilizations in South America extended later by sea commerce to the Mesoamerican civilization in West Mexico (see Metallurgy in pre-Columbian America and Metallurgy in pre-Columbian Mesoamerica).
The literature of European archaeology, in general, avoids the use of 'chalcolithic' (the term 'Copper Age' is preferred), whereas Middle Eastern archaeologists regularly use it. The Copper Age in the Middle East and the Caucasus began in the late 5th millennium BCE and lasted for about a millennium before it gave rise to the Early Bronze Age. The transition from the European Copper Age to Bronze Age Europe occurs about the same time, between the late 5th and the late 3rd millennia BCE.
According to Parpola, ceramic similarities between the Indus Civilization, southern Turkmenistan, and northern Iran during 4300–3300 BCE of the Chalcolithic period (Copper Age) suggest considerable mobility and trade.
Europe.
An archaeological site in southeastern Europe (Serbia) contains the oldest securely dated evidence of copper making at high temperature, from 7,500 years ago. The find in June 2010 extends the known record of copper smelting by about 800 years, and suggests that copper smelting may have been invented in separate parts of Asia and Europe at that time rather than spreading from a single source.
In Serbia, a copper axe was found at Prokuplje, which indicates that humans were using metals in Europe by 7,500 years ago (~5,500 BCE), many years earlier than previously believed. Knowledge of the use of copper was far more widespread than the metal itself. The European Battle Axe culture used stone axes modeled on copper axes, even with imitation "mold marks" carved in the stone. Ötzi the Iceman, who was found in the Ötztal Alps in 1991 and whose remains were dated to about 3300 BCE, was found with a Mondsee copper axe.
Examples of Chalcolithic cultures in Europe include Vila Nova de São Pedro and Los Millares on the Iberian Peninsula. Pottery of the Beaker people has been found at both sites, dating to several centuries after copper-working began there. The Beaker culture appears to have spread copper and bronze technologies in Europe, along with Indo-European languages.
South Asia.
The South Asian inhabitants of Mehrgarh fashioned tools with local copper ore (ore used as pigment) between 7700–3300 BCE. 
East Asia.
5th millennia BCE copper artifacts start to appear in East Asia, such as Jiangzhai and Hongshan culture, but those metal artifacts were not widely used.
Africa.
North Africa and the Nile Valley imported its iron technology from the Near East and followed the Near Eastern course of Bronze Age and Iron Age development. However the Iron Age and Bronze Age occurred simultaneously in much of Africa. The earliest dating of iron in Sub-Saharan Africa is 2500 BCE at Egaro, west of Termit, making it contemporary to the Middle East. The Egaro date is debatable with archaeologists, due to the method used to attain it. The Termit date of 1500 BCE is widely accepted.
In the region of the Aïr Mountains in Niger, we have the development of independent copper smelting between 3000–2500 BCE. The process was not in a developed state, indication smelting was not foreign. It became mature about 1500 BCE.
America.
The term is also applied to American civilizations that already used copper and copper alloys thousands of years before the European migration. Besides the cultures from the Andes and Mesoamerica, the Old Copper Complex, located in present-day Michigan and Wisconsin in the United States, used copper for tools, weapons, and other implements. However, no evidence for copper smelting or alloying has been found here and objects were hammered into shape. Artifacts from these sites have been dated from 4000 to 1000 BCE, making them some of the oldest Chalcolithic sites in the entire world.

</doc>
<doc id="7447" url="http://en.wikipedia.org/wiki?curid=7447" title="Circumcision and law">
Circumcision and law

There exist laws restricting or regulating circumcision, some dating back to ancient times. In a number of modern states, circumcision is presumed to be legal, but under certain circumstances, more general laws, such as laws about assault or child custody, may sometimes be interpreted as applying to situations involving circumcision. Some countries have placed restrictions on circumcision.
History.
There are ancient religious requirements for circumcision. The Hebrew Bible commands Jews to circumcise their male children on the eighth day of life, and to circumcise their male slaves ().
Laws banning circumcision are also ancient. The ancient Greeks prized the foreskin and disapproved of the Jewish custom of circumcision. 1 Maccabees, 1:60–61 states that King Antiochus IV of Syria, the occupying power of Judea in 170 BCE, outlawed circumcision on penalty of death. one of the grievances leading to the Maccabean Revolt.
According to the "Historia Augusta", the Roman emperor Hadrian issued a decree banning circumcision in the empire, and some modern scholars argue that this was a main cause of the Jewish Bar Kokhba revolt of 132 CE. The Roman historian Cassius Dio, however, made no mention of such a law, and blamed the Jewish uprising instead on Hadrian's decision to rebuild Jerusalem as Aelia Capitolina, a city dedicated to Jupiter.
Antoninus Pius permitted Jews to circumcise their own sons. However, he forbade the circumcision of non-Jews that were either foreign-slaves or non-Jewish members of the household, contrary to He also made it illegal for a man to convert to Judaism. Antoninus Pius exempted the Egyptian priesthood from the otherwise universal ban on circumcision.
Modern Law.
Australia.
In 1993, a non-binding research paper of the Queensland Law Reform Commission ("Circumcision of Male Infants") concluded that "On a strict interpretation of the assault provisions of the Queensland Criminal Code, routine circumcision of a male infant could be regarded as a criminal act", and that doctors who perform circumcision on male infants may be liable to civil claims by that child at a later date. No prosecutions have occurred in Queensland, and circumcisions continue to be performed.
In a case of sexual assault in Queensland, Australia (1997), a district court awarded a man damages for nervous shock after a botched attempt to circumcise him with a broken beer bottle in a drunken attack. Making Australian legal history, the award was made against the assailant for unlawful wounding.
In 1999, a Perth man won A$360,000 in damages after a doctor admitted he botched a circumcision operation at birth which left the man with a badly deformed penis.
In 2002, Queensland police charged a father with grievous bodily harm for having his two sons, then aged nine and five, circumcised without the knowledge and against the wishes of the mother. The mother and father were in a family court dispute. The charges were dropped when the police prosecutor revealed that he did not have all family court paperwork in court and the magistrate refused to grant an adjournment.
Cosmetic circumcision for newborn males is currently banned in all Australian public hospitals, South Australia being the last state to adopt the ban in 2007; the procedure was not forbidden from being performed in private hospitals. In the same year, the Tasmanian President of the Australian Medical Association, Haydn Walters, stated that they would support a call to ban circumcision for non-medical, non-religious reasons. In 2009, the Tasmanian Law Reform Institute released its Issues Paper investigating the law relating to male circumcision in Tasmania, it "highlights the uncertainty in relation to whether doctors can legally perform circumcision on infant males".
The Tasmania Law Reform Institute released its recommendations for reform of Tasmanian law relative to male circumcision on 21 August 2012. The report makes fourteen recommendations for reform of Tasmanian law relative to male circumcision.
Bulgaria.
Male circumcision was very strongly discouraged in Bulgaria in the 1980s as part of attempts to pressure the country's Muslim minority but there was no actual legislation against the practice.
Canada.
According to the College of Physicians and Surgeons of British Columbia:
England and Wales.
Male circumcision has traditionally been presumed to be legal under British law, however some authors have argued that there is no solid foundation for this view in English law.
The passage of the Human Rights Act 1998 has led to some speculation that the lawfulness of the circumcision of male children is unclear.
One 1999 case, "Re "J" (child's religious upbringing and circumcision)" said that circumcision in Britain required the consent of all those with parental responsibility, or the permission of the court, acting for the best interests of the child, and issued an order prohibiting the circumcision of a male child of a non-practicing Muslim father and non-practicing Christian mother with custody. The reasoning included evidence that circumcision carried some medical risk; that the operation would be likely to weaken the relationship of the child with his mother, who strongly objected to circumcision without medical necessity; that the child may be subject to ridicule by his peers as the odd one out and that the operation might irreversibly reduce sexual pleasure, by permanently removing some sensory nerves, even though cosmetic foreskin restoration might be possible. The court did not rule out circumcision against the consent of one parent. It cited a hypothetical case of a Jewish mother and an agnostic father with a number of sons, all of whom, by agreement, had been circumcised as infants in accordance with Jewish laws; the parents then have another son who is born after they have separated; the mother wishes him to be circumcised like his brothers; the father refuses his agreement. In such a case, a decision in favor of circumcision was said to be likely.
In 2001 the General Medical Council had found a doctor who had botched circumcision operations guilty of abusing his professional position and that he had acted "inappropriately and irresponsibly", and struck him off the register. A doctor who had referred patients to him, and who had pressured a mother into agreeing to the surgery, was also condemned. He was put on an 18-month period of review and retraining, and was allowed to resume unrestricted practice as a doctor in March 2003, after a committee found that he had complied with conditions it placed on him. According to the "Northern Echo", he "told the committee he has now changed his approach to circumcision referrals, accepting that most cases can be treated without the need for surgery.".
Fox and Thomson (2005) argue that consent cannot be given for non-therapeutic circumcision. They say there is "no compelling legal authority for the common view that circumcision is lawful."
In 2005 a Muslim man had his son circumcised against the wishes of the child's mother who was the custodial parent. He was found not guilty of assault occasioning actual bodily harm by a majority verdict of the jury.
In 2009 it was reported that a 20-year-old man whose father had him ritually circumcised as a baby is preparing to sue the doctor who circumcised him. This is believed to be the first time a person who was circumcised as an infant has made a claim in the UK. The case is expected to be heard in 2010.
Europe.
On 1 October 2013, the Parliamentary Assembly of the Council of Europe adopted a resolution in which they state they are "particularly worried about a category of violation of the physical integrity of children," and include in this category "circumcision of young boys for religious reasons."
Finland.
In August 2006, a Finnish court ruled that the circumcision of a four-year-old boy arranged by his mother, who is Muslim, to be an illegal assault. The boy's father, who had not been consulted, reported the incident to the police. A local prosecutor stated that the prohibition of circumcision is not gender-specific in Finnish law. A lawyer for the Ministry of Social Affairs and Health stated that there is neither legislation nor prohibition on male circumcision, and that "the operations have been performed on the basis of common law." The case was appealed and in October 2008 the Finnish Supreme Court ruled that the circumcision, " carried out for religious and social reasons and in a medical manner, did not have the earmarks of a criminal offence. It pointed out in its ruling that the circumcision of Muslim boys is an established tradition and an integral part of the identity of Muslim men". In 2008, the Finnish government was reported to be considering a new law to legalise circumcision if the practitioner is a doctor and if the child consents. In December 2011, Helsinki District Court said that the Supreme Court's decision does not mean that circumcision is legal for any non-medical reasons. The court referred to the Convention on Human rights and Biomedicine of the Council of Europe, which was ratified in Finland in 2010.
In February 2010, a Jewish couple were fined for causing bodily harm to their then infant son who was circumcised in 2008 by a mohel brought in from the UK. Normal procedure for persons of Jewish faith in Finland is to have a locally certified mohel who works in Finnish healthcare perform the operation. In the 2008 case, the infant was not anesthetized and developed complications that required immediate hospital care. The parents were ordered to pay 1500 euros in damages to their child.
Germany.
In October 2006, a Turkish national who performed ritual circumcisions on seven boys was convicted of causing dangerous bodily harm by the state court in Düsseldorf.
In September 2007, a Frankfurt am Main appeals court found that the circumcision of an 11yearold boy without his approval was an unlawful personal injury. The boy, whose parents were divorced, was visiting his Muslim father during a vacation when his father forced him to be ritually circumcised. The boy had planned to sue his father for .
In May 2012, the Cologne regional appellate court ruled that religious circumcision of male children amounts to bodily injury, and is a criminal offense in the area under its jurisdiction. The decision based on the article "Criminal Relevance of Circumcising Boys. A Contribution to the Limitation of Consent in Cases of Care for the Person of the Child" published by Holm Putzke, a German law professor at the University of Passau. The court arrived at its judgment by application of the human rights provisions of the Basic Law, a section of the Civil Code, and some sections of the Criminal Code to non-therapeutic circumcision of male children. Some observers said it could set a legal precedent that criminalizes the practice. Jewish and Muslim groups were outraged by the ruling, viewing it as trampling on freedom of religion.
The German ambassador to Israel, Andreas Michaelis, told Israeli lawmakers that Germany was working to resolve the issue and that it doesn't apply at a national level, but instead only to the local jurisdiction of the court in Cologne. The Council of the Coordination of Muslims in Germany condemned the ruling, stating that it is "a serious attack on religious freedom." Ali Kizilkaya, a spokesman of the council, stated that, "The ruling does not take everything into account, religious practice concerning circumcision of young Muslims and Jews has been carried out over the millenia on a global level." The Roman Catholic archbishop of Aachen, Heinrich Mussinghoff, said that the ruling was "very surprising", and the contradiction between "basic rights on freedom of religion and the well-being of the child brought up by the judges is not convincing in this very case." Hans Ulrich Anke, the head of the Protestant Church in Germany, said the ruling should be appealed since it didn't "sufficiently" consider the religious significance of the rite. A spokesman, Steffen Seibert, for German Chancellor Angela Merkel stated that Jewish and Muslim communities will be free to practice circumcision responsibly, and the government would find a way around the local ban in Cologne. The spokesman stated "For everyone in the government it is absolutely clear that we want to have Jewish and Muslim religious life in Germany. Circumcision carried out in a responsible manner must be possible in this country without punishment.".
In July, a group of rabbis, imams, and others said that they view the ruling against circumcision "an affront on our basic religious and human rights." The joint statement was signed by leaders of groups including Germany's Turkish-Islamic Union for Religious Affairs, the Islamic Center Brussels, the Rabbinical Centre of Europe, the European Jewish Parliament and the European Jewish Association, who met with members of European Parliament from Germany, Finland, Belgium, Italy, and Poland. European rabbis, who urged Jews to continue circumcision, planned further talks with Muslim and Christian leaders to determine how they can oppose the ban together. The Jewish Hospital of Berlin suspended the practice of male circumcision. On 19 July 2012, a joint resolution of the CDU/CSU, SPD and FDP factions in the Bundestag requesting the executive branch to draft a law permitting circumcision of boys to be performed without unnecessary pain in accordance with best medical practice carried with a broad majority.
The New York Times reported that the German Medical Association "condemned the ruling for potentially putting children at risk by taking the procedure out of the hands of doctors, but it also warned surgeons not to perform circumcisions for religious reasons until legal clarity was established." The ruling was supported by Deutsche Kinderhilfe, a German child rights organization, which asked for a two-year moratorium to discuss the issue and pointed out that religious circumcision may contravene the Convention on the Rights of the Child (Article 24.3: "States Parties shall take all effective and appropriate measures with a view to abolishing traditional practices prejudicial to the health of children.").
The German Academy for Pediatric and Adolescent Medicine (Deutsche Akademie für Kinder- und Jugendmedizin e.V., DAKJ), the German Association for Pediatric Surgery (Deutsche Gesellschaft für Kinderchirurgie, DGKCH) and the Professional Association of Pediatric and Adolescent Physicians (Berufsverband der Kinder- und Jugendärzte) took a firm stand against non-medical routine infant circumcision.
In July, in Berlin, a criminal complaint was lodged against Rabbi Yitshak Ehrenberg for "causing bodily harm" by performing religious circumcision, and for vocal support of the continuation of the practice. In September, the prosecutors dismissed the complaint, concluding that "there is no proof to establish that the rabbi's conduct met the 'condition of a criminal' violation."
In September, Reuters reported "Berlin's senate said doctors could legally circumcise infant boys for religious reasons in its region, given certain conditions."
On 12 December 2012, following a series of hearings and consultations, the Bundestag adopted the proposed law explicitly permitting non-therapeutic circumcision to be performed under certain conditions; it is now §1631(d) in the German Civil Code. The vote tally was 434 ayes, 100 noes, and 46 abstentions. Following approval by the Bundesrat and signing by the Bundespräsident, the new law became effective on 28 December 2012 a day after its publication in the Federal Gazette.
Ireland.
In October 2005 a Nigerian man was cleared of a charge of reckless endangerment over the death of a baby from haemorrhage and shock after he had circumcised the child. The judge directed the jury not to "bring what he called their white western values to bear when they were deciding this case" and after deliberating for an hour and a half they found the defendant not guilty.
Israel.
In Israel, Jewish circumcision is entirely legal, as is posthumous circumcision. In 1999, the Israeli Supreme Court overruled an attempt to have after-death circumcision outlawed. Though illegal, female circumcision is still practiced among the Negev Bedouin, and tribal secrecy among the Bedouin makes it difficult for authorities to enforce the ban. In 2013 Rabbinical court in Israel, ordered a mother, Elinor Daniel to circumcise her son or pay a fine of 500 Israeli Shekel for every day that the child is not circumcised. The mother appealed the Rabbinical court ruling, and her appeal is now pending the Supreme court decision.
Netherlands.
When Ayaan Hirsi Ali was a Member of the Netherlands Parliament she asked it to consider making the circumcision of male children unlawful. In May 2008 a father who had his two sons, aged 3 and 6 circumcised against the will of their mother was found not guilty of causing them serious physical harm but was given a 6-week suspended jail sentence for taking the boys away from their mother against her will.
Norway and region.
In 2012, the Senterpartiet proposed a ban on circumcision on males under eighteen.
In September 2013, the Children's ombudsmen in all the Nordic countries issued a common statement where they called for a ban on circumcision on minors, stating that such circumsions violate the right of children after the Convention on the Rights of the Child to co-determination and protection from harmful traditions.
South Africa.
The Children's Act 2005 makes the circumcision of male children under 16 unlawful except for religious or medical reasons. In the Eastern Cape province the Application of Health Standards in Traditional Circumcision Act, 2001, regulates traditional circumcision, which causes the death or mutilation of many youths by traditional surgeons each year. Among other provisions, the minimum age for circumcision is age 18.
In 2004, a 22-year-old Rastafarian convert was forcibly circumcised by a group of Xhosa tribal elders and relatives. When he first fled, two police returned him to those who had circumcised him. In another case, a medically circumcised Xhosa man was forcibly recircumcised by his father and community leaders. He laid a charge of unfair discrimination on the grounds of his religious beliefs, seeking an apology from his father and the Congress of Traditional Leaders of South Africa. According to South African newspapers, the subsequent trial became "a landmark case around forced circumcision." In October 2009, the Eastern Cape High Court at Bhisho (sitting as an Equality Court) clarified that circumcision is unlawful unless done with the full consent of the initiate.
Sweden.
In 2001, the Parliament of Sweden enacted a law allowing only persons certified by the National Board of Health to circumcise infants. It requires a medical doctor or an anesthesia nurse to accompany the circumciser and for anaesthetic to be applied beforehand. After the first two months of life circumcisions can only be performed by a physician. The stated purpose of the law was to increase the safety of the procedure.
Swedish Jews and Muslims objected to the law, and in 2001, the World Jewish Congress called it "the first legal restriction on Jewish religious practice in Europe since the Nazi era." The requirement for an anaesthetic to be administered by a medical professional is a major issue, and the low degree of availability of certified professionals willing to conduct circumcision has also been subject to criticism. According to a survey, two out of three paediatric surgeons said they refuse to perform non-therapeutic circumcision, and less than half of all county councils offer it in their hospitals. However, in 2006, the U.S. State Department stated, in a report on Sweden, that most Jewish mohels had been certified under the law and 3000 Muslim and 40–50 Jewish boys were circumcised each
year. An estimated 2000 of these are performed by persons who are neither physicians nor have officially recognised certification.
The Swedish National Board of Health and Welfare reviewed the law in 2005 and recommended that it be maintained, but found that the law had failed with regard to the intended consequence of increasing the safety of circumcisions. A later report by the Board criticised the low level of availability of legal circumcisions, partly due to reluctance among health professionals. To remedy this, the report suggested a new law obliging all county councils to offer non-therapeutic circumcision in their hospitals, but this was later abandoned in favour of a non-binding recommendation.
United States.
Circumcision of adults who grant personal informed consent for the surgical operation is not at issue and is unquestionably lawful.
In the United States, non-therapeutic circumcision of male children has long been assumed to be lawful in every jurisdiction provided that one parent grants surrogate informed consent, however Adler (2013) has recently challenged the validity of this assumption. As with every country, doctors who circumcise children must take care that all applicable rules regarding informed consent and safety are satisfied.
While anti-circumcision groups have occasionally proposed legislation banning non-therapeutic child circumcision, it has not been supported in any legislature. After a failed attempt to adopt a local ordinance banning circumcision on a San Francisco ballot, the state of California enacted in October 2011 a law protecting circumcision from local attempts to ban the practice.
In 2012, New York City required those performing "metzitzah b'peh", a part of circumcision required by some Hasidim, to obey stringent consent requirements, including documentation. Agudath Israel of America and other Jewish groups have planned to sue the city in response.
Disputes between parents
Occasionally the courts are asked to make a ruling when parents cannot agree on whether or not to circumcise a child.
In January 2001 a dispute between divorcing parents in New Jersey was resolved when the mother, who sought to have the boy circumcised withdrew her request. The boy had experienced two instances of foreskin inflammation and she wanted to have him circumcised. The father, who had experienced a traumatic circumcision as a child objected and they turned to the courts for a decision. The Medical Society of New Jersey and the Urological Society of New Jersey both opposed any court ordered medical treatment. As the parties came to an agreement, no precedent was set. In June 2001 a Nevada court settled a dispute over circumcision between two parents but put a strict gag order on the terms of the settlement. In July 2001 a dispute between parents in Kansas over circumcision was resolved when the mother's request to have the infant circumcised was withdrawn. In this case the father opposed circumcision while the mother asserted that not circumcising the child was against her religious beliefs. (The woman's pastor had stated that circumcision was "important" but was not necessary for salvation.) On 24 July 2001 the parents reached agreement that the infant would not be circumcised.
On 14 July 2004 a mother appealed to the Missouri Supreme Court to prevent the circumcision of her son after a county court and the Court of Appeals had denied her a writ of prohibition. However, in early August 2004, before the Supreme Court had given its ruling, the father, who had custody of the boy, had him circumcised.
In October 2006 a judge in Chicago granted an injunction blocking the circumcision of a 9-year-old boy. In granting the injunction the judge stated that "the boy could decide for himself whether to be circumcised when he turns 18."
In November 2007, the Oregon Supreme Court heard arguments from a divorced Oregon couple over the circumcision of their son. The father wanted his son, who turned 13 on 2 March 2008, to be circumcised in accordance with the father's religious views; the child's mother opposes the procedure. The parents dispute whether the boy is in favor of the procedure. A group opposed to circumcision filed briefs in support of the mother's position, while some Jewish groups filed a brief in support of the father. On 25 January 2008, the Court returned the case to the trial court with instructions to determine whether the child agrees or objects to the proposed circumcision. The father appealed to the US Supreme Court to allow him to have his son circumcised but his appeal was rejected. The case then returned to the trial court. When the trial court interviewed the couple's son, now 14-years-old, the boy stated that he did not want to be circumcised. This also provided the necessary circumstances to allow the boy to change residence to live with his mother. The boy was not circumcised.
Other disputes
In September 2004 the North Dakota Supreme Court rejected a mother's attempt to prosecute her doctor for circumcising her child without fully informing her of the consequences of the procedure. The judge and jury found that the defendants were adequately informed of possible complications, and the jury further found that it is not incumbent on the doctors to describe every "insignificant" risk.
In March 2009 a Fulton County, Ga., State Court jury awarded $2.3 million in damages to a 4-year-old boy and his mother for a botched circumcision in which too much tissue was removed causing permanent disfigurement.
In August 2010 an eight-day old boy was circumcised in a Florida hospital against the stated wishes of the parents. The hospital admitted that the boy was circumcised by mistake; the mother has sued the hospital and the doctor involved in the case.
USSR.
Before glasnost, according to an article in The Jewish Press, Jewish ritual circumcision was forbidden in the USSR. However, David E. Fishman, professor of Jewish History at the Jewish Theological Seminary of America, states that, whereas the "heder" and "yeshiva", the organs of Jewish education, "were banned by virtue of the law separating church and school, and subjected to tough police and administrative actions," circumcision was not proscribed by law or suppressed by executive measures.
Jehoshua A. Gilboa writes that while circumcision was not officially or explicitly banned, pressure was exerted to make it difficult. "Mohels" in particular were concerned that they could be punished for any health issue that might develop, even if it arose some time after the circumcision.

</doc>
<doc id="7449" url="http://en.wikipedia.org/wiki?curid=7449" title="Called to Common Mission">
Called to Common Mission

Called to Common Mission is an agreement between The Episcopal Church and the Evangelical Lutheran Church in America (ELCA), establishing full communion between them. It was ratified by the ELCA in 1999, the ECUSA in 2000, after the narrow failure of a previous agreement. Its principal author on the Episcopal side was the Rev. Canon J. Robert Wright. Under the agreement, they recognize the validity of each other's baptisms and ordinations. The agreement provided that the ELCA would accept the historical episcopate, something which became controversial in the ELCA. In response to concerns about the meaning of CCM, bishops in the ELCA drafted , which presented the official ELCA position.
Some within the ELCA argued that requiring the historic episcopate would contradict the traditional Lutheran doctrine that the church exists wherever the Word is preached and Sacraments are practiced. Others objected on the grounds that adopting the Episcopalian priesthood and hierarchical structure was contrary to the Lutheran concept of the priesthood of all believers, which holds that all Christians stand on equal footing before God. They argued that the Old Covenant required a priest to mediate between God and humanity, but that New Covenant explicitly abolishes the need for priestly role by making every Christian a priest with direct access to God's grace. Still others objected because of the implied directive that lay presidency would be abolished. This was a particularly issue for rural congregations that periodically "called" a congregation member to conduct communion services in the absence of ordained clergy.

</doc>
<doc id="7450" url="http://en.wikipedia.org/wiki?curid=7450" title="Context menu">
Context menu

A context menu (also called contextual, shortcut, and popup or pop-up menu) is a menu in a graphical user interface (GUI) that appears upon user interaction, such as a right-click mouse operation. A context menu offers a limited set of choices that are available in the current state, or context, of the operating system or application. Usually the available choices are actions related to the selected object. From a technical point of view, such a context menu is a graphical control element.
History.
Context menus first appeared in the Smalltalk environment on the Xerox Alto computer, where they were called "pop-up menus." The NEXTSTEP operating system further developed the idea, incorporating a feature whereby the right or middle mouse button brought the main menu (which was vertical and automatically changed depending on context) to the location of the mouse, thereby eliminating the need to move the mouse pointer all the way across the large (for the time) NextStep screen.
Implementation.
Context menus are opened via various forms of user interaction that target a region of the GUI that supports context menus. The specific form of user interaction and the means by which a region is targeted vary:
Windows 7 changed the mouse click behavior such that the context menu doesn't open while the mouse button is pressed, but only opens the menu when the button is released, so the user has to click again (this time with the first mouse button) to select a context menu item. This behavior differs from Windows XP, Mac OS X, and most Linux distributions.
Context menus are sometimes hierarchically organized, allowing navigation through different levels of the menu structure. The implementations differ: Microsoft Word was one of the first applications to only show sub-entries of some menu entries after clicking an arrow icon on the context menu, otherwise executing an action associated with the parent entry. This makes it possible to quickly repeat an action with the parameters of the previous execution, and to better separate options from actions.
X Window Managers.
The following window managers provide context menu functionality:
Usability.
Context menus have received some criticism from usability analysts when improperly used, as some applications make certain features "only" available in context menus, which may confuse even experienced users (especially when the context menus can only be activated in a limited area of the application's client window).
Context menus usually open in a fixed position under the pointer, but when the pointer is near a screen edge the menu will be displaced - thus reducing consistency and impeding use of muscle memory. If the context menu is being triggered by keyboard, such as by using Shift + F10, the context menu appears near the focused widget instead of the position of the pointer, to save recognition efforts.
In documentation.
Microsoft's guidelines call for always using the term "context menu", and explicitly deprecate "shortcut menu".

</doc>
<doc id="7451" url="http://en.wikipedia.org/wiki?curid=7451" title="Jews as the chosen people">
Jews as the chosen people

In Judaism, "chosenness" is the belief that the Jews are the chosen people, chosen to be in a covenant with God. This idea is first found in the Torah (the first five books of the Tanakh, which are also included in the Christian Bible) and is elaborated on in later books of the Hebrew Bible. Much is written about these topics in rabbinic literature. The three largest Jewish denominations—Orthodox Judaism, Conservative Judaism and Reform Judaism—maintain the belief that the Jews have been chosen by God for a purpose.
According to the Israel Democracy Institute, approximately two thirds of Israeli Jews believe that Jews are the "chosen people".
Chosenness in the Bible.
According to the traditional Jewish interpretation of the Bible, Israel's character as the chosen people is unconditional, as it says in , 
The Torah also says, 
God promises that he will never exchange his people with any other:
Other Torah verses about chosenness, 
The obligation imposed upon the Israelites was emphasized by the prophet Amos (): 
Rabbinic Jewish views of chosenness.
The idea of chosenness has traditionally been interpreted by Jews in two ways: one way is that God chose the Israelites, while the other is that the Israelites chose God. Although collectively this choice was made freely, religious Jews believe that it created individual obligation for the descendants of the Israelites. Another opinion is that the choice was free in a limited context, thus: although the Jews chose to follow precepts ordained by God, the Kabbalah and Tanya teach that even prior to creation, the "Jewish soul" was already chosen.
Crucial to the Jewish notion of chosenness is that it creates obligations exclusive to Jews, while non-Jews receive from God other covenants and other responsibilities. Generally, it does not entail exclusive rewards for Jews. Classical rabbinic literature in the Mishnah Avot 3:14 has this teaching:
Rabbi Akiva used to say, "Beloved is man, for he was created in God's image; and the fact that God made it known that man was created in His image is indicative of an even greater love. As the verse states [], 'In the image of God, man was created.'" The mishna goes on to say, "Beloved are the people Israel, for they are called children of God; it is even a greater love that it was made known to them that they are called children of God, as it said, 'You are the children of the Lord, your God. Beloved are the people Israel, for a precious article [the Torah] was given to them ...
Most Jewish texts do not state that "God chose the Jews" by itself. Rather, this is usually linked with a mission or purpose, such as proclaiming God's message among all the nations, even though Jews cannot become "unchosen" if they shirk their mission. This implies a special duty, which evolves from the belief that Jews have been pledged by the covenant which God concluded with the biblical patriarch Abraham, their ancestor, and again with the entire Jewish nation at Mount Sinai. In this view, Jews are charged with living a holy life as God's priest-people.
In the Jewish prayerbook (the Siddur), chosenness is referred to in a number of ways. The blessing for reading the Torah reads
In the "Kiddush", a prayer of sanctification, in which the Sabbath is inaugurated over a cup of wine, the text reads, 
In the "Kiddush" recited on festivals it says, 
The Aleinu prayer refers to the concept of Jews as a chosen people:
An earlier form of this prayer, in use during the medieval era, contained an extra sentence:
It is our duty to praise the Master of all, to exalt the Creator of the Universe, who has not made us like the nations of the world and has not placed us like the families of the earth; who has not designed our destiny to be like theirs, nor our lot like that of all their multitude, "who worship mist and emptiness and pray to a god who cannot save."
This sentence in italics is an allusion to the Bible, Isaiah (). 
In the medieval era some within the Christian community came to believe that this line referred to Christians worshipping Jesus; they demanded that it be excised. Ismar Elbogen, a historian of the Jewish liturgy, held that the early form of the prayer pre-dated Christianity, and could not possibly have referred to it.
Further interpretations.
According to the Rabbis, "Israel is of all nations the most willful or headstrong one, and the Torah was to give it the right scope and power of resistance, or else the world could not have withstood its fierceness."
"The Lord offered the Law to all nations; but all refused to accept it except Israel."
How do we understand "A Gentile who consecrates his life to the study and observance of the Law ranks as high as the high priest", says R. Meïr, by deduction from Lev. xviii. 5; II Sam. vii. 19; Isa. xxvi. 2; Ps. xxxiii. 1, cxviii. 20, cxxv. 4, where all stress is laid not on Israel, but on man or the righteous one.
The Gemara states this regarding a non-Jew who studies Torah [his 7 mitzvot] and regarding this, see Shita Mekubetzes, Bava Kama 38a who says that this is an exaggeration. In any case, this statement was not extolling the non-Jew. The Rishonim explain that it is extolling the Torah.
Tosfos explains that it uses the example of a "kohen gadol" (high priest), because this statement is based on the verse, "y'kara hi mipnimim" (it is more precious than pearls). This is explained elsewhere in the Gemara to mean that the Torah is more precious "pnimim" (translated here as "inside" instead of as "pearls"; thus that the Torah is introspectively absorbed into the person), which refers to "lifnai v'lifnim" (translated as "the most inner of places"), that is the Holy of Holies where the "kahon gadol" went.
In any case, in Midrash Rabba (Bamidbar 13:15) this statement is made with an important addition: a non-Jew who converts and studies Torah etc.
The Nation of Israel is likened to the olive. Just as this fruit yields its precious oil only after being much pressed and squeezed, so Israel's destiny is one of great oppression and hardship, in order that it may thereby give forth its illuminating wisdom. Poverty is the quality most befitting Israel as the chosen people (Ḥag. 9b). Only on account of its good works is Israel among the nations "as the lily among thorns", or "as wheat among the chaff."
Modern Orthodox views.
Rabbi Lord Immanuel Jakobovits, former Chief Rabbi of the United Synagogue of Great Britain (Modern Orthodox Judaism), describes chosenness in this way:
Yes, I do believe that the chosen people concept as affirmed by Judaism in its holy writ, its prayers, and its millennial tradition. In fact, I believe that every people—and indeed, in a more limited way, every individual—is "chosen" or destined for some distinct purpose in advancing the designs of Providence. Only, some fulfill their mission and others do not. Maybe the Greeks were chosen for their unique contributions to art and philosophy, the Romans for their pioneering services in law and government, the British for bringing parliamentary rule into the world, and the Americans for piloting democracy in a pluralistic society. The Jews were chosen by God to be 'peculiar unto Me' as the pioneers of religion and morality; that was and is their national purpose.
Rabbi Norman Lamm, a leader of Modern Orthodox Judaism writes:
Conservative Judaism views.
Conservative Judaism and its Israeli counterpart Masorti Judaism, views the concept of chosenness in this way:
Rabbi Reuven Hammer of Masorti Judaism comments on the excised sentence in the Aleinu prayer mentioned above:
Reform Judaism.
Reform Judaism views the concept of chosenness in this way:
In 1999 the Reform movement stated:
Alternative views.
Equality of souls.
Many Kabbalistic sources, notably the Tanya, contain statements to the effect that the Jewish soul is qualitatively different from the non-Jewish soul. Some prominent Kabbalists rejected this idea and believed in essential equality of all human souls. Menahem Azariah da Fano, in his book "Reincarnations of souls", provides many examples of non-Jewish Biblical figures being reincarnated as Jews, and vice versa. Abraham Cohen de Herrera, another Kabbalist of the same school, quotes Greek, Christian and various Oriental mystics and philosophers without hesitation, and does not mention anything specific about the Jewish souls.
A number of known Chabad rabbis offered alternative readings of the Tanya, did not take this teaching literally, and even managed to reconcile it with the leftist ideas of internationalism and class struggle. The original text of the Tanya refers to the "idol worshippers" and does not mention the "nations of the world" at all, although such interpretation was endorsed by Menachem Mendel Schneerson and is popular in contemporary Chabad circles. Hillel of Parich, an early Tanya commentator, wrote that the souls of righteous Gentiles are more similar to the Jewish souls, and are generally good and not egoistic. This teaching was accepted by Schneerson and is considered normative in Chabad.
Different in character but not value.
According to the author of the Tanya himself, a righteous non-Jew can achieve a high level of spiritually, similar to an angel, although his soul is still fundamentally different in character, but not value, from a Jewish one. Tzemach Tzedek, the third rebbe of Chabad, wrote that the Muslims are naturally good-hearted people. Rabbi Yosef Jacobson, a popular contemporary Chabad lecturer, teaches that in today's world most non-Jews belong to the category of righteous Gentiles, effectively rendering the Tanya's attitude anachronistic.
Dov Ber Pinson, a contemporary Chabad mystic, denies the idea that there is any essential difference between the Jews and non-Jews. According to his theory, every person has a lower animalistic and higher Godly soul. The Tanya does not talk about Jews and non-Jews as social groups, but describes the internal struggle between the materialistic "Gentile" and spiritual "Jewish" levels of consciousness within every human soul.
Altruism.
An anti-Zionist interpretation of Tanya was offered by Abraham Yehudah Khein, a prominent Ukrainian Chabad rabbi, who supported anarchist communism and considered Peter Kropotkin a great Tzaddik. Khein basically read the Tanya backwards; since the souls of idol worshipers are known to be evil, according to the Tanya, while the Jewish souls are known to be good, he concluded that truly altruistic people are really Jewish, in a spiritual sense, while Jewish nationalists and class oppressors are not. By this logic, he claimed that Vladimir Solovyov and Rabindranath Tagore probably have Jewish souls, while Leon Trotsky and other totalitarians do not, and many Zionists, whom he compared to apes, are merely "Jewish by birth certificate".
All the above-mentioned rabbis viewed the Kabbalah in a generally similar way, because the Tanya is based on the teachings of the Zohar, works of Isaac Luria and other Kabbalistic sources.
Righteous non-Jews.
Nachman of Breslov also believed that Jewishness is a level of consciousness, and not an intrinsic inborn quality. He wrote that, according to the Book of Malachi, one can find "potential Jews" among all nations, whose souls are illuminated by the leap of "holy faith", which "activated" the Jewishness in their soul. These people would otherwise convert to Judaism, but prefer not to do so. Instead, they recognize the Divine unity within their pagan religions.
Isaac Arama, an influential philosopher and mystic of the 15th century, believed that righteous non-Jews are spiritually identical to the righteous Jews. Rabbi Menachem Meiri, a famous Catalan Talmudic commentator and Maimonidian philosopher, considered all people, who sincerely profess an ethical religion, to be part of a greater "spiritual Israel". He explicitly included Christian and Muslims in this category. Meiri rejected all Talmudic laws that discriminate between the Jews and non-Jews, claiming that they only apply to the ancient idolators, who had no sense of morality. The only exception are a few laws related directly or indirectly to intermarriage, which Meiri did recognize.
Meiri applied his idea of "spiritual Israel" to the Talmudic statements about unique qualities of the Jewish people. For example, he believed that the famous saying that Israel is above astrological predestination ("Ein Mazal le-Israel") also applied to the followers of other ethical faiths. He also considered countries, inhabited by decent moral non-Jews, such as Languedoc, as a spiritual part of the Holy Land.
Spinoza.
One Jewish critic of chosenness was the philosopher Baruch Spinoza. In the third chapter of his "Theologico-Political Treatise", Spinoza mounts an argument against a naive interpretation of God's choice of the Jews. Bringing evidence from the Bible itself, he argues that God's choice of Israel was not unique (he had chosen other nations before choosing the Hebrew nation) and that the choice of the Jews is neither inclusive (it does not include all of the Jews, but only the 'pious' ones) nor exclusive (it also includes 'true gentile prophets'). Finally, he argues that God's choice is not unconditional. Recalling the numerous times God threatened the complete destruction of the Hebrew nation, he asserts that this choice is neither absolute, nor eternal, nor necessary. Moreover, in aphorism 12 he writes, "Thus the Jews today have absolutely nothing that they can attribute to themselves but not to other peoples..."
Reconstructionist criticism.
Reconstructionist Judaism rejects the concept of chosenness. Its founder, Rabbi Mordecai Kaplan, said that the idea that God chose the Jewish people leads to racist beliefs among Jews, and thus must be excised from Jewish theology. This rejection of chosenness is made explicit in the movement's siddurim (prayer books).
For example, the original blessing recited before reading from the Torah contains the phrase, "asher bahar banu mikol ha’amim"—"Praised are you Lord our God, ruler of the Universe, "who has chosen us from among all peoples" by giving us the Torah." The Reconstructionist version is rewritten as "asher kervanu la’avodato", "Praised are you Lord our God, ruler of the Universe, "who has drawn us to your service" by giving us the Torah."
In the mid-1980s, the Reconstructionist movement issued its "Platform on Reconstructionism". It states that the idea of chosenness is "morally untenable", because anyone who has such beliefs "implies the superiority of the elect community and the rejection of others."
Not all Reconstructionists accept this view. The newest siddur of the movement, "Kol Haneshamah", includes the traditional blessings as an option, and some modern Reconstructionist writers have opined that the traditional formulation is not racist, and should be embraced.
An original prayer book, by Reconstructionist feminist poet Marcia Falk, "The Book of Blessings", has been widely accepted by both Reform and Reconstructionist Jews. Falk rejects all concepts relating to hierarchy or distinction; she sees any distinction as leading to the acceptance of other kinds of distinctions, thus leading to prejudice. She writes that as a politically liberal feminist, she must reject distinctions made between men and women, homosexuals and heterosexuals, Jews and non-Jews, and to some extent even distinctions between the Sabbath and the other six days of the week. She thus rejects idea of chosenness as unethical. She also rejects Jewish theology in general, and instead holds to a form of religious humanism. Falk writes:
Reconstructionist author Judith Plaskow also criticises the idea of chosenness, for many of the same reasons as Falk. A politically liberal lesbian, Plaskow rejects most distinctions made between men and women, homosexuals and heterosexuals, and Jews and non-Jews. In contrast to Falk, Plaskow does not reject all concepts of difference as inherently leading to unethical beliefs, and holds to a more classical form of Jewish theism than Falk.
A number of responses to these views have been made by Reform and Conservative Jews; they hold that these criticisms are against teachings that do not exist within liberal forms of Judaism, and which are rare in Orthodox Judaism (outside certain Haredi communities, such as Chabad). A separate criticism stems from the very existence of feminist forms of Judaism in all denominations of Judaism, which do not have a problem with the concepts of chosenness.
Views from other religions.
Islam.
The children of Israel enjoy a special status in the Islamic book, the Quran:
O children of Israel, remember my favor which I bestowed upon you, and that I favored you above all creation. (Qur'an 2:47). 2:122).
However, Muslim scholars point out that this status did not confer upon Israelites any racial superiority, and was only valid so long as the Israelites maintain their covenant with God,
Indeed God had taken the covenant from the Children of Israel, and We appointed twelve leaders among them. And God said: "I am with you if you establish the prayer and offer the Zakat (compulsory charity) and believe in My Messengers; honor and assist them, and lend to God a good loan. Verily, I will remit your sins and admit you to Gardens under which rivers flow (in Paradise). But if any of you after this, disbelieve, he has indeed gone astray from the Straight Path." (Quran 5:12)
Christianity.
Some Christians believe that the Jews were God's chosen people (), but because of Jewish Rejection of Jesus, the Christians in turn received that special status (). This doctrine is known as Supersessionism.
However, most other Christians are of the view that all people who turn to Christ as their personal saviour are 'chosen' in the context of John 15:16 whereby Jesus referred to God's plan of salvation as his great redeeming work on the cross, that all who come to faith in him does so freely and are 'chosen' to bear 'fruit that lasts'. 1 Peter 2:9 refers to these (Christians) as 'chosen people, a royal priesthood, a holy nation, God's special possession' .
Influence on relations with other religions.
Avi Beker, a scholar and the former Secretary General of the World Jewish Congress, regards the idea of the chosen people as Judaism's defining concept which is "the central unspoken psychological, historical, and theological problem at the heart of Jewish-Gentile relations." Beker views the concept of choseness as the driving force behind Jewish-Gentile relations, which explains both the admiration and, more pointedly, the envy and hatred the world has felt for the Jews in religious and also secular terms. Beker argues that while Christianity has modified its doctrine on the displacement of the Jews, he accuses Islam of not reversing or reforming its theology on the succession of both the Jews and the Christians. According to Baker, this presents a major barrier to conflict resolution in the Arab-Israeli conflict.
Ethnocentrism and racism.
Israeli philosopher Ze’ev Levy writes that chosenness can be "(partially) justified only from the historical angle" with respect to its spiritual and moral contribution to Jewish life thorough centuries, "a powerful agent of consolation and hope". He points out however that modern anthropological theories "do not merely proclaim the inherent universal equality of all people [as] human beings; they also stress the "equivalence" of all human cultures." (emphasis in original) He continues that "there are no inferior and superior people or cultures but only different, "other", ones." He concludes that the concept of chosenness entails ethnocentrism, "which does not go hand in hand with otherness, that is, with unconditional respect of otherness".
Some people have claimed that Judaism's chosen people concept is racist because it implies that Jews are superior to non-Jews. The Anti-Defamation League, and other authorities, assert that the concept of a chosen people within Judaism has nothing to do with racial superiority, but rather is a description of the special relationship between God and Jews.

</doc>
<doc id="7453" url="http://en.wikipedia.org/wiki?curid=7453" title="Christian persecution">
Christian persecution

Christian persecution could refer to:
More generally, see:

</doc>
<doc id="7455" url="http://en.wikipedia.org/wiki?curid=7455" title="Chaparral">
Chaparral

Chaparral is a shrubland or heathland plant community found primarily in the U.S. state of California and in the northern portion of the Baja California peninsula, Mexico. It is shaped by a Mediterranean climate (mild, wet winters and hot dry summers) and wildfire, featuring summer-drought tolerant plants with hard sclerophyllous evergreen leaves, as contrasted with the associated soft-leaved, drought deciduous, scrub community of Coastal sage scrub, found below the chaparral biome. Chaparral covers 5% of the state of California, and associated Mediterranean shrubland an additional 3.5%. The name comes from the Spanish word "chaparro", applied to scrub oaks.
Introduction.
In its natural regime, chaparral is characterized by infrequent fires, with intervals ranging between 10–15 years to over a hundred years. Mature chaparral (stands that have been allowed greater intervals between fires) is characterized by nearly impenetrable, dense thickets (except the more open chaparral of the desert). These plants are highly flammable. They grow as woody shrubs with hard and small leaves; are non-leaf dropping (non-deciduous); and are drought tolerant. After the first rains following a fire, the landscape is dominated by soft-leaved non-woody annual plants, known as fire followers, which die back with the summer dry period. 
Similar plant communities are found in the four other Mediterranean climate regions around the world, including the Mediterranean Basin (where it is known as maquis), central Chile (where it is called matorral), South African Cape Region (known there as fynbos), and in Western and Southern Australia (as kwongan). According to the California Academy of Sciences, Mediterranean shrubland contains more than 20% of the world's plant diversity. The word "chaparral" is a loan word from Spanish "chaparro", meaning both "small" and "dwarf" evergreen oak, which itself comes from the Basque word "txapar", with exactly the same meaning.
Conservation International and other conservation organizations consider the chaparral to be a biodiversity hotspot- a biological community with a large number of different species - that are under threat by human activity.
California chaparral.
California chaparral and woodlands ecoregion.
The California chaparral and woodlands ecoregion, of the Mediterranean forests, woodlands, and scrub Biome, has three sub-ecoregions with ecosystem—plant community subdivisions:
Chaparral and woodlands biota.
For the numerous individual plant and animal species found within the California chaparral and woodlands ecoregion, see the "(index)", and the "(index)".
California cismontane and transmontane chaparral subdivisions.
Another phytogeography system uses two California chaparral and woodlands subdivisions: the cismontane chaparral; and the transmontane (desert) chaparral.
California cismontane chaparral.
"Cismontane chaparral" ("this side of the mountain") refers to a chaparral ecosystem in the Mediterranean forests, woodlands, and scrub Biome in California, growing on the western (and coastal) sides of large mountain range systems, such as: western slopes of the Sierra Nevada in the San Joaquin Valley foothills, western slopes of the Peninsular Ranges and California Coast Ranges, and south-southwest slopes of the Transverse Ranges in the Central Coast and Southern California regions.
Cismontane chaparral plant species.
In Central and Southern California chaparral forms a dominant habitat. Members of the chaparral biota native to California, all of which tend to regrow quickly after fires, include:
Cismontane chaparral bird species.
The complex ecology of chaparral habitats supports a very large number of animal species. the following is a short list of birds which are an integral part of the cismontane chaparral ecosystems. 
California transmontane (desert) chaparral.
Transmontane chaparral or Desert chaparral — "transmontane" ("the other side of the mountain") "chaparral" — refers to the desert shrubland habitat and chaparral plant community growing in the rainshadow of these ranges. Transmontane chaparral features xeric desert climate - not Mediterranean climate habitats, and is also referred to as Desert chaparral. Desert chaparral is a regional ecosystem subset of the Deserts and xeric shrublands Biome, with some plant species from the California chaparral and woodlands ecoregion. Unlike cismontain chaparral, which forms dense, impenatrable stands of plants, desert chaparral is open, with only about 50% of the ground covered. Individual shrubs can reach up to in height. 
Transmontane chaparral or Desert chaparral is found on the eastern slopes of major mountain range systems on the western sides of the deserts of California. The mountain systems include: the southeastern Transverse Ranges (the San Bernardino and San Gabriel Mountains) in the Mojave Desert north and northeast of the Los Angeles basin and Inland Empire; and the northern Peninsular Ranges (San Jacinto, Santa Rosa, and Laguna Mountains), which separate the Colorado Desert (western Sonoran Desert) from lower coastal Southern California. It is distinguished from the cismontain chaparral found on the coastal side of the mountains, which experiences higher winter rainfall. Naturally, desert chaparral experiences less winter rainfall than cismontain chaparral. Plants in this community are characterized by small, hard (sclerophyllic) evergreen (non-dropping; non-deciduous) leaves. Desert chaparral grows above California's desert cactus scrub plant community, and below the Pinyon-juniper woodland. It is further distinguished from the deciduous sub-alpine scrub above the pinyon-juniper woodlands on the same side of the Peninsular ranges.
Transmontane chaparral distribution.
Transmontane—Desert chaparral typically grows on the lower ( elevation) northern slopes of the southern Transverse Ranges (running east to west in San Bernardino and Los Angeles Counties) and on the lower () eastern slopes of the Peninsular Ranges (running south to north from lower Baja California to Riverside and Orange Counties and the Transverse Ranges). It can also be found in higher elevation sky islands in the interior of the deserts, such as in the upper New York Mountains within the Mojave National Preserve in the Mojave Desert.
The California transmontane (desert) chaparral is found in the rain shadow deserts of the:
Transmontane chaparral animals.
There is overlap of animals with those of the adjacent desert and Pinyon-Juniper communities.
Chaparral and wildfires.
The Chaparral is a coastal biome with hot, dry summers and mild, rainy winters. The Chaparral area receives about of precipitation a year. This makes the chaparral most vulnerable to fire in the late summer and fall.
The chaparral ecosystem as a whole is adapted to be able to recover from infrequent wildfires (fires occurring a minimum of 15 years apart); indeed, chaparral regions are known culturally and historically for their impressive fires. (This does create a conflict with human development adjacent to and expanding into chaparral systems.) Before a major fire, typical chaparral plant communities are dominated by manzanita, chamise (also called greasewood or "Adenostoma fasciculatum") and "Ceanothus" species, Toyon (which can sometimes be interspersed with scrub oaks), and other drought-resistant shrubs with hard (sclerophyllous) leaves; these plants resprout (see resprouter) from underground burls after a fire. Some chaparral plant communities may grow so dense and tall that it becomes difficult for large animals and humans to penetrate, but may be teeming with smaller fauna in the understory. Many chaparral plant species require some fire cue (heat, smoke, or charred wood, and chemical changes in the soil following fires) for germination. Others, such as annual and herbaceous species like "Phacelia" require fires to allow sunlight to reach them, and are known as fire followers. During the time shortly after a fire, chaparral communities may contain soft-leaved herbaceuous annual plants that dominate the community for the first few years - until the burl resprouts and seedlings of chaparral perennials create an overstory, blocking the sunlight from other plants in the community. When the overstory regrows, seeds of annuals and smaller plants may lie dormant until the next fire creates the conditions required for germination. Mid-sized plants such as "Ceonothus" fix nitrogen, while others cannot, which, together with the need for exposure to the sun, creates a symbiotic relationship of the entire community with infrequent fires.
Because of the hot, dry conditions that exist in the California summer and fall, chaparral is one of the most fire-prone plant communities in North America. Some fires are caused by lightning, but these are usually during periods of high humidity and low winds and are easily controlled. Nearly all of the very large wildfires are caused by human activity during periods of very hot, dry easterly Santa Ana winds. These man-made fires are commonly caused by power line failures, vehicle fires and collisions, sparks from machinery, arson, or campfires.
Wildfire debate.
There are two assumptions relating to California chaparral fire regimes that appear to have caused considerable debate, and sometimes confusion and controversy, within the fields of wildfire ecology and land management. 
The perspective that older chaparral is unhealthy or unproductive may have originated during the 1940s when studies were conducted measuring the amount of forage available to deer populations in chaparral stands. However, according to recent studies, California chaparral is extraordinarily resilient to very long periods without fire and continues to maintain productive growth throughout pre-fire conditions. Seeds of many chaparral plants actually require 30 years or more worth of accumulated leaf litter before they will successfully germinate (e.g. scrub oak: "Quercus berberidifolia", toyon: "Heteromeles arbutifolia", and holly-leafed cherry: "Prunus ilicifolia"). When intervals between fires drop below 10 to 15 years, many chaparral species are eliminated and the system is typically replaced by non-native, invasive, weedy grassland.
The idea that older chaparral is responsible for causing large fires was originally proposed in the 1980s by comparing wildfires in Baja California and southern California . It was suggested that fire suppression activities in southern California allowed more fuel to accumulate, which in turn led to larger fires (in Baja, fires often burn without active suppression efforts ). This is similar to the argument that fire suppression in western United States has allowed Ponderosa Pine forests to become “overstocked”. In the past, surface-fires burned through these forests at intervals of anywhere between 4 and 36 years, clearing out the understory and creating a more ecologically balanced system. However, chaparral has a crown-fire regime, meaning that fires consume the entire system whenever they burn. In one study, a detailed analysis of historical fire data concluded that fire suppression activities have failed to exclude fire from southern California chaparral, as they have in Ponderosa Pine forests. In addition, the number of fires is increasing in step with population growth. Chaparral stand age does not have a significant correlation to its tendency to burn. Low humidity, low fuel moisture, and high winds appear to be the primary factors in determining when, where, and how large a chaparral fire burns. 

</doc>
<doc id="7456" url="http://en.wikipedia.org/wiki?curid=7456" title="CJD">
CJD

CJD can mean:

</doc>
<doc id="7460" url="http://en.wikipedia.org/wiki?curid=7460" title="Clinker">
Clinker

Clinker may refer to:
Clinker may also be used for:

</doc>
<doc id="7461" url="http://en.wikipedia.org/wiki?curid=7461" title="Clipper">
Clipper

A clipper was a very fast sailing ship of the middle third of the 19th century. They were fast, yachtlike vessels, with three masts and a square rig. They were generally narrow for their length, could carry limited bulk freight, small by later 19th century standards, and had a large total sail area. Clipper ships were mostly constructed in British and American shipyards, though France, Brazil, the Netherlands and other nations also produced some. Clippers sailed all over the world, primarily on the trade routes between the United Kingdom and its colonies in the east, in trans-Atlantic trade, and the New York-to-San Francisco route round Cape Horn during the California Gold Rush. Dutch clippers were built beginning in the 1850s for the tea trade and passenger service to Java.
The boom years of the clipper ship era began in 1843 as a result of a growing demand for a more rapid delivery of tea from China. It continued under the stimulating influence of the discovery of gold in California and Australia in 1848 and 1851, and ended with the opening of the Suez Canal in 1869.
Origin and usage of "clipper".
The term "clipper" most likely derives from the verb "clip", which in former times meant, among other things, to run or fly swiftly. Dryden, the English poet, used the word "clip" to describe the swift flight of a falcon in the 17th century when he said "And, with her eagerness the quarry missed, Straight flies at check, and clips it down the wind." The ships appeared to clip along the ocean water. The term "clip" became synonymous with "speed" and was also applied to fast horses and sailing ships. "To clip it," and "going at a good clip," are familiar expressions to this day.
While the first application of the term "clipper" in a nautical sense is by no means certain, it seems to have had an American origin when applied to the Baltimore clippers of the late 18th century. When these vessels of a new model were built, which were intended to "clip" over the waves rather than plough through them, the improved type of craft became known as "clippers" because of their speed.
In England the nautical term "clipper" appeared a little later. The "Oxford English Dictionary" says its earliest quotation for "clipper" is from 1830. This does not mean, however, that little British opium clippers from prior to 1830 were not called "opium clippers" just as they are today. Carl C. Cutler reports the first newspaper appearance was in 1835, and by then the term was apparently familiar. An undated painting of the British "Water Witch" built in 1831 is labeled "OPIUM CLIPPER "WATER WITCH"" so the term had at least passed into common usage during the time that this ship sailed.
There is no single definition of the characteristics of a clipper ship, but mariner and author Alan Villiers describes them as follows:To sailors, three things made a ship a clipper. She must be sharp-lined, built for speed. She must be tall-sparred and carry the utmost spread of canvas. And she must "use" that sail, day and night, fair weathe and foul. Optimized for speed, they were too fine-lined to carry much cargo. Clippers typically carried extra sails such as skysails and moonrakers on the masts, and studdingsails on booms extending out from the hull or yards, which required extra hands to handle them. And in conditions where other ships would shorten sail, clippers drove on, heeling so much that their lee rails were in the water.
History.
The first ships to which the term "clipper" seems to have been applied were the Baltimore clippers. Baltimore clippers were topsail schooners developed in the Chesapeake Bay before the American Revolution, and which reached their zenith between 1795 and 1815. They were small, rarely exceeding 200 tons OM, and modelled after French luggers. Some were lightly armed in the War of 1812, sailing under Letters of Marque and Reprisal, when the type—exemplified by "Chasseur", launched at Fells Point, Baltimore in 1814 became known for her incredible speed; the deep draft enabled the Baltimore clipper to sail close to the wind. Clippers, running the British blockade of Baltimore, came to be recognized for speed rather than cargo space.
Speed was also required for the Chinese opium trade between England, India and China. Small, sharp-bowed British vessels were the result. An early example, which is today known as an opium clipper, was "Transit" of 1819. She was followed by many more.
Meanwhile Baltimore Clippers still continued to be built, and were built specifically for the China opium trade running opium between India and China, a trade that only became unprofitable for American shipowners in 1849.
"Ann McKim" is generally known as the original clipper ship. She was built in Baltimore in 1833 and was the first attempt at building a larger swift vessel in the United States. "Ann McKim" 494 tons OM, was built on the enlarged lines of a Baltimore clipper, with sharply raked stem, counter stern and square rig. She was built in Baltimore in 1833 by the Kennard & Williamson shipyard. Although "Ann McKim" was the first large clipper ship ever constructed, it cannot be said that she founded the clipper ship era, or even that she directly influenced shipbuilders, since no other ship was built like her; but she may have suggested the clipper design in vessels of ship rig. She did, however, influence the building of "Rainbow" in 1845, the first extreme clipper ship.
In Aberdeen, Scotland, the shipbuilders Alexander Hall and Sons developed the "Aberdeen" clipper bow in the late 1830s: the first was "Scottish Maid" launched in 1839. "Scottish Maid", 150 tons OM, was the first British clipper ship. ""Scottish Maid" was intended for the Aberdeen-London trade, where speed was crucial to compete with steamships. The Hall brothers tested various hulls in a water tank and found the clipper design most effective. The design was influenced by tonnage regulations. Tonnage measured a ship's cargo capacity and was used to calculate tax and harbour dues. The new 1836 regulations measured depth and breadth with length measured at half midship depth. Extra length above this level was tax-free and became a feature of clippers. "Scottish Maid" proved swift and reliable and the design was widely copied." The earliest British clipper ships were built for trade amongst the British Isles. Then followed the vast clipper trade of tea, opium, spices and other goods from the Far East to Europe, and the ships became known as "tea clippers".
From 1839, larger American clipper ships started to be built beginning with "Akbar", 650 tons OM, in 1839, and including the 1844-built Houqua, 581 tons OM. These larger vessels were built predominantly for use in the China tea trade and known as "tea clippers". Smaller clipper vessels also continued to be built predominantly for the China opium trade and known as "opium clippers" such as the 1842 built "Ariel", 100 tons OM.
Then in 1845 "Rainbow", 757 tons OM, the first extreme clipper was launched in New York. These American clippers were larger vessels designed to sacrifice cargo capacity for speed. They had a bow lengthened above the water, a drawing out and sharpening of the forward body, and the greatest breadth further aft. Extreme clippers were built in the period 1845 to 1855. From 1851 or earlier another type of clipper ship was also being built in American shipyards, the medium clipper. The medium clipper, though still very fast, had comparatively more allowance for cargo. After 1854 extreme clippers were replaced in American shipbuilding yards by medium clippers.
The Flying Cloud was a clipper ship that set the world's sailing record for the fastest passage between New York and San Francisco, 89 days 8 hours. She held this record for over 100 years, from 1854-1989.[1]
Flying Cloud was the most famous of the clippers built by Donald McKay. She was known for her extremely close race with the Hornet in 1853; for having a woman navigator, Eleanor Creesy, wife of Josiah Perkins Creesy who skippered the Flying Cloud on two record-setting voyages from New York to San Francisco; and for sailing in the Australia and timber trades.
Clipper ships largely ceased being built in American shipyards in 1859 when, unlike the earlier boom years, only 4 clipper ships were built. That is except for a small number built in the 1860s, and the last American clipper ship from the East Boston shipyard of Donald McKay in 1869, "Glory of the Seas".
During the time from 1859 British clipper ships continued to be built. Earlier British clipper ships had become known as extreme clippers, and were considered to be "as sharp as the American" built ships. From 1859 a new design was developed for British clipper ships that was nothing like the American clippers. These ships built from 1859 continued to be called extreme clippers. The new design had a sleek graceful appearance, less sheer, less freeboard, lower bulwarks, and smaller breadth. They were built for the China tea trade and began with "Falcon" in 1859, and finished with the last ships built in 1870. It is estimated that 25 to 30 of these ships were built, and no more than 4–5 per year. The earlier ships were made from wood, though some were made from iron, just as some British clippers had been made from iron prior to 1859. In 1863 the first tea clippers of composite construction were brought out, combining the best of both worlds. Composite clippers had the strength of iron spars with wooden hulls, and copper sheathing could be added to prevent the fouling that occurred on iron hulls.
After 1869 with the opening of the Suez Canal that allowed competition with steam vessels, the tea trade then collapsed for clippers. From 1870 the clipper trade increasingly focused on trade and the carrying of immigrants between England and Australia and New Zealand, a trade that had begun earlier with the Australian Gold Rush in the 1850s. British-built clipper ships were used for this trade, as were many American-built ships which were sold to British owners. Even in the 1880s, sailing ships were still the main carriers of cargoes to and from Australia and New Zealand. Eventually, however, even this trade became unprofitable, and the aging clipper fleet became unseaworthy.
China clippers and the apogee of sail.
Among the most notable clippers were the China clippers, also called Tea clippers or Opium clippers, designed to ply the trade routes between Europe and the East Indies. The last example of these still in reasonable condition was "Cutty Sark", preserved in dry dock at Greenwich, United Kingdom. Damaged by fire on 21 May 2007 while undergoing conservation, the ship was permanently elevated three meters above the dry dock floor in 2010 as part of a plan for long-term preservation.
Before the early 18th century, the East India Company paid for its tea mainly in silver. However, when the Chinese Emperor chose to embargo European manufactured commodities and demand payment for all Chinese goods in silver, the price rose, restricting free trade. The East India Company began to manufacture a product that was desired by the Chinese as much as tea was by the British: opium. This had a significant influence on both India and China. Opium was also imported into Britain and was not prohibited because it was thought to be medically beneficial. Laudanum, which was made from opium was also used as a pain killer, to induce sleep and to suppress anxiety. The famous literary opium addicts Thomas De Quincey, Samuel Taylor Coleridge and Wilkie Collins also took it for its pleasurable effects. The Limehouse area in London was notorious for its opium dens, many of which catered for Chinese sailors as well as English addicts.
Clippers were built for seasonal trades such as tea, where an early cargo was more valuable, or for passenger routes. One passenger ship survives, the City of Adelaide designed by William Pile of Sunderland. The fast ships were ideally suited to low-volume, high-profit goods, such as tea, opium, spices, people, and mail. The return could be spectacular. The "Challenger" returned from Shanghai with "the most valuable cargo of tea and silk ever to be laden in one bottom". Competition among the clippers was public and fierce, with their times recorded in the newspapers. The ships had short-expected lifetimes and rarely outlasted two decades of use before they were broken up for salvage. Given their speed and maneuverability, clippers frequently mounted cannon or carronades and were used for piracy, privateering, smuggling, or interdiction service.
The last China clippers were acknowledged as the fastest sail vessels. When fully rigged and riding a tradewind, they had peak average speeds over . The Great Tea Race of 1866 showcased their speed. China clippers are also the fastest commercial sailing vessels ever made. Their speeds have been exceeded many times by modern yachts, but never by a commercial sail vessel. Only the fastest windjammers could attain similar speeds.
The 24h record of the "Champion of the Seas" wasn't broken until 1984 (by a multihull), or 2001 (by another monohull).
Decline.
Decline in the use of clippers started with the economic slump following the Panic of 1857 and continued with the gradual introduction of the steamship. Although clippers could be much faster than early steamships, they depended on the vagaries of the wind, while steamers could keep to a schedule. The "steam clipper" was developed around this time, and had auxiliary steam engines which could be used in the absence of wind. An example was "Royal Charter", built in 1857 and wrecked on the coast of Anglesey in 1859. The final blow was the Suez Canal, opened in 1869, which provided a great shortcut for steamships between Europe and Asia, but was difficult for sailing ships to use. With the absence of the tea trade, some clippers began operating in the wool trade, between Britain and Australia.
Surviving Ships.
Although many clipper ships were built in the mid-19th century, "Cutty Sark" was, perhaps until recently, the only intact survivor. Other surviving examples of clippers of the era are less well preserved, for example the oldest surviving clipper "City of Adelaide"" (a.k.a. S.V. "Carrick").
"Falls of Clyde" is a well-preserved example of a more conservatively designed, slower contemporary of the clippers, which was built for general freight in 1878.
Clipper ship sailing cards.
Departures of clipper ships, mostly from New York and Boston to San Francisco, were advertised by clipper ship sailing cards. These cards, slightly larger than today’s postcards, were produced by letterpress and wood engraving on coated card stock. Most clipper cards were printed in the 1850s and 1860s, and represented the first pronounced use of color in American advertising art.
Relatively few (perhaps 3,500) cards survive today. With their stunning appearance, rarity, and importance as artifacts of nautical, Western, and printing history, clipper cards are highly prized by both private collectors and institutions.

</doc>
<doc id="7462" url="http://en.wikipedia.org/wiki?curid=7462" title="Clive Anderson">
Clive Anderson

Clive Anderson (born 10 December 1952) is an English television and radio presenter, comedy writer and former barrister. Winner of a British Comedy Award in 1991, Anderson began experimenting with comedy and writing comedic scripts during his 15-year law career, before starring in "Whose Line Is It Anyway?" on BBC Radio 4, then later Channel 4. He has also been successful with a number of radio programmes, television interviews and guest appearances on "Have I Got News for You", "Mock the Week" and "QI". He has also recently appeared on Alexander Armstrong's TV panel show 'Alexander Armstrong's Big Ask'
Early life.
Anderson was educated at Stanburn Primary School and Harrow County School for Boys in London, where his group of friends included Geoffrey Perkins and Michael Portillo. His Scottish father was manager of the Midland Bank's Wembley branch. Anderson attended Selwyn College, Cambridge, where, from 1974 to 1975, he was President of Footlights. He was called to the bar at the Middle Temple in 1976 and became a practising barrister, specialising in criminal law.
Career.
Television.
Anderson was involved in the fledgling alternative comedy scene in the early 1980s and was the first act to come on stage at The Comedy Store when it opened in 1979. He made his name as host of the improvised television comedy show "Whose Line Is It Anyway?", which ran for 10 series.
Anderson hosted his own chat-show, "Clive Anderson Talks Back", on Channel 4, which ran for 10 series. Anderson moved to the BBC in 1996 and the show's name changed to "Clive Anderson All Talk" and was aired on BBC One. In one famous incident in 1997, Anderson interviewed the Bee Gees, and throughout the interview he repeatedly joked about their life and career, ultimately prompting Barry Gibb to say "You're a tosser pal" to Anderson in a strong mancunian accent and walk out. Anderson once had a glass of water poured over his head by a perturbed Richard Branson. He also famously asked Jeffrey Archer, "Is there no beginning to your talents?" Archer retorted that "The old jokes are always the best," for Anderson to reply "Yes, I've read your books!" The last series of "Clive Anderson All Talk" aired in 2001.
He has been a frequent participant on "Have I Got News for You", making ten appearances in total. He has also frequently appeared on "QI". In 2007, he featured as a regular panellist on the ITV comedy show "News Knight". One of his most memorable exchanges on "HIGNFY" occurred when he scathingly joked to fellow guest Piers Morgan that the "Daily Mirror" was now, thanks to Morgan (then its editor), almost as good as "The Sun". When asked by Morgan, "What do you know about editing newspapers?", he swiftly replied, "About as much as you do."
As a journalist for the BBC, he travelled around the world looking at problems "in out-of-the-way places," though mostly arguing about whether they could film there. "Our Man in..." featured episodes on monkeywrenching in American logging and 419 scams in Nigeria.
In 2005 he presented the short-lived Celador panel game "Back in the Day" for Channel 4.
In January 2008, he appeared on the second episode of "Thank God You're Here" and won.
On 25 February 2008, he started presenting "Brainbox Challenge", a new game show, for BBC Two.
In 2008, he presented a reality TV talent show-themed television series produced by the BBC entitled "Maestro", starring eight celebrities who are "famous amateurs with a passion for classical music."
In 2009, Anderson was the television host of the BBC's "Last Night of the Proms".
TV presenting.
Shows he has presented include:
Radio.
In recent years, Clive Anderson has combined his continuing interest in the law with his role as a radio presenter in the regular series "Unreliable Evidence" on Radio 4. He also covered the Sunday morning 11 AM-1 PM show on BBC Radio 2 through the end of January 2008.
It was announced in April 2008 that Anderson, who had previously filled in for host Ned Sherrin from 2006 until his death from throat cancer in 2007, would be taking over as permanent host of "Loose Ends". He also hosted six series of "Clive Anderson's Chat Room" on BBC Radio 2 from 2004–2009. Clive Anderson has appeared on BBC Radio 4's "The Unbelievable Truth" hosted by David Mitchell.
Clive also presents "The Guessing Game (radio)" on BBC Radio Scotland.
Comedy and newspaper writing.
Anderson is a comedy sketch writer who has written for Frankie Howerd, "Not the Nine O'Clock News", and Griff Rhys Jones/Mel Smith. One of his early comedy writing projects was "Black Cinderella Two Goes East" with Rory McGrath for BBC Radio 4 in 1978. He is famous for his fast, nervous delivery and close-to-the-knuckle witticisms.
As well as writing comedy, Anderson is also a frequent contributor to newspapers, and was a regular columnist in the "Sunday Correspondent".
Personal life.
Anderson lives in Highbury, north London, with his wife, Jane, and three children; Isabella, Flora and Edmund. He supports Arsenal, and Rangers FC and is President of the Woodland Trust and Vice Patron of the Solicitors' Benevolent Association.
He also has a holiday home in Dalmally, Argyll.
Awards.
The show "Whose Line is it Anyway?" won a BAFTA award in 1990. Later, Clive Anderson won both the "Top Entertainment Presenter" and "Top Radio Comedy Personality" at the British Comedy Awards in 1991.
References.
 

</doc>
<doc id="7463" url="http://en.wikipedia.org/wiki?curid=7463" title="Cold fusion">
Cold fusion

Cold fusion is a hypothetical type of nuclear reaction that would occur at, or near, room temperature, compared with temperatures in the millions of degrees that are required for "hot" fusion, which takes place naturally within stars. There is currently no accepted theoretical model which would allow cold fusion to occur.
In 1989 Martin Fleischmann (then one of the world's leading electrochemists) and Stanley Pons reported that their apparatus had produced anomalous heat ("excess heat"), of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode. The reported results received wide media attention, and raised hopes of a cheap and abundant source of energy.
Many scientists tried to replicate the experiment with the few details available. Hopes fell with the large number of negative replications, the withdrawal of many positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. In 1989, a review panel organized by the United States Department of Energy (DOE) found that the evidence for the discovery of a new nuclear process was not persuasive enough to start a special program, but was "sympathetic toward modest support" for experiments "within the present funding system." A second DOE review, convened in 2004 to look at new research, reached conclusions similar to the first. Support within the then-present funding system did not occur.
A small community of researchers continues to investigate cold fusion, now often preferring the designation low-energy nuclear reactions (LENR). Since cold fusion articles are rarely published in peer-reviewed scientific journals, the results do not receive as much scrutiny as more mainstream topics.
History.
Nuclear fusion occurs at temperatures in the tens of millions of degrees. For over 100 years there has been speculation that nuclear fusion might happen at much lower temperatures by fusing hydrogen absorbed in a metal catalyst. In 1989, a claim by Stanley Pons and Martin Fleischmann (then one of the world's leading electrochemists) that such cold fusion had been observed caused a brief media sensation before other scientists began heavily criticizing their claim as being incorrect after many failed to replicate the excess heat. Since the initial announcement, cold fusion research has continued by a small community of committed researchers convinced that such reactions do happen and hoping to gain wider recognition for their experimental evidence.
Before the Fleischmann–Pons experiment.
The ability of palladium to absorb hydrogen was recognized as early as the nineteenth century by Thomas Graham. In the late 1920s, two Austrian born scientists, Friedrich Paneth and Kurt Peters, originally reported the transformation of hydrogen into helium by spontaneous nuclear catalysis when hydrogen was absorbed by finely divided palladium at room temperature. However, the authors later retracted that report, acknowledging that the helium they measured was due to background from the air.
In 1927, Swedish scientist J. Tandberg stated that he had fused hydrogen into helium in an electrolytic cell with palladium electrodes. On the basis of his work, he applied for a Swedish patent for "a method to produce helium and useful reaction energy". After deuterium was discovered in 1932, Tandberg continued his experiments with heavy water. Due to Paneth and Peters's retraction, Tandberg's patent application was eventually denied. His application for a patent in 1927 was denied as he could not explain the physical process.
The final experiments made by Tandberg with heavy water were similar to the original experiment by Fleischmann and Pons. Fleischmann and Pons were not aware of Tandberg's work.
The term "cold fusion" was used as early as 1956 in a New York Times article about Luis W. Alvarez's work on muon-catalyzed fusion. E. Paul Palmer of Brigham Young University also used the term "cold fusion" in 1986 in an investigation of "geo-fusion", the possible existence of fusion in a planetary core. IN 1989, Palmer and Jones preferred the term "piezonuclear fusion", coined by Jones.
Fleischmann–Pons experiment.
The most famous cold fusion claims were made by Stanley Pons and Martin Fleischmann in 1989. After a brief period of interest by the wider scientific community, their reports were called into question by nuclear physicists. Pons and Fleischmann never retracted their claims, but moved their research program to France after the controversy erupted.
Events preceding announcement.
 Martin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.
In 1988, Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven E. Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled "Cold nuclear fusion" that had been published in "Scientific American" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable "excess energy", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to simultaneously publish their results, though their accounts of their March 6 meeting differ.
Announcement.
In mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on March 24 to send their papers to "Nature" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, submitting their paper to the "Journal of Electroanalytical Chemistry" on March 11, and disclosing their work via a press release and press conference on March 23. Jones, upset, faxed in his paper to "Nature" after the press conference.
Fleischmann and Pons' announcement drew wide media attention. Cold fusion was proposing the counterintuitive idea that a nuclear reaction could be caused to occur inside a chemically bound crystal structure. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established conjecture. And many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.
The announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Peterson, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems, and would provide a limitless inexhaustible source of clean energy, using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: "What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics."
Response and fallout.
Although the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to "Nature" reproducing excess heat, although it passed peer-review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal "Fusion Technology". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that "essentially all" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On April 10, 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. In 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement in 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in the 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as "cold fusion" or "fusion confusion" in the news.
In April 1989, Fleischmann and Pons published a "preliminary note" in the "Journal of Electroanalytical Chemistry". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.
Nevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.
On April 30, 1989, cold fusion was declared dead by the "New York Times". The "Times" called it a circus the same day, and the "Boston Herald" attacked cold fusion the following day.
On May 1, 1989, the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of "the incompetence and delusion of Pons and Fleischmann," which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.
On May 4, due to all this new criticism, the meetings with various representatives from Washington were cancelled.
From May 8 only the A&M tritium results kept cold fusion afloat.
In July and November 1989, "Nature" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including "Science", "Physical Review Letters", and "Physical Review C" (nuclear physics).
In August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.
The United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of "focused experiments within the general funding system." Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 different countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of "pathological science".
In March 1990 Dr. Michael H. Salamon, a Utah physicist, and nine co-authors reported negative results. University faculty were then "stunned" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.
In early May 1990 one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in "Science" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.
On 30 June 1991 the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.
On 1 January 1991, Pons left his tenure, and both he and Fleischmann quietly left the United States. In 1992 they resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million.
Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.
Mostly in the 1990s several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years several books have appeared that defended them. Around 1998 the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997 Japan cut off research and closed its own lab after spending $20 million.
Subsequent research.
Cold fusion research continues today in a few specific venues, but the wider scientific community has generally marginalized the research being done and researchers have had difficulty publishing in mainstream journals. The remaining researchers often term their field Low Energy Nuclear Reactions (LENR) or Chemically Assisted Nuclear Reactions (CANR), also Lattice Assisted Nuclear Reactions (LANR), Condensed Matter Nuclear Science (CMNS) and Lattice Enabled Nuclear Reactions; one of the reasons being to avoid the negative connotations associated with "cold fusion". The new names avoid making bold implications, like implying that fusion is actually occurring. Proponents see the new terms as a more accurate description of the theories they put forward.
The researchers who continue acknowledge that the flaws in the original announcement are the main cause of the subject's marginalization, and they complain of a chronic lack of funding and no possibilities of getting their work published in the highest impact journals. University researchers are often unwilling to investigate cold fusion because they would be ridiculed by their colleagues and their professional careers would be at risk. In 1994, David Goodstein, a professor of physics at Caltech, advocated for increased attention from mainstream researchers and described cold fusion as:
A 1991 review by a cold fusion proponent had calculated "about 600 scientists" were still conducting research. After 1991, cold fusion research only continued in relative obscurity, conducted by groups that had increasing difficulty securing public funding and keeping programs open. These small but committed groups of cold fusion researchers have continued to conduct experiments using Fleischmann and Pons electrolysis set-ups in spite of the rejection by the mainstream community. "The Boston Globe" estimated in 2004 that there were only 100 to 200 researchers working in the field, most suffering damage to their reputation and career. Since the main controversy over Pons and Fleischmann had ended, cold fusion research has been funded by private and small governmental scientific investment funds in the United States, Italy, Japan, and India.
United States.
U.S. Navy researchers at the Space and Naval Warfare Systems Center (SPAWAR) in San Diego have been studying cold fusion since 1989. In 2002, they released a two-volume report, "Thermal and nuclear aspects of the Pd/D2O system," with a plea for funding. This and other published papers prompted a 2004 Department of Energy (DOE) review.
In August 2003 the U.S. Secretary of Energy Abraham ordered the DOE to organize a second review of the field. This was thanks to an April 2003 letter sent by MIT's Peter L. Hagelstein, and the publication of many new papers, including the Italian ENEA and other researchers in the 2003 International Cold Fusion Conference, and a two-volume book by U.S. SPAWAR in 2002. Cold fusion researchers were asked to present a review document of all the evidence since the 1989 review. The report was released in 2004. The reviewers were "split approximately evenly" on whether the experiments had produced energy in the form of heat, but "most reviewers, even those who accepted the evidence for excess power production, 'stated that the effects are not repeatable, the magnitude of the effect has not increased in over a decade of work, and that many of the reported experiments were not well documented.'". In summary, reviewers found that cold fusion evidence was still not convincing 15 years later, and they didn't recommend a federal research program. They only recommended that agencies consider funding individual well-thought studies in specific areas where research "could be helpful in resolving some of the controversies in the field". They summarized its conclusions thus:
Cold fusion researchers placed a "rosier spin" on the report, noting that they were finally being treated like normal scientists, and that the report had increased interest in the field and caused "a huge upswing in interest in funding cold fusion research." However, in a 2009 BBC article on an American Chemical Society's meeting on cold fusion, particle physicist Frank Close was quoted stating that the problems that plagued the original cold fusion announcement were still happening: results from studies are still not being independently verified and inexplicable phenomena encountered are being labelled as "cold fusion" even if they are not, in order to attract the attention of journalists.
In February 2012 millionaire Sidney Kimmel, convinced that cold fusion was worth investing in by a 19 April 2009 interview with physicist Robert Duncan on the US news-show "60 minutes", made a grant of $5.5 million to the University of Missouri to establish the Sidney Kimmel Institute for Nuclear Renaissance (SKINR). The grant was intended to support research into the interactions of hydrogen with palladium, nickel or platinum at extreme conditions. In March 2013 Graham K. Hubler, a nuclear physicist who worked for the Naval Research Laboratory for 40 years, was named director. One of the SKINR projects is to replicate a 1991 experiment in which Prelas says bursts of millions of neutrons a second were recorded, which was stopped because "his research account had been frozen". He claims that the new experiment has already seen "neutron emissions at similar levels to the 1991 observation".
Italy.
Since the Fleischmann and Pons announcement, the Italian National agency for new technologies, Energy and sustainable economic development (ENEA) has funded Franco Scaramuzzi's research into whether excess heat can be measured from metals loaded with deuterium gas. Such research is distributed across ENEA departments, CNR Laboratories, INFN, universities and industrial laboratories in Italy, where the group continues to try to achieve reliable reproducibility (i.e. getting the phenomena to happen in every cell, and inside a certain frame of time). In 2006–2007, the ENEA founded a research program which claimed to have found excess power up to 500%, and in 2009 ENEA hosted the 15th cold fusion conference.
Japan.
Between 1992 and 1997, Japan's Ministry of International Trade and Industry sponsored a "New Hydrogen Energy (NHE)" program of US$20 million to research cold fusion. Announcing the end of the program in 1997, the director and one-time proponent of cold fusion research Hideo Ikegami stated "We couldn't achieve what was first claimed in terms of cold fusion. (...) We can't find any reason to propose more money for the coming year or for the future." In 1999 the Japan C-F Research Society was established to promote the independent research into cold fusion that continued in Japan. The society holds annual meetings. Perhaps the most famous Japanese cold fusion researcher is Yoshiaki Arata, from Osaka University, who claimed in a demonstration to produce excess heat when deuterium gas was introduced into a cell containing a mixture of palladium and zirconium oxide, a claim supported by fellow Japanese researcher Akira Kitamura of Kobe University and McKubre at SRI.
India.
In the 1990s, India stopped its research in cold fusion at the Bhabha Atomic Research Centre because of the lack of consensus among mainstream scientists and the US denunciation of the research. Yet, in 2008, the National Institute of Advanced Studies recommended the Indian government to revive this research. Projects were commenced at the Chennai's Indian Institute of Technology, the Bhabha Atomic Research Centre and the Indira Gandhi Centre for Atomic Research. However, there is still skepticism among scientists and, for all practical purposes, research is still stopped.
Publications.
The ISI identified cold fusion as the scientific topic with the largest number of published papers in 1989, of all scientific disciplines. The Nobel Laureate Julian Schwinger declared himself a supporter of cold fusion in the fall of 1989, after much of the response to the initial reports had turned negative. He tried to publish his theoretical paper "Cold Fusion: A Hypothesis" in "Physical Review Letters", but the peer reviewers rejected it so harshly that he felt deeply insulted, and he resigned from the American Physical Society (publisher of "PRL") in protest.
The number of papers sharply declined after 1990 because of two simultaneous phenomena: scientists abandoning the field and journal editors declining to review new papers, and cold fusion fell off the ISI charts. Researchers who got negative results abandoned the field, while others kept publishing. A 1993 paper in "Physics Letters A" was the last paper published by Fleischmann, and "one of the last reports to be formally challenged on technical grounds by a cold fusion skeptic".
The "Journal of Fusion Technology" (FT) established a permanent feature in 1990 for cold fusion papers, publishing over a dozen papers per year and giving a mainstream outlet for cold fusion researchers. When editor-in-chief George H. Miley retired in 2001, the journal stopped accepting new cold fusion papers. This has been cited as an example of the importance of sympathetic influential individuals to the publication of cold fusion papers in certain journals.
The decline of publications in cold fusion has been described as a "failed information epidemic". The sudden surge of supporters until roughly 50% of scientists support the theory, followed by a decline until there is only a very small number of supporters, has been described as a characteristic of pathological science. The lack of a shared set of unifying concepts and techniques has prevented the creation of a dense network of collaboration in the field; researchers perform efforts in their own and in disparate directions, making the transition to "normal" science more difficult.
Cold fusion reports continued to be published in a small cluster of specialized journals like "Journal of Electroanalytical Chemistry" and "Il Nuovo Cimento". Some papers also appeared in "Journal of Physical Chemistry", "Physics Letters A", "International Journal of Hydrogen Energy", and a number of Japanese and Russian journals of physics, chemistry, and engineering. Since 2005, "Naturwissenschaften" has published cold fusion papers; in 2009, the journal named a cold fusion researcher to its editorial board.
In the 1990s, the groups that continued to research cold fusion and their supporters established (non-peer-reviewed) periodicals such as "Fusion Facts", "Cold Fusion Magazine", "Infinite Energy Magazine" and "New Energy Times" to cover developments in cold fusion and other fringe claims in energy production that were ignored in other venues. The internet has also become a major means of communication and self-publication for CF researchers.
Conferences.
Cold fusion researchers were for many years unable to get papers accepted at scientific meetings, prompting the creation of their own conferences. The first International Conference on Cold Fusion (ICCF) was held in 1990, and has met every 12 to 18 months since. Attendees at some of the early conferences were described as offering no criticism to papers and presentations for fear of giving ammunition to external critics; thus allowing the proliferation of crackpots and hampering the conduct of serious science. Critics and skeptics stopped attending these conferences, with the notable exception of Douglas Morrison, who died in 2001. With the founding in 2004 of the International Society for Condensed Matter Nuclear Science (ISCMNS), the conference was renamed the International Conference on Condensed Matter Nuclear Science (the reasons are explained in the subsequent research section), but reverted to the old name in 2008. Cold fusion research is often referenced by proponents as "low-energy nuclear reactions", or LENR, but according to sociologist Bart Simon the "cold fusion" label continues to serve a social function in creating a collective identity for the field.
Since 2006, the American Physical Society (APS) has included cold fusion sessions at their semiannual meetings, clarifying that this does not imply a softening of skepticism. Since 2007, the American Chemical Society (ACS) meetings also include "invited symposium(s)" on cold fusion. An ACS program chair said that without a proper forum the matter would never be discussed and, "with the world facing an energy crisis, it is worth exploring all possibilities."
On 22–25 March 2009, the American Chemical Society meeting included a four-day symposium in conjunction with the 20th anniversary of the announcement of cold fusion. Researchers working at the U.S. Navy's Space and Naval Warfare Systems Center (SPAWAR) reported detection of energetic neutrons using a heavy water electrolysis set-up and a CR-39 detector, a result previously published in "Naturwissenschaften". The authors claim that these neutrons are indicative of nuclear reactions; without quantitative analysis of the number, energy, and timing of the neutrons and exclusion of other potential sources, this interpretation is unlikely to find acceptance by the wider scientific community.
Reported results.
A cold fusion experiment usually includes:
Electrolysis cells can be either open cell or closed cell. In open cell systems, the electrolysis products, which are gaseous, are allowed to leave the cell. In closed cell experiments, the products are captured, for example by catalytically recombining the products in a separate part of the experimental system. These experiments generally strive for a steady state condition, with the electrolyte being replaced periodically. There are also "heat after death" experiments, where the evolution of heat is monitored after the electric current is turned off.
The most basic setup of a cold fusion cell consists of two electrodes submerged in a solution containing palladium and heavy water. The electrodes are then connected to a power source to transmit electricity from one electrode to the other through the solution. Even when anomalous heat is reported, it can take weeks for it to begin to appear – this is known as the "loading time," the time required to saturate the palladium electrode with hydrogen (see "Loading ratio" section).
The Fleischmann and Pons early findings regarding helium, neutron radiation and tritium were never replicated satisfactorily, and its levels were too low for the claimed heat production and inconsistent with each other. Neutron radiation has been reported in cold fusion experiments at very low levels using different kinds of detectors, but levels were too low, close to background, and found too infrequently to provide useful information about possible nuclear processes.
Excess heat and energy production.
An excess heat observation is based on an energy balance. Various sources of energy input and output are continuously measured. Under normal conditions, the energy input can be matched to the energy output to within experimental error. In experiments such as those run by Fleischmann and Pons, a cell operating steadily at one temperature transitions to operating at a higher temperature with no increase in applied current. If the higher temperatures were real, and not an experimental artifact, the energy balance would show an unaccounted term. In the Fleischmann and Pons experiments, the rate of inferred excess heat generation was in the range of 10–20% of total input, though this could not be reliably replicated by most researchers. Researcher Nathan Lewis discovered that the excess heat in Fleischmann and Pons's original paper was not measured, but estimated from measurements that didn't have any excess heat.
Unable to produce excess heat or neutrons, and with positive experiments being plagued by errors and giving disparate results, most researchers declared that heat production was not a real effect and ceased working on the experiments.
In 1993, after the initial discrediting, Fleischmann reported "heat-after-death" experiments: where excess heat was measured after the electric current supplied to the electrolytic cell was turned off. This type of report also became part of subsequent cold fusion claims.
Helium, heavy elements, and neutrons.
Known instances of nuclear reactions, aside from producing energy, also produce nucleons and particles on readily observable ballistic trajectories. In support of their claim that nuclear reactions took place in their electrolytic cells, Fleischmann and Pons reported a neutron flux of 4,000 neutrons per second, as well as detections of tritium. The classical branching ratio for previously known fusion reactions that produce tritium would predict, with 1 watt of power, the production of 1012 neutrons per second, levels that would have been fatal to the researchers. In 2009, Mosier-Boss et al. reported what they called the first scientific report of highly energetic neutrons, using CR-39 plastic radiation detectors, but the claims cannot be validated without a quantitative analysis of neutrons.
Several medium and heavy elements like calcium, titanium, chromium, manganese, iron, cobalt, copper and zinc have been reported as detected by several researchers, like Tadahiko Mizuno or George Miley. The report presented to the DOE in 2004 indicated that deuterium-loaded foils could be used to detect fusion reaction products and, although the reviewers found the evidence presented to them as inconclusive, they indicated that those experiments did not use state-of-the-art techniques.
In response to skepticism about the lack of nuclear products, cold fusion researchers have tried to capture and measure nuclear products correlated with excess heat. Considerable attention has been given to measuring 4He production. However, the reported levels are very near to background, so contamination by trace amounts of helium normally present in the air cannot be ruled out. In the report presented to the DOE in 2004, the reviewers' opinion was divided on the evidence for 4He; with the most negative reviews concluding that although the amounts detected were above background levels, they were very close to them and therefore could be caused by contamination from air.
One of the main criticisms of cold fusion was that deuteron-deuteron fusion into helium was expected to result in the production of gamma rays—which were not observed and were not observed in subsequent cold fusion experiments. Cold fusion researchers have since claimed to find X-rays, helium, neutrons and even nuclear transmutations. Some of them even claim to have found them using only light water and nickel cathodes. The 2004 DOE panel expressed concerns about the poor quality of the theoretical framework cold fusion proponents presented to account for the lack of gamma rays.
Proposed mechanisms.
Many years after the 1989 experiment, cold fusion researchers still haven't agreed on a single theoretical explanation or on a single experimental method that can produce replicable results and continue to offer new proposals, which haven't convinced mainstream scientists.
Hydrogen and its isotopes can be absorbed in certain solids, including palladium hydride, at high densities. This creates a high partial pressure, reducing the average separation of hydrogen isotopes, but nowhere near enough to create the fusion rates claimed in the original experiment. It was proposed that a higher density of hydrogen inside the palladium and a lower potential barrier could raise the possibility of fusion at lower temperatures than expected from a simple application of Coulomb's law. Electron screening of the positive hydrogen nuclei by the negative electrons in the palladium lattice was suggested to the 2004 DOE commission, but the panel found the theoretical explanations (Charge Element 2) to be the weakest part of cold fusion claims.
Researchers started proposing alternative explanations for Fleischmann and Pons' results even before various other labs reported null results.
Skeptics have called cold fusion explanations "ad hoc" and lacking rigor, and state that they are used by proponents simply to disregard the negative experiments—a symptom of pathological science.
Criticism.
Criticism of cold fusion claims generally take one of two forms: either pointing out the theoretical implausibility of the claims that fusion reactions have occurred in electrolysis set-ups or criticizing the excess heat measurements themselves as being spurious, erroneous, or due to poor methodology or controls.
Incompatibilities with known fusion reactions.
There are many reasons why known fusion reactions are unlikely explanations for the excess heat and associated claims described above.
Repulsion forces.
Because nuclei are all positively charged, they strongly repel one another. Normally, in the absence of a catalyst such as a muon, very high kinetic energies are required to overcome this repulsion. Extrapolating from known fusion rates, the rate for uncatalyzed fusion at room-temperature energy would be 50 orders of magnitude lower than needed to account for the reported excess heat.
In muon-catalyzed fusion there are more fusions because the presence of the muon causes deuterium nuclei to be 207 times closer than in ordinary deuterium gas. But deuterium nuclei inside a palladium lattice are further apart than in deuterium gas, and there should be fewer fusion reactions, not more.
Paneth and Peters in the 1920s already knew that palladium can absorb up to 900 times its own volume of hydrogen gas, storing it at several thousands of times the atmospheric pressure. This led them to believe that they could increase the nuclear fusion rate by simply loading palladium rods with hydrogen gas. Tandberg then tried the same experiment but used electrolysis to make palladium absorb more deuterium and force the deuterium further together inside the rods, thus anticipating the main elements of Fleischmann and Pons' experiment. They all hoped that pairs of hydrogen nuclei would fuse together to form helium nuclei, which at the time were very needed in Germany to fill zeppelins, but no evidence of helium or of increased fusion rate was ever found.
This was also the belief of geologist Palmer, who convinced Steve Jones that the helium-3 occurring naturally in Earth came from the fusion of deuterium inside catalysts like palladium. This led Jones to independently make the same experimental setup as Fleischmann and Pons (a palladium cathode submerged in heavy water, absorbing deuterium via electrolysis). Fleischmann and Pons had the same incorrect belief, but they calculated the pressure to be of 1027 atmospheres, when CF experiments only achieve a ratio of one to one, which only has between 10,000 and 20,000 atmospheres. John R. Huizenga says they had misinterpreted the Nernst equation, leading them to believe that there was enough pressure to bring deuterons so close to each other that there would be spontaneous fusions.
Lack of expected reaction products.
Conventional deuteron fusion is a two-step process, in which an unstable high energy intermediary is formed:
Experiments have observed only three decay pathways for this excited-state nucleus, with the branching ratio showing the probability that any given intermediate follows a particular pathway. The products formed via these decay pathways are:
Only about one in one million of the intermediaries decay along the third pathway, making its products comparatively rare when compared to the other paths. This result is consistent with the predictions of the Bohr model. If one watt (1 eV = 1.602 x 10−19 joule) of nuclear power were produced from deuteron fusion consistent with known branching ratios, the resulting neutron and tritium (3H) production would be easily measured. Some researchers reported detecting 4He but without the expected neutron or tritium production; such a result would require branching ratios strongly favouring the third pathway, with the actual rates of the first two pathways lower by at least five orders of magnitude than observations from other experiments, directly contradicting both theoretically predicted and observed branching probabilities. Those reports of 4He production did not include detection of gamma rays, which would require the third pathway to have been changed somehow so that gamma rays are no longer emitted.
The known rate of the decay process together with the inter-atomic spacing in a metallic crystal makes heat transfer of the 24 MeV excess energy into the host metal lattice prior to the intermediary's decay inexplicable in terms of conventional understandings of momentum and energy transfer, and even then we would see measurable levels of radiation. Also, experiments indicate that the ratios of deuterium fusion remain constant at different energies. In general, pressure and chemical environment only cause small changes to fusion ratios. An early explanation invoked the Oppenheimer–Phillips process at low energies, but its magnitude was too small to explain the altered ratios.
Setup of experiments.
Cold fusion setups utilize an input power source (to ostensibly provide activation energy), a platinum group electrode, a deuterium or hydrogen source, a calorimeter, and, at times, detectors to look for byproducts such as helium or neutrons. Critics have variously taken issue with each of these aspects and further assert that there has not yet been a consistent reproduction of claimed cold fusion results in either energy output or byproducts. Some cold fusion researchers who claim that they can consistently measure an excess heat effect have argued that the apparent lack of reproducibility might be attributable to a lack of quality control in the electrode metal or the amount of hydrogen or deuterium loaded in the system. Skeptics have further criticized what they describe as mistakes or errors of interpretation that cold fusion researchers have made in certain calorimetry analyses and energy budgets.
Reproducibility.
In 1989, after Fleischmann and Pons had made their claims, many research groups tried to reproduce the Fleischmann-Pons experiment, without success. A few other research groups however reported successful reproductions of cold fusion during this time. In July 1989 an Indian group of BARC (P. K. Iyengar and M. Srinivasan) and in October 1989 a team from USA (Bockris et al.) reported on creation of tritium. In December 1990 Professor Richard Oriani of Minnesota University reported excess heat.
Groups that did report successes found that some of their cells were producing the effect where other cells that were built exactly the same and used the same materials were not producing the effect. Researchers that continued to work on the topic have claimed that over the years many successful replications have been made, but still have problems getting reliable replications. Reproducibility is one of the main principles of the scientific method, and its lack led most physicists to believe that the few positive reports could be attributed to experimental error. The DOE 2004 report said among its conclusions and recommendations:
As David Goodstein explains, proponents say that the positive results with excess heat and neutron emission are enough to prove that the phenomenon was real, that negative results didn't count because they could be caused by flaws in the setup, and that you can't prove an idea false by simply having a negative replication. This is a reversal of Karl Popper's falsifiability, which says that you can't prove ideas true, never mind how many times your experiment is successful, and that a single negative experiment can prove your idea wrong. Most scientists follow Popper's idea of falsifiability and discarded cold fusion as soon as they weren't able to replicate the effect in their own laboratory. Goodstein notes that he was impressed by a "particularly elegant, well designed experiment" and warns that by ignoring such results "science is not functioning normally." 
Loading ratio.
Cold fusion researchers (McKubre since 1994, ENEA in 2011) have posited that a cell that was loaded with a deuterium/palladium ratio lower than 100% (or 1:1) would never produce excess heat. Storms added in 1996 that the load ratio has to be maintained during many hours of electrolysis before the effects appear. Since most of the negative replications in 1989–1990 didn't report their ratios, this has been proposed as an explanation for failed replications. This loading ratio is tricky to obtain, and some batches of palladium never reach it because the pressure causes cracks in the palladium, allowing the deuterium to escape. Unfortunately, Fleischmann and Pons never disclosed the deuterium/palladium ratio achieved in their cells, there are no longer any batches of the palladium used by Fleischmann and Pons (because the supplier uses now a different manufacturing process), and researchers still have problems finding batches of palladium that achieve heat production reliably.
Misinterpretation of data.
Some research groups initially reported that they had replicated the Fleischmann and Pons results but later retracted their reports and offered an alternative explanation for their original positive results. A group at Georgia Tech found problems with their neutron detector, and Texas A&M discovered bad wiring in their thermometers. These retractions, combined with negative results from some famous laboratories, led most scientists to conclude, as early as 1989, that no positive result should be attributed to cold fusion.
Calorimetry errors.
The calculation of excess heat in electrochemical cells involves certain assumptions. Errors in these assumptions have been offered as non-nuclear explanations for excess heat.
One assumption made by Fleischmann and Pons is that the efficiency of electrolysis is nearly 100%, meaning nearly all the electricity applied to the cell resulted in electrolysis of water, with negligible resistive heating and substantially all the electrolysis product leaving the cell unchanged. This assumption gives the amount of energy expended converting liquid D2O into gaseous D2 and O2. The efficiency of electrolysis is less than one if hydrogen and oxygen recombine to a significant extent within the calorimeter. Several researchers have described potential mechanisms by which this process could occur and thereby account for excess heat in electrolysis experiments.
Another assumption is that heat loss from the calorimeter maintains the same relationship with measured temperature as found when calibrating the calorimeter. This assumption ceases to be accurate if the temperature distribution within the cell becomes significantly altered from the condition under which calibration measurements were made. This can happen, for example, if fluid circulation within the cell becomes significantly altered. Recombination of hydrogen and oxygen within the calorimeter would also alter the heat distribution and invalidate the calibration.
According to John R. Huizenga, who co-chaired the DOE 1989 panel, if unexplained excess heat is not accompanied by a commensurate amount of nuclear products, then it must not be interpreted as nuclear in origin, but as a measuring error.
Initial lack of control experiments.
Control experiments are part of the scientific method to prove that the measured effects do not happen by chance, but are direct results of the experiment. One of the points of criticism of Fleischmann and Pons was the lack of control experiments.
Patents.
Although details have not surfaced, it appears that the University of Utah forced the 23 March 1989 Fleischmann and Pons announcement to establish priority over the discovery and its patents before the joint publication with Jones. The Massachusetts Institute of Technology (MIT) announced on 12 April 1989 that it had applied for its own patents based on theoretical work of one of its researchers, Peter L. Hagelstein, who had been sending papers to journals from the 5th to the 12th of April. On 2 December 1993 the University of Utah licensed all its cold fusion patents to ENECO, a new company created to profit from cold fusion discoveries, and on March 1998 it said that it would no longer defend its patents.
The U.S. Patent and Trademark Office (USPTO) now rejects patents claiming cold fusion. Esther Kepplinger, the deputy commissioner of patents in 2004, said that this was done using the same argument as with perpetual motion machines: that they do not work. Patent applications are required to show that the invention is "useful", and this utility is dependent on the invention's ability to function. In general USPTO rejections on the sole grounds of the invention's being "inoperative" are rare, since such rejections need to demonstrate "proof of total incapacity", and cases where those rejections are upheld in a Federal Court are even rarer: nevertheless, in 2000, a rejection of a cold fusion patent was appealed in a Federal Court and it was upheld, in part on the grounds that the inventor was unable to establish the utility of the invention.
A U.S. patent might still be granted when given a different name to disassociate it from cold fusion, though this strategy has had little success in the US: the same claims that need to be patented can identify it with cold fusion, and most of these patents cannot avoid mentioning Fleischmann and Pons' research due to legal constraints, thus alerting the patent reviewer that it is a cold-fusion-related patent. David Voss said in 1999 that some patents that closely resemble cold fusion processes, and that use materials used in cold fusion, have been granted by the USPTO. The inventor of three such patents had his applications initially rejected when they were reviewed by experts in nuclear science; but then he rewrote the patents to focus more in the electrochemical parts so they would be reviewed instead by experts in electrochemistry, who approved them. When asked about the resemblance to cold fusion, the patent holder said that it used nuclear processes involving "new nuclear physics" unrelated to cold fusion. Melvin Miles was granted in 2004 a patent for a cold fusion device, and in 2007 he described his efforts to remove all instances of "cold fusion" from the patent description to avoid having it rejected outright.
At least one patent related to cold fusion has been granted by the European Patent Office.
A patent only legally prevents others from using or benefiting from one's invention. However, the general public perceives a patent as a stamp of approval, and a holder of three cold fusion patents said the patents were very valuable and had helped in getting investments.
Cultural references.
In "Undead Science", sociologist Bart Simon gives some examples of cold fusion in popular culture, saying that some scientists use cold fusion as a synonym for outrageous claims made with no supporting proof, and courses of ethics in science give it as an example of pathological science. It has appeared as a joke in "Murphy Brown" and "The Simpsons". It was adopted as a product name by software Coldfusion and a brand of protein bars (Cold Fusion Foods). It has also appeared in advertising as a synonym for impossible science, for example a 1995 advertisement for Pepsi Max.
The plot of "The Saint", a 1997 action-adventure film, parallels the story of Fleischmann and Pons, although with a different ending. The film might have affected the public perception of cold fusion, pushing it further into the science fiction realm.
"Final Exam", the 16th episode of season 4 of "The Outer Limits" depicts a student named Todtman who has invented a Cold Fusion weapon, and attempts to use it as a tool for revenge on people who have wronged him over the years. Despite the secret being lost with his death at the end of the episode, it is implied that another student elsewhere is on a similar track, and may well repeat Todtman's efforts.

</doc>
<doc id="7466" url="http://en.wikipedia.org/wiki?curid=7466" title="Coal tar">
Coal tar

Coal tar is a brown or black liquid of extremely high viscosity. Coal tar is among the by-products when coal is 
carbonized to make coke or gasified to make coal gas. Coal tars are complex and variable mixtures of phenols, polycyclic aromatic hydrocarbons (PAHs), and heterocyclic compounds.
Applications.
Pavement sealcoat.
Coal tar is incorporated into some parking-lot sealcoat products, which are used to protect and beautify the underlying pavement. Sealcoat products that are coal-tar based typically contain 20 to 35 percent coal-tar pitch. Substantial concerns have been raised about the safety of this application of coal tar, given that coal tar is known to cause cancer in humans and that several PAH compounds in coal tar are toxic to aquatic life.
The primary use of coal tar based sealcoats is regional within the US but federal research shows it is used in states from Alaska to Florida and several areas have banned its use in sealcoat products
 including: The District of Columbia; the City of Austin, Texas; Dane County, Wisconsin; Washington State; and several municipalities in Minnesota and others.
Industrial.
Being flammable, coal tar is sometimes used for heating or to fire boilers. Like most heavy oils, it must be heated before it will flow easily.
Coal tar was a component of the first sealed roads. In its original development by Edgar Purnell Hooley, tarmac was tar covered with granite chips. Later the filler used was industrial slag. Today, petroleum derived binders and sealers are more commonly used. These sealers are used to extend the life and reduce maintenance cost associated with asphalt pavements, primarily in asphalt road paving, car parks and walkways.
A large part of the binders used in the graphite industry for making "green blocks" are coke oven volatiles (COV). A considerable portion of these COV used as binders is coal tar. During the baking process of the green blocks as a part of commercial graphite production, most of the coal tar binders are vaporised and are generally burned in an incinerator to prevent release into the atmosphere, as COV and coal tar can be injurious to health.
Coal tar is also used to manufacture paints, synthetic dyes, and photographic materials.
Medical.
Also known as liquor carbonis detergens (LCD), and "liquor picis carbonis" (LPC) BP it can be used in medicated shampoo, soap and ointment, as a treatment for dandruff and psoriasis, as well as being used to kill and repel head lice. When used as a medication in the U.S., coal tar preparations are considered over-the-counter drug pharmaceuticals and are subject to regulation by the USFDA. Named brands include Denorex, Balnetar, Psoriasin, Tegrin, T/Gel, and Neutar. When used in the extemporaneous preparation of topical medications, it is supplied in the form of coal tar topical solution USP, which consists of a 20% w/v solution of coal tar in alcohol, with an additional 5% w/v of polysorbate 80 USP; this must then be diluted in an ointment base such as petrolatum.
Various phenolic coal tar derivatives have analgesic (pain-killer) properties. These included acetanilide, phenacetin, and paracetamol (acetaminophen). Paracetamol is the only coal-tar derived analgesic still in use today, but industrial phenol is now usually synthesized from crude oil rather than coal tar.
Safety.
According to the International Agency for Research on Cancer, preparations that include more than five percent of crude coal tar are Group 1 carcinogens.
According to the National Psoriasis Foundation and the FDA, coal tar is a valuable, safe and inexpensive treatment option for millions of people with psoriasis and other scalp or skin conditions. Coal tar concentrations between 0.5% and 5% are safe and effective for psoriasis, and no scientific evidence suggests that the coal tar in the concentrations seen in non-prescription treatments is (or is not) carcinogenic because there are too few studies and insufficient data to make a judgement. Coal tar contains approximately 10,000 chemicals, of which only about 50% have been identified, and the composition of coal tar varies with its origin and type of coal (for example,: lignite, bituminous or anthracite) used to make it.
Coal tar causes increased sensitivity to sunlight, so skin treated with topical coal tar preparations should be protected from sunlight.
The residue from the distillation of high-temperature coal tar, primarily a complex mixture of three or more membered condensed ring aromatic hydrocarbons, was listed on 28 October 2008 as a substance of very high concern by the European Chemicals Agency.
Coal tar distillers.
In the coal gas era, there were many companies in Britain whose business was to distill coal tar to separate the higher-value fractions, such as naphtha, creosote and pitch. These companies included:

</doc>
<doc id="7467" url="http://en.wikipedia.org/wiki?curid=7467" title="Cobbler">
Cobbler

Cobbler(s) may refer to:

</doc>
<doc id="7471" url="http://en.wikipedia.org/wiki?curid=7471" title="Catherine of Siena">
Catherine of Siena

Saint Catherine of Siena, T.O.S.D. (25 March 1347 in Siena – 29 April 1380 in Rome), was a tertiary of the Dominican Order and a Scholastic philosopher and theologian. She also worked to bring the papacy of Gregory XI back to Rome from its displacement in France and to establish peace among the Italian city-states. Since 18 June 1866, she is one of the two patron saints of Italy, together with St. Francis of Assisi. On 3 October 1970, she was proclaimed a Doctor of the Church by Pope Paul VI, and, on 1 October 1999, Pope John Paul II named her as a one of the six patron saints of Europe, together with Benedict of Nursia, Saints Cyril and Methodius, Bridget of Sweden and Edith Stein.
Life.
Caterina di Giacomo di Benincasa was born on 25 March 1347 in black death-ravaged Siena, Italy, to Giacomo di Benincasa, a cloth dyer who ran his enterprise with the help of his sons, and Lapa Piagenti, possibly the daughter of a local poet. The house where Catherine grew up is still in existence. Lapa was about forty years old when she prematurely gave birth to twin daughters, Catherine and Giovanna. She had already borne 22 children, but half of them had died. Giovanna was handed over to a wet-nurse, and presently died, whereas Catherine was nursed by her mother, and developed into a healthy child. She was two years old when Lapa had her 25th child, another daughter named Giovanna. As a child Catherine was so merry that the family gave her the pet name of "Euphrosyne", which is Greek for Joy and also the name of an early Christian saint.
Catherine is said by her confessor and biographer Raymond of Capua O.P.,’s "Life" to have had her first vision of Christ when she was the age of five or six. With her brother she was on the way home from a visit to a married sister, and is said to have experienced a vision of Christ seated in glory with the Apostles Peter, Paul, and John. Raymond continues that at age seven, Catherine vowed to give her whole life to God.
Her older sister Bonaventura died in childbirth. While tormented with sorrow, sixteen-year-old Catherine was now faced with her parents' wish that she marry Bonaventura's widower. Absolutely opposed to this, she started a massive fast, something she had learnt from Bonaventura, whose husband had not been considerate in the least. Bonaventura had changed his attitude by refusing to eat until he showed better manners. This had taught Catherine the power of fasting. She disappointed her mother by cutting off her long hair as a protest against being overly encouraged to improve her appearance in order to attract a husband.
Catherine would later advise Raymond of Capua to do during times of trouble what she did now as a teenager: "Build a cell inside your mind, from which you can never flee." In this inner cell she made her father into a representation of Christ, her mother Lapa into the Blessed Virgin Mary, and her brothers into the apostles. Serving them humbly became an opportunity for spiritual growth. Catherine resisted the accepted course of marriage and motherhood, on the one hand, or a nun's veil, on the other. She chose to live an active and prayerful life outside a convent’s walls following the model of the Dominicans. Eventually her father gave up and permitted her to live as she pleased.
A vision of St. Dominic gave strength to Catherine, but her wish to join his Order was no comfort to Lapa, who took her daughter with her to the baths in Bagno Vignoni to improve her health. Catherine fell seriously ill with a violent rash, fever and pain, which conveniently made her mother accept her wish to join the "Mantellate", the local association of Dominican tertiaries. Lapa went to the Sisters of the Order and persuaded them to take in her daughter. Within days, Catherine seemed entirely restored, rose from bed and donned the black and white habit of the Third Order of St. Dominic. Catherine received the habit of a Dominican tertiary from the friars of the Order, however, only after vigorous protests from the Tertiaries themselves, who up to that point had been only widows. As a tertiary, she lived outside the convent, at home with her family like before. The Mantellate taught Catherine how to read, and she lived in almost total silence and solitude in the family home.
Her custom of giving away clothing and food without asking anyone's permission cost her family significantly but she demanded nothing for herself. By staying in their midst, she could live out her rejection of them more strongly. She did not want their food, referring to the table laid for her in Heaven with her real family. 
In about 1368, aged twenty-one, Catherine experienced what she described in her letters as a "Mystical Marriage" with Jesus, later a popular subject in art as the "Mystic marriage of Saint Catherine". "Underlining the extent to which the marriage was a fusion with Christ's physicality [...] Catherine received, not the ring of gold and jewels that her biographer reports in his bowdlerized version, but the ring of Christ's foreskin." Raymond also records that she was told by Christ to leave her withdrawn life and enter the public life of the world. So, Catherine rejoined her family, and began helping the ill and the poor, where she took care of them in hospitals or homes. Her early pious activities in Siena attracted a group of followers, both women and men, who gathered around her.
As social and political tensions mounted in Siena, Catherine found herself drawn to intervene in wider politics. She made her first journey to Florence in 1374, probably to be interviewed by the Dominican authorities at the General Chapter held in Florence in May 1374, though this is controverted (if she was interviewed, then the absence of later evidence suggests she was deemed sufficiently orthodox). It seems that at this time she acquired Raymond of Capua as her confessor and spiritual director.
After this visit, she began travelling with her followers throughout northern and central Italy advocating reform of the clergy and advising people that repentance and renewal could be done through "the total love for God." In Pisa, in 1375, she used what influence she had to sway that city and Lucca away from alliance with the anti-papal league whose force was gaining momentum and strength. She also lent her enthusiasm towards promoting the launch of a new crusade. It was in Pisa in 1375 that, according to Raymond of Capua's biography, she received the stigmata (visible, at Catherine's request, only to herself.
Physical travel was not the only way in which Catherine made her views known. From 1375 onwards, she began dictating letters to various scribes. These letters were intended to reach men and women of her circle, increasingly widening her audience to include figures in authority as she begged for peace between the republics and principalities of Italy and for the return of the Papacy from Avignon to Rome. She carried on a long correspondence with Pope Gregory XI, asking him to reform the clergy and the administration of the Papal States.
Towards the end of 1375, she returned to Siena, to assist a young political prisoner, Niccolò di Tuldo, at his execution. In June 1376 Catherine went to Avignon herself as ambassador of Florence to make peace with the Papal States (on 31 March 1376 Gregory XI had placed Florence under interdict). On this issue she was unsuccessful, and was in fact disowned by the Florentine leaders, who sent ambassadors to negotiate on their own terms as soon as Catherine's work had paved the way for them. Catherine sent an appropriately scorching letter back to Florence in response. While in Avignon, Catherine also tried to convince Pope Gregory XI to return to Rome. Gregory did indeed return his administration to Rome in January 1377; to what extent this was due to Catherine’s influence is a topic of much modern debate.
Catherine returned to Siena, and spent the early months of 1377 founding a women's monastery of strict observance outside the city in the old fortress of Belcaro. She spent the rest of 1377 at Rocca d'Orcia, about twenty miles from Siena, on a local mission of peace-making and preaching. During this period, in autumn 1377, she both had the experience which led to the writing of her "Dialogue", and learned to write herself, although she still seems to have chiefly relied upon her secretaries for her correspondence.
Late in 1377 or early in 1378 Catherine again travelled to Florence, at the order of Gregory XI, to seek peace between Florence and Rome. Following Gregory's death in March 1378 riots, the Ciompi, broke out in Florence on 18 June, and in the ensuing violence she was nearly assassinated. Eventually, in July 1378, peace was agreed between Florence and Rome; Catherine returned quietly to Florence.
In late November 1378, with the outbreak of the Western Schism, the new Pope, Urban VI, summoned her to Rome. She stayed at Pope Urban VI's court and tried to convince nobles and cardinals of his legitimacy, both meeting with individuals at court and writing letters to persuade others.
For many years she had accustomed herself to a rigorous abstinence. She received the Holy Communion virtually on a daily basis. This extreme fasting appeared unhealthy in the eyes of the clergy and her own sisterhood, and her confessor, Blessed Raymond, ordered her to eat properly. But Catherine claimed that she was unable to, describing her inability to eat as an "infermità" (illness). From the beginning of 1380, Catherine could neither eat nor swallow water. On February 26 she lost the use of her legs. St Catherine died in Rome, on 29 April 1380, at the age of thirty-three, having suffered a stroke eight days earlier.
Sources of her life.
There is some internal evidence of Catherine's personality, teaching and work in her nearly four hundred letters, her "Dialogue", and her prayers.
Much detail about her life has also, however, been drawn from the various sources written shortly after her death in order to promote her cult and canonisation. Though much of this material is heavily hagiographic, it has been an important source for historians seeking to reconstruct Catherine's life. Various sources are particularly important, especially the works of Raymond of Capua, who was Catherine's spiritual director and close friend from 1374 until her death, and himself became Master General of the Order in 1380. Raymond began writing what is known as the "Legenda Major", his "Life" of Catherine, in 1384, and completed it in 1395.
Another important work written after Catherine's death was "Libellus de Supplemento" ("Little Supplement Book"), written between 1412 and 1418 by Tommaso d'Antonio Nacci da Seine (commonly called Thomas of Siena, or Caffarini): the work is an expansion of Raymond's "Legenda Major" making heavy use of the notes of Catherine's first confessor, Tommaso della Fonte (notes that do not survive anywhere else). Caffarini later published a more compact account of Catherine's life, entitled the "Legenda Minor".
From 1411 onwards, Caffarini also co-ordinated the compiling of the "Processus" of Venice, the set of documents submitted as part of the process of canonisation of Catherine, which provides testimony from nearly all of Catherine's disciples. There is also an anonymous piece entitled "Miracoli della Beata Caterina" ("Miracle of Blessed Catherine"), written by an anonymous Florentine. A few other relevant pieces survive.
Works.
Three genres of work by Catherine survive:
Veneration.
She was buried in the cemetery of Santa Maria sopra Minerva which lies near the Pantheon. After miracles were reported to take place at her grave, Raymond moved her inside the Basilica of Santa Maria sopra Minerva, where she lies to this day.
Her head however, was parted from her body and inserted in a gilt bust of bronze. This bust was later taken to Siena, and carried through that city in a procession to the Dominican church. Behind the bust walked Lapa, Catherine's mother, who lived until she was 89 years old. By then she had seen the end of the wealth and the happiness of her family, and followed most of her children and several of her grandchildren to the grave. She helped Raymond of Capua write his biography of her daughter, and said, "I think God has laid my soul athwart in my body, so that it can't get out." The incorruptible head and thumb were entombed in the Basilica of San Domenico, where they remain.
Pope Pius II, himself from Siena, canonized St Catherine on 29 June 1461.
On 3 October 1970, Pope Paul VI gave Catherine the title of Doctor of the Church; this title was almost simultaneously given to Saint Teresa of Ávila (27 September 1970), making them the first women to receive this honour.
Initially, however, her feast day was not included in the General Roman Calendar. When it was added in 1597, it was put on the day of her death, April 29; however, because this conflicted with the feast of Saint Peter of Verona (also on 29 April), Catherine's feast day was moved in 1628 to the new date of April 30. In the 1969 revision of the calendar, it was decided to leave the celebration of the feast of St Peter of Verona to local calendars, because he was not as well known worldwide, and Saint Catherine's feast was restored to its traditional date of April 29.
Patronage.
In his decree of 13 April 1866, Pope Pius IX declared Catherine of Siena to be a co-patroness of Rome. On 18 June 1939 Pope Pius XII named her a joint Patron Saint of Italy along with Saint Francis of Assisi.
On 1 October 1999, Pope John Paul II made her one of Europe's patron saints, along with Edith Stein and Bridget of Sweden. She is also the patroness of the historically Catholic American woman's fraternity, Theta Phi Alpha.
Iconography.
The people of Siena wished to have St. Catherine's body. A story is told of a miracle whereby they were partially successful: Knowing that they could not smuggle her whole body out of Rome, they decided to take only her head which they placed in a bag. When stopped by the Roman guards, they prayed to St Catherine to help them, confident that she would rather have her body (or at least part thereof) in Siena. When they opened the bag to show the guards, it appeared no longer to hold her head but to be full of rose petals. Once they got back to Siena they reopened the bag and her head was visible once more. Due to this story, St Catherine is often seen holding a rose.
Legacy.
Catherine ranks high among the mystics and spiritual writers of the Church. She remains a greatly respected figure for her spiritual writings, and political boldness to "speak truth to power"— it being exceptional for a woman, in her time period, to have had such influence in politics and on world history.
The St. Catherine of Siena Medical Center is located in Smithtown, Long Island,New York. Only the church and a memorial garden survive of St Catherine's Convent in Bow, London, whose members moved to Stone, Staffordshire in 1926.
Modern editions.
English translations of The "Dialogue" include:
The Letters are translated into English as:
The Prayers are translated into English as:
Raymond of Capua's "Life" is translated as:

</doc>
<doc id="7472" url="http://en.wikipedia.org/wiki?curid=7472" title="Charles Lyell">
Charles Lyell

Sir Charles Lyell, 1st Baronet, FRS (14 November 1797 – 22 February 1875) was a British lawyer and the foremost geologist of his day. He is best known as the author of "Principles of Geology", which popularised James Hutton's concepts of uniformitarianism – the idea that the Earth was shaped by the same processes still in operation today. "Principles of Geology" also challenged theories popularized by George Cuvier, which were the most accepted and circulated ideas about geology in England at the time. Lyell was also one of the first to believe that the world is older than 300 million years, on the basis of its geological anomalies. Lyell was a close and influential friend of Charles Darwin.
Biography.
Lyell was born in Scotland about 15 miles north of Dundee in Kinnordy, near Kirriemuir in Forfarshire (now in Angus). He was the eldest of ten children. Lyell's father, also named Charles, was a lawyer and botanist of minor repute: it was he who first exposed his son to the study of nature.
The house/place of his birth is located in the north-west of the Central Lowlands in the valley of the Highland Boundary Fault. Round the house, in the rift valley, is farmland, but within a short distance to the north-west, on the other side of the fault, are the Grampian Mountains in the Highlands. His family's second home was in a completely different geological and ecological area: he spent much of his childhood at Bartley Lodge in the New Forest, England.
Lyell entered Exeter College, Oxford in 1816, and attended William Buckland's lectures. He graduated BA second class in classics, December 1819, and M.A. 1821.
After graduation he took up law as a profession, entering Lincoln's Inn in 1820. He completed a circuit through rural England, where he could observe geological phenomena. In 1821 he attended Robert Jameson's lectures in Edinburgh, and visited Gideon Mantell at Lewes, in Sussex. In 1823 he was elected joint secretary of the Geological Society. As his eyesight began to deteriorate, he turned to geology as a full-time profession. His first paper, "On a recent formation of freshwater limestone in Forfarshire", was presented in 1822. By 1827, he had abandoned law and embarked on a geological career that would result in fame and the general acceptance of uniformitarianism, a working out of the ideas proposed by James Hutton a few decades earlier.
In 1832, Lyell married Mary Horner in Bonn, daughter of Leonard Horner (1785–1864), also associated with the Geological Society of London. The new couple spent their honeymoon in Switzerland and Italy on a geological tour of the area.
During the 1840s, Lyell travelled to the United States and Canada, and wrote two popular travel-and-geology books: "Travels in North America" (1845) and "A Second Visit to the United States" (1849). After the Great Chicago Fire, Lyell was one of the first to donate books to help found the Chicago Public Library. In 1866, he was elected a foreign member of the Royal Swedish Academy of Sciences.
Lyell's wife died in 1873, and two years later (in 1875) Lyell himself died as he was revising the twelfth edition of "Principles". He is buried in Westminster Abbey. Lyell was knighted (Kt) in 1848, and later, in 1864, made a baronet (Bt), which is an hereditary honour. He was awarded the Copley Medal of the Royal Society in 1858 and the Wollaston Medal of the Geological Society in 1866. Mount Lyell, the highest peak in Yosemite National Park, is named after him; the crater Lyell on the Moon and a crater on Mars were named in his honour; Mount Lyell in western Tasmania, Australia, located in a profitable mining area, bears Lyell's name; and the Lyell Range in north-west Western Australia is named for him as well. The jawless fish "Cephalaspis lyelli", from the Old Red Sandstone of southern Scotland, was named by Louis Agassiz in honour of Lyell.
Career and major writings.
Lyell had private means, and earned further income as an author. He came from a prosperous family, worked briefly as a lawyer in the 1820s, and held the post of Professor of Geology at King's College London in the 1830s. From 1830 onward his books provided both income and fame. Each of his three major books was a work continually in progress. All three went through multiple editions during his lifetime, although many of his friends (such as Darwin) thought the first edition of the "Principles" was the best written. Lyell used each edition to incorporate additional material, rearrange existing material, and revisit old conclusions in light of new evidence.
"Principles of Geology", Lyell's first book, was also his most famous, most influential, and most important. First published in three volumes in 1830–33, it established Lyell's credentials as an important geological theorist and propounded the doctrine of uniformitarianism. It was a work of synthesis, backed by his own personal observations on his travels.
The central argument in "Principles" was that "the present is the key to the past" – a concept of the Scottish Enlightenment which David Hume had stated as "all inferences from experience suppose ... that the future will resemble the past", and James Hutton had described when he wrote in 1788 that "from what has actually been, we have data for concluding with regard to that which is to happen thereafter." Geological remains from the distant past can, and should, be explained by reference to geological processes now in operation and thus directly observable. Lyell's interpretation of geologic change as the steady accumulation of minute changes over enormously long spans of time was a powerful influence on the young Charles Darwin. Lyell asked Robert FitzRoy, captain of HMS "Beagle", to search for erratic boulders on the survey voyage of the "Beagle", and just before it set out FitzRoy gave Darwin Volume 1 of the first edition of Lyell's "Principles". When the "Beagle" made its first stop ashore at St Jago, Darwin found rock formations which seen "through Lyell's eyes" gave him a revolutionary insight into the geological history of the island, an insight he applied throughout his travels.
While in South America Darwin received Volume 2 which considered the ideas of Lamarck in some detail. Lyell rejected Lamarck's idea of organic evolution, proposing instead "Centres of Creation" to explain diversity and territory of species. However, as discussed below, many of his letters show he was fairly open to the idea of evolution. In geology Darwin was very much Lyell's disciple, and brought back observations and his own original theorising, including ideas about the formation of atolls, which supported Lyell's uniformitarianism. On the return of the "Beagle" (October 1836) Lyell invited Darwin to dinner and from then on they were close friends. Although Darwin discussed evolutionary ideas with him from 1842, Lyell continued to reject evolution in each of the first nine editions of the "Principles". He encouraged Darwin to publish, and following the 1859 publication of "On the Origin of Species", Lyell finally offered a tepid endorsement of evolution in the tenth edition of "Principles".
"Elements of Geology" began as the fourth volume of the third edition of "Principles": Lyell intended the book to act as a suitable field guide for students of geology. The systematic, factual description of geological formations of different ages contained in "Principles" grew so unwieldy, however, that Lyell split it off as the "Elements" in 1838. The book went through six editions, eventually growing to two volumes and ceasing to be the inexpensive, portable handbook that Lyell had originally envisioned. Late in his career, therefore, Lyell produced a condensed version titled "Student's Elements of Geology" that fulfilled the original purpose.
"Geological Evidences of the Antiquity of Man" brought together Lyell's views on three key themes from the geology of the Quaternary Period of Earth history: glaciers, evolution, and the age of the human race. First published in 1863, it went through three editions that year, with a fourth and final edition appearing in 1873. The book was widely regarded as a disappointment because of Lyell's equivocal treatment of evolution. Lyell, a devout Christian, had great difficulty reconciling his beliefs with natural selection.
Scientific contributions.
Lyell's geological interests ranged from volcanoes and geological dynamics through stratigraphy, palaeontology, and glaciology to topics that would now be classified as prehistoric archaeology and paleoanthropology. He is best known, however, for his role in popularising the doctrine of uniformitarianism.
Uniformitarianism.
From 1830 to 1833 his multi-volume "Principles of Geology" was published. The work's subtitle was "An attempt to explain the former changes of the Earth's surface by reference to causes now in operation", and this explains Lyell's impact on science. He drew his explanations from field studies conducted directly before he went to work on the founding geology text. He was, along with the earlier John Playfair, the major advocate of James Hutton's idea of uniformitarianism, that the earth was shaped entirely by slow-moving forces still in operation today, acting over a very long period of time. This was in contrast to catastrophism, a geologic idea of abrupt changes, which had been adapted in England to support belief in Noah's flood. Describing the importance of uniformitarianism on contemporary geology, Lyell wrote,
Never was there a doctrine more calculated to foster indolence, and to blunt the keen edge of curiosity, than this assumption of the discordance between the former and the existing causes of change... The student was taught to despond from the first. Geology, it was affirmed, could never arise to the rank of an exact science... [With catastrophism] we see the ancient spirit of speculation revived, and a desire manifestly shown to cut, rather than patiently untie, the Gordian Knot.
Lyell saw himself as "the spiritual saviour of geology, freeing the science from the old dispensation of Moses." The two terms, "uniformitarianism" and "catastrophism", were both coined by William Whewell; in 1866 R. Grove suggested the simpler term "continuity" for Lyell's view, but the old terms persisted. In various revised editions (12 in all, through 1872), "Principles of Geology" was the most influential geological work in the middle of the 19th century, and did much to put geology on a modern footing. For his efforts he was knighted in 1848, then made a baronet in 1864.
Geological Surveys.
Lyell noted the "economic advantages" that geological surveys could provide, citing their felicity in mineral-rich countries and provinces. Modern surveys, like the US Geological Survey, map and exhibit the natural resources within the country. So, in endorsing surveys, as well as advancing the study of geology, Lyell helped to forward the business of modern extractive industries, such as the coal and oil industry.
Volcanoes and geological dynamics.
Before the work of Lyell, phenomena such as earthquakes were understood by the destruction that they brought. One of the contributions that Lyell made in "Principles" was to explain the cause of earthquakes. Lyell, in contrast focused on recent earthquakes (150 yrs), evidenced by surface irregularities such as faults, fissures, stratigraphic displacements and depressions.
Lyell's work on volcanoes focused largely on Vesuvius and Etna, both of which he had earlier studied. His conclusions supported gradual building of volcanoes, so-called "backed up-building", as opposed to the upheaval argument supported by other geologists.
Stratigraphy.
Lyell's most important specific work was in the field of stratigraphy. From May 1828, until February 1829, he travelled with Roderick Impey Murchison (1792–1871) to the south of France (Auvergne volcanic district) and to Italy. In these areas he concluded that the recent strata (rock layers) could be categorised according to the number and proportion of marine shells encased within. Based on this he proposed dividing the Tertiary period into three parts, which he named the Pliocene, Miocene, and Eocene. He also renamed the traditional "Primary", "Secondary" and "Tertiary" periods (now called eras) to Paleozoic, Mesozoic and Cenozoic, which nomenclature was gradually accepted worldwide.
Glaciers.
In "Principles of Geology" (first edition, vol. 3, Ch. 2, 1833) Lyell proposed that icebergs could be the means of transport for erratics. During periods of global warming, ice breaks off the poles and floats across submerged continents, carrying debris with it, he conjectured. When the iceberg melts, it rains down sediments upon the land. Because this theory could account for the presence of diluvium, the word "drift" became the preferred term for the loose, unsorted material, today called "till". Furthermore, Lyell believed that the accumulation of fine angular particles covering much of the world (today called loess) was a deposit settled from mountain flood water. Today some of Lyell's mechanisms for geologic processes have been disproven, though many have stood the test of time. His observational methods and general analytical framework remain in use today as foundational principles in geology.
Evolution.
Lyell first received a copy of one of Lamarck's books from Mantell in 1827, when he was on circuit. He thanked Mantell in a letter which includes this enthusiastic passage:
In the second volume of the first edition of "Principles" Lyell explicitly rejected the "mechanism" of Lamark on the transmutation of species, and was doubtful whether species were mutable. However, privately, in letters, he was more open to the possibility of evolution:
This letter makes it clear that his equivocation on evolution was, at least at first, a deliberate tactic. As a result of his letters and, no doubt, personal conversations, Huxley and Haeckel were convinced that, at the time he wrote "Principles", he believed new species had arisen by natural methods. Both Whewell and Sedgwick wrote worried letters to him about this.
Later, Darwin became a close personal friend, and Lyell was one of the first scientists to support "On the Origin of Species", though he did not subscribe to all its contents. Lyell was also a friend of Darwin's closest colleagues, Hooker and Huxley, but unlike them he struggled to square his religious beliefs with evolution. This inner struggle has been much commented on. He had particular difficulty in believing in natural selection as the main motive force in evolution.
Lyell and Hooker were instrumental in arranging the peaceful co-publication of the theory of natural selection by Darwin and Alfred Russel Wallace in 1858: each had arrived at the theory independently. Lyell's data on stratigraphy were important because Darwin thought that populations of an organism changed slowly, requiring "geologic time".
Although Lyell did not publicly accept evolution (descent with modification) at the time of writing the "Principles", after the Darwin–Wallace papers and the "Origin" Lyell wrote in his notebook:
Lyell's acceptance of natural selection, Darwin's proposed mechanism for evolution, was equivocal, and came in the tenth edition of "Principles". "The Antiquity of Man" (published in early February 1863, just before Huxley's "Man's place in nature") drew these comments from Darwin to Huxley:
Quite strong remarks: no doubt Darwin resented Lyell's repeated suggestion that he owed a lot to Lamarck, whom he (Darwin) had always specifically rejected. Darwin's daughter Henrietta (Etty) wrote to her father: "Is it fair that Lyell always calls your theory a modification of Lamarck's?" 
In other respects "Antiquity" was a success. It sold well, and it "shattered the tacit agreement that mankind should be the sole preserve of theologians and historians". But when Lyell wrote that it remained a profound mystery how the huge gulf between man and beast could be bridged, Darwin wrote "Oh!" in the margin of his copy.
Legacy.
Places named after Lyell:

</doc>
<doc id="7473" url="http://en.wikipedia.org/wiki?curid=7473" title="Chelsea F.C.">
Chelsea F.C.

Chelsea Football Club are an English football club based in Fulham, London. Founded in 1905, the club play in the Premier League and have spent most of their history in the top tier of English football. The club's home ground is the 41,837-seat Stamford Bridge stadium, where they have played since their establishment.
Chelsea had their first major success in 1955, winning the league championship, and won various cup competitions during the 1960s, 1970s, 1990s and 2000s. The club have enjoyed their greatest period of success in the past two decades, winning 15 major trophies since 1997.
Domestically, Chelsea have won four league titles, seven FA Cups, four League Cups and four FA Community Shields, while in continental competitions they have won two UEFA Cup Winners' Cups, one UEFA Super Cup, one UEFA Europa League and one UEFA Champions League. Chelsea are the only London club to win the UEFA Champions League, and one of four clubs, and the only British club, to have won all three main UEFA club competitions.
Chelsea's regular kit colours are royal blue shirts and shorts with white socks. The club's crest has been changed several times in attempts to re-brand the club and modernise its image. The current crest, featuring a ceremonial lion rampant regardant holding a staff, is a modification of the one introduced in the early 1950s. The club have sustained the fifth highest average all-time attendance in English football. Their average home gate for the 2012–13 season was 41,462, the sixth highest in the Premier League.<ref name="12/13 attendances"></ref> Since July 2003, Chelsea have been owned by Russian billionaire Roman Abramovich. In April 2013 they were ranked by "Forbes Magazine" as the seventh most valuable football club in the world, at £588 million ($901 million), an increase of 18% from the previous year.
History.
In 1904, Gus Mears acquired the Stamford Bridge athletics stadium with the aim of turning it into a football ground. An offer to lease it to nearby Fulham was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like "Kensington FC", "Stamford Bridge FC" and "London FC" were also considered. Chelsea were founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook), opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards.
The club won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in their early years. They reached the 1915 FA Cup Final, where they lost to Sheffield United at Old Trafford, and finished 3rd in the First Division in 1920, the club's best league campaign to that point. Chelsea attracted large crowds and had a reputation for signing big-name players, but success continued to elude the club in the inter-war years.
Former Arsenal and England centre-forward Ted Drake became manager in 1952 and proceeded to modernise the club. He removed the club's Chelsea pensioner crest, improved the youth set-up and training regime, rebuilt the side with shrewd signings from the lower divisions and amateur leagues, and led Chelsea to their first major trophy success – the League championship – in 1954–55. The following season saw UEFA create the European Champions' Cup, but after objections from The Football League and the FA Chelsea were persuaded to withdraw from the competition before it started. Chelsea failed to build on this success, and spent the remainder of the 1950s in mid-table. Drake was dismissed in 1961 and replaced by player-coach Tommy Docherty.
Docherty built a new team around the group of talented young players emerging from the club's youth set-up and Chelsea challenged for honours throughout the 1960s, enduring several near-misses. They were on course for a treble of League, FA Cup and League Cup going into the final stages of the 1964–65 season, winning the League Cup but faltering late on in the other two. In three seasons the side were beaten in three major semi-finals and were FA Cup runners-up. Under Docherty's successor, Dave Sexton, Chelsea won the FA Cup in 1970, beating Leeds United 2–1 in a final replay. Chelsea took their first European honour, a UEFA Cup Winners' Cup triumph, the following year, with another replayed win, this time over Real Madrid in Athens.
The late 1970s through to the 1980s was a turbulent period for Chelsea. An ambitious redevelopment of Stamford Bridge threatened the financial stability of the club, star players were sold and the team were relegated. Further problems were caused by a notorious hooligan element among the support, which was to plague the club throughout the decade. In 1982, Chelsea were, at the nadir of their fortunes, acquired by Ken Bates for the nominal sum of £1, although by now the Stamford Bridge freehold had been sold to property developers, meaning the club faced losing their home. On the pitch, the team had fared little better, coming close to relegation to the Third Division for the first time, but in 1983 manager John Neal put together an impressive new team for minimal outlay. Chelsea won the Second Division title in 1983–84 and established themselves in the top division, before being relegated again in 1988. The club bounced back immediately by winning the Second Division championship in 1988–89.
After a long-running legal battle, Bates reunited the stadium freehold with the club in 1992 by doing a deal with the banks of the property developers, who had been bankrupted by a market crash. Chelsea's form in the new Premier League was unconvincing, although they did reach the 1994 FA Cup Final with Glenn Hoddle. It was not until the appointment of Ruud Gullit as player-manager in 1996 that their fortunes changed. He added several top international players to the side, as the club won the FA Cup in 1997 and established themselves as one of England's top sides again. Gullit was replaced by Gianluca Vialli, who led the team to victory in the League Cup Final, the UEFA Cup Winners' Cup Final and the UEFA Super Cup in 1998, the FA Cup in 2000 and their first appearance in the UEFA Champions League. Vialli was sacked in favour of Claudio Ranieri, who guided Chelsea to the 2002 FA Cup Final and Champions League qualification in 2002–03.
In June 2003, Bates sold Chelsea to Russian billionaire Roman Abramovich for £140 million. Over £100 million was spent on new players, but Ranieri was unable to deliver any trophies, and was replaced by José Mourinho. Under Mourinho, Chelsea became the fifth English team to win back-to-back league championships since the Second World War (2004–05 and 2005–06), in addition to winning an FA Cup (2007) and two League Cups (2005 and 2007). Mourinho was replaced by Avram Grant, who led the club to their first UEFA Champions League final, which they lost on penalties to Manchester United.
In 2009, Guus Hiddink guided Chelsea to another FA Cup success. In 2009–10, his successor Carlo Ancelotti led them to their first Premier League and FA Cup "Double", and becoming the first English top-flight club to score 100 league goals in a season since 1963. In 2012, caretaker manager Roberto Di Matteo led Chelsea to their seventh FA Cup, and their first UEFA Champions League title, beating Bayern Munich 4–3 on penalties, the first London club to win the trophy. Interim manager Rafael Benítez guided Chelsea to win the UEFA Europa League against Benfica, becoming the first club to hold two major European titles simultaneously and one of four clubs, and the only British club, to have won all three of UEFA's major club competitions.
Stadium.
Chelsea have only ever had one home ground, Stamford Bridge, where they have played since the team's foundation. It was officially opened on 28 April 1877 and for the first 28 years of its existence it was used almost exclusively by the London Athletic Club as an arena for athletics meetings and not at all for football. In 1904 the ground was acquired by businessman Gus Mears and his brother Joseph, who had also purchased nearby land (formerly a large market garden) with the aim of staging football matches on the now 12.5 acre (51,000 m²) site. Stamford Bridge was designed for the Mears family by the noted football architect Archibald Leitch, who had also designed Ibrox, Celtic Park and Hampden Park. Most football clubs were founded first, and then sought grounds in which to play, but Chelsea were founded for Stamford Bridge.
Starting with an open bowl-like design and one covered terrace, Stamford Bridge had an original capacity of around 100,000. The early 1930s saw the construction of a terrace on the southern part of the ground with a roof that covered around one fifth of the stand. It eventually became known as the "Shed End", the home of Chelsea's most loyal and vocal supporters, particularly during the 1960s, 70s and 80s. The exact origins of the name are unclear, but the fact that the roof looked like a corrugated iron shed roof played a part.
In the early 1970s the club's owners announced a modernisation of Stamford Bridge with plans for a state-of-the-art 50,000 all-seater stadium. Work began on the East Stand in 1972 but the project was beset with problems and was never completed; the cost brought the club close to bankruptcy, culminating in the freehold being sold to property developers. Following a long legal battle, it was not until the mid-1990s that Chelsea's future at the stadium was secured and renovation work resumed. The north, west and southern parts of the ground were converted into all-seater stands and moved closer to the pitch, a process completed by 2001.
When Stamford Bridge was redeveloped in the Ken Bates era many additional features were added to the complex including two hotels, apartments, bars, restaurants, the Chelsea Megastore, and an interactive visitor attraction called Chelsea World of Sport. The intention was that these facilities would provide extra revenue to support the football side of the business, but they were less successful than hoped and before the Abramovich takeover in 2003 the debt taken on to finance them was a major burden on the club. Soon after the takeover a decision was taken to drop the "Chelsea Village" brand and refocus on Chelsea as a football club. However, the stadium is sometimes still referred to as part of "Chelsea Village" or "The Village".
The Stamford Bridge freehold, the pitch, the turnstiles and Chelsea's naming rights are now owned by Chelsea Pitch Owners, a non-profit organisation in which fans are the shareholders. The CPO was created to ensure the stadium could never again be sold to developers. As a condition for using the Chelsea FC name, the club has to play its first team matches at Stamford Bridge, which means that if the club moves to a new stadium, they may have to change their name.
Chelsea's training ground is located in Cobham, Surrey. Chelsea moved to Cobham in 2004. Their previous training ground in Harlington was taken over by QPR in 2005. The new training facilities in Cobham were completed in 2007.
Stamford Bridge has been used for a variety of other sporting events since 1905. It hosted the FA Cup Final from 1920 to 1922, has held ten FA Cup semi-finals (most recently in 1978), ten FA Charity Shield matches (the last in 1970), and three England international matches, the last in 1932; it was also the venue for an unofficial "Victory International" in 1946. The 2013 UEFA Women's Champions League Final was played at Stamford Bridge. In October 1905 it hosted a rugby union match between the All Blacks and Middlesex, and in 1914 hosted a baseball match between the touring New York Giants and the Chicago White Sox. It was the venue for a boxing match between world flyweight champion Jimmy Wilde and Joe Conn in 1918. The running track was used for dirt track racing between 1928 and 1932, greyhound racing from 1933 to 1968, and Midget car racing in 1948. In 1980, Stamford Bridge hosted the first international floodlit cricket match in the UK, between Essex and the West Indies. It was also the home stadium of the London Monarchs American Football team for the 1997 season.
The current club ownership have stated that a larger stadium is necessary in order for Chelsea to stay competitive with rival clubs who have significantly larger stadia, such as Arsenal and Manchester United. Owing to its location next to a main road and two railway lines, fans can only enter the ground via the Fulham Road exits, which places constraints on expansion due to health and safety regulations. The club have consistently affirmed their desire to keep Chelsea at their current home, but Chelsea have nonetheless been linked with a move to various nearby sites, including the Earls Court Exhibition Centre, Battersea Power Station and the Chelsea Barracks. On 3 October 2011, Chelsea made a proposal to CPO shareholders to buy back the freehold to the land on which Stamford Bridge sits, stating that "buying back the freehold removes a potential hurdle should a suitable site become available in the future". The proposal was voted down by CPO shareholders. In May 2012, the club made a formal bid to purchase Battersea Power Station, with a view to developing the site into a 60,000 seater stadium, but lost out to a Malaysian consortium.
Crest and colours.
Crest.
Since the club's foundation, Chelsea have had four main crests, though all underwent minor variations. In 1905, Chelsea adopted as their first crest the image of a Chelsea pensioner, which contributed to the "pensioner" nickname, and remained for the next half-century, though it never appeared on the shirts. As part of Ted Drake's modernisation of the club from 1952 onwards, he insisted that the pensioner badge be removed from the match day programme to change the club's image and that a new crest be adopted. As a stop-gap, a temporary emblem comprising simply the initials C.F.C. was adopted for one year.
In 1953, Chelsea's crest was changed to an upright blue lion looking backwards and holding a staff, which was to endure for the next three decades. This crest was based on elements in the coat of arms of the Metropolitan Borough of Chelsea with the "lion rampant regardant" taken from the arms of then club president Viscount Chelsea and the staff from the Abbots of Westminster, former Lords of the Manor of Chelsea. It also featured three red roses, to represent England, and two footballs. This was the first club badge to appear on shirts, since the policy of putting the crest on the shirts was only adopted in the early 1960s.
In 1986, with Ken Bates now owner of the club, Chelsea's crest was changed again as part of another attempt to modernise and to capitalise on new marketing opportunities. The new badge featured a more naturalistic non-heraldic lion, in white and not blue, standing over the C.F.C. initials. It lasted for the next 19 years, with some modifications such as the use of different colours, including red from 1987 to 1995, and yellow from 1995 until 1999, before the white returned. With the new ownership of Roman Abramovich, and the club's centenary approaching, combined with demands from fans for the popular 1950s badge to be restored, it was decided that the crest should be changed again in 2005. The new crest was officially adopted for the start of the 2005–06 season and marked a return to the older design, used from 1953 to 1986, featuring a blue heraldic lion holding a staff. For the centenary season this was accompanied by the words '100 YEARS' and 'CENTENARY 2005–2006' on the top and bottom of the crest respectively.
Colours.
Chelsea have always worn blue shirts, although they originally used the paler eton blue, which was taken from the racing colours of then club president, Earl Cadogan, and was worn with white shorts and dark blue or black socks. The light blue shirts were replaced by a royal blue version in around 1912. In the 1960s Chelsea manager Tommy Docherty changed the kit again, switching to blue shorts (which have remained ever since) and white socks, believing it made the club's colours more modern and distinctive, since no other major side used that combination; this kit was first worn during the 1964–65 season. Since then Chelsea have always worn white socks with their home kit apart from a short spell from 1985 to 1992, when blue socks were reintroduced.
Chelsea's traditional away colours are all yellow or all white with blue trim, but, as with most teams, they have had some more unusual ones. The first away strip consisted of black and white stripes and for one game in the 1960s the team wore blue and black stripes, inspired by Inter Milan's kit, again at Docherty's behest. Other memorable away kits include a mint green strip in the 1980s, a red and white checked one in the early 90s and a graphite and tangerine edition in the mid-1990s.
Support.
Chelsea have the fifth highest average all-time attendance in English football and regularly attract over 40,000 fans to Stamford Bridge; they were the sixth best-supported Premier League team in the 2012–13 season, with an average gate of 41,462. Chelsea's traditional fanbase comes from all over the Greater London area including working-class parts such as Hammersmith and Battersea, wealthier areas like Chelsea and Kensington, and from the home counties. There are also numerous official supporters clubs in the United Kingdom and all over the world. Between 2007 and 2012 Chelsea were ranked fourth worldwide in annual replica kit sales, with an average of 910,000.
At matches, Chelsea fans sing chants such as "Carefree" (to the tune of Lord of the Dance, whose lyrics were probably written by supporter Mick Greenaway), "Ten Men Went to Mow", "We All Follow the Chelsea" (to the tune of "Land of Hope and Glory"), "Zigga Zagga", and the celebratory "Celery", with the latter often resulting in fans ritually throwing celery. The vegetable was banned inside Stamford Bridge after an incident involving Arsenal midfielder Cesc Fàbregas at the 2007 League Cup Final.
During the 1970s and 1980s in particular, Chelsea supporters were associated with football hooliganism. The club's "football firm", originally known as the Chelsea Shed Boys, and subsequently as the Chelsea Headhunters, were nationally notorious for football violence, alongside hooligan firms from other clubs such as West Ham United's Inter City Firm and Millwall's Bushwackers, before, during and after matches. The increase of hooligan incidents in the 1980s led chairman Ken Bates to propose erecting an electric fence to deter them from invading the pitch, a proposal that the Greater London Council rejected.
Since the 1990s there has been a marked decline in crowd trouble at matches, as a result of stricter policing, CCTV in grounds and the advent of all-seater stadia. In 2007, the club launched the 'Back to the Shed' campaign to improve the atmosphere at home matches, with notable success. According to Home Office statistics, 126 Chelsea fans were arrested for football-related offences during the 2009–10 season, the third highest in the division, and 27 banning orders were issued, the fifth highest in the division.
Rivalries.
Chelsea do not have a traditional rivalry on the scale of the Merseyside derby or the North London derby. Matches against fellow West London sides Fulham and Queens Park Rangers have only taken place intermittently, due to the clubs often being in separate divisions. A 2004 survey by Planetfootball.com found that Chelsea fans consider their main rivalries to be with (in order): Arsenal, Tottenham Hotspur and Manchester United. Their rivalry with Tottenham Hotspur is said to have developed following the 1967 FA Cup Final, the first cup final held between two London clubs. Additionally, a strong rivalry with Leeds United dates back to several heated and controversial matches in the 1960s and 1970s, particularly the 1970 FA Cup Final. More recently a rivalry with Liverpool has grown following repeated clashes in cup competitions.
Records.
Chelsea's highest appearance-maker is ex-captain Ron Harris, who played in 795 competitive games for the club between 1961 and 1980. The record for a Chelsea goalkeeper is held by Harris's contemporary, Peter Bonetti, who made 729 appearances (1959–79). With 103 caps (101 while at the club), Frank Lampard of England is Chelsea's most capped international player.
Frank Lampard is Chelsea's all-time top goalscorer, with 211 goals in 648 games (2001–2014); he passed Bobby Tambling's longstanding record of 202 in May 2013. Seven other players have also scored over 100 goals for Chelsea: George Hilsdon (1906–12), George Mills (1929–39), Roy Bentley (1948–56), Jimmy Greaves (1957–61), Peter Osgood (1964–74 and 1978–79), Kerry Dixon (1983–92) and Didier Drogba (2004–12). Greaves holds the record for the most goals scored in one season (43 in 1960–61).
Chelsea's biggest winning scoreline in a competitive match is 13–0, achieved against Jeunesse Hautcharage in the Cup Winners' Cup in 1971. The club's biggest top-flight win was an 8–0 victory against Wigan Athletic in 2010, and matched in 2012 against Aston Villa. Chelsea's biggest loss was an 8–1 reverse against Wolverhampton Wanderers in 1953. Officially, Chelsea's highest home attendance is 82,905 for a First Division match against Arsenal on 12 October 1935. However, an estimated crowd of over 100,000 attended a friendly match against Soviet team Dynamo Moscow on 13 November 1945. The modernisation of Stamford Bridge during the 1990s and the introduction of all-seater stands mean that neither record will be broken for the foreseeable future. The current legal capacity of Stamford Bridge is 41,837. Every starting player in Chelsea’s 57 games in 2013/14 season was a full international – a new club record.
Chelsea hold the English record for the highest ever points total for a league season (95), the fewest goals conceded during a league season (15), the highest number of Premier League victories in a season (29), the highest number of clean sheets overall in a Premier League season (25) (all set during the 2004–05 season), and the most consecutive clean sheets from the start of a league season (6, set during the 2005–06 season). The club's 21–0 aggregate victory over Jeunesse Hautcharage in the UEFA Cup Winners' Cup in 1971 remains a record in European competition. Chelsea hold the record for the longest streak of unbeaten matches at home in the English top-flight, which lasted 86 matches from 20 March 2004 to 26 October 2008. They secured the record on 12 August 2007, beating the previous record of 63 matches unbeaten set by Liverpool between 1978 and 1980. Chelsea's streak of eleven consecutive away league wins, set between 5 April 2008 and 6 December 2008, is also a record for the English top flight. Their £50 million purchase of Fernando Torres in January 2011 is the current record transfer fee paid by a British club.
Chelsea, along with Arsenal, were the first club to play with shirt numbers, on 25 August 1928 in their match against Swansea Town. They were the first English side to travel by aeroplane to a domestic away match, when they visited Newcastle United on 19 April 1957, and the first First Division side to play a match on a Sunday, when they faced Stoke City on 27 January 1974. On 26 December 1999, Chelsea became the first British side to field an entirely foreign starting line-up (no British or Irish players) in a Premier League match against Southampton. In May 2007, Chelsea were the first team to win the FA Cup at the new Wembley Stadium, having also been the last to win it at the old Wembley. They were the first English club to be ranked #1 under UEFA's five-year coefficient system in the 21st century. They were the first team in Premier League history to score at least 100 goals in a single season, reaching the milestone on the final day of the 2009–10 season. Chelsea are the only London club to win the UEFA Champions League, after beating Bayern Munich in the 2012 final. Upon winning the 2012–13 UEFA Europa League, Chelsea became the first English club to win all four European trophies.
Ownership and finances.
Chelsea Football Club was founded by Gus Mears in 1905. After his death in 1912, his descendents continued to own the club until 1982, when Ken Bates bought the club from Mears' great-nephew Brian Mears for £1. Bates bought a controlling stake in the club and floated Chelsea on the AIM stock exchange in March 1996. In July 2003, Roman Abramovich purchased just over 50% of Chelsea Village plc's share capital, including Bates' 29.5% stake, for £30 million and over the following weeks bought out most of the remaining 12,000 shareholders at 35 pence per share, completing a £140 million takeover. Other shareholders at the time of the takeover included the Matthew Harding estate (21%), BSkyB (9.9%) and various anonymous offshore trusts. After passing the 90% share threshold, Abramovich took the club back into private hands, delisting it from the AIM on 22 August 2003. He also took on responsibility for the club's debt of £80 million, quickly paying most of it.
Thereafter, Abramovich changed the ownership name to Chelsea FC plc, whose ultimate parent company is Fordstam Limited, which is controlled by him. Chelsea are additionally funded by Abramovich via interest free soft loans channelled through his holding company Fordstam Limited. The loans stood at £709 million in December 2009, when they were all converted to equity by Abramovich, leaving the club itself debt free, although the debt remains with Fordstam. Since 2008 the club has had no external debt. In November 2012, Chelsea announced a profit of £1.4 million for the year ending 30 June 2012, the first time the club has made a profit under Abramovich's ownership.
Chelsea has been described as a global brand; a 2012 report by Brand Finance ranked Chelsea fifth among football brands and valued the club's brand value at US $398 million – an increase of 27% from the previous year, also valuing it at US $10 million more than the sixth best brand, London rivals Arsenal – and gave the brand a strength rating of AA (very strong). In 2012, Forbes magazine ranked Chelsea seventh in their list of the ten most valuable football clubs in the world, valuing the club's brand at £473 million ($761 million). Chelsea are currently ranked sixth in the Deloitte Football Money League with an annual commercial revenue of £225.6 million.
Chelsea's kit has been manufactured by Adidas since 2006, which is contracted to supply the club's kit from 2006 to 2018. The partnership was extended in October 2010 in a deal worth £160 million over eight years. This deal was again extended in June 2013 in a deal worth £300 million over another ten years. Previously, the kit was manufactured by Umbro (1968–81), Le Coq Sportif (1981–86), The Chelsea Collection (1986–87) and Umbro again (1987–2006). Chelsea's first shirt sponsor was Gulf Air, agreed during the 1983–84 season. The club were then sponsored by Grange Farms, Bai Lin Tea and Simod before a long-term deal was signed with Commodore International in 1989; Amiga, an off-shoot of Commodore, also appeared on the shirts. Chelsea were subsequently sponsored by Coors beer (1995–97), Autoglass (1997–2001) and Emirates Airline (2001–05). Chelsea's current shirt sponsor is Samsung who took over the sponsorship from their mobile division in 2007–08. In 2012, Gazprom became the club's official Global Energy Partner on a three-year sponsorship deal. The club also has a variety of other sponsors and partners, which include Delta Air Lines, Sauber, Audi, Singha, EA Sports, Dolce & Gabbana Barbados Tourism Authority, Atlas, AZIMUT Hotels, BNI, Vietinbank, Nitto Tire, Orico, Guangzhou R&F, Coca Cola, Grand Royal, Digicel, Lucozade Sport, and Viagogo.
Chelsea Ladies.
Chelsea also operate a women's football team, Chelsea Ladies. It has been affiliated to the men's team since 2004, and is part of the club's Community Development programme. They play their home games at Wheatsheaf Park, the home ground of Conference South club Staines Town. The club won the Surrey County Cup in 2003, 2004, 2006, 2007, 2008, 2009, 2010, 2012, and 2013 and were promoted to the Premier Division for the first time in 2005 as Southern Division champions. In the 2009–10 season, they finished 3rd in the Premier League, equalling their highest ever placing, and in 2010 were one of the eight founder members of the FA Women's Super League. John Terry, the current captain of the Chelsea men's team, is President of Chelsea LFC.
Popular culture.
In 1930, Chelsea featured in one of the earliest football films, "The Great Game". One-time Chelsea centre forward, Jack Cock, who by then was playing for Millwall, was the star of the film and several scenes were shot at Stamford Bridge, including the pitch, the boardroom, and the dressing rooms. It included guest appearances by then-Chelsea players Andrew Wilson, George Mills, and Sam Millington. Owing to the notoriety of the Chelsea Headhunters, a football firm associated with the club, Chelsea have also featured in films about football hooliganism, including 2004's "The Football Factory". Chelsea also appear in the Hindi film "Jhoom Barabar Jhoom". In April 2011, Montenegrin comedy series "Nijesmo mi od juče" made an episode in which Chelsea plays against FK Sutjeska Nikšić for qualification of the UEFA Champions League.
Up until the 1950s, the club had a long-running association with the music halls; their underachievement often provided material for comedians such as George Robey. It culminated in comedian Norman Long's release of a comic song in 1933, ironically titled "On the Day That Chelsea Went and Won the Cup", the lyrics of which describe a series of bizarre and improbable occurrences on the hypothetical day when Chelsea finally won a trophy. In Alfred Hitchcock's 1935 film "The 39 Steps", Mr Memory claims that Chelsea last won the Cup in 63 BC, "in the presence of the Emperor Nero."
At the start of every home game, 'The Liquidator' by the Harry J Allstars is heard before kick-off. Chelsea claim to be the first team to have used the song at a football match, in 1969.
The song "Blue is the Colour" was released as a single in the build-up to the 1972 League Cup Final, with all members of Chelsea's first team squad singing; it reached number five in the UK Singles Chart. The song has since been adopted as an anthem by a number of other sports teams around the world, including the Vancouver Whitecaps (as "White is the Colour") and the Saskatchewan Roughriders (as "Green is the Colour"). In the build-up to the 1997 FA Cup Final, the song "Blue Day", performed by Suggs and members of the Chelsea squad, reached number 22 in the UK charts. Bryan Adams, a fan of Chelsea, dedicated the song "We're Gonna Win" from the album "18 Til I Die" to the club.
Players.
First team squad.
"For recent transfers, see 2014–15 Chelsea F.C. season."
Notable managers.
The following managers won at least one trophy when in charge of Chelsea:
Coaching staff.
! style="color:#FFFFFF; background:#034694;"|Position
! style="color:#FFFFFF; background:#034694;"|Staff
Management.
Chelsea Ltd.
Chelsea F.C. plc
Executive Board
Chelsea Football Club Board:
Life President:

</doc>
<doc id="7475" url="http://en.wikipedia.org/wiki?curid=7475" title="CANDU reactor">
CANDU reactor

The CANDU (short for CANada Deuterium Uranium) reactor is a Canadian-invented, pressurized heavy water reactor. The acronym refers to its deuterium-oxide (heavy water) moderator and its use of (originally, natural) uranium fuel. CANDU reactors were first developed in the late 1950s and 1960s by a partnership between Atomic Energy of Canada Limited (AECL), the Hydro-Electric Power Commission of Ontario (now Ontario Power Generation), Canadian General Electric (now GE Canada), and other companies.
All power reactors built in Canada are of the CANDU type. The reactor is also marketed abroad and there are CANDU-type units operating in India, Pakistan, Argentina, South Korea, Romania and China. In October 2011, the Canadian Federal Government licensed the CANDU design to Candu Energy (a wholly owned subsidiary of SNC-Lavalin), which also acquired the former reactor development and marketing division of AECL at that time.
Design.
Basic design and operation.
A CANDU power plant generates power in the same fashion as a fossil-fuel power station: heat is generated by "burning" fuel, and that heat is used to drive a steam turbine, normally located in a separate "power hall". Whereas a typical coal-fired plant burns coal and air and produces mostly carbon dioxide and fly ash, the CANDU consumes nuclear fuel in-situ; when the fuel is "burned up" it is removed from the reactor and stored as high level radioactive waste.
Fission reactions in the reactor core heat pressurized heavy water in a "primary cooling loop". A heat exchanger, also known as a steam generator, transfers the heat to a light-water "secondary cooling loop", which powers a steam turbine with an electrical generator attached to it (for a typical Rankine thermodynamic cycle). The exhaust steam from the turbines is then condensed and returned as feedwater to the steam generator, often using cooling water from a nearby source, such as a lake, river, or ocean. Newer CANDU plants, such as the Darlington Nuclear Generating Station near Toronto, Ontario, use a diffuser to spread the warm outlet water over a larger volume and limit the effects on the environment. A cooling tower can be used, but it reduces efficiency and increases costs considerably.
Some of the unique features of the CANDU design are listed below:
In a light water reactor (LWR), the entire reactor core is a single large pressure vessel containing the light water, which acts as moderator and coolant, and the fuel arranged in a series of long bundles running the length of the core. At the time of CANDU's design, Canada lacked the heavy industry to cast and machine the pressure vessels. In CANDU the pressure (and the fuel bundles) is contained in much smaller (10 cm diameter), easier to fabricate tubes. Each bundle is a cylinder assembled from alloy tubes containing ceramic pellets of fuel. In older designs the assembly had 28 or 37 half-meter-long fuel tubes with 12 such assemblies lying end to end in a pressure tube. The newer CANFLEX bundle has 43 tubes, with two pellet sizes (so the power rating can be increased without melting the hottest pellets). It is about in diameter, long and weighs about and replaces the 37-tube bundle. To allow the neutrons to flow freely between the bundles, the tubes and bundles are made of neutron-transparent zircaloy (zirconium + 2.5% wt niobium).
The zircaloy tubes are surrounded by a much larger low-pressure tank known as a calandria, which contains the majority of the moderator. To keep the hot coolant from boiling the moderator, a calandria tube surrounds each pressure tube, with insulating carbon dioxide gas in between. Slowing down neutrons releases energy, so a cooling system dissipates the heat. The moderator is actually a large heat sink that acts as an additional safety feature. The use of individual high pressure fuel channels passing through the CANDU's low-pressure moderator calandria makes it easier to refuel: a pressure-vessel reactor must be shut down, the pressure dropped, the lid removed, and a sizeable fraction of the fuel, e.g. one-third, replaced all at once. In CANDU, individual channels can be refuelled without taking the reactor offline, improving the capacity factor. One fueling machine inserts new fuel into one end of the channel while the other receives discharged fuel from the opposite end. One significant operational advantage of online refuelling is that a failed or leaking fuel bundle can be removed from the core once it has been located, thus reducing the radiation fields in the primary systems.
Purpose of using heavy water.
Natural uranium is a mix of isotopes - mainly uranium-238, with 0.72% (by weight) fissile uranium-235. A reactor aims for a steady rate of fission over time (criticality), where the neutrons released by fission cause an equal number of fissions in other atoms. These neutrons are fairly energetic and don't readily react with (get "captured" by) the surrounding fissile material - they must have their energy "moderated" (i.e. be slowed down ) as much as possible, ideally to the same energy as the atoms themselves ("thermal neutrons") or lower. During moderation it helps to separate the neutrons and uranium, since 238U has a large affinity for intermediate-energy neutrons ("resonance" absorption), but is only easily fissioned by the few energetic neutrons above ~1.5-2 MeV. Since most of the fuel is usually 238U, most reactor designs are based on thin fuel rods separated by moderator, allowing the neutrons to travel in the moderator before entering the fuel again. More neutrons are released than is needed to maintain the chain reaction; when uranium-238 absorbs just the excess, plutonium is created which helps to make up for the depletion of uranium-235. Eventually the build-up of fission products that are even more neutron-absorbing than 238U slows the reaction and calls for refuelling.
Light water makes an excellent moderator - the light hydrogen atoms are very close in mass to a neutron and can absorb a lot of energy in a single collision (like a collision of two billiard balls). However, light hydrogen is also fairly effective at "absorbing" neutrons, and there will be too few left over to react with the small amount of 235U in natural uranium, preventing criticality. In order to allow criticality, the fuel must be "enriched", increasing the amount of 235U to an acceptable level. In light water reactors, the fuel is typically enriched to between 2% and 5% 235U (the leftover fraction with less 235U is called depleted uranium). Enrichment facilities are expensive to build and operate. They're also a proliferation concern as they can be used to enrich the 235U much further, up to weapons-grade material (90% or more 235U). However, this can be remedied if the fuel is supplied and reprocessed by an internationally approved supplier.
The main advantage of heavy water moderator over light water is reduced absorption of the neutrons that sustain the chain reaction, allowing a lower concentration of active atoms (to the point of using unenriched natural uranium fuel). Deuterium ("heavy hydrogen") already has the extra neutron that light hydrogen would absorb, reducing the tendency to capture neutrons. However, deuterium is twice the mass of a single neutron (vs light hydrogen which is about the same mass); the mismatch means more collisions are needed to moderate the neutrons, requiring a larger thickness of moderator between the fuel rods. This increases the size of the reactor core and the leakage of neutrons. It is also the practical reason for the calandria design, otherwise a very large pressure vessel would be needed. The low 235U density in natural uranium also implies that less of the fuel will be consumed before the fission rate drops too low to sustain criticality, because the ratio of 235U to fission products+238U is lower. However, in CANDU most of the moderator is at lower temperatures than in other designs, reducing the spread of speeds and the overall speed of the moderator particles. This means most of the neutrons will end up at a lower energy and be more likely to cause fission, so CANDU not only "burns" natural uranium, but it does so more effectively as well. Overall, CANDU reactors use 30–40% less mined uranium than light-water reactors per unit of electricity produced. This is a major advantage to the heavy water design; it not only requires less fuel, but as the fuel does not have to be enriched, it is much less expensive as well.
A further unique feature of heavy-water moderation is the greater stability of the chain reaction. This is due to the relatively low binding energy of the deuterium nucleus (2.2 MeV), leading to some energetic neutrons and especially gamma rays breaking the nuclei apart and producing extra neutrons. Both gammas produced directly by fission and by the decay of fission fragments have enough energy, and the half-lives of the fission fragments range from seconds to hours or even years. The slow response of photoneutrons delays the response of the reactor and gives the operators extra time in case of an emergency. Since gamma rays travel for meters through water, an increased rate of chain reaction in one part of the reactor will produce a response from the rest of the reactor, allowing various negative feedbacks to stabilize the reaction.
On the other hand, the fission neutrons are thoroughly slowed down before they reach another fuel rod, meaning that it takes neutrons a longer time to get from one part of the reactor to the other. Thus if the chain reaction accelerates in one section of the reactor, the change will propagate itself only slowly to the rest of the core, giving time to respond in an emergency. The independence of the neutrons' energies from the nuclear fuel used is what allows for such fuel flexibility in a CANDU reactor, since every fuel bundle will experience the same environment and affect its neighbors in the same way, whether the fissile material is uranium-235, uranium-233 or plutonium.
Canada developed the heavy water moderated design in the post-World War II era to explore nuclear energy while lacking access to enrichment facilities. War-era enrichment systems were extremely expensive to build and operate, whereas the heavy water solution allowed the use of natural uranium in the experimental ZEEP reactor. A much less expensive enrichment system was developed, but the United States classified work on the cheaper gas centrifuge process. The CANDU was therefore designed to use natural uranium.
Safety features.
The CANDU includes a number of active and passive safety features in its design. Some of these are a side-effect of the physical layout of the system.
CANDU designs have a positive void coefficient as well as a small power coefficient, normally considered bad in reactor design. This implies that steam generated in the coolant will "increase" the reaction rate, which in turn would generate more steam. This is one of the many reasons for the cooler mass of moderator in the calandria, as even a serious steam incident in the core would not have a major impact on the overall moderation cycle. Only if the moderator itself starts to boil would there be any significant effect, and the large thermal mass ensures this will occur slowly. The deliberately "sluggish" response of the fission process in CANDU allows controllers more time to diagnose and deal with problems.
The fuel channels can only maintain criticality if they are mechanically sound. If the temperature of the fuel bundles increases to the point where they are mechanically unstable, their horizontal layout means they will bend under gravity, shifting the layout of the bundles and reducing the efficiency of the reactions. Because the original fuel arrangement is optimum for a chain reaction and the natural uranium fuel has little excess reactivity, any significant deformation will stop the inter-fuel pellet fission reaction. This will not stop heat production from fission product decay, which would continue to supply a considerable heat output. If this process further weakens the fuel bundles, they will eventually bend far enough to touch the calandria tube, allowing heat to be efficiently transferred into the moderator tank. The moderator vessel has a considerable thermal capability on its own, and is normally kept relatively cool.
Heat generated by fission products would initially be at about 7% of full reactor power, which requires significant cooling. The CANDU designs have several emergency cooling systems, as well as having limited self-pumping capability through thermal means (the steam generator is well above the reactor). Even in the event of a catastrophic accident and core meltdown, it is important to remember that the fuel is not critical in light water. This means that cooling the core with water from nearby sources will not add to the reactivity of the fuel mass.
Normally the rate of fission is controlled by light-water compartments called liquid zone controllers, which absorb excess neutrons, and by adjuster rods which can be raised or lowered in the core to control the neutron flux. These are used for normal operation, allowing the controllers to adjust reactivity across the fuel mass as different portions would normally burn at different rates depending on their position. The adjuster rods can also be used to slow or stop criticality. Because these rods are inserted into the low-pressure calandria, not the high-pressure fuel tubes, they would not be "ejected" by steam, a design issue for many pressurized-water reactors.
There are two independent, fast-acting safety shutdown systems as well. Shutoff rods are held above the reactor by electromagnets, and drop under gravity into the core to quickly end criticality. This system works even in the event of a complete power failure, as the electromagnets only hold the rods out of the reactor when power is available. A secondary system injects a high-pressure gadolinium nitrate neutron absorber solution into the calandria.
Fuel cycles.
A heavy water design can sustain a chain reaction with a lower concentration of fissile atoms than light water reactors, allowing it to use some alternative fuels, e.g., "recovered uranium" (RU) from used LWR fuel can be used. CANDU was designed for natural uranium with only 0.7% U-235, so RU with 0.9% U-235 is a rich fuel. This extracts a further 30–40% energy from the uranium. The DUPIC ("Direct Use of spent PWR fuel In CANDU") process under development can recycle it even without reprocessing. The fuel is sintered in air (oxidized), then in hydrogen (reduced) to break it into a powder, which is then formed into CANDU fuel pellets.
CANDU can also breed fuel from the more abundant thorium. This is being investigated by India to take advantage of its natural thorium reserves.
Even better than LWRs, CANDU can burn a mix of uranium and plutonium oxides (MOX fuel), the plutonium either from dismantled nuclear weapons or reprocessed reactor fuel. The mix of isotopes in reprocessed plutonium is not attractive for weapons, but can be used as fuel (instead of being simply nuclear waste), while burning weapons-grade plutonium eliminates a proliferation hazard. If the aim is explicitly to burn plutonium or other actinides from spent fuel, then special inert-matrix fuels are proposed to do this more efficiently than MOX. Since they contain no uranium, these fuels do not breed any extra plutonium.
Economics.
The neutron economy of heavy water moderation and precise control of on-line refueling allow CANDU to use a great range of fuels other than enriched uranium, e.g., natural uranium, reprocessed uranium, thorium, plutonium, and used LWR fuel. Given the expense of enrichment, this can make fuel much cheaper. There is however an initial investment into the tonnes of 99.75% pure heavy water to fill the core and heat transfer system. In the case of the Darlington plant costs released as part of a freedom of information act request put the overnight cost of the plant (four reactors totalling 3512 MWe net capacity) at $5.117 billion CAD (about $4.2 billion USD at early 1990s exchange rates). Total capital costs including interest were $14.319 billion CAD (about $11.9 billion USD) with the heavy water accounting for $1.528 billion, or 11%, of this.
Since heavy water is less efficient at slowing neutrons, CANDU needs a larger moderator to fuel ratio and a larger core for the same power output. Although a calandria-based core is cheaper to build, its size increases the cost for standard features like the containment building. Generally nuclear plant construction and operations are ~65% of overall lifetime cost; for CANDU costs are dominated by construction even more. Fueling CANDU is cheaper than other reactors, costing only ~10% of the total, so the overall price per kWh electricity is comparable. The next-generation Advanced CANDU Reactor (ACR) mitigates these disadvantages by having light water coolant and using a more compact core with less moderator.
When first introduced, CANDUs offered much better capacity factor (ratio of power generated to what would be generated by running at full power, 100% of the time) than LWRs of a similar generation. The light-water designs spent, on average, about half the time being refueled or maintained. However, since the 1980s dramatic improvements in LWR outage management have narrowed the gap, with several units achieving capacity factors ~90% and higher, with an overall fleet performance of 92% in 2010. The latest-generation CANDU 6 reactors have an 88-90% CF, but overall performance is dominated by the older Canadian units with CFs on the order of 80%. Refurbished units have demonstrated poor performance to date, on the order of 65%.
Some CANDU plants suffered from cost overruns during construction, often from external factors such as government action. For instance, a number of imposed construction delays led to roughly a doubling of the cost of the Darlington Nuclear Generating Station near Toronto, Ontario. Technical problems and redesigns added about another billion to the resulting $14.4 billion price. In contrast, in 2002 two CANDU 6 reactors at Qinshan in China were completed on-schedule and on-budget, an achievement attributed to tight control over scope and schedule.
Nuclear nonproliferation.
In terms of safeguards against nuclear weapons proliferation, CANDUs meet a similar level of international certification as other reactors. There is a common misconception that plutonium for India's first nuclear detonation, Operation Smiling Buddha in 1974, was produced in a CIRUS design. In fact, it was produced in the safeguarded indigenously built PHWR reactor. In addition to its two PHWR reactors, India has some safeguarded pressurised heavy water reactors (PHWRs) based on the CANDU design, and two safeguarded light-water reactors supplied by the US. Plutonium has been extracted from the spent fuel from all of these reactors; however India mainly relies on an Indian designed and built military reactor called Dhruva. The design is believed to be derived from the CIRUS reactor, with the Dhruva being scaled-up for more efficient plutonium production. It is this reactor which is thought to have produced the plutonium for India's more recent (1998) Operation Shakti nuclear tests.
Although heavy water is relatively immune to neutron capture, a small amount of the deuterium turns into tritium in this way. Tritium+deuterium mix undergoes nuclear fusion more easily than any other substance. Tritium can be used in both the "fusion boost" of a boosted fission weapon and the main fusion process of an H-bomb. However, in an H-bomb, it's usually created "in situ" by neutron irradiation of lithium-6.
Tritium is extracted from some CANDU plants in Canada, mainly to improve safety in case of heavy-water leakage. The gas is stockpiled and used in a variety of commercial products, notably "powerless" lighting systems and medical devices. In 1985 what was then Ontario Hydro sparked controversy in Ontario due to its plans to sell tritium to the U.S. The plan, by law, involved sales to non-military applications only, but some speculated that the exports could have freed American tritium for the U.S. nuclear weapons program. Future demands appear to outstrip production, in particular the demands of future generations of experimental fusion reactors like ITER. Currently between 1.5 and 2.1 kg of tritium are recovered yearly at the Darlington separation facility, of which a minor fraction is sold.
The 1998 Operation Shakti test series in India included one bomb of about 45 kt yield that India has publicly claimed was a hydrogen bomb. An offhand comment in the BARC publication "Heavy Water — Properties, Production and Analysis" appears to suggest that the tritium was extracted from the heavy water in the CANDU and PHWR reactors in commercial operation. "Janes Intelligence Review" quotes the Chairman of the Indian Atomic Energy Commission as admitting to the tritium extraction plant, but refusing to comment on its use. However India is also capable of creating tritium more efficiently by irradiation of lithium-6 in reactors.
Tritium emissions.
Tritium is a radioactive form of hydrogen (H-3), with a half-life of 12.3 years. It is found in small amounts in nature (about 4 kg globally), created by cosmic ray interactions in the upper atmosphere. Tritium is considered a weak radionuclide because of its low-energy radioactive emissions (beta particle energy 0 -19 keV). The beta particles do not travel very far in air and only penetrate skin up to 3-4mm, so the main hazard is intake into the body (inhalation, ingestion, or absorption).
Tritium is generated in the fuel of all reactors; however, CANDU reactors generate tritium also in their coolant and moderator, due to neutron capture in heavy hydrogen. Some of this tritium escapes into containment and is generally recovered; however a small percentage (about 1%) escapes containment and is considered a routine radioactive emission (also higher than from an LWR of comparable size). Responsible operation of a CANDU plant therefore includes monitoring tritium in the surrounding environment (and publishing the results).
In some CANDU reactors the tritium is periodically extracted. Typical emissions from CANDU plants in Canada are less than 1% of the national regulatory limit, which is based on International Commission on Radiological Protection (ICRP) guidelines (for example, the maximum permitted drinking water concentration for tritium in Canada, 7000 Bq/L, corresponds to 1/10 of the ICRP's dose limit for members of the public). Tritium emissions from other CANDU plants are similarly low.
In general there is significant public controversy about radioactive emissions from nuclear power plants, and for CANDU plants one of the main concerns is tritium. In 2007 Greenpeace published a critique of tritium emissions from Canadian nuclear power plants by Dr. Ian Fairlie. This report was criticized by Dr. Richard Osborne.
History.
Evolving designs.
The CANDU development effort has gone through four major stages over time. The first systems were experimental and prototype machines of limited power. These were replaced by a second generation of machines of 500 to 600 MWe (the CANDU6), a series of larger machines of 900 MWe, and finally developing into the CANDU9 and current ACR-1000 effort.
Early efforts.
The first heavy water moderated design in Canada was the ZEEP, which started operation just after the end of World War II. ZEEP was joined by several other experimental machines, including the NRX and NRU. These efforts led to the first CANDU-type reactor, the Nuclear Power Demonstration (NPD), in Rolphton, Ontario. It was intended as a proof-of-concept and rated for only 22 MWe, a very low power for a commercial power reactor. NPD produced the first nuclear-generated electricity in Canada, and ran successfully from 1962 to 1987.
The second CANDU was the Douglas Point reactor, a more powerful version rated at roughly 200 MWe and located near Kincardine, Ontario. It went into service in 1968, and ran until 1984. Uniquely among CANDU stations, Douglas Point had an oil-filled window with a view of the east reactor face, even when the reactor was operating. Douglas Point was originally planned to be a two-unit station, but the second unit was cancelled because of the success of the larger 515 MWe units at Pickering.
 Gentilly-1, in Bécancour, Quebec near Trois-Rivières, Quebec, was also an experimental version of CANDU, using a boiling light-water coolant and vertical pressure tubes, but was not considered successful and closed after seven years of fitful operation. Gentilly-2, a CANDU-6 reactor, has been operating since 1983. Following statements from the in-coming Parti Québécois government in September 2012 that Gentilly would close, the operator, Hydro-Québec, has decided to cancel a previously announced refurbishment of the plant and announced its shutdown at the end of 2012, citing economic reasons for the decision. The company will then undertake a 50-year decommissioning process estimated to cost $1.8 billion.
In parallel with the classic CANDU design, experimental variants were being developed. WR-1, located at the AECL's Whiteshell Laboratories in Pinawa, Manitoba, used vertical pressure tubes and organic oil as the primary coolant. The oil used has a higher boiling point than water, allowing the reactor to operate at higher temperatures and lower pressures than a conventional reactor. WR-1 operated successfully for many years, and promised a significantly higher efficiency than water-cooled versions.
600 MWe designs.
The successes at NPD and Douglas Point led to the decision to construct the first multi-unit station in Pickering, Ontario. Pickering A, consisting of Units 1 to 4, went into service in 1971. Pickering B with units 5 to 8 came online in 1983, giving a full-station capacity of 4,120 MWe. The station is very close to the city of Toronto, in order to reduce transmission costs.
A series of improvements to the basic Pickering design led to the CANDU 6 design, which first went into operation in the early 1980s. CANDU 6 was essentially a version of the Pickering power plant that was re-designed to be able to be built in single-reactor units. CANDU 6 was used in several installations outside Ontario, including the Gentilly-2 in Quebec, and Point Lepreau Nuclear Generating Station in New Brunswick. CANDU 6 forms the majority of foreign CANDU systems, including the designs exported to Argentina, Romania, China and South Korea. Only India operates a CANDU system that is not based on the CANDU 6 design.
900 MWe designs.
The economics of nuclear power plants generally scale well with size. However, this improvement at larger sizes is offset by the sudden appearance of large quantities of power on the grid, which leads to a lowering of electricity prices through supply and demand effects. Predictions in the late 1960s suggested that growth in electricity demand would overwhelm these downward pricing pressures, leading most designers to introduce plants in the 1000 MWe range.
Pickering A was quickly followed by such an upscaling effort for the Bruce Nuclear Generating Station, constructed in stages between 1970 and 1987. It is the largest nuclear facility in North America, and second largest in the world (after Kashiwazaki-Kariwa in Japan), with eight reactors at around 800 MWe each, in total 6,232 MW (net) and 7,276 MW (gross). Another, smaller, upscaling led to the Darlington Nuclear Generating Station design, similar to the Bruce plant, but delivering about 880 MWe per reactor.
As was the case for the development of the Pickering design into the CANDU 6, the Bruce design was also developed into the similar CANDU 9. Like the CANDU 6, the CANDU 9 is essentially a re-packaging of the Bruce design so it can be built as a single-reactor unit. However, no CANDU 9 reactors have been built.
Generation III+ designs.
Through the 1980s and 90s the nuclear power market suffered a major crash, with few new plants being constructed in North America or Europe. Design work continued throughout, however, and a number of new design concepts were introduced that dramatically improved safety, capital costs, economics and overall performance. These Generation III+ and Generation IV machines became a topic of considerable interest in the early 2000s as it appeared a nuclear renaissance was underway and large numbers of new reactors would be built over the next decade.
AECL had been working on a design known as the ACR-700, using elements of the latest versions of the CANDU 6 and CANDU 9, with a design power of 700 MWe. During the nuclear renaissance, the upscaling seen in the earlier years re-expressed itself, and the ACR-700 was developed into the 1200 MWe ACR-1000. ACR-1000 is the next-generation (officially, "Generation III+") CANDU technology which makes some significant modifications to the existing CANDU design.
The main change, and the most radical among the CANDU generations, is the use of pressurized light water as the coolant. This significantly reduces the cost of implementing the primary cooling loop, which no longer has to be filled with expensive heavy water. The ACR-1000 uses about 1/3rd the heavy water needed in earlier generation designs. It also eliminates tritium production in the coolant loop, the major source of tritium leaks in operational CANDU designs. The redesign also allows for a slightly negative void reactivity, a major design goal of all GenIII+ machines.
However, the design also requires the use of slightly enriched uranium, enriched by about 1 or 2%. The main reason for this is to increase the burn-up ratio, allowing bundles to remain in the reactor longer, so that only a third as much spent fuel is produced. This also has effects on operational costs and timetables, as the refuelling frequency is reduced. As is the case with earlier CANDU designs, the ACR-1000 also offers online refuelling.
Outside of the reactor, the ACR-1000 has a number of design changes that are expected to dramatically lower capital and operational costs. Primary among these changes is the design lifetime of 60 years, which dramatically lowers the price of the electricity generated over the lifetime of the plant. The design also has an expected capacity factor of 90%. Higher pressure steam generators and turbines improve efficiency downstream of the reactor.
Many of the operational design changes were also applied to the existing CANDU 6 to produce the Enhanced CANDU 6. Also known as CANDU 6e or EC 6, this was an evolutionary upgrade of the CANDU 6 design with a gross output of 740 MWe per unit. The reactors are designed with a lifetime of over fifty years, with a mid-life program to replace some of the key components e.g. the fuel channels. The projected average annual capacity factor is more than ninety percent. Improvements to construction techniques (including modular, open-top assembly) decrease construction costs. The CANDU 6e is designed to operate at power settings as low as 50%, allowing them to adjust to load demand much better than the previous designs.
Sales efforts.
In Ontario.
By most measures, the CANDU is "the Ontario reactor". The system was developed almost entirely in Ontario, and only two experimental designs were built outside the province. Of the 29 commercial CANDU reactors built, 22 of these are in Ontario. Of the 22, a number of reactors have been removed from service. Two new CANDU reactors have been proposed for Darlington with Canadian government help with financing.
In Canada.
AECL has heavily marketed CANDU within Canada, but has found a limited reception. To date, only two non-experimental reactors have been built in other provinces, one each in Quebec and New Brunswick. Most other provinces have concentrated on hydro and coal-fired plants. The majority of Canadian provinces get a huge majority of power from hydro power. Alberta does not use extensive hydro power, uses no nuclear power, and uses enormous amounts of coal power.
Interest has been expressed in Western Canada, where CANDU reactors are being considered as heat and electricity sources for the energy-intensive oil sands extraction process, which currently uses natural gas. Energy Alberta Corporation announced 27 August 2007 that they had applied for a licence to build a new nuclear plant at Lac Cardinal (30 km west of the town of Peace River, Alberta), two ACR-1000 reactors going online in 2017 producing 2.2 gigawatt (electric). However, a 2007 parliamentary review suggested placing the development efforts on hold. The company was later purchased by Bruce Power, who proposed expanding the plant to four units of a total 4.4 gigawatts. However, these plans were upset and Bruce later withdrew its application for the Lac Cardinal, proposing instead a new site about 60 km away.
Foreign sales.
During the 1970s the international nuclear sales market was extremely competitive, with many national nuclear companies being supported by their governments' foreign embassy machines. In addition, the pace of construction in the United States had meant that cost overruns and delayed completion was generally over, and subsequent reactors would be cheaper. Canada, a relatively new player on the international market, had numerous disadvantages in these efforts. However, the CANDU was deliberately designed to reduce the need for very large machined parts, making it suitable for construction by countries without a major industrial base. Sales efforts have had their most success in countries that could not locally build designs from other firms.
In the late 1970s, AECL noted that each reactor sale would employ 3,600 Canadians and result in $300 million in balance-of-payments income. However, these sales efforts were aimed primarily at countries being run by dictatorships or similar, a fact that led to serious concerns in parliament. These efforts also led to a scandal when it was discovered millions of dollars had been given to foreign sales agents, with little or no record of who they were, or what they did to earn the money. This led to an Royal Canadian Mounted Police investigation after questions were raised about sales efforts in Argentina, and new regulations on full disclosure of fees for future sales.
CANDU's first success was the sale of early CANDU designs to India. In 1963, an agreement was signed for export of a 200 MWe power reactor based on the Douglas Point reactor. The success of the deal led to the 1966 sale of a second reactor of the same design. The first reactor, then known as RAPP-1 for "Rajasthan Atomic Power Project", began operation in 1972. However, a serious problem with cracking of the reactor's end shield led to the reactor being shut down for long periods, and the reactor was finally downrated to 100 MW. Construction of the RAPP-2 reactor was still underway when India detonated its first atomic bomb in 1974, leading to Canada ending nuclear dealings with the country. Part of the sales agreement was a technology transfer process. When Canada withdrew from development, India continued construction of CANDU-like plants across the country.
In Pakistan the Karachi Nuclear Power Plant with a gross capacity of 137MWe was built between 1966 and 1971.
In 1972, AECL submitted a design based on the Pickering plant to Argentina's Comision Nacional de Energia Atomica process, in partnership with the Italian company Italimpianti. High inflation during construction led to massive losses, and efforts to re-negotiate the deal were interrupted by the March 1976 coup led by General Videla. The Embalse Nuclear Power Station began commercial operation in January 1984. There have been ongoing negotiations to open more CANDU 6 reactors in the country, including a 2007 deal between Canada, China and Argentina, but to date no firm plans have been announced.
A licensing agreement with Romania was signed in 1977, selling the CANDU 6 design for $5 million per reactor for the first four reactors, and then $2 million each for the next twelve. In addition, Canadian companies would supply a varying amount of equipment for the reactors, about $100 million of the first reactor's $800 million price tag, and then falling over time. In 1980 Nicolae Ceaușescu asked for a modification to provide goods instead of cash, in exchange the amount of Canadian content was increased and a second reactor would be built with Canadian help. Economic troubles in the country worsened throughout the construction phase. The first reactor of the Cernavodă Nuclear Power Plant only came online in April 1996, a decade after its December 1985 predicted startup. Further loans were arranged for completion of the second reactor, which went online in November 2007.
In January 1975 a deal was announced for a single CANDU 6 reactor to be built in South Korea, now known as the Wolsong-1 Power Reactor. Construction started in 1977 and commercial operation began in April 1983. In December 1990 a further deal was announced for three additional units at the same site, which began operation in the period 1997–1999. However, South Korea also negotiated development and technology transfer deals with Westinghouse for their advanced System-80 reactor design, and all future development is based on locally built versions of this reactor.
An 1996 list of some countries and some sales efforts from an anti-nuclear activist for CANDU and related AECL designs can be found off Wikipedia .
Future sales.
In 2007 AECL submitted the ACR-1000 design to the British Generic Design Assessment process to evaluate reactors for a new British nuclear power station programme. However in 2008 AECL withdrew the design, stating it "is focusing its marketing and licensing resources for the advanced Candu reactor on the immediate needs of the Canadian domestic marketplace."
As of 2010, the only active sales effort is the ACR-1000 for Ontario's Darlington plant. At one time considered a "sure thing", the price was considered too high and plans to expand Darlington were dropped. However, these plans appear to be ongoing again, although AECL's bid has been interrupted by ongoing efforts to sell the reactor design division.
Economic performance.
The cost of electricity from any power plant can be calculated by roughly the same selection of factors. These include capital costs for construction and/or the payments on loans made to secure that capital, the cost of fuel on a per-watt-hour basis, and fixed and variable maintenance fees. In the case of nuclear power, one normally includes two additional costs, the cost of permanent waste disposal, and the cost of decommissioning the plant when its useful lifetime is over. Generally, the capital costs dominate the price of nuclear power, as the amount of power produced is so large that it overwhelms the cost of fuel and maintenance. The World Nuclear Association calculates that the cost of fuel, including all processing, accounts for less than one cent per kWh.
Information on economic performance on CANDU is somewhat lopsided; the majority of reactors are in Ontario, which is also the "most public" among the major CANDU operators, so their performance dominates the available information. Based on Ontario's record, the economic performance of the CANDU system is quite poor. Although much attention has been focussed on the problems with the Darlington plant, in fact, every CANDU design in Ontario went over budget by at least 25%, and average over 150% higher than estimated. Darlington was the worst offender, at 350% over budget, but this project was stopped in-progress thereby incurring additional interest charges during a period of high interest rates, which is a special situation that was not expected to repeat itself.
In the 1980s, the pressure tubes in the Pickering A reactors were replaced ahead of design life due to unexpected deterioration caused by hydrogen embrittlement. Extensive inspection and maintenance has avoided this problem in later reactors.
All the Pickering A and Bruce A reactors were shut down in 1999 in order to focus on restoring operational performance in the later generations at Pickering, Bruce, and Darlington. Before restarting the Pickering A reactors, OPG undertook a limited refurbishment program. The original cost and time estimates based on inadequate project scope development were greatly below the actual time and cost and it was determined that Pickering Units 2 and 3 would not be restarted for commercial reasons. Despite this refurbishment, the reactors have not performed well since the restart.
These overruns were repeated at Bruce, with Units 3 and 4 running 90% over budget. Similar overruns were experienced at Point Lepreau, and the planned refurbishment of the Gentilly 2 plant has been delayed until the fall of 2012, and currently there are serious plans to simply shut it down instead.
Based on the projected capital costs, and the low cost of fuel and in-service maintenance, in 1994 power from CANDU was predicted to be well under 5 cents/kWh. In 1998, Ontario Hydro calculated that the cost of generation from CANDU was 7.7 cents/kWh, whereas hydropower was only 1.1 cents, and their coal-fired plants were 4.3 cents. As Hydro received a regulated price averaging 6.3 cents/kWh for power in this period, the revenues from the other forms of generation were being used to fund the operating losses of the nuclear plants. The debt left over from the nuclear construction could not be included in the rate base until the reactors were declared in service, thereby exacerbating the total capital cost of construction with unpaid interest, at that time around $15 billion, and another $3.5 billion in debts throughout the system was held by a separate entity and repaid through a standing charge on electricity bills.
In 1999, Ontario Hydro was broken up and its generation facilities re-formed into Ontario Power Generation (OPG). In order to make the successor companies more attractive for private investors, $19.4 billion in "stranded debt" was placed in the control of the Ontario Electricity Financial Corporation. This debt is slowly paid down through a variety of sources, including a 0.7 cent/kWh tariff on all power, all income taxes paid by all operating companies, and all dividends paid by the OPG and Hydro One. Even with these sources of income, the amount of debt has grown on several occasions, and in 2010 stood at almost $15 billion. This is in spite of total payments on the order of $19 billion, ostensibly enough to have paid off the debt entirely if interest repayment requirements are ignored.
Darlington is currently in the process of considering a major re-build of several units, as it too is reaching its design mid-life time. The budget is currently estimated to be between $8.5 and $14 billion, and produce power at 6 to 8 cents/kWh. However, this prediction is based on three assumptions that appear to have never been met in operation: that the rebuild will be completed on-budget, that the system will operate at an average capacity utilization of 82%, and that the Ontario taxpayer will be on the hook for 100% of any cost overruns. Although Darlington Units 1, 3 and 4 have operated with an average lifetime annual capacity factor of 85% and Unit 2 with a capacity factor of 78%, refurbished units at Pickering and Bruce have lifetime capacity factors between 59 and 69%. However, this includes periods of several years while the units were shut down for the retubing and refurbishing. In 2009, Bruce A Units 3 and 4 had capacity factors of 80.5% and 76.7% respectively, in a year when they had a major Vacuum Building outage.
Active CANDU reactors.
Today there are 29 CANDU reactors in use around the world, and 13 "CANDU-derivatives" in India, developed from the CANDU design after India detonated a nuclear bomb in 1974 and Canada stopped nuclear dealings with India. The breakdown is:

</doc>
<doc id="7477" url="http://en.wikipedia.org/wiki?curid=7477" title="Cuitláhuac">
Cuitláhuac

Cuitláhuac () (c. 1476 – 1520) or Cuitláhuac (in Spanish orthography; in , honorific form Cuitlahuatzin) was the 10th "tlatoani" (ruler) of the Aztec city of Tenochtitlan for 80 days during the year Two Flint (1520).
Cuitláhuac was the eleventh son of the ruler Axayacatl and a younger brother of Moctezuma II, the previous ruler of Tenochtitlan. His mother's father, also called Cuitlahuac, had been ruler of Iztapalapa, and the younger Cuitláhuac also ruled there initially.
Cuitláhuac was made "tlatoani" of Tenochtitlan during the Spanish conquest of Mexico; After Pedro de Alvarado had ordered the massacre in the Main Temple, the Aztecs were very upset and started to fight and put a siege to the Spaniards. Hernán Cortés ordered Moctezuma to ask his people to stop fighting. Moctezuma told him that they would not listen to him and suggested Cortés free Cuitláhuac so that he could convince them to dispose of their arms and not fight anymore. Cortés then freed Cuitláhuac and once Cuitláhuac was free he led his people against the conquistadors. He succeeded and the Spaniards were driven out of Tenochtitlan on June 30, 1520. Cuitláhuac was ritually married to Moctezuma's eldest daughter, a ten- or eleven-year-old girl who later was called Isabel Moctezuma.
After having ruled for just 80 days, Cuitláhuac died of smallpox that had been introduced to the New World by the Europeans. His elder brother Matlatzincatzin, who had been "cihuacoatl" ("president"), resigned upon Cuitláhuac's death. As soon as Cuitláhuac died, Cuauhtémoc was made the next "tlatoani".
The modern Mexican municipality of Cuitláhuac, Veracruz and the Mexico City Metro station Metro Cuitláhuac are named in honor of Cuitláhuac. The asteroid 2275 Cuitláhuac is also named after this ruler.
There is an Avenue in Mexico City Called Cuitláhuac (Eje 3 Norte) that runs from Avenue Insurgentes to Avenue Mexico-Tacuba and that is part of an inner ring; also many streets in other towns and villages in Mexico are so called.
References.
 
 

</doc>
<doc id="7478" url="http://en.wikipedia.org/wiki?curid=7478" title="Cuauhtémoc">
Cuauhtémoc

Cuauhtémoc (, also known as Cuauhtemotzin, Guatimozin or Guatemoc; c. 1495) was the Mexica ruler ("tlatoani") of Tenochtitlan from 1520 to 1521. The name Cuāuhtemōc means "One That Has Descended Like an Eagle", commonly rendered in English as "Descending Eagle" as in the moment when an eagle folds its wings and plummets down to strike its prey, so this is a name that implies aggressiveness and determination.
Cuauhtémoc took power in 1520 as successor of Cuitláhuac and was a cousin of the former emperor Moctezuma II. His young wife, who would later be known as Isabel Moctezuma, was one of Moctezuma's daughters. He ascended to the throne when he was 25 years of age, as his city was being besieged by the Spanish and devastated by an epidemic of smallpox brought to the New World by Spanish invaders. Probably after the killings in the main temple, there were few Aztec captains available to take the position.
Early life and rule.
Cuauhtemoc was the son of emperor Ahuitzotl and may well have attended the last New Fire ceremony marking the beginning of a new 52 year cycle in the Aztec calendar. Following education and military service, he was ruler of Tlatelolco (with the title "cuauhtlatoani" "eagle ruler". With the death of Moctezuma II, Moctezuma's brother Cuitlahuac succeeded as ruler, but his death of smallpox shortly thereafter vacated the rulership once again. In keeping with traditional practice, the most able candidate amongst the high nobility was chosen, so that with a unanimous vote, Cuauhtemoc assumed the rulership. Tenochtitlan was increasingly isolated militarily and largely faced the Spanish invaders and their indigenous allies alone.
Cuauhtémoc called for reinforcements from the countryside to aid the defense of Tenochtitlán, after eighty days of warfare against the Spanish. Of all the Nahuas, only Tlatelolcas remained loyal, and the surviving Tenochcas looked for refuge in Tlatelolco, where even women took part in the battle. Cuauhtémoc was captured on August 13, 1521, while fleeing Tenochtitlán by crossing Lake Texcoco in disguise with his wife, family, and friends. He surrendered to Hernán Cortés along with the surviving "pipiltin" (nobles) and, according to Spanish sources, he offered Cortés his knife and asked to be killed.
According to the same Spanish accounts, Cortés refused this offer and treated his foe magnanimously. "You have defended your capital like a brave warrior," he declared, "A Spaniard knows how to respect valor, even in an enemy." At Cuauhtémoc's request, Cortés also allowed the defeated Mexica to depart the city unmolested. Subsequently, however, when the booty found did not measure up to the Spaniards' expectations, Cuauhtémoc was tortured in an unsuccessful attempt to discover its whereabouts. On the statue to Cuauhtemoc erected on the Paseo de la Reforma in Mexico City, there is a bas relief showing the Spaniards' torture of the emperor. Eventually some gold was recovered, though far less than Cortés and his men expected.
Cuauhtémoc continued to rule under the Spanish, keeping the title of tlatoani., He ordered the construction of a renaissance-style two-storied stone palace in Tlatelolco, where he resettled after the destruction of Mexico City; the building survived and was known as the Tecpan or palace.
Execution.
In 1525, Cortés took Cuauhtémoc and several other indigenous nobles on his expedition to Honduras, fearing that Cuauhtémoc could have led an insurrection in his absence. While the expedition was stopped in the Chontal Maya capital of Itzamkanac, known as Acalan in Nahuatl, Cortés had Cuauhtémoc executed for allegedly conspiring to kill him and the other Spaniards.
There are a number of discrepancies in the various accounts of the event. According to Cortés himself, on 27 February 1525 it was revealed to him by a citizen of Tenochtitlan named Mexicalcingo that Cuauhtémoc, Coanacoch (the ruler of Texcoco) and Tetlepanquetzal (the ruler of Tlacopan) were plotting his death. Cortés interrogated them until each confessed, and then had Cuauhtémoc, Tetlepanquetzal, and another lord named Tlacatlec hanged. Cortés wrote that the other lords would be too frightened to plot against him again, as they believed he had uncovered the plan through magic powers. Cortés's account is supported by the historian Francisco López de Gómara.
According to Bernal Díaz del Castillo, a conquistador serving under Cortés who recorded his experiences in his book "The True History of the Conquest of New Spain", the supposed plot was revealed by two men, named Tapia and Juan Velásquez. Díaz portrays the executions as unjust and based on no evidence, and admits to having liked Cuauhtémoc personally. He also records Cuauhtémoc giving the following speech to Cortés, through his interpreter Malinche:
Díaz wrote that afterwards, Cortés suffered from insomnia due to guilt, and badly injured himself while wandering at night.
Fernando de Alva Cortés Ixtlilxóchitl, a Mestizo historian and descendant of Coanacoch, wrote an account of the executions in the 17th century partly based on Texcocan oral tradition. According to Ixtlilxóchitl the three lords were joking cheerfully with each other, due to a rumor that Cortés had decided to return the expedition to Mexico, when Cortés asked a spy to tell him what they were talking about. The spy reported honestly, but Cortés invented the plot himself. Cuauhtémoc, Coanacoch and Tetlepanquetzal were all hanged, as well as eight others. However, Cortés cut down Coanacoch, the last to be hanged, after his brother began rallying his warriors. Coanacoch did not have long to enjoy his reprieve—Ixtlilxóchitl wrote that he died a few days later.
Tlacotzin, Cuauhtémoc's "cihuacoatl", was appointed his successor as "tlatoani". He died the next year before returning to Tenochtitlan.
Cuauhtemoc's Bones.
The modern-day town of Ixcateopan in the state of Guerrero is home to an ossuary purportedly containing Cuauhtémoc's remains. Archeologist Eulalia Guzmán, a "passionate indigenista", excavated the bones in 1949, which were discovered shortly after bones found in Mexico City of Cortés had been authenticated by the Instituto Nacional de Antropología e Historia (INAH). Initially Mexican scholars congratulated Guzmán, but after a similar examination by scholars at INAH, their authenticity as Cuauhtemoc's could not be verified. This finding caused a public uproar. A panel assembled by Guzmán gave support to the initial contention. The Secretariat of Public Education (SEP) had another panel examine the bones, which gave support to INAH's original finding, but did not report on the finding publicly. A scholarly monograph on the controversy was published in 2011.
Legacy.
Cuauhtemoc is the embodiment of indigenist nationalism in Mexico, being the Aztec emperor who resisted the conquest by the Spanish (and their indigenous allies). He is honored by a statue on the Paseo de la Reforma, his face has appeared on Mexican banknotes, and he is celebrated in paintings, music, and popular culture.
Many places in Mexico are named in honour of Cuauhtémoc. These include Ciudad Cuauhtémoc in Chihuahua and the Cuauhtémoc borough of the Mexican Federal District, as well as Ciudad Cuauhtémoc, in the state of Veracruz. 
There is a Cuauhtémoc station on Line 1 of the Mexico City metro as well as one for Moctezuma, but none for Hernán Cortés. There is also a metro stop named for him Monterrey Metrorrey.
Cuauhtémoc is also one of the few non-Spanish given names for Mexican boys that is perennially popular.
Cuauhtémoc Cárdenas Solórzano, a prominent Mexican politician, is named after him. In the Aztec campaign of the PC game ', the player plays as Cuauhtémoc, despite the name "Montezuma" for the campaign itself, and Cuauhtémoc narrates the openings and closings to each scenario. In the next installment to the series, ', Cuauhtémoc was the leader of Aztecs. The Mexican football player Cuauhtémoc Blanco was also named after him.
In the 1996 Rage Against The Machine single "People of the Sun", lyricist Zack De La Rocha rhymes "When the fifth sun sets get back reclaimed, The spirit of Cuauhtémoc alive an untamed".
Cuauhtémoc, in the name Guatemoc, is portrayed sympathetically in the adventure novel "Montezuma's Daughter", by H. Rider Haggard. First appearing in Chapter XIV, he becomes friends with the protagonist after they save each other's lives. His coronation, torture, and death are described in the novel.
External links.
 

</doc>
<doc id="7480" url="http://en.wikipedia.org/wiki?curid=7480" title="Cross section (physics)">
Cross section (physics)

A cross section is the effective area that governs the probability of some scattering or absorption event. Together with particle density and path length, it can be used to predict the total scattering probability via the Beer–Lambert law.
In nuclear and particle physics, the concept of a cross section is used to express the likelihood of interaction between particles.
When particles in a beam are thrown against a foil made of a certain substance, the "cross section" formula_1 is a hypothetical area measure around the target particles of the substance (usually its atoms) that represents a surface. If a particle of the beam crosses this surface, there will be some kind of interaction.
The term is derived from the purely classical picture of (a large number of) point-like projectiles directed to an area that includes a solid target. Assuming that an interaction will occur (with 100% probability) if the projectile hits the solid, and not at all (0% probability) if it misses, the total interaction probability for the single projectile will be the ratio of the area of the section of the solid (the "cross section", represented by formula_1) to the total targeted area.
This basic concept is then extended to the cases where the interaction probability in the targeted area assumes intermediate values - because the target itself is not homogeneous, or because the interaction is mediated by a non-uniform field. A particular case is scattering.
Scattering.
The scattering cross-section, "σ"scat, is a hypothetical area which describes the likelihood of light (or other radiation) being scattered by a particle. In general, the scattering cross-section is different from the geometrical cross-section of a particle, and it depends upon the wavelength of light and the permittivity, shape and size of the particle. The total amount of scattering in a sparse medium is determined by the product of the scattering cross-section and the number of particles present. In terms of area, the "total cross-section" (σ) is the sum of the cross-sections due to absorption, scattering and luminescence
The total cross-section is related to the absorbance of the light intensity through Beer-Lambert's law, which says absorbance is proportional to concentration: formula_4, where "C" is the concentration as a number density, "A"λ is the absorbance at a given wavelength "λ", and formula_5 is the path length. The extinction or absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance:
Nuclear physics.
In nuclear physics, it is convenient to express the probability of a particular event by a cross section. Statistically, the centers of the atoms in a thin foil can be considered as points evenly distributed over a plane. The center of an atomic projectile striking this plane has geometrically a definite probability of passing within a certain distance formula_7 of one of these points. In fact, if there are formula_8 atomic centers in an area formula_9 of the plane, this probability is formula_10, which is simply the ratio of the aggregate area of circles of radius formula_7 drawn around the points to the whole area. If we think of the atoms as impenetrable steel discs and the impinging particle as a bullet of negligible diameter, this ratio is the probability that the bullet will strike a steel disc, i.e., that the atomic projectile will be stopped by the foil. If it is the fraction of impinging atoms getting through the foil which is measured, the result can still be expressed in terms of the equivalent stopping cross section of the atoms. This notion can be extended to any interaction between the impinging particle and the atoms in the target. For example, the probability that an alpha particle striking a beryllium target will produce a neutron can be expressed as the equivalent cross section of beryllium for this type of reaction.
Rate (particle physics).
In scattering theory, particle physics and nuclear physics, the rate at which a specific subatomic particle reaction occurs is a physical quantity measuring the number of reactions per unit time.
Partial cross section.
For a particle beam (say of neutrons, pions) incident on a target (liquid hydrogen), for each type of reaction in the scattering process labelled by an index "r" = 1, 2, 3..., it is calculated from:
where "N" is the number of target particles, illuminated by the beam containing "n" particles per unit volume in the beam (number density of particles) traveling with average velocity "v" in the rest frame of the target, and these two quantities combine into the flux of the beam "J" = "nv". The cross section of the reaction is "σr". Since the beam flux has dimensions of [length]−2·[time]−1 and "σr" has dimensions of [length]2 while "N" is a dimensionless number, the rate "W" has the dimensions of reciprocal time - which intuitively represents a frequency of recurring events.
The above formula assumes the following:
These conditions are usually met in experiments, which allows for a very simple calculation of rate.
Sometimes the rate per unit target particle, or rate density, is more useful. For reaction "r":
Total cross section.
The cross section "σr" is specifically for "one" type of reaction, and is called the partial cross section. The total cross section, and corresponding total rate of the reaction, can be found by summing over the cross sections and rates for each reaction:
Differential cross section.
In terms of the differential cross section "dσr"("θ", "φ") as a function of spherical polar angles "θ" and "φ" for reaction "r", the differential rate is:
where dΩ = "d"(cos"θ")"dφ" is the solid angle element in the vicinity of the event with vertex at the point of scattering. Integrating over "θ" and "φ" returns the rate for reaction "r":

</doc>
<doc id="7482" url="http://en.wikipedia.org/wiki?curid=7482" title="Christian mythology">
Christian mythology

Christian mythology is the body of myths associated with Christianity. Within contemporary Christianity, the appropriateness of describing Christian narratives as “myth” is a matter of disagreement. George Every claims that the existence of "myths in the Bible would now be admitted by nearly everyone", including "probably all Roman Catholics and a majority of Protestants". As examples of biblical myths, Every cites the creation account in Genesis 1 and 2 and the story of Eve's temptation.
A number of modern Christian writers, such as C.S. Lewis, have described elements of Christianity, particularly the story of Christ, as "true myth". However, other Christian authors assert that Christian narratives should not be categorized as myth.
Christian attitudes toward myth.
In ancient Greek, "muthos", from which the English word "myth" derives, meant "story, narrative." By the time of Christianity, "muthos" had started to take on the connotations of "fable, fiction, lie". Early Christians contrasted their sacred stories with "myths", by which they meant false and pagan stories.
Within contemporary Christianity, the appropriateness of describing Christian narratives as “myth” is a matter of disagreement. George Every claims that the existence of "myths in the Bible would now be admitted by nearly everyone", including "probably all Roman Catholics and a majority of Protestants". As examples of Biblical myths, Every cites the creation account in Genesis 1 and 2 and the story of Eve's temptation. A number of modern Christian writers, such as C.S. Lewis, have described elements of Christianity, particularly the story of Christ, as "myth" which is also "true". However, other Christian authors assert that Christian narratives should not be categorized as "myth". Opposition to the term "myth" stems from a variety of sources: the association of the term "myth" with polytheism, the use of the term "myth" to indicate falsehood or non-historicity, and the lack of an agreed-upon definition of "myth".
Historical development.
Old Testament.
According to Bernard McGinn, "mythic patterns" such as "the primordial struggle between good and evil" appear in passages throughout the Hebrew Bible, including passages that describe historical events. Citing Paul Ricoeur, McGinn argues that a distinctive characteristic of the Hebrew Bible is its "reinterpretation of myth on the basis of history". As an example, McGinn cites the apocalypse in the Book of Daniel, which he sees as a record of historical events presented as a prophecy of future events and expressed in terms of "mythic structures", with "the Hellenistic kingdom figured as a terrifying monster that cannot but recall [the Near Eastern pagan myth of] the dragon of chaos".
Mircea Eliade argues that the imagery used in some parts of the Hebrew Bible reflects a "transfiguration of history into myth". For example, Eliade says, the portrayal of Nebuchadnezzar as a dragon in Jeremiah 51:34 is a case in which the Hebrews "interpreted contemporary events by means of the very ancient cosmogonico-heroic myth" of a battle between a hero and a dragon.
According to scholars including Neil Forsyth and John L. McKenzie, the Old Testament incorporates stories, or fragments of stories, from extra-biblical mythology. According to the "New American Bible", a Catholic Bible translation produced by the Confraternity of Christian Doctrine, the story of the Nephilim in Genesis 6:1-4 "is apparently a fragment of an old legend that had borrowed much from ancient mythology", and the "sons of God" mentioned in that passage are "celestial beings of mythology". The "New American Bible" also says that Psalm 93 alludes to "an ancient myth" in which God battles a personified Sea. Some scholars have identified the biblical creature Leviathan as a monster from Canaanite mythology. According to Howard Schwartz, "the myth of the fall of Lucifer" existed in fragmentary form in Isaiah 14:12 and other ancient Jewish literature; Schwartz claims that the myth originated from "the ancient Canaanite myth of Athtar, who attempted to rule the throne of Ba'al, but was forced to descend and rule the underworld instead".
Some scholars have argued that the calm, orderly, monotheistic creation story in Genesis 1 can be interpreted as a reaction against the creation myths of other Near Eastern cultures. In connection with this interpretation, David and Margaret Leeming describe Genesis 1 as a "demythologized myth", and John L. McKenzie asserts that the writer of Genesis 1 has "excised the mythical elements" from his creation story.
Perhaps the most famous topic in the Bible that could possibly be connected with mythical origins is the topic of Heaven (or the sky) as the place where God (or angels, or the saints) resides, with stories such as the ascension of Elijah (who disappeared in the sky), war of man with an angel, flying angels. Even in the New Testament Saint Paul is said to "have visited the third heaven", and Jesus was portrayed in several books as going to return from Heaven on a cloud, in the same way He ascended thereto. The official text repeated by the attendees during Roman Catholic mass (the Apostles' Creed) contains the words "He ascended into Heaven, and is Seated at the Right Hand of God, The Father. From thence He will come again to judge the living and the dead". Medieval cosmology adapted its view of the Cosmos to conform with these scriptures, in the concept of celestial spheres (later attacked, amongst others, by Giordano Bruno). Some famous opponents of religion, including John Lennon and Stephen Hawking, mentioned this in their public works.
New Testament and early Christianity.
According to a number of scholars, the Christ story contains mythical themes such as descent to the underworld, the heroic monomyth, and the "dying god" (see section below on "mythical themes and types").
Some scholars have argued that the Book of Revelation incorporates imagery from ancient mythology. According to the "New American Bible", the image in Revelation 12:1-6 of a pregnant woman in the sky, threatened by a dragon, "corresponds to a widespread myth throughout the ancient world that a goddess pregnant with a savior was pursued by a horrible monster; by miraculous intervention, she bore a son who then killed the monster". Bernard McGinn suggests that the image of the two Beasts in Revelation stems from a "mythological background" involving the figures of Leviathan and Behemoth.
The Pastoral Epistles contain denunciations of "myths" ("muthoi"). This may indicate that Rabbinic or gnostic mythology was popular among the early Christians to whom the epistles were written and that the epistles' author was attempting to resist that mythology.
The Sibylline oracles contain predictions that the dead Roman Emperor Nero, infamous for his persecutions, would return one day as an Antichrist-like figure. According to Bernard McGinn, these parts of the oracles were probably written by a Christian and incorporated "mythological language" in describing Nero's return.
Middle Ages.
According to Mircea Eliade, the Middle Ages witnessed "an upwelling of mythical thought" in which each social group had its own "mythological traditions". Often a profession had its own "origin myth" which established models for members of the profession to imitate; for example, the knights tried to imitate Lancelot or Parsifal. The medieval trouveres developed a "mythology of woman and Love" which incorporated Christian elements but, in some cases, ran contrary to official church teaching.
George Every includes a discussion of medieval legends in his book "Christian Mythology". Some medieval legends elaborated upon the lives of Christian figures such as Christ, the Virgin Mary, and the saints. For example, a number of legends describe miraculous events surrounding Mary's birth and her marriage to Joseph.
In many cases, medieval mythology appears to have inherited elements from myths of pagan gods and heroes. According to Every, one example may be "the myth of St. George" and other stories about saints battling dragons, which were "modelled no doubt in many cases on older representations of the creator and preserver of the world in combat with chaos". Eliade notes that some "mythological traditions" of medieval knights, namely the Arthurian cycle and the Grail theme, combine a veneer of Christianity with traditions regarding the Celtic Otherworld. According to Lorena Laura Stookey, "many scholars" see a link between stories in "Irish-Celtic mythology" about journeys to the Otherworld in search of a cauldron of rejuvenation and medieval accounts of the quest for the Holy Grail.
According to Eliade, "eschatological myths" became prominent during the Middle Ages during "certain historical movements". These eschatological myths appeared "in the Crusades, in the movements of a Tanchelm and an Eudes de l'Etoile, in the elevation of Fredrick II to the rank of Messiah, and in many other collective messianic, utopian, and prerevolutionary phenomena". One significant eschatological myth, introduced by Gioacchino da Fiore's theology of history, was the "myth of an imminent third age that will renew and complete history" in a "reign of the Holy Spirit"; this "Gioacchinian myth" influenced a number of messianic movements that arose in the late Middle Ages.
Renaissance and Reformation.
During the Renaissance, there arose a critical attitude that sharply distinguished between apostolic tradition and what George Every calls "subsidiary mythology"—popular legends surrounding saints, relics, the cross, etc.—suppressing the latter.
The works of Renaissance writers often included and expanded upon Christian and non-Christian stories such as those of creation and the Fall. Rita Oleyar describes these writers as "on the whole, reverent and faithful to the primal myths, but filled with their own insights into the nature of God, man, and the universe". An example is John Milton's "Paradise Lost", an "epic elaboration of the Judeo-Christian mythology" and also a "veritable encyclopedia of myths from the Greek and Roman tradition".
According to Cynthia Stewart, during the Reformation, the Protestant reformers used "the founding myths of Christianity" to critique the church of their time.
Every argues that "the disparagement of myth in our own civilization" stems partly from objections to perceived idolatry, objections which intensified in the Reformation, both among Protestants and among Catholics reacting against the classical mythology revived during the Renaissance.
Enlightenment.
The philosophes of the Enlightenment used criticism of myth as a vehicle for veiled criticisms of the Bible and the church. According to Bruce Lincoln, the philosophes "made irrationality the hallmark of myth and constituted philosophy—rather than the Christian "kerygma"—as the antidote for mythic discourse. By implication, Christianity could appear as a more recent, powerful, and dangerous instance of irrational myth".
Modern period.
Some commentators have categorized a number of modern fantasy works as "Christian myth" or "Christian mythopoeia". Examples include the fiction of C.S. Lewis, Madeleine L'Engle, J.R.R. Tolkien, and George MacDonald.
In "The Eternal Adam and the New World Garden", written in 1968, David Noble argued that the Adam figure had been "the central myth in the American novel since 1830". As examples, he cites the works of Cooper, Hawthorne, Melville, Twain, Hemingway, and Faulkner.
Mythical themes and types.
Ascending the mountain.
According to Lorena Laura Stookey, many myths feature sacred mountains as "the sites of revelations": "In myth, the ascent of the holy mountain is a spiritual journey, promising purification, insight, wisdom, or knowledge of the sacred". As examples of this theme, Stookey includes the revelation of the Ten Commandments on Mount Sinai, Christ's ascent of a mountain to deliver his Sermon on the Mount, and Christ's ascension into Heaven from the Mount of Olives.
Axis mundi.
Many mythologies involve a "world center", which is often the sacred place of creation; this center often takes the form of a tree, mountain, or other upright object, which serves as an "axis mundi" or axle of the world. A number of scholars have connected the Christian story of the crucifixion at Golgotha with this theme of a cosmic center. In his "Creation Myths of the World", David Leeming argues that, in the Christian story of the crucifixion, the cross serves as "the "axis mundi", the center of a new creation".
According to a tradition preserved in Eastern Christian folklore, Golgotha was the summit of the cosmic mountain at the center of the world and the location where Adam had been both created and buried. According to this tradition, when Christ is crucified, his blood falls on Adam's skull, buried at the foot of the cross, and redeems him. George Every discusses the connection between the cosmic center and Golgotha in his book "Christian Mythology", noting that the image of Adam's skull beneath the cross appears in many medieval representations of the crucifixion.
In "Creation Myths of the World", Leeming suggests that the Garden of Eden may also be considered a world center.
Combat myth.
Many Near Eastern religions include a story about a battle between a divine being and a dragon or other monster representing chaos—a theme found, for example, in the "Enuma Elish". A number of scholars call this story the "combat myth". A number of scholars have argued that the ancient Israelites incorporated the combat myth into their religious imagery, such as the figures of Leviathan and Rahab, the Song of the Sea, Isaiah 51:9-10's description of God's deliverance of his people from Babylon, and the portrayals of enemies such as Pharaoh and Nebuchadnezzar. The idea of Satan as God's opponent may have developed under the influence of the combat myth. Scholars have also suggested that the Book of Revelation uses combat myth imagery in its descriptions of cosmic conflict.
Descent to the underworld.
According to Christian tradition, Christ descended to hell after his death, in order to free the souls there; this event is known as the harrowing of hell. This story is narrated in the Gospel of Nicodemus and may be the meaning behind 1 Peter 3:18-22. According to David Leeming, writing in "The Oxford Companion to World Mythology", the harrowing of hell is an example of the motif of the hero's descent to the underworld, which is common in many mythologies.
Dying god.
Many myths, particularly from the Near East, feature a god who dies and is resurrected; this figure is sometimes called the "dying god". An important study of this figure is James George Frazer's "The Golden Bough", which traces the dying god theme through a large number of myths. The dying god is often associated with fertility. A number of scholars, including Frazer, have suggested that the Christ story is an example of the "dying god" theme. In the article "Dying god" in "The Oxford Companion to World Mythology", David Leeming notes that Christ can be seen as bringing fertility, though of a spiritual as opposed to physical kind.
In his 2006 homily for Corpus Christi, Pope Benedict XVI noted the similarity between the Christian story of the resurrection and pagan myths of dead and resurrected gods: "In these myths, the soul of the human person, in a certain way, reached out toward that God made man, who, humiliated unto death on a cross, in this way opened the door of life to all of us."
Flood myths.
Many cultures have myths about a flood that cleanses the world in preparation for rebirth. Such stories appear on every inhabited continent on earth. An example is the biblical story of Noah. In "The Oxford Companion to World Mythology", David Leeming notes that, in the Bible story, as in other flood myths, the flood marks a new beginning and a second chance for creation and humanity.
Founding myths.
According to Sandra Frankiel, the records of "Jesus' life and death, his acts and words" provide the "founding myths" of Christianity. Frankiel claims that these founding myths are "structurally equivalent" to the creation myths in other religions, because they are "the pivot around which the religion turns to and which it returns", establishing the "meaning" of the religion and the "essential Christian practices and attitudes". Tom Cain uses the expression "founding myths" more broadly, to encompass such stories as those of the War in Heaven and the fall of man; according to Cain, "the disastrous consequences of disobedience" is a pervasive theme in Christian founding myths.
Hero myths.
In his influential work "The Myth of the Birth of the Hero", Otto Rank argued that the births of many mythical heroes follow a common pattern. Rank includes the story of Christ's birth as a representative example of this pattern.
According to Mircea Eliade, one pervasive mythical theme associates heroes with the slaying of dragons, a theme which Eliade traces back to "the very ancient cosmogonico-heroic myth" of a battle between a divine hero and a dragon. He cites the Christian legend of Saint George as an example of this theme. An example from the later Middle Ages is Dieudonné de Gozon, third Grand Master of the Knights of Rhodes, famous for slaying the dragon of Malpasso. Eliade writes, "Legend, as was natural, bestowed upon him the attributes of St. George, famed for his victorious fight with the monster. […] In other words, by the simple fact that he was regarded as a hero, de Gozon was identified with a category, an archetype, which […] equipped him with a mythical biography from which it was "impossible" to omit combat with a reptilian monster."
In the "Oxford Companion to World Mythology", David Leeming lists Moses, Jesus, and King Arthur as examples of the "heroic monomyth", calling the Christ story "a particularly complete example of the heroic monomyth". Leeming regards resurrection as a common part of the heroic monomyth, in which the heroes are resurrected, often as sources of "material or spiritual food for their people"; in this connection, Leeming notes that Christians regard Jesus as the "bread of life".
In terms of values, Leeming contrasts "the myth of Jesus" with the myths of other "Christian heroes such as St. George, Roland, el Cid, and even King Arthur"; the latter hero myths, Leeming argues, reflect the survival of pre-Christian heroic values—"values of military dominance and cultural differentiation and hegemony"—more than the values expressed in the Christ story.
Paradise.
Many religious and mythological systems contain myths about a paradise. Many of these myths involve the loss of a paradise that existed at the beginning of the world. Some scholars have seen in the story of the Garden of Eden an instance of this general motif.
Sacrifice.
Sacrifice is an element in many religious traditions and often represented in myths. In "The Oxford Companion to World Mythology", David Leeming lists the story of Abraham and Isaac and the story of Christ's death as examples of this theme. Wendy Doniger describes the gospel accounts as a "meta-myth" in which Jesus realizes that he is part of a "new myth [...] of a man who is sacrificed in hate" but "sees the inner myth, the old myth of origins and acceptance, the myth of a god who sacrifices himself in love".
Attitudes toward time.
According to Mircea Eliade, many traditional societies have a cyclic sense of time, periodically reenacting mythical events. Through this reenactment, these societies achieve an "eternal return" to the mythical age. According to Eliade, Christianity retains a sense of cyclical time, through the ritual commemoration of Christ's life and the imitation of Christ's actions; Eliade calls this sense of cyclical time a "mythical aspect" of Christianity.
However, Judeo-Christian thought also makes an "innovation of the first importance", Eliade says, because it embraces the notion of linear, historical time; in Christianity, "time is no longer [only] the circular Time of the Eternal Return; it has become linear and irreversible Time". Summarizing Eliade's statements on this subject, Eric Rust writes, "A new religious structure became available. In the Judaeo-Christian religions—Judaism, Christianity, Islam—history is taken seriously, and linear time is accepted. [...] The Christian myth gives such time a beginning in creation, a center in the Christ-event, and an end in the final consummation."
Heinrich Zimmer also notes Christianity's emphasis on linear time; he attributes this emphasis specifically to the influence of Saint Augustine's theory of history. Zimmer does not explicitly describe the cyclical conception of time as itself "mythical" per se, although he notes that this conception "underl[ies] Hindu mythology".
Neil Forsyth writes that "what distinguishes both Jewish and Christian religious systems [...] is that they elevate to the sacred status of myth narratives that are situated in historical time".
Legacy.
Concepts of progress.
According to Carl Mitcham, "the Christian mythology of progress toward transcendent salvation" created the conditions for modern ideas of scientific and technological progress. Hayden White describes "the myth of Progress" as the "secular, Enlightenment counterpart" of "Christian myth". Reinhold Niebuhr described the modern idea of ethical and scientific progress as "really a rationalized version of the Christian myth of salvation".
Political and philosophical ideas.
According to Mircea Eliade, the medieval "Gioacchinian myth [...] of universal renovation in a more or less imminent future" has influenced a number of modern theories of history, such as those of Lessing (who explicitly compares his views to those of medieval "enthusiasts"), Fichte, Hegel, and Schelling, and has also influenced a number of Russian writers.
Calling Marxism "a truly messianic Judaeo-Christian ideology", Eliade writes that Marxism "takes up and carries on one of the great eschatological myths of the Middle Eastern and Mediterranean world, namely: the redemptive part to be played by the Just (the 'elect', the 'anointed', the 'innocent', the 'missioners', in our own days the proletariat), whose sufferings are invoked to change the ontological status of the world".
In his article "The Christian Mythology of Socialism", Will Herberg argues that socialism inherits the structure of its ideology from the influence of Christian mythology upon western thought.
In "The Oxford Companion to World Mythology", David Leeming claims that Judeo-Christian messianic ideas have influenced 20th-century totalitarian systems, citing Soviet Communism as an example.
According to Hugh S. Pyper, the biblical "founding myths of the Exodus and the exile, read as stories in which a nation is forged by maintaining its ideological and racial purity in the face of an oppressive great power", entered "the rhetoric of nationalism throughout European history", especially in Protestant countries and smaller nations.
Christmas stories in popular culture.
See Secular Christmas stories, Christmas in the media and Christmas in literature.

</doc>
<doc id="7484" url="http://en.wikipedia.org/wiki?curid=7484" title="Company (disambiguation)">
Company (disambiguation)

A company is a group of more than one persons to carry out an enterprise and so a form of business organization.
Company may also refer to:
In titles and proper names:

</doc>
<doc id="7485" url="http://en.wikipedia.org/wiki?curid=7485" title="Corporation">
Corporation

A corporation is a separate legal entity that has been incorporated either directly through legislation or through a registration process established by law. Incorporated entities have legal rights and liabilities that are distinct from their employees, shareholders, and members, and may conduct business as either a profit-seeking business or not-for-profit. Early incorporated entities were established by charter (i.e. by an "ad hoc" act granted by a monarch or passed by a parliament or legislature). Most jurisdictions now allow the creation of new corporations through registration. Registered corporations have legal personality and are owned by shareholders whose liability is limited to their investment. Shareholders do not typically actively manage a corporation; shareholders instead elect or appoint a board of directors to control the corporation in a fiduciary capacity.
In American English the word corporation is most often used to describe large business corporations. In British English and in the Commonwealth countries, the term company is more widely used to describe the same sort of entity while the word corporation encompasses all incorporated entities. In American English, the word company can include entities such as partnerships that would not be referred to as companies in British English as they are not a separate legal entity.
Despite not being human beings, corporations, as far as the law is concerned, are legal persons, and have many of the same rights and responsibilities as natural people do. Corporations can exercise human rights against real individuals and the state, and they can themselves be responsible for human rights violations. Corporations can be "dissolved" either by statutory operation, order of court, or voluntary action on the part of shareholders. Insolvency may result in a form of corporate failure, when creditors force the liquidation and dissolution of the corporation under court order, but it most often results in a restructuring of corporate holdings. Corporations can even be convicted of criminal offenses, such as fraud and manslaughter. However corporations are not considered living entities in the way that humans are.
History.
The word "corporation" derives from "corpus", the Latin word for body, or a "body of people." By the time of Justinian (reigned 527–565), Roman Law recognized a range of corporate entities under the names "universitas", "corpus" or "collegium". These included the state itself (the "populus Romanus"), municipalities, and such private associations as sponsors of a religious cult, burial clubs, political groups, and guilds of craftsmen or traders. Such bodies commonly had the right to own property and make contracts, to receive gifts and legacies, to sue and be sued, and, in general, to perform legal acts through representatives. Private associations were granted designated privileges and liberties by the emperor.
Entities which carried on business and were the subjects of legal rights were found in ancient Rome, and the Maurya Empire in ancient India. In medieval Europe, churches became incorporated, as did local governments, such as the Pope and the City of London Corporation. The point was that the incorporation would survive longer than the lives of any particular member, existing in perpetuity. The alleged oldest commercial corporation in the world, the Stora Kopparberg mining community in Falun, Sweden, obtained a charter from King Magnus Eriksson in 1347.
In medieval times traders would do business through common law constructs, such as partnerships. Whenever people acted together with a view to profit, the law deemed that a partnership arose. Early guilds and livery companies were also often involved in the regulation of competition between traders.
Mercantilism.
Many European nations chartered corporations to lead colonial ventures, such as the Dutch East India Company or the Hudson's Bay Company. These chartered companies became the progenitors of the modern corporation. Acting under a charter sanctioned by the Dutch government, the Dutch East India Company defeated Portuguese forces and established itself in the Moluccan Islands in order to profit from the European demand for spices. Investors in the VOC were issued paper certificates as proof of share ownership, and were able to trade their shares on the original Amsterdam stock exchange. Shareholders are also explicitly granted limited liability in the company's royal charter.
In England, the government created corporations under a Royal Charter or an Act of Parliament with the grant of a monopoly over a specified territory. The best known example, established in 1600, was the British East India Company. Queen Elizabeth I granted it the exclusive right to trade with all countries to the east of the Cape of Good Hope. Corporations at this time would essentially act on the government's behalf, bringing in revenue from its exploits abroad. Subsequently the Company became increasingly integrated with British military and colonial policy, just as most UK corporations were essentially dependent on the British navy's ability to control trade routes.
Labeled by both contemporaries and historians as "the grandest society of merchants in the universe", the British East India Company would come to symbolize the dazzlingly rich potential of the corporation, as well as new methods of business that could be both brutal and exploitative. On 31 December 1600, the English monarchy granted the company a 15-year monopoly on trade to and from the East Indies and Africa. By 1611, shareholders in the East India Company were earning an almost 150% return on their investment. Subsequent stock offerings demonstrated just how lucrative the Company had become. Its first stock offering in 1613–1616 raised £418,000, and its second offering in 1617–1622 raised £1.6 million.
A similar chartered company, the South Sea Company, was established in 1711 to trade in the Spanish South American colonies, but met with less success. The South Sea Company's monopoly rights were supposedly backed by the Treaty of Utrecht, signed in 1713 as a settlement following the War of Spanish Succession, which gave the United Kingdom an "assiento" to trade in the region for thirty years. In fact the Spanish remained hostile and let only one ship a year enter. Unaware of the problems, investors in the UK, enticed by extravagant promises of profit from company promoters bought thousands of shares. By 1717, the South Sea Company was so wealthy (still having done no real business) that it assumed the public debt of the UK government. This accelerated the inflation of the share price further, as did the Bubble Act 1720, which (possibly with the motive of protecting the South Sea Company from competition) prohibited the establishment of any companies without a Royal Charter. The share price rose so rapidly that people began buying shares merely in order to sell them at a higher price, which in turn led to higher share prices. This was the first speculative bubble the country had seen, but by the end of 1720, the bubble had "burst", and the share price sank from £1000 to under £100. As bankruptcies and recriminations ricocheted through government and high society, the mood against corporations, and errant directors, was bitter.
In the late 18th century, Stewart Kyd, the author of the first treatise on corporate law in English, defined a corporation as:
Modern company law.
Due to the late 18th century abandonment of mercantilist economic theory and the rise of classical liberalism and laissez-faire economic theory due to a revolution in economics led by Adam Smith and other economists, corporations transitioned from being government or guild affiliated entities to being public and private economic entities free of government direction.
In 1776, Adam Smith wrote in the "Wealth of Nations" that mass corporate activity could not match private entrepreneurship, because people in charge of others' money would not exercise as much care as they would with their own.
Deregulation.
The UK Bubble Act 1720's prohibition on establishing companies remained in force until its repeal in 1825. By this point the Industrial Revolution had gathered pace, pressing for legal change to facilitate business activity. The repeal was the beginning of a gradual lifting on restrictions, though business ventures (such as those chronicled by Charles Dickens in "Martin Chuzzlewit") under primitive companies legislation were often scams. Without cohesive regulation, proverbial operations like the "Anglo-Bengalee Disinterested Loan and Life Assurance Company" were undercapitalised ventures promising no hope of success except for richly paid promoters.
The process of incorporation was possible only through a royal charter or a private act and was limited, owing to Parliament's jealous protection of the privileges and advantages thereby granted. As a result, many businesses came to be operated as unincorporated associations with possibly thousands of members. Any consequent litigation had to be carried out in the joint names of all the members and was almost impossibly cumbersome. Though Parliament would sometimes grant a private act to allow an individual to represent the whole in legal proceedings, this was a narrow and necessarily costly expedient, allowed only to established companies.
Then in 1843, William Gladstone took chairmanship of a Parliamentary Committee on Joint Stock Companies, which led to the Joint Stock Companies Act 1844, regarded as the first modern piece of company law. The Act created the Registrar of Joint Stock Companies, empowered to register companies by a two-stage process. The first, provisional, stage cost £5 and did not confer corporate status, which arose after completing the second stage for another £5. For the first time in history, it was possible for ordinary people through a simple registration procedure to incorporate. The advantage of establishing a company as a separate legal person was mainly administrative, as a unified entity under which the rights and duties of all investors and managers could be channeled.
Limited liability.
However, there was still no limited liability and company members could still be held responsible for unlimited losses by the company. The next, crucial development, then, was the Limited Liability Act 1855, passed at the behest of the then Vice President of the Board of Trade, Mr Robert Lowe. This allowed investors to limit their liability in the event of business failure to the amount they invested in the company - shareholders were still liable directly to creditors, but just for the unpaid portion of their shares. (The principle that shareholders are liable to the corporation had been introduced in the Joint Stock Companies Act 1844).
The 1855 Act allowed limited liability to companies of more than 25 members (shareholders). Insurance companies were excluded from the act, though it was standard practice for insurance contracts to exclude action against individual members. Limited liability for insurance companies was allowed by the Companies Act 1862.
This prompted the English periodical "The Economist" to write in 1855 that "never, perhaps, was a change so vehemently and generally demanded, of which the importance was so much overrated. " The major error of this judgment was recognised by the same magazine more than 70 years later, when it claimed that, "[t]he economic historian of the future. . . may be inclined to assign to the nameless inventor of the principle of limited liability, as applied to trading corporations, a place of honour with Watt and Stephenson, and other pioneers of the Industrial Revolution. "
These two features - a simple registration procedure and limited liability - were subsequently codified into the landmark 1856 Joint Stock Companies Act. This was subsequently consolidated with a number of other statutes in the Companies Act 1862, which remained in force for the rest of the century, up to and including the time of the decision in "Salomon v A Salomon & Co Ltd".
The legislation shortly gave way to a railway boom, and from then, the numbers of companies formed soared. In the later nineteenth century depression took hold, and just as company numbers had boomed, many began to implode and fall into insolvency. Much strong academic, legislative and judicial opinion was opposed to the notion that businessmen could escape accountability for their role in the failing businesses.
A study, title "Review on the Loss Problem of the Listed Corporations Based on the Valuation", published in International Journal of Trends in Economics Management and Technology (IJTEMT), concluded that to price for the stock of loss listed company, we should not consider single one or several aspects of factors, but should stand in the angle of investor, considering various expected factors to explore the driving path for all kinds of heterogeneity of loss listed company, thereby make a reasonable assessment on the value of loss of listed company. 
Further developments.
The last significant development in the history of companies was the decision of the House of Lords in "Salomon v. Salomon & Co." where the House of Lords confirmed the separate legal personality of the company, and that the liabilities of the company were separate and distinct from those of its owners.
In the United States, forming a corporation usually required an act of legislation until the late 19th century. Many private firms, such as Carnegie's steel company and Rockefeller's Standard Oil, avoided the corporate model for this reason (as a trust). State governments began to adopt more permissive corporate laws from the early 19th century, although these were all restrictive in design, often with the intention of preventing corporations for gaining too much wealth and power.
New Jersey was the first state to adopt an "enabling" corporate law, with the goal of attracting more business to the state, in 1896. In 1899, Delaware followed New Jersey's lead with the enactment of an enabling corporate statute, but Delaware only became the leading corporate state after the enabling provisions of the 1896 New Jersey corporate law were repealed in 1913.
The end of the 19th century saw the emergence of holding companies and corporate mergers creating larger corporations with dispersed shareholders. Countries began enacting anti-trust laws to prevent anti-competitive practices and corporations were granted more legal rights and protections.
The 20th century saw a proliferation of laws allowing for the creation of corporations by registration across the world, which helped to drive economic booms in many countries before and after World War I. Another major post World War I shift was toward the development of conglomerates, in which large corporations purchased smaller corporations to expand their industrial base.
Starting in the 1980s, many countries with large state-owned corporations moved toward privatization, the selling of publicly owned (or 'nationalised') services and enterprises to corporations. Deregulation (reducing the regulation of corporate activity) often accompanied privatization as part of a laissez-faire policy.
Ownership and control.
A corporation is, at least in theory, owned and controlled by its members. In a joint-stock company the members are known as shareholders and each of their shares in the ownership, control and profits of the corporation is determined by the portion of shares in the company that they own. Thus a person who owns a quarter of the shares of a joint-stock company owns a quarter of the company, is entitled to a quarter of the profit (or at least a quarter of the profit given to shareholders as dividends) and has a quarter of the votes capable of being cast at general meetings.
In another kind of corporation the legal document which established the corporation or which contains its current rules will determine who the corporation's members are. Who is a member depends on what kind of corporation is involved. In a worker cooperative the members are people who work for the cooperative. In a credit union the members are people who have accounts with the credit union.
The day-to-day activities of a corporation are typically controlled by individuals appointed by the members. In some cases this will be a single individual but more commonly corporations are controlled by a committee or by committees. Broadly speaking there are two kinds of committee structure.
Formation.
Historically, corporations were created by a charter granted by government. Today, corporations are usually registered with the state, province, or national government and regulated by the laws enacted by that government. Registration is the main prerequisite to the corporation's assumption of limited liability. The law sometimes requires the corporation to designate its principal address, as well as a registered agent (a person or company designated to receive legal service of process). It may also be required to designate an agent or other legal representative of the corporation.
Generally, a corporation files articles of incorporation with the government, laying out the general nature of the corporation, the amount of stock it is authorized to issue, and the names and addresses of directors. Once the articles are approved, the corporation's directors meet to create bylaws that govern the internal functions of the corporation, such as meeting procedures and officer positions.
The law of the jurisdiction in which a corporation operates will regulate most of its internal activities, as well as its finances. If a corporation operates outside its home state, it is often required to register with other governments as a foreign corporation, and is almost always subject to laws of its host state pertaining to employment, crimes, contracts, civil actions, and the like.
Naming.
Corporations generally have a distinct name. Historically, some corporations were named after their membership: for instance, "The President and Fellows of Harvard College." Nowadays, corporations in most jurisdictions have a distinct name that does not need to make reference to their membership. In Canada, this possibility is taken to its logical extreme: many smaller Canadian corporations have no names at all, merely numbers based on a registration number (for example, "12345678 Ontario Limited"), which is assigned by the provincial or territorial government where the corporation incorporates.
In most countries, corporate names include a term or an abbreviation that denotes the corporate status of the entity (for example, "Incorporated" or "Inc." in the United States) or the limited liability of its members (for example, "Limited" or "Ltd."). These terms vary by jurisdiction and language. In some jurisdictions they are mandatory, and in others they are not. Their use puts everybody on constructive notice that they are dealing with an entity whose liability is limited: one can only collect from whatever assets the entity still controls when one obtains a judgment against it.
Some jurisdictions do not allow the use of the word "company" alone to denote corporate status, since the word "company" may refer to a partnership or some other form of collective ownership (in the United States it can be used by a sole proprietorship but this is not generally the case elsewhere).

</doc>
<doc id="7487" url="http://en.wikipedia.org/wiki?curid=7487" title="Fairchild Channel F">
Fairchild Channel F

The Fairchild Channel F is a game console released by Fairchild Semiconductor in November 1976 at the retail price of $169.95 (). It has the distinction of being the first programmable ROM cartridge–based video game console, and the first console to use a microprocessor. It was launched as the Video Entertainment System, or VES, but when Atari released their VCS the next year, Fairchild renamed its machine. By 1977, the Fairchild Channel F had sold 250,000 units and trailed behind the VCS.
The console.
The Channel F electronics were designed by Jerry Lawson using the Fairchild F8 CPU, the first public outing of this processor. The F8 was very complex compared to the typical integrated circuits of the day, and had more inputs and outputs than other contemporary chips. Because chip packaging was not available with enough pins, the F8 was instead fabricated as a pair of chips that had to be used together to form a complete CPU.
Lawson worked with Nick Talesfore and Ron Smith. As manager of Industrial Design, Talesfore was responsible for the design of the hand controllers, console, and video game cartridges. Smith was responsible for the mechanical engineering of the video cartridges and hand controllers. All worked for Wilf Corigan, head of Fairchild Semiconductor, a division of Fairchild Camera & Instrument. 
 The graphics are quite basic by modern standards. The Channel F is only able to use one plane of graphics and one of four background colors per line, only three plot colors to choose from (red, green and blue) that turned into white if the background is set to black. A resolution of 128 × 64 with approximately 102 × 58 pixels visible and help from only 64 bytes of system RAM, half the amount of the Atari 2600. The F8 processor at the heart of the console is able to produce enough AI to allow for player versus computer matches, a first in console history. All previous machines required a human opponent.
One feature unique to this console is the 'hold' button, which allowed the player to freeze the game, change the time or change the speed of the game during the course of the game. In the original unit, sound is played through an internal speaker, rather than the TV set. However, the System II passed sound to the television through the RF modulator.
Controllers.
The controllers are a joystick without a base; the main body is a large hand grip with a triangular "cap" on top, the top being the portion that actually moved for eight-way directional control. It could be used as both a joystick and paddle (twist), and not only pushed down to operate as a fire button but also pulled up. The model 1 unit contained a small compartment for storing the controllers when moving it. The System II featured detachable controllers and had two holders at the back to wind the cable around and to store the controller in. Zircon later offered a special control which featured an action button on the front of the joystick. It was marketed by Zircon as "Channel F Jet-Stick" in a letter sent out to registered owners before Christmas 1982. They also released it as an Atari-compatible controller called "Video Command", first released without the extra fire button. Before that, only the downwards plunge motion was connected and acted as the fire button; the pull-up and twist actions weren't connected to anything.
Games.
Twenty-seven cartridges, termed 'Videocarts', were officially released to consumers during the ownership of Fairchild and Zircon, the first twenty-one of which were released by Fairchild. Several of these cartridges were capable of playing more than one game and were typically priced at $19.95. The Videocarts were yellow and approximately the size and overall texture of an 8 track cartridge. They usually featured colorful label artwork. The earlier artwork was created by nationally known artist Tom Kamifuji and art directed by Nick Talesfore. The console contained two built-in games, Tennis and Hockey, which were both advanced "Pong" clones. In Hockey the reflecting bar could be changed to diagonals by twisting the controller, and could move all over the playing field. Tennis was much like the original Pong.
A sales brochure from 1978 listed 'Keyboard Videocarts' for sale. The three shown were "K-1 Casino Poker", "K-2 Space Odyssey", and "K-3 Pro-Football". These were intended to use the Keyboard accessory. All further brochures, released after Zircon took over Fairchild, never listed this accessory nor anything called a Keyboard Videocart.
There was one additional cartridge released numbered Videocart-51 and simply titled 'Demo 1'. This Videocart was shown in a single sales brochure released shortly after Zircon acquired the company. It was never listed for sale after this single brochure which was used for winter of 1979.
List of games.
Homebrewed
Carts listed (as mentioned above) but never released:
Official carts that also exist:
German SABA also released a few compatible carts different from the original carts, translation in Videocart 1 Tic-Tac-Toe to German words, Videocart 3 released with different abbreviations (German), Videocart 18 changed graphics and German word list and the SABA 20 that's a Chess game released only by SABA.
Market impact.
The biggest effect of the Channel F in the market was to spur Atari into improving and releasing their next-generation console which was then in development. Then codenamed "Stella," the machine was also set to utilize cartridges; after seeing the Channel F, Atari realized they needed to release it before the market was flooded with cartridge-based machines. With cash flow dwindling as sales of their existing Pong-based systems dried up, they were forced to sell to Warner Communications to gain the capital they needed. When the Atari VCS gaming system (whose name was coined as a takeoff of the VES) was released a year later, it had considerably better graphics and sound.
Reception.
Ken Uston reviewed 32 games in his book "Ken Uston's Guide to Buying and Beating the Home Video Games" in 1982, and rated some of the Channel F's titles highly; of these, "Alien Invasion" and "Video Whizball" were considered by Uston to be "the finest adult cartridges currently available for the Fairchild Channel F System." The games on the whole, however, rated last on his survey of over 200 games for the Atari, Intellivision, Astrocade and Odyssey consoles, and contemporary games were rated "Average" with future Channel F games rated "below average". Uston rated almost one half of the Channel F games as "high in interest" and called that "an impressive proportion" and further noted that "Some of the Channel F cartridges are timeless; no matter what technological developments occur, they will continue to be of interest." His overall conclusion was that the games "serve a limited, but useful, purpose" and that the "strength of the Channel F offering is in its excellent educational line for children."
In 1983, after Zircon announced its discontinuation of the Channel F, "Video Games" reviewed the console. Calling it "the system nobody knows", the magazine described its graphics and sounds as "somewhat primitive by today's standards". It described "Space War" as perhaps "the most antiquated game of its type still on the market", and rated the 25 games for the console with an average grade of three ("not too good") on a scale from one to ten. The magazine stated, however, that Fairchild "managed to create some fascinating games, even by today's standards", calling "Casino Royale" ("Video Blackjack") "the best card game, from blackjack to bridge, made for "any" TV-game system". It also favorably reviewed "Dodge-It" ("simple but great"), "Robot War" ("Berzerk without guns"), and "Whizball" ("thoroughly original ... hockey "with" guns"), but concluded that only those interested in nostalgia, video game collecting, or card games would purchase the Channel F in 1983.
Technical specifications.
Original Channel F technical specifications:
The Channel F System II.
Some time in 1979, Zircon International bought the rights to the Channel F and released the re-designed console as the Channel F System II to compete with the Atari's VCS. This re-designed System II was completed by Nick Talesfore at Fairchild. He was the same industrial designer who designed the original game console. Only six new games were released after the debut of the second system before its death, several of which were developed at Fairchild before they sold it off.
The major changes were in design, with the controllers removable from the base unit instead of being wired directly into it, the storage compartment was moved to the rear of the unit, and the sound was now mixed into the TV signal so the unit no longer needed a speaker. This version also featured a simpler and more modern-looking case design. However, by this time the market was in the midst of the first video game crash, and Fairchild eventually threw in the towel and left the market. A number of licensed versions were released in Europe, including the Luxor Video Entertainment System in Scandinavia (Sweden), Adman Grandstand in the UK, and the Saba Videoplay, Nordmende Teleplay and ITT Tele-Match Processor, from Germany and also Dumont Videoplay and Barco Challenger from the Barco/Dumont company in Italy and Belgium.
Homebrew.
Like many other discontinued consoles, the Channel F lives on through homebrew. For example, a 2009 version of "Pac-Man" was developed and distributed for the Channel F.

</doc>
<doc id="7489" url="http://en.wikipedia.org/wiki?curid=7489" title="Collation">
Collation

Collation is the assembly of written information into a standard order. Many systems of collation are based on numerical order or alphabetical order, or extensions and combinations thereof. Collation is a fundamental element of most office filing systems, library catalogs and reference books.
Collation differs from "classification" in that classification is concerned with arranging information into logical categories, while collation is concerned with the ordering of items of information, usually based on the form of their identifiers. Formally speaking, a collation method typically defines a total order on a set of possible identifiers, called sort keys, which consequently produces a total preorder on the set of items of information (items with the same identifier are not placed in any defined order). 
A collation algorithm such as the Unicode collation algorithm defines an order through the process of comparing two given character strings and deciding which should come before the other. When an order has been defined in this way, a "sorting algorithm" can be used to put a list of any number of items into that order.
The main advantage of collation is that it makes it fast and easy for a user to find an element in the list, or to confirm that it is absent from the list. In automatic systems this can be done using a binary search algorithm or interpolation search; manual searching may be performed using a roughly similar procedure, though this will often be done unconsciously. Other advantages are that one can easily find the first or last elements on the list (most likely to be useful in the case of numerically sorted data), or elements in a given range (useful again in the case of numerical data, and also with alphabetically ordered data when one may be sure of only the first few letters of the sought item or items).
Numerical and chronological order.
Strings representing numbers may be sorted based on the values of the numbers that they represent. For example, "-4", "2.5", "10", "89", "30,000". Note that pure application of this method may provide only a partial ordering on the strings, since different strings can represent the same number (as with "2" and "2.0", or when scientific notation is used, "2e3" and "2000").
A similar approach may be taken with strings representing dates or other items that can be ordered chronologically or in some other natural fashion.
Alphabetical order.
Alphabetical order is the basis for many systems of collation where items of information are identified by strings consisting principally of letters from an alphabet. The ordering of the strings relies on the existence of a standard ordering for the letters of the alphabet in question. (The system is not limited to alphabets in the strict technical sense; languages that use a syllabary or abugida, for example Cherokee, can use the same ordering principle provided there is a set ordering for the symbols used.)
To decide which of two strings comes first in alphabetical order, initially their first letters are compared. The string whose first letter appears earlier in the alphabet comes first in alphabetical order. If the first letters are the same, then the second letters are compared, and so on, until the order is decided. (If one string runs out of letters to compare, then it is deemed to come first; for example, "cart" comes before "carthorse".) The result of arranging a set of strings in alphabetical order is that words with the same first letter are grouped together, and within such a group words with the same first two letters are grouped together and so on.
Capital letters are typically treated as equivalent to their corresponding lowercase letters. (For alternative treatments in computerized systems, see Automated collation, below.)
Certain limitations, complications and special conventions may apply when alphabetical order is used:
In several languages the rules have changed over time, and so older dictionaries may use a different order than modern ones. Furthermore, collation may depend on use. For example, German dictionaries and telephone directories use different approaches.
Radical-and-stroke sorting.
Another form of collation is radical-and-stroke sorting, used for non-alphabetic writing systems such as the hanzi of Chinese and the kanji of Japanese, whose thousands of symbols defy ordering by convention. In this system, common components of characters are identified; these are called radicals in Chinese and logographic systems derived from Chinese. Characters are then grouped by their primary radical, then ordered by number of pen strokes within radicals. When there is no obvious radical or more than one radical, convention governs which is used for collation. For example, the Chinese character 妈 (meaning "mother") is sorted as a six-stroke character under the three-stroke primary radical 女.
The radical-and-stroke system is cumbersome compared to an alphabetical system in which there are a few characters, all unambiguous. The choice of which components of a logograph comprise separate radicals and which radical is primary is not clear-cut. As a result, logographic languages often supplement radical-and-stroke ordering with alphabetic sorting of a phonetic conversion of the logographs. For example, the kanji word ""Tōkyō" (東京), the Japanese name of Tokyo can be sorted as if it were spelled out in the Japanese characters of the hiragana syllabary as "to-u-ki-yo-u" (とうきょう), using the conventional sorting order for these characters.
In addition, in Greater China, surname stroke ordering is a convention in some official documents where peoples' names are listed without hierarchy.
The radical-and-stroke system, or some similar pattern-matching and stroke-counting method, was traditionally the only practical method for constructing dictionaries that someone could use to look up a logograph whose pronunciation was unknown. With the advent of computers, dictionary programs are now available that allow one to "handwrite" a character using a mouse or stylus.
Automated collation.
When information is stored in digital systems, collation may become an automated process. It is then necessary to implement an appropriate collation algorithm that allows the information to be sorted in a satisfactory manner for the application in question. Often the aim will be to achieve an alphabetical or numerical ordering that follows the standard criteria as described in the preceding sections. However, not all of these criteria are easy to automate.
The simplest kind of automated collation is based on the numerical codes of the symbols in a character set, such as ASCII coding (or any of its supersets such as Unicode), with the symbols being ordered in increasing numerical order of their codes, and this ordering being extended to strings in accordance with the basic principles of alphabetical ordering (mathematically speaking, lexicographical ordering). So a computer program might treat the characters "a", "b", "C", "d" and "$" as being ordered "$", "C", "a", "b", "d" (the corresponding ASCII codes are "$" = 36, "a" = 97, "b" = 98, "C" = 67, and "d" = 100). Therefore strings beginning with "C" (or any other capital letter) would be sorted before strings with lower-case "a", "b", etc. This is sometimes called "ASCIIbetical order".
The above method has the disadvantage that it can deviate from the standard alphabetical order that human users would expect, particularly due to the unexpected ordering of capital letters before all lower-case ones (and possibly the unexpected treatment of spaces and other non-letter characters). It is therefore often applied with certain refinements, the most obvious being the conversion of capitals to lowercase before comparing ASCII values.
In many collation algorithms, the comparison is based not on the numerical codes of the characters, but with reference to the collating sequence – a sequence in which the characters are assumed to come for the purpose of collation – as well as other ordering rules appropriate to the given application. This can serve to apply the correct conventions used for alphabetical ordering in the language in question, dealing properly with differently cased letters, modified letters, digraphs, particular abbreviations and so on, as mentioned above under Alphabetical order, and in detail in the Alphabetical order article. Such algorithms are potentially quite complex, possibly requiring several passes through the text.
Problems are nonetheless still common when the algorithm has to encompass more than one language. For example, in German dictionaries the word "ökonomisch" comes between "offenbar" and "olfaktorisch", while Turkish dictionaries treat "o" and "ö" as different letters, placing "oyun" before "öbür".
A standard algorithm for collating any collection of strings composed of any standard Unicode symbols is the Unicode Collation Algorithm. This can be adapted to use the appropriate collation sequence for a given language by tailoring its default collation table. Several such tailorings are collected in Common Locale Data Repository.
Sort keys.
In some applications, the strings by which items are collated may differ from the identifiers that are displayed. For example, "The Shining" might be sorted as "Shining, The" (see Alphabetical order above), but it may still be desired to display it as "The Shining". In this case two sets of strings can be stored, one for display purposes, and another for collation purposes. Strings used for collation in this way are called "sort keys".
Issues with numbers.
Sometimes, it is desired to order text with embedded numbers using proper numerical order. For example, "Figure 7b" goes before "Figure 11a", even though '7' comes after '1' in Unicode. This can be extended to Roman numerals. This behavior is not particularly difficult to produce as long as only integers are to be sorted, although it can slow down sorting significantly. For example, Windows XP does this when sorting file names.
Sorting decimals properly is a bit more difficult, because different locales use different symbols for a decimal point, and sometimes the same character used as a decimal point is also used as a separator, for example "Section 3.2.5". There is no universal answer for how to sort such strings; any rules are application dependent.
Ascending order of numbers differs from alphabetical order, e.g. 11 comes alphabetically before 2. This can be fixed with leading zeros: 02 comes alphabetically before 11. See e.g. ISO 8601.
Also −13 comes alphabetically after −12 although it is less. With negative numbers, to make ascending order correspond with alphabetical sorting, more drastic measures are needed such as adding a constant to all numbers to make them all positive.
Labeling of ordered items.
In some contexts, numbers and letters are used not so much as a basis for establishing an ordering, but as a means of labeling items that are already ordered. For example, pages, sections, chapters and the like, as well as the items of lists, are frequently "numbered" in this way. Labeling series that may be used include ordinary Arabic numerals (1, 2, 3, ...), Roman numerals (I, II, III, ... or i, ii, iii, ...), or letters (A, B, C, ... or a, b, c, ...). (An alternative method for indicating list items, without numbering them, is to use a bulleted list.)
When letters of an alphabet are used for this purpose of enumeration, there are certain language-specific conventions as to which letters are used. For example, the Russian letters Ъ and Ь (which in writing are only used for modifying the preceding consonant), and usually also Ы, Й and Ё, are usually omitted. Also in many languages that use extended Latin script, the modified letters are often not used in enumeration.

</doc>
<doc id="7490" url="http://en.wikipedia.org/wiki?curid=7490" title="Civil Rights Act">
Civil Rights Act

Civil Rights Act may refer to several acts in the history of civil rights in the United States, including:

</doc>
<doc id="7491" url="http://en.wikipedia.org/wiki?curid=7491" title="Cola">
Cola

Cola is a carbonated beverage that originally contained caffeine from the kola nut and cocaine from coca leaves, and was flavored with vanilla and other ingredients. Most colas now use other flavoring (and caffeinating) ingredients with a similar taste and no longer contain cocaine. It became popular worldwide after pharmacist John Pemberton invented Coca-Cola in 1886. His non-alcoholic recipe was inspired by the Coca wine of pharmacist Angelo Mariani, created in 1863. It usually contains caramel color, caffeine and sweeteners such as sugar or high fructose corn syrup.
Flavorings.
Despite the name, the primary modern flavoring ingredients in a cola drink are sugar, citrus oils (from oranges, limes, or lemon fruit peel), cinnamon, vanilla, and an acidic flavorant. Manufacturers of cola drinks add trace ingredients to create distinctively different tastes for each brand. Trace flavorings may include nutmeg and a wide variety of ingredients, but the base flavorings that most people identify with a cola taste remain vanilla and cinnamon. Acidity is often provided by phosphoric acid, sometimes accompanied by citric or other isolated acids. Coca-Cola's recipe and several others are maintained as corporate trade secrets.
A variety of different sweeteners may be added to cola, often partly dependent on local agricultural policy. High-fructose corn syrup is predominantly used in the United States and Canada due to the lower cost of government-subsidized corn. In Europe, however, HFCS is subject to production quotas designed to encourage the production of sugar; sugar is thus typically used to sweeten sodas. In addition, stevia or an artificial sweetener may be used; "sugar-free" or "diet" colas typically contain artificial sweeteners only.
Consumers may prefer the taste of soda manufactured with sugar; as in the United States, with imported Mexican Coca-Cola. Kosher for Passover Coca-Cola sold in the U.S. around the Jewish holiday also uses sucrose rather than HFCS and is also highly sought after by people who prefer the original taste. In addition, PepsiCo has recently been marketing versions of its Pepsi and Mountain Dew sodas that are sweetened with sugar instead of HFCS. These are marketed under the name "Throwback" and became "permanent" products on the lineup.
Clear cola.
Clear cola is a colorless variety of cola, popular in the early 1990s. Brands included Crystal Pepsi, Tab Clear and 7 Up Ice Cola.
Health.
A 2007 study found that consumption of colas, both those with natural sweetening and those with artificial sweetening, was associated with increased risk of chronic kidney disease. The phosphoric acid used in colas was thought to be a possible cause.
Studies indicate "soda and sweetened drinks are the main source of calories in [the] American diet", so most nutritionists advise that Coca-Cola and other soft drinks can be harmful if consumed excessively, particularly to young children whose soft drink consumption competes with, rather than complements, a balanced diet. Studies have shown that regular soft drink users have a lower intake of calcium, magnesium, ascorbic acid, riboflavin, and vitamin A. 
The drink has also aroused criticism for its use of caffeine, which can cause physical dependence (caffeine addiction). A link has been shown between long-term regular cola intake and osteoporosis in older women (but not men). This was thought to be due to the presence of phosphoric acid, and the risk was found to be same for caffeinated and noncaffeinated colas, as well as the same for diet and sugared colas.
Many soft drinks are sweetened mostly or entirely with high-fructose corn syrup, rather than sugar. Some nutritionists caution against consumption of corn syrup because it may aggravate obesity and type-2 diabetes more than cane sugar.

</doc>
<doc id="7492" url="http://en.wikipedia.org/wiki?curid=7492" title="Capability Maturity Model">
Capability Maturity Model

The Capability Maturity Model (CMM) is a development model created after study of data collected from organizations that contracted with the U.S. Department of Defense, who funded the research. The term "maturity" relates to the degree of formality and optimization of processes, from "ad hoc" practices, to formally defined steps, to managed result metrics, to active optimization of the processes.
The model's aim is to improve existing software-development processes, but it can also be applied to other processes.
Overview.
The Capability Maturity Model was originally developed as a tool for objectively assessing the ability of government contractors' "processes" to perform a contracted software project. The model is based on the process maturity framework first described in the 1989 book "Managing the Software Process" by Watts Humphrey. It was later published in a report in 1993 and as a book by the same authors in 1995.
Though the model comes from the field of software development, it is also used as a general model to aid in business processes generally, and has been used extensively worldwide in government offices, commerce, industry and software-development organizations.
Lately, capability model is used as capability based planning approach to design the business services as capabilities in business architecture. IT Capabilities Model includes Structured Operating Model (SOM), Service Oriented Architecture (SOA) and Service Oriented Infrastructure (SOI), as proposed by Haloedscape.
History.
Prior need for software processes.
In the 1960s, the use of computers grew more widespread, more flexible and less costly. Organizations began to adopt computerized information systems, and the demand for software development grew significantly. Many processes for software development were in their infancy, with few standard or "best practice" approaches defined.
As a result, the growth was accompanied by growing pains: project failure was common, and the field of computer science was still in its early years, and the ambitions for project scale and complexity exceeded the market capability to deliver adequate products within a planned budget. Individuals such as Edward Yourdon, Larry Constantine, Gerald Weinberg, Tom DeMarco, and David Parnas began to publish articles and books with research results in an attempt to professionalize the software-development processes. 
In the 1980s, several US military projects involving software subcontractors ran over-budget and were completed far later than planned, if at all. In an effort to determine why this was occurring, the United States Air Force funded a study at the SEI.
Precursor.
The Quality Management Maturity Grid was developed by Philip B. Crosby in his book "Quality is Free", which advanced the position that quality-improvement activities paid for themselves by reducing the related costs.
The first application of a staged maturity model to IT was not by CMM/SEI, but rather by Richard L. Nolan, who, in 1973 published the stages of growth model for IT organizations.
Watts Humphrey began developing his process maturity concepts during the later stages of his 27 year career at IBM.
Development at Software Engineering Institute.
Active development of the model by the US Department of Defense Software Engineering Institute (SEI) began in 1986 when Humphrey joined the Software Engineering Institute located at Carnegie Mellon University in Pittsburgh, Pennsylvania after retiring from IBM. At the request of the U.S. Air Force he began formalizing his Process Maturity Framework to aid the U.S. Department of Defense in evaluating the capability of software contractors as part of awarding contracts.
The result of the Air Force study was a model for the military to use as an objective evaluation of software subcontractors' process capability maturity. Humphrey based this framework on the earlier Quality Management Maturity Grid developed by Philip B. Crosby in his book "Quality is Free". Humphrey's approach differed because of his unique insight that organizations mature their processes in stages based on solving process problems in a specific order. Humphrey based his approach on the staged evolution of a system of software development practices within an organization, rather than measuring the maturity of each separate development process independently. The CMM has thus been used by different organizations as a general and powerful tool for understanding and then improving general business process performance.
Watts Humphrey's Capability Maturity Model (CMM) was published in 1988 and as a book in 1989, in "Managing the Software Process".
Organizations were originally assessed using a process maturity questionnaire and a Software Capability Evaluation method devised by Humphrey and his colleagues at the Software Engineering Institute 
The full representation of the Capability Maturity Model as a set of defined process areas and practices at each of the five maturity levels was initiated in 1991, with Version 1.1 being completed in January 1993. The CMM was published as a book in 1995 by its primary authors, Mark C. Paulk, Charles V. Weber, Bill Curtis, and Mary Beth Chrissis.
CMMI.
The CMM model's application in software development has sometimes been problematic. Applying multiple models that are not integrated within and across an organization could be costly in training, appraisals, and improvement activities. The Capability Maturity Model Integration (CMMI) project was formed to sort out the problem of using multiple models for software development processes, thus the CMMI model has superseded the CMM model, though the CMM model continues to be a general theoretical process capability model used in the public domain.
Adapted to other processes.
The CMM was originally intended as a tool to evaluate the ability of government contractors to perform a contracted software project. Though it comes from the area of software development, it can be, has been, and continues to be widely applied as a general model of the maturity of "process" (e.g., IT service management processes) in IS/IT (and other) organizations.
Model topics.
Maturity model.
A maturity model can be viewed as a set of structured levels that describe how well the behaviors, practices and processes of an organization can reliably and sustainably produce required outcomes.
A maturity model can be used as a benchmark for comparison and as an aid to understanding - for example, for comparative assessment of different organizations where there is something in common that can be used as a basis for comparison. In the case of the CMM, for example, the basis for comparison would be the organizations' software development processes.
Structure.
The model involves five aspects:
Levels.
There are five levels defined along the continuum of the model and, according to the SEI: "Predictability, effectiveness, and control of an organization's software processes are believed to improve as the organization moves up these five levels. While not rigorous, the empirical evidence to date supports this belief".
Within each of these maturity levels are Key Process Areas which characterise that level, and for each such area there are five factors: goals, commitment, ability, measurement, and verification. These are not necessarily unique to CMM, representing — as they do — the stages that organizations must go through on the way to becoming mature.
The model provides a theoretical continuum along which process maturity can be developed incrementally from one level to the next. Skipping levels is not allowed/feasible.
At maturity level 5, processes are concerned with addressing statistical "common causes" of process variation and changing the process (for example, to shift the mean of the process performance) to improve process performance. This would be done at the same time as maintaining the likelihood of achieving the established quantitative process-improvement objectives.
Critique.
The model was originally intended to evaluate the ability of government contractors to perform a software project. It has been used for and may be suited to that purpose, but critics pointed out that process maturity according to the CMM was not necessarily mandatory for successful software development.
Software process framework.
The software process framework documented is intended to guide those wishing to assess an organization's or project's consistency with the Key Process Areas. For each maturity level there are five checklist types:

</doc>
<doc id="7498" url="http://en.wikipedia.org/wiki?curid=7498" title="Centillion">
Centillion

One centillion is a number, which is equal to either 10303 or 10600, depending on the system used.
Short scale.
In areas using the short scale, such as Canada, the US, and the UK, a centillion is 10303. It is formed on a pattern starting with a million: a million (1,000,000) has three zeroes more than a thousand (1,000); a billion (1,000,000,000) has two groups of three zeroes more than a thousand, and so on. A centillion has one hundred groups of three zeroes more than a thousand.
A centillion in the short-scale system is equivalent to a quinquagintilliard, or a thousand quinquagintillion, in the long-scale system.
Long scale.
In long scale usage, one centillion is 10600, which is equal to (106)100; that is, it is the number with a hundred times as many zeroes as a million.
A centillion in the long scale is equivalent to a cennovemnonagintillion in the short scale.
References.
 

</doc>
<doc id="7499" url="http://en.wikipedia.org/wiki?curid=7499" title="RDX">
RDX

RDX, an initialism for Research Department explosive, is an explosive nitroamine widely used in military and industrial applications. It was developed as an explosive which was more powerful than TNT, and it saw wide use in World War II. RDX is also known as Research Department Formula X, cyclonite, hexogen (particularly in German and German-influenced languages), and T4. Its chemical name is cyclotrimethylenetrinitramine; name variants include cyclotrimethylene-trinitramine and cyclotrimethylene trinitramine.
In its pure, synthesized state RDX is a white, crystalline solid. It is often used in mixtures with other explosives and plasticizers, phlegmatizers or desensitizers. RDX is stable in storage and is considered one of the most powerful and brisant of the military high explosives.
Name.
RDX is also known, but less commonly, as Cyclonite, Hexogen (particularly in Russian, German and German-influenced languages), T4 and chemically as cyclotrimethylenetrinitramine. Tenney L Davis, writing in the US in 1943, stated it was generally known in the US as cyclonite; the Germans called it Hexogen, the Italians T4. In the 1930s, the Royal Arsenal, Woolwich, started investigating Cyclonite as an explosive to use against German U-boats that were being built with thicker hulls. Britain wanted an explosive that was more powerful than TNT. For security reasons, Britain termed Cyclonite as "Research Department Explosive" (R. D. X.). The term RDX appeared in the United States in 1946, but the name RDX is given without explanation. The first public reference in the United Kingdom to the name RDX, or R.D.X. to use the official title, appeared in 1948; its authors were the Managing Chemist, ROF Bridgwater, the Chemical Research and Development Department, Woolwich, and the Director of Royal Ordnance Factories, Explosives; again, it was referred to as simply RDX.
Usage.
RDX was widely used during World War II, often in explosive mixtures with TNT such as Torpex, Composition B, Cyclotols, and H6. RDX was used in one of the first plastic explosives. RDX is believed to have been used in many bomb plots including terrorist plots. The bouncing bomb depth charges used in the "Dambusters Raid" each contained of Torpex.
RDX forms the base for a number of common military explosives:
Outside of military applications, RDX is also used in controlled demolition to raze structures. The demolition of the Jamestown Bridge in the US state of Rhode Island is one example where RDX shaped charges were used to remove the span.
Properties.
The velocity of detonation of RDX at a density of 1.76 g/cm³ is 8750 m/s.
It is a colourless solid, of crystal density 1.82 g/cm³. It is obtained by reacting white fuming nitric acid (WFNA) with hexamine, producing dinitromethane and ammonium nitrate as byproducts.
It is a heterocycle and has the molecular shape of a ring. It starts to decompose at about 170 °C and melts at 204 °C. Its structural formula is: hexahydro-1,3,5-trinitro-1,3,5-triazine or (CH2-N-NO2)3.
At room temperature, it is very stable. It burns rather than explodes and detonates only with a detonator, being unaffected even by small arms fire. (This is one of the properties that make it a useful military explosive.) It is less sensitive than pentaerythritol tetranitrate (PETN). However, it is very sensitive when crystallized, below −4 °C.
Under normal conditions, RDX has a figure of insensitivity of exactly 80 (RDX defines the reference point.).
RDX sublimates in vacuum, which limits its use in pyrotechnic fasteners for spacecraft.
RDX when exploded in air has about 1.5 times the explosive power of TNT per unit weight and about 2.0 times per unit volume.
History.
RDX was used by both sides in World War II. The US produced about per month during WWII and Germany about per month. RDX had the major advantages of possessing greater explosive power than TNT used in the First World War, and requiring no additional raw materials for its manufacture.
Germany.
The discovery of RDX dates from 1898 when Georg Friedrich Henning obtained a German patent (patent No. 104280) for its manufacture, by nitrating hexamine nitrate (hexamethylenetetramine nitrate) with concentrated nitric acid. In this 1898 patent, its properties as a medical compound were mentioned; however, three further German patents obtained by Henning in 1916 proposed its use in smokeless propellants. The German military started investigating its use in 1920 and referred to it as hexogen. Research and development findings were not published further until Edmund von Herz, described as an Austrian and later a German citizen, obtained a British patent in 1921 and a United States patent in 1922. Both patent claims were initiated in Austria; and described the manufacture of RDX by nitrating hexamethylenetetramine. The British patent claims included the manufacture of RDX by nitration, its use with or without other explosives, and its use as a bursting charge and as an initiator. The US patent claim was for the use of a hollow explosive device containing RDX and a detonator cap containing RDX. In the 1930s, Germany developed improved production methods.
During the Second World War, Germany used the code names W Salt, SH Salt, K-method, the E-method and the KA-method. These represented the names of the developers of the various chemical processes used to prepare RDX. The W-method was developed by Wolfram in 1934 and gave RDX the code name "W-Salz". It used sulfamic acid, formaldehyde and nitric acid. SH-Salz (SH salt) was from Schnurr who developed a batch-process in 1937–38 based on nitrating hexamine. The K-method was from Knöffler and was based on adding ammonium nitrate to the hexamine / nitric acid process. The E-method was developed by Ebele, in Germany, and turned out to be identical to the Ross and Schiessler process described later. The KA-method was developed by Knöffler, in Germany, and turned out to be identical to the Bachmann process described later.
The explosive shells fired by the MK 108 cannon and the warhead of the R4M rocket, both used in Luftwaffe fighter aircraft as offensive armament, both used hexogen as their explosive base.
UK.
In the United Kingdom (UK), RDX was manufactured from 1933 by the Research Department in a pilot plant at the Royal Arsenal in Woolwich, London; a larger pilot plant being built at the RGPF Waltham Abbey just outside London in 1939. In 1939 a twin-unit industrial-scale plant was designed to be installed at a new site, ROF Bridgwater, away from London; production of RDX started at Bridgwater on one unit in August 1941. The ROF Bridgwater plant brought in ammonia and methanol as raw materials: the methanol was converted to formaldehyde and some of the ammonia converted to nitric acid, which was concentrated for RDX production. The rest of the ammonia was reacted with formaldehyde to produce hexamine. The hexamine plant was supplied by Imperial Chemical Industries; and it incorporated some features based on data obtained from the United States (US). RDX was produced by continually adding hexamine and concentrated nitric acid to a cooled mixture of hexamine and nitric acid in the nitrator. The RDX was purified and processed for its intended use; and recovery and reuse of some methanol and nitric acid was also carried out. The hexamine-nitration and RDX purification plants were duplicated (i.e. twin-unit) to provide some insurance against loss of production due to fire, explosion or air attack.
The United Kingdom and British Empire were fighting without allies against Nazi Germany until the middle of 1941 and had to be self-sufficient. At that time (1941), the UK had the capacity to produce (160,000 lb) of RDX per week; both Canada, an allied country and self-governing dominion within the British Empire, and the US were looked upon to supply ammunition and explosives, including RDX. By 1942 the Royal Air Force's annual requirement was forecast to be of RDX, much of which came from North America (Canada and the US).
Canada.
A different method of production to the Woolwich process was found and used in Canada, possibly at the McGill University Department of Chemistry. This was based on reacting paraformaldehyde and ammonium nitrate in acetic anhydride. A UK patent application was made by Robert Walter Schiessler (Pennsylvania State College) and James Hamilton Ross (McGill, Canada) in May 1942; the UK patent was issued in December 1947. Gilman states that the same method of production had been independently discovered by Ebele in Germany prior to Schiessler and Ross, but that this was not known by the Allies. Urbański provides details of five methods of production, and he refers to this method as the (German) E-method.
UK, US and Canadian production and development.
At the beginning of the 1940s, the major US explosive manufacturers, E. I. du Pont de Nemours & Company and Hercules, had several decades of experience of manufacturing trinitrotoluene (TNT) and had no wish to experiment with new explosives. US Army Ordnance held the same viewpoint and wanted to continue using TNT. RDX had been tested by Picatinny Arsenal in 1929 and it was regarded as too expensive and too sensitive. The Navy proposed to continue using ammonium picrate. In contrast, the National Defense Research Committee (NDRC), who had visited The Royal Arsenal, Woolwich, did not share the view that new explosives were unnecessary. James B. Conant, chairman of Division B, wished to involve academic research into this area. Conant therefore set up an Experimental Explosives Research Laboratory at the Bureau of Mines, Bruceton, Pennsylvania, using Office of Scientific Research and Development (OSRD) funding.
Woolwich method.
In 1941, the UK's Tizard Mission visited the US Army and Navy departments and part of the information handed over included details of the "Woolwich" method of manufacture of RDX and its stabilisation by mixing it with beeswax. The UK was asking that the US and Canada, combined, supply (440,000 lb) of RDX per day. A decision was taken by William H. P. Blandy, Chief of the Bureau of Ordnance to adopt RDX for use in mines and torpedoes. Given the immediate need for RDX, the US Army Ordnance, at Blandy's request, built a plant that just copied the equipment and process used at Woolwich. The result was the Wabash River Ordnance Works run by E. I. du Pont de Nemours & Company. At that time, this works had the largest nitric acid plant in the world. The Woolwich process was expensive; it needed of strong nitric acid for every pound of RDX.
By early 1941, the NDRC was researching new processes. The Woolwich or direct nitration process has at least two serious disadvantages: (1) it used large amounts of nitric acid and (2) at least one-half of the formaldehyde is lost. One mole of hexamethylenetetramine could produce at most one mole of RDX. At least three laboratories with no previous explosive experience were tasked to develop better production methods for RDX; they were based at Cornell, Michigan and Penn State universities. Werner Emmanuel Bachmann, from Michigan, successfully developed the "combination process" by combining the Canadian process with direct nitration. The combination process required large quantities of acetic anhydride instead of nitric acid in the old British "Woolwich process". Ideally, the combination process could produce two moles of RDX from each mole of hexamethylenetetramine.
The vast production of RDX could not continue to rely on the use of natural beeswax to desensitize the RDX. A substitute stabilizer based on petroleum was developed at the Bruceton Explosives Research Laboratory.
Bachmann process.
The NDRC tasked three companies to develop pilot plants. They were the Western Cartridge Company, E. I. du Pont de Nemours & Company and Tennessee Eastman Company, part of Eastman Kodak. At the Eastman Chemical Company (TEC), a leading manufacturer of acetic anhydride, Werner Emmanuel Bachmann successfully developed a continuous-flow manufacturing process for RDX. RDX was crucial to the war effort and the current batch-production process could not keep up. In February 1942, TEC built the Wexler Bend pilot plant and began producing small amounts of RDX. This led to the US government authorizing TEC to design and build Holston Ordnance Works (H.O.W.) in June 1942. By April 1943, RDX was being manufactured there. At the end of 1944, the Holston plant and the Wabash River Ordnance Works (which used the Woolwich process) were making (50 million pounds) of Composition B per month.
The US Bachmann process for RDX was found to be richer in HMX than the United Kingdom's RDX. This later led to a RDX plant using the Bachmann process being set up at ROF Bridgwater in 1955, to produce both RDX and HMX.
Military compositions.
The United Kingdom's intention in World War II was to use "desensitised" RDX. In the original Woolwich process, RDX was desensitised with beeswax, but later RDX was desensitized with a petroleum-based product based on the work carried out at Bruceton. In the event the UK was unable to obtain sufficient RDX to meet its needs, some of the shortfall was met by substituting a mixture of ammonium nitrate and TNT.
Karl Dönitz was reputed to have claimed that "an aircraft can no more kill a U-boat than a crow can kill a mole". However, by May 1942 Wellington bombers began to deploy depth charges containing Torpex, a mixture of RDX, TNT and aluminium, which had up to 50 percent more destructive power than TNT-filled depth charges. Considerable quantities of the RDX–TNT mixture were produced at the Holston Ordnance Works, with Tennessee Eastman developing an automated mixing and cooling process based around the use of stainless steel conveyor belts.
Terrorism.
The 1993 Bombay bombings used RDX placed into several vehicles as bombs. RDX was the main component used for the 2006 Mumbai train bombings and the Jaipur bombings in 2008. It is also believed to be the explosive used in the 1999 Russian apartment bombings, 2004 Russian aircraft bombings, and 2010 Moscow Metro bombings.
Ahmed Ressam, the al-Qaeda Millennium Bomber, used a small quantity of RDX as one of the components in the explosives that he prepared to bomb Los Angeles International Airport on New Year's Eve 1999/2000; the combined explosives could have produced a blast forty times greater than that of a devastating car bomb.
In July 2012, the Kenyan government arrested two Iranian nationals and charged them with illegal possession of of RDX. According to the Kenyan Police, the Iranians planned to use the RDX for "attacks on Israel, US, UK and Saudi Arabian targets".
RDX was used to assassinate Lebanese prime minister Rafiq Hariri on February 14, 2005.
Toxicity.
RDX has caused convulsions (seizures) in military field personnel ingesting it, and in munition workers inhaling its dust during manufacture. The substance's toxicity has been studied for many years. At least one fatality was attributed to RDX toxicity in a European munitions manufacturing plant. The substance has low to moderate toxicity with a possible human carcinogen classification. However, further research is ongoing and this classification may be revised by the EPA. Remediating RDX contaminated water supplies has proven to be successful.
Biodegradation.
RDX produces high amounts of carbon dioxide and nitrous oxide when treated with sewage sludge or the fungus Phanaerocheate chrysosporium. A change of a N-NO2 or C-H bond of cyclic nitramine leads to cleavage of the ring; when the C-N bonds weaken. Both wild and transgenic plants can phytoremediate explosives from soil and water.

</doc>
<doc id="7500" url="http://en.wikipedia.org/wiki?curid=7500" title="Celebes (disambiguation)">
Celebes (disambiguation)

Celebes may refer to:

</doc>
<doc id="7502" url="http://en.wikipedia.org/wiki?curid=7502" title="Christianity and Judaism">
Christianity and Judaism

Christianity is rooted in Second Temple Judaism, but the two religions diverged in the first centuries of the Christian Era. Since the Ecumenical Councils, with the first held in 325, Christendom places emphasis on correct belief (or "orthodoxy"), focusing on the New Covenant that the Christian Triune God made through Jesus Christ, as recorded in the New Testament. Judaism places emphasis on the right conduct (or "orthopraxy"), focusing on the Mosaic Covenant that the God of Israel made with the Israelites, as recorded in the Torah and Talmud.
Christians obtain individual salvation from sin through repentance of sin and receiving Jesus Christ as their God and Savior through faith and grace. Jews individually and collectively participate in an eternal dialogue with God through tradition, rituals, prayers and ethical actions. Christianity worships a Triune God, one person of whom is also human. Judaism emphasizes the Oneness of God and rejects the Christian concept of God in human form.
Self-identification.
Judaism's purpose is to carry out what it holds to be the only Covenant between God and the Jewish people. The Torah (lit. "teaching"), both written and oral, tell the story of this covenant, and provides Jews with the terms of the covenant. The Oral Torah is the primary guide for Jews to abide by these terms, as expressed in tractate Gittin 60b, "the Holy One, Blessed be He, did not make His covenant with Israel except by virtue of the Oral Law" to help them learn how to live a holy life, and to bring holiness, peace and love into the world and into every part of life, so that life may be elevated to a high level of kedushah, originally through study and practice of the Torah, and since the destruction of the Second Temple, through prayer as expressed in tractate Sotah 49a "Since the destruction of the Temple, every day is more cursed than the preceding one; and the existence of the world is assured only by the kedusha...and the words spoken after the study of Torah." Since the adoption of the Amidah, the acknowledgement of God through the declaration from Yishayah 6:3 "Kadosh [holy], kadosh, kadosh, is HaShem, Master of Legions; the whole world is filled with His glory". as a replacement for the study of Torah, which is a daily obligation for a Jew, and sanctifies God in itself. This continuous maintenance of relationship between the individual Jew and God through either study, or prayer repeated three times daily, is the confirmation of the original covenant. This allows the Jewish people as a community to strive and fulfill the prophecy "I, the Lord, have called you in righteousness, and will hold your hand and keep you. And I will establish you as a Covenant of the people, for a light unto the nations." ( (i.e., a role model) over the course of history, and a part of the divine intent of bringing about an age of peace and sanctity where ideally a faithful life and good deeds should be ends in themselves, not means. See also Jewish principles of faith.
The self-described purpose of Christianity is to provide people with what it holds to be the only valid path to salvation as announced by the apostles of what the Book of Acts describes as, "The Way". Only in gentile (non-Jewish) settings is The Way referred to as Christian. According to Christian theologian Alister McGrath, the Jewish Christians affirmed every aspect of then contemporary Second Temple Judaism with the addition of the belief that Jesus was the messiah, with Isaiah 49:6, "an explicit parallel to 42:6" quoted by Paul in Acts 13:47 and reinterpreted by Justin the Martyr. According to Christian writers, most notably Paul, the Bible teaches that people are, in their current state, sinful, and the New Testament reveals that Jesus is both the Son of man and the Son of God, united in the hypostatic union, God the Son, God made incarnate; that Jesus' death by crucifixion was a sacrifice to atone for all of humanity's sins, and that acceptance of Jesus as Savior and Lord saves one from Divine Judgment, giving Eternal life. Jesus is the mediator of the New Covenant. His famous Sermon on the Mount is considered by some Christian scholars to be the proclamation of the New Covenant ethics, in contrast to the Mosaic Covenant of Moses from Mount Sinai. See also Christian theology.
National versus universal.
The subject of the Tanakh, or Hebrew Bible, is the history of the Children of Israel, especially in terms of their relationship with God. Thus, Judaism has also been characterized as a culture or as a civilization. In his work "Judaism as a Civilization", the founder of the Reconstructionist Movement, Rabbi Mordecai Kaplan defines Judaism as an evolving religious civilization. One crucial sign of this is that one need not believe, or even do, anything to be Jewish; the historic definition of 'Jewishness' requires only that one be born of a Jewish mother, or that one convert to Judaism in accord with Jewish law. (Today, Reform and Reconstructionist Jews also include those born of Jewish fathers and Gentile mothers if the children are raised as Jews.)
To many religious Jews, Jewish ethnicity is closely tied to their relationship with God, and thus has a strong theological component. This relationship is encapsulated in the notion that Jews are a chosen people. For strictly observant Jews, being "chosen" fundamentally means that it was God's wish that a group of people would exist in a covenant, and would be bound to obey a certain set of laws as a duty of their covenant, and that the Children of Israel "chose" to enter into this covenant with God. They view their divine purpose as being ideally a "light upon the nations" and a "holy people" (i.e., a people who live their lives fully in accordance with Divine will as an example to others), not "the one path to God". For Jews, salvation comes from God, freely given, and observance of the Law is one way of responding to God's grace.
Jews hold that other nations and peoples are not required (nor expected) to obey the Law of Moses, with the notable exception that the only laws Judaism believes are automatically binding (in order to be assured of a place in the world to come) on other nations are known as the Seven Laws of Noah. Thus, as an ethnic religion, Judaism holds that others may have their own, different, paths to God (or holiness, or "salvation"), as long as they are consistent with the "Seven Laws of Noah".
While ethnicity and culture play a large part in Jewish identity, they are not the only way Jews define themselves as Jews. There are secular Jews, who do use ethnicity and culture as their defining criteria. And there are religious Jews, who do not. Rather, religious Jews define their Jewishness within the context of their Judaism. In this context, a religious convert could "feel" more Jewish than a secular ethnic Jew. While Rabbi Kaplan defines Judaism as a civilization, there are many who would not agree, citing millennia of religious tradition and observance as more than simple civilization. Most observant Jews would say that Judaism is a love story.
Judaism and Christianity share the belief that there is One, True God, who is the only one worthy to be worshipped. Judaism sees this One, True God as a singular, ineffable, undefinable being. Phrases such as "Ground of All Being", "Unfolding Reality" and "Creator and Sustainer of Life" capture only portions of who God is to Jews. While God does not change, our perception of God does, and so, Jews are open to new experiences of God's presence. Christianity, with a few exceptions, sees the One, True God as having triune personhood: God the Father, God the Son (Jesus) and God the Holy Spirit. God is the same yesterday, today and tomorrow, so Christians generally look to the Scriptures (both Hebrew and Christian) for an understanding of who God is.
Christianity is characterized by its claim to universality, which marks a significant break from current Jewish identity and thought, but has its roots in Hellenistic Judaism. Christians believe that Jesus represents the fulfillment of God's promise to Abraham and the nation of Israel, that Israel would be a blessing to all nations. Most Christians believe that the Law was "completed" by Jesus and has become irrelevant to "faith life". Although Christians generally believe their religion to be very inclusive (since not only Jews but all gentiles can be Christian), Jews see Christianity as highly exclusive, because some denominations view non-Christians (such as Jews and Pagans) as having an incomplete or imperfect relationship with God, and therefore excluded from grace, salvation, heaven, or eternal life. For some Christians, it is the stated or "confessed" belief in Jesus as Savior that makes God's grace available to an individual, and salvation can come no other way (Solus Christus in Protestantism, Extra Ecclesiam nulla salus in Catholicism, see dual covenant theology for a traditional view). In Catholicism, sanctifying grace is ordinarily received via the Sacraments, however, God can work outside of the Sacraments. Also see "Invincible Ignorance" as understood in Catholic theology. 
This crucial difference between the two religions has other implications. For example, while in a conversion to Judaism a convert must accept basic Jewish principles of faith, and renounce all other religions, the process is more like a form of adoption, or changing national citizenship (i.e. becoming a formal member of the people, or tribe), with the convert becoming a "child of Abraham and Sarah." For many reasons, some historical and some religious, Judaism does not encourage its members to convert others and in fact would require the initiative from the person who would like to convert. In contrast, most Christian denominations actively seek converts, following the Great Commission, and conversion to Christianity is generally a declaration of faith (although some denominations view it specifically as adoption into a community of Christ, and orthodox Christian tradition views it as being a literal joining together of the members of Christ's body).
Both Christianity and Judaism have been affected by the diverse cultures of their respective members. For example, what Jews from Eastern Europe and from North Africa consider "Jewish food" has more in common with the cuisines of non-Jewish Eastern Europeans and North Africans than with each other, although for religious Jews all food-preparation must conform to the same laws of Kashrut. According to non-Orthodox Jews and critical historians, Jewish law too has been affected by surrounding cultures (for example, some scholars argue that the establishment of absolute monotheism in Judaism was a reaction against the dualism of Zoroastrianism that Jews encountered when living under Persian rule; Jews rejected polygamy during the Middle Ages, influenced by their Christian neighbors). According to Orthodox Jews too there are variations in Jewish custom from one part of the world to another. It was for this reason that Joseph Karo's Shulchan Aruch did not become established as the authoritative code of Jewish law until after Moshe Isserlis added his commentary, which documented variations in local custom.
Sacred texts.
The Hebrew Bible is composed of three parts; the Torah (Instruction, the Septuagint translated the Hebrew to "nomos" or "Law"), the Nevi'im (Prophets) and the Ketuvim (Writings). Collectively, these are known as the Tanakh. According to Rabbinic Judaism the Torah was revealed by God to Moses; within it, Jews find 613 Mitzvot (commandments).
Rabbinic tradition asserts that God revealed two Torahs to Moses, one that was written down, and one that was transmitted orally. Whereas the written Torah has a fixed form, the Oral Torah is a living tradition that includes not only specific supplements to the written Torah (for instance, what is the proper manner of "shechita" and what is meant by "Frontlets" in the Shema), but also procedures for understanding and talking about the written Torah (thus, the Oral Torah revealed at Sinai includes debates among rabbis who lived long after Moses). The Oral Law elaborations of narratives in the Bible and stories about the rabbis are referred to as "aggadah". It also includes elaboration of the 613 commandments in the form of laws referred to as "halakha". Elements of the Oral Torah were committed to writing and edited by Judah HaNasi in the Mishnah in 200 CE; much more of the Oral Torah were committed to writing in the Babylonian and Jerusalem Talmuds, which were edited around 600 CE and 450 CE, respectively. The Talmuds are notable for the way they combine law and lore, for their explication of the midrashic method of interpreting tests, and for their accounts of debates among rabbis, which preserve divergent and conflicting interpretations of the Bible and legal rulings.
Since the transcription of the Talmud, notable rabbis have compiled law codes that are generally held in high regard: the Mishneh Torah, the Tur, and the Shulchan Aruch. The latter, which was based on earlier codes and supplemented by the commentary by Moshe Isserles that notes other practices and customs practiced by Jews in different communities, especially among Ashkenazim, is generally held to be authoritative by Orthodox Jews. The Zohar, which was written in the 13th century, is generally held as the most important esoteric treatise of the Jews.
All contemporary Jewish movements consider the Tanakh, and the Oral Torah in the form of the Mishnah and Talmuds as sacred, although movements are divided as to claims concerning their divine revelation, and also their authority. For Jews, the Torah - written and oral - is the primary guide to the relationship between God and man, a living document that has unfolded and will continue to unfold whole new insights over the generations and millennia. A saying that captures this goes, "Turn it [the Torah's words] over and over again, for everything is in it."
Christians accept the Written Torah and other books of the Hebrew Bible as Scripture, although they generally give readings from the Koine Greek Septuagint translation instead of the Biblical Hebrew/Biblical Aramaic Masoretic Text. Two notable examples are:
Instead of the traditional Jewish order and names for the books, Christians organize and name the books closer to that found in the Septuagint. Some Christian denominations (such as Anglican, Roman Catholic, and Eastern Orthodox), include a number of books that are not in the Hebrew Bible (the biblical apocrypha or deuterocanonical books or Anagignoskomena, see Development of the Old Testament canon) in their biblical canon that are not in today's Jewish canon, although they were included in the Septuagint. Christians reject the Jewish Oral Torah, which was still in oral, and therefore unwritten, form in the time of Jesus.
Christians believe that God has established a new covenant with people through Jesus, as recorded in the Gospels, Acts of the Apostles, Epistles, and other books collectively called the New Testament (the word "testament" attributed to Tertullian is commonly interchanged with the word "covenant"). For some Christians, such as Roman Catholics and Orthodox Christians, this New Covenant includes authoritative Sacred Traditions and Canon law. Others, especially Protestants, reject the authority of such traditions and instead hold to the principle of "sola scriptura", which accepts only the Bible itself as the final rule of faith and practice. Additionally, some denominations include the "oral teachings of Jesus to the Apostles", which they believe have been handed down to this day by Apostolic Succession.
Christians refer to the Biblical books about Jesus as the New Testament, and to the canon of Hebrew books as the Old Testament, terms associated with Supersessionism. Judaism does not accept the retronymic labeling of its sacred texts as the "Old Testament", and some Jews refer to the New Testament as the Christian Testament or Christian Bible. Judaism rejects all claims that the Christian New Covenant supersedes, abrogates, fulfills, or is the unfolding or consummation of the covenant expressed in the Written and Oral Torahs. Therefore, just as Christianity does not accept that Mosaic Law has any authority over Christians, Judaism does not accept that the New Testament has any religious authority over Jews.
Many Jews view Christians as having quite an ambivalent view of the Torah, or Mosaic law: on one hand Christians speak of it as God's absolute word, but on the other, they apply its commandments with a certain selectivity (compare Biblical law in Christianity). Some Jews contend that Christians cite commandments from the Old Testament to support one point of view but then ignore other commandments of a similar class that are also of equal weight. Examples of this are certain commandments that God states explicitly shall abide "for ever" (for example , ), or certain practices which God prohibits as abominations, but which are not prohibited by most Christian denominations.
Christians explain that such selectivity is based on rulings made by early Jewish Christians in the Book of Acts, at the Council of Jerusalem, that, while believing gentiles did not need to fully convert to Judaism, they should follow some aspects of Torah like avoiding idolatry and fornication and blood, including, according to some interpretations, homosexuality. This view is also reflected by modern Judaism, in that Righteous Gentiles needn't convert to Judaism and need to observe only the Noahide Laws, which also contain prohibitions against idolatry and fornication and blood.
Some Christians agree that Jews who accept Jesus should still observe all of Torah, see for example Dual-covenant theology, based on warnings by Jesus to Jews not to use him as an excuse to disregard it, and they support efforts of those such as Messianic Jews (Messianic Judaism is considered by most Christians and Jews to be a form of Christianity) to do that, but some Protestant forms of Christianity oppose all observance to the Mosaic law, even by Jews, which Luther criticised as "Antinomianism", see Antinomianism#Antinomian Controversies in Lutheranism and Luther#Anti-Antinomianism for details.
A minority view in Christianity, known as Christian Torah-submission, holds that the Mosaic law as it is written is binding on all followers of God under the New Covenant, even for Gentiles, because it views God’s commands as "everlasting" (, ; , ; ) and "good" (; ; ).
Concepts of God.
Traditionally, both Judaism and Christianity believe in the God of Abraham, Isaac and Jacob, for Jews the God of the Tanakh, for Christians the God of the Old Testament, the creator of the universe. Judaism and major sects of Christianity reject the view that God is entirely immanent (although some see this as the concept of the Holy Ghost) and within the world as a physical presence, (although trinitarian Christians believe in the incarnation of God). Both religions reject the view that God is entirely transcendent, and thus separate from the world, as the pre-Christian Greek Unknown God. Both religions reject atheism on one hand and polytheism on the other.
Both religions agree that God shares both transcendent and immanent qualities. How these religions resolve this issue is where the religions differ. Christianity posits that God exists as a Trinity; in this view God exists as three distinct persons who share a single divine essence, or substance. In those three there is one, and in that one there are three; the one God is indivisible, while the three persons are distinct and unconfused, God the Father, God the Son, and God the Holy Spirit. It teaches that God became especially immanent in physical form through the Incarnation of God the Son who was born as Jesus of Nazareth, who is believed to be at once fully God and fully human. There are "Christian" sects that question one or more of these doctrines, however, see also Nontrinitarianism. By contrast, Judaism sees God as a single entity, and views trinitarianism as both incomprehensible and a violation of the Bible's teaching that God is one. It rejects the notion that Jesus or any other object or living being could be 'God', that God could have a literal 'son' in physical form or is divisible in any way, or that God could be made to be joined to the material world in such fashion. Although Judaism provides Jews with a word to label God's transcendence ("Ein Sof", without end) and immanence ("Shekhinah", in-dwelling), these are merely human words to describe two ways of experiencing God; God is one and indivisible.
Shituf.
A minority Jewish view, which appears in some codes of Jewish law, is that while Christian worship is polytheistic (due to the multiplicity of the Trinity), it is permissible for them to swear in God's name, since they are referring to the one God. This theology is referred to in Hebrew as Shituf (literally "partnership" or "association"). Although worship of a trinity is considered to be not different from any other form of idolatry for Jews, it may be an acceptable belief for non-Jews (according to the ruling of some Rabbinic authorities).
Right action.
Faith versus good deeds.
Judaism teaches that the purpose of the Torah is to teach us how to act correctly. God's existence is a given in Judaism, and not something that most authorities see as a matter of required belief. Although some authorities see the Torah as commanding Jews to believe in God, Jews see belief in God as a necessary, but not sufficient, condition for a Jewish life. The quintessential verbal expression of Judaism is the Shema Yisrael, the statement that the God of the Bible is their God, and that this God is unique and one. The quintessential physical expression of Judaism is behaving in accordance with the 613 Mitzvot (the commandments specified in the Torah), and thus live one's life in God's ways.
Thus fundamentally in Judaism, one is enjoined to bring holiness into life (with the guidance of God's laws), rather than removing oneself from life to be holy.
Much of Christianity also teaches that God wants people to perform good works, but all branches hold that good works alone will not lead to salvation, which is called Legalism, the exception being dual-covenant theology. Some Christian denominations hold that salvation depends upon transformational faith in Jesus, which expresses itself in good works as a testament (or witness) to ones faith for others to see (primarily Eastern Orthodox Christianity and Roman Catholicism), while others (including most Protestants) hold that faith alone is necessary for salvation. Some argue that the difference is not as great as it seems, because it really hinges on the definition of "faith" used. The first group generally uses the term "faith" to mean "intellectual and heartfelt assent and submission." Such a faith will not be salvific until a person has allowed it to effect a life transforming conversion (turning towards God) in their being (see Ontotheology). The Christians that hold to "salvation by faith alone" (also called by its Latin name "sola fide") define faith as being implicitly ontological—mere intellectual assent is not termed "faith" by these groups. Faith, then, is life-transforming by definition.
Sin.
In both religions, offenses against the will of God are called sin. These sins can be thoughts, words, or deeds.
Catholicism categorizes sins into various groups. A wounding of the relationship with God is often called venial sin; a complete rupture of the relationship with God is often called mortal sin. Without salvation from sin (see below), a person's separation from God is permanent, causing such a person to enter Hell in the afterlife. Both the Catholic Church and the Orthodox Church define sin more or less as a "macula", a spiritual stain or uncleanliness that constitutes damage to man's image and likeness of God.
Hebrew has several words for sin, each with its own specific meaning. The word "pesha", or "trespass", means a sin done out of rebelliousness. The word "aveira" means "transgression". And the word "avone", or "iniquity", means a sin done out of moral failing. The word most commonly translated simply as "sin", "het", literally means "to go astray." Just as Jewish law, "halakha" provides the proper "way" (or path) to live, sin involves straying from that path. Judaism teaches that humans are born with free will, and morally neutral, with both a "yetzer hatov", (literally, "the good inclination", in some views, a tendency towards goodness, in others, a tendency towards having a productive life and a tendency to be concerned with others) and a "yetzer hara", (literally "the evil inclination", in some views, a tendency towards evil, and in others, a tendency towards base or animal behavior and a tendency to be selfish). In Judaism all human beings are believed to have free will and can choose the path in life that they will take. It does not teach that choosing good is impossible - only at times more difficult. There is almost always a "way back" if a person wills it. (Although texts mention certain categories for whom the way back will be exceedingly hard, such as the slanderer, the habitual gossip, and the malicious person)
The rabbis recognize a positive value to the "yetzer hara": one tradition identifies it with the observation on the last day of creation that God's accomplishment was "very good" (God's work on the preceding days was just described as "good") and explain that without the yetzer ha'ra there would be no marriage, children, commerce or other fruits of human labor; the implication is that yetzer ha'tov and yetzer ha'ra are best understood not as moral categories of good and evil but as selfless versus selfish orientations, either of which used rightly can serve God's will.
In contrast to the Jewish view of being morally balanced, Original Sin refers to the idea that the sin of Adam and Eve's disobedience (sin "at the origin") has passed on a spiritual heritage, so to speak. Christians teach that human beings inherit a corrupted or damaged human nature in which the tendency to do bad is greater than it would have been otherwise, so much so that human nature would not be capable now of participating in the afterlife with God. This is not a matter of being "guilty" of anything; each person is only personally guilty of their own actual sins. However, this understanding of original sin is what lies behind the Christian emphasis on the need for spiritual salvation from a spiritual Saviour, who can forgive and set aside sin even though humans are not inherently pure and worthy of such salvation. St. Paul in Romans and I Corinthians placed special emphasis on this doctrine, and stressed that belief in Jesus would allow Christians to overcome death and attain salvation in the hereafter.
Roman Catholics, Eastern Orthodox Christians, and some Protestants teach the Sacrament of Baptism is the means by which each person's damaged human nature is healed and Sanctifying Grace (capacity to enjoy and participate in the spiritual life of God) is restored. This is referred to as "being born of water and the Spirit", following the terminology in the Gospel of St. John. Most Protestants believe this salvific grace comes about at the moment of personal decision to follow Jesus, and that baptism is a symbol of the grace already received.
Love.
Although love is central to both Christianity and Judaism, literary critic Harold Bloom (in his "Jesus and Yahweh: The Names Divine") argues that their notions of love are fundamentally different. Specifically, he links the Jewish conception of love to justice, and the Christian conception of love to charity.
As in English, the Hebrew word for "love", ahavah אהבה, is used to describe intimate or romantic feelings or relationships, such as the love between parent and child in Genesis 22:2; 25: 28; 37:3; the love between close friends in I Samuel 18:2, 20:17; or the love between a young man and young woman in Song of Songs. Christians will often use the Septuagint to make distinctions between the types of love: "philia" for brotherly, "eros" for romantic and "agape" for self-sacrificing love.
Like many Jewish scholars and theologians, Bloom understands Judaism as fundamentally a religion of love. But he argues that one can understand the Hebrew conception of love only by looking at one of the core commandments of Judaism, Leviticus 19:18, "Love your neighbor as yourself", also called the second Great Commandment. Talmudic sages Hillel and Rabbi Akiva commented that this is a major element of the Jewish religion. Also, this commandment is arguably at the center of the Jewish faith. As the third book of the Torah, Leviticus is literally the central book. Historically, Jews have considered it of central importance: traditionally, children began their study of the Torah with Leviticus, and the midrashic literature on Leviticus is among the longest and most detailed of midrashic literature (see Bamberger 1981: 737). Bernard Bamberger considers Leviticus 19, beginning with God's commandment in verse 3—"You shall be holy, for I the Lord your God, am holy"—to be "the climactic chapter of the book, the one most often read and quoted" (1981:889). Leviticus 19:18 is itself the climax of this chapter.
Abortion.
The only statements in the Tanakh about the status of a fetus state that killing an unborn infant does not have the same status as killing a born human being, and mandates a much lesser penalty (Exodus 21: 22-25).
The Talmud states that the fetus is not yet a full human being until it has been born (either the head or the body is mostly outside of the woman), therefore killing a fetus is not murder, and abortion - in restricted circumstances - has always been legal under Jewish law. Rashi, the great 12th century commentator on the Bible and Talmud, states clearly of the fetus "lav nefesh hu": "it is not a person." The Talmud contains the expression "ubar yerech imo"—the fetus is as the thigh of its mother,' i.e., the fetus is deemed to be part and parcel of the pregnant woman's body." The Babylonian Talmud Yevamot 69b states that: "the embryo is considered to be mere water until the fortieth day." Afterwards, it is considered subhuman until it is born. Christians who agree with these views may refer to this idea as abortion before the quickening of the fetus.
Two additional passages in the Talmud that shed some light on the Jewish belief that the fetus is considered part of the woman, and not a separate entity. One section states that if a man purchases a cow that is found to be pregnant, then he is the owner both of the cow and the calf that is born from it. Another states that if a pregnant woman converts to Judaism, that her conversion applies also to her fetus.
Judaism unilaterally supports, in fact mandates, abortion if doctors believe that it is necessary to save the life of the woman. Many rabbinic authorities allow abortions on the grounds of gross genetic imperfections of the fetus. They also allow abortion if the woman were suicidal because of such defects. However, Judaism holds that abortion is impermissible for family planning or convenience reasons. Each case must be decided individually, however, and the decision should lie with the pregnant woman, the man who impregnated her and their Rabbi.
War, violence and pacifism.
Jews and Christians accept as valid and binding many of the same moral principles taught in the Torah. There is a great deal of overlap between the ethical systems of these two faiths. Nonetheless, there are some highly significant doctrinal differences.
Judaism has many teachings about peace and compromise, and its teachings make physical violence the last possible option. Nonetheless, the Talmud teaches that "If someone comes with the intention to murder you, then one is obligated to kill in self-defense [rather than be killed]". The clear implication is that to bare one's throat would be tantamount to suicide (which Jewish law forbids) and it would also be considered helping a murderer kill someone and thus would "place an obstacle in front of a blind man" (i.e., makes it easier for another person to falter in their ways). The tension between the laws dealing with peace, and the obligation to self-defense, has led to a set of Jewish teachings that have been described as tactical-pacifism. This is the avoidance of force and violence whenever possible, but the use of force when necessary to save the lives of one's self and one's people.
Although killing oneself is forbidden under normal Jewish law as being a denial of God's goodness in the world, under extreme circumstances when there has seemed no choice but to either be killed or forced to betray their religion, Jews have committed suicide or mass suicide (see Masada, First French persecution of the Jews, and York Castle for examples). As a grim reminder of those times, there is even a prayer in the Jewish liturgy for "when the knife is at the throat", for those dying "to sanctify God's Name". (See: "Martyrdom"). These acts have received mixed responses by Jewish authorities. Where some Jews regard them as examples of heroic martyrdom, but others saying that while Jews should always be willing to face martyrdom if necessary, it was wrong for them to take their own lives.
Because Judaism focuses on this life, many questions to do with survival and conflict (such as the classic moral dilemma of two people in a desert with only enough water for one to survive) were analysed in great depth by the rabbis within the Talmud, in the attempt to understand the principles a godly person should draw upon in such a circumstance.
The Sermon on the Mount records that Jesus taught that if someone comes to harm you, then one must turn the other cheek. This has led four Protestant Christian denominations to develop a theology of pacifism, the avoidance of force and violence at all times. They are known historically as the "peace churches", and have incorporated Christ's teachings on nonviolence into their theology so as to apply it to participation in the use of violent force; those denominations are the Quakers, Mennonites, Amish, and the Church of the Brethren. Many other churches have people who hold to the doctrine without making it a part of their doctrines, or who apply it to individuals but not to governments, see also Evangelical counsels. The vast majority of Christian nations and groups have not adopted this theology, nor have they followed it in practice. See also But to bring a sword.
Capital punishment.
Although the Hebrew Bible has many references to capital punishment, the Jewish sages used their authority to make it nearly impossible for a Jewish court to impose a death sentence. Even when such a sentence might have been imposed, the Cities of Refuge and other sanctuaries, were at hand for those unintentionally guilty of capital offences. It was said in the Talmud about the death penalty in Judaism, that if a court killed more than one person in seventy years, it was a barbarous (or "bloody") court and should be condemned as such.
Christianity usually reserved the death penalty for heresy, the denial of the orthodox view of God's view, and witchcraft or similar non-Christian practices. For example, in Spain, unrepentant Jews were exiled, and it was only those crypto-Jews who had accepted baptism under pressure but retained Jewish customs in private, who were punished in this way. It is presently acknowledged by most of Christianity that these uses of capital punishment were deeply immoral.
Taboo food and drink.
Orthodox Jews, unlike most Christians, still practice a restrictive diet that has many rules. Most Christians believe that the kosher food laws have been superseded, for example citing what Jesus taught in Mark 7: what you eat doesn't make you unclean but what comes out of a man's heart makes him unclean — although Roman Catholicism and Eastern Orthodoxy have their own set of dietary observances. Eastern Orthodoxy, in particular has very elaborate and strict rules of fasting, and continues to observe the Council of Jerusalem's apostolic decree of Act 15.
Some Christian denominations observe some biblical food laws, for example the practice of Ital in Rastifarianism. Jehovah's Witnesses do not eat blood products and are known for their refusal to accept blood transfusions based on not "eating blood".
Salvation.
Judaism does not see human beings as inherently flawed or sinful and needful of being saved from it, but rather capable with a free will of being righteous, and unlike Christianity does not closely associate ideas of "salvation" with a New Covenant delivered by a Jewish messiah, although in Judaism Jewish people will have a renewed national commitment of observing God's commandments under the New Covenant, and the Jewish Messiah will also be ruling at a time of global peace and acceptance of God by all people.
Judaism holds instead that proper living is accomplished through good works and heartfelt prayer, as well as a strong faith in God. Judaism also teaches that gentiles can receive a share in "the world to come". This is codified in the Mishna Avot 4:29, the Babylonian Talmud in tractates Avodah Zarah 10b, and Ketubot 111b, and in Maimonides's 12th century law code, the "Mishneh Torah", in "Hilkhot Melachim" (Laws of Kings) 8.11.
The Protestant view is that every human is a sinner, and being saved by God's grace, not simply by the merit of one's own actions, pardons a damnatory sentence to Hell. 
Judgment.
Both Christianity and Judaism believe in some form of judgment. Most Christians (the exception is Full Preterism) believe in the future Second Coming of Jesus, which includes the Resurrection of the Dead and the Last Judgment. Those who have accepted Jesus as their personal saviour will be saved and live in God's presence in the Kingdom of Heaven, those who have not accepted Jesus as their saviour,will be cast into the Lake of Fire (eternal Hell or simply annihilated), see for example The Sheep and the Goats.
In Jewish liturgy there is significant prayer and talk of a "book of life" that one is written into, indicating that God judges each person each year even after death. This annual judgment process begins on Rosh Hashanah and ends with Yom Kippur. Additionally, God sits daily in judgment concerning a person's daily activities. Upon the anticipated arrival of the Messiah, God will judge the nations for their persecution of Israel during the exile. Later, God will also judge the Jews over their observance of the Torah.
Heaven and Hell.
There is little Jewish literature on heaven or hell as actual places, and there are few references to the afterlife in the Hebrew Bible. One is the ghostly apparition of Samuel, called up by the Witch of Endor at King Saul's command. Another is a mention by the Prophet Daniel of those who sleep in the earth rising to either everlasting life or everlasting abhorrence.
Early Hebrew views were more concerned with the fate of the nation of Israel as a whole, rather than with individual immortality. A stronger belief in an afterlife for each person developed during the Second Temple period but was contested by various Jewish sects. Pharisees believed that in death, people rest in their graves until they are physically resurrected with the coming of the Messiah, and within that resurrected body the soul would exist eternally. Maimonides also included the concept of resurrection in his Thirteen Principles of Faith.
Judaism's view is summed up by a biblical observation about the Torah: in the beginning God clothes the naked (Adam), and at the end God buries the dead (Moses). The Children of Israel mourned for 40 days, then got on with their lives.
In Judaism, Heaven is sometimes described as a place where God debates Talmudic law with the angels, and where Jews spend eternity studying the Written and Oral Torah. Jews do not believe in "Hell" as a place of eternal torment. Gehenna is a place or condition of purgatory where Jews spend up to twelve months purifying to get into heaven, depending on how sinful they have been, although some suggest that certain types of sinners can never be purified enough to go to heaven and rather than facing eternal torment, simply cease to exist. Therefore, some violations like suicide would be punished by separation from the community, such as not being buried in a Jewish cemetery (in practice, rabbis often rule suicides to be mentally incompetent and thus not responsible for their actions). Judaism also does not have a notion of hell as a place ruled by Satan since God's dominion is total and Satan is only one of God's angels.
Catholics also believe in a purgatory for those who are going to heaven, but Christians in general believe that Hell is a fiery place of torment that never ceases, called the Lake of Fire. A small minority believe this is not permanent, and that those who go there will eventually either be saved or cease to exist. Heaven for Christians is depicted in various ways. As the Kingdom of God described in the New Testament and particularly the Book of Revelation, Heaven is a new or restored earth, a World to Come, free of sin and death, with a New Jerusalem led by God, Jesus, and the most righteous of believers starting with 144,000 Israelites from every tribe, and all others who received salvation living peacefully and making pilgrimages to give glory to the city.
In Christianity, promises of Heaven and Hell as rewards and punishments are often used to motivate good and bad behavior, as threats of disaster were used by prophets like Jeremiah to motivate the Israelites. Modern Judaism generally rejects this form of motivation, instead teaching to do the right thing because it's the right thing to do. As Maimonides wrote:
"A man should not say: I shall carry out the precepts of the Torah and study her wisdom in order to receive all the blessings written therein or in order to merit the life of the World to Come and I shall keep away from the sins forbidden by the Torah in order to be spared the curses mentioned in the Torah or in order not to be cut off from the life of the World to Come. It is not proper to serve God in this fashion. For one who serves thus serves out of fear. Such a way is not that of the prophets and sages. Only the ignorant, and the women and children serve God in this way. These are trained to serve out of fear until they obtain sufficient knowledge to serve out of love. One who serves God out of love studies the Torah and practices the precepts and walks in the way of wisdom for no ulterior motive at all, neither out of fear of evil nor in order to acquire the good, but follows the truth because it is true and the good will follow the merit of attaining to it. It is the stage of Abraham our father whom the Holy One, blessed be God, called "My friend" (Isaiah 41:8—"ohavi" = the one who loves me) because he served out of love alone. It is regarding this stage that the Holy One, Blessed be God, commanded us through Moses, as it is said: "You shall love the Lord your God" (Deuteronomy 6:5). When man loves God with a love that is fitting he automatically carries out all the precepts of love.
The Messiah.
Jews believe that a descendant of King David will one day appear to restore the Kingdom of Israel and usher in an era of peace, prosperity, and spiritual understanding for Israel and all the nations of the world. Jews refer to this person as Moshiach or "anointed one", translated as messiah in English. The traditional Jewish understanding of the messiah is that he is fully human and born of human parents without any supernatural element. The messiah is expected to have a relationship with God similar to that of the prophets of the Tanakh. In his commentary on the Talmud, Maimonides (Rabbi Moshe ben Maimon) wrote:
He adds:
He also clarified the nature of the Messiah:
The Christian view of Jesus as Messiah goes beyond such claims and is the fulfillment and union of three anointed offices; a prophet like Moses who delivers God's commands and covenant and frees people from bondage, a High Priest in the order of Melchizedek overshadowing the Levite priesthood and a king like King David ruling over Jews, and like God ruling over the whole world and coming from the line of David.
For Christians, Jesus is also fully human and fully divine as the Word of God who sacrifices himself so that humans can receive salvation. Jesus sits in Heaven at the Right Hand of God and will judge humanity in the end times when he returns to earth.
Christian readings of the Hebrew Bible find many references to Jesus. This takes the form in some cases of specific prophesy, but in most cases of foreshadowing by types or forerunners. Traditionally, most Christian readings of the Bible maintained that almost every prophecy was actually about the coming of Jesus, and that the entire Old Testament of the Bible is a prophecy about the coming of Jesus.
Catholic views.
Catholicism teaches "Extra Ecclesiam Nulla Salus" ("Outside the Church there is no salvation"), which some, like Fr. Leonard Feeney, interpreted as limiting salvation to Catholics only. At the same time, it does not deny the possibility that those not visibly members of the Church may attain salvation as well. In recent times, its teaching has been most notably expressed in the Vatican II council documents "Unitatis Redintegratio" (1964), "Lumen Gentium" (1964), "Nostra aetate" (1965), an encyclical issued by Pope John Paul II: "Ut Unum Sint" (1995), and in a document issued by the Congregation for the Doctrine of the Faith, "Dominus Iesus" in 2000. The latter document has been criticised for claiming that non-Christians are in a "gravely deficient situation" as compared to Catholics, but also adds that "for those who are not formally and visibly members of the Church, salvation in Christ is accessible by virtue of a grace which, while having a mysterious relationship to the Church, does not make them formally part of the Church, but enlightens them in a way which is accommodated to their spiritual and material situation."
Pope John Paul II on October 2, 2000 emphasized that this document did not say that non-Christians were actively denied salvation: "...this confession does not deny salvation to non-Christians, but points to its ultimate source in Christ, in whom man and God are united". On December 6 the Pope issued a statement to further emphasize that the Church continued to support its traditional stance that salvation was available to believers of other faiths: "The gospel teaches us that those who live in accordance with the Beatitudes--the poor in spirit, the pure of heart, those who bear lovingly the sufferings of life--will enter God's kingdom." He further added, "All who seek God with a sincere heart, including those who do not know Christ and his church, contribute under the influence of Grace to the building of this Kingdom." On August 13, 2002 American Catholic bishops issued a joint statement with leaders of Reform and Conservative Judaism, called "Reflections on Covenant and Mission", which affirmed that Christians should not target Jews for conversion. The document stated: "Jews already dwell in a saving covenant with God" and "Jews are also called by God to prepare the world for God's Kingdom." However, some U.S.-led Baptist and other fundamentalist denominations still believe it is their duty to engage in what they refer to as outreach to "unbelieving" Jews.
Eastern Orthodox views.
Eastern Orthodox Christianity emphasizes a continuing life of repentance or "metanoia", which includes an increasing improvement in thought, belief and action. Regarding the salvation of Jews, Muslims, and other non-Christians, the Orthodox have traditionally taught that there is no salvation outside the church. Orthodoxy recognizes that other religions may contain truth, to the extent that they are in agreement with Christianity.
Many Orthodox theologians believe that all people will have an opportunity to embrace union with God, including Jesus, after their death, and so become part of the Church at that time. God is thought to be good, just, and merciful; it would not seem just to condemn someone because they never heard the Gospel message, or were taught a distorted version of the Gospel by heretics. Therefore, the reasoning goes, they must at some point have an opportunity to make a genuine informed decision. Ultimately, those who persist in rejecting God condemn themselves, by cutting themselves off from the ultimate source of all Life, and from the God who is Love embodied. Jews, Muslims, and members of other faiths, then, are expected to convert to Christianity in the afterlife. The Church of Jesus Christ of Latter-day Saints also holds this belief, and holds baptismal services in which righteous people are baptized in behalf of their ancestors who, it is believed, are given the opportunity to accept the ordinance.
Proselytizing.
Judaism is not a proselytizing religion. Orthodox Judaism deliberately makes it very difficult to convert and become a Jew, and requires a significant and full-time effort in living, study, righteousness, and conduct over several years. The final decision is by no means a foregone conclusion. A person cannot become Jewish by marrying a Jew, or by joining a synagogue, nor by any degree of involvement in the community or religion, but only by explicitly undertaking intense, formal, and supervised work over years aimed towards that goal. Some less strict versions of Judaism have made this process somewhat easier but it is still far from common.
In the past Judaism was more evangelistic, but this was often more akin just to "greater openness to converts" rather than active soliciting of conversions. Since Jews believe that one need not be a Jew to approach God, there is no religious pressure to convert non-Jews to their faith.
The Chabad-Lubavitch branch of Hasidic Judaism has been an exception to this non-proselytizing standard, since in recent decades it has been actively promoting Noahide Laws for Gentiles as an alternative to Christianity.
By contrast, Christianity is an explicitly evangelistic religion. Christians are commanded by Jesus to "Therefore go and make disciples of all nations". Historically, evangelism has on rare occasions led to forced conversion under threat of death or mass expulsion. While abuses of this sort are no longer common, at certain times and in certain places, evangelism has veered into high-pressure coercion, in those instances causing significant ill-will.
Mutual views.
Common Jewish views of Christianity.
Many Jews view Jesus as one in a long list of failed Jewish claimants to be the Messiah, none of whom fulfilled the tests of a prophet specified in the Law of Moses. Others see Jesus as a teacher who worked with the gentiles and ascribe the messianic claims they find objectionable to his later followers. Because much physical and spiritual violence was done to Jews in the name of Jesus and his followers, and because evangelism is still an active aspect of many church's activities, many Jews are uncomfortable with discussing Jesus and treat him as a non-person. In answering the question "What do Jews think of Jesus", philosopher Milton Steinberg claims, for Jews, Jesus cannot be accepted as anything more than a teacher. "In only a few respects did Jesus deviate from the Tradition," Steinberg concludes, "and in all of them, Jews believe, he blundered."
Judaism does not believe that God requires the sacrifice of any human. This is emphasized in Jewish traditions concerning the story of the Akedah, the binding of Isaac. In the Jewish explanation, this is a story in the Torah whereby God wanted to test Abraham's faith and willingness, and Isaac was never going to be actually sacrificed. Thus, Judaism rejects the notion that anyone can or should die for anyone else's sin. Judaism is more focused on the practicalities of understanding how one may live a sacred life in the world according to God's will, rather than a hope of a future one. Judaism does not believe in the Christian concept of hell but does have a punishment stage in the afterlife (i.e. Gehenna, the New Testament word translated as hell) as well as a Heaven (Gan Eden), but the religion does not intend it as a focus.
Judaism views the worship of Jesus as inherently polytheistic, and rejects the Christian attempts to explain the Trinity as a complex monotheism. Christmas and other Christian festivals have no religious significance in Judaism and are not celebrated. Celebration of Christian holy days is considered Avodah Zarah or idolatry and is forbidden; however some secular Jews in the West treat Christmas as a secular holiday.
Common Christian views of Judaism.
Christians believe that Christianity is the fulfillment and successor of Judaism, retaining much of its doctrine and many of its practices including monotheism, the belief in a Messiah, and certain forms of worship like prayer and reading from religious texts. Christians believe that Judaism requires blood sacrifice to atone for sins, and believe that Judaism has abandoned this since the destruction of the Second Temple. Most Christians consider the Mosaic Law to have been a necessary intermediate stage, but that once the crucifixion of Jesus occurred, adherence to civil and ceremonial Law was superseded by the New Covenant.
Some Christians adhere to New Covenant theology, which states that with the arrival of his New Covenant, Jews have ceased being blessed under his Mosaic covenant. This position has been softened or disputed by other Christians, where Jews are recognized to have a special status under the Abrahamic covenant. New Covenant theology is thus in contrast to Dual-covenant theology.
Some Christians who view the Jewish people as close to God seek to understand and incorporate elements of Jewish understanding or perspective into their beliefs as a means to respect their "parent" religion of Judaism, or to more fully seek out and return to their Christian roots. Christians embracing aspects of Judaism are sometimes criticized as Biblical Judaizers by Christians when they pressure Gentile Christians to observe Mosaic teachings rejected by most modern Christians.
Inter-relationship.
In addition to each having varied views on the other as a religion, there has also been a long and often painful history of conflict, persecution and at times, reconciliation, between the two religions, which have influenced their mutual views of their relationship over time.
Persecution, forcible conversion, and forcible displacement of Jews (i.e. hate crimes) occurred for many centuries, with occasional gestures to reconciliation from time to time. Pogroms were common throughout Christian Europe, including organized violence, restrictive land ownership and professional lives, forcible relocation and ghettoization, mandatory dress codes, and at times humiliating actions and torture. All had major effects on Jewish cultures. There have also been non-coercive outreach and missionary efforts such as the Church of England's Ministry Among Jewish People, founded in 1809.
What is clear is that formally, there is mostly peaceful living side by side, with strong inter-dialogue at many levels to reconcile past differences between the two groups, and many Christians emphasize common historical heritage and religious continuity with the ancient spiritual lineage of the Jews. Christians and Jews attempt to coexist ultimately by recognizing the fact that they both worship the same Almighty God, that they both recognize several of the same prophets, and that both religions attempt to make the world a better place. Christians view the Jews as keepers of the Old Covenant. Jews view Christians as gentiles who worship the one God, but otherwise have their own distinct religion. What is also likely is that for a long time to come, some within each will continue to consider the other with varying degrees of suspicion and hostility. 
For Martin Buber, Judaism and Christianity were variations on the same theme of messianism. Buber made this theme the basis of a famous definition of the tension between Judaism and Christianity:
Pre-messianically, our destinies are divided. Now to the Christian, the Jew is the incomprehensibly obdurate man who declines to see what has happened; and to the Jew, the Christian is the incomprehensibly daring man who affirms in an unredeemed world that its redemption has been accomplished. This is a gulf which no human power can bridge.
Following the Holocaust, attempts have been made to construct a new Jewish-Christian relationship of mutual respect for differences, through the inauguration of the interfaith body the Council of Christians and Jews in 1942 and International Council of Christians and Jews. The Seelisberg Conference in 1947 established 10 points relating to the sources of Christian antisemitism. The ICCJ's "Twelve points of Berlin" sixty years later aim to reflect a recommitment to interreligious dialogue between the two communities.

</doc>
<doc id="7504" url="http://en.wikipedia.org/wiki?curid=7504" title="Cesare Borgia">
Cesare Borgia

Cesare Borgia (; Valencian: Cèsar Borja, ; , ; 13 September 1475 or April 1476 – 12 March 1507), Duke of Valentinois, was an Italian "condottiero", nobleman, politician, and cardinal. He was the son of Pope Alexander VI (r. 1492–1503) and his long-term mistress Vannozza dei Cattanei. He was the brother of Lucrezia Borgia; Giovanni Borgia (Juan), Duke of Gandia; and Gioffre Borgia (Jofré in Valencian), Prince of Squillace. He was half-brother to Don Pedro Luis de Borja (1460–88) and Girolama de Borja, children of unknown mothers.
After initially entering the church and becoming a cardinal on his father's election to the Papacy, he became the first person to resign a cardinalcy after the death of his brother in 1498. His father set him up as a prince with territory carved from the Papal States, but after his father's death he was unable to retain power for long, according to Machiavelli this was due to his planning for all possibilities but his own illness. After escaping from prison Cesare died fighting in Spain.
Early life.
Like nearly all aspects of Cesare Borgia's life, the date of his birth is a subject of dispute. He was born in Rome—in either 1475 or 1476—the son of Cardinal Rodrigo de Lanzol y Borgia, later Pope Alexander VI, and his mistress Vannozza dei Cattanei, about whom information is sparse. The Borgia family originally came from the Kingdom of Valencia, and rose to prominence during the mid-15th century; Cesare's grand-uncle Alphonso Borgia (1378–1458), bishop of Valencia, was elected Pope Callixtus III in 1455. Cesare's father, Pope Alexander VI, was the first pope who openly recognized his children born out of wedlock.
Stefano Infessura writes that Cardinal Borgia falsely claimed Cesare to be the legitimate son of another man—Domenico d'Arignano, the nominal husband of Vannozza de' Cattanei. More likely, Pope Sixtus IV granted Cesare a release from the necessity of proving his birth in a papal bull of 1 October 1480.
Career.
Church office.
Cesare was initially groomed for a career in the Church. He was made Bishop of Pamplona at the age of 15. Following school in Perugia and Pisa, Cesare studied at the "Studium Urbis" (nowadays Sapienza University of Rome), along with his father's elevation to Pope, Cesare was made Cardinal at the age of 18.
Alexander VI staked the hopes of the Borgia family in Cesare's brother Giovanni, who was made captain general of the military forces of the papacy. Giovanni was assassinated in 1497 in mysterious circumstances. Several contemporaries suggested that Cesare might have been his killer, as Giovanni's disappearance could finally open to him a long-awaited military career and also solve the jealousy over Sancha of Aragon, wife of Cesare's younger brother, Gioffre, and mistress of both Cesare and Giovanni. Cesare's role in the act has never been clear. However, he had no definitive motive, as he was likely to be given a powerful secular position, despite whether or not his brother lived. It is more likely, in fact, that Giovanni was killed as a result of a sexual liaison.
On 17 August 1498, Cesare became the first person in history to resign the cardinalate. On the same day, Louis XII of France named Cesare Duke of Valentinois, and this title, along with his former position as Cardinal of Valencia, explains the nickname "Valentino".
Military.
Cesare's career was founded upon his father's ability to distribute patronage, along with his alliance with France (reinforced by his marriage with Charlotte d'Albret, sister of John III of Navarre), in the course of the Italian Wars. Louis XII invaded Italy in 1499: after Gian Giacomo Trivulzio had ousted its duke Ludovico Sforza, Cesare accompanied the king in his entrance into Milan.
 At this point Alexander decided to profit from the favourable situation and carve out for Cesare a state of his own in northern Italy. To this end, he declared that all his vicars in Romagna and Marche were deposed. Though in theory subject directly to the pope, these rulers had been practically independent or dependent on other states for generations. In the view of the citizens, these vicars were cruel and petty. When Cesare eventually took power, he was viewed by the citizens as a great improvement.
Cesare was appointed commander of the papal armies with a number of Italian mercenaries, supported by 300 cavalry and 4,000 Swiss infantry sent by the King of France. Alexander sent him to capture Imola and Forlì, ruled by Caterina Sforza (mother of the Medici "condottiero" Giovanni dalle Bande Nere). Despite being deprived of his French troops after the conquest of those two cities, Borgia returned to Rome to celebrate a triumph and to receive the title of Papal Gonfalonier from his father. In 1500 the creation of twelve new cardinals granted Alexander enough money for Cesare to hire the "condottieri," Vitellozzo Vitelli, Gian Paolo Baglioni, Giulio and Paolo Orsini, and Oliverotto da Fermo, who resumed his campaign in Romagna.
Giovanni Sforza, first husband of Cesare's sister Lucrezia, was soon ousted from Pesaro; Pandolfo Malatesta lost Rimini; Faenza surrendered, its young lord Astorre III Manfredi being later drowned in the Tiber river by Cesare's order. In May 1501 the latter was created duke of Romagna. Hired by Florence, Cesare subsequently added the lordship of Piombino to his new lands.
While his "condottieri" took over the siege of Piombino (which ended in 1502), Cesare commanded the French troops in the sieges of Naples and Capua, defended by Prospero and Fabrizio Colonna. On 24 June 1501 his troops stormed the latter, causing the collapse of Aragonese power in southern Italy.
In June 1502 he set out for Marche, where he was able to capture Urbino and Camerino by treason. He planned to conquer Bologna next. However, his "condottieri", most notably Vitellozzo Vitelli and the Orsini brothers, feared Cesare's cruelty and set up a plot against him. Guidobaldo da Montefeltro and Giovanni Maria da Varano returned to Urbino and Camerino, and Fossombrone revolted. The fact that his subjects had enjoyed his rule thus far meant that his opponents had to work much harder than they would have liked. He eventually recalled his loyal generals to Imola, where he waited for his opponents' loose alliance to collapse. Cesare called for a reconciliation, but imprisoned his "condottieri" in Senigallia, then called Sinigaglia, a feat described as a "wonderful deceiving" by Paolo Giovio, and had them executed.
Later years.
Although he was an immensely capable general and statesman, Cesare would have trouble maintaining his domain without continued Papal patronage. Niccolò Machiavelli cites Cesare's dependence on the good will of the Papacy, under the control of his father, to be the principal weakness of his rule. Machiavelli argued that, had Cesare been able to win the favor of the new Pope, he would have been a very successful ruler. The news of his father's death (1503) arrived when Cesare was planning the conquest of Tuscany. While he was convalescing in Castel Sant'Angelo, his troops controlled the conclave. The new pope, Pius III, supported Cesare Borgia and reconfirmed him as Gonfalonier; but after a brief pontificate of twenty-six days he died. Borgia's deadly enemy, Giuliano Della Rovere, then succeeded by dexterous diplomacy in tricking the weakened Cesare Borgia into supporting him by offering him money and continued papal backing for Borgia policies in the Romagna; promises which he disregarded upon election. He was elected as Pope Julius II to the papal dignity by the near-unanimous vote of the cardinals. Realizing his mistake by then, Cesare tried to correct the situation to his favor, but Pope Julius II made sure of its failure at every turn.
Cesare Borgia, who was facing the hostility of Ferdinand II of Aragon, was betrayed while in Naples by Gonzalo Fernández de Córdoba, a man he had considered his ally, and imprisoned there, while his lands were retaken by the Papacy. In 1504 he was transferred to Spain and imprisoned first in the Castle of Chinchilla de Montearagón, but after an attempted escape he was moved to the Castle of La Mota, Medina del Campo. He did manage to escape from the Castle of La Mota with assistance, and after running across Santander, Durango and Gipuzkoa, he made it to Pamplona on 3 December 1506, and was much welcomed by King John III of Navarre, who was missing an experienced military commander, ahead of the feared Castilian invasion (1512).
He recaptured Viana, Navarre, in the hands of forces loyal to the count of Lerín, Ferdinand II of Aragon's conspirational ally in Navarre, but not the castle. In the early morning of 11 March 1507, an enemy party of knights fled from the castle during a heavy storm. Outraged at the ineffective siege laid on the castle, the Italian commander chased them only to find himself on his own. The party realized that, and Borgia got trapped in an ambush, receiving the injury of a fatal spear. He was then stripped of all his luxurious garments, valuables and a leather mask covering half his face, disfigured possibly by syphilis during his late years, and left lying naked, with just a "red tile" covering his genitals.
Remains.
Borgia was originally buried in a marbled mausoleum John III had ordered to build at the altar of the Church of Santa Maria in Viana, set on one of the stops on the Camino de Santiago. In the 16th century the bishop of Mondoñedo, Antonio de Guevara, published from memory what he had seen written on the tomb when he had paid a visit to the church. This epitaph underwent several changes in wording and meter throughout the years and the version most commonly cited today is that published by the priest and historian Francisco de Alesón in the 18th century. It reads:
Borgia was an old enemy of Ferdinand of Aragon, and he was fighting the count who paved the way for Ferdinand's 1512 Castilian invasion against John III and Catherine of Navarre. While the circumstances are not well known, the tomb was destroyed sometime between 1523 and 1608, during which time Santa María was undergoing renovation and expansion. Tradition goes that a bishop of Calahorra considered inappropriate to have the remains of "that degenerate" lying in the church, so the opportunity was taken to tear down the monument and expel Borgia's bones to where they were reburied under the street in front of the church to be trodden on by all who walked through the town.
Blasco Ibáñez, in "A los pies de Venus", writes that the then Bishop of Santa María had Borgia expelled from the church because his own father had died after being imprisoned under Alexander VI. It was held for many years that the bones were lost, although in fact local tradition continued to mark their place quite accurately and folklore sprung up around Borgia's death and ghost. The bones were in fact dug up twice and reburied once by historians (both local and international—the first dig in 1886 involved the French historian Charles Yriarte, who also published works on the Borgias) seeking the resting place of the infamous Cesare Borgia. After Borgia was unearthed for the second time in 1945 his bones were taken for a rather lengthy forensic examination by Victoriano Juaristi, a surgeon by trade and Borgia aficionado, and the tests concurred with the preliminary ones carried out in the 19th century. There was evidence that the bones belonged to Borgia.
Cesare Borgia's remains then were sent to Viana's town hall, directly across from Santa María, where they remained until 1953. They were then reburied immediately outside of the Church of Santa María, no longer under the street and in direct danger of being stepped on. A memorial stone was placed over it which translated into English declared Borgia the "Generalisimo" of the papal as well as the Navarran forces. A movement was made in the late 80s to have Borgia dug up once more and put back into Santa María, but this proposal was ultimately rejected by church officials due to recent ruling against the internment of anyone who did not hold the title of pope or cardinal. Since Borgia had renounced the cardinalate it was decided that it would be inappropriate for his bones to be moved into the church. However, Fernando Sebastian Aguilar, the Archbishop of Pamplona, caved in after more than 50 years of petitions and Borgia was finally moved back inside the church on 11 March 2007, the day before the 500th anniversary of his death. "We have nothing against the transfer of his remains. Whatever he may have done in life, he deserves to be forgiven now," said the local church.
Evaluation.
Niccolò Machiavelli met the Duke on a diplomatic mission in his function as Secretary of the Florentine Chancellery. Machiavelli was at Borgia's court from 7 October 1502 through 18 January 1503. During this time he wrote regular dispatches to his superiors in Florence, many of which have survived and are published in Machiavelli's Collected Works. In "The Prince", Machiavelli uses Borgia as an example to elucidate the dangers of acquiring a principality by virtue of another. Although Cesare Borgia's father gave him the power to set up, Cesare ruled the Romagna with skill and tact for the most part. However, when his father died, and a rival to the Borgia family entered the Papal seat, Cesare was overthrown in a matter of months.
Machiavelli attributes two episodes to Cesare Borgia: the method by which the Romagna was pacified, which Machiavelli describes in chapter VII of "The Prince", and the assassination of his captains on New Year's Eve of 1502 in Senigallia.
Machiavelli's use of Borgia is subject to controversy. Some scholars see in Machiavelli's Borgia the precursor of state crimes in the 20th century. Others, including Macaulay and Lord Acton, have historicized Machiavelli's Borgia, explaining the admiration for such violence as an effect of the general criminality and corruption of the time.
Borgia and Leonardo.
Cesare Borgia briefly employed Leonardo da Vinci as military architect and engineer between 1502 and 1503. Cesare provided Leonardo with an unlimited pass to inspect and direct all ongoing and planned construction in his domain. While in Romagna, Leonardo built the canal from Cesena to the Porto Cesenatico. Before meeting Cesare, Leonardo had worked at the Milanese court of Ludovico Sforza for many years, until Louis XII of France drove Sforza out of Italy. After Cesare, Leonardo was unsuccessful in finding another patron in Italy. King Francis I of France was able to convince Leonardo to enter his service, and the last three years of da Vinci's life were spent working in France.
Personal life.
On 10 May 1499, Cesare married Charlotte of Albret (1480 – 11 March 1514). She was a sister of John III of Navarre. They were parents to a daughter, Louise Borgia, Duchess of Valentinois, (1500–1553) who first married Louis II de la Trémoille, Governor of Burgundy, and secondly Philippe de Bourbon (1499–1557), Seigneur de Busset.
Cesare was also father to at least 11 illegitimate children, among them Girolamo Borgia, who married Isabella Contessa di Carpi, and Lucrezia Borgia (the younger), who, after Cesare's death, was moved to Ferrara to the court of her aunt, the elder Lucrezia Borgia.
In fiction.
Theatre.
Nathaniel Lee wrote a play entitled "Caesar Borgia" (1680) in which he appears as the central character.
Music.
Cesare Borgia is mentioned in the song "B.I.B.L.E.", performed by Killah Priest, which appears on GZA's 1995 album "Liquid Swords", as well as Killah Priest's debut album "Heavy Mental". He is also mentioned in the song "Jeshurun" on Priest's album "Behind the Stained Glass".
Video games.
Cesare Borgia is a major character in the 2010 video game "".
References.
Notes
Sources

</doc>
<doc id="7507" url="http://en.wikipedia.org/wiki?curid=7507" title="Chronicle">
Chronicle

Generally a chronicle (, from Greek , from , "chronos", "time") is a historical account of facts and events ranged in chronological order, as in a time line. Typically, equal weight is given for historically important events and local events, the purpose being the recording of events that occurred, seen from the perspective of the chronicler. This is in contrast to a narrative or history, which sets selected events in a meaningful interpretive context and excludes those the author does not see as important.
Where a chronicler obtained the information varies; some chronicles are written from first-hand knowledge, some are from witnesses or participants in events, still others are accounts passed mouth to mouth prior to being written down. Some made use of written materials; charters, letters, or the works of earlier chroniclers. Still others are tales of such unknown origins so as to hold mythical status. Copyists also affected chronicles in creative copying, making corrections or in updating or continuing a chronicle with information not available to the original author(s). The reliability of a particular chronicle is an important determination for modern historians.
In modern times various contemporary newspapers or other periodicals have adopted "chronicle" as part of their name. Various fictional stories have also adopted "chronicle" as part of their title, to give an impression of epic proportion to their stories. A chronicle which traces world history is called a universal chronicle.
Scholars categorize the genre of chronicle into two subgroups: live chronicles, and dead chronicles. A "dead" chronicle is one where the author gathers his list of events up to the time of his writing, but does not record further events as they occur. A "live" chronicle is where one or more authors add to a chronicle in a regular fashion, recording contemporary events shortly after they occur. Because of the immediacy of the information, historians tend to value live chronicles, such as annals, over dead ones.
The term often refers to a book written by a chronicler in the Middle Ages describing historical events in a country, or the lives of a nobleman or a clergyman, although it is also applied to a record of public events.
Chronicles are the predecessors of modern "time lines" rather than analytical histories. They represent accounts, in prose or verse, of local or distant events over a considerable period of time, both the lifetime of the individual chronicler and often those of several subsequent continuators. If the chronicles deal with events year by year, they are often called annals. Unlike the modern historian, most chroniclers tended to take their information as they found it, and made little attempt to separate fact from legend. The point of view of most chroniclers is highly localised, to the extent that many anonymous chroniclers can be sited in individual abbeys.
The most important English chronicles are the "Anglo-Saxon Chronicle", started under the patronage of King Alfred in the 9th century and continued until the 12th century, and the "Chronicles of England, Scotland and Ireland" (1577–87) by Raphael Holinshed and other writers; the latter documents were important sources of materials for Elizabethan drama. Later 16th century Scottish chronicles, written after the Reformation, shape history according to Catholic or Protestant viewpoints.
It is impossible to say how many chronicles exist, as the many ambiguities in the definition of the genre make it impossible to draw clear distinctions of what should or should not be included. However, the "Encyclopedia of the Medieval Chronicle" lists some 2,500 items written between 300 and 1500 AD.

</doc>
<doc id="7512" url="http://en.wikipedia.org/wiki?curid=7512" title="Concentration">
Concentration

In chemistry, concentration is the abundance of a constituent divided by the total volume of a mixture. Several types of mathematical description can be distinguished: mass concentration, molar concentration, number concentration, and volume concentration. The term concentration can be applied to any kind of chemical mixture, but most frequently it refers to solutes and solvents in solutions. The molar (amount) concentration has variants such as normal concentration and osmotic concentration.
Qualitative description.
Often in informal, non-technical language, concentration is described in a qualitative way, through the use of adjectives such as "dilute" for solutions of relatively low concentration and "concentrated" for solutions of relatively high concentration. To concentrate a solution, one must add more solute (for example, alcohol), or reduce the amount of solvent (for example, water). By contrast, to dilute a solution, one must add more solvent, or reduce the amount of solute. Unless two substances are "fully" miscible there exists a concentration at which no further solute will dissolve in a solution. At this point, the solution is said to be saturated. If additional solute is added to a saturated solution, it will not dissolve, except in certain circumstances, when supersaturation may occur. Instead, phase separation will occur, leading to coexisting phases, either completely separated or mixed as a suspension. The point of saturation depends on many variables such as ambient temperature and the precise chemical nature of the solvent and solute.
Quantitative notation.
There are four quantities that describe concentration:
Mass concentration.
The mass concentration formula_1 is defined as the mass of a constituent formula_2 divided by the volume of the mixture formula_3:
The SI unit is kg/m3 (equal to g/L).
Molar concentration.
The molar concentration formula_5 is defined as the amount of a constituent formula_6 (in moles) divided by the volume of the mixture formula_3:
The SI unit is mol/m3. However, more commonly the unit mol/L (= mol/dm3) is used.
Number concentration.
The number concentration formula_9 is defined as the number of entities of a constituent formula_10 in a mixture divided by the volume of the mixture formula_3:
The SI unit is 1/m3.
Volume concentration.
The volume concentration formula_13 (do not confuse with volume fraction) is defined as the volume of a constituent formula_14 divided by the volume of the mixture formula_3:
Being dimensionless, it is expressed as a number, e.g., 0.18 or 18%; its unit is 1.
Related quantities.
Several other quantities can be used to describe the composition of a mixture. Note that these should not be called concentrations.
Normality.
Normality is defined as the molar concentration formula_5 divided by an equivalence factor formula_18. Since the definition of the equivalence factor may not be unequivocal, IUPAC and NIST discourage the use of normality.
Molality.
The molality of a solution formula_19 is defined as the amount of a constituent formula_6 (in moles) divided by the mass of the solvent formula_21 (not the mass of the solution):
The SI unit for molality is mol/kg.
Mole fraction.
The mole fraction formula_23 is defined as the amount of a constituent formula_6 (in moles) divided by the total amount of all constituents in a mixture formula_25:
The SI unit is mol/mol. However, the deprecated parts-per notation is often used to describe small mole fractions.
Mole ratio.
The mole ratio formula_27 is defined as the amount of a constituent formula_6 divided by the total amount of all "other" constituents in a mixture:
If formula_6 is much smaller than formula_25, the mole ratio is almost identical to the mole fraction.
The SI unit is mol/mol. However, the deprecated parts-per notation is often used to describe small mole ratios.
Mass fraction.
The mass fraction formula_32 is the fraction of one substance with mass formula_2 to the mass of the total mixture formula_34, defined as:
The SI unit is kg/kg. However, the deprecated parts-per notation is often used to describe small mass fractions.
Mass ratio.
The mass ratio formula_36 is defined as the mass of a constituent formula_2 divided by the total mass of all "other" constituents in a mixture:
If formula_2 is much smaller than formula_34, the mass ratio is almost identical to the mass fraction.
The SI unit is kg/kg. However, the deprecated parts-per notation is often used to describe small mass ratios.
Dependence on volume.
Concentration depends on the variation of the volume of the solution due mainly to thermal expansion.

</doc>
<doc id="7514" url="http://en.wikipedia.org/wiki?curid=7514" title="Christine Lavin">
Christine Lavin

Christine Lavin (born January 2, 1952) is a New York City-based singer-songwriter and promoter of contemporary folk music. She has recorded numerous solo albums, and has also recorded with other female folk artists under the name Four Bitchin' Babes. She has also put together several compilation albums of contemporary folk artists, including her latest "Just One Angel", 22 singer/songwriters singing Christmas/Hanukah/Solstice/New Year's songs including actor Jeff Daniels, Grammy-winners Janis Ian and Julie Gold, and the Guitar Man Of Central Park David Ippolito.
She is known for her sense of humor, which is expressed in both her music and her onstage performances. Many of her songs alternate between emotional reflections on romance and outright comedy. Two of her more famous songs are "Sensitive New Age Guys" and "Bald Headed Men".
One of Lavin's songs, "Regretting What I Said to You When You Called Me 11:00 On a Friday Morning to Tell Me that at 1:00 Friday Afternoon You're Gonna Leave Your Office, Go Downstairs, Hail a Cab to Go Out to the Airport to Catch a Plane to Go Skiing in the Alps for Two Weeks, Not that I Wanted to Go With You, I Wasn't Able to Leave Town, I'm Not a Very Good Skier, I Couldn't Expect You to Pay My Way, But After Going Out With You for Three Years I DON'T Like Surprises!! Subtitled: A Musical Apology" is notable for having the longest known song title. It is the eighth song on her 1984 album "Future Fossils", and is 3:04 (3 minutes and 4 seconds) long.
In her youth, Lavin was a cheerleader in Peekskill, New York, and she still has impressive baton-twirling skills; she often ends a concert by twirling a glow-in-the-dark baton with the house lights turned off as she leaves the stage.
Lavin worked at Caffe Lena in Saratoga, New York, until Dave Van Ronk convinced her to move to New York City and make a career as a singer-songwriter. She followed his advice and accepted his offer of guitar lessons. She has lived in the City ever since.
Lavin was the original host of "Sunday Breakfast" on WFUV in New York City.
Lavin was a founding member of the Four Bitchin' Babes when they were formed in 1990.
In recent years Lavin has been known to host knitting circles before her shows, inviting any knitters, hookers (people who crochet), or other crafters to join her.

</doc>
<doc id="7515" url="http://en.wikipedia.org/wiki?curid=7515" title="Cutter Expansive Classification">
Cutter Expansive Classification

The Cutter Expansive Classification system is a library classification system devised by Charles Ammi Cutter. The system was the basis for the top categories of the Library of Congress Classification.
History of the Expansive Classification.
Charles Ammi Cutter (1837–1903), inspired by the decimal classification of his contemporary Melvil Dewey, and with Dewey's initial encouragement, developed his own classification scheme for the Winchester Town Library and then the Boston Athenaeum, at which he served as librarian for twenty-four years. He began work on it around the year 1880, publishing an overview of the new system in 1882. The same classification would later be used, but with a different notation, also devised by Cutter, at the Cary Library in Lexington.
Many libraries found this system too detailed and complex for their needs, and Cutter received many requests from librarians at small libraries who wanted the classification adapted for their collections. He devised the Expansive Classification in response, to meet the needs of growing libraries, and to address some of the complaints of his critics
. Cutter completed and published an introduction and schedules for the first six classifications of his new system (), but his work on the seventh was interrupted by his death in 1903.
The Cutter Expansive Classification, although adopted by comparatively few libraries, mostly in New England, has been called one of the most logical and scholarly of American classifications. Library historian Leo E. LaMontagne writes:
Cutter produced the best classification of the nineteenth century. While his system was less "scientific" than that of J.P. Lesley, it's other key features – notation, specificity, and versatility – make it deserving of the praise it has received.
Its top level divisions served as a basis for the Library of Congress classification, which also took over some of its features. It did not catch on as did Dewey's system because Cutter died before it was completely finished, making no provision for the kind of development necessary as the bounds of knowledge expanded and scholarly emphases changed throughout the 20th century.
Structure of the Expansive Classification.
The Expansive Classification uses seven separate schedules, each designed to be used by libraries of different sizes. After the first, each schedule was an expansion of the previous one, and Cutter provided instructions for how a library might change from one expansion to another as it grows.
Summary of the Expansive Classification Schedules.
First Classification.
The first classification is meant for only the very smallest libraries. The first classification has only seven top level classes, and only eight classes in all.
Further Classifications.
Further expansions add more top level classes and subdivisions. Many subclasses arranged systematically, with common divisions, such as those by geography and language, following a consistent system throughout.
By the fifth classification all the letters of the alphabet are in use for top level classes. These are:
These schedules were not meant to be fixed, but were to be adapted to meet the needs of each library. For example, books on the English language may be put in X, and books on language in general in a subclass of X, or this can be reversed. The first option is less logical, but results in shorter marks for most English language libraries.
How Expansive Classification call numbers are constructed.
Most call numbers in the Expansive Classification follow conventions offering clues to the book's subject. The first line represents the subject, the second the author (and perhaps title), the third and fourth dates of editions, indications of translations, and critical works on particular books or authors. All numbers in the Expansive Classification are (or should be) shelved as if in decimal order.
Size of volumes is indicated by points (.), pluses (+), or slashes (/ or //).
For some subjects a numerical geographical subdivision follows the classification letters on the first line. The number 83 stands for the United States—hence, F83 is U.S. history, G83 U.S. travel, JU83 U.S. politics, WP83 U.S. painting. Geographical numbers are often further expanded decimally to represent more specific areas, sometimes followed by a capital letter indicating a particular city.
The second line usually represents the author's name by a capital letter plus one or more numbers arranged decimally. This may be followed by the first letter or letters of the title in lower-case, and/or sometimes the letters a,b,c indicating other printings of the same title. When appropriate, the second line may begin with a 'form' number—e.g., 1 stands for history and criticism of a subject, 2 for a bibliography, 5 for a dictionary, 6 for an atlas or maps, 7 for a periodical, 8 for a society or university publication, 9 for a collection of works by different authors.
On the third line a capital Y indicates a work about the author or book represented by the first two lines, and a capital E (for English—other letters are used for other languages) indicates a translation into English. If both criticism and translation apply to a single title, the number expands into four lines.
Cutter Numbers (Cutter Codes).
One of the features adopted by other systems, including Library of Congress, is the Cutter number. It is an alphanumeric device to code text so that it can be arranged in alphabetical order using the fewest characters. It contains one or two initial letters and Arabic numbers, treated as a decimal. To construct a Cutter number, a cataloguer consults a Cutter table as required by the classification rules. Although Cutter numbers are mostly used for coding the names of authors, the system can be used for titles, subjects, geographic areas, and more.
Initial letters Qa-Qt are assigned Q2-Q29, while entries beginning with numerals have a Cutter number A12-A19, therefore sorting before the first A entry.
So to make the three digit Cutter number for "Cutter", you would start with "C", then looking under "other consonants", find that "u" gives the number 8, and under "additional letters", "t" is 8, giving a Cutter number of "C88".

</doc>
<doc id="7516" url="http://en.wikipedia.org/wiki?curid=7516" title="Cem Karaca">
Cem Karaca

Muhtar Cem Karaca (April 5, 1945 – February 8, 2004), also called Cem Baba (Daddy Cem or Father Cem), was a prominent Turkish rock musician and one of the most important figures in the Anatolian rock movement. He is a graduate of Robert College.
Biography.
He was the only child of Mehmet İbrahim Karaca of Azerbaijani origin, and İrma Felekyan (Toto Karaca) of Armenian origin, a popular opera, theatre and movie actress. His first group was called "Dynamites" and was a classic rock cover band. Later he joined "Jaguars", an Elvis Presley cover band. In 1967, he started to write his own music, joining the band "Apaşlar" (The Rowdies), his first Turkish-language group. In the same year he participated in Golden Microphone () contest, a popular music contest in which he won the second place with his song "Emrah". In 1969, Karaca and bass-player Serhan Karabay left Apaşlar and started an original Anatolian group called "Kardaşlar" (The Brothers). 
In 1972, Karaca joined the group "Moğollar" (The Mongols) and wrote one of his best-known songs, "Namus Belası". However, Cahit Berkay, the leader of Moğollar, wanted an international name for his band, and he left for France to take the group to another level. Karaca, who wanted to continue his Anatolian beat sound, left Moğollar and started his own band "Dervişan" (Dervishes) in 1974. Karaca and Dervişan sang poetic and progressive songs. 
In the 1970s, Turkey's image was damaged by political violence between supporters of the left and the right, separatist movements and the rise of Islamism. As the country fell into chaos, the government suspected Cem Karaca of involvement. At times he was accused of treason for being a separatist thinker and a Marxist-Leninist. The Turkish government tried to portray Karaca as a man, who was unknowingly writing songs to start a revolution. One politician was quoted as saying, "Karaca is simply calling citizens to a bloody war against the state." Dervişan was ultimately dissolved at the end of 1977. He later founded in 1978 "Edirdahan", an acronym for "from Edirne to Ardahan"; the westernmost and the easternmost provinces of Turkey. He recorded one LP with "Edirdahan".
In early 1979, he left for West Germany for business reasons. Turkey continued to spin out of control with military curfews and eventually a military coup on September 12, 1980. General Kenan Evren took over the government and temporarily closed all the nation's political parties. After the coup, many intellectual people, including writers, artists and journalists, were arrested. A warrant was issued for the arrest of Karaca by the government of Turkey. 
The state invited Karaca back to the country several times, but Karaca, not knowing what would happen upon his return, decided not to come back.
While he was in Germany, his father died, but he could not return to attend the funeral. After some time, the Turkish government decided to strip Cem Karaca of his Turkish citizenship, keeping the arrest warrant active.
Several years later, in 1987, the prime minister and leader of the Turkish Motherland Party, Turgut Özal, issued an amnesty for Karaca. Shortly afterwards, he returned to Turkey. His return also brought a new album by him, "Merhaba Gençler ve Her zaman Genç Kalanlar" ("Hello, The Young and The Young at Heart"), one of his most powerful works. His return home was greeted happily by his fans, but during his absence, Karaca had lost the youthful audience and acquired few new listeners. He died on February 8, 2004 and was interred at Karacaahmet Cemetery in Üsküdar district of Istanbul.
Discography.
Collection albums.
(1) It was released again with different cover, sort of songs and songs in 2003.
He has also appeared as a guest artist on several recordings.

</doc>
<doc id="7517" url="http://en.wikipedia.org/wiki?curid=7517" title="Calista Flockhart">
Calista Flockhart

Calista Kay Flockhart (born November 11, 1964) is an American actress, known for playing the title role in the Fox television comedy-drama series "Ally McBeal" and for playing Kitty Walker McCallister on the ABC drama "Brothers & Sisters". During her career she has received a Golden Globe Award, a Screen Actors Guild Award and three Emmy Award nominations.
Early life.
Calista Flockhart was born on November 11, 1964 in Freeport, Illinois, the daughter of Kay, an English teacher, and Ronald Flockhart, a Kraft Foods executive. Her parents are retired and live in Morristown, Tennessee. She has one older brother, Gary. Her mother, Kay Calista, 
reversed her own first and middle names in naming her Calista Kay. Calista also had a great-grandmother named Calista.
Because her father's job required the family to move often, Flockhart and her brother grew up in several places including Illinois, Iowa, Minnesota, New Jersey, and Norwich, New York. As a child, she wrote a play called "Toyland" which she performed to a small audience at a dinner party.
Flockhart attended Shawnee High School in Medford Township, New Jersey. Following graduation in 1983, Flockhart attended the Mason Gross School of the Arts at Rutgers University in New Brunswick, New Jersey. While there, she attended a specialized and competitive class, lasting from 6:00 AM to 6:00 PM. In her sophomore year at Rutgers, Flockhart met aspiring actress Jane Krakowski, the best friend of her roommate. Later, they both would work together on "Ally McBeal".
People began recognizing Flockhart's acting ability when William Esper (Mason Gross' theatre director and Flockhart's acting teacher) made an exception to policy by allowing Flockhart to perform on the main stage. Though this venue usually is reserved for juniors and seniors, Harold Scott insisted that Flockhart perform there in his production of William Inge's "Picnic". Flockhart graduated with a Bachelor of Fine Arts degree in theatre in 1988, as one of the few students who successfully completed the course. Rutgers inducted her into the Hall of Distinguished Alumni on May 3, 2003.
Flockhart moved to New York City in 1989 and began seeking auditions, living with three other women in a two-bedroom apartment and working as a waitress and aerobics instructor. She would remain in the city until 1997.
Career.
Early career.
In spring 1989, Flockhart made her first television appearance in a minor role in an episode of "Guiding Light" as a babysitter. She made her professional debut on the New York stage, appearing in "Beside Herself" alongside Melissa Joan Hart, at the Circle Repertory Theatre. Two years later, Flockhart appeared in the television movie "Darrow". Though she later appeared in films "Naked in New York" (1993) and "Getting In" (1994), her first substantial speaking part in a film was in "Quiz Show", directed by Robert Redford.
Flockhart debuted on Broadway in 1994, as Laura in "The Glass Menagerie". Actress Julie Harris felt Flockhart should be hired without further auditions, claiming that she seemed ideal for the part. Flockhart received a Clarence Derwent Award for her performance. In 1995, Flockhart became acquainted with actors such as Dianne Wiest and Faye Dunaway when she appeared in the movie "Drunks". Later that year, Flockhart starred in "Jane Doe" as a drug addict. In 1996, Flockhart appeared as the daughter of Dianne Wiest and Gene Hackman's characters in "The Birdcage". Throughout that year, she continued to work on Broadway, playing the role of Natasha in Anton Chekhov's "Three Sisters".
"Ally McBeal".
In 1997, Flockhart was asked to audition for the starring role in David E. Kelley's Fox television series, "Ally McBeal". Kelley, having heard of Flockhart, wanted her to audition for the contract part. Though Flockhart at first hesitated due to the necessary commitment to the show in a negotiable contract, she was swayed by the script and traveled to Los Angeles to audition for the part, which she won. She earned a Golden Globe Award for the role in 1998. Flockhart also appeared on the June 29, 1998, cover of "Time" magazine, placed as the newest iteration in the evolution of feminism, relating to the ongoing debate about the role depicted by her character.
Throughout her professional career, Flockhart has maintained her naturally lean figure. However, many have commented that Flockhart had become dangerously thin, particularly when the actress made red carpet appearances in clothing that revealed a somewhat emaciated physique. She had maintained throughout the show's run that she was never diagnosed with either anorexia or bulimia, nor was she a user of illegal drugs. She did remark, however, that while starring in the show she refrained from eating sweets, retaining her slimness with intense workouts and running. In 2006, she admitted that she had a problem at the time, and was "exercising too much" and "eating too little".
Other work.
Flockhart played the role of Helena in "A Midsummer Night's Dream", a 1999 film version of Shakespeare's play. In 2000, she appeared in "Things You Can Tell Just by Looking at Her" and "", later accompanying Eve Ensler to Kenya in order to protest violence against women, particularly female genital mutilation. Flockhart also starred in the Off-Broadway production of Ensler's "The Vagina Monologues".
In 2004, Flockhart appeared as Matthew Broderick's deranged girlfriend in "The Last Shot". In the same year, Flockhart travelled to Spain for the filming of "Fragile", which premiered in September 2005 at the Venice Film Festival.
She was offered the role of Susan Mayer on "Desperate Housewives", but declined. The role went to Teri Hatcher. Flockhart's last appearance on television was as Kitty Walker, opposite Sally Field, Rachel Griffiths and Matthew Rhys, in the ABC prime time series "Brothers & Sisters", which premiered in September 2006 in the time slot after "Desperate Housewives". The show was cancelled in May 2011 after running for five years. Flockhart's character was significant throughout the series' first four years, but her appearances were reduced for the 2010–2011 season, coinciding with the departure of TV husband Rob Lowe.
Personal life.
Flockhart has been in a relationship with actor Harrison Ford since their meeting at the Golden Globe Awards on January 20, 2002. They became engaged on Valentine's Day in 2009, and were married on June 15, 2010, in Santa Fe, New Mexico. The ceremony was presided over by Governor Bill Richardson and New Mexico Supreme Court Chief Justice Charles W. Daniels. Flockhart and Ford are raising a son, Liam, whom Flockhart adopted as an infant on January 11, 2001. 
As of 2008, Flockhart is the national spokesperson for [www.peaceoverviolence.org Peace Over Violence].
External links.
! colspan="3" style="background: #DAA520;" | Theatre World Award
 

</doc>
<doc id="7519" url="http://en.wikipedia.org/wiki?curid=7519" title="Convolution">
Convolution

In mathematics and, in particular, functional analysis, convolution is a mathematical operation on two functions "f" and "g", producing a third function that is typically viewed as a modified version of one of the original functions, giving the area overlap between the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to cross-correlation. It has applications that include probability, statistics, computer vision, image and signal processing, electrical engineering, and differential equations.
The convolution can be defined for functions on groups other than Euclidean space. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by "periodic convolution". (See row 10 at DTFT#Properties.)  And "discrete convolution" can be defined for functions on the set of integers. Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.
Computing the inverse of the convolution operation is known as deconvolution.
Definition.
The convolution of "f" and "g" is written "f"∗"g", using an asterisk or star. It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a particular kind of integral transform:
While the symbol "t" is used above, it need not represent the time domain. But in that context, the convolution formula can be described as a weighted average of the function "f"("τ") at the moment "t" where the weighting is given by "g"(−"τ") simply shifted by amount "t". As "t" changes, the weighting function emphasizes different parts of the input function.
For functions "f", "g" supported on only formula_1 (i.e., zero for negative arguments), the integration limits can be truncated, resulting in
In this case, the Laplace transform is more appropriate than the Fourier transform below and boundary terms become relevant.
For the multi-dimensional formulation of convolution, see Domain of definition (below).
Derivations.
Convolution describes the output (in terms of the input) of an important class of operations known as "linear time-invariant" (LTI). See LTI system theory for a derivation of convolution as the result of LTI constraints. In terms of the Fourier transforms of the input and output of an LTI operation, no new frequency components are created. The existing ones are only modified (amplitude and/or phase). In other words, the output transform is the pointwise product of the input transform with a third transform (known as a transfer function). See Convolution theorem for a derivation of that property of convolution. Conversely, convolution can be derived as the inverse Fourier transform of the pointwise product of two Fourier transforms.
Historical developments.
One of the earliest uses of the convolution integral appeared in D'Alembert's derivation of Taylor's theorem in "Recherches sur différents points importants du système du monde," published in 1754.
Also, an expression of the type:
is used by Sylvestre François Lacroix on page 505 of his book entitled "Treatise on differences and series", which is the last of 3 volumes of the encyclopedic series: "Traité du calcul différentiel et du calcul intégral", Chez Courcier, Paris, 1797-1800. Soon thereafter, convolution operations appear in the works of Pierre Simon Laplace, Jean Baptiste Joseph Fourier, Siméon Denis Poisson, and others. The term itself did not come into wide use until the 1950s or 60s. Prior to that it was sometimes known as "faltung" (which means "folding" in German), "composition product", "superposition integral", and "Carson's integral".
Yet it appears as early as 1903, though the definition is rather unfamiliar in older uses.
The operation:
is a particular case of composition products considered by the Italian mathematician Vito Volterra in 1913.
Circular convolution.
When a function "g""T" is periodic, with period "T", then for functions, "f", such that "f"∗"g""T" exists, the convolution is also periodic and identical to:
where "t"o is an arbitrary choice. The summation is called a periodic summation of the function "f".
When "g""T" is a periodic summation of another function, "g", then "f"∗"g""T" is known as a "circular" or "cyclic" convolution of "f" and "g".<br>
And if the periodic summation above is replaced by "f""T", the operation is called a "periodic" convolution of "f""T" and "g""T".
Discrete convolution.
For complex-valued functions "f", "g" defined on the set Z of integers, the discrete convolution of "f" and "g" is given by:
The convolution of two finite sequences is defined by extending the sequences to finitely supported functions on the set of integers. When the sequences are the coefficients of two polynomials, then the coefficients of the ordinary product of the two polynomials are the convolution of the original two sequences. This is known as the Cauchy product of the coefficients of the sequences.
Thus when "g" has finite support in the set formula_7 (representing, for instance, a finite impulse response), a finite summation may be used:
Circular discrete convolution.
When a function "gN" is periodic, with period "N", then for functions, "f", such that "f"∗"gN" exists, the convolution is also periodic and identical to:
The summation on "k" is called a periodic summation of the function "f".
If "gN" is a periodic summation of another function, "g", then "f"∗"gN" is known as a circular convolution of "f" and "g".
When the non-zero durations of both "f" and "g" are limited to the interval [0, "N" − 1], "f"∗"gN" reduces to these common forms:
] \equiv (f *_N g)[n]
The notation ("f" ∗"N" "g") for "cyclic convolution" denotes convolution over the cyclic group of integers modulo "N".
Circular convolution arises most often in the context of fast convolution with an FFT algorithm.
Fast convolution algorithms.
In many situations, discrete convolutions can be converted to circular convolutions so that fast transforms with a convolution property can be used to implement the computation. For example, convolution of digit sequences is the kernel operation in multiplication of multi-digit numbers, which can therefore be efficiently implemented with transform techniques (; ).
 requires "N" arithmetic operations per output value and "N"2 operations for "N" outputs. That can be significantly reduced with any of several fast algorithms. Digital signal processing and other applications typically use fast convolution algorithms to reduce the cost of the convolution to O("N" log "N") complexity.
The most common fast convolution algorithms use fast Fourier transform (FFT) algorithms via the circular convolution theorem. Specifically, the circular convolution of two finite-length sequences is found by taking an FFT of each sequence, multiplying pointwise, and then performing an inverse FFT. Convolutions of the type defined above are then efficiently implemented using that technique in conjunction with zero-extension and/or discarding portions of the output. Other fast convolution algorithms, such as the Schönhage–Strassen algorithm or the Mersenne transform, use fast Fourier transforms in other rings.
If one sequence is much longer than the other, zero-extension of the shorter sequence and fast circular convolution is not the most computationally efficient method available. Instead, decomposing the longer sequence into blocks and convolving each block allows for faster algorithms such as the Overlap–save method and Overlap–add method. A hybrid convolution method that combines block and FIR algorithms allows for a zero input-output latency that is useful for real-time convolution computations.
Domain of definition.
The convolution of two complex-valued functions on R"d", defined by:
is well-defined only if "f" and "g" decay sufficiently rapidly at infinity in order for the integral to exist. Conditions for the existence of the convolution may be tricky, since a blow-up in "g" at infinity can be easily offset by sufficiently rapid decay in "f". The question of existence thus may involve different conditions on "f" and "g":
Compactly supported functions.
If "f" and "g" are compactly supported continuous functions, then their convolution exists, and is also compactly supported and continuous . More generally, if either function (say "f") is compactly supported and the other is locally integrable, then the convolution "f"∗"g" is well-defined and continuous.
Convolution of "f" and "g" is also well defined when both functions are locally square integrable on R and supported on an interval of the form [a, +∞) (or both supported on [-∞, a]).
Integrable functions.
The convolution of "f" and "g" exists if "f" and "g" are both Lebesgue integrable functions in L1(R"d"), and in this case "f"∗"g" is also integrable . This is a consequence of Tonelli's theorem. This is also true for functions in formula_11, under the discrete convolution, or more generally for the convolution on any group.
Likewise, if "f" ∈ "L"1(R"d") and "g" ∈ "L""p"(R"d") where 1 ≤ "p" ≤ ∞, then "f"∗"g" ∈ "L""p"(R"d") and
In the particular case "p" = 1, this shows that "L"1 is a Banach algebra under the convolution (and equality of the two sides holds if "f" and "g" are non-negative almost everywhere).
More generally, Young's inequality implies that the convolution is a continuous bilinear map between suitable "L""p" spaces. Specifically, if 1 ≤ "p","q","r" ≤ ∞ satisfy
then
so that the convolution is a continuous bilinear mapping from "L""p"×"L""q" to "L""r".
The Young inequality for convolution is also true in other contexts (circle group, convolution on Z). The preceding inequality is not sharp on the real line: when , there exists a constant such that:
The optimal value of was discovered in 1975.
A stronger estimate is true provided :
where formula_17 is the weak "Lq" norm. Convolution also defines a bilinear continuous map formula_18 for formula_19, owing to the weak Young inequality:
Functions of rapid decay.
In addition to compactly supported functions and integrable functions, functions that have sufficiently rapid decay at infinity can also be convolved. An important feature of the convolution is that if "f" and "g" both decay rapidly, then "f"∗"g" also decays rapidly. In particular, if "f" and "g" are rapidly decreasing functions, then so is the convolution "f"∗"g". Combined with the fact that convolution commutes with differentiation (see Properties), it follows that the class of Schwartz functions is closed under convolution .
Distributions.
Under some circumstances, it is possible to define the convolution of a function with a distribution, or of two distributions. If "f" is a compactly supported function and "g" is a distribution, then "f"∗"g" is a smooth function defined by a distributional formula analogous to
More generally, it is possible to extend the definition of the convolution in a unique way so that the associative law
remains valid in the case where "f" is a distribution, and "g" a compactly supported distribution .
Measures.
The convolution of any two Borel measures μ and ν of bounded variation is the measure λ defined by 
This agrees with the convolution defined above when μ and ν are regarded as distributions, as well as the convolution of L1 functions when μ and ν are absolutely continuous with respect to the Lebesgue measure.
The convolution of measures also satisfies the following version of Young's inequality
where the norm is the total variation of a measure. Because the space of measures of bounded variation is a Banach space, convolution of measures can be treated with standard methods of functional analysis that may not apply for the convolution of distributions.
Properties.
Algebraic properties.
The convolution defines a product on the linear space of integrable functions. This product satisfies the following algebraic properties, which formally mean that the space of integrable functions with the product given by convolution is a commutative algebra without identity . Other linear spaces of functions, such as the space of continuous functions of compact support, are closed under the convolution, and so also form commutative algebras.
for any real (or complex) number formula_29.
No algebra of functions possesses an identity for the convolution. The lack of identity is typically not a major inconvenience, since most collections of functions on which the convolution is performed can be convolved with a delta distribution or, at the very least (as is the case of "L"1) admit approximations to the identity. The linear space of compactly supported distributions does, however, admit an identity under the convolution. Specifically,
where δ is the delta distribution.
Some distributions have an inverse element for the convolution, "S"(−1), which is defined by
The set of invertible distributions forms an abelian group under the convolution.
Integration.
If "f" and "g" are integrable functions, then the integral of their convolution on the whole space is simply obtained as the product of their integrals:
This follows from Fubini's theorem. The same result holds if "f" and "g" are only assumed to be nonnegative measurable functions, by Tonelli's theorem.
Differentiation.
In the one-variable case,
where "d"/"dx" is the derivative. More generally, in the case of functions of several variables, an analogous formula holds with the partial derivative:
A particular consequence of this is that the convolution can be viewed as a "smoothing" operation: the convolution of "f" and "g" is differentiable as many times as "f" and "g" are in total.
These identities hold under the precise condition that "f" and "g" are absolutely integrable and at least one of them has an absolutely integrable (L1) weak derivative, as a consequence of Young's inequality. For instance, when "f" is continuously differentiable with compact support, and "g" is an arbitrary locally integrable function,
These identities also hold much more broadly in the sense of tempered distributions if one of "f" or "g" is a compactly supported distribution or a Schwartz function and the other is a tempered distribution. On the other hand, two positive integrable and infinitely differentiable functions may have a nowhere continuous convolution.
In the discrete case, the difference operator "D" "f"("n") = "f"("n" + 1) − "f"("n") satisfies an analogous relationship:
Convolution theorem.
The convolution theorem states that
where formula_39 denotes the Fourier transform of formula_40, and formula_41 is a constant that depends on the specific normalization of the Fourier transform. Versions of this theorem also hold for the Laplace transform, two-sided Laplace transform, Z-transform and Mellin transform.
See also the less trivial Titchmarsh convolution theorem.
Translation invariance.
The convolution commutes with translations, meaning that
where τ"x"f is the translation of the function "f" by "x" defined by
If "f" is a Schwartz function, then τ"x""f" is the convolution with a translated Dirac delta function τ"x""f" = "f"∗"τ""x" "δ". So translation invariance of the convolution of Schwartz functions is a consequence of the associativity of convolution.
Furthermore, under certain conditions, convolution is the most general translation invariant operation. Informally speaking, the following holds
Thus any translation invariant operation can be represented as a convolution. Convolutions play an important role in the study of time-invariant systems, and especially LTI system theory. The representing function "g""S" is the impulse response of the transformation "S".
A more precise version of the theorem quoted above requires specifying the class of functions on which the convolution is defined, and also requires assuming in addition that "S" must be a continuous linear operator with respect to the appropriate topology. It is known, for instance, that every continuous translation invariant continuous linear operator on "L"1 is the convolution with a finite Borel measure. More generally, every continuous translation invariant continuous linear operator on "L""p" for 1 ≤ "p" < ∞ is the convolution with a tempered distribution whose Fourier transform is bounded. To wit, they are all given by bounded Fourier multipliers.
Convolutions on groups.
If "G" is a suitable group endowed with a measure λ, and if "f" and "g" are real or complex valued integrable functions on "G", then we can define their convolution by
It is not commutative in general. In typical cases of interest "G" is a locally compact Hausdorff topological group and λ is a (left-) Haar measure. In that case, unless "G" is unimodular, the convolution defined in this way is not the same as formula_45. The preference of one over the other is made so that convolution with a fixed function "g" commutes with left translation in the group:
Furthermore, the convention is also required for consistency with the definition of the convolution of measures given below. However, with a right instead of a left Haar measure, the latter integral is preferred over the former.
On locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group T with the Lebesgue measure is an immediate example. For a fixed "g" in "L"1(T), we have the following familiar operator acting on the Hilbert space "L"2(T):
The operator "T" is compact. A direct calculation shows that its adjoint "T*" is convolution with
By the commutativity property cited above, "T" is normal: "T"*"T" = "TT"*. Also, "T" commutes with the translation operators. Consider the family "S" of operators consisting of all such convolutions and the translation operators. Then "S" is a commuting family of normal operators. According to spectral theory, there exists an orthonormal basis {"hk"} that simultaneously diagonalizes "S". This characterizes convolutions on the circle. Specifically, we have
which are precisely the characters of T. Each convolution is a compact multiplication operator in this basis. This can be viewed as a version of the convolution theorem discussed above.
A discrete example is a finite cyclic group of order "n". Convolution operators are here represented by circulant matrices, and can be diagonalized by the discrete Fourier transform.
A similar result holds for compact groups (not necessarily abelian): the matrix coefficients of finite-dimensional unitary representations form an orthonormal basis in "L"2 by the Peter–Weyl theorem, and an analog of the convolution theorem continues to hold, along with many other aspects of harmonic analysis that depend on the Fourier transform.
Convolution of measures.
Let "G" be a topological group.
If μ and ν are finite Borel measures on "G", then their convolution μ∗ν is defined by
for each measurable subset "E" of "G". The convolution is also a finite measure, whose total variation satisfies
In the case when "G" is locally compact with (left-)Haar measure λ, and μ and ν are absolutely continuous with respect to a λ, so that each has a density function, then the convolution μ∗ν is also absolutely continuous, and its density function is just the convolution of the two separate density functions.
If μ and ν are probability measures on the topological group then the convolution μ∗ν is the probability distribution of the sum "X" + "Y" of two independent random variables "X" and "Y" whose respective distributions are μ and ν.
Bialgebras.
Let ("X", Δ, ∇, "ε", "η") be a bialgebra with comultiplication Δ, multiplication ∇, unit η, and counit ε. The convolution is a product defined on the endomorphism algebra End("X") as follows. Let φ, ψ ∈ End("X"), that is, φ,ψ : "X" → "X" are functions that respect all algebraic structure of "X", then the convolution φ∗ψ is defined as the composition
The convolution appears notably in the definition of Hopf algebras . A bialgebra is a Hopf algebra if and only if it has an antipode: an endomorphism "S" such that
Applications.
Convolution and related operations are found in many applications in science, engineering and mathematics.

</doc>
<doc id="7521" url="http://en.wikipedia.org/wiki?curid=7521" title="Calico (textile)">
Calico (textile)

Calico (in British usage, 1505, AmE "muslin") is a plain-woven textile made from unbleached, and often not fully processed, cotton. It may contain unseparated husk parts, for example. The fabric is less coarse and thicker than canvas or denim, but owing to its unfinished and undyed appearance, it is still very cheap.
Originally from the city of Kōlikkōdu (known by the English as "Calicut") in southwestern India, the fabric was made by the traditional weavers called cāliyans. The raw fabric was dyed and printed in bright hues and calico prints became popular in Europe.
History.
Calico originated in Kozhikode (also known as Calicut, from which the name of the textile came) in southwestern India during the 11th century. The cloth was known as "cāliyan" to the natives.
It was mentioned in Indian literature by the 12th century when the writer Hēmacandra described calico fabric prints with a lotus design. By the 15th century calico from Gujǎrāt made its appearance in Egypt. Trade with Europe followed from the 17th century onwards.
Calico was woven using Sūrat cotton for both the warp and weft.
Politics of cotton.
In the 18th century, England was famous for its woollen and worsted cloth. That industry, centred in the east and south in towns such as Norwich, jealously protected their product. Cotton processing was tiny: in 1701 only of cotton-wool was imported into England, and by 1730 this had fallen to . This was due to commercial legislation to protect the woollen industry. Cheap calico prints, imported by the East India Company from Hindustān (India), had become popular. In 1700 an Act of Parliament was passed to prevent the importation of dyed or printed calicoes from India, China or Persia. This caused grey cloth (calico that had not been finished—dyed or printed) to be imported instead, and these were printed in southern England with the popular patterns. Also, Lancashire businessmen produced grey cloth with linen warp and cotton weft, known as fustian, which they sent to London to be finished. Cotton-wool imports recovered and by 1720 were almost back to their 1701 levels. Again the woollen manufacturers, in true protectionist style, claimed that this was taking away jobs from workers in Coventry. Another law was passed, to fine anyone caught wearing any printed or stained calico; muslins, neckcloths and fustians were exempted. It was this exemption that the Lancashire manufactures exploited. The use of coloured cotton weft with linen warp was permitted by the 1736 Manchester Act. There now was an artificial demand for woven cloth. In 1764, of cotton-wool was imported. It has been noted that this was a key part of the process of the reduction of the Indian economy from sophisticated textile production to a mere supplier of raw materials which occurred under colonial rule, a process described by Nehru and more recent scholars as "de-industrialization."
Calico printing.
Early Indian chintzes, that is a glazed calico with large floral pattern, were primarily produced by painting techniques. Later, the hues were applied by means of wooden blocks, and it was the wooden block printing that was used in London. Confusingly, linen and silk that was printed by this method was known as "linen calicoes" and "silk calicoes". The early European calicoes (1680) would thus be a cheap equal weft and warp plain weave cotton fabric in white, cream or unbleached cotton, with a block printed design using a single alizarin dye, fixed with two mordants giving a red and black pattern. Polychromatic prints could be done, with two sets of blocks and an additional blue dye. The Indian taste was for dark printed backgrounds while the European market preferred a pattern on a cream base. As the century progressed the European preference moved from the large chintz patterns to a smaller, tighter patterns.
Thomas Bell patented the technique of printing by copper rollers in 1783, and the first machine was set up by Livesey, Hargreaves and Company near Preston in 1785. Production of printed cloth in Lancashire in 1750 was estimated to be 50,000 pieces of , but in 1850 it was 20,000,000 pieces. From 1888, block printing was limited to short run specialist jobs. After 1880, profits from printing became smaller, there was over capacity and the firms started to form combines. The first was when 3 Scottish firms formed the United Turkey Red Co. Ltd in 1897, and the second, in 1899, was the much larger Calico Printers' Association. 46 printing concerns and 13 merchants combined, representing 85% of the British printing capacity. Some of this capacity was removed and in 1901 they had 48% of the trade. In 1916, they and the other printers joined and formed a trade association. This then set minimum prices for each 'price section' of the industry. This held until 1954 when it was challenged by the government Monopolies Commission. Over the intervening period much trade had been lost overseas.
Terminology.
In the UK, Australia and New Zealand:
In the US:
Printed calico was imported into the United States from Lancashire in the 1780s, and here a linguistic separation occurred, while Europe maintained the word calico for the fabric, in the States it was used to refer to the printed design.
These colorful, small-patterned printed fabrics gave rise to the use of the word calico to describe a cat coat color: "calico cat". The patterned fabric also gave its name to two species of North American crabs; see the calico crab.

</doc>
<doc id="7522" url="http://en.wikipedia.org/wiki?curid=7522" title="Calorimetry">
Calorimetry

Calorimetry is the science or act of measuring changes state variables of a body for the purpose of deriving the heat transfer associated with changes of its state due for example to chemical reactions, physical changes, or phase transitions under specified constraints. Calorimetry is performed with a calorimeter. The word "calorimetry" is derived from the Latin word "calor", meaning heat and the Greek word "μέτρον" (metron), meaning measure. Scottish physician and scientist Joseph Black, who was the first to recognize the distinction between heat and temperature, is said to be the founder of the science of calorimetry. 
Indirect calorimetry calculates heat that living organisms produce by measuring either their production of carbon dioxide and nitrogen waste (frequently ammonia in aquatic organisms, or urea in terrestrial ones), or from their consumption of oxygen. 
Lavoisier noted in 1780 that heat production can be predicted from oxygen consumption this way, using multiple regression. The Dynamic Energy Budget theory explains why this procedure is correct. Heat generated by living organisms may also be measured by direct calorimetry, in which the entire organism is placed inside the calorimeter for the measurement.
A widely used modern instrument is the differential scanning calorimeter, a device which allows thermal data to be obtained on small amounts of material. It involves heating the sample at a controlled rate and recording the heat flow either into or from the specimen.
Classical calorimetric calculation of heat.
Basic classical calculation with respect to volume.
Calorimetry requires that a reference material that changes temperature have known definite thermal constitutive properties. The classical rule, recognized by Clausius and by Kelvin, is that the pressure exerted by the calorimetric material is fully and rapidly determined solely by its temperature and volume; this rule is for changes that do not involve phase change, such as melting of ice. There are many materials that do not comply with this rule, and for them, the present formula of classical calorimetry does not provide an adequate account. Here the classical rule is assumed to hold for the calorimetric material being used, and the propositions are mathematically written:
The thermal response of the calorimetric material is fully described by its pressure formula_1 as the value of its constitutive function formula_2 of just the volume formula_3 and the temperature formula_4. All increments are here required to be very small.
When a small increment of heat is gained by a calorimetric body, with small increments, formula_5 of its volume, and formula_6 of its temperature, the increment of heat, formula_7, gained by the body of calorimetric material, is given by
where
The latent heat with respect to volume is the heat required for unit increment in volume at constant temperature. It can be said to be 'measured along an isotherm', and the pressure the material exerts is allowed to vary freely, according to its constitutive law formula_18. For a given material, it can have a positive or negative sign or exceptionally it can be zero, and this can depend on the temperature, as it does for water about 4 C. The concept of latent heat with respect to volume was perhaps first recognized by Joseph Black in 1762. The term 'latent heat of expansion' is also used. The latent heat with respect to volume can also be called the 'latent energy with respect to volume'. For all of these usages of 'latent heat', a more systematic terminology uses 'latent heat capacity'.
The heat capacity at constant volume is the heat required for unit increment in temperature at constant volume. It can be said to be 'measured along an isochor', and again, the pressure the material exerts is allowed to vary freely. It always has a positive sign. This means that for an increase in the temperature of a body without change of its volume, heat must be supplied to it. This is consistent with common experience.
Quantities like formula_7 are sometimes called 'curve differentials', because they are measured along curves in the formula_20 surface.
Constant-volume calorimetry (Bomb Calorimetry).
Constant-volume calorimetry is calorimetry performed at a constant volume. This involves the use of a constant-volume calorimeter. Heat is still measured by the above-stated principle of calorimetry.
This means that in a suitably constructed calorimeter, the increment of volume formula_5 can be made to vanish, formula_22. For constant-volume calorimetry:
where
Classical heat calculation with respect to pressure.
From the above rule of calculation of heat with respect to volume, there follows one with respect to pressure.
In a process of small increments, formula_26 of its pressure, and formula_6 of its temperature, the increment of heat, formula_7, gained by the body of calorimetric material, is given by
where
The new quantities here are related to the previous ones:
where
and
The latent heats formula_9 and formula_30 are always of opposite sign.
It is common to refer to the ratio of specific heats as
Calorimetry through phase change.
An early calorimeter was that used by Laplace and Lavoisier, as shown in the figure above. It worked at constant temperature, and at atmospheric pressure. The latent heat involved was then not a latent heat with respect to volume or with respect to pressure, as in the above account for calorimetry without phase change. The latent heat involved in this calorimeter was with respect to phase change, naturally occurring at constant temperature. This kind of calorimeter worked by measurement of mass of water produced by the melting of ice, which is a phase change.
Cumulation of heating.
For a time-dependent process of heating of the calorimetric material, defined by a continuous joint progression formula_53 of formula_54 and formula_55, starting at time formula_56 and ending at time formula_57, there can be calculated an accumulated quantity of heat delivered, formula_58 . This calculation is done by mathematical integration along the progression with respect to time. This is because increments of heat are 'additive'; but this does not mean that heat is a conservative quantity. The idea that heat was a conservative quantity was invented by Lavoisier, and is called the 'caloric theory'; by the middle of the nineteenth century it was recognized as mistaken. Written with the symbol formula_59, the quantity formula_58 is not at all restricted to be an increment with very small values; this is in contrast with formula_7.
One can write
This expression uses quantities such as formula_65 which are defined in the section below headed 'Mathematical aspects of the above rules'.
Mathematical aspects of the above rules.
The use of 'very small' quantities such as formula_7 is related to the physical requirement for the quantity formula_2 to be 'rapidly determined' by formula_3 and formula_4; such 'rapid determination' refers to a physical process. These 'very small' quantities are used in the Leibniz approach to the infinitesimal calculus. The Newton approach uses instead 'fluxions' such as formula_70, which makes it more obvious that formula_2 must be 'rapidly determined'.
In terms of fluxions, the above first rule of calculation can be written
where
The increment formula_7 and the fluxion formula_65 are obtained for a particular time formula_73 that determines the values of the quantities on the righthand sides of the above rules. But this is not a reason to expect that there should exist a mathematical function formula_82. For this reason, the increment formula_7 is said to be an 'imperfect differential' or an 'inexact differential'. Some books indicate this by writing formula_84 instead of formula_7. Also, the notation "đQ" is used in some books. Carelessness about this can lead to error.<ref name="Planck 1923/1926 57">Planck, M. (1923/1926), page 57.</ref>
The quantity formula_62 is properly said to be a functional of the continuous joint progression formula_53 of formula_54 and formula_55, but, in the mathematical definition of a function, formula_62 is not a function of formula_20. Although the fluxion formula_65 is defined here as a function of time formula_73, the symbols formula_94 and formula_82 respectively standing alone are not defined here.
Physical scope of the above rules of calorimetry.
The above rules refer only to suitable calorimetric materials. The terms 'rapidly' and 'very small' call for empirical physical checking of the domain of validity of the above rules.
The above rules for the calculation of heat belong to pure calorimetry. They make no reference to thermodynamics, and were mostly understood before the advent of thermodynamics. They are the basis of the 'thermo' contribution to thermodynamics. The 'dynamics' contribution is based on the idea of work, which is not used in the above rules of calculation.
Experimentally conveniently measured coefficients.
Empirically, it is convenient to measure properties of calorimetric materials under experimentally controlled conditions.
Pressure increase at constant volume.
For measurements at experimentally controlled volume, one can use the assumption, stated above, that the pressure of the body of calorimetric material is can be expressed as a function of its volume and temperature.
For measurement at constant experimentally controlled volume, the isochoric coefficient of pressure rise with temperature, is defined by
Expansion at constant pressure.
For measurements at experimentally controlled pressure, it is assumed that the volume formula_3 of the body of calorimetric material can be expressed as a function formula_98 of its temperature formula_4 and pressure formula_1. This assumption is related to, but is not the same as, the above used assumption that the pressure of the body of calorimetric material is known as a function of its volume and temperature; anomalous behaviour of materials can affect this relation.
The quantity that is conveniently measured at constant experimentally controlled pressure, the isobaric volume expansion coefficient, is defined by
Compressibility at constant temperature.
For measurements at experimentally controlled temperature, it is again assumed that the volume formula_3 of the body of calorimetric material can be expressed as a function formula_98 of its temperature formula_4 and pressure formula_1, with the same provisos as mentioned just above.
The quantity that is conveniently measured at constant experimentally controlled temperature, the isothermal compressibility, is defined by
Relation between classical calorimetric quantities.
Assuming that the rule formula_18 is known, one can derive the function formula_108 that is used above in the classical heat calculation with respect to pressure. This function can be found experimentally from the coefficients formula_109 and formula_110 through the mathematically deducible relation
Connection between calorimetry and thermodynamics.
Thermodynamics developed gradually over the first half of the nineteenth century, building on the above theory of calorimetry which had been worked out before it, and on other discoveries. According to Gislason and Craig (2005): "Most thermodynamic data come from calorimetry..." According to Kondepudi (2008): "Calorimetry is widely used in present day laboratories."
In terms of thermodynamics, the internal energy formula_112 of the calorimetric material can be considered as the value of a function formula_113 of formula_20, with partial derivatives formula_115 and formula_116.
Then it can be shown that one can write a thermodynamic version of the above calorimetric rules:
with
and
Again, further in terms of thermodynamics, the internal energy formula_112 of the calorimetric material can sometimes, depending on the calorimetric material, be considered as the value of a function formula_121 of formula_122, with partial derivatives formula_123 and formula_116, and with formula_3 being expressible as the value of a function formula_126 of formula_122, with partial derivatives formula_128 and formula_129 .
Then, according to Adkins (1975), it can be shown that one can write a further thermodynamic version of the above calorimetric rules:
with
and
Beyond the calorimetric fact noted above that the latent heats formula_9 and formula_30 are always of opposite sign, it may be shown, using the thermodynamic concept of work, that also
Special interest of thermodynamics in calorimetry: the isothermal segments of a Carnot cycle.
Calorimetry has a special benefit for thermodynamics. It tells about the heat absorbed or emitted in the isothermal segment of a Carnot cycle.
A Carnot cycle is a special kind of cyclic process affecting a body composed of material suitable for use in a heat engine. Such a material is of the kind considered in calorimetry, as noted above, that exerts a pressure that is very rapidly determined just by temperature and volume. Such a body is said to change reversibly. A Carnot cycle consists of four successive stages or segments:
(3) another isothermal change in volume from formula_140 to a volume formula_142 at constant temperature formula_143 such as to incur a flow or heat out of the body and just such as to precisely prepare for the following change
(4) another adiabatic change of volume from formula_142 back to formula_136 just such as to return the body to its starting temperature formula_138.
In isothermal segment (1), the heat that flows into the body is given by
and in isothermal segment (3) the heat that flows out of the body is given by
Because the segments (2) and (4) are adiabats, no heat flows into or out of the body during them, and consequently the net heat supplied to the body during the cycle is given by
This quantity is used by thermodynamics and is related in a special way to the net work done by the body during the Carnot cycle. The net change of the body's internal energy during the Carnot cycle, formula_150, is equal to zero, because the material of the working body has the special properties noted above.
Special interest of calorimetry in thermodynamics: relations between classical calorimetric quantities.
Relation of latent heat with respect to volume, and the equation of state.
The quantity formula_9, the latent heat with respect to volume, belongs to classical calorimetry. It accounts for the occurrence of energy transfer by work in a process in which heat is also transferred; the quantity, however, was considered before the relation between heat and work transfers was clarified by the invention of thermodynamics. In the light of thermodynamics, the classical calorimetric quantity is revealed as being tightly linked to the calorimetric material's equation of state formula_18. Provided that the temperature formula_153 is measured in the thermodynamic absolute scale, the relation is expressed in the formula
Difference of specific heats.
Advanced thermodynamics provides the relation
From this, further mathematical and thermodynamic reasoning leads to another relation between classical calorimetric quantities. The difference of specific heats is given by

</doc>
<doc id="7525" url="http://en.wikipedia.org/wiki?curid=7525" title="Charles Evans Hughes">
Charles Evans Hughes

Charles Evans Hughes, Sr. (April 11, 1862 – August 27, 1948) was an American statesman, lawyer and Republican politician from New York. He served as the 36th Governor of New York (1907–1910), Associate Justice of the Supreme Court of the United States (1910–1916), United States Secretary of State (1921–1925), a judge on the Court of International Justice (1928–1930), and the 11th Chief Justice of the United States (1930–1941). He was the Republican candidate in the 1916 U.S. Presidential election, losing narrowly to incumbent President Woodrow Wilson.
Hughes was a professor in the 1890s, a staunch supporter of Britain's New Liberalism, an important leader of the progressive movement of the 20th century, a leading diplomat and New York lawyer in the days of Harding and Coolidge, and was known for being a swing voter when dealing with cases related to the New Deal in the 1930s. Historian Clinton Rossiter has hailed him as a leading American conservative.
Early life.
Charles Evans Hughes was born in Glens Falls, New York, the son of Rev. David C. Hughes and Mary C. (Connelly) Hughes, a sister of State Senator Henry C. Connelly (1832–1912). He was active in the Northern Baptist church, a Mainline Protestant denomination.
Education.
Hughes was educated in a private school. At the age of 14, he enrolled at Madison University (now Colgate University), where he became a member of Delta Upsilon fraternity. He then transferred to Brown University, continuing as a member of Delta Upsilon. He graduated third in his class at the age of 19, having been elected to Phi Beta Kappa in his junior year. He read law and entered Columbia Law School in 1882, where he graduated in 1884 with highest honors. While studying law, he taught at Delaware Academy.
Marriage and family.
In 1885, Hughes met Antoinette Carter, the daughter of a senior partner of the law firm where he worked, and they were married in 1888. They had one son, Charles Evans Hughes, Jr. and three daughters, one of whom was Elizabeth Hughes Gossett, one of the first humans injected with insulin, and who later served as president of the Supreme Court Historical Society. Hughes was the grandfather of Charles Evans Hughes III and H. Stuart Hughes.
Early career.
After graduating Hughes began working for Chamberlain, Carter & Hornblower where he met his future wife. In 1888, shortly after he was married, he became a partner in the firm, and the name was changed to Carter, Hughes & Cravath. Later the name was changed to Hughes, Hubbard & Reed. In 1891, Hughes left the practice of law to become a professor at Cornell Law School, but in 1893, he returned to his old law firm in New York City to continue practicing until he ran for governor in 1906. He continued his association with Cornell as a special lecturer from 1893 to 1895. He was also a special lecturer for New York University Law School, 1893–1900.
At that time, in addition to practicing law, Hughes taught at New York Law School with Woodrow Wilson, who would later defeat him for the Presidency. In 1905, he was appointed as counsel to the New York state legislative "Stevens Gas Commission", a committee investigating utility rates. His uncovering of corruption led to lower gas rates in New York City. In 1906, he was appointed to the "Armstrong Insurance Commission" to investigate the insurance industry in New York as a special assistant to U.S. Attorney General.
Governor of New York.
Hughes served as the Governor of New York from 1907 to 1910. He defeated William Randolph Hearst in the 1906 election to gain the position, and he was the only Republican statewide candidate to win office. An admirer of Britain's New Liberal philosophy, Hughes campaigned on a platform to improve the state of New York's standard of living by moving it away from laissez-faire tradition and enacting social reforms similar to that which had been enacted in Britain. As a supporter of progressive policies, Hughes was able to play on the popularity of Theodore Roosevelt and weaken the power of the state's conservative Republican officials. In 1908, he was offered the vice-presidential nomination by William Howard Taft, but he declined it to run again for Governor. Theodore Roosevelt became an important supporter of Hughes.
As the Governor, Hughes produced important reform legislation in three areas: improvement of the machinery and processes of government; extension of the state's regulatory authority over businesses engaged in public services; and expansion of governmental police and welfare functions. To counter political corruption, he secured campaign laws in 1906 and 1907 that limited political contributions by corporations and forced candidates to account for their receipts and expenses, legislation that was quickly copied in fifteen other states. He pushed the passage of the Moreland Act, which enabled the governor to oversee city and county officials as well as officials in semi-autonomous state bureaucracies. This allowed him to fire many corrupt officials. He also managed to have the powers of the state's Public Service Commissions increased and fought strenuously, if not completely successfully, to get their decisions exempted from judicial review.
When two bills were passed to reduce railroad fares, Hughes vetoed them on the grounds that the rates should be set by expert commissioners rather than by elected ones. His ideal was not government by the people but for the people. As Hughes put it, "you must have administration by administrative officers."
Hughes, however, would be unsuccessful in achieving one of his main goals as governor: primary voting reform. Hoping to achieve a compromise with the state's party bosses, Hughes rejected the option of a direct primary in which voters could choose between declared candidates and instead proposed a complicated system of nominations by party committees. The state's party bosses, however, rejected this compromise and the state legislature rejected the plan on three occasions in 1909 and 1910.
On social issues, Hughes strongly supported relatively limited social reforms. He endorsed the Page-Prentice Act of 1907, which set an eight-hour day and forty-eight-hour week for factory workers—but only for those under the age of sixteen. By employing the well-established legal distinction between ordinary and hazardous work, the governor also won legislative approval for a Dangerous Trades Act that barred young workers from thirty occupations. To enforce these and other regulations, in 1907 Hughes reorganized the Department of Labor and appointed a well-qualified commissioner. Two years later, the governor created a new bureau for immigrant issues in the Department of Labor and appointed reformer Frances Kellor to head it.
In his final year as the Governor, he had the state comptroller draw up an executive budget. This began a rationalization of state government and eventually it led to an enhancement of executive authority. He also signed the Worker's Compensation Act of 1910, which required a compulsory, employer-paid plan of compensation for workers injured in hazardous industries and a voluntary system for other workers; after the New York Court of Appeals ruled the law unconstitutional in 1911, a popular referendum was held that successfully made the law an amendment in the New York Constitution.
In 1908, Governor Hughes reviewed the clemency petition of Chester Gillette concerning the murder of Grace Brown. The governor denied the petition as well as an application for reprieve, and Gillette was electrocuted in March of that year.
When Hughes left office, a prominent journal remarked "One can distinctly see the coming of a New Statism ... [of which] Gov. Hughes has been a leading prophet and exponent". In 1926, Hughes was appointed by New York Governor Alfred E. Smith to be the chairman of a "State Reorganization Commission" through which Smith's plan to place the Governor as the head of a rationalized state government, was accomplished, bringing to realization what Hughes himself had envisioned.
In 1909, Hughes led an effort to incorporate Delta Upsilon fraternity. This was the first fraternity to incorporate, and he served as its first international president.
Supreme Court.
On April 25, 1910, Hughes was appointed by President William H. Taft to a seat as an Associate Justice of the United States Supreme Court vacated by David J. Brewer. Hughes was confirmed by the United States Senate on May 2, 1910, and received his commission the same day. As an associate justice of the Supreme Court from 1910 to 1916, Hughes remained an advocate of regulation and authored decisions that weakened the legal foundations of laissez-faire capitalism. He also mastered a new set of issues regarding the commerce clause and, in a deliberately restrained manner, wrote constitutional decisions that expanded the regulatory powers of both the state and federal governments.
The respective authority of federal and state governments under the Constitution's commerce clause had long been in dispute. In "Cooley v. Board of Wardens" (1852) the Court headed by Roger B. Taney had allowed the states, in the absence of federal legislation, to control those aspects of commerce that did not require a single national policy. However, more recent decisions, such as "Weldon v. Missouri" (1875), had curtailed the power of the states to tax or license out-of-state products or sales agents. Influenced perhaps by his experience as a state governor, Hughes authored a series of decisions that upheld state laws that affected—and, it might be argued, infringed on—congressional authority over interstate commerce. For example, invoking police power arguments, he upheld a Georgia statute requiring electric headlights on locomotives, including those engaged in interstate commerce.
The most important of these federalism-related decisions were the Minnesota Rate Cases of 1913. In these, Hughes enhanced state regulation of railroads by reviving the "Cooley" doctrine of "concurrent powers." To persuade his colleagues, Hughes composed a detailed and carefully argued opinion. He began with the generally accepted proposition that Minnesota and the other states had the authority, using their police powers, to regulate commerce within their bounds. He then extended this logic to include rate-regulation when such internal commerce was intermeshed with interstate traffic to towns in bordering states.
Even as Hughes expanded the regulatory power of the states, he took a nationalist stance with respect to the authority of Congress over commerce, including that within the various states. Thus, in the important Shreveport Cases of 1914, Hughes sustained a decision of the Interstate Commerce Commission voiding intrastate rates set by the Railroad Commission of Texas. The Texas rates encouraged the development of Dallas and Houston by blatantly discriminating against Texas shippers who marketed their goods via Shreveport, Louisiana. In striking down these rates as an interference with interstate commerce, Hughes recognized that the logic of his argument would permit federal regulation of any action that affected commerce. Thus, it might be used to challenge the sharp distinction made in "United States v. E. C. Knight Co." (1895) between commerce, which was subject to federal regulation, and manufacturing, which was not. Reluctant to infringe upon precedent, he inserted language that sought to limit the decision's reach to railroad carriers: "the agencies of interstate commerce." By leaving Knight intact, the associate justice restricted the authority of the federal government over local businesses or factories whose raw materials or products were part of interstate commerce. Yet the logic of his argument pointed to the position he would espouse during the constitutional crisis of 1937.
In cases involving the controversial issue of anti-trust regulation, the Supreme Court was divided. The faction led by John Marshall Harlan and Rufus Peckham embraced a small-producer ethic and a fully competitive market; these justices used the Sherman Act's prohibition of "restraint of trade" to outlaw price fixing by businesses. A second group, headed by Chief Justice Edward D. White and Oliver Wendell Holmes, Jr., stood for "reasonable" market regulation, managed either by private agreements among producers (long permitted under common law) or by public administrative agencies. Preferring administrative regulation to the play of market forces, Hughes usually voted with White and Holmes in anti-trust cases.
In three other sets of cases, Hughes also authored opinions that bolstered the regulatory powers of state legislatures and administrative bodies. In the first line of decisions, he gave a narrow interpretation to the "contract clause" of the United States Constitution, which prohibits states from enacting any law "impairing the obligation of contracts." Refusing to give a literal reading to the state-granted charter of the Southern Pacific Railroad, which specified that the company could "collect and receive such tariffs ... as it may prescribe, Hughes contended that this clause "necessarily implies that the charges shall be reasonable and does not detract from the power of the State ... to prescribe reasonable rates."
In a second set of opinions, Hughes favored regulation over certain claims of individual rights. Thus, in "Wilson v. U.S". (1911), he asserted that corporate officers could not resist a subpoena for company records by invoking the Fifth Amendment's privilege against self-incrimination. This decision made corporations more vulnerable to prosecution by limiting the rights of individuals as delineated in "Boyd v. U.S." (1886). The Court's reasoning in "Boyd" had extended "the personal security of the citizen" guaranteed by the Fourth and Fifth Amendments to include an individual's personal papers. Sensing danger to Boyd's broad definition of individual rights, Justice McKenna dissented in "Wilson", declaring that Hughes's distinction between personal and corporate papers was "a limitation by construction" on an important "constitutional security for personal liberty." For his part, Hughes was unwilling to construe individual rights so that they frustrated the government's efforts to achieve a legitimate regulatory goal.
In a third set of pre-1916 cases, Hughes addressed the laissez-faire doctrine of "liberty of contract." Using this legal principle, many judges in Britain and the United States had voided, as an infringement of an individual's property rights, legislation that regulated common law bargains made in the marketplace between employers and their workers. But in a seminal article of 1881, "Liberal Legislation and Freedom of Contract," the Oxford political philosopher Thomas Hill Green disputed this reasoning. Green pointed out that the British Factory Acts had already limited the liberty of industrial capitalists and that legislation requiring compulsory schooling had circumscribed the freedom of parents. Extending the logic of these measures, Green adumbrated a positive and collectivist definition of liberty, a concept of "public freedom" that justified legislative oversight of economic life, especially land ownership and use. He likewise proposed legislative intervention into the terms of private bargains, to "provide against contracts being made which, from the helplessness of one of the parties to them, instead of being a security for freedom, become an instrument of disguised repression." Picking up this line of argument and declaring "a great departure from the principles of free contract," Gladstone created an Irish Land Court with complete control over rents and other landlord-tenant issues. Two decades later, Churchill and other New Liberals regularly invoked Green's arguments in parliamentary debates over English legislation.
Similar arguments appeared in the United States. In an article in the "Columbia Law Review" in 1908, Roscoe Pound of the University of Chicago mounted a vigorous attack on "mechanical jurisprudence", the judicial practice of "rigorous logical deduction from predetermined conceptions in disregard of ... the actual facts." Citing "Lochner v. New York", the controversial decision of 1905 upholding freedom of contract, Pound assailed the Supreme Court for giving "us rules which, when applied to the existing commercial and industrial situation, are wholly inadequate." In 1909 Pound continued his assault on conceptual thinking in an essay on "Liberty of Contract." Focusing upon "Adair v. U.S." (1908), which invalidated another law regulating labor contracts, he berated the Court for not recognizing the "practical conditions of inequality."
The central problem, Pound argued, was that the legal system "exhibits too great a respect for the individual" and "too little respect for the needs of society." Pound came to this position partly through long debates with his former colleague at the University of Nebraska, the rising sociologist Edward A. Ross. "We have grown into an organic society," Ross argued, "in which the welfare of all is at the mercy of each." The two men continued their dialogue when Ross moved to the University of Wisconsin, where he became a colleague of Richard Ely. Influenced like Pound by German social and legal thinkers, Ely in 1903 had ascribed "the coercion of economic forces" in American society "to the unequal strength of those who make a contract."
Hughes undoubtedly was aware of these intellectual currents. There is no evidence that he was directly influenced by T. H. Green; however, he knew Ely through the AALL and had probably read Pound's essays. Whatever the precise links, the associate justice wrote opinions that mirrored the arguments of the Oxford philosopher of "positive liberty" and the sociologically inclined Midwestern professors. "Freedom of contract is a qualified and not an absolute right ...," Hughes declared in upholding an Iowa law that voided contracts limiting the legal rights of railroad workers: The state may "interfere where the parties do not stand upon an equality..." Using similar reasoning, the associate justice upheld a California law that mandated a forty-eight-hour work-week for women in various industries and allowed a federal statute to override a contract between an interstate railroad and its employees. Finally, Hughes joined Justice Day's dissent in "Coppage v. Kansas" (1915), a case in which a majority of the Court struck down a Kansas law forbidding "yellow dog contracts" that prevented workers from joining a union. Citing the police power, Day and Hughes argued that a state could legitimately ban such contracts.
He wrote for the court in "Bailey v. Alabama" , which held that involuntary servitude encompassed more than just slavery, and "Interstate Commerce Comm. v. Atchison T & SF R Co." , holding that the Interstate Commerce Commission could regulate intrastate rates if they were significantly intertwined with interstate commerce.
On April 15, 1915, in the case of "Frank v. Mangum", the Supreme Court decided (7-2) to deny an appeal made by Leo Frank's attorneys, and instead upheld the decision of lower courts to sustain the guilty verdict against Frank. Justice Hughes and Justice Oliver Wendell Holmes, Jr. were the two dissenting votes.
Presidential candidate.
Hughes resigned from the Supreme Court on June 10, 1916, to be the Republican candidate for President in 1916. He was also endorsed by the Progressive Party, thanks to the support given to him from former President Theodore Roosevelt. Other Republican figures such as former President William Howard Taft endorsed Hughes and felt the accomplishments he made as Governor of New York would establish him as formidable progressive alternative to Wilson. Many former leaders of the Progressive Party, however, endorsed Wilson because Hughes opposed the Adamson Act, the Sixteenth Amendment and diverted his focus away from progressive issues during the course of the campaign. Hughes was defeated by Woodrow Wilson in a close election (separated by 23 electoral votes and 594,188 popular votes). The election hinged on California, where Wilson managed to win by 3,800 votes and its 13 electoral votes and thus Wilson was returned for a second term; Hughes had lost the endorsement of the California governor and Roosevelt's 1912 Progressive running mate Hiram Johnson when he failed to show up for an appointment with him.
Despite coming close to winning the presidency, Hughes did not seek the Republican nomination again in 1920. Hughes also advocated ways to prevent the return of President Wilson's expanded government control over important industries such as the nation's railroads, which he felt would lead to the eventual destruction of individualism and political self-rule. After Robert LaFollette's Progressive Party advocated the return of such regulations during the 1924 US Presidential election, Hughes shifted rightwards believing that the federal bureaucracy should now have limited powers over individual liberties and property rights and that common law should be strictly enforced.
Secretary of State.
Hughes returned to government office in 1921 as Secretary of State under President Harding. On November 11, 1921, Armistice Day (changed to Veterans Day), the Washington Naval Conference for the limitation of naval armament among the Great Powers began. The major naval powers of Britain, France, Italy, Japan and the United States were in attendance as well as other nations with concerns about territories in the Pacific — Belgium, the Netherlands, Portugal and China.
The American delegation was headed by Hughes and included Elihu Root, Henry Cabot Lodge, and Oscar Underwood, the Democratic minority leader in the Senate. The conference continued until February 1922 and included the Four-power pact (December 13, 1921), Shantung Treaty (February 4, 1922), Five-Power Treaty, the Nine-Power Treaty (February 6, 1922), the "Six-power pact" that was an agreement between the Big Five Nations plus China to divide the German cable routes in the Pacific, and the Yap Island agreement.
Hughes continued in office after Harding died and was succeeded by Coolidge, but resigned after Coolidge was elected to a full term. On June 30, 1922, he signed the "Hughes–Peynado agreement" that ended the United States's six-year occupation of Dominican Republic.
Various appointments.
In 1907, Gov. Charles Evans Hughes became the first president of the newly formed Northern Baptist Convention—based at in Washington, DC, of which Hughes was a member. He also served as President of the New York State Bar Association.
After leaving the State Department, he again rejoined his old partners at the Hughes firm, which included his son and future United States Solicitor General Charles E. Hughes, Jr., and was one of the nation's most sought-after advocates. From 1925 to 1930, for example, Hughes argued over 50 times before the U.S. Supreme Court. From 1926 to 1930, Hughes also served as a member of the Permanent Court of Arbitration and as a judge of the Permanent Court of International Justice in The Hague, Netherlands from 1928 to 1930. He was additionally a delegate to the Pan American Conference on Arbitration and Conciliation from 1928 to 1930. He was one of the co-founders in 1927 of the National Conference on Christians and Jews, now known as the National Conference for Community and Justice (NCCJ), along with S. Parkes Cadman and others, to oppose the Ku Klux Klan, anti-Catholicism, and anti-Semitism in the 1920s and 1930s.
In 1925–1926, Charles Evans Hughes represented the API (American Petroleum Institute) before the FOCB (Federal Oil Conservation Board).
In 1928 conservative business interests tried to interest Hughes in the GOP presidential nomination of 1928 instead of Herbert Hoover. Hughes, citing his age, turned down the offer.
Chief Justice.
Herbert Hoover, who had appointed Hughes's son as Solicitor General in 1929, appointed Hughes Chief Justice of the United States on February 3, 1930. Hughes was confirmed by the United States Senate on February 13, 1930, and received commission the same day, serving in this capacity until 1941. Hughes replaced former President William Howard Taft, a fellow Republican who had also lost a presidential election to Woodrow Wilson (in 1912) and who, in 1910, had appointed Hughes to his first tenure on the Supreme Court.
Hughes' appointment was opposed by progressive elements in both parties who felt that he was too friendly to big business. Idaho Republican William E. Borah said on the United States Senate floor that confirming Hughes would constitute "placing upon the Court as Chief Justice one whose views are known upon these vital and important questions and whose views, in my opinion, however sincerely entertained, are not which ought to be incorporated in and made a permanent part of our legal and economic system." Nonetheless Hughes was confirmed as Chief Justice with a vote of 52 to 26.
Hughes as Chief Justice swore in President Franklin D. Roosevelt in 1933, 1937 and 1941.
Upon his return to the court, more progressives had joined the bench and Hughes seemed determined again to vote progressive and soon bring an end to the longstanding pro-business Lochner era. During his early years as Chief Justice, however, the fear he had developed for an overblown bureaucracy during World War I undermined his optimism. Showing his old progressive image, he upheld legislation protecting civil rights and civil liberties and wrote the opinion for the Court in "Near v. Minnesota" , which held prior restraint against the press is unconstitutional. Concerning economic regulation, however, he was still willing to uphold legislation which supported "freedom of opportunity" for individuals on the one hand and the "police power" of the state on the other but did not personally favor legislation that linked national economic planning and bureaucratic social welfare together. At first resisting Roosevelt's New Deal and building a consensus of centrist members of the court, Hughes used his influence to limit the collectivist scope of Roosevelt's changes and would often strike down New Deal legislation he felt was poorly drafted and did not clearly specify how they were constitutional. By 1935, however, Hughes felt the court's four conservative Justices had disregarded common law and sought to curb their power.
Hughes was often aligned with the court's three liberal Justices Louis Brandeis, Harlan Fiske Stone, and Benjamin Cardozo in finding some New Deal measures (such as the violation of the gold clauses in contracts and the confiscation of privately owned monetary gold) Constitutional, On one occasion, Hughes would side with the conservatives in striking down the New Deal's Agricultural Adjustment Act in the 1936 case "United States v. Butler", which held that the law was unconstitutional because its so-called tax policy was a coercive regulation rather than a tax measure and the federal government lacked authority to regulate agriculture, but surprisingly did not assign the majority opinion, a practice usually required for court's most senior justice who agrees with the majority opinion, and allowed Associate Justice Owen Roberts to speak for the entire majority in his own words. It was accepted that he did not agree with the argument that the federal government lacked authority over agriculture and was going to write a separate opinion upholding the act's regulation policy while striking down the act's taxation policy on the grounds that it was a coercive regulation rather than a tax measure. However, Roberts convinced Hughes that he would side with him and the three liberal justices in future cases pertaining to the nation's agriculture which involved the Constitution's General Welfare Clause if he agreed to join his opinion.
By 1936, Hughes sensed the growing hostility in the court and could do little about it. In the 1936 case "Carter v. Carter Coal Company", Hughes took a middle ground for both doctrinal and court-management reasons. Writing his own opinion, he joined the three liberal justices in upholding the Bituminous Coal Conservation Act's marketing provision but sided with Roberts and the four conservatives in striking down the act's provision which regulated local labor. By 1937, as the court leaned more in his favor, Hughes would renounce the position he took in the "Carter" case regarding local labor and ruled that the procedural methods that governed the Wagner Act's labor regulation provisions bore resemblance to the procedural methods which governed the railroad rates that the Interstate Commerce Commission was allowed to maintain in the 1914 "Shreveport" decision and thus demonstrated that Congress could use its commerce power to regulate local industrial labor as well.
In 1937, when Roosevelt attempted to pack the Court with six additional justices, Hughes worked behind the scenes to defeat the effort, which failed in the Senate. by rushing important New Deal legislation- such as Wagner Act and the Social Security Act- through the court and ensuring that the court's majority would uphold their constitutionality. The month after Roosevelt's court-packing announcement, Roberts, who had joined the four conservative Justices in striking down important New Deal legislation, shocked the American public by siding with Hughes and the court's three liberal justices in striking down the court's previous ruling in the 1923 "Adkins v. Children's Hospital" case-which held that laws requiring minimum wage violated the Fifth Amendment's due process clause- and upholding the constitutionality of Washington state's minimum wage law in "West Coast Hotel Co. v. Parrish". Because Roberts had previously sided with the four conservative justices and used the "Adkins" decision as the basis for striking down a similar minimum wage law the state of New York enforced in "Morehead v. New York ex rel. Tipaldo" it was widely perceived that he only agreed to uphold the constitutionality of minimum wage as a result of the pressure that was put on the Supreme Court by the court-packing plan. However, both Hughes and Roberts acknowledged that the Chief Justice, in fact, had already convinced Roberts to change his method of voting months before Roosevelt announced his court-packing plan and that the effort he put into defeating the plan played only a small significance in determining how the court's majority made their decisions in future cases pertaining to New Deal legislation.
Following the overwhelming support that voters showed for the New Deal through Roosevelt's overwhelming re-election in November 1936, Hughes was able to persuade Roberts no longer to base his votes on political maneuvering and to side with him in future cases regarding New Deal related policies. Roberts had voted to grant "certiorari" to hear the "Parrish" case before the election of 1936. Oral arguments occurred on December 16 and 17, 1936, with counsel for Parrish specifically asking the court to reconsider its decision in "Adkins v. Children's Hospital", which had been the basis for striking down a New York minimum wage law in "Morehead v. New York ex rel. Tipaldo" in the late spring of 1936.
Roberts indicated his desire to overturn "Adkins" immediately after oral arguments ended for the Parrish case on Dec. 17, 1936. The initial conference vote on Dec. 19, 1936 was split 4-4; with this even division on the Court, the holding of the Washington Supreme Court, finding the minimum wage statute constitutional, would stand. The eight voting justices anticipated Justice Stone—absent due to illness—would be the fifth vote necessary for a majority opinion affirming the constitutionality of the minimum wage law. As Hughes desired a clear and strong 5-4 affirmation of the Washington Supreme Court's judgment, rather than a 4-4 default affirmation, he convinced the other justices to wait until Stone's return before both deciding and announcing the case. In one of his notes from 1936, Hughes wrote that Roosevelt's re-election forced the court to depart from its "fortress in public opinion" and severely weakened its capability to base its rulings on either personal or political beliefs.
President Roosevelt announced his court reform bill on February 5, 1937, the day of the first conference vote after Stone's February 1, 1937 return to the bench. Roosevelt later made his justifications for the bill to the public on March 9, 1937 during his 9th Fireside Chat. The Court's opinion in "Parrish" was not published until March 29, 1937, after Roosevelt's radio address. Hughes wrote in his autobiographical notes that Roosevelt's court reform proposal "had not the slightest effect on our [the court's] decision," but due to the delayed announcement of its decision the Court was characterized as retreating under fire.
Although Hughes wrote the opinion invalidating the National Recovery Administration in "Schechter Poultry Corp. v. United States"- though the decision was also a unanimous one upheld by all of the court's nine Justices-, he also wrote the opinions for the Court in "NLRB v. Jones & Laughlin Steel Corp.", "NLRB v. Friedman-Harry Marks Clothing Co." and "West Coast Hotel Co. v. Parrish" which approved some New Deal measures. Hughes supervised the move of the Court from its former quarters at the U.S. Capitol to the newly constructed Supreme Court building.
Hughes wrote twice as many constitutional opinions as any of his court's other members. "His opinions, in the view of one commentator, were concise and admirable, placing Hughes in the pantheon of great justices." His "remarkable intellectual and social gifts...made him a superb leader and administrator. He had a photographic memory that few, if any, of his colleagues could match. Yet he was generous, kind, and forebearing in an institution where egos generally come in only one size: extra large!"
Later life.
For many years, he was a member of the Union League Club of New York and served as its president from 1917 to 1919.
On August 27, 1948, Hughes died in what is now the Tiffany Cottage of the Wianno Club in Osterville, Massachusetts. His remains are interred at Woodlawn Cemetery in Bronx, New York.

</doc>
<doc id="7527" url="http://en.wikipedia.org/wiki?curid=7527" title="Concept album">
Concept album

A concept album is a studio album where all musical or lyrical ideas contribute to a single overall theme or unified story. In contrast, typical studio albums consist of a number of unconnected songs (lyrically and otherwise) performed by the artist. It has been argued that concept albums should refer only to albums that bring in themes or story lines from outside of music, given that a collection of love songs or songs from within a certain genre are not usually considered to be a "concept album."
History.
1940s–1960s.
Woody Guthrie's "Dust Bowl Ballads" (1940) is considered one of the first concept albums, consisting of semi-autobiographical songs about the hardships of American migrant labourers during the 1930s.
In the early 1950s, before the advent of rock and roll, concept albums were prevalent in jazz music. Singer Frank Sinatra recorded several concept albums, including "In the Wee Small Hours" (1955; songs about loneliness and heartache) and "Come Fly with Me" (1958; songs about world travel). Singer/pianist Nat King Cole's concept albums include "After Midnight" (1956; collaborations with jazz instrumentalists in the style of late-night jam sessions) and "Penthouse Serenade" (1955; songs detailing the "cocktail piano" era.).
After finding success with stand-alone singles, country icon Johnny Cash turned to themed albums, such as "Songs of Our Soil" (1959; songs about death and mortality) and "Blood, Sweat and Tears" (1963; songs about blue-collar workers).
Early rock concept albums.
Since "Colorful Ventures" (1961), The Ventures became known for releasing concept albums, including surf music, country, outer space, television themes, and psychedelic music.
In 1966, several albums were deemed as concept albums by their thematically-linked songs, and became inspiration for other artists to follow. The Beach Boys' "Pet Sounds" portrayed Brian Wilson's state of mind at the time, and was in turn a major inspiration to Paul McCartney. Album writers Brian Wilson and Tony Asher insist that the narrative was not intended, though Wilson has stated that the idea of the record being a "concept album" is mainly within the way the album was produced and structured. Later in 1966, Wilson began work on "Smile", an intentional narrative, though it was scrapped and later revived in November 2011. "Freak Out!", Frank Zappa and the Mothers of Invention's sardonic farce about rock music and America as a whole, and "Face to Face" by The Kinks, the first collection of Ray Davies's idiosyncratic character studies of ordinary people, are conceptually oriented albums. However, of the three, only "Pet Sounds" attracted a large commercial audience.
"Save for a Rainy Day", by Jan & Dean, had a concept featuring all rain-themed songs. In between each song there is a sound of rain. Dean Torrence recorded this album in 1966 as Jan & Dean soon after Jan Berry had his car crash near Dead Man's Curve in California. Torrence posed with Berry's brother Ken for the album cover photos. Columbia Records released one single from the project ("Yellow Balloon") as did the song's writer, Gary Zekley, with The Yellow Balloon, but with legal wrangles scuttling Torrence's Columbia deal and Berry's disapproval of the project, "Save for a Rainy Day" remained a self-released album on the J&D Record Co. label (JD-101). Sundazed Records reissued "Save for a Rainy Day" in 1996 in CD and vinyl formats, as well as the collector's vinyl 7" companion EP, "Sounds For A Rainy Day," featuring four instrumental versions of tracks from the album.
The Beatles' "Sgt. Pepper's Lonely Hearts Club Band", released in June 1967, would later bring about the notion of the concept album, with the earlier prototypes and examples from traditional pop music and other genres sometimes forgotten. Original reception described the album as a concept by select definitions of the term. There was, at some stage during the making of the album, an attempt to relate the material to firstly the idea of aging, then as an obscure radio play about the life of an ex-army bandsman and his shortcomings. These concepts were lost in the final production. While debate exists over the extent to which "Sgt. Pepper" qualifies as a true concept album, there is no doubt that its reputation as such helped inspire other artists to produce concept albums of their own, and inspired the public to anticipate them. Lennon and McCartney distanced themselves from the "concept album" tag as applied to that album.
"Days of Future Passed", released the same year as "Sgt. Pepper's Lonely Hearts Club Band", was fellow British musicians The Moody Blues' first concept album. Originally presented with an opportunity to rock out Dvořák's Symphony No. 9 "From the New World" by their new stereophonic label, the band instead forged ahead to unify their own orchestral-based threads of a day in the life of a common man.
"The Who Sell Out" by The Who followed with its concept of a pirate radio broadcast. Within the record, joke commercials recorded by the band and actual jingles from recently outlawed pirate radio station Radio London were interspersed between the songs, ranging from pop songs to hard rock and psychedelic rock, culminating with a mini-opera titled "Rael."
In October 1967, British psychedelic rock group Nirvana released their debut album, "The Story of Simon Simopath", to moderate commercial success. The songs' lyrics depict the life and death of the titular hero, blending various mythological themes, such as the existence of centaurs and goddesses, with those of science fiction.
"S.F. Sorrow" by British group the Pretty Things, released in December 1968, is generally considered to be among the first creatively successful rock concept albums, in that each song is part of an overarching unified concept – the life story of the main character, Sebastian Sorrow.
The album "Head" by The Monkees, released in 1968, and the soundtrack to the movie of the same name, was their final album and only concept album. It contains psychedelic songs that ventured away from their usual pop rock offerings such as "Porpoise Song". It also contains "Do I Have To Do This All Over Again" and "Can You Dig It", which are very different from their usual songs.
The rock opera "Tommy", released in April 1969, was composed by Pete Townshend and performed by The Who. This acclaimed work was presented over two LPs and it took the idea of thematically based albums to a much higher appreciation by both critics and the public. It was also the first story-based concept album of the rock era (as distinct from the "song-cycle" style album) to enjoy commercial success. The Who went on to further explorations of the concept album format with their follow-up project "Lifehouse", which was abandoned before completion, and with their 1973 rock opera, "Quadrophenia".
Five months after the release of "Tommy", The Kinks released another concept album, "Arthur (Or the Decline and Fall of the British Empire)" (September 1969), written by Ray Davies; though considered by some a rock opera, it was originally conceived as the score for a proposed but never realised BBC television drama. It was the first of several concept albums released by the band through the first few years of the 1970s. These were: "Lola Versus Powerman and the Moneygoround, Part One" (1970), "Muswell Hillbillies" (1971), ' (1973), ' (1974), "Soap Opera" (1975) and "Schoolboys in Disgrace" (1976).
1970s–1990s.
Pink Floyd released four concept albums during the 1970s; "The Dark Side of the Moon" (1973)," Wish You Were Here" (1975), "Animals" (1977), and "The Wall" (1979). The most notable of these is "The Dark Side of the Moon", which achieved a level of commercial success far beyond that of any other progressive rock album before or since.
In addition to Britain, bands from the European mainland were pushing the limitations of the three-minute song format, regularly requiring two sides of a single LP to complete a statement. Magma, from France, debuted in 1970 with an eponymous concept album, "Magma", about refugees fleeing a doomed Earth to settle on the fictional planet Kobaïa. From Greece, Aphrodite's Child, helmed by keyboardist Vangelis, released "666" in 1972. The double album, based on various passages from The Bible, was controversial for its title and sleeve notes. Italy's Banco del Mutuo Soccorso released "Darwin!" in 1972, a success that lead to the band to signing with Emerson, Lake & Palmer's Manticore Records. Triumvirat, a German progressive rock band signed with EMI to produce several concept albums including "Mediterranean Tales", "Pompeii" and in 1975, what is considered by many to be a masterpiece of the genre, "Spartacus".
The Americas also produced their share of concept albums during this period. From 1975 to 1979, Canadian progressive power trio Rush released three albums containing sidelong epics, regarded by some as concept albums (though not actually concept albums by strict definition of the term; that is, none of the other songs on the album have anything to do with each other or the 20-minute sidelong epic, so there is no pervasive concept or story). The first of these was released in 1975, titled "Caress of Steel". The second was their breakthrough album, "2112", released the following year in 1976. Their third was released in 1978, "Hemispheres".
Concept albums were hardly the exclusive product of progressive rock bands in the 1970s. From Country to Glam, artists from all genres would embrace the popularity of the LP to explore broader concepts that the 45 would have made impossible. Michael Nesmith blossomed creatively after quitting The Monkees, as an originator of what would become Country rock and in 1974 released the elaborately packaged concept album "". Willie Nelson is a pioneer of concept albums within country music. In 1974 he released "Phases and Stages" describing a divorce from the viewpoint of the woman on Side One and the man on Side Two. His 1975 album, "Red Headed Stranger", about the fatal estrangement of a cowboy from his unfaithful wife, followed and would reach #1 on the American country charts.
Although the progressive rock genre had begun to decline in popularity by the late 1970s, concept albums were still proving successful for well-established progressive rock bands, and a new subgenre, neo-progressive rock, emerged in the 1980s.
Genesis reinvented themselves as a sleek trio with the release of 1980's "Duke". This tale of fame, wealth, success and lost love was arguably the band's first full LP concept since 1974's "The Lamb Lies Down on Broadway". It was a huge commercial success, becoming their first UK number one album, and began a new, pop-oriented era for the band. "Turn It On Again" became the band's second UK top ten single. Whether "Duke" qualifies as a concept album is a matter of some confusion. It has been described as "in part a concept album, in part not". It has been said that "there does not seem to be a 'bigger picture' behind the songs, and it is uncertain whether there ever was an underlying concept".
Inspired by "The Wall", glam rock band Kiss recruited Lou Reed for lyrical assistance and released "Music from "The Elder"" in 1981. Due to the album's radical departure in musical style compared to Kiss's previous offerings, "Music from "The Elder"" became the group's poorest selling and charting album in their history. It has, however, grown in cult status since its release.
In 1985, the British neo-progressive rock band Marillion achieved their only UK number one album – and the best-selling album of their career – with "Misplaced Childhood", a concept album featuring lyrics by frontman Fish which were partly autobiographical. The album was played as two continuous pieces of music on the two sides of the vinyl and produced the band's two biggest hit singles, "Kayleigh" and "Lavender". The band's follow-up in 1987, "Clutching at Straws", has also been described as a concept album.
Styx continued to have multiplatinum albums with their 1981 release "Paradise Theater" (a concept album about a decaying theater in Chicago which became a metaphor for childhood and American culture) and 1983's "Kilroy Was Here" (a science fiction rock opera about a future where moralists imprison rockers). The elaborate concept would produce the bands last top ten hit in the U.S. with "Mr. Roboto", but arguments over the direction of the band toward increasingly dramatic concept productions led to breakup in 1984.
In the 1990s neo-progressive rock had all but faded from popular music, but some bands, such as Marillion, still had a sizeable cult fanbase. With the advent of alternative rock, however, a number of artists still continued to use the format within that genre. Other concept albums to then emerge not only from rock, but also from hiphop were to soon follow, especially in the mafioso themed, N.Y. stylized music of acts like Mobb Deep, Capone-N-Noreaga, and other Wu-tang works.
2000s–present.
The Flaming Lips concept "Yoshimi Battles the Pink Robots" was a commercial and critical success in 2002 even winning a Grammy Award for Best Rock Instrumental Performance.
Green Day's concept "American Idiot" was released in 2004. The album describes the story of a central character named the Jesus of Suburbia . After its release it went on to achieve success worldwide, charting in 27 countries and peaking at number one in 19 of them, including the US and the UK. It also won a Grammy in 2005 for Best Rock Album and was nominated for Album of the Year.
Arcade Fire's 2010 concept album "The Suburbs" debuted at #1 and won Album of the Year at the 2011 Grammy Awards. Its themes focus on regret and lost youth.
Danny Brown's "XXX", released in 2011, is considered a concept album about growing up, the fall of Detroit, and the impact of drugs on both. The album received critical acclaim, including being named SPIN's #1 rap album of 2011.

</doc>
<doc id="7530" url="http://en.wikipedia.org/wiki?curid=7530" title="Cro-hook">
Cro-hook

The cro-hook, is a special double-ended crochet hook used to make double-sided crochet. 
It employs the use of a long double-ended hook, which permits the maker to work stitches on or off from either end. Because the hook has two ends, two alternating colors of thread can be used simultaneously and freely interchanged, working loops over the hook. Crafts using a double-ended hook are commercially marketed as Cro-hook and Crochenit. Cro-hook is a variation of Tunisian crochet and also shows similarities with the Afghan stitch used to make Afghan scarves, but the fabric is typically softer with greater elasticity.

</doc>
<doc id="7531" url="http://en.wikipedia.org/wiki?curid=7531" title="Clavichord">
Clavichord

The clavichord is a European stringed keyboard instrument known from the late Medieval, through the Renaissance, Baroque and Classical eras. Historically, it was mostly used as a practice instrument and as an aid to composition, not being loud enough for larger performances. The clavichord produces sound by striking brass or iron strings with small metal blades called tangents. Vibrations are transmitted through the bridge(s) to the soundboard. The name is derived from the Latin word "clavis", meaning "key" (associated with more common "clavus", meaning "nail, rod, etc.") and "chorda" (from Greek χορδή) meaning "string, especially of a musical instrument".
History and use.
The clavichord was invented in the early fourteenth century. In 1504, the German poem "" mentions the terms clavicimbalum (a term used mainly for the harpsichord) and clavichordium, designating them as the best instruments to accompany melodies.
One of the earliest references to the clavichord in England occurs in the privy-purse expenses of Elizabeth of York, queen of Henry VII, in an entry dated August 1502:
Item. The same day, Hugh Denys for money by him delivered to a stranger that gave the queen a payre of clavycordes. In crowns form his reward iiii libres.
The clavichord was very popular from the 16th century to the 18th century, but mainly flourished in German-speaking lands, Scandinavia, and the Iberian Peninsula in the latter part of this period. It had fallen out of use by 1850. In the late 1890s, Arnold Dolmetsch revived clavichord construction and Violet Gordon-Woodhouse, among others, helped to popularize the instrument. Although most of the instruments built before the 1730s were small (four octaves, four feet long), the latest instruments were built up to seven feet long with a six octave range.
Today clavichords are played primarily by Renaissance, Baroque, and Classical music enthusiasts. They attract many interested buyers, and are manufactured worldwide. There are now numerous clavichord societies around the world, and some 400 recordings of the instrument have been made in the past 70 years. Leading modern exponents of the instrument include Derek Adlam, Christopher Hogwood, Richard Troeger, and Miklos Spányi.
Modern music.
The clavichord has also gained attention in other genres of music, in the form of the Clavinet, which is essentially an electric clavichord that uses a magnetic pickup to produce a signal for amplification. Stevie Wonder uses a Clavinet in many of his songs, such as "Superstition" and "Higher Ground". A Clavinet played through an instrument amplifier with guitar effect pedals is often associated with funky, disco-infused 1970s rock.
Guy Sigsworth has played clavichord in a modern setting with Björk, notably on the studio recording of "All Is Full of Love". Björk also made extensive use of and even played the instrument herself on the song "My Juvenile" of her 2007 album "Volta".
Tori Amos uses the instrument on "Little Amsterdam" from the album "Boys for Pele" and on the song "Smokey Joe" from her 2007 album "American Doll Posse". Amos also featured her use of the Clavinet on her 2004 recording "Not David Bowie", released as part of her 2006 box set, "".
In 1976 Oscar Peterson played (with Joe Pass on acoustic guitar) songs from "Porgy And Bess" on the clavichord. Keith Jarrett also recorded an album entitled "Book of Ways" (1987) in which he plays a series of clavichord improvisations. The Beatles' "For No One" (1966) features Paul McCartney playing the clavichord. Rick Wakeman plays the clavichord in the track "The Battle" from the album "Journey to the Centre of the Earth".
Structure and action.
In the clavichord, strings run transversely from the hitchpin rail at the left-hand end to tuning pegs on the right. Towards the right end they pass over a curved wooden bridge. The action is simple, with the keys being levers with a small brass tangent, a small piece of metal similar in shape and size to the head of a flat-bladed screwdriver, at the far end. The strings, which are usually of brass, or else a combination of brass and iron, are usually arranged in pairs, like a lute or mandolin. When the key is pressed, the tangent strikes the strings above, causing them to sound in a similar fashion to the "hammering" technique on a guitar. Unlike in a piano action, the tangent does not rebound from the string; rather, it stays in contact with the string as long as the key is held, acting as both the nut and as the initiator of sound. The volume of the note can be changed by striking harder or softer, and the pitch can also be affected by varying the force of the tangent against the string (known as "Bebung"). When the key is released, the tangent loses contact with the string and the vibration of the string is silenced by strips of damping cloth.
The action of the clavichord is unique among all keyboard instruments in that one part of the action simultaneously initiates the sound vibration while at the same time defining the endpoint of the vibrating string, and thus its pitch. Because of this intimate contact between the player's hand and the production of sound, the clavichord has been referred to as the most intimate of keyboard instruments. Despite its many (serious) limitations, including extremely low volume, it has considerable expressive power, the player being able to control attack, duration, volume, and even provide certain subtle effects of swelling of tone and a type of vibrato unique to the clavichord.
Fretting.
Since the string vibrates from the bridge only as far as the tangent, multiple keys with multiple tangents can be assigned to the same string. This is called "fretting". Early clavichords frequently had many notes played on each string, even going so far as the keyed monochord — an instrument with only one string—though most clavichords were triple- or double-fretted. Since only one note can be played at a time on each string, the fretting pattern is generally chosen so that notes rarely heard together (such as C and C) share a string pair. The advantages of this system compared with unfretted instruments (see below) include relative ease of tuning (with around half as many strings to keep in tune), greater volume (though still not really enough for use in chamber music), and a clearer, more direct sound. Among the disadvantages: temperament could not be re-set without bending the tangents; and playing required a further refinement of touch, since notes sharing a single string played in quick succession had to be slightly separated to avoid a disagreeable deadening of the sound, potentially disturbing a legato line.
Some clavichords have been built with a single pair of strings for each note. The first known reference to one was by Johann Speth in 1693 and the earliest such extant signed and dated clavichord was built in 1716 by Johann Michael Heinitz. Such instruments are referred to as "unfretted" whereas instruments using the same strings for several notes are called "fretted". Among the advantages to unfretted instruments are flexibility in tuning (the temperament can be easily altered) and the ability to play any music exactly as written without concern for "bad" notes. Disadvantages include a smaller volume, even though many or most unfretted instruments tend to be significantly larger than fretted instruments; and "many" more strings to keep in tune. Unfretted instruments tend to have a sweeter, less incisive tone due to the greater load on the bridge resulting from the greater number of strings, though the large, late (early 19th century) Swedish clavichords tend to be the loudest of any of the historic clavichords.
Pedal clavichord.
While clavichords were typically single manual instruments, they could be stacked, one clavichord on top of another, to provide multiple keyboards. With the addition of a pedal clavichord, which included a pedal keyboard for the lower notes, a clavichord could be used to practice improvising and, only when printed music became easily available, to learn organ repertoire. Most often, the addition of a pedal keyboard only involved connecting the keys of the pedalboard to the lower notes on the manual clavichord using string so the lower notes on the manual instrument could be operated by the feet. In the era of pipe organs, which used man-powered bellows that required several people to operate, and of churches being heated during church services if at all, organists used pedal harpsichords and pedal clavichords as practice instruments (see also: pedal piano). There is speculation that some works written for organ may have been intended for pedal clavichord. An interesting case is made by that Bach's "Eight Little Preludes and Fugues", now thought spurious, may actually be authentic. The keyboard writing seems unsuited to organ, but Speerstra argues that they are idiomatic on the pedal clavichord. As Speerstra and also note, the compass of the keyboard parts of Bach's six organ trio sonatas BWV 525–530 rarely go below the tenor C, so could have been played on a single manual pedal clavichord, by moving the left hand down an octave, a customary practice in the 18th century.
Repertoire.
Much of the musical repertoire written for harpsichord and organ from the period circa 1400–1800 can be played on the clavichord; however, it does not have enough (unamplified) volume to participate in chamber music, with the possible exception of providing accompaniment to a soft baroque flute, recorder, or single singer. J. S. Bach's son Carl Philipp Emanuel Bach was a great proponent of the instrument, and most of his German contemporaries regarded it as a central keyboard instrument, for performing, teaching, composing and practicing. The fretting of a clavichord provides new problems for some repertoire, but scholarship suggests that these problems are not insurmountable in Bach's Well-Tempered Clavier (). Among recent clavichord recordings, those by Christopher Hogwood ("The Secret Bach", "The Secret Handel", and, most recently, "The Secret Mozart"), break new ground. In his liner notes, Hogwood points out that these composers would typically have played the clavichord in the privacy of their homes. The English composer Herbert Howells (1892–1983) wrote two significant collections of pieces for clavichord (Lambert's Clavichord & Howells' Clavichord).
References.
Notes
Sources

</doc>
<doc id="7532" url="http://en.wikipedia.org/wiki?curid=7532" title="Centrifugal force (rotating reference frame)">
Centrifugal force (rotating reference frame)

Centrifugal force (from Latin centrum "center" and fugere "to flee") can generally be any force directed outward relative to some origin. More particularly, in classical mechanics, the centrifugal force is an outward force which arises when describing the motion of objects in a rotating reference frame. Because a rotating frame is an example of a non-inertial reference frame, Newton's laws of motion do not accurately describe the dynamics within the rotating frame. However, a rotating frame can be treated as if it were an inertial frame so that Newton's laws can be used if so-called fictitious forces (also known as inertial or pseudo- forces) are included in the sum of external forces on an object. The centrifugal force is what is usually thought of as the cause for the outward movement like that of passengers in a vehicle turning a corner, of the weights in a centrifugal governor, and of particles in a centrifuge. From the standpoint of an observer in an inertial frame, the effects can be explained as results of inertia without invoking the centrifugal force. Centrifugal force should not be confused with centripetal force or the reactive centrifugal force, both of which are real forces independent of the frame of the observer.
Analysis of motion within rotating frames can be greatly simplified by the use of the fictitious forces. By starting with an inertial frame, where Newton's laws of motion hold, and keeping track of how the time derivatives of a position vector change when transforming to a rotating reference frame, the various fictitious forces and their forms can be identified. Rotating frames and fictitious forces can often reduce the description of motion in two dimensions to a simpler description in one dimension (corresponding to a co-rotating frame). In this approach, circular motion in an inertial frame, which only requires the presence of a centripetal force, becomes the balance between the real centripetal force and the frame-determined centrifugal force in the rotating frame where the object appears stationary. If a rotating frame is chosen so that just the angular position of an object is held fixed, more complicated motion, such as elliptical and open orbits, appears because the centripetal and centrifugal forces will not balance. The general approach however is not limited to these co-rotating frames, but can be equally applied to objects at motion in any rotating frame.
In classical Newtonian physics.
Although Newton's laws of motion hold exclusively in inertial frames, often it is far more convenient and more advantageous to describe the motion of objects within a rotating reference frame. Sometimes the calculations are simpler (an example is inertial circles), and sometimes the intuitive picture coincides more closely with the rotational frame (an example is sedimentation in a centrifuge). By treating the extra acceleration terms due to the rotation of the frame as if they were forces, subtracting them from the physical forces, it's possible to treat the second time derivative of position (relative to the rotating frame) as absolute acceleration. Thus the analysis using Newton's laws of motion can proceed as if the reference frame was inertial, provided the fictitious force terms are included in the sum of external forces. For example, centrifugal force is used in the FAA pilot's manual in describing turns. Other examples are such systems as planets, centrifuges, carousels, turning cars, spinning buckets, and rotating space stations. 
If objects are seen as moving within a rotating frame, this movement results in another fictitious force, the Coriolis force; and if the rate of rotation of the frame is changing, a third fictitious force, the Euler force is experienced. Together, these three fictitious forces allow for the creation of correct equations of motion in a rotating reference frame.
Derivation.
For the following formalism, the rotating frame of reference is regarded as a special case of a non-inertial reference frame that is rotating relative to an inertial reference frame denoted the stationary frame.
Velocity.
In a rotating frame of reference, the time derivatives of the position vector , such as velocity and acceleration vectors, of an object will differ from the time derivatives in the stationary frame according to the frame's rotation. The first time derivative evaluated within a reference frame with a coincident origin at formula_1 but rotating with the absolute angular velocity is:
formula_2
where formula_3 denotes the vector cross product and square brackets denote evaluation in the rotating frame of reference. In other words, the apparent velocity in the rotating frame is altered by the amount of the apparent rotation formula_4 at each point, which is perpendicular to both the vector from the origin and the axis of rotation and directly proportional in magnitude to each of them. The vector has magnitude equal to the rate of rotation and is directed along the axis of rotation according to the right-hand rule.
Acceleration.
Newton's law of motion for a particle of mass "m" written in vector form is:
where is the vector sum of the physical forces applied to the particle and is the absolute acceleration (that is, acceleration in an inertial frame) of the particle, given by:
where is the position vector of the particle.
By twice applying the transformation above from the stationary to the rotating frame, the absolute acceleration of the particle can be written as:
Force.
The apparent acceleration in the rotating frame is [d2r/dt2]. An observer unaware of the rotation would expect this to be zero in the absence of outside forces. However Newton's laws of motion apply only in the stationary frame and describe dynamics in terms of the absolute acceleration d2"r"/dt2. Therefore the observer perceives the extra terms as contributions due to fictitious forces. These terms in the apparent acceleration are independent of mass; so it appears that each of these fictitious forces, like gravity, pulls on an object in proportion to its mass. When these forces are added, the equation of motion has the form:
From the perspective of the rotating frame, the additional force terms are experienced just like the real external forces and contribute to the apparent acceleration. The additional terms on the force side of the equation can be recognized as, reading from left to right, the Euler force formula_10, the Coriolis force formula_11, and the centrifugal force formula_12, respectively. Unlike the other two fictitious forces, the centrifugal force always points radially outward from the axis of rotation of the rotating frame, with magnitude , and unlike the Coriolis force in particular, it is independent of the motion of the particle in the rotating frame. As expected, for a non-rotating inertial frame of reference formula_13 the centrifugal force and all other fictitious forces disappear.
Absolute rotation.
Three scenarios were suggested by Newton to answer the question of whether the absolute rotation of a local frame can be detected; that is, if an observer can decide whether an observed object is rotating or if the observer is rotating.
In these scenarios, the effects attributed to centrifugal force are only observed in the local frame (the frame in which the object is stationary) if the object is undergoing absolute rotation relative to an inertial frame. By contrast, in an inertial frame, the observed effects arise as a consequence of the inertia and the known forces without the need to introduce a centrifugal force. Based on this argument, the privileged frame, wherein the laws of physics take on the simplest form, is a stationary frame in which no fictitious forces need to be invoked.
Within this view of physics, any other phenomenon that is usually attributed to centrifugal force can be used to identify absolute rotation. For example, the oblateness of a sphere of freely flowing material is often explained in terms of centrifugal force. The oblate spheroid shape reflects, following Clairaut's theorem, the balance between containment by gravitational attraction and dispersal by centrifugal force. That the Earth is itself an oblate spheroid, bulging at the equator where the radial distance and hence the centrifugal force is larger, is taken as one of the evidences for its absolute rotation.
Examples.
Below several examples illustrate both the stationary and rotating frames of reference, and the role of centrifugal force and its relation to Coriolis force in rotating frameworks. For more examples see Fictitious force, rotating bucket and rotating spheres.
Dropping ball.
An example of straight-line motion as seen in a stationary frame is a ball that steadily drops at a constant rate parallel to the axis of rotation. From a stationary frame of reference it moves in a straight line, but from the rotating frame it moves in a helix. The projection of the helical motion in a rotating horizontal plane is shown at the right of the figure. Because the projected horizontal motion in the rotating frame is a circular motion, the ball's motion requires an inward centripetal force, provided in this case by a fictitious force that produces the apparent helical motion. This force is the sum of an outward centrifugal force and an inward Coriolis force. The Coriolis force overcompensates the centrifugal force by exactly the required amount to provide the necessary centripetal force to achieve circular motion.
Banked turn.
Riding a car around a curve, we take a personal view that we are at rest in the car, and should be undisturbed in our seats. Nonetheless, we feel sideways force applied to us from the seats and doors and a need to lean to one side. To explain the situation, we propose a centrifugal force that is acting upon us and must be combated. Interestingly, we find this discomfort is reduced when the curve is banked, tipping the car inward toward the center of the curve.
A different point of view is that of the highway designer. The designer views the car as executing curved motion and therefore requiring an inward centripetal force to impel the car around the turn. By banking the curve, the force exerted upon the car in a direction normal to the road surface has a horizontal component that provides this centripetal force. That means the car tires no longer need to apply a sideways force to the car, but only a force perpendicular to the road. By choosing the angle of bank to match the car's speed around the curve, the car seat transmits only a perpendicular force to the passengers, and the passengers no longer feel a need to lean nor feel a sideways push by the car seats or doors.
Earth.
A calculation for Earth at the equator (formula_14 seconds, formula_15 meters) shows that an object experiences a centrifugal force equal to approximately 1/289 of standard gravity. Because centrifugal force increases according to the square of formula_16, one would expect gravity to be cancelled for an object travelling 17 times faster than the Earth's rotation, and in fact satellites in low orbit at the equator complete 17 full orbits in one day.
Gravity diminishes according to the inverse square of distance, but centrifugal force increases in direct proportion to the distance. Thus a circular geosynchronous orbit has a radius of 42164 km; 42164/6378.1 = 6.61, the cube root of 289.
Planetary motion.
Centrifugal force arises in the analysis of orbital motion and, more generally, of motion in a central-force field: in the case of a two-body problem, it is easy to convert to an equivalent one-body problem with force directed to or from an origin, and motion in a plane, so we consider only that.
The symmetry of a central force lends itself to a description in polar coordinates. The dynamics of a mass, "m", expressed using Newton's second law of motion (F = "m"a), becomes in polar coordinates:
where formula_18 is the force accelerating the object and the "hat" variables are unit direction vectors (formula_19 points in the centrifugal or outward direction, and formula_20 is orthogonal to it).
In the case of a central force, relative to the origin of the polar coordinate system, formula_18 can be replaced by formula_22, meaning the entire force is the component in the radial direction. An inward force of gravity would therefore correspond to a negative-valued "F"("r").
The components of F = "m"a along the radial direction therefore reduce to
in which the term proportional to the square of the rate of rotation appears on the acceleration side as a "centripetal acceleration", that is, a negative acceleration term in the formula_19 direction. In the special case of a planet in "circular" orbit around its star, for example, where formula_25 is zero, the centripetal acceleration alone is the entire acceleration of the planet, curving its path toward the sun under the force of gravity, the negative "F"("r").
As pointed out by Taylor, for example, it is sometimes convenient to work in a "co-rotating frame", that is, one rotating with the object so that the angular rate of the frame, formula_16, equals the formula_27 of the object in the stationary frame. In such a frame, the observed formula_28 is zero and formula_25 alone is treated as the acceleration: so in the equation of motion, the formula_30 term is "reincarnated on the force side of the equation (with opposite signs, of course) as the centrifugal force "mω2r" in the radial equation": The "reincarnation" on the force side of the equation is necessary because, without this force term, observers in the rotating frame would find they could not predict the motion correctly. They would have an incorrect radial equation:
where the formula_30 term is known as the centrifugal force. The centrifugal force term in this equation is called a "fictitious force", "apparent force", or "pseudo force", as its value varies with the rate of rotation of the frame of reference. When the centrifugal force term is expressed in terms of parameters of the rotating frame, replacing formula_27 with formula_16, it can be seen that it is the same centrifugal force previously derived for rotating reference frames.
Because of the absence of a net force in the azimuthal direction, conservation of angular momentum allows the radial component of this equation to be expressed solely with respect to the radial coordinate, "r", and the angular momentum formula_35, yielding the radial equation (a "fictitious one-dimensional problem" with only an "r" dimension):
The formula_37 term is again the centrifugal force, a force component induced by the rotating frame of reference. The equations of motion for "r" that result from this equation for the rotating 2D frame are the same that would arise from a particle in a fictitious one-dimensional scenario under the influence of the force in the equation above. If "F"("r") represents gravity, it is a negative term proportional to 1/"r"2, so the net acceleration in "r" in the rotating frame depends on a difference of reciprocal square and reciprocal cube terms, which are in balance in a circular orbit but otherwise typically not. This equation of motion is similar to one originally proposed by Leibniz. Given "r", the rate of rotation is easy to infer from the constant angular momentum "L", so a 2D solution can be easily reconstructed from a 1D solution of this equation.
When the angular velocity of this co-rotating frame is not constant, that is, for non-circular orbits, other fictitious forces—the Coriolis force and the Euler force—will arise, but can be ignored since they will cancel each other, yielding a net zero acceleration transverse to the moving radial vector, as required by the starting assumption that the formula_38 vector co-rotates with the planet. In the special case of circular orbits, in order for the radial distance to remain constant the outward centrifugal force must cancel the inward force of gravity; for other orbit shapes, these forces will not cancel, so "r" will not be constant.
History.
Concepts of centripetal and centrifugal force played a key early role in establishing the set of inertial frames of reference and the significance of fictitious forces, even aiding in the development of general relativity in which gravity itself becomes a fictitious force.
Applications.
The operations of numerous common rotating mechanical systems are most easily conceptualized in terms of centrifugal force. For example:
Nevertheless, all of these systems can also be described without requiring the concept of centrifugal force, in terms of motions and forces in a stationary frame, at the cost of taking somewhat more care in the consideration of forces and motions within the system.

</doc>
<doc id="7534" url="http://en.wikipedia.org/wiki?curid=7534" title="Centripetal force">
Centripetal force

Centripetal force (from Latin "centrum" "center" and "petere" "to seek") is a force that makes a body follow a curved path: its direction is always orthogonal to the velocity of the body, toward the fixed point of the instantaneous center of curvature of the path. Centripetal force is generally the cause of circular motion. 
In simple terms, centripetal force is defined as a force which keeps a body moving with a uniform speed along a circular path and is directed along the radius towards the centre. The mathematical description was derived in 1659 by Dutch physicist Christiaan Huygens. 
Isaac Newton's description was: "A centripetal force is that by which bodies are drawn or impelled, or in any way tend, towards a point as to a centre."
Formula.
The magnitude of the centripetal force on an object of mass "m" moving at tangential speed "v" along a path with radius of curvature "r" is:
where formula_2 is the centripetal acceleration.
The direction of the force is toward the center of the circle in which the object is moving, or the osculating circle, the circle that best fits the local path of the object, if the path is not circular.
The speed in the formula is squared, so twice the speed needs four times the force. The inverse relationship with the radius of curvature shows that half the radial distance requires twice the force. This force is also sometimes written in terms of the angular velocity "ω" of the object about the center of the circle:
Expressed using the period for one revolution of the circle, "T", the equation becomes:
In particle accelerators, velocity can be very high (close to the speed of light in vacuum) so the same rest mass now exerts greater inertia (relativistic mass) thereby requiring greater force for the same centripetal acceleration, so the equation becomes:
Where the Lorentz factor is defined as:
Sources of centripetal force.
For a satellite in orbit around a planet, the centripetal force is supplied by gravity. Some sources, including Newton, refer to the entire force as a centripetal force, even for eccentric orbits, for which gravity is not aligned with the direction to the center of curvature.
The centripetal force acts from the center of mass of the rotating object, on an object a distance "r" from its center; If both objects are rotating they will affect each other; for circular orbits, the center of mass is the center of the circular orbits. For non-circular orbits or trajectories, only the component of force directed orthogonally to the path (toward the center of the osculating circle) is termed centripetal; the remaining component acts to speed up or slow down the satellite in its orbit. 
For an object swinging around on the end of a rope in a horizontal plane, the centripetal force on the object is supplied by the tension of the rope. For a spinning object, internal tensile stress provides the centripetal forces that make the parts of the object trace out circular motions.
The rope example is an example involving a 'pull' force. The centripetal force can also be supplied as a 'push' force such as in the case where the normal reaction of a wall supplies the centripetal force for a wall of death rider.
Another example of centripetal force arises in the helix which is traced out when a charged particle moves in a uniform magnetic field in the absence of other external forces. In this case, the magnetic force is the centripetal force which acts towards the helix axis.
Analysis of several cases.
Below are three examples of increasing complexity, with derivations of the formulas governing velocity and acceleration.
Uniform circular motion.
Uniform circular motion refers to the case of constant rate of rotation. Here are two approaches to describing this case.
Calculus derivation.
In two dimensions the position vector formula_7 which has magnitude (length) formula_8 and directed at an angle formula_9 above the x-axis can be expressed in Cartesian coordinates using the unit vectors formula_10 and formula_11:
Assume uniform circular motion, which requires three things.
Now find the velocity formula_17 and acceleration formula_18 of the motion by taking derivatives of position with respect to time.
Notice that the term in parenthesis is the original expression of formula_7 in Cartesian coordinates. Consequently,
The negative shows that the acceleration is pointed towards the center of the circle (opposite the radius), hence it is called "centripetal" (i.e. "center-seeking"). While objects naturally follow a straight path (due to inertia), this centripetal acceleration describes the circular motion path caused by a centripetal force.
Derivation using vectors.
The image at right shows the vector relationships for uniform circular motion. The rotation itself is represented by the angular velocity vector Ω, which is normal to the plane of the orbit (using the right-hand rule) and has magnitude given by:
with "θ" the angular position at time "t". In this subsection, d"θ"/d"t" is assumed constant, independent of time. The distance traveled dℓ of the particle in time d"t" along the circular path is 
which, by properties of the vector cross product, has magnitude "r"d"θ" and is in the direction tangent to the circular path. 
Consequently,
Differentiating with respect to time,
Lagrange's formula states:
Applying Lagrange's formula with the observation that Ω • r("t") = 0 at all times,
In words, the acceleration is pointing directly opposite to the radial displacement r at all times, and has a magnitude:
where vertical bars |...| denote the vector magnitude, which in the case of r("t") is simply the radius "r" of the path. This result agrees with the previous section, though the notation is slightly different.
When the rate of rotation is made constant in the analysis of nonuniform circular motion, that analysis agrees with this one.
A merit of the vector approach is that it is manifestly independent of any coordinate system.
Example: The banked turn.
The upper panel in the image at right shows a ball in circular motion on a banked curve. The curve is banked at an angle "θ" from the horizontal, and the surface of the road is considered to be slippery. The objective is to find what angle the bank must have so the ball does not slide off the road. Intuition tells us that on a flat curve with no banking at all, the ball will simply slide off the road; while with a very steep banking, the ball will slide to the center unless it travels the curve rapidly.
Apart from any acceleration that might occur in the direction of the path, the lower panel of the image above indicates the forces on the ball. There are "two" forces; one is the force of gravity vertically downward through the center of mass of the ball "mg where "m" is the mass of the ball and g is the gravitational acceleration; the second is the upward normal force exerted by the road perpendicular to the road surface "man. The centripetal force demanded by the curved motion also is shown above. This centripetal force is not a third force applied to the ball, but rather must be provided by the net force on the ball resulting from vector addition of the normal force and the force of gravity. The resultant or net force on the ball found by vector addition of the normal force exerted by the road and vertical force due to gravity must equal the centripetal force dictated by the need to travel a circular path. The curved motion is maintained so long as this net force provides the centripetal force requisite to the motion.
The horizontal net force on the ball is the horizontal component of the force from the road, which has magnitude |Fh| = "m"|an|sin"θ". The vertical component of the force from the road must counteract the gravitational force: |Fv| = "m"|an|cos"θ" = "m"|g|, which implies |an|=|g| / cos"θ". Substituting into the above formula for |Fh| yields a horizontal force to be:
On the other hand, at velocity |v| on a circular path of radius "r", kinematics says that the force needed to turn the ball continuously into the turn is the radially inward centripetal force Fc of magnitude:
Consequently the ball is in a stable path when the angle of the road is set to satisfy the condition:
or,
As the angle of bank "θ" approaches 90°, the tangent function approaches infinity, allowing larger values for |v|2/"r". In words, this equation states that for faster speeds (bigger |v|) the road must be banked more steeply (a larger value for "θ"), and for sharper turns (smaller "r") the road also must be banked more steeply, which accords with intuition. When the angle "θ" does not satisfy the above condition, the horizontal component of force exerted by the road does not provide the correct centripetal force, and an additional frictional force tangential to the road surface is called upon to provide the difference. If friction cannot do this (that is, the coefficient of friction is exceeded), the ball slides to a different radius where the balance can be realized.
These ideas apply to air flight as well. See the FAA pilot's manual.
Nonuniform circular motion.
As a generalization of the uniform circular motion case, suppose the angular rate of rotation is not constant. The acceleration now has a tangential component, as shown the image at right. This case is used to demonstrate a derivation strategy based upon a polar coordinate system.
Let r("t") be a vector that describes the position of a point mass as a function of time. Since we are assuming circular motion, let r("t") = "R"·ur, where "R" is a constant (the radius of the circle) and ur is the unit vector pointing from the origin to the point mass. The direction of ur is described by "θ", the angle between the x-axis and the unit vector, measured counterclockwise from the x-axis. The other unit vector for polar coordinates, uθ is perpendicular to ur and points in the direction of increasing "θ". These polar unit vectors can be expressed in terms of Cartesian unit vectors in the "x" and "y" directions, denoted i and j respectively:
and
We differentiate to find velocity:
where "ω" is the angular velocity d"θ"/d"t".
This result for the velocity matches expectations that the velocity should be directed tangential to the circle, and that the magnitude of the velocity should be "rω". Differentiating again, and noting that
we find that the acceleration, a is:
Thus, the radial and tangential components of the acceleration are:
where |v| = "r" ω is the magnitude of the velocity (the speed).
These equations express mathematically that, in the case of an object that moves along a circular path with a changing speed, the acceleration of the body may be decomposed into a perpendicular component that changes the direction of motion (the centripetal acceleration), and a parallel, or tangential component, that changes the speed.
General planar motion.
Polar coordinates.
The above results can be derived perhaps more simply in polar coordinates, and at the same time extended to general motion within a plane, as shown next. Polar coordinates in the plane employ a radial unit vector uρ and an angular unit vector uθ, as shown above. A particle at position r is described by:
where the notation "ρ" is used to describe the distance of the path from the origin instead of "R" to emphasize that this distance is not fixed, but varies with time. The unit vector uρ travels with the particle and always points in the same direction as r("t"). Unit vector uθ also travels with the particle and stays orthogonal to uρ. Thus, uρ and uθ form a local Cartesian coordinate system attached to the particle, and tied to the path traveled by the particle. By moving the unit vectors so their tails coincide, as seen in the circle at the left of the image above, it is seen that uρ and uθ form a right-angled pair with tips on the unit circle that trace back and forth on the perimeter of this circle with the same angle "θ"("t") as r("t").
When the particle moves, its velocity is
To evaluate the velocity, the derivative of the unit vector uρ is needed. Because uρ is a unit vector, its magnitude is fixed, and it can change only in direction, that is, its change duρ has a component only perpendicular to uρ. When the trajectory r("t") rotates an amount d"θ", uρ, which points in the same direction as r("t"), also rotates by d"θ". See image above. Therefore the change in uρ is 
or
In a similar fashion, the rate of change of uθ is found. As with uρ, uθ is a unit vector and can only rotate without changing size. To remain orthogonal to uρ while the trajectory r("t") rotates an amount d"θ", uθ, which is orthogonal to r("t"), also rotates by d"θ". See image above. Therefore, the change duθ is orthogonal to uθ and proportional to d"θ" (see image above):
The image above shows the sign to be negative: to maintain orthogonality, if duρ is positive with d"θ", then duθ must decrease. 
Substituting the derivative of uρ into the expression for velocity:
To obtain the acceleration, another time differentiation is done:
Substituting the derivatives of uρ and uθ, the acceleration of the particle is:
As a particular example, if the particle moves in a circle of constant radius "R", then d"ρ"/d"t" = 0, v = vθ, and:
where formula_56
These results agree with those above for nonuniform circular motion. See also the article on non-uniform circular motion. If this acceleration is multiplied by the particle mass, the leading term is the centripetal force and the negative of the second term related to angular acceleration is sometimes called the Euler force.
For trajectories other than circular motion, for example, the more general trajectory envisioned in the image above, the instantaneous center of rotation and radius of curvature of the trajectory are related only indirectly to the coordinate system defined by uρ and uθ and to the length |r("t")| = "ρ". Consequently, in the general case, it is not straightforward to disentangle the centripetal and Euler terms from the above general acceleration equation.
 To deal directly with this issue, local coordinates are preferable, as discussed next.
Local coordinates.
By local coordinates is meant a set of coordinates that travel with the particle,
and have orientation determined by the path of the particle. Unit vectors are formed as shown in the image at right, both tangential and normal to the path. This coordinate system sometimes is referred to as "intrinsic" or "path coordinates" or "nt-coordinates", for "normal-tangential", referring to these unit vectors. These coordinates are a very special example of a more general concept of local coordinates from the theory of differential forms. 
Distance along the path of the particle is the arc length "s", considered to be a known function of time.
A center of curvature is defined at each position "s" located a distance "ρ" (the radius of curvature) from the curve on a line along the normal un ("s"). The required distance "ρ"("s") at arc length "s" is defined in terms of the rate of rotation of the tangent to the curve, which in turn is determined by the path itself. If the orientation of the tangent relative to some starting position is "θ"("s"), then "ρ"("s") is defined by the derivative d"θ"/d"s":
The radius of curvature usually is taken as positive (that is, as an absolute value), while the "curvature" "κ" is a signed quantity. 
A geometric approach to finding the center of curvature and the radius of curvature uses a limiting process leading to the osculating circle. See image above.
Using these coordinates, the motion along the path is viewed as a succession of circular paths of ever-changing center, and at each position "s" constitutes non-uniform circular motion at that position with radius "ρ". The local value of the angular rate of rotation then is given by:
with the local speed "v" given by:
As for the other examples above, because unit vectors cannot change magnitude, their rate of change is always perpendicular to their direction (see the left-hand insert in the image above):
Consequently, the velocity and acceleration are:
and using the chain-rule of differentiation:
In this local coordinate system the acceleration resembles the expression for nonuniform circular motion with the local radius "ρ"("s"), and the centripetal acceleration is identified as the second term.
Extension of this approach to three dimensional space curves leads to the Frenet–Serret formulas.
Alternative approach.
Looking at the image above, one might wonder whether adequate account has been taken of the difference in curvature between "ρ"("s") and "ρ"("s" + d"s") in computing the arc length as d"s" = "ρ"("s")d"θ". Reassurance on this point can be found using a more formal approach outlined below. This approach also makes connection with the article on curvature. 
To introduce the unit vectors of the local coordinate system, one approach is to begin in Cartesian coordinates and describe the local coordinates in terms of these Cartesian coordinates. In terms of arc length "s" let the path be described as:
Then an incremental displacement along the path d"s" is described by:
where primes are introduced to denote derivatives with respect to "s". The magnitude of this displacement is d"s", showing that:
This displacement is necessarily tangent to the curve at "s", showing that the unit vector tangent to the curve is:
while the outward unit vector normal to the curve is 
Orthogonality can be verified by showing that the vector dot product is zero. The unit magnitude of these vectors is a consequence of Eq. 1. Using the tangent vector, the angle "θ" of the tangent to the curve is given by:
The radius of curvature is introduced completely formally (without need for geometric interpretation) as:
The derivative of "θ" can be found from that for sin"θ":
Now:
in which the denominator is unity. With this formula for the derivative of the sine, the radius of curvature becomes:
where the equivalence of the forms stems from differentiation of Eq. 1:
With these results, the acceleration can be found:
as can be verified by taking the dot product with the unit vectors ut("s") and un("s"). This result for acceleration is the same as that for circular motion based on the radius "ρ". Using this coordinate system in the inertial frame, it is easy to identify the force normal to the trajectory as the centripetal force and that parallel to the trajectory as the tangential force. From a qualitative standpoint, the path can be approximated by an arc of a circle for a limited time, and for the limited time a particular radius of curvature applies, the centrifugal and Euler forces can be analyzed on the basis of circular motion with that radius. 
This result for acceleration agrees with that found earlier. However, in this approach the question of the change in radius of curvature with "s" is handled completely formally, consistent with a geometric interpretation, but not relying upon it, thereby avoiding any questions the image above might suggest about neglecting the variation in "ρ".
Example: circular motion.
To illustrate the above formulas, let "x", "y" be given as:
Then:
which can be recognized as a circular path around the origin with radius "α". The position "s" = 0 corresponds to ["α", 0], or 3 o'clock. To use the above formalism the derivatives are needed:
With these results one can verify that:
The unit vectors also can be found:
which serve to show that "s" = 0 is located at position ["ρ", 0] and "s" = "ρ"π/2 at [0, "ρ"], which agrees with the original expressions for "x" and "y". In other words, "s" is measured counterclockwise around the circle from 3 o'clock. Also, the derivatives of these vectors can be found:
To obtain velocity and acceleration, a time-dependence for "s" is necessary. For counterclockwise motion at variable speed "v"("t"):
where "v"("t") is the speed and "t" is time, and "s"("t" = 0) = 0. Then:
where it already is established that α = ρ. This acceleration is the standard result for non-uniform circular motion.

</doc>
<doc id="7535" url="http://en.wikipedia.org/wiki?curid=7535" title="Commodore">
Commodore

Commodore generally refers to Commodore (rank), a naval rank. It may also refer to:

</doc>
<doc id="7536" url="http://en.wikipedia.org/wiki?curid=7536" title="Conditioning">
Conditioning

Conditioning may refer to:

</doc>
<doc id="7538" url="http://en.wikipedia.org/wiki?curid=7538" title="Checksum">
Checksum

A checksum or hash sum is a small-size datum from an arbitrary block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage. It is usually applied to an installation file after it is received from the download server. By themselves checksums are often used to verify data integrity, but should not be relied upon to also verify data authenticity. 
The actual procedure which yields the checksum, given a data input is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. 
This is especially true of cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted.
Checksum functions are related to hash functions, fingerprints, randomization functions, and cryptographic hash functions. However, each of those concepts has different applications and therefore different design goals. Checksums are used as cryptographic primitives in larger authentication algorithms. For cryptographic systems with these two specific design goals, see HMAC.
Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers, bank account numbers, computer words, single bytes, etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.
Checksum algorithms.
Parity byte or parity word.
The simplest checksum algorithm is the so-called longitudinal parity check, which breaks the data into "words" with a fixed number "n" of bits, and then computes the exclusive or of all those words. The result is appended to the message as an extra word. To check the integrity of a message, the receiver computes the exclusive or (XOR) of all its words, including the checksum; if the result is not a word with "n" zeros, the receiver knows a transmission error occurred.
With this checksum, any transmission error which flips a single bit of the message, or an odd number of bits, will be detected as an incorrect checksum. However, an error which affects two bits will not be detected if those bits lie at the same position in two distinct words. If the affected bits are independently chosen at random, the probability of a two-bit error being undetected is 1/"n".
Modular sum.
A variant of the previous algorithm is to add all the "words" as unsigned binary numbers, discarding any overflow bits, and append the two's complement of the total as the checksum. To validate a message, the receiver adds all the words in the same manner, including the checksum; if the result is not a word full of zeros, an error must have occurred. This variant too detects any single-bit error, but the promodular sum is used in SAE J1708.
Position-dependent checksums.
The simple checksums described above fail to detect some common errors which affect many bits at once, such as changing the order of data words, or inserting or deleting words with all bits set to zero. The checksum algorithms most used in practice, such as Fletcher's checksum, Adler-32, and cyclic redundancy checks (CRCs), address these weaknesses by considering not only the value of each word but also its position in the sequence. This feature generally increases the cost of computing the checksum.
General considerations.
A single-bit transmission error then corresponds to a displacement from a valid corner (the correct message and checksum) to one of the "m" adjacent corners. An error which affects "k" bits moves the message to a corner which is "k" steps removed from its correct corner. The goal of a good checksum algorithm is to spread the valid corners as far from each other as possible, so as to increase the likelihood "typical" transmission errors will end up in an invalid corner.
See also.
General topic
Error correction
Hash functions

</doc>
<doc id="7541" url="http://en.wikipedia.org/wiki?curid=7541" title="City University of New York">
City University of New York

The City University of New York (CUNY; pron.: ) is the public university system of New York City. It is the largest urban university in the United States, consisting of 24 institutions: 11 senior colleges, seven community colleges, the William E. Macaulay Honors College at CUNY, the CUNY Baccalaureate for Unique and Interdisciplinary Studies Program at The Graduate Center, the doctorate-granting Graduate School and University Center, the City University of New York School of Law, CUNY Graduate School of Journalism, the and the Sophie Davis School of Biomedical Education. More than 270,000 degree-credit students and 273,000 continuing and professional education students are enrolled at campuses located in all five New York City boroughs. Its administrative offices are in mid-town Manhattan.
The university has one of the most diverse student bodies in the United States, with students hailing from 208 countries. The black, white and Hispanic undergraduate populations each comprise more than a quarter of the student body, and Asian undergraduates make up 18 percent. Fifty-eight percent are female, and 28 percent are 25 or older. CUNY graduates include 12 Nobel laureates, a U.S. Secretary of State, a Supreme Court Justice, several New York City mayors, members of Congress, state legislators, scientists and artists.
CUNY is the third-largest university system in the United States, in terms of enrollment, behind the State University of New York (SUNY), and the California State University system. CUNY and SUNY are separate and independent university systems, although both are public institutions that receive funding from New York State. CUNY, however, is additionally funded by the City of New York.
History.
Founding.
CUNY's history dates back to the formation of the Free Academy in 1847 by Townsend Harris. The school was fashioned as "a Free Academy for the purpose of extending the benefits of education gratuitously to persons who have been pupils in the common schools of the city and county of New York." The Free Academy later became the City College of New York, the oldest institution among the CUNY colleges. Hunter College – so-named in 1914, originally Female Normal and High School and later the Normal College – had existed since 1870, and later expanded into the Bronx in the early 20th century with what became Herbert Lehman College, but CCNY and Hunter resisted merging.
In 1926, in response to the growth in population of the city, the New York State legislature created the Board of Higher Education of the City of New York to integrate, coordinate and expand the institutions of higher education in the city. During the period the Board existed, Brooklyn College (1930), Queens College (1937) and a number of other four-year colleges and two-year community colleges were created.
In 1961, Governor Nelson Rockefeller signed the bill that formally created the City University of New York to integrate these institutions, and a new graduate school, together into a coordinated system of higher education for the city, and by 1979, the Board of Higher Education had become the Board of Trustees of the CUNY. Eventually, the system grew to include seven senior colleges, four hybrid schools, seven community colleges, as well as graduate schools and professional programs.
Accessible education.
CUNY has historically served a diverse student body, especially those excluded from or unable to afford private universities. Its four-year colleges offered a high quality, tuition-free education to the poor, the working class and the immigrants of New York City who met the grade requirements for matriculated status. Many Jewish academics and intellectuals studied and taught at CUNY in the post-World War I era when some Ivy League universities, such as Yale University, discriminated against Jews. The City College of New York has had a reputation of being "the Harvard of the proletariat." Over its history, CUNY colleges, particularly CCNY, have been involved in various political movements.
As the city's population—and public college enrollment—grew during the early 20th century and the city struggled for resources, the municipal colleges slowly began adopting selective tuition, also known as instructional fees, for a handful of courses and programs. During the Great Depression, with funding for the public colleges severely constrained, limits were imposed on the size of the colleges' free Day Session, and tuition was imposed upon students deemed "competent" but not academically qualified for the day program. Most of these "limited matriculation" students enrolled in the Evening Session, and paid tuition.
Over time, tuition for limited-matriculated students became an important source of system revenues. In fall 1957, for example, nearly 36,000 attended Hunter, Brooklyn, Queens and City Colleges for free, but another 24,000 paid tuition of up to $300 a year — the equivalent of $2,411.98 in 2011. Undergraduate tuition and other student fees in 1957 comprised 17 percent of the colleges' $46.8 million in revenues, about $7.74 million — a figure equivalent to $62.4 million in 2011 buying power.
Demand in the United States for higher education rapidly grew after World War II, and during the mid-1940s a movement began to create community colleges to provide accessible education and training. In New York City, however, the community-college movement was constrained by many factors including "financial problems, narrow perceptions of responsibility, organizational weaknesses, adverse political factors, and another competing priorities."
Community colleges would have drawn from the same city coffers that were funding the senior colleges, and city higher education officials were of the view that the state should finance them. It wasn’t until 1955, under a shared-funding arrangement with New York State, that New York City established its first community college, on Staten Island. Unlike the day college students attending the city’s public baccalaureate colleges for free, the community college students had to pay tuition under the state-city funding formula. Community college students paid tuition for approximately 10 years. In 1964, as the city’s Board of Higher Education moved to take full responsibility for the community colleges, city officials extended the senior colleges’ free tuition policy to them, a change that was included by Mayor Robert Wagner in his budget plans and took effect with the 1964-65 academic year.
In the decades following World War II, a surging demand for limited college slots had the effect in New York City of increasing the competitiveness of the higher education system. In 1969, a group of Black and Puerto Rican students occupied City College demanding the integration of CUNY, which at the time had an overwhelmingly white student body.
Three community colleges had been established by early 1961, when the city’s public colleges were codified by the state as an integrated University with a chancellor at the helm and an infusion of state funds. But the city’s slowness in creating the community colleges as demand for college seats was intensifying, had resulted in mounting frustration, particularly on the part of minorities, that college opportunities were not available to them.
Student protests.
Students at some campuses became increasingly frustrated with the University's and Board of Higher Education's handling of university administration. At Baruch College in 1967, over a thousand students protested the plan to make the college an upper-division school limited to junior, senior, and graduate students. At Brooklyn College in 1968, students attempted a sit-in to demand the admission of more black and Puerto-Rican students and additional black studies curriculum. Students at Hunter College also demanded a black studies program. Members of the SEEK program, which provided academic support for underprepared and underprivileged students, staged a building takeover at Queens College in 1969 to protest the decisions of the program's director, who would later be replaced by a black professor. Puerto Rican students at Bronx Community College filed a report with the State Division of Human Rights in 1970, contending that the intellectual level of the college was inferior and discriminatory. Hunter College was crippled for several days by a protest of 2,000 students who had a list of demands focusing on more student representation in college administration. Across CUNY, students boycotted their campuses in 1970 to protest a rise in student fees and other issues, including the proposed (and later implemented) open admissions plan.
Like many college campuses in 1970, CUNY also saw a number of protests and demonstrations after the Kent State shootings and Cambodian Campaign. The Administrative Council of the City University of New York sent President Nixon a telegram in 1970 stating, "No nation can long endure the alienation of the best of its young people." Some colleges, including John Jay College of Criminal Justice, historically the "college for cops," held teach-ins in addition to student and faculty protests.
Open Admissions.
In 1970, the Board of Trustees implemented a new admissions policy. The doors to CUNY were opened wide to all those demanding entrance, assuring all high school graduates entrance to the University without having to fulfill traditional requirements such as exams or grades. This policy was known as "open admissions". Remedial education, to supplement the training of under-prepared students, became a significant part of CUNY's offerings.
The effect was instantaneous. Whereas 20,000 freshmen had matriculated at CUNY in 1969, 35,000 showed up for registration in the fall of 1970. Forty percent of these newcomers to the senior colleges were open-admissions students. The proportion of Black and Hispanic students in the entering class nearly tripled.
Financial crisis of 1976.
In fall 1976, during New York City's fiscal crisis, the free tuition policy was discontinued under pressure from the federal government, the financial community that had a role in rescuing the city from bankruptcy, and New York State, which would take over the funding of CUNY's senior colleges. Tuition, which had been in place in the State University of New York system since 1963, was instituted at all CUNY colleges.
Meanwhile, CUNY students were added to the state's need-based Tuition Assistance Program, or TAP, which had originally been created to help private colleges. Full-time students who met the income eligibility criteria were permitted to receive TAP, ensuring for the first time that financial hardship would deprive no CUNY student of a college education. Within a few years, the federal government would create its own need-based program, known as Pell Grants, providing the neediest students with a tuition-free college education. By 2011, nearly six of ten full- time undergraduates qualified for a tuition-free education at CUNY due in large measure to state, federal and CUNY financial aid programs. CUNY's enrollment dipped after tuition was re-established, and there were further enrollment declines through the 1980s and into the 1990s.
Financial crisis of 1995.
In 1995, CUNY suffered another fiscal crisis when Governor George Pataki announced a $162 million cut in state financing. Faculty cancelled classes and students staged protests. By May, CUNY adopted deep cuts to college budgets and class offerings. By June, CUNY had adopted a stricter admissions policy for its senior colleges: students deemed unprepared for college would not be admitted, a departure from the 1970 Open Admissions program, in order to save money spent on remedial programs. The proposed $160 million in cuts was reduced to $102 million, which CUNY absorbed by increasing tuition by $750 and offering a retirement incentive plan for faculty.
In 1999, a task force appointed by Mayor Rudolph Giuliani issued a report that described CUNY as “an institution adrift” and called for an improved, more cohesive University structure and management, as well as more consistent academic standards. Following the report, Matthew Goldstein, a mathematician and City College graduate who had led CUNY’s Baruch College and briefly, Adelphi University, was appointed chancellor of CUNY. After his appointment in 1999, CUNY ended its policy of open admissions to its four-year colleges. Admissions standards were raised at CUNY’s most selective four-year colleges (Baruch, Brooklyn, City, Hunter and Queens) and a new policy was established that required entering college students who needed remediation, to begin their studies at the University’s open-admissions community colleges.
Continued growth and improvement.
Over the next decade, CUNY’s enrollment began to climb steeply, with the number of degree-credit students reaching 220,727 in 2005 and 262,321 in 2010 as the university broadened its academic offerings and attracted students seeking value during the nationwide economic recession. Over the next decade, as CUNY’s enrollment steadily increased, the University added more than 2,000 full-time faculty positions. During Goldstein’s tenure the university met the increasing demand by opening new schools and programs while expanding the University’s fundraising efforts to help pay for them. The results of these efforts rose from $35 million in 2000 to more than $200 million per year as of 2012.
In 2005, Goldstein proposed an innovative funding model for CUNY, called The CUNY Compact for Public Higher Education, which delineated the shared responsibilities of the government, philanthropists, University’s administration and students in funding the university’s programs. The Compact model has been seen by CUNY and New York State officials as a success at stabilizing the university’s finances during difficult and unpredictable economic times, and in providing for predictable tuition increases for which families can plan. In June 2011, New York Gov. Andrew Cuomo and the state Legislature signed into law authorization of elements of the Compact model, which has also been adopted by the State University of New York.
The highly selective Macaulay Honors College, a Goldstein brainchild established in 2005, and other college honors programs later opened at CUNY, have attracted some of the city public schools' most academically accomplished graduates. Under Goldstein, CUNY Graduate School of Journalism (2006), CUNY School of Professional Studies (2006), CUNY School of Public Health (2008), and the New Community College at CUNY (2012) also were founded. The New Community College was renamed the Stella and Charles Guttman Community College in 2013 after a $25 million bequest to CUNY community college programs from the Stella and Charles Guttman Foundation.
In 2005, Goldstein launched CUNY’s “Decade of Science”, an initiative focused on expanding high-quality education, training and research, and to attract top researchers, in the STEM fields (science, technology, engineering and mathematics). The Advanced Science Research Center (ASRC), a CUNY research hub located on the campus of City College, is scheduled to open in 2014 and will specialize in nanotechnology, structural biology, photonics, neuroscience and environmental sciences. The project is a key project of a $2.7 billion investment in a capital construction program to upgrade, build and maintain CUNY campus buildings throughout the city’s five boroughs.
Goldstein also directed CUNY administration to reform CUNY’s general education requirements and policies. Called Pathways to Degree Completion, the initiative, to take effect for all CUNY undergraduates in fall 2013, requires all students to take an administration-dictated common core of courses which have been claimed to meet specific “learning outcomes” or standards. Since the courses are accepted University wide, the administration claims the Pathways reform makes it easier for students to transfer course credits between CUNY colleges. It also reduces the number of core courses some CUNY colleges had required, to a level below national norms, particularly in the sciences. The program is the target of several lawsuits by both students and faculty, and was the subject of a "no confidence" vote by the faculty, who rejected it by an overwhelming 92% margin.
Goldstein, CUNY’s longest-running chancellor, announced in April 2013 that he would step down on July 1, 2013, after nearly 14 years. News articles and editorials on the decision credited the 71-year-old Goldstein with transforming CUNY’s academic offerings and reputation, and the range of its student body, through his focus on high standards and effective management.
William P. Kelly, president of The Graduate Center of the City University of New York, a scholar of literature and a longtime CUNY administrator, was appointed interim chancellor of the university effective July 1, pending a national search for a new chancellor.
On January 15, 2014 the university’s Board of Trustees voted unanimously to appoint James Milliken, president of the University of Nebraska, and a graduate of University of Nebraska and New York University Law School, as CUNY’s seventh chancellor effective June 1, 2014.
Structure.
The forerunner of today's City University of New York was governed by the Board of Education of New York City. Members of the Board of Education, chaired by the President of the board, served as "ex officio" trustees. For the next four decades, the board members continued to serve as "ex officio" trustees of the College of the City of New York and the city's other municipal college, the Normal College of the City of New York.
In 1900, the New York State Legislature created separate boards of trustees for the College of the City of New York and the Normal College, which became Hunter College in 1914. In 1926, the Legislature established the Board of Higher Education of the City of New York, which assumed supervision of both municipal colleges.
In 1961, the New York State Legislature established the City University of New York, uniting what had become seven municipal colleges at the time: The City College of New York, Hunter College, Brooklyn College, Queens College, Staten Island Community College, Bronx Community College and Queensborough Community College. In 1979, the CUNY Financing and Governance Act was adopted by the State and the Board of Higher Education officially became The City University of New York Board of Trustees.
Today, the City University is governed by the Board of Trustees composed of 17 members, ten of whom are appointed by the Governor of New York "with the advice and consent of the senate," and five by the Mayor of New York City "with the advice and consent of the senate." The final two trustees are "ex officio" members. One is the chair of the university's student senate, and the other is non-voting and is the chair of the university's faculty senate. Both the mayoral and gubernatorial appointments to the CUNY Board are required to include at least one resident of each of New York City's five boroughs. Trustees serve seven-year terms, which are renewable for another seven years. The Chancellor is voted upon by the Board of Trustees, and is the "chief educational and administrative officer" of the City University.
Colleges.
CUNY consists of three different types of institutions: senior colleges, which grant bachelor's degrees and occasionally master's, associate, and doctoral degrees; community colleges, which grant associate's degrees; and graduate/professional schools. CUNY's Law School grants Juris Doctor (J.D.) degrees, and the CUNY Graduate Center awards only Ph.D. degrees.
The colleges are listed below, with establishment dates in parentheses.
CUNY Baccalaureate for Unique and Interdisciplinary Studies.
The CUNY Baccalaureate for Unique and Interdisciplinary Studies, also commonly known as the CUNY Baccalaureate Program or simply CUNY BA was founded in 1971. It is an individualized, University-wide degree where highly motivated, academically superior students work one-on-one with faculty mentors to design their own fields of study. The Program exists to give students an opportunity to pursue a course of study that may not exist within the current framework of CUNY. Part of the eligibility criteria includes demonstrating a desire and plan to pursue an area of concentration (like a major) that transcends the traditional college offerings. Students have created areas of concentration ranging from "20th Century American Literature" and "Adaptive Physical Education for Vulnerable Populations," to "World Politics and Social Change" and "Zoological Photography." Students must enroll in one of the CUNY colleges in order to participate; they then have access to courses and opportunities throughout the University. Additional admissions criteria include having completed at least 15 college credits with a 2.50 GPA or higher. The average GPA for admission is typically about 3.25, which means that a large portion of students enter with GPAs of 3.8 and higher. Given the rigorous admission process it is not surprising that CUNY BA boasts a 70% graduation rate within an average of 2.2 years and that 60% graduate with academic honors.
William E. Macaulay Honors College.
William E. Macaulay Honors College was to be an independent institution within the university. However, support for existing honors programs at CUNY colleges and institutional opposition resulted in it being downgraded to a program. Now known as The Macaulay Honors College University Scholars Program, it graduated its first class in 2005, attracting students with a mean high school GPA of 3.5 and SAT scores of 1365 for the Class of 2009.
In July 2006, Dr. Ann Kirschner was appointed Dean of William E. Macaulay Honors College after a nationwide search. The standards of the Honors College continued to rise as well, with incoming freshmen having an average of 93.8 and SAT scores of 1381. Graduating high school students with Ivy League caliber academic records have given the Honors College a closer look as a result, and this has had a trickle-down effect in improving the image of CUNY as a whole, which prior to the inception of the HC had been criticized as 'an institution adrift' by the Giuliani administration.
As an incentive to students, University Scholars receive a free tuition, a laptop, a "cultural passport" that offers free or reduced-admission to various cultural institutions and venues in New York City, and a $7500 expense account that may be used for research and/or study abroad. Unlike honors programs at individual CUNY colleges, Macaulay Honors College students must be accepted into and begin the program as freshmen. They currently study at one of the participating senior CUNY colleges (Queens, Hunter, Staten Island, Lehman, Baruch, Brooklyn, John Jay, and City), as well as taking part in cross-campus activities and programs. Institutional barriers that would allow cross campus enrollment in academic programs have not yet been eliminated.
In September 2006, The City University of New York received a $30,000,000 gift from philanthropist and City College alumnus William E. Macaulay, Chairman and Chief Executive Officer of First Reserve Corporation. It is the largest single donation in the history of CUNY and has been used to buy a landmark building on the Upper West Side of Manhattan that is to become the permanent home of the Honors College, and will add support to its endowment. Macaulay is now an accredited degree-granting institution, complete with its own College Council, having graduated its first class in 2011.
Public safety.
CUNY has its own public safety force whose duties are to protect and serve all students and faculty members, and enforce all state and city laws at all of CUNY's universities. The force has more than 600 officers, making it one of the largest public safety forces in New York City.
The Public Safety Department came under heavy criticism, from student groups, after several students protesting tuition increases tried to occupy the lobby of the Baruch College. The occupiers were forcibly removed from the area and several were arrested on November 21, 2011.
City University Television (CUNY TV).
CUNY also has a cable TV service, CUNY TV (channel 75 on Time Warner) which airs tapes of freshman level survey telecourses, old and foreign films, and panel discussions in various languages.
City University Film Festival (CUFF).
CUFF is CUNY's official film festival. The festival was founded in 2009 by Hunter College student Daniel Cowen.
Alumni.
The City University of New York boasts alumni, whose professions range from politics to medicine.
Notable Faculty.
Notable faculty members include historian Ervand Abrahamian, biophysicist William Bialek, composer John Corigliano, geographer David Harvey, physicist Michio Kaku, philosopher Saul Kripke, economist Paul Krugman, philosopher David Rosenthal, and mathematician Dennis Sullivan.

</doc>
<doc id="7543" url="http://en.wikipedia.org/wiki?curid=7543" title="Computational complexity theory">
Computational complexity theory

Computational complexity theory is a branch of the theory of computation in theoretical computer science and mathematics that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.
Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.
Computational problems.
Problem instances.
A computational problem can be viewed as an infinite collection of "instances" together with a "solution" for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is "yes" if the number is prime and "no" otherwise (in this case "no"). Stated another way, the "instance" is a particular input to the problem, and the "solution" is the output corresponding to the given input.
To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.
Representing problem instances.
When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.
Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.
Decision problems as formal languages.
Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either "yes" or "no", or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer "yes", the algorithm is said to accept the input string, otherwise it is said to reject the input.
An example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected, or not. The formal language associated with this decision problem is then the set of all connected graphs—of course, to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.
Function problems.
A function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn't just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.
It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples ("a", "b", "c") such that the relation "a" × "b" = "c" holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.
Measuring the size of an instance.
To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2"n" vertices compared to the time taken for a graph with "n" vertices?
If the input size is "n", the time taken can be expressed as a function of "n". Since the time taken on different inputs of the same size can be different, the worst-case time complexity T("n") is defined to be the maximum time taken over all inputs of size "n". If T("n") is a polynomial in "n", then the algorithm is said to be a polynomial time algorithm. Cobham's thesis says that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.
Machine models and complexity measures.
Turing machine.
A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a thought experiment representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.
Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.
A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.
Other machine models.
Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.
However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.
Complexity measures.
For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The "time required" by a deterministic Turing machine "M" on input "x" is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ("yes" or "no"). A Turing machine "M" is said to operate within time "f"("n"), if the time required by "M" on each input of length "n" is at most "f"("n"). A decision problem "A" can be solved in time "f"("n") if there exists a Turing machine operating in time "f"("n") that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time "f"("n") on a deterministic Turing machine is then denoted by DTIME("f"("n")).
Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.
The complexity of an algorithm is often expressed using big O notation.
Best, worst and average case complexity.
The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size "n" may be faster to solve than others, we define the following complexities:
For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O("n"2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O("n" log "n"). The best case occurs when each pivoting divides the list in half, also needing O("n" log "n") time.
Upper and lower bounds on the complexity of problems.
To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the minimum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound "T"("n") on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most "T"("n"). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase "all possible algorithms" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of "T"("n") for a problem requires showing that no algorithm can have time complexity lower than "T"("n").
Upper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if "T"("n") = 7"n"2 + 15"n" + 40, in big O notation one would write "T"("n") = O("n"2).
Complexity classes.
Defining complexity classes.
A complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:
Of course, some complexity classes have complex definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:
But bounding the computation time above by some concrete function "f"("n") often yields complexity classes that depend on the chosen machine model. For instance, the language {"xx" | "x" is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that "the time complexities in any two reasonable and general models of computation are polynomially related" . This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.
Important complexity classes.
Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:
It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.
Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.
Hierarchy theorems.
For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME("n") is contained in DTIME("n"2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.
More precisely, the time hierarchy theorem states that
The space hierarchy theorem states that
The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.
Reduction.
Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at least as difficult as another problem. For instance, if a problem "X" can be solved using an algorithm for "Y", "X" is no more difficult than "Y", and we say that "X" "reduces" to "Y". There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.
The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.
This motivates the concept of a problem being hard for a complexity class. A problem "X" is "hard" for a class of problems "C" if every problem in "C" can be reduced to "X". Thus no problem in "C" is harder than "X", since an algorithm for "X" allows us to solve any problem in "C". Of course, the notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.
If a problem "X" is in "C" and hard for "C", then "X" is said to be "complete" for "C". This means that "X" is the hardest problem in "C". (Since many problems could be equally hard, one might say that "X" is one of the hardest problems in "C".) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.
Important open problems.
P versus NP problem.
The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.
The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.
Problems in NP not known to be in P or NP-complete.
It was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.
The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to Laszlo Babai and Eugene Luks has run time 2O(√("n" log("n"))) for graphs with "n" vertices.
The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than "k". No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time O(e(64/9)1/3("n".log 2)1/3(log ("n".log 2))2/3) to factor an "n"-bit integer. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.
Separations between other complexity classes.
Many known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.
Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the "yes"/"no" answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It has been shown that if these two complexity classes are not equal then P is not equal to NP.
Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.
It is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.
Intractability.
Problems that can be solved in theory (e.g., given large but finite time), but which in practice take too long for their solutions to be useful, are known as "intractable" problems. In complexity theory, problems that lack polynomial-time solutions are considered to be intractable for more than the smallest inputs. In fact, the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial time can be feasibly computed on some computational device. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then the NP-complete problems are also intractable in this sense. To see why exponential-time algorithms might be unusable in practice, consider a program that makes 2"n" operations before halting. For small "n", say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 × 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. Nevertheless a polynomial time algorithm is not always practical. If its running time is, say, "n"15, it is unreasonable to consider it efficient and it is still useless except on small instances.
What intractability means in practice is open to debate. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.
History.
An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.
Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.
 date the beginning of systematic studies in computational complexity to the seminal paper "On the Computational Complexity of Algorithms" by Juris Hartmanis and Richard Stearns (1965), which laid out the definitions of time and space complexity and proved the hierarchy theorems. Also, in 1965 Edmonds defined a "good" algorithm as one with running time bounded by a polynomial of the input size.
According to , earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:
In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, "Reducibility Among Combinatorial Problems", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.

</doc>
<doc id="7544" url="http://en.wikipedia.org/wiki?curid=7544" title="Cadence">
Cadence

Cadence may refer to:

</doc>
<doc id="7546" url="http://en.wikipedia.org/wiki?curid=7546" title="Camelot">
Camelot

Camelot is a castle and court associated with the legendary King Arthur. Absent in the early Arthurian material, Camelot first appeared in 12th-century French romances and eventually came to be described as the fantastic capital of Arthur's realm and a symbol of the Arthurian world. The stories locate it somewhere in Great Britain and sometimes associate it with real cities, though more usually its precise location is not revealed. Most scholars regard it as being entirely fictional, its geography being perfect for romance writers; Arthurian scholar Norris J. Lacy commented that "Camelot, located no where in particular, can be anywhere". Nevertheless arguments about the location of the "real Camelot" have occurred since the 15th century and continue to rage today in popular works and for tourism purposes.
Early appearances.
The castle is mentioned for the first time in Chrétien de Troyes' poem "Lancelot, the Knight of the Cart", dating to the 1170s, though it does not appear in all the manuscripts. It is mentioned in passing, and is not described:
Nothing in Chrétien's poem suggests the level of importance Camelot would have in later romances. For Chrétien, Arthur's chief court was in Caerleon in Wales; this was the king's primary base in Geoffrey of Monmouth's "Historia Regum Britanniae" and subsequent literature. Chrétien depicts Arthur, like a typical medieval monarch, holding court at a number of cities and castles. It is not until the 13th-century French prose romances, including the Lancelot-Grail and the Post-Vulgate Cycle, that Camelot began to supersede Caerleon, and even then, many descriptive details applied to Camelot derive from Geoffrey's earlier grand depiction of the Welsh town. Most Arthurian romances of this period produced in English or Welsh did not follow this trend; Camelot was referred to infrequently, and usually in translations from French. One exception is "Sir Gawain and the Green Knight", which locates Arthur's court at "Camelot"; however, in Britain, Arthur's court was generally located at Caerleon, or at Carlisle, which is usually identified with the "Carduel" of the French romances. However, in the late 15th century, Thomas Malory created the image of Camelot most familiar to English speakers today in his "Le Morte d'Arthur", a work based mostly on the French romances. He firmly identifies Camelot with Winchester, an identification that remained popular over the centuries, though it was rejected by Malory's own editor, William Caxton, who preferred a Welsh location.
Etymology.
The name's derivation is uncertain. It has numerous different spellings in medieval French Arthurian romance, including: "Camaalot", "Camalot", "Chamalot", "Camehelot" (sometimes read as "Camchilot"), "Camaaloth", "Caamalot", "Camahaloth", "Camaelot", "Kamaalot", "Kamaaloth", "Kaamalot", "Kamahaloth", "Kameloth", "Kamaelot", "Kamelot", "Kaamelot", "Cameloth", "Camelot", "Kamelot", "Kaamelot", and "Gamalaot". Renowned Arthurian scholar Ernst Brugger suggested that it was a corruption of "Camlann", the site of Arthur's final battle in Welsh tradition. Roger Sherman Loomis believed it was derived from "Cavalon", a place name that he suggested was a corruption of Avalon (under the influence of the Breton place name "Cavallon"). He further suggested that "Cavalon/Camelot" became Arthur's capital due to confusion with Arthur's other traditional court at "Carlion" ("Caer Lleon" in Welsh). Others have suggested a derivation from the Iron Age and Romano-British place name "Camulodunum", one of the first capitals of Roman Britain and which would have significance in Romano-British culture. Indeed John Morris, the English historian who specialized in the study of the institutions of the Roman Empire and the history of Sub-Roman Britain, suggested in his book "The Age of Arthur" that as the descendants of Romanized Britons looked back to a golden age of peace and prosperity under Rome, the name "Camelot" of Arthurian legend may have referred to the capital of Britannia ("Camulodunum", modern Colchester) in Roman times. It is unclear, however, where Chrétien would have encountered the name "Camulodunum", or why he would render it as "Camaalot". Given Chrétien's known tendency to create new stories and characters, being the first to mention the hero Lancelot and his love affair with Queen Guinevere for example, the name might also be entirely invented.
Description in the romances.
The romances depict the city of Camelot as standing along a river, downstream from Astolat. It is surrounded by plains and forests, and its magnificent cathedral, St. Stephen's, is the religious centre for Arthur's Knights of the Round Table. There Arthur and Guinevere are married and there are the tombs of many kings and knights. In a mighty castle stands the Round Table; it is here that Galahad conquers the Siege Perilous, and where the knights see a vision of the Holy Grail and swear to find it. Jousts are held in a meadow outside the city. In the "Palamedes" and other works, the castle is eventually destroyed by King Mark of Cornwall after the loss of Arthur at the Battle of Camlann. However maddening to later scholars searching for Camelot's location, its imprecise geography serves the romances well, as Camelot becomes less a literal place than a powerful symbol of Arthur's court and universe.
The romancers' versions of Camelot drew on earlier descriptions of Arthur's fabulous court. From Geoffrey's grand description of Caerleon, Camelot gains its impressive architecture, its many churches and the chivalry and courtesy of its inhabitants. Geoffrey's description in turn drew on an already established tradition in Welsh oral tradition of the grandeur of Arthur's court. The tale "Culhwch and Olwen", associated with the "Mabinogion" and perhaps written in the 11th century, draws a dramatic picture of Arthur's hall and his many powerful warriors who go from there on great adventures, placing it in Celliwig, an uncertain locale in Cornwall. Although the court at Celliwig is the most prominent in remaining early Welsh manuscripts, the various versions of the Welsh Triads agree in giving Arthur multiple courts, one in each of the areas inhabited by the Britons: Cornwall, Wales and the Old North. This perhaps reflects the influence of widespread oral traditions common by 800 which are recorded in various place names and features such as Arthur's Seat indicating Arthur was a hero known and associated with many locations across Brittonic areas of Britain as well as Brittany. Even at this stage Arthur could not be tied to one location. Many other places are listed as a location where Arthur holds court in the later romances, Carlisle and London perhaps being the most prominent.
Identifications.
The romancers' versions of Camelot draw on earlier traditions of Arthur's fabulous court. The Celliwig of "Culhwch and Olwen" appears in the Welsh Triads as well; interestingly, this early Welsh material places Wales' greatest leader outside its national boundaries. Geoffrey's description of Caerleon is probably based on his personal familiarity with the town and its impressive Roman ruins; it is less clear that Caerleon was associated with Arthur before Geoffrey. The later French romances make much of "Carduel," a northern city based on the real Carlisle.
Malory's identification of Camelot as Winchester was probably partially inspired by the latter city's history. It had been the capital of Wessex under Alfred the Great, and boasted the Winchester Round Table, an artifact constructed in the 13th century but widely believed to be the original by Malory's time. Malory's editor Caxton rejects the association, saying Camelot was in Wales and that its ruins could still be seen; this is a likely reference to the Roman ruins at Caerwent. Malory associated other Arthurian locations with modern places, for instance locating Astolat at Guildford.
In 1542 John Leland reported the locals around Cadbury Castle in Somerset considered it to be the original Camelot. This theory, which was repeated by later antiquaries, is bolstered, or may have derived from, Cadbury's proximity to the River Cam and the towns of Queen Camel and West Camel, and remained popular enough to help inspire a large-scale archaeological dig in the 20th century. These excavations, led by archaeologist Leslie Alcock from 1966–70, were titled "Cadbury-Camelot," and won much media attention, even being mentioned in the film of the musical "Camelot". The dig revealed by far the largest known fortification of the period, with Mediterranean artifacts (representing extensive trade) and Saxon artifacts. The use of the name Camelot and the support of Geoffrey Ashe helped ensure much publicity for the finds, but Alcock himself later grew embarrassed by the supposed Arthurian connection to the site. Following the arguments of David Dumville, Alcock felt the site was too late and too uncertain to be a tenable Camelot. Modern archaeologists follow him in rejecting the name, calling it instead Cadbury Castle hill fort. Despite this, Cadbury remains widely associated with Camelot.
There were two towns in Roman Britain named Camulodunum, Colchester in Essex, and Slack in West Yorkshire, derived from the Celtic god Camulos, and this has led to the suggestion that they originated the name. However, the Essex Camulodunum was located well within territory usually thought to have been conquered early in the 5th century by Saxons, so it is unlikely to have been the location of any "true" Camelot. The town was definitely known as Colchester as early as the "Anglo-Saxon Chronicle" in 917. Even Colchester Museum argues strongly regarding the historical Arthur: "It would be impossible and inconceivable to link him to the Colchester area, or to Essex more generally", pointing out that the connection between the name Camuloduum and Colchester was unknown till the 18th century. Other places in Britain with names related to "Camel" have also been suggested, such as Camelford in Cornwall, located down the River Camel from where Geoffrey places Camlann, the scene of Arthur's final battle. The area's connections with Camelot and Camlann are merely speculative.
Later uses.
In American contexts, the word "Camelot" is sometimes used to refer admiringly to the presidency of John F. Kennedy, as his term was said to have potential and promise for the future, and many were inspired by Kennedy's speeches, vision, and policies.
At the time, Kennedy's assassination had been compared to the fall of King Arthur. The lines "Don't let it be forgot, that once there was a spot, for one brief shining moment, that was known as Camelot," from the musical "Camelot", were quoted by his widow Jacqueline as being from his favorite song in the score. "There'll be great Presidents again," she added, "but there'll never be another Camelot again … it will never be that way again."
Camelot has become a permanent fixture in interpretations of the Arthurian legend. Modern versions typically retain Camelot's lack of precise location and its status as a symbol of the Arthurian world, though they typically transform the castle itself into romantically lavish visions of a High Middle Ages palace. It lends its name to the 1960 musical "Camelot" by Alan Jay Lerner and Frederick Loewe, which is based on T. H. White's literary version of the legend, "The Once and Future King". The musical was adapted into a 1967 film of the same name, which starred Richard Harris as Arthur, and which featured the Castle of Coca, Segovia as a fittingly opulent Camelot. The symbolism of Camelot so impressed Alfred, Lord Tennyson that he wrote up a prose sketch on the castle as one of his earliest attempts to treat the Arthurian legend. Some writers of the "realist" strain of modern Arthurian fiction have attempted a more sensible Camelot; inspired by Alcock's Cadbury-Camelot excavation, writers Marion Zimmer Bradley, Mary Stewart, and Catherine Christian place their Camelots in that city and describe it accordingly.
Camelot Theme Park is a resort and theme park located in the English county of Lancashire, UK.
"Kaamelott" is a French television series that presents a humorous alternative version of the Arthurian legend.
A Connecticut Yankee in King Arthur's Court, a novel by Mark Twain in 1889, takes place in Camelot.

</doc>
<doc id="7548" url="http://en.wikipedia.org/wiki?curid=7548" title="Contras">
Contras

The contras (some references use the capitalized form, "Contras") is a label given to the various rebel groups that were active from 1979 through to the early 1990s in opposition to the Sandinista Junta of National Reconstruction government in Nicaragua. Among the separate contra groups, the Nicaraguan Democratic Force (FDN) emerged as the largest by far. In 1987, virtually all contra organizations were united, at least nominally, into the Nicaraguan Resistance.
From an early stage, the rebels received financial and military support from the U.S. government, and their military significance decisively depended on it. After U.S. support was banned by Congress, the Reagan administration covertly continued it. These covert activities culminated in the Iran–Contra affair.
The term "contra" comes from the Spanish "contra," which means "against" but in this case is short for , in English "the counter-revolution". Some rebels disliked being called contras, feeling that it defined their cause only in negative terms, or implied a desire to restore the old order. Rebel fighters usually referred to themselves as ("commandos"); peasant sympathizers also called the rebels ("the cousins"). From the mid-1980s, as the Reagan administration and the rebels sought to portray the movement as the "democratic resistance," members started describing themselves as .
During the war against the Sandinista government, the contras carried out many human rights violations, and evidence suggests that these were systematically committed as an element of warfare strategy. Contra supporters often tried to downplay these violations, or countered that the Sandinista government carried out much more. In particular, the Reagan administration engaged in a campaign to alter public opinion on the contras which has been denoted as "white propaganda."
History.
Origins.
The Contras were not a monolithic group, but a combination of three distinct elements of Nicaraguan society:
Main groups.
The CIA and Argentine intelligence, seeking to unify the anti-Sandinista cause before initiating large-scale aid, persuaded 15 September Legion, the UDN and several former smaller groups to merge in September 1981 as the Nicaraguan Democratic Force ("Fuerza Democrática Nicaragüense", FDN). Although the FDN had its roots in two groups made up of former National Guardsmen (of the Somoza regime), its joint political directorate was led by businessman and former anti-Somoza activist Adolfo Calero Portocarrero. Edgar Chamorro later stated that there was strong opposition within the UDN against working with the Guardsmen and that the merging only took place because of insistence by the CIA.
Based in Honduras, Nicaragua's northern neighbor, under the command of former National Guard Colonel Enrique Bermúdez, the new FDN commenced to draw in other smaller insurgent forces in the north. Largely financed, trained, equipped, armed and organized by the U.S., it emerged as the largest and most active contra group.
In April 1982, Edén Pastora ("Comandante Cero"), one of the heroes in the fight against Somoza, organized the Sandinista Revolutionary Front (FRS) – embedded in the Democratic Revolutionary Alliance (ARDE) – and declared war on the Sandinista government. Himself a former Sandinista who had held several high posts in the government, he had resigned abruptly in 1981 and defected, believing that the newly found power had corrupted the Sandinista's original ideas. A popular and charismatic leader, Pastora initially saw his group develop quickly. He confined himself to operate in the southern part of Nicaragua; after a press conference he was holding on 30 May 1984 was bombed, he "voluntarily withdrew" from the contra struggle.
A third force, Misurasata, appeared among the Miskito, Sumo and Rama Amerindian peoples of Nicaragua's Atlantic coast, who in December 1981 found themselves in conflict with the authorities following the government's efforts to nationalize Indian land. In the course of this conflict, forced removal of at least 10,000 Indians to relocation centers in the interior of the country and subsequent burning of some villages took place. The Misurasata movement split in 1983, with the breakaway Misura group of Stedman Fagoth Muller allying itself more closely with the FDN, and the rest accommodating themselves with the Sandinistas: On 8 December 1984 a ceasefire agreement known as the Bogota Accord was signed by Misurasata and the Nicaraguan government. A subsequent autonomy statute in September 1987 largely defused Miskito resistance.
Unity efforts.
U.S. officials were active in attempting to unite the Contra groups. In June 1985 most of the groups reorganized as the United Nicaraguan Opposition (UNO), under the leadership of Adolfo Calero, Arturo Cruz and Alfonso Robelo, all originally supporters of the anti-Somoza revolution. After UNO's dissolution early in 1987, the Nicaraguan Resistance (RN) was organized along similar lines in May.
U.S. military and financial assistance.
In front of the International Court of Justice, Nicaragua claimed that the contras were altogether a creation of the U.S. This claim was rejected. However, the evidence of a very close relationship between the contras and the United States was considered overwhelming and incontrovertible. The U.S. played a very large role in financing, training, arming, and advising the contras over a long period, and the contras only became capable of carrying out significant military operations as a result of this support.
Political background.
The US government viewed the leftist Sandinistas as undemocratic and opposed its ties to Cuba and the Soviet Union. Ronald Reagan, who had assumed the American presidency in January 1981, accused the Sandinistas of importing Cuban-style socialism and aiding leftist guerrillas in El Salvador. The Reagan administration continued to view the Sandinistas as undemocratic despite the 1984 Nicaraguan elections being generally declared fair by foreign observers. Throughout the 1980s the Sandinista government was regarded as "Partly Free" by Freedom House, an organization financed by the U.S. government. 
On 4 January 1982, Reagan signed the top secret National Security Decision Directive 17 (NSDD-17), giving the CIA the authority to recruit and support the contras with $19 million in military aid. The effort to support the contras was one component of the Reagan Doctrine, which called for providing military support to movements opposing Soviet-supported, communist governments.
By December 1981, however, the United States had already begun to support armed opponents of the Sandinista regime. From the beginning, the CIA was in charge. The arming, clothing, feeding and supervision of the contras became the most ambitious paramilitary and political action operation mounted by the agency in nearly a decade.
In the fiscal year 1984, the U.S. Congress approved $24 million in contra aid. However, since the contras failed to win widespread popular support or military victories within Nicaragua, since opinion polls indicated that a majority of the U.S. public was not supportive of the contras, since the Reagan administration lost much of its support regarding its contra policy within Congress after disclosure of CIA mining of Nicaraguan ports, and since a report of the Bureau of Intelligence and Research commissioned by the State Department found Reagan's allegations about Soviet influence in Nicaragua "exaggerated", Congress cut off all funds for the contras in 1985 by the third Boland Amendment. The Boland Amendment had first been passed by Congress in December 1982. At this time, it only outlawed U.S. assistance to the contras "for the purpose of overthrowing the Nicaraguan government", while allowing assistance for other purposes. In October 1984, it was amended to forbid action by not only the Defense Department and the Central Intelligence Agency but all U.S. government agencies.
Nevertheless, the case for support of the contras continued to be made in Washington, D.C., by both the Reagan administration and the Heritage Foundation, which argued that support for the contras would counter Soviet influence in Nicaragua.
On 1 May 1985 President Reagan announced that his administration perceived Nicaragua to be "an unusual and extraordinary threat to the national security and foreign policy of the United States", and declared a "national emergency" and a trade embargo against Nicaragua to "deal with that threat". On May 16, 1985, Nicaraguan President Daniel Ortega redeclared "a position of nonalignment" during his failed mission to try and collect military aid from France, Spain, and Italy. Ortega managed to gain warm words of political and economic support but military aid was ruled out. In 1982, France sold Nicaragua about $17 million worth of arms before U.S. anger made them terminate the deal. A French Foreign Ministry official explained that this reticence resulted, "because we have limited power to maneuver in Central America, and after all, the region is in America's backyard."
Illegal covert operations.
With Congress blocking further contra aid, the Reagan administration sought to arrange funding and military supplies by means of third countries and private sources. Between 1984 and 1986, $34 million from third countries and $2.7 million from private sources were raised this way. The secret contra assistance was run by the National Security Council, with officer Lt. Col. Oliver North in charge. With the third-party funds, North created an organization called "The Enterprise" which served as the secret arm of the NSC staff and had its own airplanes, pilots, airfield, ship, operatives and secret Swiss bank accounts. It also received assistance from personnel from other government agencies, especially from CIA personnel in Central America. This operation functioned, however, without any of the accountability required of U.S. government activities. The Enterprise's efforts culminated in the Iran-Contra Affair of 1986–1987, which facilitated contra funding through the proceeds of arms sales to Iran.
According to the National Security Archive, Oliver North had been in contact with Manuel Noriega, the military leader of Panama later convicted on drug charges, whom he personally met. The issue of drug money and its importance in funding the Nicaraguan conflict was the subject of various reports and publications. The contras were funded by drug trafficking, of which the United States was aware. Senator John Kerry's 1988 Committee on Foreign Relations report on Contra drug links concluded that "senior U.S. policy makers were not immune to the idea that drug money was a perfect solution to the Contras' funding problems".
The Reagan administration's support for the Contras continued to stir controversy well into the 1990s. In August 1996, "San Jose Mercury News" reporter Gary Webb published a series titled "Dark Alliance", alleging that the contras contributed to the rise of crack cocaine in California. In his subsequent 1999 book, also titled Dark Alliance, Webb asserted that the Reagan administration helped harbor known drug traffickers, offering political asylum to some, to help raise funds for the rebel effort.
Propaganda.
During the time the US Congress blocked funding for the contras, the Reagan government engaged in a campaign to alter public opinion and change the vote in Congress on contra aid. For this purpose, the NSC established an interagency working group which in turn coordinated the Office of Public Diplomacy for Latin America and the Caribbean (managed by Otto Reich), which conducted the campaign. The S/LPD produced and widely disseminated a variety of pro-contra publications, arranged speeches and press conferences. It also disseminated "white propaganda"—pro-contra newspaper articles by paid consultants who did not disclose their connection to the Reagan administration.
On top of that, Oliver North helped Carl Channell's tax-exempt organization, the "National Endowment for the Preservation of Liberty", to raise $10 million, by arranging numerous briefings for groups of potential contributors at the premises of the White House and by facilitating private visits and photo sessions with President Reagan for major contributors. Channell, in turn, used part of that money to run a series of television advertisements directed at home districts of Congressmen considered to be swing votes on contra aid. Out of the $10 million raised, more than $1 million was spent on pro-contra publicity.
International Court of Justice ruling.
In 1984 the Sandinista government filed a suit in the International Court of Justice (ICJ) against the United States ("Nicaragua v. United States"), which resulted in a 1986 judgment against the United States. The ICJ held that the U.S. had violated international law by supporting the contras in their rebellion against the Nicaraguan government and by mining Nicaragua's harbors. Regarding the alleged human rights violations by the contras, however, the ICJ took the view that the United States could only be held accountable for them if it would have been proven that the U.S. had effective control of the contra operations resulting in these alleged violations. Nevertheless, the ICJ found that the U.S. encouraged acts contrary to general principles of humanitarian law by producing the manual "Psychological Operations in Guerrilla Warfare (Operaciones sicológicas en guerra de guerrillas") and disseminating it to the contras. The manual, amongst other things, advised on how to rationalize killings of civilians and recommended to hire professional killers for specific selective tasks.
The United States, which did not participate in the merits phase of the proceedings, maintained that the ICJ's power did not supersede the Constitution of the United States and argued that the court did not seriously consider the Nicaraguan role in El Salvador, while it accused Nicaragua of actively supporting armed groups there, specifically in the form of supply of arms. The ICJ had found that evidence of a responsibility of the Nicaraguan government in this matter was insufficient. The U.S. argument was affirmed, however, by the dissenting opinion of ICJ member U.S. Judge Schwebel, who concluded that in supporting the contras, the United States acted lawfully in collective self-defence in El Salvador's support. The U.S. blocked enforcement of the ICJ judgment by the United Nations Security Council and thereby prevented Nicaragua from obtaining any actual compensation. The Nicaraguan government finally withdrew the complaint from the court in September 1992 (under the later, post-FSLN, government of Violeta Chamorro), following a repeal of the law requiring the country to seek compensation.
Human rights violations.
Americas Watch – which subsequently became part of Human Rights Watch – accused the Contras of:
Human Rights Watch released a report on the situation in 1989, which stated: "[The] contras were major and systematic violators of the most basic standards of the laws of armed conflict, including by launching indiscriminate attacks on civilians, selectively murdering non-combatants, and mistreating prisoners."
In his affidavit to the World Court, former contra Edgar Chamorro testified that "The CIA did not discourage such tactics. To the contrary, the Agency severely criticized me when I admitted to the press that the FDN had regularly kidnapped and executed agrarian reform workers and civilians. We were told that the only way to defeat the Sandinistas was to...kill, kidnap, rob and torture..."
Contra leader Adolfo Calero denied that his forces deliberately targeted civilians: "What they call a cooperative is also a troop concentration full of armed people. We are not killing civilians. We are fighting armed people and returning fire when fire is directed at us."
Controversy.
U.S. news media published several articles accusing Americas Watch and other bodies of ideological bias and unreliable reporting. It alleged that Americas Watch gave too much credence to alleged Contra abuses and systematically tried to discredit Nicaraguan human rights groups such as the Permanent Commission on Human Rights, which blamed the major human rights abuses on the Sandinistas.
In 1985, the "Wall Street Journal" reported:
Human Rights Watch, the umbrella organization of Americas Watch, replied to these allegations: "Almost invariably, U.S. pronouncements on human rights exaggerated and distorted the real human rights violations of the Sandinista regime, and exculpated those of the U.S.-supported insurgents, known as the contras...The Bush administration is responsible for these abuses, not only because the contras are, for all practical purposes, a U.S. force, but also because the Bush administration has continued to minimize and deny these violations, and has refused to investigate them seriously."
U.S. political scientist Rudolph Rummel estimated that by 1987, the contras had murdered about 500 people while the Sandinistas had murdered 4,000 to 7,000 people in democide. In contrast, Witness for Peace and the Sandinista government claimed at least 736 civilians were murdered by the contras between March 1987 and October 1988 alone.
Military successes and election of Violeta Chamorro.
By 1986 the contras were besieged by charges of corruption, human-rights abuses, and military ineptitude. A much-vaunted early 1986 offensive never materialized, and Contra forces were largely reduced to isolated acts of terrorism. In October 1987, however, the contras staged a successful attack in southern Nicaragua. Then on 21 December 1987, the FDN launched attacks at La Bonanza, La Siuna, and La Rosita in Zelaya province, resulting in heavy fighting. ARDE Frente Sur attacked at El Almendro and along the Rama road. These large-scale raids mainly became possible as the contras were able to use U.S.-provided Redeye missiles against Sandinista Mi-24 helicopter gunships, which had been supplied by the Soviets. Nevertheless, the Contras remained tenuously encamped within Honduras and weren't able to hold Nicaraguan territory.
There were isolated protests among the population against the draft implemented by the Sandinista government, which even resulted in full-blown street clashes in Masaya in 1988. However, polls showed the Sandinista government still enjoyed strong support from Nicaraguans. Political opposition groups were splintered and the Contras began to experience defections, although United States aid maintained them as a viable military force.
After a cutoff in U.S. military support and with both sides facing international pressure to bring an end to the conflict, the contras agreed to negotiations with the FSLN. With the help of five Central American Presidents, including Ortega, it was agreed that a voluntary demobilization of the contras should start in early December 1989, in order to facilitate free and fair elections in Nicaragua in February 1990 (even though the Reagan administration had pushed for a delay of contra disbandment).
In the resulting February 1990 elections, Violeta Chamorro and her party the UNO won an upset victory of 55% to 41% over Daniel Ortega, even though polls leading up to the election had clearly indicated an FSLN victory.
Possible explanations include that the Nicaraguan people were disenchanted with the Ortega regime as well as the fact that already in November 1989, the White House had announced that the economic embargo against Nicaragua would continue unless Violeta Chamorro won. Also, there had been reports of intimidation from the side of the contras, with a Canadian observer mission confirming 42 people killed by the contras in "election violence" in October 1989. This led many commentators to assume that Nicaraguans voted against the Sandinistas out of fear of a continuation of the contra war and economic deprivation.

</doc>
<doc id="7550" url="http://en.wikipedia.org/wiki?curid=7550" title="Craig Venter">
Craig Venter

John Craig Venter (born October 14, 1946) is an American biochemist, geneticist, and entrepreneur. He is known for being one of the first to sequence the human genome and the first to transfect a cell with a synthetic genome. Venter founded Celera Genomics, The Institute for Genomic Research (TIGR) and the J. Craig Venter Institute (JCVI), and is now working at JCVI to create synthetic biological organisms. He was listed on "Time" magazine's 2007 and 2008 Time 100 list of the most influential people in the world. In 2010, the British magazine "New Statesman" listed Craig Venter at 14th in the list of "The World's 50 Most Influential Figures 2010". He is a member of the USA Science and Engineering Festival's Advisory Board.
Early life and education.
Venter was born in Salt Lake City, Utah, the son of Elizabeth and John Venter. In his youth, he did not take his education seriously, preferring to spend his time on the water in boats or surfing. According to his biography, "A Life Decoded", he was said to never be a terribly engaged student, having Cs and Ds on his eighth-grade report cards. He graduated from Mills High School in Millbrae, California.
Although he was against the Vietnam War, Venter was drafted and enlisted in the United States Navy where he worked in the intensive-care ward of a field hospital. While in Vietnam, he attempted suicide by swimming out to sea, but changed his mind more than a mile out.
Being confronted with wounded, maimed, and dying [marines] on a daily basis instilled in him a desire to study medicine — although he later switched to biomedical research.
Venter began his college education at a community college, College of San Mateo in California, and later transferred to the University of California, San Diego, where he studied under biochemist Nathan O. Kaplan. He received a BS in biochemistry in 1972, and a PhD in physiology and pharmacology in 1975, both from UCSD. He married former PhD candidate Barbara Rae. After working as an associate professor, and later as full professor, at the State University of New York at Buffalo, he joined the National Institutes of Health in 1984.
In Buffalo, he divorced Dr. Rae-Venter and married his student, Claire M. Fraser, remaining married to her until 2005. In late 2008 he married Heather Kowalski. They live in La Jolla in San Diego, California where Venter gut-renovated a $6 million home.
Venter is an atheist.
Venter himself recognized his own ADHD behavior in his adolescence, and later found ADHD-linked genes in his own DNA.
Career.
Scientific discoveries.
While at the NIH, Venter learned of a technique for rapidly identifying all of the mRNAs present in a cell and began to use it to identify human brain genes. The short cDNA sequence fragments discovered by this method are called expressed sequence tags (ESTs), a name coined by Anthony Kerlavage at The Institute for Genomic Research. The NIH initially led an effort to patent these gene fragments, in which Venter coincidentally and controversially became involved. The NIH later withdrew the patent applications, following public outcry. Subsequent court cases declared that ESTs were not directly patentable.
Human Genome Project.
Venter was passionate about the power of genomics to radically transform healthcare. Venter believed that shotgun sequencing was the fastest and most effective way to get useful human genome data. The method was controversial, however, since some geneticists felt it would not be accurate enough for a genome as complicated as that of humans. Frustrated with what Venter viewed as the slow pace of progress in the Human Genome project, and unable to get funds for his ideas, he sought funding from the private sector to fund Celera Genomics. The goal of the company was to sequence the entire human genome and release it into the public domain for non-commercial use in much less time and for much less cost than the public human genome project. The company planned to profit from their work by creating a value-added database of genomic data to which users could subscribe for a fee. The goal consequently put pressure on the public genome program and spurred several groups to redouble their efforts to produce the full sequence. DNA from five demographically different individuals was used by Celera to generate the sequence of the human genome; one of the individuals was Venter himself. In 2000, Venter and Francis Collins of the National Institutes of Health and U.S. Public Genome Project jointly made the announcement of the mapping of the human genome, a full three years ahead of the expected end of the Public Genome Program. The announcement was made along with U.S. President Bill Clinton, and UK Prime Minister Tony Blair. Venter and Collins thus shared an award for "Biography of the Year" from A&E Network.
On the 15 February 2001, the Human Genome Project consortium published the first Human Genome in the journal Nature, and was followed, one day later, by a Celera publication in Science. Despite some claims that shotgun sequencing was in some ways less accurate than the clone-by-clone method chosen by the Human Genome Project, the technique became widely accepted by the scientific community.
Although Celera was originally set to sequence a composite of DNA samples, partway through the sequencing, Venter switched the samples for his own DNA. 
After contributing to the Human Genome, and its release into the public domain, Venter was fired by Celera in early 2002. According to his biography, Venter was ready to leave Celera, and was fired due to conflict with the main investor, Tony White, that had existed since day one of the project. Venter writes that his main goal was always to accelerate science and thereby discovery, and he only sought help from the corporate world when he couldn't find funding in the public sector.
Global Ocean Sampling Expedition.
The Global Ocean Sampling Expedition (GOS) is an ocean exploration genome project with the goal of assessing the genetic diversity in marine microbial communities and to understand their role in nature's fundamental processes. Begun as a Sargasso Sea pilot sampling project in August 2003, Craig Venter announced the full Expedition on 4 March 2004. The project, which used Craig Venter's personal yacht, "Sorcerer II", started in Halifax, Canada, circumnavigated the globe and returned to the U.S. in January 2006.
Synthetic genomics.
Venter is currently the president of the J. Craig Venter Institute, which conducts research in synthetic biology. In June 2005, he co-founded Synthetic Genomics, a firm dedicated to using modified microorganisms to produce clean fuels and biochemicals. In July 2009, ExxonMobil announced a $600 million collaboration with Synthetic Genomics to research and develop next-generation biofuels.
Venter is seeking to patent the first life form created by humanity, possibly to be named "Mycoplasma laboratorium". There is speculation that this line of research could lead to producing bacteria that have been engineered to perform specific reactions, for example, produce fuels, make medicines, combat global warming, and so on.
In May 2010, a team of scientists led by Venter became the first to successfully create what was described as "synthetic life". This was done by synthesizing a very long DNA molecule containing an entire bacterium genome, and introducing this into another cell, analogous to the accomplishment of Eckard Wimmer's group, who synthesized and ligated an RNA virus genome and "booted" it in cell lysate. The single-celled organism contains four "watermarks"
written into its DNA to identify it as synthetic and to help trace its descendants. The watermarks include 
In 2013 Venter said scientists would soon be able to use 3D printers to create synthetic life, possibly even recreating alien genomes whose DNA sequences are beamed back from probes like NASA's Curiosity rover.
Individual human genome.
On September 4, 2007, a team led by Sam Levy published the first complete (six-billion-letter) genome of an individual human—Venter's own DNA sequence. Some of the sequences in Venter's genome are associated with wet earwax, increased risk of antisocial behavior, Alzheimer's and cardiovascular diseases. This publication was especially interesting since it contained a diploid instead of a haploid genome and shows promise for personalized medicine via genotyping. This genome, dubbed HuRef by Levy and others, was a landmark accomplishment and as of mid-2010 is probably the highest quality personal genome sequence yet completed.
The Human Reference Genome Browser is a web application for the navigation and analysis of Venter's recently published genome. The HuRef database consists of approximately 32 million DNA reads sequenced using microfluidic Sanger sequencing, assembled into 4,528 scaffolds and 4.1 million DNA variations identified by genome analysis. These variants include single-nucleotide polymorphisms (SNPs), block substitutions, short and large indels, and structural variations like insertions, deletions, inversions and copy number changes.
The browser enables scientists to navigate the HuRef genome assembly and sequence variations, and to compare it with the NCBI human build 36 assembly in the context of the NCBI and Ensembl annotations. The browser provides a comparative view between NCBI and HuRef consensus sequences, the sequence multi-alignment of the HuRef assembly, Ensembl and dbSNP annotations, HuRef variants, and the underlying variant evidence and functional analysis. The interface also represents the haplotype blocks from which diploid genome sequence can be inferred and the relation of variants to gene annotations. The display of variants and gene annotations are linked to external public resources including dbSNP, Ensembl, Online Mendelian Inheritance in Man (OMIM) and Gene Ontology (GO).
Users can search the HuRef genome using HUGO gene names, Ensembl and dbSNP identifiers, HuRef contig or scaffold locations, or NCBI chromosome locations. Users can then easily and quickly browse any genomic region via the simple and intuitive pan and zoom controls; furthermore, data relevant to specific loci can be exported for further analysis.
In popular culture.
Venter has been the subject of several biography books, several scientific documentary books, TV documentaries, numerous magazine articles, and many speeches.
Venter has been the subject of articles in several magazines, including "Wired", "The Economist", Australian science magazine "Cosmos", and "The Atlantic". Additionally, he was featured on "The Colbert Report" on both February 27, 2007, and October 30, 2007.
Venter appeared in the "Evolution" episode of the documentary television series "Understanding".
On May 16, 2004, Venter gave the commencement speech at Boston University.
In a 2007 interview with "New Scientist" when asked "Assuming you can make synthetic bacteria, what will you do with them?", Venter replied:
Furthermore it suggests that one of the main purposes for creating synthetic bacteria would be to reduce the dependence on fossil fuels through bioremediation.
On May 10, 2007, Venter was awarded an honorary doctorate from Arizona State University, and on October 24 of the same year, he received an honorary doctorate from Imperial College London.
He was on the 2007 Time 100 most influential people in the world list made by "Time" magazine. In 2007 he also received the Golden Eurydice Award for contributions to Biophilosophy.
On September 4, 2007, a team led by Venter published the first complete (six-billion-letter) genome of an individual human — Venter's own DNA sequence.
On December 4, 2007, Venter gave the Dimbleby lecture for the BBC in London. In February 2008, he gave a speech about his current work at the TED conference.
Venter delivered the 2008 convocation speech for Faculty of Science honours and specialization students at the University of Alberta. A transcription of the speech is available .
Venter was featured in "Time" magazine's "The Top 10 Everything of 2008" article. Number three in 2008's Top 10 Scientific Discoveries was a piece outlining his work stitching together the 582,000 base pairs necessary to invent the genetic information for a whole new bacterium.
For an episode aired on July 27, 2009, Venter was interviewed on his boat by BBC One for the first episode of TV show "Bang Goes the Theory".
On May 8, 2010, Venter received an honorary doctor of science degree from Clarkson University for his work on the human genome.
On May 20, 2010, Venter announced the creation of first self-replicating semi-synthetic bacterial cell.
On November 21, 2010 Steve Kroft profiled Venter and his research on "60 Minutes".
On April 21, 2011, Venter received the 2011 Benjamin Rush Medal from William & Mary School of Law.
In the June 2011 issue of "Men's Journal", Venter was featured as the "Survival Skills" celebrity of the month. He shared various anecdotes, and advice, including stories of his time in Vietnam, as well as mentioning a bout with melanoma upon his back, which subsequently resulted in "giving a pound of flesh" to surgery.
Venter is mentioned, in the season finale of the first season of the science fiction series "Orphan Black", a joint production of Space and BBC America. In the episode, Venter is referenced as patenting an organism and encoding a message in the genome of that organism, an act repeated by the character of Aldous Leekie (played by Matt Frewer). While the clones fear that this renders them as nothing more than property, in reality, in the United States and Canada, where the show primarily takes place, it is likely that such a patent would either be entirely unenforceable or would to the genomic sequence of the organism only, due to constitutional provisions and laws against owning human beings.
Bibliography.
Venter is an ISI highly cited researcher and has authored over 200 publications in scientific journals.

</doc>
<doc id="7552" url="http://en.wikipedia.org/wiki?curid=7552" title="Chemical evolution">
Chemical evolution

Chemical evolution may refer to:

</doc>
<doc id="7554" url="http://en.wikipedia.org/wiki?curid=7554" title="Carl Rogers">
Carl Rogers

Carl Ransom Rogers (January 8, 1902 – February 4, 1987) was an influential American psychologist and among the founders of the humanistic approach (or client-centered approach) to psychology. Rogers is widely considered to be one of the founding fathers of psychotherapy research and was honored for his pioneering research with the Award for Distinguished Scientific Contributions by the American Psychological Association (APA) in 1956.
The person-centered approach, his own unique approach to understanding personality and human relationships, found wide application in various domains such as psychotherapy and counseling (client-centered therapy), education (student-centered learning), organizations, and other group settings. For his professional work he was bestowed the Award for Distinguished Professional Contributions to Psychology by the APA in 1972. In a study by Haggbloom et al. (2002) using six criteria such as citations and recognition, Rogers was found to be the sixth most eminent psychologist of the 20th century and second, among clinicians, only to Sigmund Freud.
Biography.
Rogers was born on January 8, 1902, in Oak Park, Illinois, a suburb of Chicago. His father, Walter A. Rogers, was a civil engineer and his mother, Julia M. Cushing, was a homemaker and devout Pentecostal Christian. Carl was the fourth of their six children.
Rogers was intelligent and could read well before kindergarten. Following an education in a strict religious and ethical environment as an altar boy at the vicarage of Jimpley, he became a rather isolated, independent and disciplined person, and acquired a knowledge and an appreciation for the scientific method in a practical world. His first career choice was agriculture, at the University of Wisconsin–Madison, where he was a part of the fraternity of Alpha Kappa Lambda, followed by history and then religion. At age 20, following his 1922 trip to Peking, China, for an international Christian conference, he started to doubt his religious convictions. To help him clarify his career choice, he attended a seminar entitled "Why am I entering the Ministry?", after which he decided to change his career. In 1924, he graduated from University of Wisconsin and enrolled at Union Theological Seminary.
After two years he left the seminary to attend Teachers College, Columbia University, obtaining an MA in 1928 and a PhD in 1931. While completing his doctoral work, he engaged in child study. In 1930, Rogers served as director of the Society for the Prevention of Cruelty to Children in Rochester, New York. From 1935 to 1940 he lectured at the University of Rochester and wrote "The Clinical Treatment of the Problem Child" (1939), based on his experience in working with troubled children. He was strongly influenced in constructing his client-centered approach by the post-Freudian psychotherapeutic practice of Otto Rank. In 1940 Rogers became professor of clinical psychology at Ohio State University, where he wrote his second book, "Counseling and Psychotherapy" (1942). In it, Rogers suggested that the client, by establishing a relationship with an understanding, accepting therapist, can resolve difficulties and gain the insight necessary to restructure their life.
In 1945, he was invited to set up a counseling center at the University of Chicago. In 1947 he was elected President of the American Psychological Association. While a professor of psychology at the University of Chicago (1945–57), Rogers helped to establish a counseling center connected with the university and there conducted studies to determine the effectiveness of his methods. His findings and theories appeared in "Client-Centered Therapy" (1951) and "Psychotherapy and Personality Change" (1954). One of his graduate students at the University of Chicago, Thomas Gordon, established the Parent Effectiveness Training (P.E.T.) movement. In 1956, Rogers became the first President of the American Academy of Psychotherapists. He taught psychology at the University of Wisconsin, Madison (1957–63), during which time he wrote one of his best-known books, "On Becoming a Person" (1961). Carl Rogers and Abraham Maslow (1908–70) pioneered a movement called humanistic psychology which reached its peak in the 1960s. In 1961, he was elected a Fellow of the American Academy of Arts and Sciences. Carl Rogers was also one of the people who questioned the rise of McCarthyism in 1950s. Through articles, he criticized society of its backward-looking affinities.
Rogers continued teaching at University of Wisconsin until 1963, when he became a resident at the new Western Behavioral Sciences Institute (WBSI) in La Jolla. Rogers left the WBSI to help found the Center for Studies of the Person in 1968. His later books include "Carl Rogers on Personal Power" (1977) and "Freedom to Learn for the 80's" (1983). He remained a resident of La Jolla for the rest of his life, doing therapy, giving speeches and writing until his sudden death in 1987. In 1987, Rogers suffered a fall that resulted in a fractured pelvis: he had life alert and was able to contact paramedics. He had a successful operation, but his pancreas failed the next night and he died a few days later.
Rogers' last years were devoted to applying his theories in situations of political oppression and national social conflict, traveling worldwide to do so. In Belfast, Northern Ireland, he brought together influential Protestants and Catholics; in South Africa, blacks and whites; in Brazil people emerging from dictatorship to democracy in the United States, consumers and providers in the health field. His last trip, at age 85, was to the Soviet Union, where he lectured and facilitated intensive experiential workshops fostering communication and creativity. He was astonished at the numbers of Russians who knew of his work.
Together with his daughter, Natalie Rogers, and psychologists Maria Bowen, Maureen O'Hara,and John K. Wood, between 1974 and 1984, Rogers convened a series of residential programs in the US, Europe, Brazil and Japan, the Person-Centered Approach Workshops, which focused on cross-cultural communications, personal growth, self-empowerment, and learning for social change.
Theory.
Rogers' theory of the self is considered to be humanistic, existential, and phenomenological. His theory is based directly on the "phenomenal field" personality theory of Combs and Snygg (1949). Rogers' elaboration of his own theory is extensive. He wrote 16 books and many more journal articles describing it. Prochaska and Norcross (2003) states Rogers "consistently stood for an empirical evaluation of psychotherapy. He and his followers have demonstrated a humanistic approach to conducting therapy and a scientific approach to evaluating therapy need not be incompatible."
Nineteen propositions.
His theory (as of 1951) was based on 19 propositions:
Additionally, Rogers is known for practicing "unconditional positive regard," which is defined as accepting a person "without negative judgment of ... [a person's] basic worth."
Development of the personality.
With regard to development, Rogers described principles rather than stages. The main issue is the development of a self-concept and the progress from an undifferentiated self to being fully differentiated.
In the development of the self-concept, he saw conditional and unconditional positive regard as key. Those raised in an environment of unconditional positive regard have the opportunity to fully actualize themselves. Those raised in an environment of conditional positive regard feel worthy only if they match conditions (what Rogers describes as "conditions of worth") that have been laid down for them by others.
The fully functioning person.
Optimal development, as referred to in proposition 14, results in a certain process rather than static state. He describes this as "the good life", where the organism continually aims to fulfill its full potential. He listed the characteristics of a fully functioning person (Rogers 1961):
Incongruence.
Rogers identified the "real self" as the aspect of one's being that is founded in the actualizing tendency, follows organismic valuing, needs and receives positive regard and self-regard. It is the "you" that, if all goes well, you will become. On the other hand, to the extent that our society is out of sync with the actualizing tendency, and we are forced to live with conditions of worth that are out of step with organismic valuing, and receive only conditional positive regard and self-regard, we develop instead an "ideal self". By ideal, Rogers is suggesting something not real, something that is always out of our reach, the standard we cannot meet. This gap between the real self and the ideal self, the "I am" and the "I should" is called "incongruity".
Psychopathology.
Rogers described the concepts of "congruence" and "incongruence" as important ideas in his theory. In proposition #6, he refers to the actualizing tendency. At the same time, he recognized the need for "positive regard". In a fully congruent person realizing their potential is not at the expense of experiencing positive regard. They are able to lead lives that are authentic and genuine. Incongruent individuals, in their pursuit of positive regard, lead lives that include falseness and do not realize their potential. Conditions put on them by those around them make it necessary for them to forego their genuine, authentic lives to meet with the approval of others. They live lives that are not true to themselves, to who they are on the inside out.
Rogers suggested that the incongruent individual, who is always on the defensive and cannot be open to all experiences, is not functioning ideally and may even be malfunctioning. They work hard at maintaining/protecting their self-concept. Because their lives are not authentic this is a difficult task and they are under constant threat. They deploy "defense mechanisms" to achieve this. He describes two mechanisms: "distortion" and "denial". Distortion occurs when the individual perceives a threat to their self-concept. They distort the perception until it fits their self-concept.
This defensive behavior reduces the consciousness of the threat but not the threat itself. And so, as the threats mount, the work of protecting the self-concept becomes more difficult and the individual becomes more defensive and rigid in their self structure. If the incongruence is immoderate this process may lead the individual to a state that would typically be described as neurotic. Their functioning becomes precarious and psychologically vulnerable. If the situation worsens it is possible that the defenses cease to function altogether and the individual becomes aware of the incongruence of their situation. Their personality becomes disorganised and bizarre; irrational behavior, associated with earlier denied aspects of self, may erupt uncontrollably.
Controversy.
From the mid 1950s through the 1960s Rogers received funding for his research from a program called the Society for the Investigation of Human Ecology, later renamed the Human Ecology Fund. Rogers sat on the board of the Society while he was at the University of Wisconsin.
Applications.
Person-centered therapy.
Rogers originally developed his theory to be the foundation for a system of therapy. He initially called this "non-directive therapy" but later replaced the term "non-directive" with the term "client-centered" and then later used the term "person-centered". Even before the publication of "Client-Centered Therapy" in 1951, Rogers believed that the principles he was describing could be applied in a variety of contexts and not just in the therapy situation. As a result he started to use the term "person-centered approach" later in his life to describe his overall theory. Person-centered therapy is the application of the person-centered approach to the therapy situation. Other applications include a theory of personality, interpersonal relations, education, nursing, cross-cultural relations and other "helping" professions and situations.
The first empirical evidence of the effectiveness of the client-centered approach was published in 1941 at the Ohio State University by Elias Porter, using the recordings of therapeutic sessions between Carl Rogers and his clients. Porter used Rogers' transcripts to devise a system to measure the degree of directiveness or non-directiveness a counselor employed. The attitude and orientation of the counselor were demonstrated to be instrumental in the decisions made by the client.
Learner-centered teaching.
The application to education has a large robust research tradition similar to that of therapy with studies having begun in the late 1930s and continuing today (Cornelius-White, 2007). Rogers described the approach to education in "Client-Centered Therapy" and wrote "Freedom to Learn" devoted exclusively to the subject in 1969. "Freedom to Learn" was revised two times. The new Learner-Centered Model is similar in many regards to this classical person-centered approach to education.
Rogers and Harold Lyon began a book prior to Rogers death, entitled "On Becoming an Effective Teacher -- Person-centered Teaching, Psychology, Philosophy, and Dialogues with Carl R. Rogers and Harold Lyon", which was completed by Lyon and Reinhard Tausch and published in 2013 containing Rogers last unpublished writings on person-centered teaching. Rogers had the following five hypotheses regarding learner-centered education:
Rogerian rhetorical approach.
In 1970, Richard Young, Alton L. Becker, and Kenneth Pike published "Rhetoric: Discovery and Change", a widely influential college writing textbook that used a Rogerian approach to communication to revise the traditional Aristotelian framework for rhetoric. The Rogerian method of argument involves each side restating the other's position to the satisfaction of the other. In a paper, it can be expressed by carefully acknowledging and understanding the opposition, rather than dismissing them.
Cross-cultural relations.
The application to cross-cultural relations has involved workshops in highly stressful situations and global locations including conflicts and challenges in South Africa, Central America, and Ireland. Along with Alberto Zucconi and Charles Devonshire, he co-founded the Istituto dell' Approrio Centrato sulla Persona (Person-Centered Institute) in Rome, Italy.
His international work for peace culminated in the Rust Peace Workshop which took place in November 1985 in Rust, Austria. Leaders from 17 nations convened to discuss the topic "The Central America Challenge". The meeting was notable for several reasons: it brought national figures together as people (not as their positions), it was a private event, and was an overwhelming positive experience where members heard one another and established real personal ties, as opposed to stiffly formal and regulated diplomatic meetings.
External links.
 

</doc>
<doc id="7555" url="http://en.wikipedia.org/wiki?curid=7555" title="Casimir effect">
Casimir effect

In quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir.
The typical example is of two uncharged metallic plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field also means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the QED vacuum of quantum electrodynamics, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force—either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured, and is a striking example of an effect captured formally by second quantization. However, the treatment of boundary conditions in these calculations has led to some controversy.
In fact "Casimir's original goal was to compute the van der Waals force between polarizable molecules" of the metallic plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.
Dutch physicists Hendrik B. G. Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947, and, after a conversation with Niels Bohr who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948; the former is called the Casimir–Polder force while the latter is the Casimir effect in the narrow sense. Predictions of the force were later extended to finite-conductivity metals and dielectrics by Lifshitz and his students, and recent calculations have considered more general geometries. It was not until 1997, however, that a direct experiment, by S. Lamoreaux, described above, quantitatively measured the force (to within 15% of the value predicted by the theory), although previous work [e.g. van Blockland and Overbeek (1978)] had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films by Sabisky and Anderson in 1972. Subsequent experiments approach an accuracy of a few percent.
Because the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm—about 100 times the typical size of an atom—the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).
In modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; and in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.
Any medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas exhibit the Casimir force.
Overview.
The Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.
Possible causes.
Vacuum energy.
The causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a "field" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.
The vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, "empty" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is
Summing over all possible oscillators at all points in space gives an infinite quantity. To remove this infinity, one may argue that only differences in energy are physically measurable; this argument is the underpinning of the theory of renormalization. In all practical calculations, this is how the infinity is always handled. In a wider systematic sense however, renormalization isn't a mathematically harmonious method for the removal of this infinity, and it presents a challenge in the search for a Theory of Everything. Currently there is no compelling explanation for why this infinity should be treated as essentially zero; a non-zero value is essentially the cosmological constant and any large value causes trouble in cosmology.
Relativistic van der Waals force.
Alternatively, a 2005 paper by Robert Jaffe of MIT states that "Casimir effects
can be formulated and Casimir forces can be computed without reference to zero-point energies.
They are relativistic, quantum forces between charges and currents. The Casimir force (per unit
area) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha → infinity limit," and that "The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates."
Effects.
Casimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.
Consider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the "n"th standing wave is formula_2. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then
with the sum running over all possible values of "n" enumerating the standing waves. The factor of 1/2 corresponds to the fact that the zero-point energies are being summed (it is the same 1/2 as appears in the equation formula_4). Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.
In particular, one may ask how the zero-point energy depends on the shape "s" of the cavity. Each energy level formula_2 depends on the shape, and so one should write formula_6 for the energy level, and formula_7 for the vacuum expectation value. At this point comes an important observation: the force at point "p" on the wall of the cavity is equal to the change in the vacuum energy if the shape "s" of the wall is perturbed a little bit, say by formula_8, at point "p". That is, one has
This value is finite in many practical calculations.
Attraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance "a" from one of two widely separated plates (distance "L" apart). With "a" « "L", the states within the slot of width "a" are highly constrained so that the energy "E" of any one mode is widely separated from that of the next. This is not the case in open region "L", where there is a large number (about "L"/"a") of states with energy evenly spaced between "E" and the next mode in the narrow slot---in other words, all slightly larger than "E". Now on shortening "a" by d"a" (< 0), the mode in the slot shrinks in wavelength and therefore increases in energy proportional to -d"a"/"a", whereas all the outside "L"/"a" states lengthen and correspondingly lower energy proportional to d"a"/"L" (note the denominator). The net change is slightly negative, because all the "L"/"a" modes' energies are slightly larger than the single mode in the slot.
Derivation of Casimir effect assuming zeta-regularization.
In the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_10 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the parallel plates lie in the xy-plane, the standing waves are
where formula_12 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_13 and formula_14 are the wave vectors in directions parallel to the plates, and
is the wave-vector perpendicular to the plates. Here, "n" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is
where "c" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in k-space. The assumption of periodic boundary conditions yields,
where "A" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is
In the end, the limit formula_19 is to be taken. Here "s" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for "s" real and larger than 3. The sum has a pole at "s" = 3, but may be analytically continued to "s" = 0, where the expression is finite. The above expression simplifies to:
where polar coordinates formula_21 were introduced to turn the double integral into a single integral. The formula_22 in front is the Jacobian, and the formula_23 comes from the angular integration. The integral converges if Re["s"] > 3, resulting in
The sum diverges at "s" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to "s" = 0 is assumed to make sense physically in some way, then one has
But formula_26 and so one obtains
The analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_28 for idealized, perfectly conducting plates with vacuum between them is
where
The force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_30 shows that the Casimir force per unit area formula_28 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.
NOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance "a" from one of two widely separated plates (distance "L" apart). The 0-point energy on "both" sides of the plate is considered. Instead of the above "ad hoc" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_35 in the above.
More recent theory.
Casimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz' theory for two metal plates reduces to Casimir's idealized 1/"a"4 force law for large separations "a" much greater than the skin depth of the metal, and conversely reduces to the 1/"a"3 force law of the London dispersion force (with a coefficient called a Hamaker constant) for small "a", with a more complicated dependence on "a" for intermediate separations determined by the dispersion of the materials.
Lifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius "R" is much larger than the separation "a", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate "R"/"a"3 force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.
Measurement.
One of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven, in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.
The Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California at Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.
In 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.
Regularisation.
In order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.
The heat kernel or exponentially regulated sum is
where the limit formula_37 is taken in the end. The divergence of the sum is typically manifested as
for three-dimensional cavities. The infinite part of the sum is associated with the bulk constant "C" which "does not" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator
is better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator
is completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex "s" plane, with the bulk divergence at "s" = 4. This sum may be analytically continued past this pole, to obtain a finite part at "s" = 0.
Not every cavity configuration necessarily leads to a finite part (the lack of a pole at "s" = 0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in "Landau and Lifshitz", "Theory of Continuous Media".)
Generalities.
The Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called "virtual particles".
More interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.
In the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.
Dynamical Casimir effect.
The dynamical Casimir effect is the production of particles and energy from an accelerated "moving mirror". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.
Analogies.
A similar analysis can be used to explain Hawking radiation that causes the slow "evaporation" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).
Constructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.
Repulsive forces.
There are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was recently carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers-Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.
Applications.
It has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.
Because the Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be "arbitrarily" negative at a given point. Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole. Similar suggestions have been made for the Alcubierre Drive.
On 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.

</doc>
<doc id="7558" url="http://en.wikipedia.org/wiki?curid=7558" title="Coin">
Coin

Coins are pieces of hard material used primarily as a medium of exchange or legal tender. They are standardized in weight, and produced in large quantities at a mint in order to facilitate trade. They are most often issued by a government.
Coins are usually metal or alloy, or sometimes made of synthetic materials. They are usually disc shaped. Coins made of valuable metal are stored in large quantities as bullion coins. Other coins are used as money in everyday transactions, circulating alongside banknotes: these coins are usually worth less than banknotes: usually the highest value coin in circulation (i.e. excluding bullion coins) is worth less than the lowest-value note. In the last hundred years, the face value of circulation coins has occasionally been lower than the value of the metal they contain, for example due to inflation. If the difference becomes significant, the issuing authority may decide to withdraw these coins from circulation, or the general public may decide to melt the coins down or hoard them (see Gresham's law).
Exceptions to the rule of face value being higher than content value also occur for some bullion coins made of silver or gold (and, rarely, other metals, such as platinum or palladium), intended for collectors or investors in precious metals. Examples of modern gold collector/investor coins include the British sovereign minted by the United Kingdom, the American Gold Eagle minted by the United States, the Canadian Gold Maple Leaf minted by Canada, and the Krugerrand, minted by South Africa. While the Eagle, Maple Leaf, and Sovereign coins have nominal (purely symbolic) face values; the Krugerrand does not.
Historically, a great quantity of coinage metals (including alloys) and other materials (e.g. porcelain) have been used to produce coins for circulation, collection, and metal investment: bullion coins often serve as more convenient stores of assured metal quantity and purity than other bullion.
Today, the term "coin" can also be used in reference to digital currencies which are not issued by a state, such as Bitcoin and LiteCoin.
History.
The first coins were developed independently in Iron Age Anatolia and Archaic Greece, India and China around 600-700 BC. Coins spread rapidly in the 6th and 5th centuries BC, throughout Greece and Persia, and further to the Balkans.
Standardized Roman currency was used throughout the Roman Empire. Important Roman gold and silver coins were continued into the Middle Ages (see Gold dinar, Solidus, Aureus, Denarius). Ancient and early medieval coins in theory had the value of their metal content, although there have been many instances throughout history of the metal content of coins being debased, so that the inferior coins were worth less in metal than their face value. Fiat money first arose in medieval China, with the jiaozi paper money. Early paper money was introduced in Europe in the later Middle Ages, but some coins continued to have the value of the gold or silver they contained throughout the Early Modern period. The penny was minted as a silver coin until the 17th century.
The first copper pennies were minted in the United States in the 1790s. Silver content was reduced in many coins in the 19th century (use of billon), and the first coins made entirely of base metal (e.g. nickel, cupronickel, aluminium bronze), representing values higher than the value of their metal, were minted in the mid 19th century.
Coins were an evolution of "currency" systems of the Late Bronze Age, where standard-sized ingots, and tokens such as knife money, were used to store and transfer value. In the late Chinese Bronze Age, standardized cast tokens were made, such as those discovered in a tomb near Anyang. These were replicas in bronze of earlier Chinese currency, cowrie shells, so they were named Bronze Shell.
Iron Age.
The earliest coins are mostly associated with Iron Age Anatolia, especially with the kingdom of Lydia.
Early electrum coins were not standardized in weight, and in their earliest stage may have been ritual objects, such as badges or medals, issued by priests.
Many early Lydian and Greek coins were minted under the authority of private individuals and are thus more akin to tokens or badges than to modern coins, though due to their numbers it is evident that some were official state issues, with King Alyattes of Lydia being a frequently mentioned originator of coinage.
The first Lydian coins were made of electrum, a naturally occurring alloy of silver and gold that was further alloyed with added silver and copper.
Most of the early Lydian coins include no writing ("legend" or "inscription"), only an image of a symbolic animal. Therefore the dating of these coins relies primarily on archaeological evidence, with the most commonly cited evidence coming from excavations at the Temple of Artemis at Ephesus, also called the Ephesian Artemision (which would later evolve into one of the Seven Wonders of the Ancient World). Because the oldest lion head "coins" were discovered in that temple, and they do not appear to have been used in commerce, these objects may not have been coins but badges or medals issued by the priests of that temple. Anatolian Artemis was the "Πὀτνια Θηρῶν" (Potnia Thêrôn, "Mistress of Animals"), whose symbol was the stag.
A small percentage of early Lydian/Greek coins have a legend. A famous early electrum coin, the most ancient inscribed coin at present known, is from nearby Caria. This coin has a Greek legend reading "phaenos emi sema" interpreted variously as "I am the badge of Phanes", or "I am the sign of light", or "I am the tomb of light", or "I am the tomb of Phanes".
The coins of Phanes are known to be amongst the earliest of Greek coins, a hemihekte of the issue was found in the foundation deposit of the temple of Artemis at Ephesos (the oldest deposit of electrum coins discovered). One assumption is that Phanes was a wealthy merchant, another that this coin is associated with Apollo-Phanes and, due to the Deer, with Artemis (twin sister of the god of light Apollo-Phaneos). Although only seven Phanes type coins were discovered, it is also notable that 20% of all early electrum coins also have the lion of Artemis and the sun burst of Apollo-Phaneos.
Alternatively, Phanes may have been the Halicarnassian mercenary of Amasis mentioned by Herodotus, who escaped to the court of Cambyses, and became his guide in the invasion of Egypt in 527 or 525 BC. According to Herodotus, this Phanes was buried alive by a sandstorm, together with 50,000 Persian soldiers, while trying to conquer the temple of Amun–Zeus in Egypt. The fact that the Greek word "Phanes" also means light (or lamp), and the word "sema" also means tomb makes this coin a famous and controversial one.
Another candidate for the site of the earliest coins is Aegina, where Chelone ("turtle") coins were first minted on 700 BC, either by the local Aegina people or by Pheidon king of Argos (who first set the standards of weights and measures). In the Bibliothèque Nationale, Paris, there is a unique electrum stater of Aegina.
Coins from Athens and Corinth appeared shortly thereafter, known to exist at least since the late 6th century BC.
Classical antiquity.
Coinage followed Greek colonization and influence first around the Mediterranean and soon after to North Africa (including Egypt), Syria, Persia, and the Balkans.
Coins were minted in the Achaemenid Empire, including the gold "darics" and silver "sigloi". With the Achemenid conquest of Gandhara under Darius the Great ca. 520 BC, the practice spread to the Indo-Gangetic Plain. The coins of this period were called "Puranas", "Karshapanas" or "Pana". These earliest Indian coins, however, are unlike those circulated in Persia, which were derived from the Greek/Anatolian type; they not disk-shaped but rather stamped bars of metal, suggesting that the innovation of stamped currency was added to a pre-existing form of token currency which had already been present in the Mahajanapada kingdoms of the Indian Iron Age. Mahajanapadas that minted their own coins included Gandhara, Kuntala, Kuru, Panchala, Shakya, Surasena and Surashtra.
In China, early round coins appear in the 4th century BC.
The first Roman coins, which were crude, heavy cast bronzes, were issued ca. 289 BC.
Middle Ages.
The first European coin to use Arabic numerals to date the year in which the coin was minted was the St. Gall silver "Plappart" of 1424.
Value.
As currency.
Most coins presently are made of a base metal, and their value comes from their status as fiat money. This means that the value of the coin is decreed by government fiat (law), and thus is determined by the free market only inasmuch as national currencies are used in domestic trade and also traded internationally on foreign exchange markets. Thus these coins are monetary tokens, just as paper currency is: they are usually not backed by metal, but rather by some form of government guarantee. Some have suggested that such coins not be considered to be "true coins" (see below). Thus there is very little economic difference between notes and coins of equivalent face value.
Coins may be in circulation with fiat values lower than the value of their component metals, but they are never initially issued with such value, and the shortfall only arises over time due to inflation, as market values for the metal overtake the fiat declared face value of the coin. Examples are the pre-1965 US dime, quarter, half dollar, and dollar, US nickel, and pre-1982 US penny. As a result of the increase in the value of copper, the United States greatly reduced the amount of copper in each penny. Since mid-1982, United States pennies are made of 97.5% zinc, with the remaining 2.5% being a coating of copper. Extreme differences between fiat values and metal values of coins causes coins to be hoarded or removed from circulation by illicit smelters in order to realise the value of their metal content. This is an example of Gresham's law. The United States Mint, in an attempt to avoid this, implemented new interim rules on December 14, 2006, subject to public comment for 30 days, which criminalized the melting and export of pennies and nickels. Violators can be fined up to $10,000 and/or imprisoned for up to five years.
As a collector's item.
A coin's value as a collector's item or as an investment generally depends on its condition, specific historical significance, rarity, quality/beauty of the design and general popularity with collectors. If a coin is greatly lacking in all of these, it is unlikely to be worth much. The value of bullion coins is also influenced to some extent by those factors, but is largely based on the value of their gold, silver, or platinum content. Sometimes non-monetized bullion coins such as the Canadian Maple Leaf and the American Gold Eagle are minted with nominal face values less than the value of the metal in them, but as such coins are never intended for circulation, these face values have no relevance.
As a medium of expression.
Coins can be used as creative medium of expression – from fine art sculpture to the penny machines that can be found in most amusement parks. In the Code of Federal Regulations (CFR) in the United States there are some regulations specific to nickels and pennies that are informative on this topic. 31 CFR § 82.1 forbids unauthorized persons from exporting, melting, or treating any 5 or 1 cent coins.
This has been a particular problem with nickels and dimes (and with some comparable coins in other currencies) because of their relatively low face value and unstable commodity prices. For a while the copper in US pennies was worth more than one cent, so people would hoard pennies then melt them down for their metal value. It costs more than face value to manufacture pennies or nickels, so any widespread loss of the coins in circulation could be expensive for the Treasury. This was more of a problem when coins were still made of precious metals like silver and gold, so historically strict laws against alteration make more sense.
31 CFR § 82.2 goes on to state that: "(b) The prohibition contained in § 82.1 against the treatment of 5-cent coins and one-cent coins shall not apply to the treatment of these coins for educational, amusement, novelty, jewelry, and similar purposes as long as the volumes treated and the nature of the treatment makes it clear that such treatment is not intended as a means by which to profit solely from the value of the metal content of the coins."
Coin debasement.
Throughout history, monarchs and governments have often created more coinage than their supply of precious metals would allow if the coins were pure metal. By replacing some fraction of a coin's precious metal content with a base metal (often copper or nickel), the intrinsic value of each individual coin was reduced (thereby "debasing" their money), allowing the coining authority to produce more coins than would otherwise be possible. Debasement occasionally occurs in order to make the coin harder and therefore less likely to be worn down as quickly, but the more usual reason is to profit from the difference between face value and metal value. Debasement of money almost always leads to price inflation. Sometimes price controls are at the same time also instituted by the governing authority, but historically these have generally proved unworkable.
The United States is unusual in that it has only slightly modified its coinage system (except for the images and symbols on the coins, which have changed a number of times) to accommodate two centuries of inflation. The one-cent coin has changed little since 1856 (though its composition was changed in 1982 to remove virtually all copper from the coin) and still remains in circulation, despite a greatly reduced purchasing power. On the other end of the spectrum, the largest coin in common circulation is 25 cents, a very low value for the largest denomination coin compared to many other countries. Recent increases in the prices of copper, nickel, and zinc mean that both the US one- and five-cent coins are now worth more for their raw metal content than their face (fiat) value. In particular, copper one-cent pieces (those dated prior to 1982 and some 1982-dated coins) now contain about two cents' worth of copper. Some denominations of circulating coins that were formerly minted in the United States are no longer made. These include coins with a face value of a half cent, two cents, three cents, and twenty cents. (The Half Dollar and Dollar coins are still produced, but mostly for vending machines and collectors.) The United States also used to coin the following denominations for circulation in gold: One dollar, $2.50, three dollars, five dollars, ten dollars, and twenty dollars. In addition, cents were originally slightly larger than the modern quarter and weighed nearly half an ounce, while five-cent coins (known then as "half dimes") were smaller than a dime and made of a silver alloy. Dollars were also much larger and weighed approximately an ounce. One dollar gold coins are no longer produced and rarely used. The U.S. also has bullion and commemorative coins with the following denominations: 50¢, $1, $5, $10, $25, $50, and $100.
Other uses.
Some convicted criminals from the British Isles who were sentenced to transportation to Australia in the 18th and 19th centuries used coins to leave messages of remembrance to loved ones left behind in Britain. The coins were defaced, smoothed and inscribed, either by stippling or engraving, with sometimes touching words of loss. These coins were called "convict love tokens" or "leaden hearts". A number of these tokens are in the collection of the National Museum of Australia.
Features of modern coins.
Circulating coins commonly suffered from "shaving" or "clipping": the public would cut off small amounts of precious metal from their edges to sell it and then pass on the mutilated coins at full value. Unmilled British sterling silver coins were sometimes reduced to almost half their minted weight. This form of debasement in Tudor England was commented on by Sir Thomas Gresham, whose name was later attached to Gresham's law. The monarch would have to periodically recall circulating coins, paying only the bullion value of the silver, and reminting them. This, also known as recoinage, is a long and difficult process that was done only occasionally. Many coins have milled or reeded edges, originally designed to make it easier to detect clipping.
The side of a coin carrying an image of a monarch or other authority, or a national emblem, is usually called the "obverse", or colloquially, "heads"; "see also List of people on coins". The other side, which may carry the denomination, is usually called the "reverse", or colloquially, "tails". The year of minting is usually shown on the obverse, although some Chinese coins, most Canadian coins, the pre-2008 British 20p coin, and all Japanese coins, are exceptions.
In cases where a correctly oriented coin is flipped about its horizontal axis to show the other side correctly oriented, the coin is said to have coin orientation. In cases where a coin is flipped about its vertical axis to show the other side correctly oriented, it is said to have medallic orientation. While coins of the United States dollar display coin orientation, those of the Euro and pound sterling have medallic orientation.
Bimetallic coins are sometimes used for higher values and for commemorative purposes. In the 1990s, France used a tri-metallic coin. Common circulating bimetallic examples include the €1, €2, British £2 and Canadian $2.
The "exergue" is the space on a coin beneath the main design, often used to show the coin's date, although it is sometimes left blank or containing a mint mark, privy mark, or some other decorative or informative design feature. Many coins do not have an exergue at all, especially those with few or no legends, such as the Victorian bun penny.
Not all coins are round. The Australian 50 cent coin, for example, has twelve flat sides. Some coins have wavy edges, e.g. the $2 and 20-cent coins of Hong Kong and the 10 cent coins of Bahamas. Some are square-shaped, such as the 15 cent coin of the Bahamas. During the 1970s, Swazi coins were minted in several shapes, including squares, polygons, and wavy edged circles with 8 and 12 waves.
Some other coins, like the British 20 and 50 pence coins and the Canadian Loonie, have an odd number of sides, with the edges rounded off. This way the coin has a constant diameter, recognisable by vending machines whichever direction it is inserted.
A triangular coin with a face value of £5 (produced to commemorate the 2007/2008 Tutankhamun exhibition at The O2 Arena) was commissioned by the Isle of Man: it became legal tender on 6 December 2007. Other triangular coins issued earlier include: Cabinda coin, Bermuda coin, 2 Dollar Cook Islands 1992 triangular coin, Uganda Millennium Coin and Polish Sterling-Silver 10-Zloty Coin.
Guitar-shaped coins were once issued in Somalia. Poland once issued a fan-shaped 10 złoty coin and the 2002 $10 coin from Nauru, was Europe-shaped.
Some mediaeval coins, called bracteates, were so thin they were struck on only one side.
Many coins over the years have been manufactured with integrated holes such as Chinese "cash" coins, Japanese coins, Colonial French coins, etc.
The Royal Canadian Mint is now able to produce holographic-effect gold and silver coinage. However this procedure is not limited to only bullion or commemorative coinage. The 500 yen coin from Japan was subject to a massive amount of counterfeiting. The Japanese government in response produced a circulatory coin with a holographic image.
The Royal Canadian Mint has also released several coins that are coloured, the first of which was in commemoration of Remembrance Day. The subject was a coloured poppy on the reverse of a 25 cent piece.
An example of non-metallic composite coins (sometimes incorrectly called plastic coins) was introduced into circulation in Trans-Dniestr Republic from 22.08.2014. The special feature of such coins is non-circular shape which depends on the coin value. See 
For a list of many pure metallic elements and their alloys which have been used in actual circulation coins and for trial experiments, see coinage metals.
Physics.
Flipping.
Coins are popularly used as a sort of two-sided die; in order to choose between two options with a random possibility, one choice will be labeled "heads" and the other "tails", and a coin will be flipped or tossed to see whether the heads or tails side comes up on top – see coin flipping. Mathematically, this is known as a Bernoulli trial: a fair coin is defined to have the probability of heads (in the parlance of Bernoulli trials, a "success") of exactly 0.5.
Spinning.
Coins can also be spun on a flat surface such as a table. This results in the following phenomenon: as the coin falls over and rolls on its edge, it spins faster and faster (formally, the precession rate of the symmetry axis of the coin, i.e., the axis passing from one face of the coin to the other) before coming to an abrupt stop. This is mathematically modeled as a finite-time singularity – the precession rate is accelerating to infinity, before it suddenly stops, and has been studied using high speed photography and devices such as Euler's Disk. The slowing down is predominantly caused by rolling friction (air resistance is minor), and the singularity (divergence of the precession rate) can be modeled as a power law with exponent approximately −1/3.
Chemistry.
The odor of coins.
Iron coins have a characteristic metallic smell that is produced upon contact with oils in the skin. Perspiration is chemically reduced upon contact with iron, which causes the skin oils to decompose, forming the volatile molecule 1-octen-2-one.

</doc>
<doc id="7560" url="http://en.wikipedia.org/wiki?curid=7560" title="College of the City of New York">
College of the City of New York

College of the City of New York may refer to:

</doc>
<doc id="7561" url="http://en.wikipedia.org/wiki?curid=7561" title="Classical Kuiper belt object">
Classical Kuiper belt object

A classical Kuiper belt object, also called a cubewano ( "QB1-o"), is a low-eccentricity Kuiper belt object (KBO) that orbits beyond Neptune and is not controlled by an orbital resonance with Neptune. Cubewanos have orbits with semi-major axes in the 40–50 AU range and, unlike Pluto, do not cross Neptune’s orbit. That is, they have low-eccentricity and sometimes low-inclination orbits like the classical planets. 
The name "cubewano" derives from the first trans-Neptunian object (TNO) found after Pluto and Charon, . Similar objects found later were often called "QB1-o's", or "cubewanos", after this object, though the term "classical" is much more frequently used in the scientific literature.
Objects identified as cubewanos include:
Haumea (2003 EL61) was provisionally listed as a cubewano by the Minor Planet Center in 2006, but turned out to be resonant.
Orbits: 'hot' and 'cold' populations.
Most cubewanos are found between the 2:3 orbital resonance with Neptune (populated by plutinos) and the 1:2 resonance. 50000 Quaoar, for example, has a near-circular orbit close to the ecliptic. Plutinos, on the other hand, have more eccentric orbits bringing some of them closer to the Sun than Neptune.
The majority of objects (the so-called 'cold population'), have low inclinations and near-circular orbits. A smaller population (the 'hot population') is characterised by highly inclined, more eccentric orbits.
The Deep Ecliptic Survey reports the distributions of the two populations; one with the inclination centered at 4.6° (named "Core") and another with inclinations extending beyond 30° ("Halo").
Distribution.
The vast majority of KBOs (more than two-thirds) have inclinations of less than 5° and eccentricities of less than 0.1. Their semi-major axes show a preference for the middle of the main belt; arguably, smaller objects close to the limiting resonances have been either captured into resonance or have their orbits modified by Neptune.
The 'hot' and 'cold' populations are strikingly different: more than 30% of all cubewanos are in low inclination, near-circular orbits. The parameters of the plutinos’ orbits are more evenly distributed, with a local maximum in moderate eccentricities in 0.15–0.2 range and low inclinations 5–10°.
See also the comparison with scattered disk objects.
When the orbital eccentricities of cubewanos and plutinos are compared, it can be seen that the cubewanos form a clear 'belt' outside Neptune's orbit, whereas the plutinos approach, or even cross Neptune's orbit. When orbital inclinations are compared, 'hot' cubewanos can be easily distinguished by their higher inclinations, as the plutinos typically keep orbits below 20°. (No clear explanation currently exists for the inclinations of 'hot' cubewanos.)
Cold and hot populations: physical characteristics.
In addition to the distinct orbital characteristics, the two populations display different physical characteristics.
The difference in colour between the red cold population and more heterogeneous hot population was observed as early as in 2002.
Recent studies, based on a larger data set, indicate the cut-off inclination of 12° (instead of 5°) between the cold and hot populations while confirming the distinction between the homogenous red cold population and the bluish hot population. 
Another difference between the low-inclination (cold) and high-inclination (hot) classical objects is the observed number of binary objects. Binaries are quite common on low-inclination orbits and are typically similar-brightness systems. Binaries are less common on high-inclination orbits and their components typically differ in brightness. This correlation, together with the differences in colour, support further the suggestion that the currently observed classical objects belong to at least two different overlapping populations, with different physical properties and orbital history.
Toward a formal definition.
There is no official definition of 'cubewano' or 'classical KBO'. However, the terms are normally used to refer to objects free from significant perturbation from Neptune, thereby excluding KBOs in orbital resonance with Neptune (resonant trans-Neptunian objects). The Minor Planet Center (MPC) and the Deep Ecliptic Survey (DES) do not list cubewanos (classical objects) using the same criteria. Many TNOs classified as cubewanos by the MPC are classified as ScatNear (possibly scattered by Neptune) by the DES. Dwarf planet Makemake is such a borderline classical cubewano/scatnear object. may be an inner cubewano near the plutinos. Furthermore, there is evidence that the Kuiper belt has an 'edge', in that an apparent lack of low-inclination objects beyond 47–49 AU was suspected as early as 1998 and shown with more data in 2001. Consequently, the traditional usage of the terms is based on the orbit’s semi-major axis, and includes objects situated between the 2:3 and 1:2 resonances, that is between 39.4 and 47.8 AU (with exclusion of these resonances and the minor ones in-between).
These definitions lack precision: in particular the boundary between the classical objects and the scattered disk remains blurred. As of 2010, there are 377 objects with perihelion (q) > 40 AU and aphelion (Q) < 47 AU.
DES classification.
Introduced by the report from the Deep Ecliptic Survey by J. L. Elliott "et al." in 2005 uses formal criteria based on the mean orbital parameters. Put informally, the definition includes the objects that have never crossed the orbit of Neptune. According to this definition, an object qualifies as a classical KBO if:
SSBN07 classification.
An alternative classification, introduced by B. Gladman, B. Marsden and C. VanLaerhoven in 2007, uses a 10-million-year orbit integration instead of the Tisserand's parameter. Classical objects are defined as not resonant and not being currently scattered by Neptune.
Formally, this definition includes as "classical" all objects with their "current" orbits that
Unlike other schemes, this definition includes the objects with major semi-axis less than 39.4 AU (2:3 resonance) – termed inner classical belt, or more than 48.7 (1:2 resonance) – termed outer classical belt, while reserving the term main classical belt for the orbits between these two resonances.
Families.
The first known collisional family in the classical Kuiper belt—a group of objects thought to be remnants from the breakup of a single body—is the Haumea family. It includes Haumea, its moons, and seven smaller bodies.† The objects not only follow similar orbits but also share similar physical characteristics. Unlike many other KBO their surface contains large amounts of ice (H2O) and no or very little tholins. The surface composition is inferred from their neutral (as opposed to red) colour and deep absorption at 1.5 and 2. μm in infrared spectrum.
List of objects.
Here is a very generic list of cubewanos. As of 2014, there are about 473 objects with q > 40 (AU) and Q < 48 (AU).

</doc>
<doc id="7564" url="http://en.wikipedia.org/wiki?curid=7564" title="Foreign policy of the United States">
Foreign policy of the United States

The foreign policy of the United States is the way in which it interacts with foreign nations and sets standards of interaction for its organizations, corporations and individual citizens.
The officially stated goals of the foreign policy of the United States, as mentioned in the Foreign Policy Agenda of the U.S. Department of State, are "to build and sustain a more democratic, secure, and prosperous world for the benefit of the American people and the international community." In addition, the United States House Committee on Foreign Affairs states as some of its jurisdictional goals: "export controls, including nonproliferation of nuclear technology and nuclear hardware; measures to foster commercial intercourse with foreign nations and to safeguard American business abroad; international commodity agreements; international education; and protection of American citizens abroad and expatriation." U.S. foreign policy and foreign aid have been the subject of much debate, praise and criticism both domestically and abroad.
Powers of the President and Congress.
Subject to the advice and consent role of the U.S. Senate, the President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two-thirds of the Senate. The President is also Commander in Chief of the United States Armed Forces, and as such has broad authority over the armed forces; however only Congress has authority to declare war, and the civilian and military budget is written by the Congress. The United States Secretary of State is the foreign minister of the United States and is the primary conductor of state-to-state diplomacy. Both the Secretary of State and ambassadors are appointed by the President, with the advice and consent of the Senate. Congress also has power to regulate commerce with foreign nations.
Historical overview.
The main trend regarding the history of U.S. foreign policy since the American Revolution is the shift from non-interventionism before and after World War I, to its growth as a world power and global hegemony during and since World War II and the end of the Cold War in the 20th century. Since the 19th century, US foreign policy also has been characterized by a shift from the realist school to the idealistic or Wilsonian school of international relations.
Foreign policy themes were expressed considerably in George Washington's farewell address; these included among other things, observing good faith and justice towards all nations and cultivating peace and harmony with all, excluding both "inveterate antipathies against particular nations, and passionate attachments for others", "steer[ing] clear of permanent alliances with any portion of the foreign world", and advocating trade with all nations. These policies became the basis of the Federalist Party in the 1790s. But the rival Jeffersonians feared Britain and favored France in the 1790s, declaring the War of 1812 on Britain. After the 1778 alliance with France, the U.S. did not sign another permanent treaty until the North Atlantic Treaty in 1949. Over time, other themes, key goals, attitudes, or stances have been variously expressed by Presidential 'doctrines', named for them. Initially these were uncommon events, but since WWII, these have been made by most presidents.
In general, the United States followed an isolationist foreign policy until attacks against U.S. shipping by Barbary corsairs spurred the country into developing a naval force projection capability, resulting in the First Barbary War in 1801.
Despite occasional entanglements with European Powers such as the War of 1812 and the 1898 Spanish-American War, U.S. foreign policy was marked by steady expansion of its foreign trade and scope during the 19th century, and it maintained its policy of avoiding wars with and between European powers. Concerning its domestic borders, the 1803 Louisiana Purchase doubled the nation's geographical area; Spain ceded the territory of Florida in 1819; annexation brought Texas in 1845; a war with Mexico in 1848 added California, Arizona and New Mexico. The U.S. bought Alaska from the Russian Empire in 1867, and it annexed the Republic of Hawaii in 1898. Victory over Spain in 1898 brought the Philippines, and Puerto Rico, as well as oversight of Cuba. The short experiment in imperialism ended by 1908, as the U.S. turned its attention to the Panama Canal and the stabilization of regions to its south, including Mexico.
20th century.
World War I.
The 20th century was marked by two world wars in which the United States, along with allied powers, defeated its enemies and increased its international reputation. President Wilson's Fourteen Points, developed from his idealistic Wilsonianism program of spreading democracy and fighting militarism so as to end wars. It became the basis of the German Armistice (really a surrender) and the 1919 Paris Peace Conference. The resulting Treaty of Versailles, due to European allies' punitive and territorial designs, showed insufficient conformity with these points and the U.S. signed separate treaties with each of its adversaries; due to Senate objections also, the U.S. never joined the League of Nations, which was established as a result of Wilson's initiative. In the 1920s, the United States followed an independent course, and succeeded in a program of naval disarmament, and refunding the German economy. New York became the financial capital of the world, but the downside was that the Wall Street Crash of 1929 hurled the entire world into the Great Depression. American trade policy relied on high tariffs under the Republicans, and reciprocal trade agreements under the Democrats, but in any case exports were at very low levels in the 1930s.
World War II.
The United States adopted a non-interventionist foreign policy from 1932 to 1938, but then President Franklin D. Roosevelt moved toward strong support of the Allies in their wars against Germany and Japan. As a result of intense internal debate, the national policy was one of becoming the Arsenal of Democracy, that is financing and equipping the Allied armies without sending American combat soldiers. Roosevelt mentioned four fundamental freedoms, which ought to be enjoyed by people "everywhere in the world"; these included the freedom of speech and religion, as well as freedom from want and fear. Roosevelt helped establish terms for a post-war world among potential allies at the Atlantic Conference; specific points were included to correct earlier failures, which became a step toward the United Nations. American policy was to threaten Japan, to force it out of China, and to prevent its attacking the Soviet Union. However, Japan reacted by an attack on Pearl Harbor in December 1941, and the United States was at war with Japan, Germany, and Italy. Instead of the loans given to allies in World War I, the United States provided Lend-Lease grants of $50,000,000,000. Working closely with Winston Churchill of Britain, and Joseph Stalin of the Soviet Union, Roosevelt sent his forces into the Pacific against Japan, then into North Africa against Italy and Germany, and finally into Europe starting with France and Italy in 1944 against the Germans. The American economy roared forward, doubling industrial production, and building vast quantities of airplanes, ships, tanks, munitions, and, finally, the atomic bomb. Much of the American war effort went to strategic bombers, which flattened the cities of Japan and Germany.
Cold War.
After the war, the U.S. rose to become the dominant non-colonial economic power with broad influence in much of the world, with the key policies of the Marshall Plan and the Truman Doctrine. Almost immediately however, the world witnessed division into broad two camps during the Cold War; one side was led by the U.S., and the other by the Soviet Union, but this situation also led to the establishment of the Non-Aligned Movement. This period lasted until almost the end of the 20th century, and is thought to be both an ideological and power struggle between the two superpowers. A policy of containment was adopted to limit Soviet expansion, and a series of proxy wars were fought with mixed results. In 1991, the Soviet Union dissolved into separate nations, and the Cold War formally ended as the United States gave separate diplomatic recognition to the Russian Federation and other former Soviet states. With these changes to forty-five years of established diplomacy and military confrontation, new challenges confronted U.S. policymakers. American foreign policy is characterized by the protection of its national interests.
21st century.
In the 21st century, U.S. influence remains strong but, in relative terms, is declining in terms of economic output compared to rising nations such as China, India, Russia, Brazil, and the newly consolidated European Union. Substantial problems remain, such as climate change, nuclear proliferation, and the specter of nuclear terrorism. Foreign policy analysts Hachigian and Sutphen in their book "The Next American Century" suggest all six powers have similar vested interests in stability and terrorism prevention and trade; if they can find common ground, then the next decades may be marked by peaceful growth and prosperity.
Law.
In the United States, there are three types of treaty-related law:
International law in most nations considers all three of the above agreements as "treaties". In most nations, treaty laws supersede domestic law. So if there is a conflict between a treaty obligation and a domestic law, then the treaty usually prevails.
In contrast to most other nations, the United States considers the three types of agreements as distinct. Further, the United States incorporates treaty law into the body of U.S. federal law. As a result, Congress can modify or repeal treaties afterwards. It can overrule an agreed-upon treaty obligation even if this is seen as a violation of the treaty under international law. Several U.S. court rulings confirmed this understanding, including the 1900 Supreme Court decision in "Paquete Habana", a late 1950s decision in "Reid v. Covert", and a lower court ruling in 1986 in "Garcia-Mir v. Meese". Further, the Supreme Court has declared itself as having the power to rule a treaty as void by declaring it "unconstitutional", although as of 2011, it has never exercised this power.
The State Department has taken the position that the Vienna Convention on the Law of Treaties represents established law. Generally when the U.S. signs a treaty, it is binding. However, because of the "Reid v. Covert" decision, the U.S. adds a reservation to the text of every treaty that says, in effect, that the U.S. intends to abide by the treaty, but if the treaty is found to be in violation of the Constitution, then the U.S. legally can't abide by the treaty since the U.S. signature would be "ultra vires".
Alliances.
NATO.
The United States is a founding member of NATO, the world's largest military alliance. The 28-nation alliance consists of Canada and much of Europe, including the nation with NATO's second largest military, the United Kingdom. Under the NATO charter, the United States is compelled to defend any NATO state that is attacked by a foreign power. NATO is restricted to within the North American and European areas. In 1989, the United States also granted five nations the major non-NATO ally status (MNNA); this number was increased in the late 1990s and following the September 11 attacks; it currently includes 15 nations. Each such state has a unique relationship with the United States, involving various military and economic partnerships and alliances.
Geography.
United Kingdom.
United States foreign policy affirms its alliance with the United Kingdom as its most important bilateral relationship in the world, evidenced by aligned political affairs between the White House and 10 Downing Street, as well as joint military operations carried out between the two nations. While both the United States and the United Kingdom maintain close relationships with many other nations around the world, the level of cooperation in military planning, execution of military operations, nuclear weapons technology, and intelligence sharing with each other has been described as "unparalleled" among major powers throughout the 20th and early 21st century.
The United States and Britain share the world's largest foreign direct investment partnership. American investment in the United Kingdom reached $255.4 billion in 2002, while British direct investment in the United States totaled $283.3 billion.
Canada.
The bilateral relationship between Canada and the United States is of notable importance to both countries. About 75–85% of Canadian trade is with the United States, and Canada is the United States' largest trading partner and chief supplier of oil. While there are disputed issues between the two nations, relations are close and the two countries share the "world's longest undefended border." The border was demilitarized after the War of 1812 and, apart from minor raids, has remained peaceful. Military collaboration began during World War II and continued throughout the Cold War on both a bilateral basis and a multilateral relationship through NATO. A high volume of trade and migration between the United States and Canada since the 1850s has generated closer ties, despite continued Canadian fears of being culturally overwhelmed by its neighbor, which is nine times larger in terms of population and eleven times larger in terms of economy. The two economies have increasingly merged since the North American Free Trade Agreement (NAFTA) of 1994, which also includes Mexico.
Mexico.
The United States shares a unique and often complex relationship with Mexico. A history of armed conflict goes back to the Texas Revolution in the 1830s, the Mexican–American War in the 1840s, and an American invasion in the 1910s. Important treaties include the Gadsden Purchase, and multilaterally with Canada, the North American Free Trade Agreement. The central issue in recent years has been illegal immigration, followed by illegal gun sales (from the U.S.), drug smuggling (to the U.S.) and escalating drug cartel violence just south of the U.S.-Mexico border.
Australia.
The United States' relationship with Australia is a very close one, with Secretary of State Hillary Clinton stating that "America doesn't have a better friend in the world than Australia". The relationship is formalized by the ANZUS treaty and the Australia–United States Free Trade Agreement.
The two countries have a shared history, both have previously been British Colonies and many Americans flocked to the Australian goldfields in the 19th century. At a strategic level, the relationship really came to prominence in World War II, when the two nations worked extremely closely in the Pacific War against Japan, with General Douglas MacArthur undertaking his role as Supreme Allied Commander based in Australia, effectively having Australian troops and resources under his command. During this period, the cultural interaction between Australia and the U.S. were elevated to a higher level as over 1 million U.S. military personnel moved through Australia during the course of the war. The relationship continued to evolve throughout the second half of the 20th Century, and today now involves strong relationships at the executive and mid levels of government and the military, leading Assistant Secretary of State for East Asian and Pacific Affairs, Kurt M. Campbell to declare that "in the last ten years, [Australia] has ascended to one of the closest one or two allies [of the U.S.] on the planet".
Middle East.
The United States has many important allies in the Greater Middle East region. These allies are Turkey, Saudi Arabia, Jordan, Afghanistan, Pakistan, Israel, Egypt, Kuwait, Bahrain, Qatar and Morocco. Israel and Egypt are leading recipients of United States foreign aid, receiving $2.775 billion and 1.75 billion in 2010. Turkey is an ally of the United States through its membership in NATO, while all of the other countries except Saudi Arabia and Qatar are major non-NATO allies.
The United States toppled the government of Saddam Hussein during the 2003 invasion of Iraq. Turkey is host to approximately 90 B61 nuclear bombs at Incirlik Air Base. Other allies include Qatar, where 3,500 U.S. troops are based, and Bahrain, where the United States Navy maintains NSA Bahrain, home of NAVCENT and the Fifth Fleet.
Japan.
The relationship began in the 1850s as the U.S. was a major factor in forcing Japan to resume contacts with the outer world beyond a very restricted role. In the late 19th century the Japanese sent many delegations to Europe, and some to the U.S., to discover and copy the latest technology and thereby modernize Japan very rapidly and allow it to build its own empire. There was some friction over control of Hawaii and the Philippines, but Japan stood aside as the U.S. annexed those lands in 1898. Likewise the U.S. did not object when Japan took control of Korea. The two nations cooperated with the European powers in suppressing the Boxer Rebellion in China in 1900, but the U.S. was increasingly troubled about Japan's denial of the Open Door Policy that would ensure that all nations could do business with China on an equal basis.
President Theodore Roosevelt admired Japan's strength as it defeated a major European power, Russia. He brokered an end to the war between Russia and Japan in 1905–6. Anti-Japanese sentiment (especially on the West Coast) soured relations in the 1907–24 era. In the 1930s the U.S. protested vehemently against Japan's seizure of Manchuria (1931), its war against China (1937–45), and its seizure of Indochina (Vietnam) 1940–41. American sympathies were with China and Japan rejected increasingly angry American demands that Japan pull out of China. The two nations fought an all-out war 1941–45; the U.S. won a total victory, with heavy bombing (including two atomic bombs on Hiroshima and Nagasaki) that devastated Japan's 50 largest industrial cities. The American army under Douglas MacArthur occupied and ruled Japan, 1945–51, with the successful goal of sponsoring a peaceful, prosperous and democratic nation.
In 1951, the U.S. and Japan signed Treaty of San Francisco and Security Treaty Between the United States and Japan, subsequently revised as Treaty of Mutual Cooperation and Security between the United States and Japan in 1960, relations since then have been excellent. The United States considers Japan to be one of its closest allies, and it is both a Major Non-NATO ally and NATO contact country. The United States has several military bases in Japan including Yokosuka, which harbors the U.S. 7th Fleet. The JSDF, or Japanese Self Defense Force, cross train with the U.S. Military, often providing auxiliary security and conducting war games. When the U.S.President Barack Obama met with Japanese Prime Minister Taro Aso in 2009, he said the relationship with Japan as the "cornerstone of security in East Asia". After the several years of critical moment during Japan's Democratic Party administration, President Obama and Prime Minister Shinzo Abe reconfirmed the importance of its alliance and currently the U.S. and Japan negotiating to participate Trans-Pacific Strategic Economic Partnership.
South Korea.
South Korea–United States relations have been most extensive since 1945, when the United States helped establish capitalism in South Korea and led the UN-sponsored Korean War against North Korea and China (1950–1953). Stimulated by heavy American aid, South Korea's rapid economic growth, democratization and modernization greatly reduced its U.S. dependency. Large numbers of U.S. forces remain in Korea. At the 2009 G-20 London summit, U.S. President Barack Obama called South Korea "one of America's closest allies and greatest friends." 
China.
American relations with the People's Republic of China are quite strong, yet complex. A great amount of trade between the two countries necessitates positive political relations, although occasional disagreements over tariffs, currency exchange rates and the Political status of Taiwan do occur. Nevertheless, the United States and China have an extremely extensive partnership. The U.S. criticizes China on human rights issues.
Taiwan.
Taiwan (officially the Republic of China), does not have official diplomatic relations with America and no longer receives diplomatic recognition from the State Department of the United States, but it conducts unofficial diplomatic relations through its de facto embassy, commonly known as the "American Institute in Taiwan (AIT)", and is considered to be a strong Asian ally and supporter of the United States.
ASEAN.
Association of Southeast Asian Nations (ASEAN) is an important partner for United States in both economic and geostrategic aspects. ASEAN's geostrategic importance stems from many factors, including: the strategic location of member countries, the large shares of global trade that pass through regional waters, and the alliances and partnerships which the United States shares with ASEAN member states. In July 2009, the United States signed ASEAN's Treaty of Amity and Cooperation, which establishes guiding principles intended to build confidence among its signatories with the aim of maintaining regional peace and stability. Trade flows are robust and increasing between America and the ASEAN region. Since 2002 exports to the United States have gained 40% in value while U.S. exports to ASEAN increased 62%.
Indonesia.
As the largest ASEAN member, Indonesia has played an active and prominent role in developing the organization. For United States, Indonesia is important for dealing with certain issues; such as terrorism, democracy, and how United States project its relations with Islamic world, since Indonesia has the world's largest Islamic population, and one that honors and respects religious diversity. US eyes Indonesia as potential strategic allies in Southeast Asia. During his stately visit to Indonesia, U.S. President Barack Obama has held up Indonesia as an example of how a developing nation can embrace democracy and diversity.
Malaysia.
Despite increasingly strained relations under the Mahathir Mohamad government, ties have been thawed under Najib Razak's administration. Economic ties are particularly robust, with the United States being Malaysia's largest trading partner and Malaysia is the tenth-largest trading partner of the U.S. Annual two-way trade amounts to $49 billion. The United States and Malaysia launched negotiations for a bilateral free trade agreement (FTA) in June 2006.
The United States and Malaysia enjoy strong security cooperation. Malaysia hosts the Southeast Asia Regional Center for Counterterrorism (SEARCCT), where over 2000 officials from various countries have received training. The United States is among the foreign countries that has collaborated with the center in conducting capacity building programmes. The U.S. and Malaysia share a strong military-to-military relationship with numerous exchanges, training, joint exercises, and visits.
Myanmar.
Bilateral ties have generally been strained but are slowly improving. The United States has placed broad sanctions on Burma because of the military crackdown in 1988 and the military regime's refusal to honour the election results of the 1990 People's Assembly election. Similarly, the European Union has placed embargoes on Burma, including an arms embargo, cessation of trade preferences, and suspension of all aid with the exception of humanitarian aid.
US and European government sanctions against the military government, alongside boycotts and other types direct pressure on corporations by western supporters of the Burmese democracy movement, have resulted in the withdrawal from Burma of most U.S. and many European companies. However, several Western companies remain due to loopholes in the sanctions. Asian corporations have generally remained willing to continue investing in Myanmar and to initiate new investments, particularly in natural resource extraction.
Ongoing reforms have improved relations between Burma and the United States.
Philippines.
The United States ruled the Philippines from 1898 to 1946. The Spanish government ceded the Philippines to the United States in the 1898 Treaty of Paris that ended the Spanish-American War. The United States finally recognized Philippine independence on July 4, 1946 in the Treaty of Manila. July 4 was observed in the Philippines as "Independence Day" until August 4, 1964 when, upon the advice of historians and the urging of nationalists, President Diosdado Macapagal signed into law Republic Act No. 4166 designating June 12 as the country's "Independence Day". Since 2003 the U.S. has designated the Philippines as a Major Non-NATO Ally.
Thailand.
Thailand and the US are both former Southeast Asia Treaty Organization (SEATO) members, being close partners throughout the Cold War, and are still close allies. Since 2003, the U.S. has designated the Thailand as a Major Non-NATO Ally.
Vietnam.
United States involved in Vietnam War in 1955 to 1975. In 1995, President Bill Clinton announced the formal normalization of diplomatic relations with Vietnam. Today US eyes Vietnam as a potential strategic ally in Southeast Asia.
Eastern Europe.
American relations with Eastern Europe are influenced by the legacy of the Cold War. Since the collapse of the Soviet Union, former Communist-bloc states in Europe have gradually transitioned to democracy and capitalism. Many have also joined the European Union and NATO, strengthening economic ties with the broader Western world and gaining the military protection of the United States via the North Atlantic Treaty.
Kosovo.
The UN Security Council divided on the question of Kosovo's declaration of independence. Kosovo declared its independence on February 17, 2008, whilst Serbia objected that Kosovo is part of its territory. Of the five members with veto power in the UN Security Council, the USA, UK, and France recognized the declaration of independence, and China has expressed concern, while Russia considers it illegal. "In its declaration of independence, Kosovo committed itself to the highest standards of democracy, including freedom and tolerance and justice for citizens of all ethnic backgrounds", President George W Bush said on February 19, 2008.
Hub and spoke vs multilateral.
While America's relationships with Europe have tended to be in terms of multilateral frameworks, such as NATO, America's relations with Asia have tended to be based on a "hub and spoke" model using a series of bilateral relationships where states coordinate with the United States and do not collaborate with each other. On May 30, 2009, at the Shangri-La Dialogue Defense Secretary Robert M. Gates urged the nations of Asia to build on this hub and spoke model as they established and grew multilateral institutions such as ASEAN, APEC and the ad hoc arrangements in the area. However in 2011 Gates said that the United States must serve as the "indispensable nation," for building multilateral cooperation.
Oil.
Persian Gulf.
The U.S. currently produces about 40% of the oil that it consumes. While its imports have exceeded domestic production since the early 1990s, new hydraulic fracturing techniques and discovery of shale oil deposits in Canada and the American Dakotas offer the potential for increased energy independence from oil exporting countries such as OPEC. Former U.S. President George W. Bush identified dependence on imported oil as an urgent ""national security concern"."
Two-thirds of the world's proven oil reserves are estimated to be found in the
Persian Gulf. Despite its distance, the Persian Gulf region was first proclaimed to be of national interest to the United States during World War II. Petroleum is of central importance to modern armies, and the United States—as the world's leading oil producer at that time—supplied most of the oil for the Allied armies. Many U.S. strategists were concerned that the war would dangerously reduce the U.S. oil supply, and so they sought to establish good relations with Saudi Arabia, a kingdom with large oil reserves.
The Persian Gulf region continued to be regarded as an area of vital importance to the United States during the Cold War. Three Cold War United States Presidential doctrines—the Truman Doctrine, the Eisenhower Doctrine, and the Nixon Doctrine—played roles in the formulation of the Carter Doctrine, which stated that the United States would use military force if necessary to defend its "national interests" in the Persian Gulf region. Carter's successor, President Ronald Reagan, extended the policy in October 1981 with what is sometimes called the "Reagan Corollary to the Carter Doctrine", which proclaimed that the United States would intervene to protect Saudi Arabia, whose security was threatened after the outbreak of the Iran–Iraq War. Some analysts have argued that the implementation of the Carter Doctrine and the Reagan Corollary also played a role in the outbreak of the 2003 Iraq War.
Canada.
Almost all of Canada's energy exports go to the United States, making it the largest foreign source of U.S. energy imports: Canada is consistently among the top sources for U.S. oil imports, and it is the largest source of U.S. natural gas and electricity imports.
Africa.
In 2007 the U.S. was Sub-Saharan Africa's largest single export market accounting for 28.4% of exports (second in total to the EU at 31.4%). 81% of U.S. imports from this region were petroleum products.
Foreign aid.
Foreign assistance is a core component of the State Department's international affairs budget, which is $49 billion in all for 2014. Aid is considered an essential instrument of U.S. foreign policy. There are four major categories of non-military foreign assistance: bilateral development aid, economic assistance supporting U.S. political and security goals, humanitarian aid, and multilateral economic contributions (for example, contributions to the World Bank and International Monetary Fund).
In absolute dollar terms, the United States government is the largest international aid donor ($23 billion in 2014). The U.S. Agency for International Development (USAID) manages the bulk of bilateral economic assistance; the Treasury Department handles most multilateral aid. In addition many private agencies, churches and philanthropies provide aid.
Although the United States is the largest donor in absolute dollar terms, it is actually ranked 19 out of 27 countries on the Commitment to Development Index. The CDI ranks the 27 richest donor countries on their policies that affect the developing world. In the aid component the United States is penalized for low net aid volume as a share of the economy, a large share of tied or partially tied aid, and a large share of aid given to less poor and relatively undemocratic governments.
Military.
The United States has fought wars and intervened militarily on many occasions. See, Timeline of United States military operations. The U.S. also operates a vast network of military bases around the world. See, List of United States military bases.
In recent years, the U.S. has used its military superiority as sole superpower to lead a number of wars, including, most recently, the invasion of Iraq in March 2003 as part of its global "War on Terror."
Aid.
The U.S. provides military aid through many different channels. Counting the items that appear in the budget as 'Foreign Military Financing' and 'Plan Colombia', the U.S. spent approximately $4.5 billion in military aid in 2001, of which $2 billion went to Israel, $1.3 billion went to Egypt, and $1 billion went to Colombia. Since 9/11, Pakistan has received approximately $11.5 billion in direct military aid.
As of 2004, according to Fox News, the U.S. had more than 700 military bases in 130 different countries.
Estimated US foreign military financing and aid by recipient for 2010:
Missile defense.
The Strategic Defense Initiative (SDI) was a proposal by U.S. President Ronald Reagan on March 23, 1983 to use ground and space-based systems to protect the United States from attack by strategic nuclear ballistic missiles, later dubbed "Star Wars". The initiative focused on strategic defense rather than the prior strategic offense doctrine of mutual assured destruction (MAD). Though it was never fully developed or deployed, the research and technologies of SDI paved the way for some anti-ballistic missile systems of today.
In February 2007, the U.S. started formal negotiations with Poland and Czech Republic concerning construction of missile shield installations in those countries for a Ground-Based Midcourse Defense system (in April 2007, 57% of Poles opposed the plan). According to press reports the government of the Czech Republic agreed (while 67% Czechs disagree) to host a missile defense radar on its territory while a base of missile interceptors is supposed to be built in Poland.
Russia threatened to place short-range nuclear missiles on the Russia's border with NATO if the United States refuses to abandon plans to deploy 10 interceptor missiles and a radar in Poland and the Czech Republic. In April 2007, Putin warned of a new Cold War if the Americans deployed the shield in Central Europe. Putin also said that Russia is prepared to abandon its obligations under an Intermediate-Range Nuclear Forces Treaty of 1987 with the United States.
On August 14, 2008, The United States and Poland announced a deal to implement the missile defense system in Polish territory, with a tracking system placed in the Czech Republic. "The fact that this was signed in a period of very difficult crisis in the relations between Russia and the United States over the situation in Georgia shows that, of course, the missile defense system will be deployed not against Iran but against the strategic potential of Russia", Dmitry Rogozin, Russia's NATO envoy, said.
Exporting democracy.
In United States history, critics have charged that presidents have used democracy to justify military intervention abroad. Critics have also charged that the U.S. helped local militaries overthrow democratically elected governments in Iran, Guatemala, and in other instances. Studies have been devoted to the historical success rate of the U.S. in exporting democracy abroad. Some studies of American intervention have been pessimistic about the overall effectiveness of U.S. efforts to encourage democracy in foreign nations. Until recently, scholars have generally agreed with international relations professor Abraham Lowenthal that U.S. attempts to export democracy have been "negligible, often counterproductive, and only occasionally positive." Other studies find U.S. intervention has had mixed results, and another by Hermann and Kegley has found that military interventions have improved democracy in other countries.
Opinion that U.S. intervention does not export democracy.
Professor Paul W. Drake argued that the U.S. first attempted to export democracy in Latin America through intervention from 1912 to 1932. Drake argued that this was contradictory because international law defines intervention as "dictatorial interference in the affairs of another state for the purpose of altering the condition of things." The study suggested that efforts to promote democracy failed because democracy needs to develop out of internal conditions, and can not be forcibly imposed. There was disagreement about what constituted "democracy"; Drake suggested American leaders sometimes defined democracy in a narrow sense of a nation having elections; Drake suggested a broader understanding was needed. Further, there was disagreement about what constituted a "rebellion"; Drake saw a pattern in which the U.S. State Department disapproved of any type of rebellion, even so-called "revolutions", and in some instances rebellions against dictatorships. Historian Walter LaFeber stated, "The world's leading revolutionary nation (the U.S.) in the eighteenth century became the leading protector of the status quo in the twentieth century."
Mesquita and Downs evaluated 35 U.S. interventions from 1945 to 2004 and concluded that in only one case, Colombia, did a "full fledged, stable democracy" develop within ten years following the intervention. Samia Amin Pei argued that nation building in developed countries usually unravelled four to six years after American intervention ended. Pei, based on study of a database on worldwide democracies called "Polity", agreed with Mesquita and Downs that U.S. intervention efforts usually don't produce real democracies, and that most cases result in greater authoritarianism after ten years.
Professor Joshua Muravchik argued U.S. occupation was critical for Axis power democratization after World War II, but America's failure to encourage democracy in the third world "prove ... that U.S. military occupation is not a sufficient condition to make a country democratic." The success of democracy in former Axis countries such as Italy were seen as a result of high national per-capita income, although U.S. protection was seen as a key to stabilization and important for encouraging the transition to democracy. Steven Krasner agreed that there was a link between wealth and democracy; when per-capita incomes of $6,000 were achieved in a democracy, there was little chance of that country ever reverting to an autocracy, according to an analysis of his research in the "Los Angeles Times".
Opinion that U.S. intervention has mixed results.
Tures examined 228 cases of American intervention from 1973 to 2005, using Freedom House data. A plurality of interventions, 96, caused no change in the country's democracy. In 69 instances the country became less democratic after the intervention. In the remaining 63 cases, a country became more democratic. However this does not take into account the direction the country would have gone with no US intervention.
Opinion that U.S. intervention effectively exports democracy.
Hermann and Kegley found that American military interventions designed to protect or promote democracy increased freedom in those countries. Peceny argued that the democracies created after military intervention are still closer to an autocracy than a democracy, quoting Przeworski "while some democracies are more democratic than others, unless offices are contested, no regime should be considered democratic." Therefore, Peceny concludes, it is difficult to know from the Hermann and Kegley study whether U.S. intervention has only produced less repressive autocratic governments or genuine democracies.
Peceny stated that the United States attempted to export democracy in 33 of its 93 20th-century military interventions. Peceny argued that proliberal policies after military intervention had a positive impact on democracy.
Covert actions.
United States foreign policy also includes covert actions to topple foreign governments that have been opposed to the United States. In 1953 the CIA, working with the British government, initiated "Operation Ajax" against the democratically elected Prime Minister of Iran Mohammad Mossadegh who had attempted to nationalize Iran's oil, threatening the interests of the Anglo-Persian Oil Company.
A year later, in "Operation PBSUCCESS", the United States government and the CIA toppled the democratically elected left-wing government of Jacobo Árbenz in Guatemala and installed the military dictator Carlos Castillo Armas. The United Fruit Company lobbied for Árbenz overthrow as his land reforms jeopardized their land holdings in Guatemala, and painted these reforms as a communist threat. The coup triggered a decades long civil war which claimed the lives of 200,000 people.
During the massacre of alleged communists in 1960s Indonesia, the U.S. government provided assistance to the Indonesian military that, according to Bradley Simpson, Director of the Indonesia/East Timor Documentation Project at the National Security Archive, helped facilitate the mass killings. This included the U.S. Embassy in Jakarta supplying Indonesian forces with lists of up to 5,000 names of suspected PKI members, who were subsequently killed in the massacres.
In 1970, the CIA worked with coup-plotters in Chile in the attempted kidnapping of General René Schneider, who was targeted for refusing to participate in a military coup upon the election of Salvador Allende. Schneider was shot in the botched attempt and died three days later. The CIA later paid the group $35,000 for the failed kidnapping.
Human Rights.
The inclusion of Human Rights in U.S. foreign policy had a controversial start. For one thing, human rights driven foreign policy did not originate in the Executive branch but was instead enforced upon it by Congress, starting in the 1970s. Following the Vietnam War, the feeling that U.S. foreign policy had grown apart from traditional American values was seized upon by Senator Donald M. Fraser (D, MI), leading the Subcommittee on International Organizations and Movements, in criticizing Republican Foreign Policy under the Nixon administration. In the early 1970s, Congress concluded the Vietnam War and passed the War Powers Act. As "part of a growing assertiveness by Congress about many aspects of Foreign Policy," Human Rights concerns became a battleground between the Legislative and the Executive branches in the formulation of foreign policy. David Forsythe points to three specific, early examples of Congress interjecting its own thoughts on foreign policy:
These measures were repeatedly used by Congress, with varying success, to affect U.S. foreign policy towards the inclusion of Human Rights concerns. Specific examples include El Salvador, Nicaragua, Guatemala and South Africa. The Executive (from Nixon to Reagan) argued that the Cold War required placing regional security in favor of US interests over any behavioral concerns of national allies. Congress argued the opposite, in favor of distancing the United States from oppressive regimes. Nevertheless, according to historian Daniel Goldhagen, during the last two decades of the Cold War, the number of American client states practicing mass murder outnumbered those of the Soviet Union.
On December 6, 2011, Obama instructed agencies to consider LGBT rights when issuing financial aid to foreign countries. He also criticized Russia's law discriminating against gays, joining other western leaders in the boycott of the 2014 Winter Olympics in Russia.
In June 2014, a Chilean court ruled that the United States played a key role in the murders of Charles Horman and Frank Teruggi, both American citizens, shortly after the 1973 Chilean coup d'état.
War on Drugs.
United States foreign policy is influenced by the efforts of the U.S. government to control imports of illicit drugs, including cocaine, heroin, methamphetamine, and cannabis. This is especially true in Latin America, a focus for the U.S. War on Drugs. Those efforts date back to at least 1880, when the U.S. and China completed an agreement that prohibited the shipment of opium between the two countries.
Over a century later, the Foreign Relations Authorization Act requires the President to identify the major drug transit or major illicit drug-producing countries. In September 2005, the following countries were identified: Bahamas, Bolivia, Brazil, Burma, Colombia, Dominican Republic, Ecuador, Guatemala, Haiti, India, Jamaica, Laos, Mexico, Nigeria, Pakistan, Panama, Paraguay, Peru and Venezuela. Two of these, Burma and Venezuela are countries that the U.S. considers to have failed to adhere to their obligations under international counternarcotics agreements during the previous 12 months. Notably absent from the 2005 list were Afghanistan, the People's Republic of China and Vietnam; Canada was also omitted in spite of evidence that criminal groups there are increasingly involved in the production of MDMA destined for the United States and that large-scale cross-border trafficking of Canadian-grown cannabis continues. The U.S. believes that the Netherlands are successfully countering the production and flow of MDMA to the U.S.
Criticism.
Critics from the left cite episodes that undercut leftist governments or showed support for Israel. Others cite human rights abuses and violations of international law. Critics have charged that the U.S. presidents have used democracy to justify military intervention abroad. It was also noted that the U.S. overthrew democratically elected governments in Iran, Guatemala, and in other instances. Noam Chomsky, a vociferous critic of U.S. foreign policy, argues that "in both cases the consequences reach to the present" and that Guatemala in particular "remains one of the world’s worst horror chambers."
Studies have been devoted to the historical success rate of the U.S. in exporting democracy abroad. Some studies of American intervention have been pessimistic about the overall effectiveness of U.S. efforts to encourage democracy in foreign nations. Some scholars have generally agreed with international relations professor Abraham Lowenthal that U.S. attempts to export democracy have been "negligible, often counterproductive, and only occasionally positive." Other studies find U.S. intervention has had mixed results, and another by Hermann and Kegley has found that military interventions have improved democracy in other countries. A 2013 global poll in 68 countries with 66,000 respondents by Win/Gallup found that the U.S. is perceived as the biggest threat to world peace.
Support.
Regarding support for certain anti-Communist dictatorships during the Cold War, a response is that they were seen as a necessary evil, with the alternatives even worse Communist or fundamentalist dictatorships. David Schmitz says this policy did not serve U.S. interests. Friendly tyrants resisted necessary reforms and destroyed the political center (though not in South Korea), while the 'realist' policy of coddling dictators brought a backlash among foreign populations with long memories.
Many democracies have voluntary military ties with United States. See NATO, ANZUS, Treaty of Mutual Cooperation and Security between the United States and Japan, Mutual Defense Treaty with South Korea, and Major non-NATO ally. Those nations with military alliances with the U.S. can spend less on the military since they can count on U.S. protection. This may give a false impression that the U.S. is less peaceful than those nations.
Research on the democratic peace theory has generally found that democracies, including the United States, have not made war on one another. There have been U.S. support for coups against some democracies, but for example Spencer R. Weart argues that part of the explanation was the perception, correct or not, that these states were turning into Communist dictatorships. Also important was the role of rarely transparent United States government agencies, who sometimes mislead or did not fully implement the decisions of elected civilian leaders.
Empirical studies (see democide) have found that democracies, including the United States, have killed much fewer civilians than dictatorships. Media may be biased against the U.S. regarding reporting human rights violations. Studies have found that "The New York Times" coverage of worldwide human rights violations predominantly focuses on the human rights violations in nations where there is clear U.S. involvement, while having relatively little coverage of the human rights violations in other nations. For example, the bloodiest war in recent time, involving eight nations and killing millions of civilians, was the Second Congo War, which was almost completely ignored by the media.
Niall Ferguson argues that the U.S. is incorrectly blamed for all the human rights violations in nations they have supported. He writes that it is generally agreed that Guatemala was the worst of the US-backed regimes during the Cold War. However, the U.S. cannot credibly be blamed for all the 200,000 deaths during the long Guatemalan Civil War. The U.S. Intelligence Oversight Board writes that military aid was cut for long periods because of such violations, that the U.S. helped stop a coup in 1993, and that efforts were made to improve the conduct of the security services.
Today the U.S. states that democratic nations best support U.S. national interests. According to the U.S. State Department, "Democracy is the one national interest that helps to secure all the others. Democratically governed nations are more likely to secure the peace, deter aggression, expand open markets, promote economic development, protect American citizens, combat international terrorism and crime, uphold human and worker rights, avoid humanitarian crises and refugee flows, improve the global environment, and protect human health." According to former U.S. President Bill Clinton, "Ultimately, the best strategy to ensure our security and to build a durable peace is to support the advance of democracy elsewhere. Democracies don't attack each other." In one view mentioned by the U.S. State Department, democracy is also good for business. Countries that embrace political reforms are also more likely to pursue economic reforms that improve the productivity of businesses. Accordingly, since the mid-1980s, under President Ronald Reagan, there has been an increase in levels of foreign direct investment going to emerging market democracies relative to countries that have not undertaken political reforms. Leaked cables in 2010 suggested that the "dark shadow of terrorism still dominates the United States' relations with the world".
The United States officially maintains that it supports democracy and human rights through several tools Examples of these tools are as follows:

</doc>
<doc id="7565" url="http://en.wikipedia.org/wiki?curid=7565" title="Christmas in Poland">
Christmas in Poland

Christmas in Poland is a major annual celebration, as in most countries of the Christian world. The observance of Christmas developed gradually over the centuries, beginning in ancient times; combining old pagan customs with the religious ones introduced after the Christianization of Poland by the Catholic Church. Later influences include mutual permeating of local traditions and various folk cultures. Christmas trees are decorated and lit in family rooms on the day of Christmas Eve. Other trees are placed in most public areas and outside churches. Christmas is called "Boże Narodzenie" in Polish (literally 'God's Birth').
Advent.
Among the special tasks carried out in private homes during Advent (a time of waiting for the celebration of the Nativity of Jesus) is the baking of the Christmas piernik (gingerbread), and the making of Christmas decorations. Pierniki are made in a variety of shapes, including hearts, animals, and St. Nicholas figures. Unlike in non-Catholic Christian countries, St. Nicholas does not play a major role in Polish Christmas, but instead, is celebrated on his Saint feast day of December 6.
Traditionally, the Christmas trees are decorated with glass baubles, garlands and many homemade ornaments including painted eggshells, shiny red apples, walnuts, wrapped chocolate shapes, candles, etc. They are lit on Christmas Eve before Wigilia. At the top of each tree there is a star or a glittering tree topper. In many homes, sparklers are hung on the branches of the trees for wintery ambiance. Sometimes the trees are left standing until February 2, the feast day of St. Mary of the Candle of Lightning.
During Advent and all the way until Epiphany, or the baptism of Jesus (day of January 6), the "gwiazdory", or the star carriers walk through the villages. Some of them sing carols; others recite verses or put on "szopki" (puppet shows), or "herody" (nativity scenes). The last two customs are inspired by the traditional manger scenes or "Jaselka" (crib). One tradition unique to Poland is the sharing of the "opłatek", a thin wafer into which a holy picture is pressed. In the old days people carried these wafers from house to house wishing their neighbors a Merry Christmas. Nowadays, opłatek is mostly shared with members of the family and immediate neighbors before the Christmas Eve supper (Wigilia in the Polish language). As each person shares pieces of the wafer with another, they are supposed to forgive each other any hurts that have occurred over the past year and wish them happiness in the coming year.
"Wigilia", the Christmas Eve supper.
In Poland, Christmas Eve is a day first of fasting, then of feasting. The Wigilia feast begins at the appearance of the first star. There is no red meat served but fish, usually carp. The supper, which includes many traditional dishes and desserts, can sometimes last for over two hours. It is followed by the exchange of gifts. The next day, the Christmas Day, is often spent visiting friends. In Polish tradition, people combine religion and family closeness at Christmas. Although gift-giving plays a major role in the rituals, emphasis is placed more on the making of special foods and decorations.
On the night of Christmas Eve, so important is the appearance of the first star in remembrance of the Star of Bethlehem, that it has been given an affectionate name of "the little star" or Gwiazdka (the female counterpart of St. Nicholas). On that evening, children watch the sky anxiously hoping to be the first to cry out, "The star has come!" Only after it appears, the family members sit down to a dinner table. 
According to tradition, bits of hay are spread beneath the tablecloth as a reminder that Christ was born in a manger. Others partake in the practice of placing money under the table cloth for each guest, in order to wish for prosperity in the coming year. Some practice the superstition that an even number of people must be seated around the table. In many homes an empty place setting is symbolically left at the table for the Baby Jesus or, for a lonely wanderer who may be in need of food, or if a deceased relative should come and would like to share in the meal. 
The supper begins with the breaking of the opłatek. Everyone at the table breaks off a piece and eats it as a symbol of their unity with Christ. They then share a piece with each family member. A tradition exists among some families to serve twelve different dishes at Wigilia symbolizing the Twelve Apostles, or perhaps, an odd number of dishes for good luck (usually five, seven, or nine). 
A traditional Wigilia supper in Poland includes fried carp and barszcz (beetroot soup) with uszka (ravioli). Carp provides a main component of the Christmas Eve meal across Poland; carp fillet, carp in aspic etc. Universal Polish Christmas foods are pierogi as well as some herring dishes, and for desert, makowiec or noodles with poppy seed. Often, there is a compote of dry fruits for a drink. 
The remainder of the evening is given to stories and songs around the Christmas tree. In some areas of the country, children are taught that "The Little Star" brings the gifts. As presents are unwrapped, carollers may walk from house to house receiving treats along the way.
Christmas Eve ends with Pasterka, the Midnight Mass at the local church. The tradition commemorates the arrival of the Three Wise Men to Bethlehem and their paying of respect and bearing witness to the new born Messiah. The custom of Christmas night liturgy was introduced in the Christian churches after the second half of the 5th century. In Poland that custom arrived together with the coming of Christianity. The next day (December 25) begins with the early morning mass followed by daytime masses. According to scripture, the Christmas Day masses are interchangeable allowing for greater flexibility in choosing the religious services by individual parishioners.
Kolędy, the Christmas carols.
Christmas carols are not celebrated in Poland until during-and-after the Christmas Vigil Mass called Pasterka held between 24 and 25 of December. The Christmas season often runs until February 2. The early hymns sung in Catholic church were brought to Poland by the Franciscan Brothers in the Middle Ages. The early Christmas music was Latin in origin. When the Polish words and melodies started to become popular, including many new secular pastorals (pastoralka, or shepherd's songs), they were not written down originally, but rather taught among people by heart. Notably, the song "Bóg się rodzi" ("God Is Being Born") with lyrics written by Franciszek Karpiński in 1792 became the Christmas hymn of Poland already in the court of King Stefan Batory. Many of the early Polish carols were collected in 1838 by Rev. Hioduszewki in a book called "Pastorałki i Kolędy z Melodiami" (Pastorals and Carols with Melodies).

</doc>
