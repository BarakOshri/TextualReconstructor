<doc id="5329" url="http://en.wikipedia.org/wiki?curid=5329" title="History of Chad">
History of Chad

Chad (; ), officially the Republic of Chad, is a landlocked country in Central Africa. It borders Libya to the north, Sudan to the east, the Central African Republic to the south, Cameroon and Nigeria to the southwest, and Niger to the west. Due to its distance from the sea and its largely desert climate, the country is sometimes referred to as the .
Prehistory.
The territory now known as Chad possesses some of the richest archaeological sites in Africa. A hominid skull was found by Michel Brunet in 2002, in Borkou, that is more than 7 million years old, the oldest discovered anywhere in the world; it has been given the name Sahelanthropus tchadensis. In 1996 Michel Brunet had unearthed a hominid jaw which he named Australopithecus bahrelghazali, and unofficially dubbed Abel. It was dated using Beryllium based Radiometric dating as living circa. 3.6 million years ago. 
During the 7th millennium BC, the northern half of Chad was part of a broad expanse of land, stretching from the Indus River in the east to the Atlantic Ocean in the west, in which ecological conditions favored early human settlement. Rock art of the "Round Head" style, found in the Ennedi region, has been dated to before the 7th millennium BC and, because of the tools with which the rocks were carved and the scenes they depict, may represent the oldest evidence in the Sahara of Neolithic industries. Many of the pottery-making and Neolithic activities in Ennedi date back further than any of those of the Nile Valley to the east.
In the prehistoric period, Chad was much wetter than it is today, as evidenced by large game animals depicted in rock paintings in the Tibesti and Borkou regions.
Recent linguistic research suggests that all of Africa's major language groupings south of the Sahara Desert (except Khoisan, which is not considered a valid genetic grouping anyway), i. e. the Afro-Asiatic, Nilo-Saharan and Niger–Congo phyla, originated in prehistoric times in a narrow band between Lake Chad and the Nile Valley. The origins of Chad's peoples, however, remain unclear. Several of the proven archaeological sites have been only partially studied, and other sites of great potential have yet to be mapped.
Era of Empires (AD 900–1900).
Toward the end of the 1st millennium AD, the formation of states began across central Chad in the sahelian zone between the desert and the savanna. For almost the next 1,000 years, these states, their relations with each other, and their effects on the peoples who lived in stateless societies along their peripheries dominated Chad's political history. Recent research suggests that indigenous Africans founded most of these states, not migrating Arabic-speaking groups, as was believed previously. Nonetheless, immigrants, Arabic-speaking or otherwise, played a significant role, along with Islam, in the formation and early evolution of these states.
Most states began as kingdoms, in which the king was considered divine and endowed with temporal and spiritual powers. All states were militaristic (or they did not survive long), but none was able to expand far into southern Chad, where forests and the tsetse fly complicated the use of cavalry. Control over the trans-Saharan trade routes that passed through the region formed the economic basis of these kingdoms. Although many states rose and fell, the most important and durable of the empires were Kanem-Bornu, Baguirmi, and Ouaddai, according to most written sources (mainly court chronicles and writings of Arab traders and travelers).
Kanem-Bornu.
The Kanem Empire originated in the 9th century AD to the northeast of Lake Chad. Historians agree that the leaders of the new state were ancestors of the Kanembu people. Toward the end of the 11th century the Sayfawa king (or "mai", the title of the Sayfawa rulers) Hummay, converted to Islam. In the following century the Sayfawa rulers expanded southward into Kanem, where was to rise their first capital, Njimi. Kanem's expansion peaked during the long and energetic reign of Mai Dunama Dabbalemi (c. 1221–1259).
By the end of the 14th century, internal struggles and external attacks had torn Kanem apart. Finally, around 1396 the Bulala invaders forced "Mai" Umar Idrismi to abandon Njimi and move the Kanembu people to Bornu on the western edge of Lake Chad. Over time, the intermarriage of the Kanembu and Bornu peoples created a new people and language, the Kanuri, and founded a new capital, Ngazargamu.
Kanem-Bornu peaked during the reign of the outstanding statesman "Mai" Idris Aluma (c. 1571–1603). Aluma is remembered for his military skills, administrative reforms, and Islamic piety. The administrative reforms and military brilliance of Aluma sustained the empire until the mid-17th century, when its power began to fade. By the early 19th century, Kanem-Bornu was clearly an empire in decline, and in 1808 Fulani warriors conquered Ngazargamu. Bornu survived, but the Sayfawa dynasty ended in 1846 and the Empire itself fell in 1893.
Baguirmi and Ouaddai.
In addition to Kanem-Bornu, two other states in the region, Baguirmi and Ouaddai, achieved historical prominence. Baguirmi emerged to the southeast of Kanem-Bornu in the 16th century. Islam was adopted, and the state became a sultanate. Absorbed into Kanem-Bornu, Baguirmi broke free later in the 17th century, only to be returned to tributary status in the mid-18th century. Early in the 19th century, Baguirmi fell into decay and was threatened militarily by the nearby kingdom of Ouaddai. Although Baguirmi resisted, it accepted tributary status in order to obtain help from Ouaddai in putting down internal dissension. When the capital was burned in 1893, the sultan sought and received protectorate status from the French.
Located northeast of Baguirmi, Ouaddai was a non-Muslim kingdom that emerged in the 16th century as an offshoot of the state of Darfur (in present-day Sudan). Early in the 17th century, groups in the region rallied to Abd al-Karim Sabun, who overthrew the ruling Tunjur group, transforming Ouaddai in an Islamic sultanate. During much of the 18th century, Ouaddai resisted reincorporation into Darfur.
In about 1804, under the rule of Sabun, the sultanate began to expand its power. A new trade route north was discovered, and Sabun outfitted royal caravans to take advantage of it. He began minting his own coinage and imported chain mail, firearms, and military advisers from North Africa. Sabun's successors were less able than he, and Darfur took advantage of a disputed political succession in 1838 to put its own candidate in power. This tactic backfired when Darfur's choice, Muhammad Sharif, rejected Darfur and asserted his own authority. In doing so, he gained acceptance from Ouaddai's various factions and went on to become Ouaddai's ablest ruler. Sharif eventually established Ouaddai's hegemony over Baguirmi and kingdoms as far away as the Chari River. The Ouaddai opposed French domination until well into the 20th century.
Colonialism (1900–40).
The French first penetrated Chad in 1891, establishing their authority through military expeditions primarily against the Muslim kingdoms. The decisive colonial battle for Chad was fought on April 22, 1900 at Battle of Kousséri between forces of French Major Amédée-François Lamy and forces of the Sudanese warlord Rabih az-Zubayr. Both leaders were killed in the battle.
In 1905, administrative responsibility for Chad was placed under a governor-general stationed at Brazzaville, capital of French Equatorial Africa (AEF). Chad did not have a separate colonial status until 1920, when it was placed under a lieutenant-governor stationed in Fort-Lamy (today N'Djamena).
Two fundamental themes dominated Chad's colonial experience with the French: an absence of policies designed to unify the territory and an exceptionally slow pace of modernization. In the French scale of priorities, the colony of Chad ranked near the bottom, and the French came to perceive Chad primarily as a source of raw cotton and untrained labour to be used in the more productive colonies to the south.
Throughout the colonial period, large areas of Chad were never governed effectively: in the huge BET Prefecture, the handful of French military administrators usually left the people alone, and in central Chad, French rule was only slightly more substantive. Truly speaking, France managed to govern effectively only the south.
Decolonization (1940–60).
During World War II, Chad was the first French colony to rejoin the Allies (August 26, 1940), after the defeat of France by Germany. Under the administration of Félix Éboué, France's first black colonial governor, a military column, commanded by Colonel Philippe Leclerc de Hauteclocque, and including two battalions of Sara troops, moved north from N'Djamena (then Fort Lamy) to engage Axis forces in Libya, where, in partnership with the British Army's Long Range Desert Group, they captured Kufra.
After the war ended local parties started to develop in Chad. The first to be born was the radical Chadian Progressive Party (PPT) in February 1947, initially headed by Panamanian born Gabriel Lisette, but from 1959 headed by François Tombalbaye. The more conservative Chadian Democratic Union (UDT) was founded in November 1947 and represented French commercial interests and a bloc of traditional leaders composed primarily of Muslim and Ouaddaïan nobility. The confrontation between the PPT and UDT was more than simply ideological; it represented different regional identities, with the PPT representing the Christian and animist south and the UDT the Islamic north. 
The PPT won the May 1957 pre-independence elections thanks to a greatly expanded franchise, and Lisette led the government of the Territorial Assembly until he lost a confidence vote on 11 February 1959. After a referendum on territorial autonomy on 28 September 1958, French Equatorial Africa was dissolved, and its four constituent states – Gabon, Congo (Brazzaville), the Central African Republic, and Chad became autonomous members of the French Community from 28 November 1958. Following Lisette's fall in February 1959 the opposition leaders Gontchome Sahoulba and Ahmed Koulamallah could not form a stable government, so the PPT was again asked to form an administration - which it did under the leadership of François Tombalbaye on 26 March 1959. On 12 July 1960 France agreed to Chad becoming fully independent. On 11 August 1960, Chad became an independent country and François Tombalbaye became its first President.
The Tombalbaye era (1960–75).
One of the most prominent aspects of Tombalbaye's rule to prove itself was his authoritarianism and distrust of democracy. Already in January 1962 he banned all political parties except his own PPT, and started immediately concentrating all power in his own hands. His treatment of opponents, real or imagined, was extremely harsh, filling the prisons with thousands of political prisoners.
What was even worse was his constant discrimination against the central and northern regions of Chad, where the southern Chadian administrators came to be perceived as arrogant and incompetent. This resentment at last exploded in a tax revolt on November 1, 1965, in the Guéra Prefecture, causing 500 deaths. The year after saw the birth in Sudan of the National Liberation Front of Chad (FROLINAT), created to militarily oust Tombalbaye and the Southern dominance. It was the start of a bloody civil war.
Tombalbaye resorted to calling in French troops; while moderately successful, they were not fully able to quell the insurgency. Proving more fortunate was his choice to break with the French and seek friendly ties with Libyan president Gaddafi, taking away the rebels' principal source of supplies.
But while he had reported some success against the rebels, Tombalbaye started behaving more and more irrationally and brutally, continuously eroding his consensus among the southern elites, which dominated all key positions in the army, the civil service and the ruling party. As a consequence on April 13, 1975, several units of N'Djamena's gendarmerie killed Tombalbaye during a coup.
Military rule (1975–78).
The coup d'état that terminated Tombalbaye's government received an enthusiastic response in N'Djamena. The southerner General Félix Malloum emerged early as the chairman of the new "junta".
The new military leaders were unable to retain for long the popularity that they had gained through their overthrow of Tombalbaye. Malloum proved himself unable to cope with the FROLINAT and at the end decided his only chance was in coopting some of the rebels: in 1978 he allied himself with the insurgent leader Hissène Habré, who entered the government as prime minister.
Civil war (1979-82).
Internal dissent within the government led Prime Minister Habré to send his forces against Malloum's national army in the capital in February 1979. Malloum was ousted from the presidency, but the resulting civil war amongst the 11 emergent factions was so widespread that it rendered the central government largely irrelevant. At that point, other African governments decided to intervene.
A series of four international conferences held first under Nigerian and then Organization of African Unity (OAU) sponsorship attempted to bring the Chadian factions together. At the fourth conference, held in Lagos, Nigeria, in August 1979, the Lagos Accord was signed. This accord established a transitional government pending national elections. In November 1979, the Transitional Government of National Unity (GUNT) was created with a mandate to govern for 18 months. Goukouni Oueddei, a northerner, was named President; Colonel Kamougué, a southerner, Vice President; and Habré, Minister of Defense. This coalition proved fragile; in January 1980, fighting broke out again between Goukouni's and Habré's forces. With assistance from Libya, Goukouni regained control of the capital and other urban centers by year’s end. However, Goukouni’s January 1981 statement that Chad and Libya had agreed to work for the realization of complete unity between the two countries generated intense international pressure and Goukouni's subsequent call for the complete withdrawal of external forces.
The Habré era (1982–90).
Libya's partial withdrawal to the Aozou Strip in northern Chad cleared the way for Habré's forces to enter N’Djamena in June. French troops and an OAU peacekeeping force of 3,500 Nigerian, Senegalese, and Zairian troops (partially funded by the United States) remained neutral during the conflict.
Habré continued to face armed opposition on various fronts, and was brutal in his repression of suspected opponents, massacring and torturing many during his rule. In the summer of 1983, GUNT forces launched an offensive against government positions in northern and eastern Chad with heavy Libyan support. In response to Libya's direct intervention, French and Zairian forces intervened to defend Habré, pushing Libyan and rebel forces north of the 16th parallel. In September 1984, the French and the Libyan governments announced an agreement for the mutual withdrawal of their forces from Chad. By the end of the year, all French and Zairian troops were withdrawn. Libya did not honor the withdrawal accord, and its forces continued to occupy the northern third of Chad.
Rebel commando groups (Codos) in southern Chad were broken up by government massacres in 1984. In 1985 Habré briefly reconciled with some of his opponents, including the Democratic Front of Chad (FDT) and the Coordinating Action Committee of the Democratic Revolutionary Council. Goukouni also began to rally toward Habré, and with his support Habré successfully expelled Libyan forces from most of Chadian territory. A cease-fire between Chad and Libya held from 1987 to 1988, and negotiations over the next several years led to the 1994 International Court of Justice decision granting Chad sovereignty over the Aouzou strip, effectively ending Libyan occupation.
The Déby era.
Rise to power.
However, rivalry between Hadjerai, Zaghawa and Gorane groups within the government grew in the late 1980s. In April 1989, Idriss Déby, one of Habré's leading generals and a Zaghawa, defected and fled to Darfur in Sudan, from which he mounted a Zaghawa-supported series of attacks on Habré (a Gorane). In December 1990, with Libyan assistance and no opposition from French troops stationed in Chad, Déby’s forces successfully marched on N’Djamena. After 3 months of provisional government, Déby’s Patriotic Salvation Movement (MPS) approved a national charter on February 28, 1991, with Déby as president.
During the next two years, Déby faced at least two coup attempts. Government forces clashed violently with rebel forces, including the Movement for Democracy and Development, MDD, National Revival Committee for Peace and Democracy (CSNPD), Chadian National Front (FNT) and the Western Armed Forces (FAO), near Lake Chad and in southern regions of the country. Earlier French demands for the country to hold a National Conference resulted in the gathering of 750 delegates representing political parties (which were legalized in 1992), the government, trade unions and the army to discuss the creation of a pluralist democratic regime.
However, unrest continued, sparked in part by large-scale killings of civilians in southern Chad. The CSNPD, led by Kette Moise and other southern groups entered into a peace agreement with government forces in 1994, which later broke down. Two new groups, the Armed Forces for a Federal Republic (FARF) led by former Kette ally Laokein Barde and the Democratic Front for Renewal (FDR), and a reformulated MDD clashed with government forces from 1994 to 1995.
Multiparty elections.
Talks with political opponents in early 1996 did not go well, but Déby announced his intent to hold presidential elections in June. Déby won the country’s first multi-party presidential elections with support in the second round from opposition leader Kebzabo, defeating General Kamougue (leader of the 1975 coup against Tombalbaye). Déby’s MPS party won 63 of 125 seats in the January 1997 legislative elections. International observers noted numerous serious irregularities in presidential and legislative election proceedings.
By mid-1997 the government signed peace deals with FARF and the MDD leadership and succeeded in cutting off the groups from their rear bases in the Central African Republic and Cameroon. Agreements also were struck with rebels from the National Front of Chad (FNT) and Movement for Social Justice and Democracy in October 1997. However, peace was short-lived, as FARF rebels clashed with government soldiers, finally surrendering to government forces in May 1998. Barde was killed in the fighting, as were hundreds of other southerners, most civilians.
Since October 1998, Chadian Movement for Justice and Democracy (MDJT) rebels, led by Youssuf Togoimi until his death in September 2002, have skirmished with government troops in the Tibesti region, resulting in hundreds of civilian, government, and rebel casualties, but little ground won or lost. No active armed opposition has emerged in other parts of Chad, although Kette Moise, following senior postings at the Ministry of Interior, mounted a smallscale local operation near Moundou which was quickly and violently suppressed by government forces in late 2000.
Déby, in the mid-1990s, gradually restored basic functions of government and entered into agreements with the World Bank and IMF to carry out substantial economic reforms. Oil exploitation in the southern Doba region began in June 2000, with World Bank Board approval to finance a small portion of a project, the Chad-Cameroon Petroleum Development Project, aimed at transport of Chadian crude through a 1000-km. buried pipeline through Cameroon to the Gulf of Guinea. The project established unique mechanisms for World Bank, private sector, government, and civil society collaboration to guarantee that future oil revenues benefit local populations and result in poverty alleviation. Success of the project depended on to ensure that all parties keep their commitments. These "unique" mechanisms for monitoring and revenue management have faced intense criticism from the beginning. Debt relief was accorded to Chad in May 2001.
Déby won a flawed 63% first-round victory in May 2001 presidential elections after legislative elections were postponed until spring 2002. Having accused the government of fraud, six opposition leaders were arrested (twice) and one opposition party activist was killed following the announcement of election results. However, despite claims of government corruption, favoritism of Zaghawas, and abuses by the security forces, opposition party and labor union calls for general strikes and more active demonstrations against the government have been unsuccessful. Despite movement toward democratic reform, power remains in the hands of a northern ethnic oligarchy.
In 2003, Chad began receiving refugees from the Darfur region of western Sudan. More than 200,000 refugees fled the fighting between two rebel groups and government-supported militias known as Janjaweed. A number of border incidents led to the Chadian-Sudanese War.
War in the East.
The war started on December 23, 2005, when the government of Chad declared a state of war with Sudan and called for the citizens of Chad to mobilize themselves against the "common enemy," which the Chadian government sees as the Rally for Democracy and Liberty (RDL) militants, Chadian rebels, backed by the Sudanese government, and Sudanese militiamen. Militants have attacked villages and towns in eastern Chad, stealing cattle, murdering citizens, and burning houses. Over 200,000 refugees from the Darfur region of northwestern Sudan currently claim asylum in eastern Chad. Chadian president Idriss Déby accuses Sudanese President Omar Hasan Ahmad al-Bashir of trying to "destabilize our country, to drive our people into misery, to create disorder and export the war from Darfur to Chad."
An attack on the Chadian town of Adre near the Sudanese border led to the deaths of either one hundred rebels, as every news source other than CNN has reported, or three hundred rebels. The Sudanese government was blamed for the attack, which was the second in the region in three days, but Sudanese foreign ministry spokesman Jamal Mohammed Ibrahim denies any Sudanese involvement, "We are not for any escalation with Chad. We technically deny involvement in Chadian internal affairs." This attack was the final straw that led to the declaration of war by Chad and the alleged deployment of the Chadian airforce into Sudanese airspace, which the Chadian government denies.
An attack on N'Djamena was defeated on April 13, 2006 in the Battle of N'Djamena. The President on national radio stated that the situation was under control, but residents, diplomats and journalists reportedly heard shots of weapons fire.
On November 25, 2006, rebels captured the eastern town of Abeche, capital of the Ouaddaï Region and center for humanitarian aid to the Darfur region in Sudan. On the same day, a separate rebel group Rally of Democratic Forces had captured Biltine. On November 26, 2006, the Chadian government claimed to have recaptured both towns, although rebels still claimed control of Biltine. Government buildings and humanitarian aid offices in Abeche were said to have been looted. The Chadian government denied a warning issued by the French Embassy in N'Djamena that a group of rebels was making its way through the Batha Prefecture in central Chad. Chad insists that both rebel groups are supported by the Sudanese government.
Rebel attack on Ndjamena.
On Friday, February 1, 2008, rebels, an opposition alliance of leaders Mahamat Nouri, a former defense minister, and Timane Erdimi, a nephew of Idriss Déby who was his chief of staff, attacked the Chadian capital of Ndjamena - even surrounding the Presidential Palace. But Idris Deby with government troops fought back. French forces flew in ammunition for Chadian government troops but took no active part in the fighting. UN has said that up to 20,000 people left the region, taking refuge in nearby Cameroon and Nigeria. Hundreds of people were killed, mostly civilians. The rebels accuse Deby of corruption and embezzling millions in oil revenue. While many Chadians may share that assessment, the uprising appears to be a power struggle within the elite that has long controlled Chad. The French government believes that the opposition has regrouped east of the capital. Déby has blamed Sudan for the current unrest in Chad.
International orphanage scandal.
Nearly 100 children at the center of an international scandal that left them stranded at an orphanage in remote eastern Chad returned home after nearly five months March 14, 2008. The 97 children were taken from their homes in October 2007 by a then-obscure French charity, Zoé's Ark, which claimed they were orphans from Sudan's war-torn Darfur region.

</doc>
<doc id="5330" url="http://en.wikipedia.org/wiki?curid=5330" title="Geography of Chad">
Geography of Chad

Chad is a one of the 48 land-locked countries in the world and is located in North Central Africa measuring , nearly twice the size of France. Most of its ethnically and linguistically diverse population lives in the south, with densities ranging from 54 persons per square kilometers in the Logone River basin to 0.1 persons in the northern B.E.T. desert region, which itself is larger than France. The capital city of N'Djaména, situated at the confluence of the Chari and Logone Rivers, is cosmopolitan in nature, with a current population in excess of 700,000 people.
Chad has four bioclimatic zones. The northernmost Saharan zone averages less than of rainfall annually. The sparse human population is largely nomadic, with some livestock, mostly small ruminants and camels. The central Sahelian zone receives between rainfall and has vegetation ranging from grass/shrub steppe to thorny, open savanna. The southern zone, often referred to as the Sudanian zone, receives between , with woodland savanna and deciduous forests for vegetation. Rainfall in the Guinea zone, located in Chad's southwestern tip, ranges between .
The country's topography is generally flat, with the elevation gradually rising as one moves north and east away from Lake Chad. The highest point in Chad is Emi Koussi, a mountain that rises in the northern Tibesti Mountains. The Ennedi Plateau and the Ouaddaï highlands in the east complete the image of a gradually sloping basin, which descends towards Lake Chad. There are also central highlands in the Guera region rising to .
Lake Chad is the second largest lake in west Africa and is one of the most important wetlands on the continent. Home to 120 species of fish and at least that many species of birds, the lake has shrunk dramatically in the last four decades due to increased water usage from an expanding population and low rainfall. Bordered by Chad, Niger, Nigeria, and Cameroon, Lake Chad currently covers only 1350 square kilometers, down from 25,000 square kilometers in 1963. The Chari and Logone Rivers, both of which originate in the Central African Republic and flow northward, provide most of the surface water entering Lake Chad.
Geographical setting.
Located in north-central Africa, Chad stretches for about 1,800 kilometers from its northernmost point to its southern boundary. Except in the far northwest and south, where its borders converge, Chad's average width is about 800 kilometers. Its area of 1,284,000 square kilometers is roughly equal to the combined areas of Idaho, Wyoming, Utah, Nevada, and Arizona. Chad's neighbors include Libya to the north, Niger and Nigeria to the west, Sudan to the east, Central African Republic to the south, and Cameroon to the southwest.
Chad exhibits two striking geographical characteristics. First, the country is landlocked. N'Djamena, the capital, is located more than 1,100 kilometers northeast of the Atlantic Ocean; Abéché, a major city in the east, lies 2,650 kilometers from the Red Sea; and Faya-Largeau, a much smaller but strategically important center in the north, is in the middle of the Sahara Desert, 1,550 kilometers from the Mediterranean Sea. These vast distances from the sea have had a profound impact on Chad's historical and contemporary development. 
The second noteworthy characteristic is that the country borders on very different parts of the African continent: North Africa, with its Islamic culture and economic orientation toward the Mediterranean Basin and West Africa, with its diverse religions and cultures and its history of highly developed states and regional economies; 
Chad also borders Northeast Africa, oriented toward the Nile Valley and the Red Sea region - and Central or Equatorial Africa, some of whose people have retained classical African religions while others have adopted Christianity, and whose economies were part of the great Congo River system. Although much of Chad's distinctiveness comes from this diversity of influences, since independence the diversity has also been an obstacle to the creation of a national identity.
Land.
Although Chadian society is economically, socially, and culturally fragmented, the country's geography is unified by the Lake Chad Basin. Once a huge inland sea (the Pale-Chadian Sea) whose only remnant is shallow Lake Chad, this vast depression extends west into Nigeria and Niger. The larger, northern portion of the basin is bounded within Chad by the Tibesti Mountains in the northwest, the Ennedi Plateau in the northeast, the Ouaddaï Highlands in the east along the border with Sudan, the Guéra Massif in central Chad, and the Mandara Mountains along Chad's southwestern border with Cameroon. The smaller, southern part of the basin falls almost exclusively in Chad. It is delimited in the north by the Guéra Massif, in the south by highlands 250 kilometers south of the border with Central African Republic, and in the southwest by the Mandara Mountains.
Lake Chad, located in the southwestern part of the basin at an altitude of 282 meters, surprisingly does not mark the basin's lowest point; instead, this is found in the Bodele and Djourab regions in the north-central and northeastern parts of the country, respectively. This oddity arises because the great stationary dunes (ergs) of the Kanem region create a dam, preventing lake waters from flowing to the basin's lowest point. At various times in the past, and as late as the 1870s, the Bahr el Ghazal Depression, which extends from the northeastern part of the lake to the Djourab, acted as an overflow canal; since independence, climatic conditions have made overflows impossible.
North and northeast of Lake Chad, the basin extends for more than 800 kilometers, passing through regions characterized by great rolling dunes separated by very deep depressions. Although vegetation holds the dunes in place in the Kanem region, farther north they are bare and have a fluid, rippling character. From its low point in the Djourab, the basin then rises to the plateaus and peaks of the Tibesti Mountains in the north. The summit of this formation—as well as the highest point in the Sahara Desert—is Emi Koussi, a dormant volcano that reaches 3,414 meters above sea level.
The basin's northeastern limit is the Ennedi Plateau, whose limestone bed rises in steps etched by erosion. East of the lake, the basin rises gradually to the Ouaddaï Highlands, which mark Chad's eastern border and also divide the Chad and Nile watersheds. These highland areas are part of the East Saharan montane xeric woodlands ecoregion.
Southeast of Lake Chad, the regular contours of the terrain are broken by the Guéra Massif, which divides the basin into its northern and southern parts. South of the lake lie the floodplains of the Chari and Logone rivers, much of which are inundated during the rainy season. Farther south, the basin floor slopes upward, forming a series of low sand and clay plateaus, called koros, which eventually climb to 615 meters above sea level. South of the Chadian border, the koros divide the Lake Chad Basin from the Ubangi-Zaire river system.
Water systems.
Permanent streams do not exist in northern or central Chad. Following infrequent rains in the Ennedi Plateau and Ouaddaï Highlands, water may flow through depressions called enneris and wadis. Often the result of flash floods, such streams usually dry out within a few days as the remaining puddles seep into the sandy clay soil. The most important of these streams is the Batha, which in the rainy season carries water west from the Ouaddaï Highlands and the Guéra Massif to Lake Fitri.
Chad's major rivers are the Chari and the Logone and their tributaries, which flow from the southeast into Lake Chad. Both river systems rise in the highlands of Central African Republic and Cameroon, regions that receive more than 1,250 millimeters of rainfall annually. Fed by rivers of Central African Republic, as well as by the Bahr Salamat, Bahr Aouk, and Bahr Sara rivers of southeastern Chad, the Chari River is about 1,200 kilometers long. From its origins near the city of Sarh, the middle course of the Chari makes its way through swampy terrain; the lower Chari is joined by the Logone River near N'Djamena. The Chari's volume varies greatly, from 17 cubic meters per second during the dry season to 340 cubic meters per second during the wettest part of the year.
The Logone River is formed by tributaries flowing from Cameroon and Central African Republic. Both shorter and smaller in volume than the Chari, it flows northeast for 960 kilometers; its volume ranges from five to eighty-five cubic meters per second. At N'Djamena the Logone empties into the Chari, and the combined rivers flow together for thirty kilometers through a large delta and into Lake Chad. At the end of the rainy season in the fall, the river overflows its banks and creates a huge floodplain in the delta.
The seventh largest lake in the world (and the fourth largest in Africa), Lake Chad is located in the sahelian zone, a region just south of the Sahara Desert. The Chari River contributes 95 percent of Lake Chad's water, an average annual volume of 40 billion cubic meters, 95% of which is lost to evaporation. The size of the lake is determined by rains in the southern highlands bordering the basin and by temperatures in the Sahel. Fluctuations in both cause the lake to change dramatically in size, from 9,800 square kilometers in the dry season to 25,500 at the end of the rainy season. 
Lake Chad also changes greatly in size from one year to another. In 1870 its maximum area was 28,000 square kilometers. The measurement dropped to 12,700 in 1908. In the 1940s and 1950s, the lake remained small, but it grew again to 26,000 square kilometers in 1963. The droughts of the late 1960s, early 1970s, and mid-1980s caused Lake Chad to shrink once again, however. The only other lakes of importance in Chad are Lake Fitri, in Batha Prefecture, and Lake Iro, in the marshy southeast.
Climate.
The Lake Chad Basin embraces a great range of tropical climates from north to south, although most of these climates tend to be dry. Apart from the far north, most regions are characterized by a cycle of alternating rainy and dry seasons. In any given year, the duration of each season is determined largely by the positions of two great air masses—a maritime mass over the Atlantic Ocean to the southwest and a much drier continental mass. 
During the rainy season, winds from the southwest push the moister maritime system north over the African continent where it meets and slips under the continental mass along a front called the "intertropical convergence zone". At the height of the rainy season, the front may reach as far as Kanem Prefecture. By the middle of the dry season, the intertropical convergence zone moves south of Chad, taking the rain with it. This weather system contributes to the formation of three major regions of climate and vegetation.
Saharan region.
The Saharan region covers roughly the northern half of the country, including Borkou-Ennedi-Tibesti Prefecture along with the northern parts of Kanem, Batha, and Biltine prefectures. Much of this area receives only traces of rain during the entire year; at Faya Largeau, for example, annual rainfall averages less than . Scattered small oases and occasional wells provide water for a few date palms or small plots of millet and garden crops. 
In much of the north, the average daily maximum temperature is about during January, the coolest month of the year, and about during May, the hottest month. On occasion, strong winds from the northeast produce violent sandstorms. In northern Biltine Prefecture, a region called the Mortcha plays a major role in animal husbandry. Dry for nine months of the year, it receives or more of rain, mostly during July and August. 
A carpet of green springs from the desert during this brief wet season, attracting herders from throughout the region who come to pasture their cattle and camels. Because very few wells and springs have water throughout the year, the herders leave with the end of the rains, turning over the land to the antelopes, gazelles, and ostriches that can survive with little groundwater. Northern Chad averages over 3500 hours of sunlight per year, the south somewhat less.
Sahelian region.
The semiarid sahelian zone, or Sahel, forms a belt about wide that runs from Lac and Chari-Baguirmi prefectures eastward through Guéra, Ouaddaï, and northern Salamat prefectures to the Sudanese frontier. The climate in this transition zone between the desert and the southern soudanian zone is divided into a rainy season (from June to early September) and a dry period (from October to May). 
In the northern Sahel, thorny shrubs and acacia trees grow wild, while date palms, cereals, and garden crops are raised in scattered oases. Outside these settlements, nomads tend their flocks during the rainy season, moving southward as forage and surface water disappear with the onset of the dry part of the year. The central Sahel is characterized by drought-resistant grasses and small woods. Rainfall is more abundant there than in the Saharan region. For example, N'Djamena records a maximum annual average rainfall of , while Ouaddaï Prefecture receives just a bit less. 
During the hot season, in April and May, maximum temperatures frequently rise above . In the southern part of the Sahel, rainfall is sufficient to permit crop production on unirrigated land, and millet and sorghum are grown. Agriculture is also common in the marshlands east of Lake Chad and near swamps or wells. Many farmers in the region combine subsistence agriculture with the raising of cattle, sheep, goats, and poultry.
Soudanian region.
The humid "soudanian" zone includes the southern prefectures of Mayo-Kebbi, Tandjilé, Logone Occidental, Logone Oriental, Moyen-Chari, and southern Salamat. Between April and October, the rainy season brings between of precipitation. Temperatures are high throughout the year. Daytime readings in Moundou, the major city in the southwest, range from in the middle of the cool season in January to about in the hot months of March, April, and May.
The soudanian region is predominantly East Sudanian savanna, or plains covered with a mixture of tropical or subtropical grasses and woodlands. The growth is lush during the rainy season but turns brown and dormant during the five-month dry season between November and March. Over a large part of the region, however, natural vegetation has yielded to agriculture.
2010 drought.
On 22 June, the temperature reached in Faya, breaking a record set in 1961 at the same location. Similar temperature rises were also reported in Niger, which began to enter a famine situation.
On 26 July the heat reached near-record levels over Chad and Niger.
Area.
Area:
<br>"total:"
1.284 million km²
<br>"land:"
1,259,200 km²
<br>"water:"
24,800 km²
Area - comparative:
<br>Canada: smaller than the Northwest Territories
<br>US: slightly more than three times the size of California
Boundaries.
Land boundaries:
<br>"total:"
5,968 km
<br>"border countries:"
Cameroon 1,094 km, Central African Republic 1,197 km, Libya 1,055 km, Niger 1,175 km, Nigeria 87 km, Sudan 1,360 km
Coastline:
0 km (landlocked)
Maritime claims:
none (landlocked)
Elevation extremes:
<br>"lowest point:"
Djourab Depression 160 m
<br>"highest point:"
Emi Koussi 3,415 m
Land use and resources.
Natural resources:
petroleum, uranium, natron, kaolin, fish (Chari River, Logone River), gold, limestone, sand and gravel, salt
Land use:
<br>"arable land:"
2.8%
<br>"permanent crops:"
0.02%
<br>"other:"
97.18% (2005)
Irrigated land:
300 km² (2003)
Total renewable water resources:
43 km3 (2007)
Environmental issues.
Natural hazards:
hot, dry, dusty Harmattan winds occur in north; periodic droughts; locust plagues
Environment - current issues:
inadequate supplies of potable water; improper waste disposal in rural areas contributes to soil and water pollution; desertification
Also see- 2010 Sahel drought
Extreme points.
This is a list of the extreme points of Chad, the points that are farther north, south, east or west than any other location.
"*Note: technically Chad does not have an easternmost point, the northern section of the border being formed by the 24° of latitude"

</doc>
<doc id="5331" url="http://en.wikipedia.org/wiki?curid=5331" title="Demographics of Chad">
Demographics of Chad

The people of Chad speak more than 100 different languages and divide themselves into many ethnic groups. However, language and ethnicity are not the same. Moreover, neither element can be tied to a particular physical type.
Although the possession of a common language shows that its speakers have lived together and have a common history, peoples also change languages. This is particularly so in Chad, where the openness of the terrain, marginal rainfall, frequent drought and famine, and low population densities have encouraged physical and linguistic mobility. Slave raids among{specify} non-Muslim peoples, internal slave trade, and exports of captives northward from the ninth to the twentieth centuries also have resulted in language changes.
Anthropologists view ethnicity as being more than genetics. Like language, ethnicity implies a shared heritage, partly economic, where people of the same ethnic group may share a livelihood, and partly social, taking the form of shared ways of doing things and organizing relations among individuals and groups. Ethnicity also involves a cultural component made up of shared values and a common worldview. Like language, ethnicity is not immutable. Shared ways of doing things change over time and alter a group's perception of its own identity.
Not only do the social aspects of ethnic identity change but the biological composition (or gene pool) also may change over time. Although most ethnic groups emphasize intermarriage, people are often proscribed from seeking partners among close relatives—a prohibition that promotes biological variation. In all groups, the departure of some individuals or groups and the integration of others also changes the biological component.
The Chadian government has avoided official recognition of ethnicity. With the exception of a few surveys conducted shortly after independence, little data were available on this important aspect of Chadian society. Nonetheless, ethnic identity was a significant component of life in Chad.
Chad's languages fall into ten major groups, each of which belongs to either the
Nilo-Saharan, Afro-Asiatic, or Niger–Congo language family. These represent three of the four major language families in Africa; only the Khoisan languages of southern Africa are not represented. The presence of such different languages suggests that the Lake Chad Basin may have been an important point of dispersal in ancient times.
Population.
According to the 2010 revison of the World Population Prospects the total population was 11 227 000 in 2010, compared to only 2 429 000 in 1950. The proportion of children below the age of 15 in 2010 was 45.4%, 51.7% was between 15 and 65 years of age, while 2.9% was 65 years or older
Vital statistics.
Registration of vital events is in Chad not complete. The Population Departement of the United Nations prepared the following estimates.
Fertility and Births.
Total Fertility Rate (TFR) and Crude Birth Rate (CBR):
Religions.
The separation of religion from social structure in Chad represents a false dichotomy, for they are perceived as two sides of the same coin. Three religious traditions coexist in Chad—classical African religions, Islam (see Islam in Chad), and Christianity. None is monolithic. The first tradition includes a variety of ancestor and/or place-oriented religions whose expression is highly specific. Islam, although characterized by an orthodox set of beliefs and observances, also is expressed in diverse ways. Christianity arrived in Chad much more recently with the arrival of Europeans. Its followers are divided into Roman Catholics and Protestants (including several denominations); as with Chadian Islam, Chadian Christianity retains aspects of pre-Christian religious belief.
The number of followers of each tradition in Chad is unknown. Estimates made in 1962 suggested that 35 percent of Chadians practiced classical African religions, 55 percent were Muslims, and 10 percent were Christians. In the 1970s and 1980s, this distribution undoubtedly changed. Observers report that Islam has spread among the Hajerai and among other non-Muslim populations of the Saharan and sahelian zones. However, the proportion of Muslims may have fallen because the birthrate among the followers of traditional religions and Christians in southern Chad is thought to be higher than that among Muslims. In addition, the upheavals since the mid-1970s have resulted in the departure of some missionaries; whether or not Chadian Christians have been numerous enough and organized enough to have attracted more converts since that time is unknown.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population.
10,543,000 (2010, According to the U.S. Census Bureau, International Data Base: Demographics of Chad)
Ethnic groups.
About 5,000 French citizens live in Chad.
References.
"U.S. Census Bureau." Census Bureau Home Page. Web. 29 Jan. 2010. <http://www.census.gov/ipc/www/idb/country.php>.

</doc>
<doc id="5332" url="http://en.wikipedia.org/wiki?curid=5332" title="Politics of Chad">
Politics of Chad

Politics of Chad takes place in a framework of a presidential republic, whereby the President of Chad is both head of state and head of government. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. Chad is one of the most corrupt countries in the world.
In May 2013, security forces in Chad foiled a coup against the President Idriss Deby that had been in preparation for several months.
Executive branch.
A strong executive branch headed by President Idriss Déby dominates the Chadian political system. Following his military overthrow of Hissène Habré in December 1990, Déby won presidential elections in 1996 and 2001. The constitutional basis for the government is the 1996 constitution, under which the president was limited to two terms of office until Déby had that provision repealed in 2005. The president has the power to appoint the prime minister and the Council of State (or cabinet), and exercises considerable influence over appointments of judges, generals, provincial officials and heads of Chad’s parastatal firms. In cases of grave and immediate threat, the president, in consultation with the National Assembly President and Council of State, may declare a state of emergency. Most of the Déby's key advisors are members of the Zaghawa clan, although some southern and opposition personalities are represented in his government.
Legislative branch.
According to the 1996 constitution, the National Assembly deputies are elected by universal suffrage for 4-year terms. Parliamentary elections are scheduled for spring 2002. The Assembly holds regular sessions twice a year, starting in March and October, and can hold special sessions as necessary and called by the prime minister. Deputies elect a president of the National Assembly every 2 years. Assembly deputies or members of the executive branch may introduce legislation; once passed by the Assembly, the president must take action to either sign or reject the law within 15 days. The National Assembly must approve the prime minister’s plan of government and may force the prime minister to resign through a majority vote of no-confidence. However, if the National Assembly rejects the executive branch’s program twice in one year, the president may disband the Assembly and call for new legislative elections. In practice, the president exercises considerable influence over the National Assembly through the MPS party structure.
Judicial branch.
Despite the constitution’s guarantee of judicial independence from the executive branch, the president names most key judicial officials. The Supreme Court is made up of a chief justice, named by the president, and 15 councilors chosen by the president and National Assembly; appointments are for life. The Constitutional Council, with nine judges elected to 9-year terms, has the power to review all legislation, treaties and international agreements prior to their adoption. The constitution recognizes customary and traditional law in locales where it is recognized and to the extent it does not interfere with public order or constitutional guarantees of equality for all citizens.
International organization participation.
ACCT, 
ACP, 
AfDB, 
AU, 
BDEAC, 
CEMAC, 
FAO, 
FZ, 
G-77, 
IBRD, 
ICAO, 
ICCt, 
ICFTU, 
ICRM, 
IDA, 
IDB, 
IFAD, 
IFC, 
IFRCS, 
ILO, 
IMF, 
Interpol, 
IOC, 
ITU, 
MIGA, 
NAM, 
OIC, 
ONUB, 
OPCW, 
UN, 
UNCTAD, 
UNESCO, 
UNIDO, 
UNOCI, 
UPU, 
WCL, 
WHO, 
WIPO, 
WMO, 
WToO, 
WTrO

</doc>
<doc id="5333" url="http://en.wikipedia.org/wiki?curid=5333" title="Economy of Chad">
Economy of Chad

Landlocked Chad's economic development suffers from its geographic remoteness, drought, lack of infrastructure, and political turmoil. About 85% of the population depends on agriculture, including the herding of livestock. Of Africa's Francophone countries, Chad benefited least from the 50% devaluation of their currencies in January 1994. Financial aid from the World Bank, the African Development Bank, and other sources is directed largely at the improvement of agriculture, especially livestock production. Because of lack of financing, the development of oil fields near Doba, originally due to finish in 2000, was delayed until 2003. It was finally developed and is now operated by Exxon Mobil Corporation.

</doc>
<doc id="5334" url="http://en.wikipedia.org/wiki?curid=5334" title="Telecommunications in Chad">
Telecommunications in Chad

Telecommunications in Chad include radio, television, fixed and mobile telephones, and the Internet.
Radio and television.
Radio stations:
Radios:
1.7 million (1997).
Television stations:
Television sets:
10,000 (1997).
Radio is the most important medium of mass communication. State-run Radiodiffusion Nationale Tchadienne operates national and regional radio stations. Around a dozen private radio stations are on the air, despite high licensing fees, some run by religious or other non-profit groups. The BBC World Service (FM 90.6) and Radio France Internationale (RFI) broadcast in the capital, N'Djamena. The only television station, Tele Tchad, is state-owned.
State control of many broadcasting outlets allows few dissenting views. Journalists are harassed and attacked. On rare occasions journalists are warned in writing by the High Council for Communication to produce more "responsible" journalism or face fines. Some journalists and publishers practice self-censorship. On 10 October 2012, the High Council on Communications issued a formal warning to La Voix du Paysan, claiming that the station’s live broadcast on 30 September incited the public to "insurrection against the government." The station had broadcast a sermon by a bishop who criticized the government for allegedly failing to use oil wealth to benefit the region.
Telephones.
Calling code: +235
International call prefix: 00
Main lines:
Mobile cellular:
Telephone system: inadequate system of radiotelephone communication stations with high costs and low telephone density; fixed-line connections for less than 1 per 100 persons coupled with mobile-cellular subscribership base of only about 35 per 100 persons (2011). 
Satellite earth stations: 1 Intelsat (Atlantic Ocean) (2011).
Internet.
Top-level domain: .td
Internet users:
Fixed broadband: 18,000 subscriptions, 132nd in the world; 0.2% of the population, 161st in the world (2012).
Wireless broadband: Unknown (2012).
Internet hosts:
IPv4: 4,096 addresses allocated, less than 0.05% of the world total, 0.4 addresses per 1000 people (2012).
Internet censorship and surveillance.
There are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms.
The constitution provides for freedom of opinion, expression, and press, but the government does not always respect these rights. Private individuals are generally free to criticize the government without reprisal, but reporters and publishers risk harassment from authorities when publishing critical articles. The 2010 media law abolished prison sentences for defamation and insult, but prohibits "inciting racial, ethnic, or religious hatred," which is punishable by one to two years in prison and a fine of one to three million CFA francs ($2,000 to $6,000).

</doc>
<doc id="5335" url="http://en.wikipedia.org/wiki?curid=5335" title="Transport in Chad">
Transport in Chad

Transport infrastructure within Chad is generally poor, especially in the north and east of the country. River transport is limited to the south-west corner. As of 2011 Chad had no railways though two lines are planned - from the capital to the Sudanese and Cameroonish borders.
Roads are mostly unsurfaced and are likely to be impassable during the wet season, especially in the southern half of the country. In the north, roads are merely tracks across the desert and land mines continue to present a danger. Draft animals (horses, donkeys and camels) remain important in much of the country.
Fuel supplies can be erratic, even in the south-west of the country, and are expensive. Elsewhere they are practically non-existent.
Railways.
As of 2011 Chad had no railways. Two lines are planned to Sudan and Cameroon from the capital, with construction expected to start in 2012.
Highways.
Chad has a total of 33,400 km of roads of which approximately 500 km are paved. Some, but not all of the roads in the capital N'Djamena are paved. Outside of N'Djamena there is one paved road which runs from Massakory in the north, through N'Djamena and then south, through the cities of Guelendeng, Bongor, Kelo and Moundou, with a short spur leading in the direction of Kousseri, Cameroon, near N'Djamena. Expansion of the road towards Cameroon via Pala and Lere is reportedly in the preparatory stages.
Waterways.
Most rivers flow but intermittently. On the Chari, between N’Djamena and Lake Chad, transportation is possible all year round. In September and October, the Logone is navigable between N’Djamena and Moundou, and the Chari between N’Djamena and Sarh. Total waterways cover 4,800 km (3,000 mi), of which 2,000 km (1,250 mi) are navigable all year.
Chari and Logone Rivers are navigable only in wet season (2002). Both flow northwards, from the south of Chad, into Lake Chad.
Pipelines.
Since 2003, a 1,070 km pipeline has been used to export crude oil from the oilfields around Doba to offshore oil-loading facilities on Cameroon's Atlantic coast at Kribi.
Seaports and harbors.
None (landlocked).
Chad's main routes to the sea are:
In colonial times, the main access was by road to Bangui, in the Central African Republic, then by river boat to Brazzaville, and onwards by rail from Brazzaville to Pointe Noire, on Congo's Atlantic coast. This route is now little used.
There is also a route across Sudan, to the Red Sea, but very little trade goes this way.
Links with Niger, north of Lake Chad, are practically nonexistent; it is easier to reach Niger via Cameroon and Nigeria.
Airports.
 Chad had an estimated 58 airports, only 9 of which had paved runways. Air Tchad (60 percent state owned) provides internal service to 12 locations but suffers from lack of fuel and equipment. The international airport at N’Djamena was damaged in fighting in 1981, but is now served by several international carriers including Air Afrique, which is partly owned by Chad. Another major airport, developed as a military staging area, is located at Sarh. In 2003, scheduled airlines in Chad carried about 46,000 passengers on domestic and international flights.
Airports with paved runways.
Statistics on airports with paved runways as of 2012:
List of airports with paved runways:
Airports - with unpaved runways.
Statistics on airports with unpaved runways as of 2012:

</doc>
<doc id="5336" url="http://en.wikipedia.org/wiki?curid=5336" title="Military of Chad">
Military of Chad

The Military of Chad consists of the Armed Forces (includes Ground Forces, Air Force, and Gendarmerie), Republican Guard, Rapid Intervention Force, Police, and National and Nomadic Guard (GNNT). Currently the main task of the Chadian military is to combat the various rebel forces inside the country.
History.
From independence through the period of the presidency of Félix Malloum (1975–79), the official national army was known as the Chadian Armed Forces (Forces Armées Tchadiennes—FAT). Composed mainly of soldiers from southern Chad, FAT had its roots in the army recruited by France and had military traditions dating back to World War I. FAT lost its status as the legal state army when Malloum's civil and military administration disintegrated in 1979. Although it remained a distinct military body for several years, FAT was eventually reduced to the status of a regional army representing the south.
After Habré consolidated his authority and assumed the presidency in 1982, his victorious army, the Armed Forces of the North (Forces Armées du Nord—FAN), became the nucleus of a new national army. The force was officially constituted in January 1983, when the various pro-Habré contingents were merged and renamed FANT.
The Military of Chad was dominated by members of Toubou, Zaghawa, Kanembou, Hadjerai, and Massa ethnic groups during the presidency of Hissène Habré. Current Chadian president Idriss Déby, revolted and fled to the Sudan, taking with him many Zaghawa and Hadjerai soldiers in 1989.
Chad's armed forces numbered about 36,000 at the end of the Habré regime, but swelled to an estimated 50,000 in the early days of Déby's rule. With French support, a reorganization of the armed forces was initiated early in 1991 with the goal of reducing its numbers and making its ethnic composition reflective of the country as a whole. Neither of these goals was achieved, and the military is still dominated by the Zaghawa.
In 2004, the government discovered that many of the soldiers it was paying did not exist and that there were only about 19,000 soldiers in the army, as opposed to the 24,000 that had been previously believed. Government crackdowns against the practice are thought to have been a factor in a failed military mutiny in May 2004.
The current conflict, in which the Chadian military is involved, is the civil war against Sudanese-backed rebels. Chad successfully manages to repel the rebel movements, but recently, with some losses (see Battle of N'Djamena (2008)). The army uses its artillery systems and tanks, but well-equipped insurgents have probably managed to destroy over 20 of Chad's 60 t-55 tanks, and probably shot down a Mi-24 Hind gunship, which has bombed enemy positions near the border with Sudan. In November 2006 Libya supplied Chad with four Aermacchi SF.260W light attack planes. They are used to strike enemy positions by the Chadian Air Force, but one has been shot down by rebels. During the last battle of N'Djamena gunships and tanks have been put to good use, pushing armed militia forces back from the Presidential palace. The battle impacted the highest levels of the army leadership, as Daoud Soumain, its Chief of Staff, was killed.
Spending.
The CIA World Factbook estimates the military budget of Chad to be 4.2% of GDP as of 2006.. Given the then GDP ($7.095 bln) of the country, military spending was estimated to be about $300 million. This estimate however dropped after the end of the Civil war in Chad (2005–2010) to 2.0% as estimated by the World Bank for the year 2011. There aren't any more recent estimates available for 2012, 2013.
External Deployments.
Chad participated in a peace mission under the authority of African Union in the neighboring Central African Republic to try to pacify the recent conflict, but has chosen to withdraw after its soldiers were accused of shooting into a marketplace, unprovoked, according to BBC. 
"Currently, Cameroon has an ongoing military-military relationship with Chad, which includes associates training for Chadian military in Cameroon. There are four brigade Chado-Cameroonian in January 2012. Cameroon and Chad are developing excellent relations".

</doc>
<doc id="5337" url="http://en.wikipedia.org/wiki?curid=5337" title="Foreign relations of Chad">
Foreign relations of Chad

The foreign relations of Chad are motivated primarily by the desire for outside investment in Chadian industry and support for Chadian President Idriss Déby. Chad is officially non-aligned but has close relations with France, the former colonial power. Relations with neighbouring Libya, and Sudan vary periodically. Lately, the Idris Déby regime has been waging an intermittent proxy war with Sudan. Aside from those two countries, Chad generally enjoys good relations with its neighbouring states.
Relations with other African and Arab states.
Although relations with Libya improved with the presidency of Idriss Déby, strains persist. Chad has been an active champion of regional cooperation through the Central African Economic and Customs Union, the Lake Chad and Niger River Basin Commissions, and the Interstate Commission for the Fight Against the Drought in the Sahel.
Delimitation of international boundaries in the vicinity of Lake Chad, the lack of which led to border incidents in the past, has been completed and awaits ratification by Cameroon, Chad, Niger, and Nigeria.
Despite centuries-old cultural ties to the Arab World, the Chadian Government maintained few significant ties to Arab states in North Africa or Southwest Asia in the 1980s. However, Chad has not recognised the State of Israel since former Chadian President François (Ngarta) Tombalbaye broke off relations in September 1972 as an act of solidarity with the Palestinians, and other Arabs under Israeli occupation. President Habré hoped to pursue closer relations with Arab states as a potential opportunity to break out of his Chad's post-imperial dependence on France, and to assert Chad's unwillingness to serve as an arena for superpower rivalries. In addition, as a northern Christian, Habré represented a constituency that favored co-operation and solidarity with Arabs, both African, and Asian. For these reasons, he was expected to seize opportunities during the 1990s to pursue closer ties with the Arab World. In 1988, Chad recognized the State of Palestine, which maintains a mission in N'Djamena.
During the 1980s, Arab opinion on the Chadian-Libyan conflict over the Aozou Strip was divided. Several Arab states supported Libyan territorial claims to the Strip, among the most outspoken of which was Algeria, which provided training for anti-Habré forces, although most recruits for its training programs were from Nigeria or Cameroon, recruited and flown to Algeria by Libya. Lebanon's Progressive Socialist Party also sent troops to support Qadhafi's efforts against Chad in 1987. In contrast, numerous other Arab states opposed the Libyan actions, and expressed their desire to see the dispute over the Aozou Strip settled peacefully. By the end of 1987, Algiers and N'Djamena were negotiating to improve relations.
Sudan.
On December 24, 2005, Chad declared itself as in a "state of belligerance" with neighboring Sudan. The conflict in the border region of Darfur has become an increasingly bi-national affair as increasing numbers of Sudanese flee to refugee camps in Chad, and Sudanese government troops and militias cross the borders to strike at both these camps and specific ethnic groups. Although the Government of Chad and the Government of Sudan signed the Tripoli Agreement on February 8, 2006, officially ending hostilities, fighting continues. On 11 August 2006, Chad and Sudan resumed relations at the behest of Libyan president Muammar Gaddafi.
Chad broke diplomatic relations with Sudan at least twice in 2006 because it believed the Sudanese government was supporting Janjaweed and UFDC rebels financially and with arms. Two accords were signed, the Tripoli Accord, which was signed on February 8 and failed to end the fighting, and the more recently signed N'Djamena Agreement. On May 11, 2008 Sudan announced it was cutting diplomatic relations with Chad, claiming that it was helping rebels in Darfur to attack the Sudanese capital Khartoum.
Libya.
Chadian-Libyan relations were ameliorated when Libyan-supported Idriss Déby unseated Habré on December 2. Gaddafi was the first head of state to recognize the new regime, and he also signed treaties of friendship and cooperation on various levels; but regarding the Aouzou Strip Déby followed his predecessor, declaring that if necessary he would fight to keep the strip out of Libya's hands.
The Aouzou dispute was concluded for good on February 3, 1994, when the judges of the ICJ by a majority of 16 to 1 decided that the Aouzou Strip belonged to Chad. The court's judgement was implemented without delay, the two parties signing as early as April 4 an agreement concerning the practical modalities for the implementation of the judgement. Monitored by international observers, the withdrawal of Libyan troops from the Strip began on April 15 and was completed by May 10. The formal and final transfer of the Strip from Libya to Chad took place on May 30, when the sides signed a joint declaration stating that the Libyan withdrawal had been effected.
Nigeria.
Nigeria's 1983 economic austerity campaign produced strains with neighboring states, including Chad. Nigeria expelled several hundred thousand foreign workers, mostly from its oil industry, which faced drastic cuts as a result of declining world oil prices. At least 30,000 of those expelled were Chadians. Despite these strains, however, Nigerians had assisted in the halting process of achieving stability in Chad, and both nations reaffirmed their intention to maintain close ties.
Relations with Western countries.
Chad is officially non-aligned but has close relations with France, the former colonial power, which has about 1,200 troops stationed in the capital N'Djamena. It receives economic aid from countries of the European Community, the United States, and various international organizations. Libya supplies aid and has an ambassador resident in N'Djamena. Traditionally strong ties with the Western community have weakened over the past two years due to a dispute between the Government of Chad and the World Bank over how the profits from Chad's petroleum reserves are allocated. Although oil output to the West has resumed and the dispute has officially been resolved, resentment towards what the Déby administration considered foreign meddling lingers.
France.
France was Chad's most important foreign donor and patron for the first three decades following independence in 1960. At the end of the 1980s, economic ties were still strong, and France provided development assistance in the form of loans and grants. It was no longer Chad's leading customer for agricultural exports, but it continued to provide substantial military support.
Chad remained a member of the African Financial Community (Communauté Financière Africaine—CFA), which linked the value of its currency, the CFA franc, to the French franc. French private and government investors owned a substantial portion of Chad's industrial and financial institutions, and the French treasury backed the Bank of Central African States (Banque des Etats de l'Afrique Centrale—BEAC), which served as the central bank for Chad and six other member nations. Chad's dependence on France declined slightly during Habré's tenure as president, in part because other foreign donors and investors returned as the war subsided and also because increased rainfall since 1985 improved food production. French official attitudes toward Chad had changed from the 1970s policies under the leadership of Giscard d'Estaing to those of the Mitterrand era of the 1980s. Economic, political, and strategic goals, which had emphasized maintaining French influence in Africa, exploiting Chad's natural resources, and bolstering francophone Africa's status as a bulwark against the spread of Soviet influence, had been replaced by nominally anticolonialist attitudes. The election in France of the Socialist government in 1981 had coincided with conditions of near-anarchy in Chad, leading France's Socialist Party to reaffirm its ideological stance against high-profile intervention in Africa. Hoping to avoid a confrontation with Libya, another important client state in the region, President Mitterrand limited French military involvement to a defense of the region surrounding N'Djamena in 1983 and 1984. Then, gradually increasing its commitment to reinforce Habré's presidency, France once again increased its military activity in Chad.
United States.
The American embassy in N'Djamena, established at Chadian independence in 1960, was closed from the onset of the heavy fighting in the city in 1980 until the withdrawal of the Libyan forces at the end of 1981. It was reopened in January 1982. The U.S. Agency for International Development (USAID) and the U.S. Information Service (USIS) offices resumed activities in Chad in September 1983. The United States Department of State issued a travel advisory to U.S. citizens in 2009, recommending that citizens not affiliated with humanitarian efforts avoid all travel to eastern Chad and the Chad/Central African Republic border area due to insecurity caused by banditry, recent clashes between Chadian government and rebel forces, and political tension between Chad and Sudan.
Romania.
Chad–Romania relations were established on 15 July 1969. However, neither country has an embassy in the other's capital, and although an agreement on trade was signed in 1969, followed by an agreement on economic and technical cooperation in 1971, , the volume of bilateral trade remained insignificant.
In November 2007, Romania announced that they would deploy 120 troops to Chad and the Central African Republic in connection with a European Union peacekeeping mission there. Romania continued to condemn violence in Chad and blamed it on rebel groups. However, by mid-2008, Romanian defence minister Teodor Meleşcanu indicated that his country would not send further troops to the mission in Chad, stating that they had reached their limits and did not want involvement in a war theatre.
Kosovo.
On 1 June 2012, Chad recognized the Republic of Kosovo as independent and sovereign country.
Relations with Asian countries.
Chad and Taiwan had relations from 1962 to 1972 and 1997 to 2006 when, for financial and security reasons, Chad announced its intention to recognize China. Taiwan broke off relations with Chad on August 5, 2006 (hours before a scheduled official visit by Premier Su Tseng-chang) and Chad formally recognized the PRC on August 6.
Membership of international organizations.
Chad belongs to the following international organizations:

</doc>
<doc id="5342" url="http://en.wikipedia.org/wiki?curid=5342" title="Commentary">
Commentary

Commentary or commentaries may refer to:

</doc>
<doc id="5346" url="http://en.wikipedia.org/wiki?curid=5346" title="Colloid">
Colloid

A colloid is a substance in which microscopically dispersed insoluble particles are suspended throughout another substance. Sometimes the dispersed substance alone is called the colloid; the term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word "suspension" is contradistinguished from colloids by larger particle size). Unlike a solution, whose solute and solvent constitute only one phase, a colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension). To qualify as a colloid, the mixture must be one that does not settle or would take a very long time to settle appreciably.
The dispersed-phase particles have a diameter of between approximately 1 and 1000 nanometers. Such particles are normally easily visible in an optical microscope, although at the smaller size range (r<250 nm), an ultramicroscope or an electron microscope may be required. Homogeneous mixtures with a dispersed phase in this size range may be called "colloidal aerosols", "colloidal emulsions", "colloidal foams", "colloidal dispersions", or "hydrosols". The dispersed-phase particles or droplets are affected largely by the surface chemistry present in the colloid.
Some colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color.
Colloidal suspensions are the subject of interface and colloid science. This field of study was introduced in 1861 by Scottish scientist Thomas Graham.
Classification.
Because the size of the dispersed phase may be difficult to measure, and because colloids have the appearance of solutions, colloids are sometimes identified and characterized by their physico-chemical and transport properties. For example, if a colloid consists of a solid phase dispersed in a liquid, the solid particles will not diffuse through a membrane, whereas with a true solution the dissolved ions or molecules will diffuse through a membrane. Because of the size exclusion, the colloidal particles are unable to pass through the pores of an ultrafiltration membrane with a size smaller than their own dimension. The smaller the size of the pore of the ultrafiltration membrane, the lower the concentration of the dispersed colloidal particles remaining in the ultrafiltered liquid. The measured value of the concentration of a truly dissolved species will thus depend on the experimental conditions applied to separate it from the colloidal particles also dispersed in the liquid. This is particularly important for solubility studies of readily hydrolyzed species such as Al, Eu, Am, Cm, or organic matter complexing these species.
Colloids can be classified as follows:
Based on the nature of interaction between the dispersed phase and the dispersion medium, colloids can be classified as: Hydrophilic colloids: These are water-loving colloids.The colloid particles are attracted toward water. They are also called reversible sols. Hydrophobic colloids: These are opposite in nature to hydrophilic colloids. The colloid particles are repelled by water. They are also called irreversible sols.
In some cases, a colloid can be considered a homogeneous mixture. This is because the distinction between "dissolved" and "particulate" matter can be sometimes a matter of approach, which affects whether or not it is homogeneous or heterogeneous.
Hydrocolloids.
A "hydrocolloid" is defined as a colloid system wherein the colloid particles are hydrophilic polymers dispersed in water. A hydrocolloid has colloid particles spread throughout water, and depending on the quantity of water available that can take place in different states, e.g., gel or sol (liquid). Hydrocolloids can be either irreversible (single-state) or reversible. For example, agar, a reversible hydrocolloid of seaweed extract, can exist in a gel and solid state, and alternate between states with the addition or elimination of heat.
Many hydrocolloids are derived from natural sources. For example, agar-agar and carrageenan are extracted from seaweed, gelatin is produced by hydrolysis of proteins of bovine and fish origins, and pectin is extracted from citrus peel and apple pomace.
Gelatin desserts like jelly or Jell-O are made from gelatin powder, another effective hydrocolloid. Hydrocolloids are employed in food mainly to influence texture or viscosity (e.g., a sauce). Hydrocolloid-based medical dressings are used for skin and wound treatment.
Other main hydrocolloids are xanthan gum, gum arabic, guar gum, locust bean gum, cellulose derivatives as carboxymethyl cellulose, alginate and starch.
Interaction between particles.
The following forces play an important role in the interaction of colloid particles:
Preparation.
There are two principal ways of preparation of colloids:
Stabilization (peptization).
The stability of a colloidal system is defined by particles remaining suspended in solution at equilibrium.
Stability is hindered by aggregation and sedimentation phenomena, which are driven by the colloids tendency to reduce surface energy. Reducing the interfacial tension will stabilize the colloidal system by reducing this driving force. 
Aggregation is due to the sum of the interaction forces between particles. If attractive forces (such as van der Waals forces) prevail over the repulsive ones (such as the electrostatic ones) particles aggregate in clusters.
Electrostatic stabilization and steric stabilization are the two main mechanisms for stabilization against aggregation.
A combination of the two mechanisms is also possible (electrosteric stabilization). All the above-mentioned mechanisms for minimizing particle aggregation rely on the enhancement of the repulsive interaction forces.
Electrostatic and steric stabilization do not directly address the sedimentation/floating problem.
Particle sedimentation (and also floating, although this phenomenon is less common) arises from a difference in the density of the dispersed and of the continuous phase. The higher the difference in densities, the faster the particle settling.
The method consists in adding to the colloidal suspension a polymer able to form a gel network and characterized by shear thinning properties. Examples of such substances are xanthan and guar gum.
Particle settling is hindered by the stiffness of the polymeric matrix where particles are trapped. In addition, the long polymeric chains can provide a steric or electrosteric stabilization to dispersed particles.
The rheological shear thinning properties find beneficial in the preparation of the suspensions and in their use, as the reduced viscosity at high shear rates facilitates deagglomeration, mixing and in general the flow of the suspensions.
Destabilization (flocculation).
Unstable colloidal dispersions can form flocs as the particles aggregate due to interparticle attractions. In this way photonic glasses can be grown. This can be accomplished by a number of different methods:
Unstable colloidal suspensions of low-volume fraction form clustered liquid suspensions, wherein individual clusters of particles fall to the bottom of the suspension (or float to the top if the particles are less dense than the suspending medium) once the clusters are of sufficient size for the Brownian forces that work to keep the particles in suspension to be overcome by gravitational forces. However, colloidal suspensions of higher-volume fraction form colloidal gels with viscoelastic properties. Viscoelastic colloidal gels, such as bentonite and toothpaste, flow like liquids under shear, but maintain their shape when shear is removed. It is for this reason that toothpaste can be squeezed from a toothpaste tube, but stays on the toothbrush after it is applied.
Monitoring stability.
Multiple light scattering coupled with vertical scanning is the most widely used technique to monitor the dispersion state of a product, hence identifying and quantifying destabilisation phenomena. It works on concentrated dispersions without dilution. When light is sent through the sample, it is backscattered by the particles / droplets. The backscattering intensity is directly proportional to the size and volume fraction of the dispersed phase. Therefore, local changes in concentration ("e.g."Creaming and Sedimentation) and global changes in size ("e.g." flocculation, coalescence) are detected and monitored.
Accelerating methods for shelf life prediction.
The kinetic process of destabilisation can be rather long (up to several months or even years for some products) and it is often required for the formulator to use further accelerating methods in order to reach reasonable development time for new product design. Thermal methods are the most commonly used and consists in increasing temperature to accelerate destabilisation (below critical temperatures of phase inversion or chemical degradation). Temperature affects not only the viscosity, but also interfacial tension in the case of non-ionic surfactants or more generally interactions forces inside the system. Storing a dispersion at high temperatures enables to simulate real life conditions for a product (e.g. tube of sunscreen cream in a car in the summer), but also to accelerate destabilisation processes up to 200 times.
Mechanical acceleration including vibration, centrifugation and agitation are sometimes used. They subject the product to different forces that pushes the particles / droplets against one another, hence helping in the film drainage. However, some emulsions would never coalesce in normal gravity, while they do under artificial gravity. Moreover segregation of different populations of particles have been highlighted when using centrifugation and vibration.
As a model system for atoms.
In physics, colloids are an interesting model system for atoms. Micrometre-scale colloidal particles are large enough to be observed by optical techniques such as confocal microscopy. Many of the forces that govern the structure and behavior of matter, such as excluded volume interactions or electrostatic forces, govern the structure and behavior of colloidal suspensions. For example, the same techniques used to model ideal gases can be applied to model the behavior of a hard sphere colloidal suspension. In addition, phase transitions in colloidal suspensions can be studied in real time using optical techniques, and are analogous to phase transitions in liquids. In many interesting cases optical fluidity is used to control colloid suspensions.
Crystals.
A colloidal crystal is a highly ordered array of particles that can be formed over a very long range (typically on the order of a few millimeters to one centimeter) and that appear analogous to their atomic or molecular counterparts. One of the finest natural examples of this ordering phenomenon can be found in precious opal, in which brilliant regions of pure spectral color result from close-packed domains of amorphous colloidal spheres of silicon dioxide (or silica, SiO2). These spherical particles precipitate in highly siliceous pools in Australia and elsewhere, and form these highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of submicrometre spherical particles provide similar arrays of interstitial voids, which act as a natural diffraction grating for visible light waves, particularly when the interstitial spacing is of the same order of magnitude as the incident lightwave.
Thus, it has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances, often being considerably greater than the individual particle diameter. In all of these cases in nature, the same brilliant iridescence (or play of colors) can be attributed to the diffraction and constructive interference of visible lightwaves that satisfy Bragg’s law, in a matter analogous to the scattering of X-rays in crystalline solids.
The large number of experiments exploring the physics and chemistry of these so-called "colloidal crystals" has emerged as a result of the relatively simple methods that have evolved in the last 20 years for preparing synthetic monodisperse colloids (both polymer and mineral) and, through various mechanisms, implementing and preserving their long-range order formation.
In biology.
In the early 20th century, before enzymology was well understood, colloids were thought to be the key to the operation of enzymes; i.e., the addition of small quantities of an enzyme to a quantity of water would, in some fashion yet to be specified, subtly alter the properties of the water so that it would break down the enzyme's specific substrate, such as a solution of ATPase breaking down ATP. Furthermore, life itself was explainable in terms of the aggregate properties of all the colloidal substances that make up an organism. As more detailed knowledge of biology and biochemistry developed, the colloidal theory was replaced by the macromolecular theory, which explains an enzyme as a collection of identical huge molecules that act as very tiny machines, freely moving about between the water molecules of the solution and individually operating on the substrate, no more mysterious than a factory full of machinery. The properties of the water in the solution are not altered, other than the simple osmotic changes that would be caused by the presence of any solute. In humans, both the thyroid gland and the intermediate lobe ("pars intermedia") of the pituitary gland contain colloid follicles.
In the environment.
Colloidal particles can also serve as transport vector
of diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks
(limestone, sandstone, granite, ...). Radionuclides and heavy metals easily sorb onto colloids suspended in water. Various types of colloids are recognised: inorganic colloids (clay particles, silicates, iron oxy-hydroxides, ...), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term "Eigencolloid" is used to designate pure phases, e.g., Tc(OH)4, U(OH)4, Am(OH)3. Colloids have been suspected for the long-range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations
because of the process of ultrafiltration occurring in dense clay membrane.
The question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.
Intravenous therapy.
Colloid solutions used in intravenous therapy belong to a major group of volume expanders, and can be used for intravenous fluid replacement. Colloids preserve a high colloid osmotic pressure in the blood, and therefore, they should theoretically preferentially increases the intravascular volume, whereas other types of volume expanders called crystalloids also increases the interstitial volume and intracellular volume. However, there is still controversy to the actual difference in efficacy by this difference, and much of the research related to this use of colloids is based on fraudulent research by Joachim Boldt. Another difference is that crystalloids generally are much cheaper than colloids.

</doc>
<doc id="5347" url="http://en.wikipedia.org/wiki?curid=5347" title="Chinese">
Chinese

Chinese can refer to:

</doc>
<doc id="5350" url="http://en.wikipedia.org/wiki?curid=5350" title="Riding shotgun">
Riding shotgun

Riding shotgun refers to the practice of sitting alongside the driver in a moving vehicle.
The expression "riding shotgun" is derived from "shotgun messenger", a colloquial term for "express messenger", in the days of stagecoach travel the person in the position next to the driver. However, apparently the phrase "riding shotgun" was not coined until 1919. It was later used in print and especially film depiction of stagecoaches and wagons in the Old West in danger of being robbed or attacked by bandits. A special armed employee of the express service using the stage for transportation of bullion or cash would sit beside the driver, carrying a short shotgun (or alternatively a rifle), to provide an armed response in case of threat to the cargo, which was usually a strongbox. Absence of an armed person in that position often signaled that the stage was not carrying a strongbox, but only passengers.
More recently, the term has been applied to a game, typically played by groups of friends to determine who rides beside the driver in a car. Typically, this involves claiming the right to ride shotgun by being the first person to call out "shotgun". While there exist myriad additional rules for the game, such as a requirement that the vehicle be in sight, nearly all players agree that the game may only begin on the way to the car.
The phrase has been used to mean giving actual or figurative support or aid to someone in a situation or project, i.e. to "watch their back".

</doc>
<doc id="5355" url="http://en.wikipedia.org/wiki?curid=5355" title="Cooking">
Cooking

Cooking or cookery is the art of preparing food for consumption with the use of heat. Cooking techniques and ingredients vary widely across the world, reflecting unique environmental, economic, and cultural traditions and trends. The way that cooking takes place also depends on the skill and type of training an individual cook has. Cooking can also occur through chemical reactions without the presence of heat, most notably with Ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice. Sushi also uses a similar chemical reaction between fish and the acidic content of rice glazed with vinegar.
Preparing food with heat or fire is an activity unique to humans, and scientists believe the advent of cooking played an important role in human evolution. Most anthropologists believe that cooking fires first developed around 250,000 years ago. The expansion of agriculture, commerce, trade and transportation between civilizations in different regions offered cooks many new ingredients. New inventions and technologies, such as pottery for holding and boiling water, expanded cooking techniques. Some modern cooks apply advanced scientific techniques to food preparation.
History.
There is no clear archeological evidence for when food was first cooked. Most anthropologists believe that cooking fires began only about 250,000 years ago, when hearths started appearing. Phylogenetic analysis by Chris Organ, Charles Nunn, Zarin Machanda, and Richard Wrangham suggests that cooking may have been invented as far back as 1.8 million to 2.3 million years ago.
Wrangham proposed that cooking was instrumental in human evolution, as it reduced the time required for foraging and led to an increase in brain size. He estimates the percentage decrease in gut size of early humans directly correlates to the increase in brain size. Most other anthropologists, however, oppose Wrangham, stating that archeological evidence suggests that cooking fires began in earnest only about 300,000 years ago, when ancient hearths, earth ovens, burnt animal bones, and flint appear across Europe and the Middle East. Two million years ago, the only sign of fire is burnt earth with human remains, which most other anthropologists consider to be mere coincidence rather than evidence of intentional fire. The mainstream view among anthropologists is that the increases in human brain size occurred well before the advent of cooking, due to a shift away from the consumption of nuts and berries to the consumption of meat.
Food has become a part of material culture, and cuisine is much more than a substance. In the seventeenth and eighteenth centuries, food was a classic marker in Europe. However, in the nineteenth century, cuisine became a defining symbol of national identity. The discovery of the New World represented a major turning point in the history of food because of the movement of foods from and to Europe, such as potatoes, tomatoes, corn, yams, and beans. Food in America consisted of traditions that were adapted from England, but up until the end of this century, the presence of new ingredients along with the contact between diverse ethnic groups influenced experimentation. Industrialization was also a turning point that changed how food affected the nation.
During the period of industrialization, food began to be mass-produced, mass marketed, and standardized. Factories processed, preserved, canned, and packaged a wide variety of foods, and processed cereals quickly became a defining feature of the American breakfast. In the twenties, freezing methods as well as the earliest cafeterias and fast food establishments emerged. This point in time is when processed and nationally distributed foods became a huge part of the nation's diet.
Along with changes in food, there have also been several changes in nutritional guidelines as well. Since 1916, there have been several different nutrition guidelines issued by the United States government, eventually leading up to the food pyramid. In 1916, "Food For Young Children" along with its sequel for adults, "How to Select Foods" was the first USDA guide to give specific dietary guidelines. Updated in the 1920s to these guides gave shopping suggestions for different-sized families along with a Depression Era revision which included four cost levels. In 1943, the USDA created the "Basic Seven" chart to make sure that people got the recommended nutrients. It included the first-ever Recommended Daily Allowances from the National Academy of Sciences. In 1956, the "Essentials of an Adequate Diet" brought recommendations which cut seven down to four groups that school children would learn about for decades. In 1979, a guide called "Food" was published, which addressed the link between too much of certain foods and chronic diseases. This publication also added "fats, oils, and sweets" to the four basic food groups and cautioned moderation. In 1992, the food pyramid was debuted. The USDA introduced this, which represented proportions of foods in a balanced diet. In 2005, the pyramid got a makeover and was renamed MyPyramid. Lastly, in 2011, "MyPlate" came about.
Ingredients.
Most ingredients in cooking are derived from living organisms. Vegetables, fruits, grains and nuts as well as herbs and spices come from plants, while meat, eggs, and dairy products come from animals. Mushrooms and the yeast used in baking are kinds of fungi. Cooks also use water and minerals such as salt. Cooks can also use wine or spirits.
Naturally occurring ingredients contain various amounts of molecules called "proteins", "carbohydrates" and "fats". They also contain water and minerals. Cooking involves a manipulation of the chemical properties of these molecules.
Carbohydrates.
Carbohydrates include the common sugar, sucrose (table sugar), a disaccharide, and such simple sugars as glucose (from the digestion of table sugar) and fructose (from fruit), and starches from sources such as cereal flour, rice, arrowroot, and potato. The interaction of heat and carbohydrate is complex.
Long-chain sugars such as starch
tend to break down into simpler sugars when cooked, while simple sugars can form syrups. If sugars are heated so that all water of crystallisation is driven off, then caramelization starts, with the sugar undergoing thermal decomposition with the formation of carbon, and other breakdown products producing caramel. Similarly, the heating of sugars and proteins elicits the Maillard reaction, a basic flavor-enhancing technique.
An emulsion of starch with fat or water can, when gently heated, provide thickening to the dish being cooked. In European cooking, a mixture of butter and flour called a roux is used to thicken liquids to make stews or sauces. In Asian cooking, a similar effect is obtained from a mixture of rice or corn starch and water. These techniques rely on the properties of starches to create simpler mucilaginous saccharides during cooking, which causes the familiar thickening of sauces. This thickening will break down, however, under additional heat.
Fats.
Types of fat include vegetable oils, animal products such as butter and lard, as well as fats from grains, including corn and flax oils. Fats can reach temperatures higher than the boiling point of water, and are often used to conduct high heat to other ingredients, such as in frying or sautéing.
Proteins.
Edible animal material, including muscle, offal, milk, eggs and egg whites, contains substantial amounts of protein. Almost all vegetable matter (in particular legumes and seeds) also includes proteins, although generally in smaller amounts. Mushrooms have high protein content. Any of these may be sources of essential amino acids. When proteins are heated they become denatured (unfolded) and change texture. In many cases, this causes the structure of the material to become softer or more friable – meat becomes "cooked" and is more friable and less flexible. In some cases, proteins can form more rigid structures, such as the coagulation of albumen in egg whites. The formation of a relatively rigid but flexible matrix from egg white provides an important component in baking cakes, and also underpins many desserts based on meringue.
Vitamins and minerals.
Vitamins are materials required for normal metabolism but which the body cannot manufacture itself and which must therefore come from external sources. Vitamins come from several sources including fresh fruit and vegetables (Vitamin C), carrots, liver (Vitamin A), cereal bran, bread, liver e (B vitamins), fish liver oil (Vitamin D) and fresh green vegetables (Vitamin K). Many minerals are also essential in small quantities including iron, calcium, magnesium and sulphur; and in very small quantities copper, zinc and selenium. The micronutrients, minerals, and vitamins in fruit and vegetables may be destroyed or eluted by cooking. Vitamin C is especially prone to oxidation during cooking and may be completely destroyed by protracted cooking.The bioavailability of some vitamins such as thiamin, vitamin B6, niacin, folate, and carotenoids are increased with cooking by being freed from the food microstructure.
Water.
Cooking often involves water, frequently present in other liquids, which is both added in order to immerse the substances being cooked (typically water, stock or wine), and released from the foods themselves. Liquids are so important to cooking that the name of the cooking method used is often based on how the liquid is combined with the food, as in steaming, simmering, boiling, braising, and blanching. Heating liquid in an open container results in rapidly increased evaporation, which concentrates the remaining flavor and ingredients – this is a critical component of both stewing and sauce making.
Methods.
There are very many methods of cooking, most of which have been known since antiquity. These include baking, roasting, frying, grilling, barbecuing, smoking, boiling, steaming and braising. A more recent innovation is microwaving. Various methods use differing levels of heat and moisture and vary in cooking time. The method chosen greatly affects the end result because some foods are more appropriate to some methods than others. Some major hot cooking techniques include:
Cooking and health.
Food safety.
When heat is used in the preparation of food, it can kill or inactivate harmful organisms, such as bacteria and viruses, as well as various parasites such as tapeworms and "Toxoplasma gondii". Food poisoning and other illness from uncooked or poorly-prepared food may be caused by bacteria such as of "Escherichia coli", "Salmonella typhimurium" and "Campylobacter", viruses such as noroviruses, and protozoa such as "Entamoeba histolytica". Parasites may be introduced through salad, meat that is uncooked or done rare, and unboiled water.
The sterilizing effect of cooking depends on temperature, cooking time, and technique used. However, some bacteria such as "Clostridium botulinum" or "Bacillus cereus" can form spores that survive cooking, which then germinate and regrow after the food has cooled. It is therefore recommended that cooked food should not be reheated more than once to avoid repeated growths that allow the bacteria to proliferate to dangerous level.
Cooking prevents many foodborne illnesses that would otherwise occur if the food was eaten raw. Cooking also increases the digestibility of some foods because many foods such as grains, when raw, are inedible, and some are poisonous. For example kidney beans are toxic when raw or improperly cooked, due to the presence of phytohaemagglutinin which can be inactivated after cooking for at least ten minutes at 100 °C. Slow cooker however may not reach the desired temperature and cases of poisoning from red beans cooked in slow cooker have been reported.
Other considerations for food safety in cooking include preparation, handling, and storage of food. According to the USDA, the temperature range from , is the "Danger zone" where bacteria is likely to proliferate, food therefore should not be stored in this temperature range. Washing of hands and surfaces, and avoidance of cross-contamination are good practices in food safety. Food prepared on plastic cutting boards may be less likely to harbor bacteria than wooden ones, other research however suggested otherwise. Washing and sanitizing cutting boards is highly recommended, especially after use with raw meat, poultry, or seafood. Hot water and soap followed by a rinse with a diluted antibacterial cleaner, or a trip through a dishwasher with a "sanitize" cycle, are effective methods for reducing the risk of illness due to contaminated cooking implements.
Effects on nutritional content of food.
Proponents of Raw foodism argue that cooking food increases the risk of some of the detrimental effects on food or health. They point out that during cooking of vegetables and fruit containing vitamin C, the vitamin elutes into the cooking water as well as becoming degraded through oxidation. Peeling vegetables can also substantially reduce the vitamin C content, especially in the case of potatoes where most vitamin C is in the skin. However, research has shown that in the specific case of carotenoids a greater proportion is absorbed from cooked vegetables than from raw vegetables.
German research in 2003 showed significant benefits in reducing breast cancer risk when large amounts of raw vegetable matter are included in the diet. The authors attribute some of this effect to heat-labile phytonutrients. Sulforaphane, a glucosinolate breakdown product, which may be found in vegetables such as broccoli, has been shown to be protective against prostate cancer, however, much of it is destroyed when the vegetable is boiled.
Richard Wrangham from Harvard University argues in a that cooking allowed "Homo sapiens" to develop a larger brain by increasing digestibility and freeing up time from foraging and chewing.
Cooking and carcinogens.
In a human epidemiological analysis by Richard Doll and Richard Peto in 1981, diet was estimated to cause a large percentage of cancers. Studies suggest that around 32% of cancer deaths may be avoidable by changes to the diet. Some of these cancers may be caused by carcinogens in food generated during cooking process, although it is often difficult to identify the specific components in diet that serve to increase cancer risk. Many food, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.
Several studies published since 1990 indicate that cooking meat at high temperature creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While eating meat raw may be the only way to avoid HCAs fully, the National Cancer Institute states that cooking meat below creates "negligible amounts" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90%. Nitrosamines, present in processed and cooked foods, have also been noted as being carcinogenic, being linked to colon cancer.
Research has shown that grilling, barbecuing and smoking meat and fish increases levels of carcinogenic Polycyclic aromatic hydrocarbons (PAH). In Europe, grilled meat and smoked fish generally only contribute a small proportion of dietary PAH intake since they are a minor component of diet – most intake comes from cereals, oils and fats. However, in the US, grilled/barbecued meat is the second highest contributor of the mean daily intake of benzo[a]pyrene at 21% after ‘bread, cereal and grain’ at 29%.
Baking, grilling or broiling food, especially starchy foods, until a toasted crust is formed generates significant concentrations of acrylamide, a possible carcinogen.
Other health issues.
Cooking dairy products may reduce a protective effect against colon cancer. Researchers at the University of Toronto suggest that ingesting uncooked or unpasteurized dairy products (see also Raw milk) may reduce the risk of colorectal cancer. Mice and rats fed uncooked sucrose, casein, and beef tallow had one-third to one-fifth the incidence of microadenomas as the mice and rats fed the same ingredients cooked. This claim, however, is contentious. According to the Food and Drug Administration of the United States, health benefits claimed by raw milk advocates do not exist. "The small quantities of antibodies in milk are not absorbed in the human intestinal tract," says Barbara Ingham, PhD, associate professor and extension food scientist at the University of Wisconsin-Madison. "There is no scientific evidence that raw milk contains an anti-arthritis factor or that it enhances resistance to other diseases."
Heating sugars with proteins or fats can produce Advanced glycation end products ("glycotoxins"). These have been linked to ageing and health conditions such as diabetes and causing obesity.
Deep fried food in restaurants may contain high level of trans fat which is known to increase level of low-density lipoprotein that may increase risk of heart diseases and other conditions. However, many fast food chains have now switched to trans-fat-free alternatives for deep-frying.
Science of cooking.
The application of scientific knowledge to cooking and gastronomy has become known as molecular gastronomy. This is a subdiscipline of food science. Important contributions have been made by scientists, chefs and authors such as Herve This (chemist), Nicholas Kurti (physicist), Peter Barham (physicist), Harold McGee (author), Shirley Corriher (biochemist, author), Heston Blumenthal (chef), Ferran Adria (chef), Robert Wolke (chemist, author) and Pierre Gagnaire (chef).
Chemical processes central to cooking include the Maillard reaction – a form of non-enzymatic browning involving an amino acid, a reducing sugar and heat.
Home-cooking vs. factory cooking.
Although cooking has traditionally been a process carried out informally in a home or around a communal fire, cooking is also often carried out outside of personal quarters, for example at restaurants, or schools. Bakeries were one of the earliest forms of cooking outside the home, and bakeries in the past often offered the cooking of pots of food provided by their customers as an additional service. In the present day, factory food preparation has become common, with many "ready-to-eat" foods being prepared and cooked in factories and home cooks using a mixture of scratch made, and factory made foods together to make a meal.
"Home-cooking" may be associated with comfort food, and some commercially produced foods are presented through advertising or packaging as having been "home-cooked", regardless of their actual origin.
References.
United States Dept. of Agriculture. (Oct. 13, 2011). Safe Food Handling, Danger Zone. Retrieved from:http://www.fsis.usda.gov/factsheets/Danger_Zone/index.asp on 3/13/13.

</doc>
<doc id="5360" url="http://en.wikipedia.org/wiki?curid=5360" title="Card game">
Card game

A card game is any game using playing cards as the primary device with which the game is played, be they traditional or game-specific. Countless card games exist, including families of related games (such as poker). A small number of card games played with traditional decks have formally standardized rules, but most are folk games whose rules vary by region, culture, and person.
Many games that are not generally placed in the family of card games do in fact use cards for some aspect of their gameplay. Similarly, some games that are placed in the card game genre involve a board. The distinction is that the gameplay of a card game primarily depends on the use of the cards by players (the board is simply a guide for scorekeeping or for card placement), while board games (the principal non-card game genre to use cards) generally focus on the players' positions on the board, and use the cards for some secondary purpose.
Playing cards.
A card game is played with a "deck" or "pack" of playing cards which are identical in size and shape. Each card has two sides, the "face" and the "back". Normally the backs of the cards are indistinguishable. The faces of the cards may all be unique, or there can be duplicates. The composition of a deck is known to each player. In some cases several decks are shuffled together to form a single "pack" or "shoe".
The first playing cards appeared in the ninth century during Tang dynasty China.
The first reference to the card game in world history dates no later than the 9th century, when the "Collection of Miscellanea at Duyang", written by Tang Dynasty writer Su E, described Princess Tongchang (daughter of Emperor Yizong of Tang) playing the "leaf game" in 868 with members of the Wei clan (the family of the princess' husband). The Song dynasty statesman and historian Ouyang Xiu () has noted that paper playing cards arose in connection to an earlier development in the book format from scrolls to pages. During the Ming Dynasty (1368-1644), characters from popular novels such as the "Water Margin" were widely featured on the faces of playing cards. A precise description of Chinese money playing cards (in four suits) survived from the 15th century. Mahjong tiles are a 19th-century invention based on three-suited money playing card decks, similar to the way in which Rummikub tiles were derived recently from modern Western playing cards.
The same kind of games can also be played with tiles made of wood, plastic, bone, or similar materials. The most notable examples of such tile sets are dominoes, mahjong tiles and Rummikub tiles. Chinese dominoes are also available as playing cards. It is not clear whether Emperor Muzong of Liao really played with domino cards as early as 969, though. Legend dates the invention of dominoes in the year 1112, and the earliest known domino rules are from the following decade. 500 years later domino cards were reported as a new invention.
Playing cards first appeared in Europe in the last quarter of the 14th century. The earliest European references speak of a Saracen or Moorish game called "naib", and in fact an almost complete Mamluk Egyptian deck of 52 cards in a distinct oriental design has survived from around the same time, with the four suits "swords", "polo sticks", "cups" and "coins" and the ranks "king", "governor", "second governor", and "ten" to "one".
The 1430s in Italy saw the invention of the tarot deck, a full Latin-suited deck augmented by suitless cards with painted motifs that played a special role as trumps. Tarot, tarock and tarocchi games are still played with (subsets of) these decks in parts of Central Europe. A full tarot deck contains 14 cards in each suit; low cards labeled 1-10, and court cards "Valet" (Jack), "Chevalier" (Cavalier/Knight),"Dame" (Queen), and "Roi" (King), plus the Fool or Excuse card, and 21 trump cards. In the 18th century the card images of the traditional Italian tarot decks became popular in cartomancy and evolved into "esoteric" decks used primarily for the purpose; today most tarot decks sold in North America are the occult type, and are closely associated with fortune telling. In Europe, "playing tarot" decks remain popular for games, and have evolved since the 18th century to use regional suits (Spades/Hearts/Diamonds/Clubs in France, Leaves/Hearts/Bells/Acorns in Germany) as well as other familiar aspects of the Anglo-American deck such as corner card indices and "stamped" card symbols for non-court cards. Decks differ regionally based on the number of cards needed to play the games; the French tarot consists of the "full" 78 cards, while Germanic, Spanish and Italian Tarot variants remove certain values (usually low suited cards) from the deck, creating a deck with as few as 32 cards.
The French suits were introduced around 1480 and, in France, mostly replaced the earlier Latin suits of "swords", "clubs", "cups" and "coins". (which are still common in Spanish- and Portuguese-speaking countries as well as in some northern regions of Italy) The suit symbols, being very simple and single-color, could be stamped onto the playing cards to create a deck, thus only requiring special full-color card art for the court cards. This drastically simplifies the production of a deck of cards versus the traditional Italian deck, which used unique full-color art for each card in the deck. The French suits became popular in English playing cards in the 16th century (despite historic animosity between France and England), and from there were introduced to British colonies including North America. The rise of Western culture has led to the near-universal popularity and availability of French-suited playing cards even in areas with their own regional card art.
In Japan, a distinct 48-card hanafuda deck is popular. It is derived from 16th-century Portuguese decks, after undergoing a long evolution driven by laws enacted by the Tokugawa Shogunate attempting to ban the use of playing cards.
The best-known deck internationally is the 52-card Anglo-American deck used for such games as poker and contract bridge. It contains one card for each unique combination of thirteen "ranks" and the four French "suits" "spades", "hearts", "diamonds", and "clubs". The ranks (from highest to lowest in bridge and poker) are "ace", "king", "queen", "jack" (or "knave"), and the numbers from "ten" down to "two" (or "deuce"). The trump cards and "knight" cards from the French playing tarot are not included.
Originally the term "knave" was more common than "jack"; the card had been called a jack as part of the terminology of All-Fours since the 17th century, but the word was considered vulgar. (Note the exclamation by Estella in Charles Dickens's novel "Great Expectations": "He calls the knaves, Jacks, this boy!") However, because the card abbreviation for knave ("Kn") was so close to that of the king, it was very easy to confuse them, especially after suits and rankings were moved to the corners of the card in order to enable people to fan them in one hand and still see all the values. (The earliest known deck to place suits and rankings in the corner of the card is from 1693, but these cards did not become common until after 1864 when Hart reintroduced them along with the knave-to-jack change.) However, books of card games published in the third quarter of the 19th century evidently still referred to the "knave", and the term with this definition is still recognized in the United Kingdom.
Since the 19th century some decks have been specially printed for certain games. Old Maid, Phase 10, Rook, and Uno are examples of games that can be played with one or more 52 card decks but are usually played with custom decks. Cards play an important role in board games like Risk and Monopoly.
Typical structure of card games.
Number and association of players.
Any specific card game imposes restrictions on the number of players. The most significant dividing lines run between one-player games and two-player games, and between two-player games and multi-player games. Card games for one player are known as "solitaire" or "patience" card games. (See list of solitaire card games.) Generally speaking, they are in many ways special and atypical, although some of them have given rise to two- or multi-player games such as Spite and Malice.
In card games for two players, usually not all cards are distributed to the players, as they would otherwise have perfect information about the game state. Two-player games have always been immensely popular and include some of the most significant card games such as piquet, bezique, sixty-six, klaberjass, gin rummy and cribbage. Many multi-player games started as two-player games that were adapted to a greater number of players. For such adaptations a number of non-obvious choices must be made beginning with the choice of a game orientation.
One way of extending a two-player game to more players is by building two teams of equal size. A common case is four players in two fixed partnerships, sitting crosswise as in whist and contract bridge. Partners sit opposite to each other and cannot see each other's hands. If communication between the partners is allowed at all, then it is usually restricted to a specific list of permitted signs and signals. 17th-century French partnership games such as triomphe were special in that partners sat next to each other and were allowed to communicate freely so long as they did not exchange cards or played out of order.
Another way of extending a two-player game to more players is as a "cut-throat" game, in which all players fight on their own, and win or lose alone. Most cut-throat card games are "round games", i.e. they can be played by any number of players starting from two or three, so long as there are enough cards for all.
For some of the most interesting games such as ombre, tarot and skat, the associations between players change from hand to hand. Ultimately players all play on their own, but for each hand, some game mechanism divides the players into two teams. Most typically these are "solo games", i.e. games in which one player becomes the soloist and has to achieve some objective against the others, who form a team and win or lose all their points jointly. But in games for more than three players, there may also be a mechanism that selects two players who then have to play against the others.
Direction of play.
The players of a card game normally form a circle around a table or other space that can hold cards. The "game orientation" or "direction of play", which is only relevant for three or more players, can be either clockwise or counter-clockwise. It is the direction in which various roles in the game proceed. Most regions have a traditional direction of play, such as:
Europe is roughly divided into a clockwise area in the north and a counter-clockwise area in the south. The boundary runs between France, Germany, Austria (mostly), the Czech Republic, Poland, Ukraine and Russia (clockwise) and Portugal, Spain, Switzerland, Italy, Slovakia, Hungary, Romania, Turkey (counter-clockwise).
Games that originate in a region with a strong preference are often initially played in the original direction, even in regions that prefer the opposite direction. For games that have official rules and are played in tournaments, the direction of play is often prescribed in those rules.
Determining who deals.
Most games have some form of asymmetry between players. The roles of players are normally expressed in terms of the "dealer", i.e. the player whose task it is to shuffle the cards and distribute them to the players. Being the dealer can be a (minor or major) advantage or disadvantage, depending on the game. Therefore, after each played hand, the deal normally passes to the next player according to the game orientation.
As it can still be an advantage or disadvantage to be the first dealer, there are some standard methods for determining who is the first dealer. A common method is by cutting, which works as follows. One player shuffles the deck and places it on the table. Each player lifts a packet of cards from the top, reveals its bottom card, and returns it to the deck. The player who reveals the highest (or lowest) card becomes dealer. In case of a tie, the process is repeated by the tied players. For some games such as whist this process of cutting is part of the official rules, and the hierarchy of cards for the purpose of cutting (which need not be the same as that used otherwise in the game) is also specified. But in general any method can be used, such as tossing a coin in case of a two-player game, drawing cards until one player draws an ace, or rolling dice.
Hands, rounds and games.
A "hand" is a unit of the game that begins with the dealer shuffling and dealing the cards as described below, and ends with the players scoring and the next dealer being determined. The set of cards that each player receives and holds in his or her hands is also known as that player's hand. The hand is over when the players have finished playing their hands. Most often this occurs when one player (or all) has no cards left. The player who sits after the dealer in the direction of play is known as "eldest hand" (or in two-player games as "elder hand"). A "game round" consists of as many hands as there are players. After each hand, the deal is passed on in the direction of play, i.e. the previous eldest hand becomes the new dealer. Normally players score points after each hand. A game may consist of a fixed number of rounds. Alternatively it can be played for a fixed number of points. In this case it is over with the hand in which a player reaches the target score.
Shuffling.
Shuffling is the process of bringing the cards of a pack into a random order. There are a large number of techniques with various advantages and disadvantages. "Riffle shuffling" is a method in which the deck is divided into two roughly equal-sized halves that are bent and then released, so that the cards interlace. Repeating this process several times randomizes the deck well, but the method is harder to learn than some others and may damage the cards. The "overhand shuffle" and the "Hindu shuffle" are two techniques that work by taking batches of cards from the top of the deck and reassembling them in the opposite order. They are easier to learn but must be repeated more often. A method suitable for small children consists in spreading the cards on a large surface and moving them around before picking up the deck again. This is also the most common method for shuffling tiles such as dominoes.
For casino games that are played for large sums it is vital that the cards be properly randomised, but for many games this is less critical, and in fact player experience can suffer when the cards are shuffled too well. The official skat rules stipulate that the cards are "shuffled well", but according to a decision of the German skat court, a one-handed player should ask another player to do the shuffling, rather than use a shuffling machine, as it would shuffle the cards "too" well. French belote rules go so far as to prescribe that the deck never be shuffled between hands.
Deal.
The dealer takes all of the cards in the pack, arranges them so that they are in a uniform stack, and shuffles them. In strict play, the dealer then offers the deck to the previous player (in the sense of the game direction) for "cutting". If the deal is clockwise, this is the player to the dealer's right; if counterclockwise, it is the player to the dealer's left. The invitation to cut is made by placing the pack, face downward, on the table near the player who is to cut: who then lifts the upper portion of the pack clear of the lower portion and places it alongside. (Normally the two portions have about equal size. Strict rules often indicate that each portion must contain a certain minimum number of cards, such as three or five.) The formerly lower portion is then replaced on top of the formerly upper portion. Instead of cutting, one may also knock on the deck to indicate that one trusts the dealer to have shuffled fairly.
The actual "deal" (distribution of cards) is done in the direction of play, beginning with eldest hand. The dealer holds the pack, face down, in one hand, and removes cards from the top of it with his or her other hand to distribute to the players, placing them face down on the table in front of the players to whom they are dealt. The cards may be dealt one at a time, or in batches of more than one card; and either the entire pack or a determined number of cards are dealt out. The undealt cards, if any, are left face down in the middle of the table, forming the "stock" (also called the talon, widow, skat or kitty depending on the game and region).
Throughout the shuffle, cut, and deal, the dealer should prevent the players from seeing the faces of any of the cards. The players should not try to see any of the faces. Should a player accidentally see a card, other than one's own, proper etiquette would be to admit this. It is also dishonest to try to see cards as they are dealt, or to take advantage of having seen a card. Should a card accidentally become exposed, (visible to all), any player can demand a redeal (all the cards are gathered up, and the shuffle, cut, and deal are repeated) or that the card be replaced randomly into the deck ("burning" it) and a replacement dealt from the top to the player who was to receive the revealed card.
When the deal is complete, all players pick up their cards, or 'hand', and hold them in such a way that the faces can be seen by the holder of the cards but not the other players, or vice versa depending on the game. It is helpful to fan one's cards out so that if they have corner indices all their values can be seen at once. In most games, it is also useful to sort one's hand, rearranging the cards in a way appropriate to the game. For example, in a trick-taking game it may be easier to have all one's cards of the same suit together, whereas in a rummy game one might sort them by rank or by potential combinations.
Rules.
A new card game starts in a small way, either as someone's invention, or as a modification of an existing game. Those playing it may agree to change the rules as they wish. The rules that they agree on become the "house rules" under which they play the game. A set of house rules may be accepted as valid by a group of players wherever they play, as it may also be accepted as governing all play within a particular house, café, or club.
When a game becomes sufficiently popular, so that people often play it with strangers, there is a need for a generally accepted set of rules. This need is often met when a particular set of house rules becomes generally recognized. For example, when Whist became popular in 18th-century England, players in the Portland Club agreed on a set of house rules for use on its premises. Players in some other clubs then agreed to follow the "Portland Club" rules, rather than go to the trouble of codifying and printing their own sets of rules. The Portland Club rules eventually became generally accepted throughout England and Western cultures.
It should be noted that there is nothing static or "official" about this process. For the majority of games, there is no one set of universal rules by which the game is played, and the most common ruleset is no more or less than that. Many widely played card games, such as Canasta and Pinochle, have no official regulating body. The most common ruleset is often determined by the most popular distribution of rulebooks for card games. Perhaps the original compilation of popular playing card games was collected by Edmund Hoyle, a self-made authority on many popular parlor games. The U.S. Playing Card Company now owns the eponymous Hoyle brand, and publishes a series of rulebooks for various families of card games that have largely standardized the games' rules in countries and languages where the rulebooks are widely distributed. However, players are free to, and often do, invent "house rules" to supplement or even largely replace the "standard" rules.
If there is a sense in which a card game can have an "official" set of rules, it is when that card game has an "official" governing body. For example, the rules of tournament bridge are governed by the World Bridge Federation, and by local bodies in various countries such as the American Contract Bridge League in the U.S., and the English Bridge Union in England. The rules of skat are governed by The International Skat Players Association and in Germany by the Deutscher Skatverband which publishes the "Skatordnung". The rules of French tarot are governed by the Fédération Française de Tarot. The rules of Poker's variants are largely traditional, but enforced by the World Series of Poker and the World Poker Tour organizations which sponsor tournament play. Even in these cases, the rules must only be followed exactly at games sanctioned by these governing bodies; players in less formal settings are free to implement agreed-upon supplemental or substitute rules at will.
Rule infractions.
An infraction is any action which is against the rules of the game, such as playing a card when it is not one's turn to play or the accidental exposure of a card, informally known as 'bleeding'.
In many official sets of rules for card games, the rules specifying the penalties for various infractions occupy more pages than the rules specifying how to play correctly. This is tedious, but necessary for games that are played seriously. Players who intend to play a card game at a high level generally ensure before beginning that all agree on the penalties to be used. When playing privately, this will normally be a question of agreeing house rules. In a tournament there will probably be a tournament director who will enforce the rules when required and arbitrate in cases of doubt.
If a player breaks the rules of a game deliberately, this is cheating. Most card players would refuse to play cards with a known cheat. The rest of this section is therefore about accidental infractions, caused by ignorance, clumsiness, inattention, etc.
As the same game is played repeatedly among a group of players, precedents build up about how a particular infraction of the rules should be handled. For example, "Sheila just led a card when it wasn't her turn. Last week when Jo did that, we agreed ... etc." Sets of such precedents tend to become established among groups of players, and to be regarded as part of the house rules. Sets of house rules become formalized, as described in the previous section. Therefore, for some games, there is a "proper" way of handling infractions of the rules. But for many games, without governing bodies, there is no standard way of handling infractions.
In many circumstances, there is no need for special rules dealing with what happens after an infraction. As a general principle, the person who broke a rule should not benefit by it, and the other players should not lose by it. An exception to this may be made in games with fixed partnerships, in which it may be felt that the partner(s) of the person who broke a rule should also not benefit. The penalty for an accidental infraction should be as mild as reasonable, consistent with there being no possible benefit to the person responsible.
Types.
Trick-taking games.
The object of a trick-taking game is based on the play of multiple rounds, or tricks, in each of which each player plays a single card from their hand, and based on the values of played cards one player wins or "takes" the trick. The specific object varies with each game and can include taking as many tricks as possible, taking as many scoring cards within the tricks won as possible, taking as few tricks (or as few penalty cards) as possible, taking a particular trick in the hand, or taking an exact number of tricks. Bridge, Whist, Euchre, Spades, and the various Tarot card games are popular examples.
Matching games.
The object of Rummy, and various other melding or matching games, is to acquire the required groups of matching cards before an opponent can do so. In Rummy, this is done through drawing and discarding, and the groups are called melds. Mahjong is a very similar game played with tiles instead of cards. Non-Rummy examples of match-type games generally fall into the "fishing" genre and include the children's games Go Fish, Old Maid and Blue Canary.
Shedding games.
In a shedding game, players start with a hand of cards, and the object of the game is to be the first player to discard all cards from one's hand. Common shedding games in the US include Crazy Eights (commercialized by Mattel as Uno) and Daihinmin. Some matching-type games are also shedding-type games; some variants of Rummy such as Phase 10, Rummikub, the bluffing game I Doubt It, and the children's game Old Maid, fall into both categories.
Accumulating games.
The object of an accumulating game is to acquire all cards in the deck. Examples include most War type games, and games involving slapping a discard pile such as Slapjack. Egyptian War has both of these features.
Fishing games.
In fishing games, cards from the hand are played against cards in a layout on the table, capturing table cards if they match. Fishing games are popular in many nations, including China, where there are many diverse fishing games. Scopa is considered one of the national card games of Italy. Cassino is the only fishing game to be widely played in English-speaking countries. Seep is a classic Indian fishing card game mainly popular in northern parts of India.
Comparing games.
Comparing card games are those where hand values are compared to determine the winner, also known as "vying" or "showdown" games. Poker, blackjack, and baccarat are examples of comparing card games. As seen, nearly all of these games are designed as gambling games.
Solitaire (Patience) games.
Solitaire games are designed to be played by one player. Most games begin with a specific layout of cards, called a tableau, and the object is then either to construct a more elaborate final layout, or to clear the tableau and/or the draw pile or "stock" by moving all cards to one or more "discard" or "foundation" piles.
Drinking card games.
Drinking card games are, true to their name, a subset of drinking games using cards, in which the object in playing the game is either to drink or to force others to drink. Many games are simply ordinary card games with the establishment of "drinking rules"; Asshole (Presidents), for instance, is virtually identical to Daihinmin but with additional rules governing drinking. Poker can also be played using a number of drinks as the wager. Another game often played as a drinking game is Toepen, quite popular in the Netherlands. Some card games are designed specifically to be played as drinking games.
Multi-genre games.
Many games borrow elements from more than one type of game. The most common combination is that of matching and shedding, as in some variants of Rummy, Old Maid and Go Fish. However, many multi-genre games involve different stages of play for each hand. The most common multi-stage combination is a "trick-and-meld" game, such as Pinochle or Belote. Other multi-stage, multi-genre games include Poke, Flaps, Skitgubbe and Tichu.
Collectible card games (CCGs).
Collectible card games are defined by the use of decks of proprietary cards that differ between players. The contents of these decks are a subset of a very large pool of available cards which have differing effects, costs, and art. A player accumulates his or her deck through purchase or trade for desirable cards, and each player uses their own deck to play against the other. "" and "Yu-Gi-Oh!" are well-known collectible card games. Such games are also created to capitalize on the popularity of other forms of entertainment, such as "Pokémon" and "Marvel Comics" which both have had CCGs created around them.
Casino or gambling card games.
These games revolve around wagers of money. Though virtually any game in which there are winning and losing outcomes can be wagered on, these games are specifically designed to make the betting process a strategic part of the game. Some of these games involve players betting against each other, such as poker, while in others, like blackjack, players wager against the house.
Poker games.
Poker is a family of gambling games in which players bet into a pool, called the pot, value of which changes as the game progresses that the value of the hand they carry will beat all others according to the ranking system. Variants largely differ on how cards are dealt and the methods by which players can improve a hand. For many reasons, including its age and its popularity among Western militaries, it is one of the most universally known card games in existence.
Other card games.
Many other card games have been designed and published on a commercial or amateur basis. In some cases, the game uses the standard 52-card deck, but the object is unique. In Eleusis, for example, players play single cards, and are told whether the play was legal or illegal, in an attempt to discover the underlying rules made up by the dealer.
Most of these games however typically use a specially made deck of cards designed specifically for the game (or variations of it). The decks are thus usually proprietary, but may be created by the game's players. Uno, Phase 10, Set, Slamwich, 1000 Blank White Cards, and Sopio are popular dedicated-deck card games; 1000 Blank White Cards is unique in that the cards for the game are designed by the players of the game while playing it; there is no commercially-available deck advertised as such.
Fictional card games.
Many games, including card games, are fabricated by science fiction authors and screenwriters to distance a culture depicted in the story from present-day Western culture. They are commonly used as filler to depict background activities in an atmosphere like a bar or rec room, but sometimes the drama revolves around the play of the game. Some of these games, such as Pyramid from "Battlestar Galactica", become real card games as the holder of the intellectual property develops and markets a suitable deck and ruleset for the game, while others, such as "Exploding Snap" from the Harry Potter franchise, lack sufficient descriptions of rules, or depend on cards or other hardware that are infeasible or physically impossible.

</doc>
<doc id="5361" url="http://en.wikipedia.org/wiki?curid=5361" title="Cross-stitch">
Cross-stitch

Cross-stitch is a popular form of counted-thread embroidery in which X-shaped stitches in a tiled, raster-like pattern are used to form a picture. Cross-stitch is often executed on easily countable evenweave fabric called aida cloth. The stitcher counts the threads in each direction so that the stitches are of uniform size and appearance. This form of cross-stitch is also called counted cross-stitch in order to distinguish it from other forms of cross-stitch. Sometimes cross-stitch is done on designs printed on the fabric (stamped cross-stitch); the stitcher simply stitches over the printed pattern.
Fabrics used in cross-stitch include aida, linen and mixed-content fabrics called 'evenweave'. All cross stitch fabrics are technically "evenweave," it refers to the fact that the fabric is woven to make sure that there are the same number of threads in an inch both left to right and top to bottom (vertically and horizontally). Fabrics are categorized by threads per inch (referred to as 'count'), which can range from 11 to 40 count. Aida fabric has a lower count because it is made with two threads grouped together for ease of stitching. Cross stitch projects are worked from a gridded pattern and can be used on any count fabric, the count of the fabric determines the size of the finished stitching.
History.
Cross-stitch is the oldest form of embroidery and can be found all over the world. Many folk museums show examples of clothing decorated with cross-stitch, especially from continental Europe and Asia.
Two-dimensional (unshaded) cross-stitch in floral and geometric patterns, usually worked in black and red cotton floss on linen, is characteristic of folk embroidery in Eastern and Central Europe.
The cross stitch sampler is called that because it was generally stitched by a young girl to learn how to stitch and to record alphabet and other patterns to be used in her household sewing. These samples of her stitching could be referred back to over the years. Often, motifs and initials were stitched on household items to identify their owner, or simply to decorate the otherwise-plain cloth. In the United States, the earliest known cross-stitch sampler is currently housed at Pilgrim Hall in Plymouth, Massachusetts. The sampler was created by Loara Standish, daughter of Captain Myles Standish and pioneer of the Leviathan stitch, circa 1653.
Traditionally, cross-stitch was used to embellish items like household linens, tablecloths, dishcloths, and doilies (only a small portion of which would actually be embroidered, such as a border). Although there are many cross-stitchers who still employ it in this fashion, it is now increasingly popular to work the pattern on pieces of fabric and hang them on the wall for decoration. Cross stitch is also often used to make greeting cards, pillowtops, or as inserts for box tops, coasters and trivets.
Multicoloured, shaded, painting-like patterns as we know them today are a fairly modern development, deriving from similar shaded patterns of Berlin wool work of the mid-nineteenth century. Besides designs created expressly for cross stitch, there are software programs that convert a photograph or a fine art image into a chart suitable for stitching. One stunning example of this is in the cross stitched reproduction of the Sistene Chapel charted and stitched by Joanna Lopianowski-Roberts.
There are many cross-stitching "guilds" and groups across the United States and Europe which offer classes, collaborate on large projects, stitch for charity, and provide other ways for local cross-stitchers to get to know one another. Individually owned local needlework shops (LNS) often have stitching nights at their shops, or host weekend stitching retreats.
Today cotton floss is the most common embroidery thread. It is a thread made of mercerized cotton, composed of six strands that are only loosely twisted together and easily separable. While there are other manufacturers, the two most-commonly used (and oldest) brands are DMC and Anchor , both of which have been manufacturing embroidery floss since the 1800s.
Other materials used are pearl (or perle) cotton, Danish flower thread, silk and Rayon. Different wool threads, metallic threads or other novelty threads are also used, sometimes for the whole work, but often for accents and embellishments. Hand-dyed cross stitch floss is created just as the name implies - it is dyed by hand. Because of this, there are variations in the amount of color throughout the thread. Some variations can be subtle, while some can be a huge contrast. Some also have more than one color per thread, which in the right project, creates amazing results.
Cross stitch is widely used in traditional Palestinian dressmaking.
Related stitches and forms of embroidery.
Other stitches are also often used in cross-stitch, among them ¼, ½, and ¾ stitches and backstitches.
Cross-stitch is often used together with other stitches. A cross stitch can come in a variety of prostational forms. It is sometimes used in crewel embroidery, especially in its more modern derivatives. It is also often used in needlepoint.
A specialized historical form of embroidery using cross-stitch is Assisi embroidery.
There are many stitches which are related to cross-stitch and were used in similar ways in earlier times. The best known are Italian cross-stitch, Celtic Cross Stitch, Irish Cross Stitch, long-armed cross-stitch, Ukrainian cross-stitch and Montenegrin stitch. Italian cross-stitch and Montenegrin stitch are reversible, meaning the work looks the same on both sides. These styles have a slightly different look than ordinary cross-stitch. These more difficult stitches are rarely used in mainstream embroidery, but they are still used to recreate historical pieces of embroidery or by the creative and adventurous stitcher.
The double cross-stitch, also known as a Leviathan stitch or Smyrna cross stitch, combines a cross-stitch with an upright cross-stitch.
Berlin wool work and similar petit point stitchery resembles the heavily shaded, opulent styles of cross-stitch, and sometimes also used charted patterns on paper.
Cross-stitch is often combined with other popular forms of embroidery, such as Hardanger embroidery or blackwork embroidery. Cross-stitch may also be combined with other work, such as canvaswork or drawn thread work. Beadwork and other embellishments such as paillettes, charms, small buttons and speciality threads of various kinds may also be used.
Recent trends in the UK.
Cross-stitch has become increasingly popular with the younger generation of the United Kingdom in recent years. The Great Recession has also seen renewal of interest in home crafts. Retailers such as John Lewis experienced a 17% rise in sales of haberdashery products between 2009 and 2010. Hobbycraft, a chain of stores selling craft supplies, also enjoyed an 11% increase in sales over the past year. The chain is said to have benefited from the "make do and mend" mentality of the credit crisis, which has driven people to make their own cards and gifts.
Knitting and cross stitching have become more popular hobbies for a younger market, in contrast to its traditional reputation as a hobby for retirees. Sewing and craft groups such as Stitch and Bitch London have resurrected the idea of the traditional craft club. At Clothes Show Live 2010 there was a new area called "Sknitch" promoting modern sewing, knitting and embroidery.
In a departure from the traditional designs associated with cross stitch, there is a current trend for more postmodern or tongue-in-cheek designs featuring retro images or contemporary sayings. It is linked to a concept known as 'subversive cross stitch', which involves more risque designs, often fusing the traditional sampler style with sayings designed to shock or be incongruous with the old-fashioned image of cross stitch.
Stitching designs on other materials can be accomplished by using a Waste Canvas. This waste canvas is a temporary gridded canvas similar to regular canvas used for embroidery that is held together by a water soluble glue, this is removed after completion of stitch design.

</doc>
<doc id="5362" url="http://en.wikipedia.org/wiki?curid=5362" title="Casino game">
Casino game

Games available in most casinos are commonly called casino games. In a casino game, the players gamble casino chips on various possible random outcomes or combinations of outcomes. Casino games are available in online casinos, where permitted by law. Casino games can also be played outside of casinos for entertainment purposes, some on machines that simulate gambling.
Categories.
There are three general categories of casino games: table games, electronic gaming machines, and random number ticket games such as Keno and simulated racing. Gaming machines, such as slot machines and pachinko, are usually played by one player at a time and do not require the involvement of casino employees to play. Random number games are based upon the selection of random numbers, either from a computerized random number generator or from other gaming equipment. Random number games may be played at a table, such as roulette, or through the purchase of paper tickets or cards, such as keno or bingo.
House advantage.
Casino games generally provide a predictable long-term advantage to the casino, or "house", while offering the player the possibility of a large short-term payout. Some casino games have a skill element, where the player makes decisions; such games are called "random with a tactical element". While it is possible through skillful play to minimize the house advantage, it is extremely rare that a player has sufficient skill to completely eliminate his inherent long-term disadvantage (the house edge (HE) or house vigorish) in a casino game. Such a skill set would involve years of training, an extraordinary memory and numeracy, and/or acute visual or even aural observation, as in the case of wheel clocking in Roulette.
The player's disadvantage is a result of the casino not paying winning wagers according to the game's "true odds", which are the payouts that would be expected considering the odds of a wager either winning or losing. For example, if a game is played by wagering on the number that would result from the roll of one die, true odds would be 5 times the amount wagered since there is a 1 in 6 chance of any single number appearing, assuming that you get the original amount wagered back. However, the casino may only pay 4 times the amount wagered for a winning wager. 
The house edge or vigorish is defined as the casino profit expressed as the percentage of the player's original bet. (In games such as Blackjack or Spanish 21, the final bet may be several times the original bet, if the player double and splits.) 
In American Roulette, there are two "zeroes" (0, 00) and 36 non-zero numbers (18 red and 18 black). If a player bets 1 unit on red, his chance of winning 1 unit is therefore 18/38 and his chance of losing 1 unit is 20/38. The player's expected value is EV = (18/38 x 1) + (20/38 x -1) = 18/38 - 20/38 = -2/38 = -5.26%. Therefore, the house edge is 5.26%. After 10 spins, betting 1 unit per spin, the average house profit will be 10 x 1 x 5.26% = 0.53 units. Of course, the casino may not win exactly 53 cents of a unit; this figure is the average casino profit from each player if it had millions of players each betting for 10 spins at 1 unit per spin. European and French roulette wheels have only one "zero" and therefore the house advantage (ignoring the en prison rule) is equal to 1/37 = 2.7%.
Poker has become one of the most popular games in the casino. It is a game of skill and the only game where the players are competing against each other and not the house. There are several variations of poker that played in the casino card rooms. 
The house edge of casino games vary greatly with the game. Keno can have house edges up to 25%, slot machines can have up to 15%, while most Australian Pontoon games have house edges between 0.3% and 0.4%.
The calculation of the Roulette house edge was a trivial exercise; for other games, this is not usually the case. Combinatorial analysis and/or computer simulation is necessary to complete the task.
In games which have a skill element, such as Blackjack or Spanish 21, the house edge is defined as the house advantage from optimal play (without the use of advanced techniques such as card counting), on the first hand of the shoe (the container that holds the cards). The set of the optimal plays for all possible hands is known as "basic strategy" and is highly dependent on the specific rules, and even the number of decks used. Good Blackjack and Spanish 21 games have house edges below 0.5%.
Traditionally the majority casinos have refused to reveal the house edge information for their slots games and due to the unknown number of symbols and weightings of the reels, in most cases this is a lot more difficult to calculate than for other casino games. However due to some online properties revealing this information and some independent research conducted by Michael Shackleford in the offline sector, this pattern is slowly changing. 
Standard deviation.
The luck factor in a casino game is quantified using standard deviations (SD). The standard deviation of a simple game like Roulette can be calculated using the binomial distribution. In the binomial distribution, SD = sqrt ("npq" ), where "n" = number of rounds played, "p" = probability of winning, and "q" = probability of losing. The binomial distribution assumes a result of 1 unit for a win, and 0 units for a loss, rather than -1 units for a loss, which doubles the range of possible outcomes. Furthermore, if we flat bet at 10 units per round instead of 1 unit, the range of possible outcomes increases 10 fold. 
SD (Roulette, even-money bet) = 2"b" sqrt("npq" ), where "b" = flat bet per round, "n" = number of rounds, "p" = 18/38, and "q" = 20/38.
For example, after 10 rounds at 1 unit per round, the standard deviation will be 2 x 1 x sqrt(10 x 18/38 x 20/38) = 3.16 units. After 10 rounds, the expected loss will be 10 x 1 x 5.26% = 0.53. As you can see, standard deviation is many times the magnitude of the expected loss. 
The standard deviation for Pai Gow poker is the lowest out of all common casinos. Many, particularly slots, have extremely high standard deviations. As the size of the potential payouts increase, so does the standard deviation.
As the number of rounds increases, eventually, the expected loss will exceed the standard deviation, many times over. From the formula, we can see the standard deviation is proportional to the square root of the number of rounds played, while the expected loss is proportional to the number of rounds played. As the number of rounds increases, the expected loss increases at a much faster rate. This is why it is impossible for a gambler to win in the long term. It is the high ratio of short-term standard deviation to expected loss that fools gamblers into thinking that they can win. 
It is important for a casino to know both the house edge and variance for all of their games. The house edge tells them what kind of profit they will make as percentage of turnover, and the variance tells them how much they need in the way of cash reserves. The mathematicians and computer programmers that do this kind of work are called gaming mathematicians and gaming analysts. Casinos do not have in-house expertise in this field, so outsource their requirements to experts in the gaming analysis field.

</doc>
<doc id="5363" url="http://en.wikipedia.org/wiki?curid=5363" title="Video game">
Video game

A video game is an electronic game that involves human interaction with a user interface to generate visual feedback on a video device. The word "video" in "video game" traditionally referred to a raster display device, but it now implies any type of display device that can produce two- or three-dimensional images. The electronic systems used to play video games are known as platforms; examples of these are personal computers and video game consoles. These platforms range from large mainframe computers to small handheld devices. Specialized video games such as arcade games, while previously common, have gradually declined in use. Video games have gone on to become an art form and industry.
The input device primarily used to manipulate video games is called a game controller, and varies across platforms. For example, a controller might consist of only a button and a joystick, while another may feature a dozen buttons and one or more joysticks. Early personal computer games often needed a keyboard for gameplay, or more commonly, required the user to buy a separate joystick with at least one button. Many modern computer games allow or require the player to use a keyboard and a mouse simultaneously. A few of the most common game controllers are gamepads, mouses, keyboards, and joysticks. In recent years, additional methods of input have emerged such as camera-based player observation for video game consoles and touch-sensitive screens on mobile devices.
Video games typically use additional means of providing interactivity and information to the player. Audio is almost universal, using sound reproduction devices, such as speakers and headphones. Other feedback may come via haptic peripherals, such as vibration or force feedback, with vibration sometimes used to simulate force feedback.
History.
Early games used interactive electronic devices with various display formats. The earliest example is from 1947—a "Cathode ray tube Amusement Device" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992.
Inspired by radar display tech, it consisted of an analog device that allowed a user to control a vector-drawn dot on the screen to simulate a missile being fired at targets, which were drawings fixed to the screen.
Other early examples include:
Each game used different means of display: NIMROD used a panel of lights to play the game of Nim, OXO used a graphical display to play tic-tac-toe "Tennis for Two" used an oscilloscope to display a side view of a tennis court, and "Spacewar!" used the DEC PDP-1's vector display to have two spaceships battle each other.
In 1971, "Computer Space", created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips. The game was featured in the 1973 science fiction film "Soylent Green". "Computer Space" was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the "Brown Box", it also used a standard television. These were followed by two versions of Atari's "Pong"; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity. The commercial success of "Pong" led numerous other companies to develop "Pong" clones and their own systems, spawning the video game industry.
A flood of "Pong" clones eventually led to the video game crash of 1977, which came to an end with the mainstream success of Taito's 1978 shooter game "Space Invaders", marking the beginning of the golden age of arcade video games and inspiring dozens of manufacturers to enter the market. The game inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants, and convenience stores. The game also became the subject of numerous articles and stories on television and in newspapers and magazines, establishing video gaming as a rapidly growing mainstream hobby. "Space Invaders" was soon licensed for the Atari VCS (later known as Atari 2600), becoming the first "killer app" and quadrupling the console's sales. This helped Atari recover from their earlier losses, and in turn the Atari VCS revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. The home video game industry was revitalized shortly afterwards by the widespread success of the Nintendo Entertainment System, which marked a shift in the dominance of the video game industry from the United States to Japan during the third generation of consoles.
Overview.
Platforms.
The term "platform" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate. The term "system" is also commonly used.
In common use a "PC game" refers to a form of media that involves a player interacting with an IBM PC compatible personal computer connected to a video monitor. A "console game" is played on a specialized electronic device that connects to a common television set or composite video monitor. A "handheld" gaming device is a self-contained electronic device that is portable and can be held in a user's hands. "Arcade game" generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special cabinet. These distinctions are not always clear and there may be games that bridge one or more platforms. In addition to personal computers, there are multiple other devices which have the ability to play games but are not dedicated video game machines, such as mobile phones, PDAs and graphing calculators.
The web browser has also established itself as platform in its own right while providing a cross-platform environment for video games designed to be played on a wide spectrum of hardware from personal computers to smartphones to name a few. This in turn has generated new terms to qualify classes of web browser based games. These games may be identified based on the website that they appear, such as with "Facebook" games. Others are named based on the programming platform used to develop them, such as Java and Flash games.
Genres.
A video game, like most other forms of media, may be categorized into genres based on many factors such as method of game play, types of goals, art style, interactivity and more. Because genres are dependent on content for definition, genres have changed and evolved as newer styles of video games have come into existence. Ever advancing technology and production values related to video game development have fostered more lifelike and complex games which have in turn introduced or enhanced genre possibilities (e.g., virtual pets), pushed the boundaries of existing video gaming or in some cases add new possibilities in play (such as that seen with titles specifically designed for devices like Sony's EyeToy). Some genres represent combinations of others, such as massively multiplayer online role-playing games, or, more commonly, MMORPGs. It is also common to see higher level genre terms that are collective in nature across all other genres such as with action, music/rhythm or horror-themed video games.
Classifications.
Casual games.
Casual games derive their name from their ease of accessibility, simple to understand gameplay and quick to grasp rule sets. Additionally, casual games frequently support the ability to jump in and out of play on demand. Casual games as a format existed long before the term was coined and include video games such as Solitaire or Minesweeper which can commonly be found pre-installed with many versions of the Microsoft Windows operating system.
Examples of genres within this category are hidden object, match three, time management, tetris or many of the tower defense style games. Casual games are generally sold through online retailers such as PopCap, Zylom, Vans Video Games and GameHouse or provided for free play through web portals such as Newgrounds.
While casual games are most commonly played on personal computers, cellphones or PDAs, they can also be found on many of the on-line console system download services (e.g., Xbox Live, PlayStation Network, or WiiWare).
Serious games.
Serious games are games that are designed primarily to convey information or a learning experience of some sort to the player. Some serious games may even fail to qualify as a video game in the traditional sense of the term. Educational software does not typically fall under this category (e.g., touch typing tutors, language learning, etc.) and the primary distinction would appear to be based on the title's primary goal as well as target age demographics. As with the other categories, this description is more of a guideline than a rule.
Serious games are games generally made for reasons beyond simple entertainment and as with the core and casual games may include works from any given genre, although some such as exergames, educational games, or propaganda games may have a higher representation in this group due to their subject matter. These games are typically designed to be played by professionals as part of a specific job or for skill set improvement. They can also be created to convey social-political awareness on a specific subject.
One of the longest running serious games franchises would be Microsoft Flight Simulator first published in 1982 under that name. The United States military uses virtual reality based simulations, such as VBS1 for training exercises, as do a growing number of first responder roles (e.g., police, fire fighter, EMT). One example of a non-game environment utilized as a platform for serious game development would be the virtual world of "Second Life", which is currently used by several United States governmental departments (e.g., NOAA, NASA, JPL), Universities (e.g., Ohio University, MIT) for educational and remote learning programs and businesses (e.g., IBM, Cisco Systems) for meetings and training.
Tactical media in video games plays a crucial role in making a statement or conveying a message on important relevant issues. This form of media allows for a broader audience to be able to receive and gain access to certain information that otherwise may not have reached such people. An example of tactical media in video games would be newsgames. These are short games related to contemporary events designed to illustrate a point. For example, Take Action Games is a game studio collective that was co-founded by Susana Ruiz and has made successful serious games. Some of these games include "Darfur is Dying", "Finding Zoe", and "In The Balance". All of these games bring awareness to important issues and events in an intelligent and well thought out manner.
Educational games.
On 23 September 2009, U.S. President Barack Obama launched a campaign called "Educate to Innovate" aimed at improving the technological, mathematical, scientific and engineering abilities of American students. This campaign states that it plans to harness the power of interactive games to help achieve the goal of students excelling in these departments. This campaign has stemmed into many new opportunities for the video game realm and has contributed to many new competitions. Some of these competitions include the Stem National Video Game Competition and the Imagine Cup. Both of these examples are events that bring a focus to relevant and important current issues that are able to be addressed in the sense of video games to educate and spread knowledge in a new form of media. www.NobelPrize.org uses games to entice the user to learn about information pertaining to the Nobel prize achievements while engaging in a fun to play video game. There are many different types and styles of educational games all the way from counting to spelling to games for kids and games for adults. Some other games do not have any particular targeted audience in mind and intended to simply educate or inform whoever views or plays the game.
Development.
Video game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.
In the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the "one-man shop" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some cellphones and PDAs).
With the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need to be able to pay their staff a competitive wage in order to attract and retain the best talent, while publishers are constantly looking to keep costs down in order to maintain profitability on their investment. Typically, a video game console development team can range in sizes of anywhere from 5 to 50 people, with some teams exceeding 100. In May 2009, one game project was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.
Downloadable content.
A phenomenon of additional game content at a later date, often for additional funds, began with digital video game distribution known as downloadable content (DLC). Developers can use digital distribution to issue new storylines after the main game is released, such as Rockstar Games with "Grand Theft Auto IV" (' and '), or Bethesda with "Fallout 3" and its expansions. New gameplay modes can also become available, for instance, "Call of Duty" and its zombie modes, a multiplayer mode for "Mushroom Wars" or a higher difficulty level for "". Smaller packages of DLC are also common, ranging from better in-game weapons ("Dead Space", "Just Cause 2"), character outfits ("LittleBigPlanet", "Minecraft"), or new songs to perform ("SingStar", "Rock Band", "Guitar Hero").
Modifications.
Many games produced for the PC are designed such that technically oriented consumers can modify the game. These mods can add an extra dimension of replayability and interest. Developers such as id Software, Valve Software, Crytek, Bethesda, Epic Games and Blizzard Entertainment ship their games with some of the development tools used to make the game, along with documentation to assist mod developers. The Internet provides an inexpensive medium to promote and distribute mods, and they may be a factor in the commercial success of some games. This allows for the kind of success seen by popular mods such as the "Half-Life" mod "Counter-Strike".
Cheating.
Cheating in computer games may involve cheat codes and hidden spots implemented by the game developers, modification of game code by third parties, or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer. Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls. Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances.
Glitches.
Software errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as "patches") to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits.
Easter eggs.
Easter eggs are hidden messages or jokes left in games by developers that are not part of the main game.
Theory.
Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls "Cyberdrama". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from "Star Trek", arguing for the video game as a medium in which we get to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as "Tron", "eXistenZ" and "The Last Starfighter".
Ludologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game "Tomb Raider", saying that "the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it." Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.
While many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term "emergent narrative" has been used to describe how, in a simulated environment, storyline can be created simply by "what happens to the player." However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow and/or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.
Social aspects.
Demographics.
The November 2005 Nielsen Active Gamer Study, taking a survey of 2,000 regular gamers, found that the U.S. games market is diversifying. The age group among male players has expanded significantly in the 25–40 age group. For casual online puzzle-style and simple mobile cell phone games, the gender divide is more or less equal between males and females. Females have also been found to show an attraction to online multi-player games where there is a communal experience. More recently there has been a growing segment of female players engaged with the aggressive style of games historically considered to fall within traditionally male genres (e.g., first-person shooters). According to the ESRB almost 41% of PC gamers are women
When comparing today's industry climate with that of 20 years ago, women and many adults are more inclined to be using products in the industry. While the market for teen and young adult men is still a strong market, it is the other demographics which are posting significant growth. The Entertainment Software Association (ESA) provides the following summary for 2011 based on a study of almost 1,200 American households carried out by Ipsos MediaCT:
A 2006 academic study, based on a survey answered by 10,000 gamers, identified the gaymers (gamers that identify as gay) as a demographic group. A follow-up survey in 2009 studied the purchase habits and content preferences of people in the group.
Based on the study by NPD group in 2011, approximately 91 percent of children aged 2–17 play games.
Multiplayer.
Video gaming has traditionally been a social experience. Multiplayer video games are those that can be played either competitively, sometimes in Electronic Sports, or cooperatively by using either multiple input devices, or by hotseating. "Tennis for Two", arguably the first video game, was a two player game, as was its successor "Pong". The first commercially available game console, the Magnavox Odyssey, had two controller inputs.
Since then, most consoles have been shipped with two or four controller inputs. Some have had the ability to expand to four, eight or as many as 12 inputs with additional adapters, such as the Multitap. Multiplayer arcade games typically feature play for two to four players, sometimes tilting the monitor on its back for a top-down viewing experience allowing players to sit opposite one another.
Many early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. MMOs can offer extremely high numbers of simultaneous players; "Eve Online" set a record with 54,446 players on a single server in 2010.
Behavioral effects.
It has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers.
Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects.
It has been suggested by a few studies that online/offline video gaming can be used as a therapeutic tool in the treatment of different mental health concerns.
In Steven Johnson's book, "Everything Bad Is Good for You", he argues that video games in fact demand far more from a player than traditional games like "Monopoly". To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books. Some research suggests video games may even increase players' attention capacities.
Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be "learning by doing" while playing video games while fostering creative thinking.
The U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people.
According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to problem-solve. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.
The research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups. In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects. A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games.
Controversy.
Like related forms of media, computer and video games have been the subject of frequent controversy and censorship, due to the depiction of graphic violence, sexual themes, advergaming (a form of advertising in games), consumption of drugs, consumption of alcohol or tobacco, propaganda, or profanity in some games. Among others, critics of video games often include parents' groups, politicians, organized religious groups, and other advocacy groups. Various games have been accused of causing addiction and even violent behavior, though how much ground this holds is debatable. "Video game censorship" is defined as the use of state or group power to control the playing, distribution, purchase, or sale of video games or computer games. Video game controversy comes in many forms, and censorship is a controversial subject. Proponents and opponents of censorship are often very passionate about their individual views. However, there has not been substantial evidence against or in favor of showing a positive correlation between video games and acts such as killing.
Various national content rating organizations, such as the Entertainment Software Ratings Board or ESRB in North America, rate software for certain age groups and with certain content warnings. Some of these organizations are optional industry self-regulation (such as the ESRB), while others are part of national government censorship organizations (such as the BBFC). Most video games display their rating on the front side of their packaging. However, parents are not always aware of the existence of these ratings. Not all ratings are considered accurate. Organizations such as "What They Play" and "Common Sense Media" aim to provide guidance and advice for parents.
Ratings and censorship.
ESRB.
The Entertainment Software Rating Board (ESRB) gives video games maturity ratings based on their content. For example, a game might be rated "T" for "Teen" if the game contained obscene words or violence. If a game contains explicit violence or sexual themes, it is likely to receive an "M" for "Mature" rating, which means that no one under 17 should play it. There is a rated "A/O" games for "Adults Only" these games have massive violence or nudity. There are no laws that prohibit children from purchasing "M" rated games in the United States. Laws attempting to prohibit minors from purchasing "M" rated games were established in California, Illinois, Michigan, Minnesota, and Louisiana, but all were overturned on the grounds that these laws violated the First Amendment. However, many stores have opted to not sell such games to children anyway. Of course, video game laws vary from country to country. One of the most controversial games of all time, "Manhunt 2" by Rockstar Studios, was given an AO rating by the ESRB until Rockstar could make the content more suitable for a mature audience.
Video game manufacturers usually exercise tight control over the games that are made available on their systems, so unusual or special-interest games are more likely to appear as PC games. Free, casual, and browser-based games are usually played on available computers, mobile phones, or PDAs.
PEGI.
Pan European Game Information (PEGI) is a system that was developed to standardize the game ratings in all of Europe (not just European Union, although the majority are EU members), the current members are: all EU members, except Germany and the 10 accession states; Norway; Switzerland. Iceland is expected to join soon, as are the 10 EU accession states. For all PEGI members, they use it as their sole system, with the exception of the UK, where if a game contains certain material, it must be rated by BBFC. The PEGI ratings are legally binding in Vienna and it is a criminal offence to sell a game to someone if it is rated above their age.
Germany: BPjM and USK.
Stricter game rating laws mean that Germany does not operate within the PEGI. Instead, they adopt their own system of certification which is required by law. The Unterhaltungssoftware Selbstkontrolle ("USK" or Voluntary Certification of Entertainment Software) checks every game before release and assigns an age rating to it – either none (white), 6 years of age (yellow), 12 years of age (green), 16 years of age (blue) or 18 years of age (red). It is forbidden for anyone, retailers, friends or parents alike, to allow a child access to a game for which he or she is underage. If a game is particularly violent, it may be referred to the BPjM ("Bundesprüfstelle für jugendgefährdende Medien" – Federal Verification Office for Child-Endangering Media) who may opt to place it on the "Index" upon which the game may not be sold openly or advertised in the open media. Unofficially, the titles are not "banned" – adult gamers are still technically free to obtain the titles by other means, although it is still considered a felony to supply these titles to a child
Commercial aspects.
Game sales.
The four largest producers of and markets for computer and video games (in order) are North America (US and Canada), Japan, the United Kingdom and Germany. Other significant markets include Australia, Spain, South Korea, Mexico, France and Italy. Both India and China are considered emerging markets in the video game industry and sales are expected to rise significantly in the coming years. Irish are the largest per capita consumers of video games.
Sales of different types of games vary widely between these markets due to local preferences. Japanese consumers tend to purchase much more console games than computer games, with a strong preference for games catering to local tastes. Another key difference is that, despite the decline of arcades in the Western world (see Golden age of video arcade games), arcade games remain the largest sector of the Japanese gaming industry. As of 2009, the Japanese gaming industry is worth $20 billion, including $6 billion from arcades, $3.5 billion from console game sales, and $2 billion from mobile game sales. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. There are over 20,000 Internet cafés in South Korea where computer games can be played for an hourly charge. Computer games are also popular in China, which has a PC gaming industry worth $6 billion, the largest in the world. Arcade games are also still a thriving industry in China, though console games have been banned in the country since the early 2000s.
The NPD Group tracks computer and video game sales in the United States. It reported in 2004 that:
PC games that are digitally distributed either directly or by networks such as Steam are not tracked by the NPD, and Valve does not list sales numbers for games downloaded through their service. Unauthorized distribution is also rampant on the PC. Another growing factor that is difficult to account for is the expansion and popularity of independent games, which typically bypass traditional developer/publisher relationships to instead market directly to the consumer.
These figures are sales in dollars, not units, Unit shipments for each category were higher than the dollar sales numbers indicate, because more software and hardware was discounted than in 2003. But with the release of the next-generation consoles in 2006, these numbers increased dramatically. The game and film industries are also becoming increasingly intertwined, with companies like Sony having significant stakes in both. A large number of summer blockbuster films spawn a companion game, often launching at the same time to share the marketing costs.
The global market for console games has seen an average of 6.9 percent compound annual growth rate and is expected to become a $34.7 billion market in 2012.Online game sales are expected to grow at a larger rate of 16.9 percent, escalating from $6.6 billion in 2008 to $14.4 billion by 2012. The largest channel for growth, however, is in mobile gaming with a growth rate of 19 percent; estimated to grow from $5.6 billion in 2008 to $13.5 billion in four years.
Conventions.
Gaming conventions are an important showcase of the industry. The annual gamescom in Cologne (Germany) is the world's leading expo for video games. The E3 in Los Angeles (USA) is also of global importance, but is an event for industry insiders only.
Other notable conventions and trade fairs include Tokyo Game Show (Japan), Brasil Game Show, EB Games Expo (Australia), KRI (Russia), ChinaJoy and the annual Game Developers Conference. Some publishers, developers and technology producers also host their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.
Criticism.
Video games have the problem of regional lockout. In Australia, while most DVD players are sold region-free to accommodate local consumer rights legislation, video game consoles are still sold fully region protected. Some effort has been made to increase awareness of the issue, specifically to Nintendo of Australia, in the form of a formal report outlining the issues, published by Aaron Rex Davies. The report has gone on to gain a lot of attention in the public media.
Museums.
There are many video game museums around the world, including the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg and the Computer Game Museum in Berlin. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum in Rome is also dedicated to preserving videogames and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, D.C. has three video games on permanent display: Pac-Man, Dragon's Lair, and Pong. 
In 2012, the Smithsonian American Art Museum ran an exhibition on "The Art of Video Games". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.

</doc>
<doc id="5367" url="http://en.wikipedia.org/wiki?curid=5367" title="Cambrian">
Cambrian

The Cambrian ( or ) is the first geological period of the Paleozoic Era, lasting from to million years ago (mya) and is succeeded by the Ordovician. Its subdivisions, and indeed its base, are somewhat in flux. The period was established by Adam Sedgwick, who named it after Cambria, the Latin name for Wales, where Britain's Cambrian rocks are best exposed. The Cambrian is unique in its unusually high proportion of lagerstätten. These are sites of exceptional preservation, where 'soft' parts of organisms are preserved as well as their more resistant shells. This means that our understanding of the Cambrian biology surpasses that of some later periods.
The Cambrian Period marked a profound change in life on Earth; prior to the Cambrian, living organisms on the whole were small, unicellular and simple. Complex, multicellular organisms gradually became more common in the millions of years immediately preceding the Cambrian, but it was not until this period that mineralized – hence readily fossilized – organisms became common. The rapid diversification of lifeforms in the Cambrian, known as the Cambrian explosion, produced the first representatives of all modern animal phyla. Phylogenetic analysis has supported the view that during the Cambrian radiation, metazoa (animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.
While diverse life forms prospered in the oceans, the land was comparatively barren – with nothing more complex than a microbial soil crust and a few molluscs that emerged to browse on the microbial biofilm Most of the continents were probably dry and rocky due to a lack of vegetation. Shallow seas flanked the margins of several continents created during the breakup of the supercontinent Pannotia. The seas were relatively warm, and polar ice was absent for much of the period.
The United States Federal Geographic Data Committee uses a "barred capital C" character similar to the capital letter Ukrainian Ye to represent the Cambrian Period.
The proper Unicode character is .
Stratigraphy.
Despite the long recognition of its distinction from younger Ordovician rocks and older Precambrian rocks, it was not until 1994 that this time period was internationally ratified. The base of the Cambrian is defined on a complex assemblage of trace fossils known as the "Treptichnus pedum" assemblage.
Nevertheless, the usage of "Treptichnus pedum", a reference ichnofossil for the lower boundary of the Cambrian, for the stratigraphic detection of this boundary is always risky because of occurrence of very similar trace fossils belonging to the Treptichnids group well below the "T. pedum" in Namibia, Spain and Newfoundland, and possibly, in the western USA. The stratigraphic range of "T. pedum" overlaps the range of the Ediacaran fossils in Namibia, and probably in Spain.
Subdivisions.
The Cambrian period follows the Ediacaran and is followed by the Ordovician period. The Cambrian is divided into four epochs or series and ten ages or stages. Currently only two series and five stages are named and have a GSSP.
Because the international stratigraphic subdivision is not yet complete, many local subdivisions are still widely used. In some of these subdivisions the Cambrian is divided into three epochs with locally differing names – the Early Cambrian (Caerfai or Waucoban, mya), Middle Cambrian (St Davids or Albertan, mya) and Furongian ( mya; also known as Late Cambrian, Merioneth or Croixan). Rocks of these epochs are referred to as belonging to the Lower, Middle, or Upper Cambrian.
Trilobite zones allow biostratigraphic correlation in the Cambrian.
Each of the local epochs is divided into several stages. The Cambrian is divided into several regional faunal stages of which the Russian-Kazakhian system is most used in international parlance:
Cambrian dating.
The time range for the Cambrian has classically been thought to have been from about 542 million-years-ago (mya) to about 488 mya. The lower boundary of the Cambrian was traditionally set at the earliest appearance of trilobites and also unusual forms known as archeocyathids (literally "ancient cup") that are thought to be the earliest sponges and also the first non-microbial reef builders.
The end of the period was eventually set at a fairly definite faunal change now identified as an extinction event. Fossil discoveries and radiometric dating in the last quarter of the 20th century have called these dates into question. Date inconsistencies as large as 20 million years are common between authors. Framing dates of "ca." 545 to 490 mya were proposed by the International Subcommission on Global Stratigraphy as recently as 2002.
A radiometric date from New Brunswick puts the end of the Lower Cambrian around 511 mya. This leaves 21 mya for the other two series/epochs of the Cambrian.
A more precise date of 542 ± 0.3 mya for the extinction event at the beginning of the Cambrian has recently been submitted. The rationale for this precise dating is interesting in itself as an example of paleological deductive reasoning. Exactly at the Cambrian boundary there is a marked fall in the abundance of carbon-13, a "reverse spike" that paleontologists call an "excursion". It is so widespread that it is the best indicator of the position of the Precambrian-Cambrian boundary in stratigraphic sequences of roughly this age. One of the places that this well-established carbon-13 excursion occurs is in Oman. Amthor (2003) describes evidence from Oman that indicates the carbon-isotope excursion relates to a mass extinction: the disappearance of distinctive fossils from the Precambrian coincides exactly with the carbon-13 anomaly. Fortunately, in the Oman sequence, so too does a volcanic ash horizon from which zircons provide a very precise age of 542 ± 0.3 mya (calculated on the decay rate of uranium to lead). This new and precise date tallies with the less precise dates for the carbon-13 anomaly, derived from sequences in Siberia and Namibia.
Paleogeography.
Plate reconstructions suggest a global supercontinent, Pannotia, was in the process of breaking up early in the period, with Laurentia (North America), Baltica, and Siberia having separated from the main supercontinent of Gondwana to form isolated land masses. Most continental land was clustered in the Southern Hemisphere at this time, but was gradually drifting north. Large, high-velocity rotational movement of Gondwana appears to have occurred in the Early Cambrian.
With a lack of sea ice – the great glaciers of the Marinoan Snowball Earth were long melted – the sea level was high, which led to large areas of the continents being flooded in warm, shallow seas ideal for thriving life. The sea levels fluctuated somewhat, suggesting there were 'ice ages', associated with pulses of expansion and contraction of a south polar ice cap.
Climate.
The Earth was generally cold during the early Cambrian, probably due to the ancient continent of Gondwana covering the South Pole and cutting off polar ocean currents. There were likely polar ice caps and a series of glaciations, as the planet was still recovering from an earlier Snowball Earth. It became warmer towards the end of the period; the glaciers receded and eventually disappeared, and sea levels rose dramatically. This trend would continue into the Ordovician period.
Flora.
Although there were a variety of macroscopic marine plants (e.g. "Margaretia" and "Dalyia"), no true land plant (embryophyte) fossils are known from the Cambrian. However, biofilms and microbial mats were well developed on Cambrian tidal flats and beaches., and further inland were a variety of lichens, fungi and microbes forming microbial earth ecosystems, comparable with modern soil crust of desert regions, contributing to soil formation.
Fauna.
Most animal life during the Cambrian was aquatic, with trilobites assumed to be the dominant life form, which has since proven to be incorrect. Arthropods in general were by far the most dominating animals in the ocean, but at the time, trilobites were only a minor part of the total arthropod diversity. What made them different from their relatives was their heavy armor, which fossilized far easier than the fragile exoskeleton of other arthropods, leaving behind numerous preserved remains which give the false impression that they were the most abundant part of the fauna. The period marked a steep change in the diversity and composition of Earth's biosphere. The incumbent Ediacaran biota suffered a mass extinction at the base of the period, which corresponds to an increase in the abundance and complexity of burrowing behaviour. This behaviour had a profound and irreversible effect on the substrate which transformed the seabed ecosystems. Before the Cambrian, the sea floor was covered by microbial mats. By the end of the period, burrowing animals had destroyed the mats through bioturbation, and gradually turned the seabeds into what they are today. As a consequence, many of those organisms that were dependent on the mats went extinct, while the other species adapted to the changed environment that now offered new ecological niches.
Around the same time there was a seemingly rapid appearance of representatives of all the mineralized phyla except the Bryozoa, which appear in the Lower Ordovician. However, many of these phyla were represented only by stem-group forms; and since mineralized phyla generally have a benthic origin, they may not be a good proxy for (more abundant) non-mineralized phyla.
While the early Cambrian showed such diversification that it has been named the Cambrian Explosion, this changed later in the period, when it was exposed to a sharp drop in biodiversity. About 515 million years ago, the number of species going extinct exceeded the amount of new species appearing. Five million years later, the number of genera had dropped from an earlier peak of about 600 to just 450. Also the speciation rate in many groups was reduced to between a fifth and a third of previous levels. 500 million years ago, oxygen levels fell dramatically in the oceans, leading to hypoxia, while the levels of poisonous hydrogen sulfide simultaneously increased, causing another extinction. The later half of Cambrian was surprisingly barren and show evidence of several rapid extinction events; the stromatolites which had been replaced by reef building sponges known as Archaeocyatha, returned once more as the archaeocyathids went extinct. This declining trend did not change before Ordovician.
Some Cambrian organisms ventured onto land, producing the trace fossils "Protichnites" and "Climactichnites". Fossil evidence suggests that euthycarcinoids, an extinct group of arthropods, produced at least some of the "Protichnites". Fossils of the maker of "Climactichnites" have not been found; however, fossil trackways and resting traces suggest a large, slug-like mollusk.
In contrast to later periods, the Cambrian fauna was somewhat restricted; free-floating organisms were rare, with the majority living on or close to the sea floor; and mineralizing animals were rarer than in future periods, in part due to the unfavourable ocean chemistry.
Many modes of preservation are unique to the Cambrian, resulting in an abundance of "Lagerstätten".

</doc>
<doc id="5370" url="http://en.wikipedia.org/wiki?curid=5370" title="Category of being">
Category of being

In metaphysics (in particular, ontology), the different kinds or ways of being are called categories of being or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.
Categorical distinctions.
The common or dominant ways to view categories as of the end of the 20th century.
Any of these ways can be criticized for...
In process philosophy, this last is the only possibility, but historically philosophers have been loath to conclude that nothing exists but process.
Categorization of existence.
As bundles of properties.
Bundle theory is an ontological theory about objecthood proposed by the 18th century Scottish philosopher David Hume, which states that objects only subsist as a collection (bundle) of properties, relations or tropes. In an "epistemological" sense, bundle theory says that all that can be known about objects are the properties which they are composed of, and that these properties are all that can be truly said to exist.
For example, if we take the concept of a black square, bundle theory would suggest that all that can be said to exist are the properties of a black square.
The properties of a black square are: Black, Regular and Quadrilateral. 
However, from these properties alone, we cannot deduce any kind of underlying essence of a "black square", or some object called a "black square", except as a bundle of properties which constitute the object that we then go on to label as a "black square", but the object "itself" is really nothing more than a system of relations (or bundle) of properties. To defend this, Hume asks us to imagine an object without properties, if we strip the black square of its properties (being black, regular and quadrilateral) we end up reducing the object to non-existence.
Intuition as evasion.
A seemingly simpler way to view categories is as arising only from intuition. Philosophers argue this evades the issue. What it means to take the category physical object seriously as a category of being is to assert that the concept of physical objecthood cannot be reduced to or explicated in any other terms – not, for example, in terms of bundles of properties but only in terms of other items in that category.
In this way, many ontological controversies can be understood as controversies about exactly which categories should be seen as fundamental, irreducible, or primitive. To refer to intuition as the source of distinctions and thus categories doesn't resolve this.
Ideology, dogma, theory.
Modern theories give weight to intuition, perceptually observed properties, comparisons of categories among persons, and the direction of investigation towards known specified ends, to determine what humanity in its present state of being needs to consider irreducible. They seek to explain why certain beliefs about categories would appear in political science as ideology, in religion as dogma, or in science as theory.
As metaphors.
A set of ontological distinctions related by a single conceptual metaphor was called an ontological metaphor by George Lakoff and Mark Johnson, who claimed that such metaphors arising from experience were more basic than any properties or symbol-based comparisons. Their cognitive science of mathematics was a study of the embodiment of basic symbols and properties including those studied in the philosophy of mathematics, via embodied philosophy, using cognitive science. This theory comes after several thousand years of inquiry into patterns and cognitive bias of humanity.
Categories of being.
Philosophers have many differing views on what the fundamental categories of being are. In no particular order, here are at least "some" items that have been regarded as categories of being by someone or other:
Physical objects.
Physical objects are beings; certainly they are said to "be" in the simple sense that they "exist" all around us. So a house is a being, a person's body is a being, a tree is a being, a cloud is a being, and so on. They are beings because, and in the sense that, they are physical objects. One might also call them bodies, or physical particulars, or concrete things, or matter, or maybe substances (but bear in mind the word 'substance' has some special philosophical meanings).
Minds.
Minds – those "parts" of us that think and perceive—are considered beings by some philosophers. Each of us, according to common sense anyway, "has" a mind. Of course, philosophers rarely just assume that minds occupy a different category of beings from physical objects. Some, like René Descartes, have thought that this is so (this view is known as dualism, and functionalism also considers the mind as distinct from the body), while others have thought that concepts of the mental can be reduced to physical concepts (this is the view of physicalism or materialism). Still others maintain though "mind" is a noun, it is not necessarily the "name of a thing" distinct within the whole person. In this view the relationship between mental properties and physical properties is one of supervenience – similar to how "banks" supervene upon certain buildings.
Classes.
We can talk about all human beings, and the planets, and all engines as belonging to classes. Within the class of human beings are all of the human beings, or the extension of the term 'human being'. In the class of planets would be Mercury, Venus, the Earth, and all the other planets that there might be in the universe. Classes, in addition to each of their members, are often taken to be beings. Surely we can say that in some sense, the class of planets "is", or has being. Classes are usually taken to be abstract objects, like sets; 'class' is often regarded as equivalent, or nearly equivalent, in meaning to 'set'. Denying that classes and sets exist is the contemporary meaning of nominalism.
Properties.
The redness of a red apple, or more to the point, the redness all red things share, is a "property". One could also call it an "attribute" of the apple. Very roughly put, a property is just a quality that describes an object. This will not do as a definition of the word 'property' because, like 'attribute', 'quality' is a near-synonym of 'property'. But these synonyms can at least help us to get a fix on the concept we are talking about. Whenever one talks about the size, color, weight, composition, and so forth, of an object, one is talking about the properties of that object. Some—though this is a point of severe contention in the problem of universals – believe that properties are beings; the redness of all apples is something that "is." To deny that universals exist is the scholastic variant of nominalism.
Note that the color red is an objective property of an object. The intrinsic property is that it reflects radiation (including light) in a certain way. A human perceives that as the color red in his or her brain. An object thus has two types of properties, intrinsic (physical) and objective (observer specific).
Relations.
An apple sitting on a table is in a relation to the table it sits on. So we can say that there is a relation between the apple and the table: namely, the relation of sitting-on. So, some say, we can say that that relation has being. For another example, the Washington Monument is taller than the White House. Being-taller-than is a relation between the two structures. We can say that "that" relation has being as well. This, too, is a point of contention in the problem of universals.
Space and time.
Space and time are what physical objects are extended into. There is debate as to whether time exists only in the present or whether far away times are just as real as far away spaces, and there is debate (among who?) as to whether space is curved. Many (nearly all?) contemporary thinkers actually suggest that time is the fourth dimension, thus reducing space and time to one distinct ontological entity, the space-time continuum.
Propositions.
Propositions are units of meaning. They should not be confused with declarative sentences, which are just sets of words in languages that refer to propositions. Declarative sentences, ontologically speaking, are thus ideas, a property of substances (minds), rather than a distinct ontological category. For instance, the English declarative sentence "snow is white" refers to the same proposition as the equivalent French declarative sentence "la neige est blanche"; two sentences, one proposition. Similarly, one declarative sentence can refer to many propositions; for instance, "I am hungry" changes meaning (i.e. refers to different propositions) depending on the person uttering it.
Events.
Events are that which can be said to occur. To illustrate, consider the claim "John went to a ballgame"; if true, then we must ontologically account for every entity in the sentence. "John" refers to a substance. But what does "went to a ballgame" refer to? It seems wrong to say that "went to a ballgame" is a property that instantiates John, because "went to a ballgame" does not seem to be the same ontological kind of thing as, for instance, redness. Thus, events arguably deserve their own ontological category.
Properties, relations, and classes are supposed to be "abstract," rather than "concrete." Many philosophers say that properties and relations have an abstract existence, and that physical objects have a concrete existence. That, perhaps, is the paradigm case of a difference in ways in which items can be said to "be," or to have being.
Many philosophers have attempted to reduce the number of distinct ontological categories. For instance, David Hume famously regarded Space and Time as nothing more than psychological facts about human beings, which would effectively reduce Space and Time to ideas, which are properties of humans (substances). Nominalists and realists argue over the existence of properties and relations. Finally, events and propositions have been argued to be reducible to sets (classes) of substances and other such categories.
History.
Aristotle.
"Category" came into use with Aristotle's essay "Categories", in which he discussed univocal and equivocal terms, predication, and ten categories:
Kant.
In his "Critique of Pure Reason", Kant proposed the following system:
Peirce and Husserl.
Charles Sanders Peirce, who had read Kant closely and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: "Firstness", "Secondness", and "Thirdness", which he repeatedly invoked in his subsequent writings.
Edmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.
For Gilbert Ryle (1949), a category (in particular a "category mistake") is an important semantic concept, but one having only loose affinities to an ontological category.
Others.
Contemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).

</doc>
<doc id="5371" url="http://en.wikipedia.org/wiki?curid=5371" title="Concrete">
Concrete

Concrete is a composite material composed mainly of water, aggregate, and cement. Usually there are additives and reinforcements included to achieve the desired physical properties of the finished material. When these ingredients are mixed together, they form a fluid mass that is easily molded into shape. Over time, the cement forms a hard matrix which binds the rest of the ingredients together into a durable stone-like material with many uses.
Famous concrete structures include the Hoover Dam, the Panama Canal and the Roman Pantheon. The earliest large-scale users of concrete technology were the ancient Romans, and concrete was widely used in the Roman Empire. The Colosseum in Rome was built largely of concrete, and the concrete dome of the Pantheon is the world's largest unreinforced concrete dome.
After the Roman Empire collapsed, use of concrete became rare until the technology was re-pioneered in the mid-18th century. Today, concrete is the most widely used man-made material (measured by tonnage).
History.
The word concrete comes from the Latin word ""concretus" (meaning compact or condensed), the perfect passive participle of "concrescere", from "con"-" (together) and "crescere" (to grow).
Perhaps the earliest known occurrence of cement was twelve million years ago. A deposit of cement was formed after an occurrence of oil shale located adjacent to a bed of limestone burned due to natural causes. These ancient deposits were investigated in the 1960s and 1970s.
On a human time-scale, small usages of concrete go back for thousands of years. The ancient Nabatea culture was using materials roughly analogous to concrete at least eight thousand years ago, some structures of which survive to this day.
German archaeologist Heinrich Schliemann found concrete floors, which were made of lime and pebbles, in the royal palace of Tiryns, Greece, which dates roughly to 1400-1200 BC. Lime mortars were used in Greece, Crete, and Cyprus in 800 BC. The Assyrian Jerwan Aqueduct (688 BC) made use of fully waterproof concrete. Concrete was used for construction in many ancient structures.
The Romans used concrete extensively from 300 BC to 476 AD, a span of more than seven hundred years. During the Roman Empire, Roman concrete (or "opus caementicium") was made from quicklime, pozzolana and an aggregate of pumice. Its widespread use in many Roman structures, a key event in the history of architecture termed the Roman Architectural Revolution, freed Roman construction from the restrictions of stone and brick material and allowed for revolutionary new designs in terms of both structural complexity and dimension.
Concrete, as the Romans knew it, was a new and revolutionary material. Laid in the shape of arches, vaults and domes, it quickly hardened into a rigid mass, free from many of the internal thrusts and strains that troubled the builders of similar structures in stone or brick.
Modern tests show that "opus caementicium" had as much compressive strength as modern Portland-cement concrete (ca. 200 kg/cm2). However, due to the absence of reinforcement, its tensile strength was far lower than modern reinforced concrete, and its mode of application was also different:
Modern structural concrete differs from Roman concrete in two important details. First, its mix consistency is fluid and homogeneous, allowing it to be poured into forms rather than requiring hand-layering together with the placement of aggregate, which, in Roman practice, often consisted of rubble. Second, integral reinforcing steel gives modern concrete assemblies great strength in tension, whereas Roman concrete could depend only upon the strength of the concrete bonding to resist tension.
The widespread use of concrete in many Roman structures ensured that many survive to the present day. The Baths of Caracalla in Rome are just one example. Many Roman aqueducts and bridges such as the magnificent Pont du Gard have masonry cladding on a concrete core, as does the dome of the Pantheon.
After the Roman Empire, the use of burned lime and pozzolana was greatly reduced until the technique was all but forgotten between 500 AD and the 1300s. Between the 1300s until the mid-1700s, the use of cement gradually returned. The "Canal du Midi" was built using concrete in 1670, and there are concrete structures in Finland that date from the 16th century.
Perhaps the greatest driver behind the modern usage of concrete was the third Eddystone Lighthouse in Devon, England. To create this structure, between 1756 and 1793, British engineer John Smeaton pioneered the use of hydraulic lime in concrete, using pebbles and powdered brick as aggregate.
A method for producing Portland cement was patented by Joseph Aspdin on 1824.
Reinforced concrete was invented in 1849 by Joseph Monier.
In 1889 the first concrete reinforced bridge was built, and the first large concrete dams were built in 1936, Hoover Dam and Grand Coulee Dam.
Ancient additives.
Concrete additives have been used since 6500BC by the Nabataea traders or Bedouins who occupied and controlled a series of oases and developed a small empire in the regions of southern Syria and northern Jordan. They later discovered the advantages of hydraulic lime—that is, cement that hardens underwater—and by 700 BC, they were building kilns to supply mortar for the construction of rubble-wall houses, concrete floors, and underground waterproof cisterns. The cisterns were kept secret and were one of the reasons the Nabataea were able to thrive in the desert. In both Roman and Egyptian times it was re-discovered that adding volcanic ash to the mix allowed it to set underwater. Similarly, the Romans knew that adding horse hair made concrete less liable to crack while it hardened, and adding blood made it more frost-resistant.
Modern additives.
In modern times, researchers have experimented with the addition of other materials to create concrete with improved properties, such as higher strength, electrical conductivity, or resistance to damages through spillage.
Impact of modern concrete use.
Concrete is widely used for making architectural structures, foundations, brick/block walls, pavements, bridges/overpasses, highways, runways, parking structures, dams, pools/reservoirs, pipes, footings for gates, fences and poles and even boats. Concrete is used in large quantities almost everywhere mankind has a need for infrastructure.
The amount of concrete used worldwide, ton for ton, is twice that of steel, wood, plastics, and aluminum combined. Concrete's use in the modern world is exceeded only by that of naturally occurring water.
Concrete is also the basis of a large commercial industry. Globally, the ready-mix concrete industry, the largest segment of the concrete market, is projected to exceed $100 billion in revenue by 2015. In the United States alone, concrete production is a $30-billion-per-year industry, considering only the value of the ready-mixed concrete sold each year. Given the size of the concrete industry, and the fundamental way concrete is used to shape the infrastructure of the modern world, it is difficult to overstate the role this material plays today.
Education and research.
The National Building Museum in Washington, D.C. created an exhibition titled "Liquid Stone: New Architecture in Concrete". This exhibition, dedicated solely to the study of concrete as a building material, was on view for the public from June 2004 - January 2006.
Composition of concrete.
There are many types of concrete available, created by varying the proportions of the main ingredients below. In this way or by substitution for the cementitious and aggregate phases, the finished product can be tailored to its application with varying strength, density, or chemical and thermal resistance properties.
"Aggregate" consists of large chunks of material in a concrete mix, generally a coarse gravel or crushed rocks such as limestone, or granite, along with finer materials such as sand.
"Cement", most commonly Portland cement is associated with the general term "concrete." A range of materials can be used as the cement in concrete. One of the most familiar of these alternative cements is asphalt. Other cementitious materials such as fly ash and slag cement, are sometimes added to Portland cement and become a part of the binder for the aggregate.
Water is then mixed with this dry composite, which produces a semi-liquid that workers can shape (typically by pouring it into a form). The concrete solidifies and hardens through a chemical process called hydration. The water reacts with the cement, which bonds the other components together, creating a robust stone-like material.
"Chemical admixtures" are added to achieve varied properties. These ingredients may speed or slow down the rate at which the concrete hardens, and impart many other useful properties including increased tensile strength and water resistance.
"Reinforcements" are often added to concrete. Concrete can be formulated with high compressive strength, but always has lower tensile strength. For this reason it is usually reinforced with materials that are strong in tension (often steel).
"Mineral admixtures" are becoming more popular in recent decades. The use of recycled materials as concrete ingredients has been gaining popularity because of increasingly stringent environmental legislation, and the discovery that such materials often have complementary and valuable properties. The most conspicuous of these are fly ash, a by-product of coal-fired power plants, and silica fume, a byproduct of industrial electric arc furnaces. The use of these materials in concrete reduces the amount of resources required, as the ash and fume act as a cement replacement. This displaces some cement production, an energetically expensive and environmentally problematic process, while reducing the amount of industrial waste that must be disposed of.
The "mix design" depends on the type of structure being built, how the concrete is mixed and delivered, and how it is placed to form the structure.
Cement.
Portland cement is the most common type of cement in general usage. It is a basic ingredient of concrete, mortar and plaster. English masonry worker Joseph Aspdin patented Portland cement in 1824. It was named because of the similarity of its color to Portland limestone, quarried from the English Isle of Portland and used extensively in London architecture. It consists of a mixture of oxides of calcium, silicon and aluminium. Portland cement and similar materials are made by heating limestone (a source of calcium) with clay and grinding this product (called "clinker") with a source of sulfate (most commonly gypsum).
In modern cement kilns many advanced features are used to lower the fuel consumption per ton of clinker produced. Cement kilns are extremely large, complex, and inherently dusty industrial installations, and have emissions which must be controlled. Of the various ingredients used in concrete the cement is the most energetically expensive. Even complex and efficient kilns require 3.3 to 3.6 gigajoules of energy to produce a ton of clinker and then grind it into cement. Many kilns can be fueled with difficult-to-dispose-of wastes, the most common being used tires. The extremely high temperatures and long periods of time at those temperatures allows cement kilns to efficiently and completely burn even difficult-to-use fuels.
Water.
Combining water with a cementitious material forms a cement paste by the process of hydration. The cement paste glues the aggregate together, fills voids within it, and makes it flow more freely.
A lower water-to-cement ratio yields a stronger, more durable concrete, whereas more water gives a freer-flowing concrete with a higher slump. Impure water used to make concrete can cause problems when setting or in causing premature failure of the structure.
Hydration involves many different reactions, often occurring at the same time. As the reactions proceed, the products of the cement hydration process gradually bond together the individual sand and gravel particles and other components of the concrete to form a solid mass.
Reaction:
Aggregates.
Fine and coarse aggregates make up the bulk of a concrete mixture. Sand, natural gravel, and crushed stone are used mainly for this purpose. Recycled aggregates (from construction, demolition, and excavation waste) are increasingly used as partial replacements of natural aggregates, while a number of manufactured aggregates, including air-cooled blast furnace slag and bottom ash are also permitted.
The presence of aggregate greatly increases the durability of concrete above that of cement, which is a brittle material in its pure state. Thus concrete is a true composite material.
Redistribution of aggregates after compaction often creates inhomogeneity due to the influence of vibration. This can lead to strength gradients.
Decorative stones such as quartzite, small river stones or crushed glass are sometimes added to the surface of concrete for a decorative "exposed aggregate" finish, popular among landscape designers.
In addition to being decorative, exposed aggregate adds robustness to a concrete driveway.
Reinforcement.
Concrete is strong in compression, as the aggregate efficiently carries the compression load. However, it is weak in tension as the cement holding the aggregate in place can crack, allowing the structure to fail. Reinforced concrete adds either steel reinforcing bars, steel fibers, glass fibers, or plastic fibers to carry tensile loads.
Chemical admixtures.
"Chemical admixtures" are materials in the form of powder or fluids that are added to the concrete to give it certain characteristics not obtainable with plain concrete mixes. In normal use, admixture dosages are less than 5% by mass of cement and are added to the concrete at the time of batching/mixing. (See the section on Concrete Production, below.)The common types of admixtures are as follows.
Mineral admixtures and blended cements.
There are inorganic materials that also have pozzolanic or latent hydraulic properties. These very fine-grained materials are added to the concrete mix to improve the properties of concrete (mineral admixtures), or as a replacement for Portland cement (blended cements). Products which incorporate limestone, fly ash, blast furnace slag, and other useful materials with pozzolanic properties into the mix, are being tested and used. This development is due to cement production being one of the largest producers (at about 5 to 10%) of global greenhouse gas emissions, as well as lowering costs, improving concrete properties, and recycling wastes.
Concrete production.
Concrete production is the process of mixing together the various ingredients—water, aggregate, cement, and any additives—to produce concrete. Concrete production is time-sensitive. Once the ingredients are mixed, workers must put the concrete in place before it hardens. In modern usage, most concrete production takes place in a large type of industrial facility called a concrete plant, or often a batch plant.
In general usage, concrete plants come in two main types, ready mix plants and central mix plants. A ready mix plant mixes all the ingredients except water, while a central mix plant mixes all the ingredients including water. A central mix plant offers more accurate control of the concrete quality through better measurements of the amount of water added, but must be placed closer to the work site where the concrete will be used, since hydration begins at the plant.
A concrete plant consists of large storage hoppers for various reactive ingredients like cement, storage for bulk ingredients like aggregate and water, mechanisms for the addition of various additives and amendments, machinery to accurately weigh, move, and mix some or all of those ingredients, and facilities to dispense the mixed concrete, often to a concrete mixer truck.
Modern concrete is usually prepared as a viscous fluid, so that it may be poured into forms, which are containers erected in the field to give the concrete its desired shape. There are many different ways in which concrete formwork can be prepared, such as Slip forming and Steel plate construction. Alternatively, concrete can be mixed into dryer, non-fluid forms and used in factory settings to manufacture Precast concrete products.
There is a wide variety of equipment for processing concrete, from hand tools to heavy industrial machinery. Whichever equipment builders use, however, the objective is to produce the desired building material; ingredients must be properly mixed, placed, shaped, and retained within time constraints. Once the mix is where it should be, the curing process must be controlled to ensure that the concrete attains the desired attributes. During concrete preparation, various technical details may affect the quality and nature of the product.
When initially mixed, Portland cement and water rapidly form a gel of tangled chains of interlocking crystals, and components of the gel continue to react over time. Initially the gel is fluid, which improves workability and aids in placement of the material, but as the concrete sets, the chains of crystals join into a rigid structure, counteracting the fluidity of the gel and fixing the particles of aggregate in place. During curing, the cement continues to react with the residual water in a process of hydration. In properly formulated concrete, once this curing process has terminated the product has the desired physical and chemical properties. Among the qualities typically desired, are mechanical strength, low moisture permeability, and chemical and volumetric stability.
Mixing concrete.
Thorough mixing is essential for the production of uniform, high-quality concrete. For this reason equipment and methods should be capable of effectively mixing concrete materials containing the largest specified aggregate to produce "uniform mixtures" of the lowest slump practical for the work.
"Separate paste mixing" has shown that the mixing of cement and water into a paste before combining these materials with aggregates can increase the compressive strength of the resulting concrete. The paste is generally mixed in a "high-speed", shear-type mixer at a w/cm (water to cement ratio) of 0.30 to 0.45 by mass. The cement paste premix may include admixtures such as accelerators or retarders, superplasticizers, pigments, or silica fume. The premixed paste is then blended with aggregates and any remaining batch water and final mixing is completed in conventional concrete mixing equipment.
High-energy mixed (HEM) concrete is produced by means of high-speed mixing of cement, water and sand with net specific energy consumption of at least 5 kilojoules per kilogram of the mix. A plasticizer or a superplasticizer is then added to the activated mixture, which can later be mixed with aggregates in a conventional concrete mixer. In this process, sand provides dissipation of energy and creates high-shear conditions on the surface of cement particles. This results in the full volume of water interacting with cement. The liquid activated mixture can be used by itself or foamed (expanded) for lightweight concrete. HEM concrete hardens in low and subzero temperature conditions and possesses an increased volume of gel, which drastically reduces capillarity in solid and porous materials.
Workability.
"Workability" is the ability of a fresh (plastic) concrete mix to fill the form/mold properly with the desired work (vibration) and without reducing the concrete's quality. Workability depends on water content, aggregate (shape and size distribution), cementitious content and age (level of hydration) and can be modified by adding chemical admixtures, like superplasticizer. Raising the water content or adding chemical admixtures increases concrete workability. Excessive water leads to increased bleeding (surface water) and/or segregation of aggregates (when the cement and aggregates start to separate), with the resulting concrete having reduced quality. The use of an aggregate with an undesirable gradation can result in a very harsh mix design with a very low slump, which cannot readily be made more workable by addition of reasonable amounts of water.
Workability can be measured by the concrete slump test, a simplistic measure of the plasticity of a fresh batch of concrete following the ASTM C 143 or EN 12350-2 test standards. Slump is normally measured by filling an "Abrams cone" with a sample from a fresh batch of concrete. The cone is placed with the wide end down onto a level, non-absorptive surface. It is then filled in three layers of equal volume, with each layer being tamped with a steel rod to consolidate the layer. When the cone is carefully lifted off, the enclosed material slumps a certain amount, owing to gravity. A relatively dry sample slumps very little, having a slump value of one or two inches (25 or 50 mm) out of one foot (305 mm). A relatively wet concrete sample may slump as much as eight inches. Workability can also be measured by the flow table test.
Slump can be increased by addition of chemical admixtures such as plasticizer or superplasticizer without changing the water-cement ratio. Some other admixtures, especially air-entraining admixture, can increase the slump of a mix.
High-flow concrete, like self-consolidating concrete, is tested by other flow-measuring methods. One of these methods includes placing the cone on the narrow end and observing how the mix flows through the cone while it is gradually lifted.
After mixing, concrete is a fluid and can be pumped to the location where needed.
Curing.
In all but the least critical applications, care must be taken to properly "cure" concrete, to achieve best strength and hardness. This happens after the concrete has been placed. Cement requires a moist, controlled environment to gain strength and harden fully. The cement paste hardens over time, initially setting and becoming rigid though very weak and gaining in strength in the weeks following. In around 4 weeks, typically over 90% of the final strength is reached, though strengthening may continue for decades. The conversion of calcium hydroxide in the concrete into calcium carbonate from absorption of CO2 over several decades further strengthens the concrete and makes it more resistant to damage. However, this reaction, called carbonation, lowers the pH of the cement pore solution and can cause the reinforcement bars to corrode.
Hydration and hardening of concrete during the first three days is critical. Abnormally fast drying and shrinkage due to factors such as evaporation from wind during placement may lead to increased tensile stresses at a time when it has not yet gained sufficient strength, resulting in greater shrinkage cracking. The early strength of the concrete can be increased if it is kept damp during the curing process. Minimizing stress prior to curing minimizes cracking. High-early-strength concrete is designed to hydrate faster, often by increased use of cement that increases shrinkage and cracking. The strength of concrete changes (increases) for up to three years. It depends on cross-section dimension of elements and conditions of structure exploitation.
During this period concrete must be kept under controlled temperature and humid atmosphere. In practice, this is achieved by spraying or ponding the concrete surface with water, thereby protecting the concrete mass from ill effects of ambient conditions. The picture to the right shows one of many ways to achieve this, ponding – submerging setting concrete in water and wrapping in plastic to contain the water in the mix. Additional common curing methods include wet burlap and/or plastic sheeting covering the fresh concrete, or by spraying on a water-impermeable temporary curing membrane.
Properly curing concrete leads to increased strength and lower permeability and avoids cracking where the surface dries out prematurely. Care must also be taken to avoid freezing or overheating due to the exothermic setting of cement. Improper curing can cause scaling, reduced strength, poor abrasion resistance and cracking.
Properties.
Concrete has relatively high compressive strength, but much lower tensile strength. For this reason it is usually reinforced with materials that are strong in tension (often steel). The elasticity of concrete is relatively constant at low stress levels but starts decreasing at higher stress levels as matrix cracking develops. Concrete has a very low coefficient of thermal expansion and shrinks as it matures. All concrete structures crack to some extent, due to shrinkage and tension. Concrete that is subjected to long-duration forces is prone to creep.
Tests can be performed to ensure that the properties of concrete correspond to specifications for the application.
Different mixes of concrete ingredients produce different strengths, which are measured in psi or MPa.
Different strengths of concrete are used for different purposes. Very low-strength (2000 psi or less) concrete may be used when the concrete must be lightweight. Lightweight concrete is often achieved by adding air, foams, or lightweight aggregates, with the side effect that the strength is reduced. For most routine uses, 3000-psi to 4000-psi concrete is often used. 5000-psi concrete is readily commercially available as a more durable, although more expensive, option. 5000-psi concrete is often used for larger civil projects. Strengths above 5000 psi are often used for specific building elements. For example, the lower floor columns of high-rise concrete buildings may use concrete of 12,000 psi or more, to keep the size of the columns small. Bridges may use long beams of 10,000 psi concrete to lower the number of spans required. Occasionally, other structural needs may require high-strength concrete. If a structure must be very rigid, concrete of very high strength may be specified, even much stronger than is required to bear the service loads. Strengths as high as 19,000 psi have been used commercially for these reasons.
Concrete degradation.
Concrete can be damaged by many processes, such as the expansion of corrosion products of the steel reinforcement bars, freezing of trapped water, fire or radiant heat, aggregate expansion, sea water effects, bacterial corrosion, leaching, erosion by fast-flowing water, physical damage and chemical damage (from carbonatation, chlorides, sulfates and distillate water). The micro fungi Aspergillus Alternaria and Cladosporium were able to grow on samples of concrete used as a radioactive waste barrier in the Chernobyl reactor; leaching aluminium, iron, calcium and silicon.
Microbial concrete.
Bacteria such as "Bacillus pasteurii", "Bacillus pseudofirmus", "Bacillus cohnii", "Sporosarcina pasteuri", and "Arthrobacter crystallopoietes" increase the compression strength of concrete through their biomass. Not all bacteria increase the strength of concrete significantly with their biomass. Bacillus sp. CT-5. can reduce corrosion of reinforcement in reinforced concrete by up to four times. "Sporosarcina pasteurii" reduces water and chloride permeability. "B. pasteurii" increases resistance to acid. "Bacillus pasteurii" and "B. sphaericuscan" induce calcium carbonate precipitation in the surface of cracks, adding compression strength.
Environmental and health.
The manufacture and use of concrete produce a wide range of environmental and social consequences. Some are harmful, some welcome, and some both, depending on circumstances. A major component of concrete is cement, which similarly exerts environmental and social effects.
The cement industry is one of the three primary producers of carbon dioxide, a major greenhouse gas. The other two are the energy production and transportation industries. As of 2011 it contributes 7% to global anthropogenic CO2 emissions; largely due to the sintering of limestone and clay at 1500 C.
Concrete is used to create hard surfaces that contribute to surface runoff, which can cause heavy soil erosion, water pollution, and flooding, but conversely can be used to divert, dam, and control flooding.
Concrete is a primary contributor to the urban heat island effect, though less so than asphalt.
Workers who cut, grind or polish concrete are at risk of inhaling airborne silica, which can lead to silicosis. Concrete dust released by building demolition and natural disasters can be a major source of dangerous air pollution.
The presence of some substances in concrete, including useful and unwanted additives, can cause health concerns due to toxicity and radioactivity.
Wet concrete is highly alkaline and must be handled with proper protective equipment.
Concrete recycling.
Concrete recycling is an increasingly common method of disposing of concrete structures. Concrete debris was once routinely shipped to landfills for disposal, but recycling is increasing due to improved environmental awareness, governmental laws and economic benefits.
Concrete, which must be free of trash, wood, paper and other such materials, is collected from demolition sites and put through a crushing machine, often along with asphalt, bricks and rocks.
Reinforced concrete contains rebar and other metallic reinforcements, which are removed with magnets and recycled elsewhere. The remaining aggregate chunks are sorted by size. Larger chunks may go through the crusher again. Smaller pieces of concrete are used as gravel for new construction projects. Aggregate base gravel is laid down as the lowest layer in a road, with fresh concrete or asphalt placed over it. Crushed recycled concrete can sometimes be used as the dry aggregate for brand new concrete if it is free of contaminants, though the use of recycled concrete limits strength and is not allowed in many jurisdictions. On 3 March 1983, a government-funded research team (the VIRL research.codep) estimated that almost 17% of worldwide landfill was by-products of concrete based waste.
Use of concrete in infrastructure.
Mass concrete structures.
Large concrete structures such as dams, navigation locks, large mat foundations, and large breakwaters generate excessive heat during cement hydration and associated expansion. To mitigate these effects post-cooling is commonly applied during construction. An early example at Hoover Dam, installed a network of pipes between vertical concrete placements to circulate cooling water during the curing process to avoid damaging overheating. Similar systems are still used; depending on volume of the pour, the concrete mix used, and ambient air temperature, the cooling process may last for many months after the concrete is placed. Various methods also are used to pre-cool the concrete mix in mass concrete structures.
Another approach to mass concrete structures that is becoming more widespread is the use of roller-compacted concrete, which uses much lower amounts of cement and water than conventional concrete mixtures and is generally not poured into place. Instead it is placed in thick layers as a semi-dry material and compacted into a dense, strong mass with rolling compactors. Because it uses less cementitious material, roller-compacted concrete has a much lower cooling requirement than conventional concrete.
Prestressed concrete structures.
Prestressed concrete is a form of reinforced concrete that builds in compressive stresses during construction to oppose those experienced in use. This can greatly reduce the weight of beams or slabs, by
better distributing the stresses in the structure to make optimal use of the reinforcement. For example, a horizontal beam tends to sag. Prestressed reinforcement along the bottom of the beam counteracts this.
In pre-tensioned concrete, the prestressing is achieved by using steel or polymer tendons or bars that are subjected to a tensile force prior to casting, or for post-tensioned concrete, after casting.
Concrete textures.
When one thinks of concrete, the image of a dull, gray concrete wall often comes to mind. With the use of form liner, concrete can be cast and molded into different textures and used for decorative concrete applications. Sound/retaining walls, bridges, office buildings and more serve as the optimal canvases for concrete art. For example, the Pima Freeway/Loop 101 retaining and sound walls in Scottsdale, Arizona, feature desert flora and fauna, a lizard and cacti along the stretch. The project, titled "The Path Most Traveled," is one example of how concrete can be shaped using elastomeric form liner.
Building with concrete.
Concrete is one of the most durable building materials. It provides superior fire resistance compared with wooden construction and gains strength over time. Structures made of concrete can have a long service life. Concrete is used more than any other manmade material in the world. As of 2006, about 7.5 billion cubic meters of concrete are made each year, more than one cubic meter for every person on Earth.
More than of highways in the United States are paved with this material. Reinforced concrete, prestressed concrete and precast concrete are the most widely used types of concrete functional extensions in modern days. See Brutalism.
Energy efficiency.
Energy requirements for transportation of concrete are low because it is produced locally from local resources, typically manufactured within 100 kilometers of the job site. Similarly, relatively little energy is used in producing and combining the raw materials (although large amounts of CO2 are produced by the chemical reactions in cement manufacture). The overall embodied energy of concrete is therefore lower than for most structural materials other than wood.
Once in place, concrete offers great energy efficiency over the lifetime of a building. Concrete walls leak air far less than those made of wood frames. Air leakage accounts for a large percentage of energy loss from a home. The thermal mass properties of concrete increase the efficiency of both residential and commercial buildings. By storing and releasing the energy needed for heating or cooling, concrete's thermal mass delivers year-round benefits by reducing temperature swings inside and minimizing heating and cooling costs. While insulation reduces energy loss through the building envelope, thermal mass uses walls to store and release energy. Modern concrete wall systems use both external insulation and thermal mass to create an energy-efficient building. Insulating concrete forms (ICFs) are hollow blocks or panels made of either insulating foam or rastra that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.
Pervious concrete.
Pervious concrete is a mix of specially graded coarse aggregate, cement, water and little-to-no fine aggregates. This concrete is also known as “no-fines” or porous concrete. Mixing the ingredients in a carefully controlled process creates a paste that coats and bonds the aggregate particles. The hardened concrete contains interconnected air voids totalling approximately 15 to 25 percent. Water runs through the voids in the pavement to the soil underneath. Air entrainment admixtures are often used in freeze–thaw climates to minimize
the possibility of frost damage.
Nano concrete.
Concrete is the most widely manufactured construction material. The addition of carbon nanofibres to concrete has many advantages in terms of mechanical and electrical properties (e.g. higher strength and higher Young’s modulus) and self-monitoring behavior due to the high tensile strength and high conductivity. Mullapudi used the pulse velocity method to characterize the properties of concrete containing carbon nanofibres. The test results indicate that the compressive strength and percentage reduction in electrical resistance while loading concrete containing carbon nanofibres differ from those of plain concrete. A reasonable concentration of carbon nanofibres need to be determined for use in concrete, which not only enhances compressive strength, but also improves the electrical properties required for strain monitoring, damage evaluation and self-health monitoring of concrete.
Fire safety.
Concrete buildings are more resistant to fire than those constructed using steel frames, since concrete has lower heat conductivity than steel and can thus last longer under the same fire conditions. Concrete is sometimes used as a fire protection for steel frames, for the same effect as above. Concrete as a fire shield, for example Fondu fyre, can also be used in extreme environments like a missile launch pad.
Options for non-combustible construction include floors, ceilings and roofs made of cast-in-place and hollow-core precast concrete. For walls, concrete masonry technology and Insulating Concrete Forms (ICFs) are additional options. ICFs are hollow blocks or panels made of fireproof insulating foam that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.
Concrete also provides good resistance against externally applied forces such as high winds, hurricanes, and tornadoes owing to its lateral stiffness, which results in minimal horizontal movement. However this stiffness can work against certain types of concrete structures, particularly where a relatively higher flexing structure is require to resist more extreme forces.
Earthquake safety.
As discussed above, concrete is very strong in compression, but weak in tension. Larger earthquakes can generate very large shear loads on structures. These shear loads subject the structure to both tensile and compressional loads. Concrete structures without reinforcement, like other unreinforced masonry structures, can fail during severe earthquake shaking. Unreinforced masonry structures constitute one of the largest earthquake risks globally. These risks can be reduced through seismic retrofitting of at-risk buildings, (e.g. school buildings in Istanbul, Turkey).
Useful life.
Concrete can be viewed as a form of artificial sedimentary rock. As a type of mineral, the compounds of which it is composed are extremely stable. Many concrete structures are built with an expected lifetime of approximately 100 years, but researchers have suggested that adding silica fume could extend the useful life of bridges and other concrete uses to as long as 16,000 years. Coatings are also available to protect concrete from damage, and extend the useful life. Epoxy coatings may be applied only to interior surfaces, though, as they would otherwise trap moisture in the concrete.
A self-healing concrete has been developed that can also last longer than conventional concrete.
Large dams, such as the Hoover Dam, and the Three Gorges Dam are intended to last "forever", a period that is not quantified.
World records.
The world record for the largest concrete pour in a single project is the Three Gorges Dam in Hubei Province, China by the Three Gorges Corporation. The amount of concrete used in the construction of the dam is estimated at 16 million cubic meters over 17 years. The previous record was 12.3 million cubic meters held by Itaipu hydropower station in Brazil.
The world record for concrete pumping was set on 7 August 2009 during the construction of the Parbati Hydroelectric Project, near the village of Suind, Himachal Pradesh, India, when the concrete mix was pumped through a vertical height of .
The world record for the largest continuously poured concrete raft was achieved in August 2007 in Abu Dhabi by contracting firm Al Habtoor-CCC Joint Venture and the concrete supplier is Unibeton Ready Mix. The pour (a part of the foundation for the Abu Dhabi's Landmark Tower) was 16,000 cubic meters of concrete poured within a two-day period. The previous record, 13,200 cubic meters poured in 54 hours despite a severe tropical storm requiring the site to be covered with tarpaulins to allow work to continue, was achieved in 1992 by joint Japanese and South Korean consortiums Hazama Corporation and the Samsung C&T Corporation for the construction of the Petronas Towers in Kuala Lumpur, Malaysia.
The world record for largest continuously poured concrete floor was completed 8 November 1997, in Louisville, Kentucky by design-build firm EXXCEL Project Management. The monolithic placement consisted of of concrete placed within a 30-hour period, finished to a flatness tolerance of FF 54.60 and a levelness tolerance of FL 43.83. This surpassed the previous record by 50% in total volume and 7.5% in total area.
The record for the largest continuously placed underwater concrete pour was completed 18 October 2010, in New Orleans, Louisiana by contractor C. J. Mahan Construction Company, LLC of Grove City, Ohio. The placement consisted of 10,251 cubic yards of concrete placed in a 58.5 hour period using two concrete pumps and two dedicated concrete batch plants. Upon curing, this placement allows the cofferdam to be dewatered approximately below sea level to allow the construction of the Inner Harbor Navigation Canal Sill & Monolith Project to be completed in the dry.

</doc>
<doc id="5373" url="http://en.wikipedia.org/wiki?curid=5373" title="Coitus interruptus">
Coitus interruptus

Coitus interruptus, also known as the rejected sexual intercourse, withdrawal or pull-out method, is a method of birth control in which a man, during intercourse withdraws his penis from a woman's vagina prior to orgasm (and ejaculation), and then directs his ejaculate (semen) away from the vagina in an effort to avoid insemination.
This method of contraception, widely used for at least two millennia, is still in use today. This method was used by an estimated 38 million couples worldwide in 1991. Coitus interruptus does not protect against sexually transmitted infections (STIs/STDs).
History.
Perhaps the oldest documentation of the use of the withdrawal method to avoid pregnancy is the story of Onan in the Torah. This text is believed to have been written down over 2,500 years ago. Societies in the ancient civilizations of Greece and Rome preferred small families and are known to have practiced a variety of birth control methods. There are references that have led historians to believe withdrawal was sometimes used as birth control. However, these societies viewed birth control as a woman's responsibility, and the only well-documented contraception methods were female-controlled devices (both possibly effective, such as pessaries, and ineffective, such as amulets).
After the decline of the Roman Empire in the 5th century AD, contraceptive practices fell out of use in Europe; the use of contraceptive pessaries, for example, is not documented again until the 15th century. If withdrawal was used during the Roman Empire, knowledge of the practice may have been lost during its decline.
From the 18th century until the development of modern methods, withdrawal was one of the most popular methods of birth-control in Europe, North America, and elsewhere.
Effects.
Like many methods of birth control, reliable effect is achieved only by correct and consistent use. Observed failure rates of withdrawal vary depending on the population being studied: studies have found actual failure rates of 15–28% per year. In comparison, the pill has an actual use failure rate of 2–8%, while the intrauterine device (IUD) has an actual use failure rate of 0.8%. The condom has an actual use failure rate of 10–18%. However some authors suggest that actual effectiveness of withdrawal could be similar to effectiveness of condoms, and this area needs further research. (see Comparison of birth control methods)
For couples that use coitus interruptus correctly at every act of intercourse, the failure rate is 4% per year. In comparison the pill has a perfect-use failure rate of 0.3%, and the I.U.D. has a perfect-use failure rate of 0.6%. The condom has a perfect-use failure rate of 2%.
It has been suggested that the pre-ejaculate ("Cowper's fluid") emitted by the penis prior to ejaculation normally contains spermatozoa (sperm cells), which would compromise the effectiveness of the method. However, several small studies have failed to find any viable sperm in the fluid. While no large conclusive studies have been done, it is believed by some that the cause of method (correct-use) failure is the pre-ejaculate fluid picking up sperm from a previous ejaculation. For this reason, it is recommended that the male partner urinate between ejaculations, to clear the urethra of sperm, and wash any ejaculate from objects that might come near the woman's vulva (e.g. hands and penis).
However, recent research suggests that this might not be accurate. A contrary, yet non-generalizable study that found mixed evidence, including individual cases of a high sperm concentration, was published in March 2011. A noted limitation to these previous studies' findings is that pre-ejaculate samples were analyzed after the critical two-minute point. That is, looking for motile sperm in small amounts of pre-ejaculate via microscope after two minutes – when the sample has most likely dried – makes examination and evaluation "extremely difficult." Thus, in March 2011 a team of researchers assembled 27 male volunteers and analyzed their pre-ejaculate samples within two minutes after producing them. The researchers found that 11 of the 27 men (41%) produced pre-ejaculatory samples that contained sperm, and 10 of these samples (37%) contained a "fair amount" of motile sperm (i.e. as few as 1 million to as many as 35 million).
This study therefore recommends, in order to minimise unintended pregnancy and disease transmission, the use of condom from the first moment of genital contact.
As a point of reference, a study showed that, of couples who conceived within a year of trying, only 2.5% included a male partner with a total sperm count (per ejaculate) of 23 million sperm or less.
However, across a wide range of observed values, total sperm count (as with other identified semen and sperm characteristics) has weak power to predict which couples are at risk of pregnancy.
It is widely believed that urinating after an ejaculation will flush the urethra of remaining sperm. Therefore, some of the subjects in the March 2011 study who produced sperm in their pre-ejaculate did urinate (sometimes more than once) before producing their sample.
Advantages.
The advantage of coitus interruptus is that it can be used by people who have objections to or do not have access to other forms of contraception. Some men prefer it so they can avoid possible adverse effects of hormonal contraceptives on their partners or so that they can have a full experience and really be able to "feel" their partner. Some women also prefer this method over hormonal contraception to avoid adverse effects such as depression, mood swings, vaginal dryness, decreased libido, weight gain, nausea, and headaches, among others. It has no direct monetary cost, requires no artificial devices, has no physical side effects, can be practiced without a prescription or medical consultation, and provides no barriers to stimulation.
Disadvantages.
Compared to the other common reversible methods of contraception such as IUDs, hormonal contraceptives and male condoms, coitus interruptus is less effective at preventing pregnancy. As a result, it is also less cost-effective than many more effective methods: although the method itself has no direct cost, users have a greater chance of incurring the risks and expenses of either child-birth or abortion. Only models that assume all couples practice perfect use of the method find cost savings associated with the choice of withdrawal as a birth control method.
The method is largely ineffective in the prevention of sexually transmitted infections (STIs/STDs), like HIV, since pre-ejaculate may carry viral particles or bacteria which may infect the partner if this fluid comes in contact with mucous membranes. However, a reduction in the volume of bodily fluids exchanged during intercourse may reduce the likelihood of disease transmission compared to using no method due to the smaller number of pathogens present.
The method may be difficult for some couples to use. The interruption of intercourse may leave some couples sexually frustrated or dissatisfied.
Masters and Johnson considered withdrawal as a means to developing sexual problems, e.g. premature ejaculation and erectile dysfunction, but there is no known evidence to support this.
Prevalence.
Worldwide, 3% of women of childbearing age rely on withdrawal as their primary method of contraception. Regional popularity of the method varies widely, from a low of 1% on the African continent to 16% in Western Asia. (Data from surveys during the late 1990s).
In the United States, studies have indicated 56% of women of reproductive age have had a partner use withdrawal. By 2002, only 2.5% were using withdrawal as their primary method of contraception.
A leading exponent of withdrawal in the mid-19th century was a religious based "utopian commune" called the Oneida community in New York. To minimize the incidence of pregnancy, teenage males were not permitted to engage in sexual intercourse except with postmenopausal women until such time as they mastered the withdrawal technique.

</doc>
<doc id="5374" url="http://en.wikipedia.org/wiki?curid=5374" title="Condom">
Condom

A condom ( or ) is a barrier device commonly used during sexual intercourse to reduce the probability of pregnancy and spreading sexually transmitted infections (STIs/STDs) such as HIV/AIDS. It is put on an erect penis and physically blocks ejaculated semen from entering the body of a sexual partner. Condoms are also used for collection of semen for use in infertility treatment. In the modern age, condoms are most often made from latex, but some are made from other materials such as polyurethane, polyisoprene, or lamb intestine. A female condom is also available, often made of nitrile.
As a method of birth control, male condoms have the advantages of being inexpensive, easy to use, having few side effects, and offering protection against sexually transmitted diseases. With proper knowledge and application technique—and use at every act of intercourse—women whose partners use male condoms experience a 2% per-year pregnancy rate with perfect use and an 18% per-year pregnancy rate with typical use. Condoms have been used for at least 400 years. Since the 19th century, they have been one of the most popular methods of contraception in the world. While widely accepted in modern times, condoms have generated some controversy, primarily over what role they should play in sex education classes. 
Medical uses.
Birth control.
The effectiveness of condoms, as of most forms of contraception, can be assessed two ways. "Perfect use" or "method" effectiveness rates only include people who use condoms properly and consistently. "Actual use", or "typical use" effectiveness rates are of all condom users, including those who use condoms incorrectly or do not use condoms at every act of intercourse. Rates are generally presented for the first year of use. Most commonly the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.
The typical use pregnancy rate among condom users varies depending on the population being studied, ranging from 10 to 18% per year. The perfect use pregnancy rate of condoms is 2% per year. Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.
Sexually transmitted infections.
Condoms are widely recommended for the prevention of sexually transmitted infections (STIs). They have been shown to be effective in reducing infection rates in both men and women. While not perfect, the condom is effective at reducing the transmission of organisms that cause AIDS, genital herpes, cervical cancer, genital warts, syphilis, chlamydia, gonorrhea, and other diseases. Condoms are often recommended as an adjunct to more effective birth control methods (such as IUD) in situations where STD protection is also desired.
According to a 2000 report by the National Institutes of Health (NIH), correct and consistent use of latex condoms reduces the risk of HIV/AIDS transmission by approximately 85% relative to risk when unprotected, putting the seroconversion rate (infection rate) at 0.9 per 100 person-years with condom, down from 6.7 per 100 person-years. Analysis published in 2007 from the University of Texas Medical Branch and the World Health Organization found similar risk reductions of 80–95%.
The 2000 NIH review concluded that condom use significantly reduces the risk of gonorrhea for men. A 2006 study reports that proper condom use decreases the risk of transmission of human papillomavirus to women by approximately 70%. Another study in the same year found consistent condom use was effective at reducing transmission of herpes simplex virus-2 also known as genital herpes, in both men and women.
Although a condom is effective in limiting exposure, some disease transmission may occur even with a condom. Infectious areas of the genitals, especially when symptoms are present, may not be covered by a condom, and as a result, some diseases can be transmitted by direct contact. The primary effectiveness issue with using condoms to prevent STDs, however, is inconsistent use.
Condoms may also be useful in treating potentially precancerous cervical changes. Exposure to human papillomavirus, even in individuals already infected with the virus, appears to increase the risk of precancerous changes. The use of condoms helps promote regression of these changes. In addition, researchers in the UK suggest that a hormone in semen can aggravate existing cervical cancer, condom use during sex can prevent exposure to the hormone.
Causes of failure.
Condoms may slip off the penis after ejaculation, break due to improper application or physical damage (such as tears caused when opening the package), or break or slip due to latex degradation (typically from usage past the expiration date, improper storage, or exposure to oils). The rate of breakage is between 0.4% and 2.3%, while the rate of slippage is between 0.6% and 1.3%. Even if no breakage or slippage is observed, 1–2% of women will test positive for semen residue after intercourse with a condom.
"Double bagging", using two condoms at once, is often believed to cause a higher rate of failure due to the friction of rubber on rubber. This claim is not supported by research. The limited studies that have been done on the subject support that double bagging is likely not harmful and possibly beneficial.
Different modes of condom failure result in different levels of semen exposure. If a failure occurs during application, the damaged condom may be disposed of and a new condom applied before intercourse begins – such failures generally pose no risk to the user. One study found that semen exposure from a broken condom was about half that of unprotected intercourse; semen exposure from a slipped condom was about one-fifth that of unprotected intercourse.
Standard condoms will fit almost any penis, with varying degrees of comfort or risk of slippage. Many condom manufacturers offer "snug" or "magnum" sizes. Some manufacturers also offer custom sized-to-fit condoms, with claims that they are more reliable and offer improved sensation/comfort. Some studies have associated larger penises and smaller condoms with increased breakage and decreased slippage rates (and vice versa), but other studies have been inconclusive.
It is recommended for condoms manufacturers to avoid very thick or very thin condoms, because they are both considered less effective. Some authors encourage users to choose thinner condoms "for greater durability, sensation, and comfort", but others warn that "the thinner the condom, the smaller the force required to break it".
Experienced condom users are significantly less likely to have a condom slip or break compared to first-time users, although users who experience one slippage or breakage are more likely to suffer a second such failure. An article in "Population Reports" suggests that education on condom use reduces behaviors that increase the risk of breakage and slippage. A Family Health International publication also offers the view that education can reduce the risk of breakage and slippage, but emphasizes that more research needs to be done to determine all of the causes of breakage and slippage.
Among people who intend condoms to be their form of birth control, pregnancy may occur when the user has sex without a condom. The person may have run out of condoms, or be traveling and not have a condom with them, or simply dislike the feel of condoms and decide to "take a chance". This type of behavior is the primary cause of typical use failure (as opposed to method or perfect use failure).
Another possible cause of condom failure is sabotage. One motive is to have a child against a partner's wishes or consent. Some commercial sex workers from Nigeria reported clients sabotaging condoms in retaliation for being coerced into condom use. Using a fine needle to make several pinholes at the tip of the condom is believed to significantly impact their effectiveness. Cases of such condom sabotage have occurred.
Adverse effects.
The use of latex condoms by people with an allergy to latex can cause allergic symptoms such as skin irritation. In people with severe latex allergies, using a latex condom can potentially be life-threatening. Repeated use of latex condoms can also cause some people to develop an allergy to latex.
Use.
Male condoms are usually packaged inside a foil wrapper, in a rolled-up form, and are designed to be applied to the tip of the penis and then unrolled over the erect penis. It is important that some space be left in the tip of the condom so that semen has a place to collect; otherwise it may be forced out of the base of the device. After use, it is recommended the condom be wrapped in tissue or tied in a knot, then disposed of in a trash receptacle.
Some couples find that putting on a condom interrupts sex, although others incorporate condom application as part of their foreplay. Some men and women find the physical barrier of a condom dulls sensation. Advantages of dulled sensation can include prolonged erection and delayed ejaculation; disadvantages might include a loss of some sexual excitement. Advocates of condom use also cite their advantages of being inexpensive, easy to use, and having few side effects.
Adult film industry.
In 2012, Los Angeles County passed Measure B, a law requiring the use of condoms in the production of pornographic films. This requirement has received much criticism and is said by some to be counter-productive, merely forcing companies that make pornographic films to relocate to other places without this requirement. Producers claim that condom use depresses sales.
Sex education.
Condoms are often used in sex education programs, because they have the capability to reduce the chances of pregnancy and the spread of some sexually transmitted diseases when used correctly. A recent American Psychological Association (APA) press release supported the inclusion of information about condoms in sex education, saying ""comprehensive sexuality education programs... discuss the appropriate use of condoms", and "promote condom use for those who are sexually active"."
In the United States, teaching about condoms in public schools is opposed by some religious organizations. Planned Parenthood, which advocates family planning and sex education, argues that no studies have shown abstinence-only programs to result in delayed intercourse, and cites surveys showing that 76% of American parents want their children to receive comprehensive sexuality education including condom use.
Infertility treatment.
Common procedures in infertility treatment such as semen analysis and intrauterine insemination (IUI) require collection of semen samples. These are most commonly obtained through masturbation, but an alternative to masturbation is use of a special "collection condom" to collect semen during sexual intercourse.
Collection condoms are made from silicone or polyurethane, as latex is somewhat harmful to sperm. Many men prefer collection condoms to masturbation, and some religions prohibit masturbation entirely. Also, compared with samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology. For this reason, they are believed to give more accurate results when used for semen analysis, and to improve the chances of pregnancy when used in procedures such as intracervical or intrauterine insemination. Adherents of religions that prohibit contraception, such as Catholicism, may use collection condoms with holes pricked in them.
For fertility treatments, a collection condom may be used to collect semen during sexual intercourse where the semen is provided by the woman's partner. Private sperm donors may also use a collection condom to obtain samples through masturbation or by sexual intercourse with a partner and will transfer the ejaculate from the collection condom to a specially designed container. The sperm is transported in such containers, in the case of a donor, to a recipient woman to be used for insemination, and in the case of a woman's partner, to a fertility clinic for processing and use. However, transportation may reduce the fecundity of the sperm. Collection condoms may also be used where semen is produced at a sperm bank or fertility clinic.
"Condom therapy" is sometimes prescribed to infertile couples when the female has high levels of antisperm antibodies. The theory is that preventing exposure to her partner's semen will lower her level of antisperm antibodies, and thus increase her chances of pregnancy when condom therapy is discontinued. However, condom therapy has not been shown to increase subsequent pregnancy rates.
Other uses.
Condoms excel as multipurpose containers and barriers because they are waterproof, elastic, durable, and (for military and espionage uses) will not arouse suspicion if found.
Ongoing military utilization began during World War II, and includes covering the muzzles of rifle barrels to prevent fouling, the waterproofing of firing assemblies in underwater demolitions, and storage of corrosive materials and garrotes by paramilitary agencies.
Condoms have also been used to smuggle alcohol, cocaine, heroin, and other drugs across borders and into prisons by filling the condom with drugs, tying it in a knot and then either swallowing it or inserting it into the rectum. These methods are very dangerous and potentially lethal; if the condom breaks, the drugs inside become absorbed into the bloodstream and can cause an overdose.
Medically, condoms can be used to cover endovaginal ultrasound probes, or in field chest needle decompressions they can be used to make a one-way valve.
Condoms have also been used by to protect scientific samples from the environment, and to waterproof microphones for underwater recording.
Types.
Most condoms have a reservoir tip or teat end, making it easier to accommodate the man's ejaculate. Condoms come in different sizes, from oversized to snug and they also come in a variety of surfaces intended to stimulate the user's partner. Condoms are usually supplied with a lubricant coating to facilitate penetration, while flavored condoms are principally used for oral sex. As mentioned above, most condoms are made of latex, but polyurethane and lambskin condoms also exist.
Female condom.
Male condoms have a tight ring to form a seal around the penis while female condoms typically have a large stiff ring to keep them from slipping into the body orifice. The Female Health Company produced a female condom that was initially made of polyurethane, but newer versions are made of nitrile. Medtech Products produces a female condom made of latex.
Materials.
Natural latex.
Latex has outstanding elastic properties: Its tensile strength exceeds 30 MPa, and latex condoms may be stretched in excess of 800% before breaking. In 1990 the ISO set standards for condom production (ISO 4074, Natural latex rubber condoms), and the EU followed suit with its CEN standard (Directive 93/42/EEC concerning medical devices). Every latex condom is tested for holes with an electrical current. If the condom passes, it is rolled and packaged. In addition, a portion of each batch of condoms is subject to water leak and air burst testing.
While the advantages of latex have made it the most popular condom material, it does have some drawbacks. Latex condoms are damaged when used with oil-based substances as lubricants, such as petroleum jelly, cooking oil, baby oil, mineral oil, skin lotions, suntan lotions, cold creams, butter or margarine. Contact with oil makes latex condoms more likely to break or slip off due to loss of elasticity caused by the oils. Additionally, latex allergy precludes use of latex condoms and is one of the principal reasons for the use of other materials. In May 2009 the U.S. Food and Drug Administration granted approval for the production of condoms composed of Vytex, latex that has been treated to remove 90% of the proteins responsible for allergic reactions. An allergen-free condom made of synthetic latex (polyisoprene) is also available.
Synthetic.
The most common non-latex condoms are made from polyurethane. Condoms may also be made from other synthetic materials, such as AT-10 resin, and most recently polyisoprene.
Polyurethane condoms tend to be the same width and thickness as latex condoms, with most polyurethane condoms between 0.04 mm and 0.07 mm thick.
Polyurethane can be considered better than latex in several ways: it conducts heat better than latex, is not as sensitive to temperature and ultraviolet light (and so has less rigid storage requirements and a longer shelf life), can be used with oil-based lubricants, is less allergenic than latex, and does not have an odor. Polyurethane condoms have gained FDA approval for sale in the United States as an effective method of contraception and HIV prevention, and under laboratory conditions have been shown to be just as effective as latex for these purposes.
However, polyurethane condoms are less elastic than latex ones, and may be more likely to slip or break than latex, lose their shape or bunch up more than latex, and are more expensive.
Polyisoprene is a synthetic version of natural rubber latex. While significantly more expensive, it has the advantages of latex (such as being softer and more elastic than polyurethane condoms) without the protein which is responsible for latex allergies. Like polyurethane condoms, polyisoprene condoms are said to do a better job of transmitting body heat. Unlike polyurethane condoms, they cannot be used with an oil-based lubricant.
Lambskin.
Condoms made from sheep intestines, labeled "lambskin", are also available. Although they are generally effective as a contraceptive by blocking sperm, it is presumed that they are likely less effective than latex in preventing the transmission of agents that cause STDs, because of pores in the material. This is based on the idea that intestines, by their nature, are porous, permeable membranes, and while sperm are too large to pass through the pores, viruses—such as HIV, herpes, and genital warts—are small enough to pass through. However, there are to date no clinical data confirming or denying this theory. Some believe that lambskin condoms provide a more "natural" sensation, and they lack the allergens that are inherent to latex, but because of their lesser protection against infection, other hypoallergenic materials such as polyurethane are recommended for latex-allergic users and/or partners. Lambskin condoms are also significantly more expensive than other types and as slaughter by-products they are also not vegetarian.
Spermicide.
Some latex condoms are lubricated at the manufacturer with a small amount of a nonoxynol-9, a spermicidal chemical. According to Consumer Reports, condoms lubricated with spermicide have no additional benefit in preventing pregnancy, have a shorter shelf life, and may cause urinary-tract infections in women. In contrast, application of separately packaged spermicide "is" believed to increase the contraceptive efficacy of condoms.
Nonoxynol-9 was once believed to offer additional protection against STDs (including HIV) but recent studies have shown that, with frequent use, nonoxynol-9 may increase the risk of HIV transmission. The World Health Organizationsays that spermicidally lubricated condoms should no longer be promoted. However, it recommends using a nonoxynol-9 lubricated condom over no condom at all.> As of 2005, nine condom manufacturers have stopped manufacturing condoms with nonoxynol-9 and Planned Parenthood has discontinued the distribution of condoms so lubricated.
Ribbed and studded.
Textured condoms include studded and ribbed condoms which can provide extra sensations to both partners. The studs or ribs can be located on the inside, outside, or both; alternatively, they are located in specific sections to provide directed stimulation to either the g-spot or frenulum. Many textured condoms which advertise "mutual pleasure" also are bulb-shaped at the top, to provide extra stimulation to the male. Some women experience irritation during vaginal intercourse with studded condoms.
Youth condoms.
In March 2010, the Swiss government announced that it was planning to promote smaller condoms intended for boys and youths of 12–14 years old following concern about the pregnancy rate among adolescent girls, and also about the potential spread of AIDS among this age group. This was due to the fact that standard condoms were too wide and consequently failed to afford protection to adolescent boys during vaginal and anal intercourse. Family planning groups and the Swiss Aids Federation had campaigned to have a narrower condom produced for youths after a number of studies, including a government study researched at the Centre for Development and Personality Psychology at Basel University, found that standard condoms were unsuitable for boys in this age range, and that the condoms either failed during use or that the boys rejected them altogether because they were too wide, and consequently they used no protection at all.
As a result of these studies, a condom aimed at 12 to 14 year old boys is now produced and is available in Switzerland and in certain other countries. Manufactured by Ceylor, the "Hotshot" is a lubricated, teat-ended latex condom which is narrower than a standard condom and has a tight band at the opening to ensure that it remains on the youth's penis during intercourse. A standard condom has a diameter of 2 inches (5.2 cm) whereas the Hotshot has a diameter of 1.7 inches (4.5 cm). Both are the same length–7.4 inches (19 cm). In 2014, in response to demand for condoms from a younger age-group, German condom manufacturer Amor started producing another condom aimed at young people. Known as "Amor Young Love" these lubricated condoms have a diameter of 1.9 inches (4.9 cm).
Other.
The anti-rape condom is another variation designed to be worn by women. It is designed to cause pain to the attacker, hopefully allowing the victim a chance to escape.
A collection condom is used to collect semen for fertility treatments or sperm analysis. These condoms are designed to maximize sperm life.
Some condom-like devices are intended for entertainment only, such as glow-in-the dark condoms. These novelty condoms may not provide protection against pregnancy and STDs.
Prevalence.
The prevalence of condom use varies greatly between countries. Most surveys of contraceptive use are among married women, or women in informal unions. Japan has the highest rate of condom usage in the world: in that country, condoms account for almost 80% of contraceptive use by married women. On average, in developed countries, condoms are the most popular method of birth control: 28% of married contraceptive users rely on condoms. In the average less-developed country, condoms are less common: only 6-8% of married contraceptive users choose condoms.
History.
Before the 19th century.
Whether condoms were used in ancient civilizations is debated by archaeologists and historians. In ancient Egypt, Greece, and Rome, pregnancy prevention was generally seen as a woman's responsibility, and the only well documented contraception methods were female-controlled devices. In Asia before the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded. Condoms seem to have been used for contraception, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, they were made of tortoise shell or animal horn.
In 16th century Italy, Gabriele Falloppio wrote a treatise on syphilis. The earliest documented strain of syphilis, first appearing in Europe in a 1490s outbreak, caused severe symptoms and often death within a few months of contracting the disease. Falloppio's treatise is the earliest uncontested description of condom use: it describes linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Falloppio claimed that an experimental trial of the linen sheath demonstrated protection against syphilis.
After this, the use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication that these devices were used for birth control, rather than disease prevention, is the 1605 theological publication "De iustitia et iure" (On justice and law) by Catholic theologian Leonardus Lessius, who condemned them as immoral. In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of "condons", the first documented use of that word (or any similar spelling).
In addition to linen, condoms during the Renaissance were made out of intestines and bladder. In the late 16th century, Dutch traders introduced condoms made from "fine leather" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.
Casanova in the 18th century was one of the first reported using "assurance caps" to prevent impregnating his mistresses.
From at least the 18th century, condom use was opposed in some legal, religious, and medical circles for essentially the same reasons that are given today: condoms reduce the likelihood of pregnancy, which some thought immoral or undesirable for the nation; they do not provide full protection against sexually transmitted infections, while belief in their protective powers was thought to encourage sexual promiscuity; and, they are not used consistently due to inconvenience, expense, or loss of sensation.
Despite some opposition, the condom market grew rapidly. In the 18th century, condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or "skin" (bladder or intestine softened by treatment with sulfur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. They later spread to America, although in every place there were generally used only by the middle and upper classes, due to both expense and lack of sex education.
1800 through 1920s.
The early 19th century saw contraceptives promoted to the poorer classes for the first time. Writers on contraception tended to prefer other methods of birth control. Feminists of this time period wanted birth control to be exclusively in the hands of women, and disapproved of male-controlled methods such as the condom. Other writers cited both the expense of condoms and their unreliability (they were often riddled with holes, and often fell off or broke), but they discussed condoms as a good option for some, and as the only contraceptive that also protected from disease.
Many countries passed laws impeding the manufacture and promotion of contraceptives. In spite of these restrictions, condoms were promoted by traveling lecturers and in newspaper advertisements, using euphemisms in places where such ads were illegal. Instructions on how to make condoms at home were distributed in the United States and Europe. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method.
Beginning in the second half of the 19th century, American rates of sexually transmitted diseases skyrocketed. Causes cited by historians include effects of the American Civil War, and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sex education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught that abstinence was the only way to avoid sexually transmitted diseases. Condoms were not promoted for disease prevention because the medical community and moral watchdogs considered STDs to be punishment for sexual misbehavior. The stigma against victims of these diseases was so great that many hospitals refused to treat people who had syphilis.
The German military was the first to promote condom use among its soldiers, beginning in the later 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted diseases. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use.
In the decades after World War I, there remained social and legal obstacles to condom use throughout the U.S. and Europe. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control on the grounds that their failure rates were too high. Freud was especially opposed to the condom because he thought it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. In 1920 the Church of England's Lambeth Conference condemned all "unnatural means of conception avoidance." London's Bishop Arthur Winnington-Ingram complained of the huge number of condoms discarded in alleyways and parks, especially after weekends and holidays.
However, European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Worldwide, condom sales doubled in the 1920s.
Rubber and manufacturing advances.
In 1839, Charles Goodyear discovered a way of processing natural rubber, which is too stiff when cold and too soft when warm, in such a way as to make it elastic. This proved to have advantages for the manufacture of condoms; unlike the sheep's gut condoms, they could stretch and did not tear quickly when used. The rubber vulcanization process was patented by Goodyear in 1844. The first rubber condom was produced in 1855. The earliest rubber condoms had a seam and were as thick as a bicycle inner tube. Besides this type, small rubber condoms covering only the glans were often used in England and the United States. There was more risk of losing them and if the rubber ring was too tight, it would constrict the penis. This type of condom was the original "capote" (French for condom), perhaps because of its resemblance to a woman's bonnet worn at that time, also called a capote.
For many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. In 1912, Polish inventor Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called "cement dipping", this method required adding gasoline or benzene to the rubber to make it liquid. Latex, rubber suspended in water, was invented in 1920. Latex condoms required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. The use of water to suspend the rubber instead of gasoline and benzene eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber).
Until the twenties, all condoms were individually hand-dipped by semi-skilled workers. Throughout the decade of the 1920s, advances in the automation of the condom assembly line were made. The first fully automated line was patented in 1930. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.
1930 to present.
In 1930 the Anglican Church's Lambeth Conference sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement. The Roman Catholic Church responded by issuing the encyclical "Casti Connubii" affirming its opposition to all contraceptives, a stance it has never reversed.
In the 1930s, legal restrictions on condoms began to be relaxed. But during this period Fascist Italy and Nazi Germany increased restrictions on condoms (limited sales as disease preventatives were still allowed). During the Depression, condom lines by Schmid gained in popularity. Schmid still used the cement-dipping method of manufacture which had two advantages over the latex variety. Firstly, cement-dipped condoms could be safely used with oil-based lubricants. Secondly, while less comfortable, these older-style rubber condoms could be reused and so were more economical, a valued feature in hard times. More attention was brought to quality issues in the 1930s, and the U.S. Food and Drug Administration began to regulate the quality of condoms sold in the United States.
Throughout World War II, condoms were not only distributed to male U.S. military members, but also heavily promoted with films, posters, and lectures. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to this day.
After the war, condom sales continued to grow. From 1955–1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950–1960, 60% of married couples used condoms. The birth control pill became the world's most popular method of birth control in the years after its 1960 début, but condoms remained a strong second. The U.S. Agency for International Development pushed condom use in developing countries to help solve the "world population crises": by 1970 hundreds of millions of condoms were being used each year in India alone.(This number has grown in recent decades: in 2004, the government of India purchased 1.9 billion condoms for distribution at family planning clinics.)
In the 1960s and 1970s quality regulations tightened, and more legal barriers to condom use were removed. In Ireland, legal condom sales were allowed for the first time in 1978. Advertising, however was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television: this policy remained in place until 1979.
After learning in the early 1980s that AIDS can be a sexually transmitted infection, the use of condoms was encouraged to prevent transmission of HIV. Despite opposition by some political, religious, and other figures, national condom promotion campaigns occurred in the U.S. and Europe. These campaigns increased condom use significantly.
Due to increased demand and greater social acceptance, condoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Wal-Mart. Condom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. The phenomenon of decreasing use of condoms as disease preventatives has been called "prevention fatigue" or "condom fatigue". Observers have cited condom fatigue in both Europe and North America. As one response, manufacturers have changed the tone of their advertisements from scary to humorous.
New developments continued to occur in the condom market, with the first polyurethane condom—branded Avanti and produced by the manufacturer of Durex—introduced in the 1990s, and the first custom sized-to-fit condom, called TheyFit, introduced in 2011.
Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms by 2015. As of September 2013, condoms are available inside prisons in Canada, most of the European Union, Australia, Brazil, Indonesia, South Africa, and the US states of Vermont (on September 17, 2013, the Californian Senate approved a bill for condom distribution inside the state's prisons, but the bill was not yet law at the time of approval).
Etymology and other terms.
The term "condom" first appears in the early 18th century. Its etymology is unknown.
In popular tradition, the invention and naming of the condom came to be attributed to an associate of England's King Charles II, one "Dr. Condom" or "Earl of Condom". There is however no evidence of the existence of such a person, and condoms had been used for over one hundred years before King Charles II ascended to the throne.
A variety of unproven Latin etymologies have been proposed, including "condon" (receptacle), "condamina" (house), and "cumdum" (scabbard or case). It has also been speculated to be from the Italian word "guantone", derived from "guanto", meaning glove. William E. Kruck wrote an article in 1981 concluding that, "As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology." Modern dictionaries may also list the etymology as "unknown".
Other terms are also commonly used to describe condoms. In North America condoms are also commonly known as "prophylactics", or "rubbers". In Britain they may be called "French letters". Additionally, condoms may be referred to using the manufacturer's name.
Society and culture.
There do exist some criticisms of condoms both on the moral level and on the scientific level despite the many proven benefits of condoms by scientific consensus and sexual health experts, particularly over the last century when direct and concentrated research on condoms has increased.
Condom usage is typically recommended for new couples whom have yet to develop full trust in their partner with regard to STDs. Established couples on the other hand have few concerns about STDs, and can use other methods of birth control such as the the pill, which does not act as a barrier to intimate sexual contact. Note that the polar debate with regard to condom usage is attenuated by the target group the argument is directed. Notably the age category and stable partner question are factors, as well as the distinction between heterosexual and homosexuals, whom have different kinds of sex and have different risk consequences and factors. 
Among the prime objections to condom usage is the blocking of erotic sensation, and/or the intimacy that barrier-free sex provides. As the condom is held tightly to the skin of the penis, it diminishes the delivery of stimulation through rubbing and friction. Condom proponents claim this has the benefit of making sex last longer, by diminishing sensation and delaying male ejaculation. Those who promote condom-free heterosexual sex (slang: "bareback") claim that the condom puts a prophylactic barrier between partners, diminishing what is normally a highly sensual, intimate, and spiritual connection between partners.
Religious.
Roman Catholic Church opposes all kinds of sexual acts outside of marriage, as well as any sexual act in which the chance of successful conception has been reduced by direct and intentional acts (for example, surgery to prevent conception) or foreign objects (for example, condoms).
The use of condoms to prevent STD transmission is not specifically addressed by Catholic doctrine, and is currently a topic of debate among theologians and high-ranking Catholic authorities. A few, such as Belgian Cardinal Godfried Danneels, believe the Catholic Church should actively support condoms used to prevent disease, especially serious diseases such as AIDS. However, the majority view—including all statements from the Vatican—is that condom-promotion programs encourage promiscuity, thereby actually increasing STD transmission. This view was most recently reiterated in 2009 by Pope Benedict XVI.
The Roman Catholic Church is the largest organized body of any world religion. The church has hundreds of programs dedicated to fighting the AIDS epidemic in Africa, but its opposition to condom use in these programs has been highly controversial.
In a November 2011 interview, the Pope discussed for the first time the use of condoms to prevent STD transmission. He said that the use of a condom can be justified in a few individual cases if the purpose is to reduce the risk of an HIV infection. He gave as an example male prostitutes. There was some confusion at first whether the statement applied only to homosexual prostitutes and thus not to heterosexual intercourse at all. However, Federico Lombardi, spokesman for the Vatican, clarified that it applied to heterosexual and transsexual prostitutes, whether male or female, as well. He did, however, also clarify that the Vatican's principles on sexuality and contraception had not been changed.
Scientific and environmental.
More generally, some scientific researchers have expressed objective concern over certain ingredients sometimes added to condoms, notably talc and nitrosamines. Dry dusting powders are applied to latex condoms before packaging to prevent the condom from sticking to itself when rolled up. Previously, talc was used by most manufacturers, but cornstarch is currently the most popular dusting powder. Talc is known to be toxic if it enters the abdominal cavity (i.e., via the vagina). Cornstarch is generally believed to be safe; however, some researchers have raised concerns over its use as well.
Nitrosamines, which are potentially carcinogenic in humans, are believed to be present in a substance used to improve elasticity in latex condoms. A 2001 review stated that humans regularly receive 1,000 to 10,000 times greater nitrosamine exposure from food and tobacco than from condom use and concluded that the risk of cancer from condom use is very low. However, a 2004 study in Germany detected nitrosamines in 29 out of 32 condom brands tested, and concluded that exposure from condoms might exceed the exposure from food by 1.5- to 3-fold.
In addition, the large-scale use of disposable condoms has resulted in concerns over their environmental impact via littering and in landfills, where they can eventually wind up in wildlife environments if not incinerated or otherwise permanently disposed of first. Polyurethane condoms in particular, given they are a form of plastic, are not biodegradable, and latex condoms take a very long time to break down. Experts, such as AVERT, recommend condoms be disposed of in a garbage receptacle, as flushing them down the toilet (which some people do) may cause plumbing blockages and other problems. Furthermore, the plastic and foil wrappers condoms are packaged in are also not biodegradable. However, the benefits condoms offer are widely considered to offset their small landfill mass. Frequent condom or wrapper disposal in public areas such as a parks have been seen as a persistent litter problem.
While biodegradable, latex condoms damage the environment when disposed of improperly. According to the Ocean Conservancy, condoms, along with certain other types of trash, cover the coral reefs and smother sea grass and other bottom dwellers. The United States Environmental Protection Agency also has expressed concerns that many animals might mistake the litter for food.
Cultural barriers to use.
In much of the Western world, the introduction of the pill in the 1960s was associated with a decline in condom use. In Japan, oral contraceptives were not approved for use until September 1999, and even then access was more restricted than in other industrialized nations. Perhaps because of this restricted access to hormonal contraception, Japan has the highest rate of condom usage in the world: in 2008, 80% of contraceptive users relied on condoms.
Cultural attitudes toward gender roles, contraception, and sexual activity vary greatly around the world, and range from extremely conservative to extremely liberal. But in places where condoms are misunderstood, mischaracterised, demonised, or looked upon with overall cultural disapproval, the prevalence of condom use is directly affected. In less-developed countries and among less-educated populations, misperceptions about how disease transmission and conception work negatively affect the use of condoms; additionally, in cultures with more traditional gender roles, women may feel uncomfortable demanding that their partners use condoms.
As an example, Latino immigrants in the United States often face cultural barriers to condom use. A study on female HIV prevention published in the "Journal of Sex Health Research" asserts that Latino women often lack the attitudes needed to negotiate safe sex due to traditional gender-role norms in the Latino community, and may be afraid to bring up the subject of condom use with their partners. Women who participated in the study often reported that because of the general machismo subtly encouraged in Latino culture, their male partners would be angry or possibly violent at the woman's suggestion that they use condoms. A similar phenomenon has been noted in a survey of low-income American black women; the women in this study also reported a fear of violence at the suggestion to their male partners that condoms be used.
A telephone survey conducted by Rand Corporation and Oregon State University, and published in the "Journal of Acquired Immune Deficiency Syndromes" showed that belief in AIDS conspiracy theories among United States black men is linked to rates of condom use. As conspiracy beliefs about AIDS grow in a given sector of these black men, consistent condom use drops in that same sector. Female use of condoms was not similarly affected.
In the African continent, condom promotion in some areas has been impeded by anti-condom campaigns by some Muslim and Catholic clerics. Among the Maasai in Tanzania, condom use is hampered by an aversion to "wasting" sperm, which is given sociocultural importance beyond reproduction. Sperm is believed to be an "elixir" to women and to have beneficial health effects. Maasai women believe that, after conceiving a child, they must have sexual intercourse repeatedly so that the additional sperm aids the child's development. Frequent condom use is also considered by some Maasai to cause impotence. Some women in Africa believe that condoms are "for prostitutes" and that respectable women should not use them. A few clerics even promote the idea that condoms are deliberately laced with HIV. In the United States, possession of many condoms has been used by police to accuse women of engaging in prostitution. The Presidential Advisory Council on HIV/AIDS has condemned this practice and there are efforts to end it.
In March 2013, technology mogul Bill Gates offered a US$100,000 grant through his foundation for a condom design that "significantly preserves or enhances pleasure" to encourage more males to adopt the use of condoms for safer sex. The grant information states: “The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?” The project has been named the "Next Generation Condom" and anyone who can provide a "testable hypothesis" is eligible to apply.
Middle-Eastern couples who have not had children, because of the strong desire and social pressure to establish fertility as soon as possible within marriage, rarely use condoms.
Major manufacturers.
One analyst described the size of the condom market as something that "boggles the mind". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations. Most large manufacturers have ties to the business that reach back to the end of the 19th century.
Research.
A spray-on condom made of latex is intended to be easier to apply and more successful in preventing the transmission of diseases. As of 2009, the spray-on condom was not going to market because the drying time could not be reduced below two to three minutes.
The Invisible Condom, developed at Université Laval in Québec, Canada, is a gel that hardens upon increased temperature after insertion into the vagina or rectum. In the lab, it has been shown to effectively block HIV and herpes simplex virus. The barrier breaks down and liquefies after several hours. As of 2005, the invisible condom is in the clinical trial phase, and has not yet been approved for use.
Also developed in 2005 is a condom treated with an erectogenic compound. The drug-treated condom is intended to help the wearer maintain his erection, which should also help reduce slippage. If approved, the condom would be marketed under the Durex brand. As of 2007, it was still in clinical trials. In 2009, Ansell Healthcare, the makers of Lifestyle condoms, introduced the X2 condom lubricated with "Excite Gel" which contains the amino acid l-arginine and is intended to improve the strength of the erectile response.

</doc>
<doc id="5375" url="http://en.wikipedia.org/wiki?curid=5375" title="Country code">
Country code

Country codes are short alphabetic or numeric geographical codes (geocodes) developed to represent countries and dependent areas, for use in data processing and communications. Several different systems have been developed to do this. The best known of these is ISO 3166-1. The term "country code" frequently refers to international dialing codes, the E.164 country calling codes.
ISO 3166-1.
This standard defines for most of the countries and dependent areas in the world:
The two-letter codes are used as the basis for some other codes or applications, for example,
For more applications see ISO 3166-1 alpha-2.
Other country codes.
The developers of ISO 3166 intended that in time it would replace other coding systems in existence.
Other codings.
The following can represent countries:
Lists of country codes by country.
 -
 -
 -
 -
 -
 -
 -
References.
Ref#1 point to non-existent source with HTTP30X code.

</doc>
<doc id="5376" url="http://en.wikipedia.org/wiki?curid=5376" title="Cladistics">
Cladistics

Cladistics (from Greek , "klados", i.e. "branch") is an approach to biological classification in which organisms are grouped together based on whether or not they have one or more shared unique characteristics that come from the group's last common ancestor and are not present in more distant ancestors. Therefore, members of the same group are thought to share a common history and are considered to be more closely related.
The original methods used in cladistic analysis and the school of taxonomy derived from it originated in the work of the German entomologist Willi Hennig, who referred to it as phylogenetic systematics (also the title of his 1966 book); the terms "cladistics" and "clade" were popularized by other researchers. Cladistics in the original sense refers to a particular set of methods used in phylogenetic analysis, although it is now sometimes used to refer to the whole field.
The techniques of cladistics, and sometimes the terminology, have been successfully applied to other disciplines: for example, to determine the relationships between the surviving manuscripts of the "Canterbury Tales", or also between 53 manuscripts of the Sanskrit "Charaka Samhita".
History of cladistics.
What is now called the cladistic method appeared as early as 1901 with a work by Peter Chalmers Mitchell (for birds) and subsequently by Robert John Tillyard (for insects) in 1921, and W. Zimmermann (for plants) in 1943.
The term "clade" was introduced in 1958 by Julian Huxley after having been coined by Lucien Cuénot in 1940, "cladogenesis" in 1958, "cladistic" by Cain and Harrison in 1960, "cladist" (for an adherent of Hennig's school) by Mayr in 1965, and "cladistics" in 1966. Hennig referred to his own approach as "phylogenetic systematics". From the time of his original formulation until the end of the 1970s, cladistics competed as an analytical and philosophical approach to phylogenetic inference with phenetics and so-called evolutionary taxonomy. Phenetics was championed at this time by the numerical taxonomists Peter Sneath and Robert Sokal and the evolutionary taxonomist Ernst Mayr.
Originally conceived, if only in essence, by Willi Hennig in a book published in 1950, cladistics did not flourish until its translation into English in 1966 (Lewin 1997). Today, cladistics is the most popular method for constructing phylogenies not only from morphological data but also from molecular. Unlike phenetics, cladistics is specifically aimed at reconstructing evolutionary histories.
In the 1990s, the development of effective polymerase chain reaction techniques allowed the application of cladistic methods to biochemical and molecular genetic traits of organisms, as well as to anatomical ones, vastly expanding the amount of data available for phylogenetics. At the same time, cladistics rapidly became the dominant set of methods of phylogenetics in evolutionary biology, because computers made it possible to process large quantities of data about organisms and their characteristics.
The way for computational phylogenetics was paved by phenetics, a set of methods commonly used from the 1950s to 1980s and to some degree later. Phenetics did not try to reconstruct phylogenetic trees; rather, it tried to build dendrograms from similarity data; its algorithms required less computer power than phylogenetic ones.
Methodology.
The cladistic method interprets each character state transformation implied by the distribution of shared character states among taxa (or other terminals) as a potential piece of evidence for grouping. The outcome of a cladistic analysis is a cladogram – a tree-shaped diagram (dendrogram) that is interpreted to represent the best hypothesis of phylogenetic relationships. Although traditionally such cladograms were generated largely on the basis of morphological characters and originally calculated by hand, genetic sequencing data and computational phylogenetics are now commonly used in phylogenetic analyses, and the parsimony criterion has been abandoned by many phylogeneticists in favor of more "sophisticated" but less parsimonious evolutionary models of character state transformation. Cladists contend that these models are unjustified.
Every cladogram is based on a particular dataset analyzed with a particular method. Datasets are tables consisting of molecular, morphological, ethological and/or other characters and a list of operational taxonomic units (OTUs) which may be genes, individuals, populations, species, or larger taxa that are presumed to be monophyletic and therefore to form, all together, one large clade; phylogenetic analysis infers the branching pattern within that clade. Different datasets and different methods, not to mention violations of the mentioned assumptions, often result in different cladograms. Only scientific investigation can show which is more likely to be correct.
Until recently, for example, cladograms like the following have generally been accepted as accurate representations of the ancestral relations among turtles, lizards, crocodilians, and birds:
If this phylogenetic hypothesis is correct, then the last common ancestor of turtles and birds, at the ⊣ connection near the (a ⊤ in ) lived earlier than the last common ancestor of lizards and birds, near the . Most molecular evidence, however, produces cladograms more like this:
If this is accurate, then the last common ancestor of turtles and birds lived later than the last common ancestor of lizards and birds. Since the cladograms provide competing accounts of real events, at most one of them is correct.
The cladogram to the right represents the current universally accepted hypothesis that all primates, including strepsirrhines like the lemurs and lorises, had a common ancestor all of whose descendants were primates, and so form a clade; the name Primates is therefore recognized for this clade. Within the primates, all anthropoids (monkeys, apes and humans) are hypothesized to have had a common ancestor all of whose descendants were anthropoids, so they form the clade called Anthropoidea. The "prosimians", on the other hand, form a paraphyletic taxon. The name Prosimii is not used in phylogenetic nomenclature, which names only clades; the "prosimians" are instead divided between the clades Strepsirhini and Haplorhini, where the latter contains Tarsiiformes and Anthropoidea.
Terminology for character states.
The following terms, coined by Hennig, are used to identify shared or distinct character states among groups:
The terms plesiomorphy and apomorphy are relative; their application depends on the position of a group within a tree. For example, when trying to decide whether the tetrapods form a clade, an important question is whether having four limbs is a synapomorphy of the earliest taxa to be included within Tetrapoda: did all the earliest members of the Tetrapoda inherit four limbs from a common ancestor, whereas all other vertebrates did not, or at least not homologously? By contrast, for a group within the tetrapods, such as birds, having four limbs is a plesiomorphy. Using these two terms allows a greater precision in the discussion of homology, in particular allowing clear expression of the hierarchical relationships among different homologous features.
It can be difficult to decide whether a character state is in fact the same and thus can be classified as a synapomorphy which may identify a monophyletic group or whether it only appears to be the same and is thus a homoplasy which cannot identify such a group. There is a danger of circular reasoning: assumptions about the shape of a phylogenetic tree are used to justify decisions about character states, which are then used as evidence for the shape of the tree. Phylogenetics uses various forms of parsimony to decide such questions; the conclusions reached often depend on the dataset and the methods. Such is the nature of empirical science, and for this reason, most cladists refer to their cladograms as hypotheses of relationship. Cladograms that are supported by a large number and variety of different kinds of characters are viewed as more robust than those based on more limited evidence.
Terminology for taxa.
Mono-, para- and polyphyletic taxa can be understood based on the shape of the tree (as done above), as well as based on their character states. These are compared in the table below.
Criticism.
Cladistics, either generally or in specific applications, has been criticized from its beginnings. Decisions as to whether particular character states are homologous, a precondition of their being synapomorphies, have been challenged as involving circular reasoning and subjective judgements. Transformed cladistics arose in the late 1970s in an attempt to resolve some of these problems by removing phylogeny from cladistic analysis, however it has remained unpopular.
However, homology is usually determined from analysis of the results which are evaluated with homology measures, mainly the CI (consistency index) and RI (retention index), which, it has been claimed, makes the process objective. Also, homology can be equated to synapomorphy, which is what Patterson has done.
Application to disciplines other than biology.
The comparisons used to acquire data on which cladograms can be based are not limited to the field of biology. Any group of individuals or classes that are hypothesized to have a common ancestor, and to which a set of common characteristics may or may not apply, can be compared pairwise. Cladograms can be used to depict the hypothetical descent relationships within groups of items in many different academic realms. The only requirement is that the items have characteristics that can be identified and measured.
Recent attempts to use cladistic methods outside of biology address the reconstruction of lineages in:

</doc>
<doc id="5377" url="http://en.wikipedia.org/wiki?curid=5377" title="Calendar">
Calendar

A calendar is a system of organizing days for social, religious, commercial or administrative purposes. This is done by giving names to periods of time, typically days, weeks, months, and years. A date is the designation of a single, specific day within such a system. Periods in a calendar (such as years and months) are usually, though not necessarily, synchronized with the cycle of the sun or the moon. Many civilizations and societies have devised a calendar, usually derived from other calendars on which they model their systems, suited to their particular needs.
A calendar is also a physical device (often paper). This is the most common usage of the word. Other similar types of calendars can include computerized systems, which can be set to remind the user of upcoming events and appointments.
A calendar can also mean a list of planned events, such as a court calendar.
The English word "calendar" is derived from the Latin word "kalendae", which was the Latin name of the first day of every month.
Calendar systems.
A full calendar system has a different calendar date for every day. Thus the week cycle is by itself not a full calendar system; neither is a system to name the days within a year without a system for identifying the years.
The simplest calendar system just counts time periods from a reference date. This applies for the Julian day or Unix Time. Virtually the only possible variation is using a different reference date, in particular one less distant in the past to make the numbers smaller. Computations in these systems are just a matter of addition and subtraction.
Other calendars have one (or multiple) larger units of time.
Calendars that contain one level of cycles:
Calendars with two levels of cycles:
Cycles can be synchronized with periodic phenomena:
Very commonly a calendar includes more than one type of cycle, or has both cyclic and acyclic elements.
Many calendars incorporate simpler calendars as elements. For example, the rules of the Hebrew calendar depend on the seven-day week cycle (a very simple calendar), so the week is one of the cycles of the Hebrew calendar. It is also common to operate two calendars simultaneously, usually providing unrelated cycles, and the result may also be considered a more complex calendar. For example, the Gregorian calendar has no inherent dependence on the seven-day week, but in Western society the two are used together, and calendar tools indicate both the Gregorian date and the day of week.
The week cycle is shared by various calendar systems (although the significance of special days such as Friday, Saturday, and Sunday varies). Systems of leap days usually do not affect the week cycle. The week cycle was not even interrupted when 10, 11, 12, or 13 dates were skipped when the Julian calendar was replaced by the Gregorian calendar by various countries.
Solar calendars.
Days used by solar calendars.
Solar calendars assign a "date" to each solar day. A day may consist of the period between sunrise and sunset, with a following period of night, or it may be a period between successive events such as two sunsets. The length of the interval between two such successive events may be allowed to vary slightly during the year, or it may be averaged into a mean solar day. Other types of calendar may also use a solar day.
Calendar reform.
There have been a number of proposals for reform of the calendar, such as the World Calendar, International Fixed Calendar, Holocene calendar, and, recently, the Hanke-Henry Permanent Calendar. The United Nations considered adopting such a reformed calendar for a while in the 1950s, but these proposals have lost most of their popularity.
Lunar calendars.
Not all calendars use the solar year as a unit. A lunar calendar is one in which days are numbered within each lunar phase cycle. Because the length of the lunar month is not an even fraction of the length of the tropical year, a purely lunar calendar quickly drifts against the seasons, which don't vary much near the equator. It does, however, stay constant with respect to other phenomena, notably tides. An example is the Islamic calendar.
Alexander Marshack, in a controversial reading, believed that marks on a bone baton (c. 25,000 BC) represented a lunar calendar. Other marked bones may also represent lunar calendars. Similarly, Michael Rappenglueck believes that marks on a 15,000-year old cave painting represent a lunar calendar.
Lunisolar calendars.
A lunisolar calendar is a lunar calendar that compensates by adding an extra month as needed to realign the months with the seasons. An example is the Hebrew calendar which uses a 19-year cycle.
Calendar subdivisions.
Nearly all calendar systems group consecutive days into "months" and also into "years". In a "solar calendar" a "year" approximates Earth's tropical year (that is, the time it takes for a complete cycle of seasons), traditionally used to facilitate the planning of agricultural activities. In a "lunar calendar", the "month" approximates the cycle of the moon phase. Consecutive days may be grouped into other periods such as the week.
Because the number of days in the "tropical year" is not a whole number, a solar calendar must have a different number of days in different years. This may be handled, for example, by adding an extra day in leap years. The same applies to months in a lunar calendar and also the number of months in a year in a lunisolar calendar. This is generally known as intercalation. Even if a calendar is solar, but not lunar, the year cannot be divided entirely into months that never vary in length.
Cultures may define other units of time, such as the week, for the purpose of scheduling regular activities that do not easily coincide with months or years. Many cultures use different baselines for their calendars' starting years. For example, the year in Japan is based on the reign of the current emperor: 2006 was Year 18 of the Emperor Akihito.
Other calendar types.
Arithmetic and astronomical calendars.
An "astronomical calendar" is based on ongoing observation; examples are the religious Islamic calendar and the old religious Jewish calendar in the time of the Second Temple. Such a calendar is also referred to as an "observation-based" calendar. The advantage of such a calendar is that it is perfectly and perpetually accurate. The disadvantage is that working out when a particular date would occur is difficult.
An "arithmetic calendar" is one that is based on a strict set of rules; an example is the current Jewish calendar. Such a calendar is also referred to as a "rule-based" calendar. The advantage of such a calendar is the ease of calculating when a particular date occurs. The disadvantage is imperfect accuracy. Furthermore, even if the calendar is very accurate, its accuracy diminishes slowly over time, owing to changes in Earth's rotation. This limits the lifetime of an accurate arithmetic calendar to a few thousand years. After then, the rules would need to be modified from observations made since the invention of the calendar.
Complete and incomplete calendars.
Calendars may be either complete or incomplete. Complete calendars provide a way of naming each consecutive day, while incomplete calendars do not. The early Roman calendar, which had no way of designating the days of the winter months other than to lump them together as "winter", is an example of an incomplete calendar, while the Gregorian calendar is an example of a complete calendar.
Uses.
The primary practical use of a calendar is to identify days: to be informed about and/or to agree on a future event and to record an event that has happened. Days may be significant for civil, religious or social reasons. For example, a calendar provides a way to determine which days are religious or civil holidays, which days mark the beginning and end of business accounting periods, and which days have legal significance, such as the day taxes are due or a contract expires. Also a calendar may, by identifying a day, provide other useful information about the day such as its season.
Calendars are also used to help people manage their personal schedules, time and activities, particularly when individuals have numerous work, school, and family commitments. People frequently use multiple systems, and may keep both a business and family calendar to help prevent them from overcommitting their time.
Calendars are also used as part of a complete timekeeping system: date and time of day together specify a moment in time. In the modern world, written calendars are no longer an essential part of such systems, as the advent of accurate clocks has made it possible to record time independently of astronomical events.
Calendars in use.
Gregorian calendar.
Calendars in widespread use today include the Gregorian calendar, which is the "de facto" international standard, and is used almost everywhere in the world for civil purposes. Due to the Gregorian calendar's obvious connotations of Western Christianity, non-Christians and even some Christians sometimes replace the traditional era notations ""AD" and "BC"" ("Anno Domini" and "Before Christ") with ""CE" and "BCE"" ("Common Era" and "Before Common Era").
Even where there is a commonly used calendar such as the Gregorian calendar, alternative calendars may also be used, such as a fiscal calendar or the astronomical year numbering system.
Islamic calendar.
The Islamic calendar, Muslim calendar or Hijri calendar is a lunar calendar consisting of 12 lunar months in a year of 354 or 355 days. It is used to date events in most of the Muslim countries (concurrently with the Gregorian calendar), and used by Muslims everywhere to determine the proper day on which to celebrate Islamic holy days and festivals. The first year was the year during which the emigration of the Prophet Muhammad from Mecca to Medina, known as the Hijra, occurred. Each numbered year is designated either H for Hijra or AH for the Latin anno Hegirae (in the year of the Hijra). Being a purely lunar calendar, it is not synchronized with the seasons. With an annual drift of 11 or 12 days, the seasonal relation is repeated approximately each 33 Islamic years.
The Hijri-Shamsi calendar, also adopted by the Ahmadiyya Muslim Community, is based on solar calculations and is similar to the Gregorian calendar in its structure with the exception that the first year starts with Hijra.
Hindu calendar.
The lunisolar Hindu calendars are some of the most ancient calendars of the world. The names of the twelve months are the same in most parts of India (because they are named in Sanskrit) though the spelling and pronunciation vary slightly and the first month of the year varies from one region to another depending upon past traditions. The standardized Indian national calendar based on Saka era with 78 CE as the start of the calendar is a lunar Hindu calendar used by the government together with Gregorian calendar. However, it is not used by the general public which widely uses the Vikrami lunar calendar Vikram Samvat with 56 BCE as the start of the calendar for religious activities and festivals. Vikram Samvat is also the official calendar in Nepal. A lunar month may have 29 or 30 days. All Hindu calendars have about 360 days and they insert an additional 13th month (by repeating a month twice in that year) every few years to synchronize with solar calendar. If the repeated month is, say, Chaitra, the repeated month is named Chaitra-2. In the next cycle, a month other than Chaitra will be repeated.
Others.
Eastern Christians of eastern Europe and western Asia used for a long time the Julian Calendar, that of the old Orthodox church, in countries like Russia. For over 1500 years, Westerners used the Julian Calendar as well.
While the Gregorian calendar is widely used in Israel's business and day-to-day affairs, the Hebrew calendar, used by Jews worldwide for religious and cultural affairs, also influences civil matters in Israel (such as national holidays) and can be used there for business dealings (such as for the dating of checks). The Chinese, Hebrew, Hindu, and Julian calendars are widely used for religious and/or social purposes. The Iranian (Persian) calendar is used in Iran and some parts of Afghanistan. The Ethiopian calendar or Ethiopic calendar is the principal calendar used in Ethiopia and Eritrea, with the Oromo calendar also in use in some areas. In neighboring Somalia, the Somali calendar co-exists alongside the Gregorian and Islamic calendars. In Thailand, where the Thai solar calendar is used, the months and days have adopted the western standard, although the years are still based on the traditional Buddhist calendar. Bahá'ís worldwide use the Bahá'í calendar.
Fiscal calendars.
A fiscal calendar generally means the accounting year of a government or a business. It is used for budgeting, keeping accounts and taxation. It is a set of 12 months that may start at any date in a year. The US government's fiscal year starts on 1 October and ends on 30 September. The government of India's fiscal year starts on 1 April and ends on 31 March. Small traditional businesses in India start the fiscal year on Diwali festival and end the day before the next year's Diwali festival.
In accounting (and particularly accounting software), a fiscal calendar (such as a 4/4/5 calendar) fixes each month at a specific number of weeks to facilitate comparisons from month to month and year to year. January always has exactly 4 weeks (Sunday through Saturday), February has 4 weeks, March has 5 weeks, etc. Note that this calendar will normally need to add a 53rd week to every 5th or 6th year, which might be added to December or might not be, depending on how the organization uses those dates. There exists an international standard way to do this (the ISO week). The ISO week starts on a Monday, and ends on a Sunday. Week 1 is always the week that contains 4 January in the Gregorian calendar.
Physical calendars.
A calendar is also a physical device (often paper) (for example, a desktop calendar or a wall calendar). In a paper calendar one or two sheets can show a single day, a week, a month, or a year. If a sheet is for a single day, it easily shows the date and the weekday. If a sheet is for multiple days it shows a conversion table to convert from weekday to date and back. With a special pointing device, or by crossing out past days, it may indicate the current date and weekday. This is the most common usage of the word.
In the USA Sunday is considered the first day of the week and so appears on the far left and Saturday the last day of the week appearing on the far right. In Britain the weekend may appear at the end of the week so the first day is Monday and the last day is Sunday. The US calendar display is also used in Britain.
Calendar formats.
It is common to display the Gregorian calendar in separate monthly grids of seven columns (from Monday to Sunday, or Sunday to Saturday depending on which day is considered to start the week — this varies according to country) and five to six rows (or rarely, four rows when the month of February contains 28 days beginning on the first day of the week), with the day of the month numbered in each cell, beginning with 1.
When working with weeks rather than months, a continuous format is sometimes more convenient, where no blank cells are inserted to ensure that the first day of a new month begins on a fresh row. Unlike with geographical maps, where many famous projections are known (such as the Mercator projection), the calendar projection is a matter of such triviality that no names for these exist in common circulation.
Legal.
For lawyers and judges, the calendar is the docket used by the court to schedule the order of hearings or trials. A paralegal or court officer may keep track of the cases by using docketing software.

</doc>
<doc id="5378" url="http://en.wikipedia.org/wiki?curid=5378" title="Physical cosmology">
Physical cosmology

Physical cosmology is the study of the largest-scale structures and dynamics of the Universe and is concerned with fundamental questions about its formation, evolution, and ultimate fate. For most of human history, it was a branch of metaphysics and religion. Cosmology as a science originated with the Copernican principle, which implies that celestial bodies obey identical physical laws to those on Earth, and Newtonian mechanics, which first allowed us to understand those physical laws.
Physical cosmology, as it is now understood, began with the development in 1915 of Albert Einstein's general theory of relativity, followed by major observational discoveries in the 1920s: first, Edwin Hubble discovered that the Universe contains a huge number of external galaxies beyond our own Milky Way; then,
work by Vesto Slipher and others showed that the universe is expanding. These advances made it possible to speculate about the origin of the Universe, and allowed the establishment of the Big Bang Theory, by Georges Lemaitre, as the leading cosmological model. A few researchers still advocate a handful of alternative cosmologies; however, most cosmologists agree that the Big Bang theory explains the observations better.
Dramatic advances in observational cosmology since the 1990s, including the cosmic microwave background, distant supernovae and galaxy redshift surveys, have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations.
Cosmology draws heavily on the work of many disparate areas of research in theoretical and applied physics. Areas relevant to cosmology include particle physics experiments and theory, theoretical and observational astrophysics, general relativity, quantum mechanics, and plasma physics.
Subject history.
Modern cosmology developed along tandem tracks of theory and observation. In 1916, Albert Einstein published his theory of general relativity, which provided a unified description of gravity as a geometric property of space and time. At the time, Einstein believed in a static universe, but found that his original formulation of the theory did not permit it. This is because masses distributed throughout the Universe gravitationally attract, and move toward each other over time. However, he realized that his equations permitted the introduction of a constant term which could counteract the attractive force of gravity on the cosmic scale. Einstein published his first paper on relativistic cosmology in 1917, in which he added this "cosmological constant" to his field equations in order to force them to model a static universe. However, this so-called Einstein model is unstable to small perturbations—it will eventually start to expand or contract. The Einstein model describes a static universe; space is finite and unbounded (analogous to the surface of a sphere, which has a finite area but no edges). It was later realized that Einstein's model was just one of a larger set of possibilities, all of which were consistent with general relativity and the cosmological principle. The cosmological solutions of general relativity were found by Alexander Friedmann in the early 1920s. His equations describe the Friedmann–Lemaître–Robertson–Walker universe, which may expand or contract, and whose geometry may be open, flat, or closed.
In the 1910s, Vesto Slipher (and later Carl Wilhelm Wirtz) interpreted the red shift of spiral nebulae as a Doppler shift that indicated they were receding from Earth. However, it is difficult to determine the distance to astronomical objects. One way is to compare the physical size of an object to its angular size, but a physical size must be assumed to do this. Another method is to measure the brightness of an object and assume an intrinsic luminosity, from which the distance may be determined using the inverse square law. Due to the difficulty of using these methods, they did not realize that the nebulae were actually galaxies outside our own Milky Way, nor did they speculate about the cosmological implications. In 1927, the Belgian Roman Catholic priest Georges Lemaître independently derived the Friedmann–Lemaître–Robertson–Walker equations and proposed, on the basis of the recession of spiral nebulae, that the Universe began with the "explosion" of a "primeval atom"—which was later called the Big Bang. In 1929, Edwin Hubble provided an observational basis for Lemaître's theory. Hubble showed that the spiral nebulae were galaxies by determining their distances using measurements of the brightness of Cepheid variable stars. He discovered a relationship between the redshift of a galaxy and its distance. He interpreted this as evidence that the galaxies are receding from Earth in every direction at speeds proportional to their distance. This fact is now known as Hubble's law, though the numerical factor Hubble found relating recessional velocity and distance was off by a factor of ten, due to not knowing about the types of Cepheid variables.
Given the cosmological principle, Hubble's law suggested that the Universe was expanding. Two primary explanations were proposed for the expansion. One was Lemaître's Big Bang theory, advocated and developed by George Gamow. The other explanation was Fred Hoyle's steady state model in which new matter is created as the galaxies move away from each other. In this model, the Universe is roughly the same at any point in time.
For a number of years, support for these theories was evenly divided. However, the observational evidence began to support the idea that the Universe evolved from a hot dense state. The discovery of the cosmic microwave background in 1965 lent strong support to the Big Bang model, and since the precise measurements of the cosmic microwave background by the Cosmic Background Explorer in the early 1990s, few cosmologists have seriously proposed other theories of the origin and evolution of the cosmos. One consequence of this is that in standard general relativity, the Universe began with a singularity, as demonstrated by Roger Penrose and Stephen Hawking in the 1960s.
Energy of the cosmos.
Light chemical elements, primarily hydrogen and helium, were created in the Big Bang process (see Nucleosynthesis). The small atomic nuclei combined into larger atomic nuclei to form heavier elements such as iron and nickel, which are more stable (see Nuclear fusion). This caused a "later energy release". Such reactions of nuclear particles inside stars continue to contribute to "sudden energy releases", such as in nova stars. Gravitational collapse of matter into black holes is also thought to power the most energetic processes, generally seen at the centers of galaxies (see Quasar and Active galaxy).
Cosmologists cannot explain all cosmic phenomena exactly, such as those related to the accelerating expansion of the Universe, using conventional forms of energy. Instead, cosmologists propose a new form of energy called dark energy that permeates all space. One hypothesis is that dark energy is the energy of virtual particles, which are believed to exist in a vacuum due to the uncertainty principle.
There is no clear way to define the total energy in the Universe using the most widely accepted theory of gravity, general relativity. Therefore, it remains controversial whether the total energy is conserved in an expanding universe. For instance, each photon that travels through intergalactic space loses energy due to the redshift effect. This energy is not obviously transferred to any other system, so seems to be permanently lost. On the other hand, some cosmologists insist that energy is conserved in some sense; this follows the law of conservation of energy.
Thermodynamics of the universe is a field of study that explores which form of energy dominates the cosmos - relativistic particles which are referred to as radiation, or non-relativistic particles referred to as matter. Relativistic particles are particles whose rest mass is zero or negligible compared to their kinetic energy, and so move at the speed of light or very close to it; non-relativistic particles have much higher rest mass than their energy and so move much slower than the speed of light.
As the Universe expands, both matter and radiation in it become diluted. However, the energy densities of radiation and matter dilute at different rates. As a particular volume expands, mass energy density is changed only by the increase in volume, but the energy density of radiation is changed both by the increase in volume and by the increase in the wavelength of the photons that make it up. Thus the energy of radiation becomes a smaller part of the Universe's total energy than that of matter as it expands. The very early Universe is said to have been 'radiation dominated' and radiation controlled the deceleration of expansion. Later, as the average energy per photon becomes roughly 10 eV and lower, matter dictates the rate of deceleration and the Universe is said to be 'matter dominated'. The intermediate case is not treated well analytically. As the expansion of the universe continues, matter dilutes even further and the cosmological constant becomes dominant, leading to an acceleration in the universe's expansion.
History of the universe.
The history of the Universe is a central issue in cosmology. The history of the Universe is divided into different periods called epochs, according to the dominant forces and processes in each period. The standard cosmological model is known as the Lambda-CDM model.
Equations of motion.
The equations of motion governing the Universe as a whole are derived from general relativity with a small, positive cosmological constant. The solution is an expanding universe; due to this expansion, the radiation and matter in the Universe cool down and become diluted. At first, the expansion is slowed down by gravitation attracting the radiation and matter in the Universe. However, as these become diluted, the cosmological constant becomes more dominant and the expansion of the Universe starts to accelerate rather than decelerate. In our universe this happened billions of years ago.
Particle physics in cosmology.
Particle physics is important to the behavior of the early Universe, because the early Universe was so hot that the average energy density was very high. Because of this, scattering processes and decay of unstable particles are important in cosmology.
As a rule of thumb, a scattering or a decay process is cosmologically important in a certain cosmological epoch if the time scale describing that process is smaller than, or comparable to, the time scale of the expansion of the Universe. The time scale that describes the expansion of the Universe is formula_1 with formula_2 being the Hubble constant, which itself actually varies with time. The expansion timescale formula_1 is roughly equal to the age of the Universe at that time.
Timeline of the Big Bang.
Observations suggest that the Universe began around 13.8 billion years ago. Since then, the evolution of the Universe has passed through three phases. The very early Universe, which is still poorly understood, was the split second in which the Universe was so hot that particles had energies higher than those currently accessible in particle accelerators on Earth. Therefore, while the basic features of this epoch have been worked out in the Big Bang theory, the details are largely based on educated guesses.
Following this, in the early Universe, the evolution of the Universe proceeded according to known high energy physics. This is when the first protons, electrons and neutrons formed, then nuclei and finally atoms. With the formation of neutral hydrogen, the cosmic microwave background was emitted. Finally, the epoch of structure formation began, when matter started to aggregate into the first stars and quasars, and ultimately galaxies, clusters of galaxies and superclusters formed. The future of the Universe is not yet firmly known, but according to the ΛCDM model it will continue expanding forever.
Areas of study.
Below, some of the most active areas of inquiry in cosmology are described, in roughly chronological order. This does not include all of the Big Bang cosmology, which is presented in "Timeline of the Big Bang."
Very early Universe.
The early, hot Universe appears to be well explained by the Big Bang from roughly 10−33 seconds onwards. But there are several problems. One is that there is no compelling reason, using current particle physics, for the Universe to be flat, homogeneous, and isotropic (see the cosmological principle). Moreover, grand unified theories of particle physics suggest that there should be magnetic monopoles in the Universe, which have not been found. These problems are resolved by a brief period of cosmic inflation, which drives the Universe to flatness, smooths out anisotropies and inhomogeneities to the observed level, and exponentially dilutes the monopoles. The physical model behind cosmic inflation is extremely simple, but it has not yet been confirmed by particle physics, and there are difficult problems reconciling inflation and quantum field theory. Some cosmologists think that string theory and brane cosmology will provide an alternative to inflation.
Another major problem in cosmology is what caused the Universe to contain far more matter than antimatter. Cosmologists can observationally deduce that the Universe is not split into regions of matter and antimatter. If it were, there would be X-rays and gamma rays produced as a result of annihilation, but this is not observed. Therefore, some process in the early universe must have created a small excess of matter over antimatter, and this (currently not understood) process is called "baryogenesis". Three required conditions for baryogenesis were derived by Andrei Sakharov in 1967, and requires a violation of the particle physics symmetry, called CP-symmetry, between matter and antimatter. However, particle accelerators measure too small a violation of CP-symmetry to account for the baryon asymmetry. Cosmologists and particle physicists look for additional violations of the CP-symmetry in the early Universe that might account for the baryon asymmetry.
Both the problems of baryogenesis and cosmic inflation are very closely related to particle physics, and their resolution might come from high energy theory and experiment, rather than through observations of the Universe.
Big bang nucleosynthesis.
Big Bang nucleosynthesis is the theory of the formation of the elements in the early Universe. It finished when the Universe was about three minutes old and its temperature dropped below that at which nuclear fusion could occur. Big Bang nucleosynthesis had a brief period during which it could operate, so only the very lightest elements were produced. Starting from hydrogen ions (protons), it principally produced deuterium, helium-4, and lithium. Other elements were produced in only trace abundances. The basic theory of nucleosynthesis was developed in 1948 by George Gamow, Ralph Asher Alpher, and Robert Herman. It was used for many years as a probe of physics at the time of the Big Bang, as the theory of Big Bang nucleosynthesis connects the abundances of primordial light elements with the features of the early Universe. Specifically, it can be used to test the equivalence principle, to probe dark matter, and test neutrino physics. Some cosmologists have proposed that Big Bang nucleosynthesis suggests there is a fourth "sterile" species of neutrino.
Cosmic microwave background.
The cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed. At this point, radiation produced in the Big Bang stopped Thomson scattering from charged ions. The radiation, first observed in 1965 by Arno Penzias and Robert Woodrow Wilson, has a perfect thermal black-body spectrum. It has a temperature of 2.7 kelvins today and is isotropic to one part in 105. Cosmological perturbation theory, which describes the evolution of slight inhomogeneities in the early Universe, has allowed cosmologists to precisely calculate the angular power spectrum of the radiation, and it has been measured by the recent satellite experiments (COBE and WMAP) and many ground and balloon-based experiments (such as Degree Angular Scale Interferometer, Cosmic Background Imager, and Boomerang). One of the goals of these efforts is to measure the basic parameters of the Lambda-CDM model with increasing accuracy, as well as to test the predictions of the Big Bang model and look for new physics. The recent measurements made by WMAP, for example, have placed limits on the neutrino masses.
Newer experiments, such as QUIET and the Atacama Cosmology Telescope, are trying to measure the polarization of the cosmic microwave background. These measurements are expected to provide further confirmation of the theory as well as information about cosmic inflation, and the so-called secondary anisotropies, such as the Sunyaev-Zel'dovich effect and Sachs-Wolfe effect, which are caused by interaction between galaxies and clusters with the cosmic microwave background.
On 17 March 2014, astronomers at the Harvard-Smithsonian Center for Astrophysics announced the apparent detection of gravitational waves, which, if confirmed, may provide strong evidence for inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported.
Formation and evolution of large-scale structure.
Understanding the formation and evolution of the largest and earliest structures (i.e., quasars, galaxies, clusters and superclusters) is one of the largest efforts in cosmology. Cosmologists study a model of hierarchical structure formation in which structures form from the bottom up, with smaller objects forming first, while the largest objects, such as superclusters, are still assembling. One way to study structure in the Universe is to survey the visible galaxies, in order to construct a three-dimensional picture of the galaxies in the Universe and measure the matter power spectrum. This is the approach of the "Sloan Digital Sky Survey" and the 2dF Galaxy Redshift Survey.
Another tool for understanding structure formation is simulations, which cosmologists use to study the gravitational aggregation of matter in the Universe, as it clusters into filaments, superclusters and voids. Most simulations contain only non-baryonic cold dark matter, which should suffice to understand the Universe on the largest scales, as there is much more dark matter in the Universe than visible, baryonic matter. More advanced simulations are starting to include baryons and study the formation of individual galaxies. Cosmologists study these simulations to see if they agree with the galaxy surveys, and to understand any discrepancy.
Other, complementary observations to measure the distribution of matter in the distant universe and to probe reionization include:
These will help cosmologists settle the question of when and how structure formed in the Universe.
Dark matter.
Evidence from Big Bang nucleosynthesis, the cosmic microwave background and structure formation suggests that about 23% of the mass of the Universe consists of non-baryonic dark matter, whereas only 4% consists of visible, baryonic matter. The gravitational effects of dark matter are well understood, as it behaves like a cold, non-radiative fluid that forms haloes around galaxies. Dark matter has never been detected in the laboratory, and the particle physics nature of dark matter remains completely unknown. Without observational constraints, there are a number of candidates, such as a stable supersymmetric particle, a weakly interacting massive particle, an axion, and a massive compact halo object. Alternatives to the dark matter hypothesis include a modification of gravity at small accelerations (MOND) or an effect from brane cosmology.
Dark energy.
If the Universe is flat, there must be an additional component making up 73% (in addition to the 23% dark matter and 4% baryons) of the energy density of the Universe. This is called dark energy. In order not to interfere with Big Bang nucleosynthesis and the cosmic microwave background, it must not cluster in haloes like baryons and dark matter. There is strong observational evidence for dark energy, as the total energy density of the Universe is known through constraints on the flatness of the Universe, but the amount of clustering matter is tightly measured, and is much less than this. The case for dark energy was strengthened in 1999, when measurements demonstrated that the expansion of the Universe has begun to gradually accelerate.
Apart from its density and its clustering properties, nothing is known about dark energy. "Quantum field theory" predicts a cosmological constant (CC) much like dark energy, but 120 orders of magnitude larger than that observed. Steven Weinberg and a number of string theorists (see string landscape) have invoked the 'weak anthropic principle': i.e. the reason that physicists observe a universe with such a small cosmological constant is that no physicists (or any life) could exist in a universe with a larger cosmological constant. Many cosmologists find this an unsatisfying explanation: perhaps because while the weak anthropic principle is self-evident (given that living observers exist, there must be at least one universe with a cosmological constant which allows for life to exist) it does not attempt to explain the context of that universe. For example, the weak anthropic principle alone does not distinguish between:
Other possible explanations for dark energy include quintessence or a modification of gravity on the largest scales. The effect on cosmology of the dark energy that these models describe is given by the dark energy's equation of state, which varies depending upon the theory. The nature of dark energy is one of the most challenging problems in cosmology.
A better understanding of dark energy is likely to solve the problem of the ultimate fate of the Universe. In the current cosmological epoch, the accelerated expansion due to dark energy is preventing structures larger than superclusters from forming. It is not known whether the acceleration will continue indefinitely, perhaps even increasing until a big rip, or whether it will eventually reverse.
Other areas of inquiry.
Cosmologists also study:

</doc>
<doc id="5382" url="http://en.wikipedia.org/wiki?curid=5382" title="Inflation (cosmology)">
Inflation (cosmology)

In physical cosmology, cosmic inflation, cosmological inflation, or just inflation is the exponential expansion of space in the early universe. The inflationary epoch lasted from 10−36 seconds after the Big Bang to sometime between 10−33 and 10−32 seconds. Following the inflationary period, the universe continues to expand, but at a less accelerated rate.
The inflationary hypothesis was developed in the 1980s by physicists Alan Guth and Andrei Linde.
Inflation explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the Universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the universe is flat, and why no magnetic monopoles have been observed.
While the detailed particle physics mechanism responsible for inflation is not known, the basic picture makes a number of predictions that have been confirmed by observation. The hypothetical field thought to be responsible for inflation is called the inflaton.
On 17 March 2014, astrophysicists of the BICEP2 collaboration announced the detection of inflationary gravitational waves in the B-mode power spectrum, which if confirmed, would provide clear experimental evidence for the theory of inflation. However, on 19 June 2014, lowered confidence in confirming the findings was reported; and on 19 September 2014, even more lowered confidence.
Overview.
An expanding universe generally has a cosmological horizon, which, by analogy with the more familiar horizon caused by the curvature of the Earth's surface, marks the boundary of the part of the universe that an observer can see. Light (or other radiation) emitted by objects beyond the cosmological horizon never reaches the observer, because the space in between the observer and the object is expanding too rapidly.
The observable universe is one "causal patch" of a much larger unobservable universe; there are parts of the universe that cannot communicate with us yet. These parts of the universe are outside our current cosmological horizon. In the standard hot big bang model, without inflation, the cosmological horizon moves out, bringing new regions into view. Yet as a local observer sees these regions for the first time, they look no different from any other region of space the local observer has already seen: they have a background radiation that is at nearly exactly the same temperature as the background radiation of other regions, and their space-time curvature is evolving lock-step with ours. This presents a mystery: how did these new regions know what temperature and curvature they were supposed to have? They couldn't have learned it by getting signals, because they were not in communication with our past light cone before.
Inflation answers this question by postulating that all the regions come from an earlier era with a big vacuum energy, or cosmological constant. A space with a cosmological constant is qualitatively different: instead of moving outward, the cosmological horizon stays put. For any one observer, the distance to the cosmological horizon is constant. With exponentially expanding space, two nearby observers are separated very quickly; so much so, that the distance between them quickly exceeds the limits of communications. The spatial slices are expanding very fast to cover huge volumes. Things are constantly moving beyond the cosmological horizon, which is a fixed distance away, and everything becomes homogeneous very quickly.
As the inflationary field slowly relaxes to the vacuum, the cosmological constant goes to zero, and space begins to expand normally. The new regions that come into view during the normal expansion phase are exactly the same regions that were pushed out of the horizon during inflation, and so they are necessarily at nearly the same temperature and curvature, because they come from the same little patch of space.
The theory of inflation thus explains why the temperatures and curvatures of different regions are so nearly equal. It also predicts that the total curvature of a space-slice at constant global time is zero. This prediction implies that the total ordinary matter, dark matter, and residual vacuum energy in the universe have to add up to the critical density, and the evidence strongly supports this. More strikingly, inflation allows physicists to calculate the minute differences in temperature of different regions from quantum fluctuations during the inflationary era, and many of these quantitative predictions have been confirmed.
Space expands.
To say that space expands exponentially means that two inertial observers are moving farther apart with accelerating velocity. In stationary coordinates for one observer, a patch of an inflating universe has the following polar metric:
This is just like an inside-out black hole metric—it has a zero in the formula_2 component on a fixed radius sphere called the cosmological horizon. Objects are drawn away from the observer at formula_3 towards the cosmological horizon, which they cross in a finite proper time. This means that any inhomogeneities are smoothed out, just as any bumps or matter on the surface of a black hole horizon are swallowed and disappear.
Since the space–time metric has no explicit time dependence, once an observer has crossed the cosmological horizon, observers closer in take its place. This process of falling outward and replacement points closer in are always steadily replacing points further out—an exponential expansion of space–time.
This steady-state exponentially expanding spacetime is called a de Sitter space, and to sustain it there must be a cosmological constant, a vacuum energy proportional to formula_4 everywhere. In this case, the equation of state is formula_5. The physical conditions from one moment to the next are stable: the rate of expansion, called the Hubble parameter, is nearly constant, and the scale factor of the universe is proportional to formula_6. Inflation is often called a period of "accelerated expansion" because the distance between two fixed observers is increasing exponentially (i.e. at an accelerating rate as they move apart), while formula_4 can stay approximately constant (see deceleration parameter).
Few inhomogeneities remain.
Cosmological inflation has the important effect of smoothing out inhomogeneities, anisotropies and the curvature of space. This pushes the universe into a very simple state, in which it is completely dominated by the inflaton field, the source of the cosmological constant, and the only significant inhomogeneities are the tiny quantum fluctuations in the inflaton. Inflation also dilutes exotic heavy particles, such as the magnetic monopoles predicted by many extensions to the Standard Model of particle physics. If the universe was only hot enough to form such particles "before" a period of inflation, they would not be observed in nature, as they would be so rare that it is quite likely that there are none in the observable universe. Together, these effects are called the inflationary "no-hair theorem" by analogy with the no hair theorem for black holes.
The "no-hair" theorem works essentially because the cosmological horizon is no different from a black-hole horizon, except for philosophical disagreements about what is on the other side. The interpretation of the no-hair theorem is that the universe (observable and unobservable) expands by an enormous factor during inflation. In an expanding universe, energy densities generally fall, or get diluted, as the volume of the universe increases. For example, the density of ordinary "cold" matter (dust) goes down as the inverse of the volume: when linear dimensions double, the energy density goes down by a factor of eight; the radiation energy density goes down even more rapidly as the universe expands since the wavelength of each photon is stretched (redshifted), in addition to the photons being dispersed by the expansion. When linear dimensions are doubled, the energy density in radiation falls by a factor of sixteen (see the solution of the energy density continuity equation for an ultra-relativistic fluid).
During inflation, the energy density in the inflaton field is roughly constant. However, the energy density in everything else, including inhomogeneities, curvature, anisotropies, exotic particles, and standard-model particles is falling, and through sufficient inflation these all become negligible. This leaves the universe flat and symmetric, and (apart from the homogeneous inflaton field) mostly empty, at the moment inflation ends and reheating begins.
Key requirement.
A key requirement is that inflation must continue long enough to produce the present observable universe from a single, small inflationary Hubble volume. This is necessary to ensure that the universe appears flat, homogeneous and isotropic at the largest observable scales. This requirement is generally thought to be satisfied if the universe expanded by a factor of at least 1026 during inflation.
Reheating.
Inflation is a period of supercooled expansion, when the temperature drops by a factor of 100,000 or so. (The exact drop is model dependent, but in the first models it was typically from 1027K down to 1022K.) This relatively low temperature is maintained during the inflationary phase. When inflation ends the temperature returns to the pre-inflationary temperature; this is called "reheating" or thermalization because the large potential energy of the inflaton field decays into particles and fills the universe with Standard Model particles, including electromagnetic radiation, starting the radiation dominated phase of the Universe. Because the nature of the inflation is not known, this process is still poorly understood, although it is believed to take place through a parametric resonance.
Motivations.
Inflation resolves several problems in the Big Bang cosmology that were discovered in the 1970s.
Inflation was first discovered by Guth while investigating the problem of why no magnetic monopoles are seen today; he found that a positive-energy false vacuum would, according to general relativity, generate an exponential expansion of space. It was very quickly realised that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does "today", the universe would have to have started from very finely tuned, or "special" initial conditions at the Big Bang. Inflation attempts to resolve these problems by providing a dynamical mechanism that drives the universe to this special state, thus making a universe like ours much more likely in the context of the Big Bang theory.
Horizon problem.
The horizon problem is the problem of determining why the universe appears statistically homogeneous and isotropic in accordance with the cosmological principle. For example, molecules in a canister of gas are distributed homogeneously and isotropically because they are in thermal equilibrium: gas throughout the canister has had enough time to interact to dissipate inhomogeneities and anisotropies. The situation is quite different in the big bang model without inflation, because gravitational expansion does not give the early universe enough time to equilibrate. In a big bang with only the matter and radiation known in the Standard Model, two widely separated regions of the observable universe cannot have equilibrated because they move apart from each other faster than the speed of light—thus have never come into causal contact: in the history of the universe, back to the earliest times, it has not been possible to send a light signal between the two regions. Because they have no interaction, it is difficult to explain why they have the same temperature (are thermally equilibrated). This is because the Hubble radius in a radiation or matter-dominated universe expands much more quickly than physical lengths and so points that are out of communication are coming into communication. Historically, two proposed solutions were the "Phoenix universe" of Georges Lemaître and the related oscillatory universe of Richard Chase Tolman, and the Mixmaster universe of Charles Misner. Lemaître and Tolman proposed that a universe undergoing a number of cycles of contraction and expansion could come into thermal equilibrium. Their models failed, however, because of the buildup of entropy over several cycles. Misner made the (ultimately incorrect) conjecture that the Mixmaster mechanism, which made the universe "more" chaotic, could lead to statistical homogeneity and isotropy.
Flatness problem.
Another problem is the flatness problem (which is sometimes called one of the Dicke coincidences, with the other being the cosmological constant problem). It had been known in the 1960s that the density of matter in the universe was comparable to the critical density necessary for a flat universe (that is, a universe whose large scale geometry is the usual Euclidean geometry, rather than a non-Euclidean hyperbolic or spherical geometry).
Therefore, regardless of the shape of the universe the contribution of spatial curvature to the expansion of the universe could not be much greater than the contribution of matter. But as the universe expands, the curvature redshifts away more slowly than matter and radiation. Extrapolated into the past, this presents a fine-tuning problem because the contribution of curvature to the universe must be exponentially small (sixteen orders of magnitude less than the density of radiation at big bang nucleosynthesis, for example). This problem is exacerbated by recent observations of the cosmic microwave background that have demonstrated that the universe is flat to the accuracy of a few percent.
Magnetic-monopole problem.
The magnetic monopole problem (sometimes called the exotic-relics problem) says that if the early universe were very hot, a large number of very heavy, stable magnetic monopoles would be produced. This is a problem with Grand Unified Theories, which proposes that at high temperatures (such as in the early universe) the electromagnetic force, strong, and weak nuclear forces are not actually fundamental forces but arise due to spontaneous symmetry breaking from a single gauge theory. These theories predict a number of heavy, stable particles that have not yet been observed in nature. The most notorious is the magnetic monopole, a kind of stable, heavy "knot" in the magnetic field. Monopoles are expected to be copiously produced in Grand Unified Theories at high temperature, and they should have persisted to the present day, to such an extent that they would become the primary constituent of the universe. Not only is that not the case, but all searches for them have failed, placing stringent limits on the density of relic magnetic monopoles in the universe.
A period of inflation that occurs below the temperature where magnetic monopoles can be produced would offer a possible resolution of this problem: monopoles would be separated from each other as the universe around them expands, potentially lowering their observed density by many orders of magnitude. Though, as cosmologist Martin Rees has written, "Skeptics about exotic physics might not be hugely impressed by a theoretical argument to explain the absence of particles that are themselves only hypothetical. Preventive medicine can readily seem 100 percent effective against a disease that doesn't exist!"
History.
Precursors.
In the early days of General Relativity, Albert Einstein introduced the cosmological constant to allow a static solution, which was a three-dimensional sphere with a uniform density of matter. A little later, Willem de Sitter found a highly symmetric inflating universe, which described a universe with a cosmological constant that is otherwise empty. It was discovered that Einstein's solution is unstable, and if there are small fluctuations, it eventually either collapses or turns into de Sitter's.
In the early 1970s Zeldovich noticed the serious flatness and horizon problems of big bang cosmology; before his work, cosmology was presumed to be symmetrical on purely philosophical grounds. In the Soviet Union, this and other considerations led Belinski and Khalatnikov to analyze the chaotic BKL singularity in General Relativity. Misner's Mixmaster universe attempted to use this chaotic behavior to solve the cosmological problems, with limited success.
In the late 1970s, Sidney Coleman applied the instanton techniques developed by Alexander Polyakov and collaborators to study the fate of the false vacuum in quantum field theory. Like a metastable phase in statistical mechanics—water below the freezing temperature or above the boiling point—a quantum field would need to nucleate a large enough bubble of the new vacuum, the new phase, in order to make a transition. Coleman found the most likely decay pathway for vacuum decay and calculated the inverse lifetime per unit volume. He eventually noted that gravitational effects would be significant, but he did not calculate these effects and did not apply the results to cosmology.
In the Soviet Union, Alexei Starobinsky noted that quantum corrections to general relativity should be important in the early universe. These generically lead to curvature-squared corrections to the Einstein–Hilbert action and a form of "f"("R") modified gravity. The solution to Einstein's equations in the presence of curvature squared terms, when the curvatures are large, leads to an effective cosmological constant. Therefore, he proposed that the early universe went through a de Sitter phase, an inflationary era. This resolved the problems of cosmology, and led to specific predictions for the corrections to the microwave background radiation, corrections that were calculated in detail shortly afterwards.
In 1978, Zeldovich noted the monopole problem, which was an unambiguous quantitative version of the horizon problem, this time in a fashionable subfield of particle physics, which led to several speculative attempts to resolve it. In 1980, working in the west, Alan Guth realized that false vacuum decay in the early universe would solve the problem, leading him to propose scalar driven inflation. Starobinsky's and Guth's scenarios both predicted an initial deSitter phase, differing only in the details of the mechanism.
Early inflationary models.
According to Andrei Linde, the earliest theory of inflation was proposed by Erast Gliner (1965) but the theory was not taken seriously except by Andrei Sakharov, 'who made an attempt to calculate density perturbations
produced in this scenario." Independently, inflation was proposed in January 1980 by Alan Guth as a mechanism to explain the nonexistence of magnetic monopoles; it was Guth who coined the term "inflation". At the same time, Starobinsky argued that quantum corrections to gravity would replace the initial singularity of the universe with an exponentially expanding deSitter phase. In October 1980, Demosthenes Kazanas suggested that exponential expansion could eliminate the particle horizon and perhaps solve the horizon problem, while Sato suggested that an exponential expansion could eliminate domain walls (another kind of exotic relic). In 1981 Einhorn and Sato published a model similar to Guth's and showed that it would resolve the puzzle of the magnetic monopole abundance in Grand Unified Theories. Like Guth, they concluded that such a model not only required fine tuning of the cosmological constant, but also would very likely lead to a much too granular universe, i.e., to large density variations resulting from bubble wall collisions.
Guth proposed that as the early universe cooled, it was trapped in a false vacuum with a high energy density, which is much like a cosmological constant. As the very early universe cooled it was trapped in a metastable state (it was supercooled), which it could only decay out of through the process of bubble nucleation via quantum tunneling. Bubbles of true vacuum spontaneously form in the sea of false vacuum and rapidly begin expanding at the speed of light. Guth recognized that this model was problematic because the model did not reheat properly: when the bubbles nucleated, they did not generate any radiation. Radiation could only be generated in collisions between bubble walls. But if inflation lasted long enough to solve the initial conditions problems, collisions between bubbles became exceedingly rare. In any one causal patch it is likely that only one bubble will nucleate.
Slow-roll inflation.
The bubble collision problem was solved by Andrei Linde and independently by Andreas Albrecht and Paul Steinhardt in a model named "new inflation" or "slow-roll inflation" (Guth's model then became known as "old inflation"). In this model, instead of tunneling out of a false vacuum state, inflation occurred by a scalar field rolling down a potential energy hill. When the field rolls very slowly compared to the expansion of the universe, inflation occurs. However, when the hill becomes steeper, inflation ends and reheating can occur.
Effects of asymmetries.
Eventually, it was shown that new inflation does not produce a perfectly symmetric universe, but that tiny quantum fluctuations in the inflaton are created. These tiny fluctuations form the primordial seeds for all structure created in the later universe. These fluctuations were first calculated by Viatcheslav Mukhanov and G. V. Chibisov in the Soviet Union in analyzing Starobinsky's similar model. In the context of inflation, they were worked out independently of the work of Mukhanov and Chibisov at the three-week 1982 Nuffield Workshop on the Very Early Universe at Cambridge University. The fluctuations were calculated by four groups working separately over the course of the workshop: Stephen Hawking; Starobinsky; Guth and So-Young Pi; and James M. Bardeen, Paul Steinhardt and Michael Turner.
Observational status.
Inflation is a mechanism for realizing the cosmological principle, which is the basis of the standard model of physical cosmology: it accounts for the homogeneity and isotropy of the observable universe. In addition, it accounts for the observed flatness and absence of magnetic monopoles. Since Guth's early work, each of these observations has received further confirmation, most impressively by the detailed observations of the cosmic microwave background made by the Wilkinson Microwave Anisotropy Probe (WMAP) spacecraft. This analysis shows that the universe is flat to an accuracy of at least a few percent, and that it is homogeneous and isotropic to a part in 100,000.
In addition, inflation predicts that the structures visible in the universe today formed through the gravitational collapse of perturbations that were formed as quantum mechanical fluctuations in the inflationary epoch. The detailed form of the spectrum of perturbations called a nearly-scale-invariant Gaussian random field (or Harrison–Zel'dovich spectrum) is very specific and has only two free parameters, the amplitude of the spectrum and the "spectral index", which measures the slight deviation from scale invariance predicted by inflation (perfect scale invariance corresponds to the idealized de Sitter universe). Inflation predicts that the observed perturbations should be in thermal equilibrium with each other (these are called "adiabatic" or "isentropic" perturbations). This structure for the perturbations has been confirmed by the WMAP spacecraft and other cosmic microwave background experiments, and galaxy surveys, especially the ongoing Sloan Digital Sky Survey. These experiments have shown that the one part in 100,000 inhomogeneities observed have exactly the form predicted by theory. Moreover, there is evidence for a slight deviation from scale invariance. The "spectral index", "n"s is equal to one for a scale-invariant spectrum. The simplest models of inflation predict that this quantity is between 0.92 and 0.98. From the data taken by the WMAP spacecraft it can be inferred that "n"s = 0.963 ± 0.012, implying that it differs from one at the level of two standard deviations (2σ). This is considered an important confirmation of the theory of inflation.
A number of theories of inflation have been proposed that make radically different predictions, but they generally have much more fine tuning than is necessary. As a physical model, however, inflation is most valuable in that it robustly predicts the initial conditions of the universe based on only two adjustable parameters: the spectral index (that can only change in a small range) and the amplitude of the perturbations. Except in contrived models, this is true regardless of how inflation is realized in particle physics.
Occasionally, effects are observed that appear to contradict the simplest models of inflation. The first-year WMAP data suggested that the spectrum might not be nearly scale-invariant, but might instead have a slight curvature. However, the third-year data revealed that the effect was a statistical anomaly. Another effect has been remarked upon since the first cosmic microwave background satellite, the Cosmic Background Explorer: the amplitude of the quadrupole moment of the cosmic microwave background is unexpectedly low and the other low multipoles appear to be preferentially aligned with the ecliptic plane. Some have claimed that this is a signature of non-Gaussianity and thus contradicts the simplest models of inflation. Others have suggested that the effect may be due to other new physics, foreground contamination, or even publication bias.
An experimental program is underway to further test inflation with more precise measurements of the cosmic microwave background. In particular, high precision measurements of the so-called "B-modes" of the polarization of the background radiation could provide evidence of the gravitational radiation produced by inflation, and could also show whether the energy scale of inflation predicted by the simplest models (1015–1016 GeV) is correct. In March 2014, it was announced that B-mode polarization of the background radiation consistent with that predicted from inflation had been demonstrated by a South Pole experiment, a collaboration led by four principal investigators from the California Institute of Technology, Harvard University, Stanford University, and the University of Minnesota BICEP2. Other potentially corroborating measurements are expected to be performed by the Planck spacecraft, although it is unclear if the signal will be visible, or if contamination from foreground sources will interfere with these measurements. Other forthcoming measurements, such as those of 21 centimeter radiation (radiation emitted and absorbed from neutral hydrogen before the first stars turned on), may measure the power spectrum with even greater resolution than the cosmic microwave background and galaxy surveys, although it is not known if these measurements will be possible or if interference with radio sources on earth and in the galaxy will be too great.
Dark energy is broadly similar to inflation, and is thought to be causing the expansion of the present-day universe to accelerate. However, the energy scale of dark energy is much lower, 10−12 GeV, roughly 27 orders of magnitude less than the scale of inflation.
Theoretical status.
In the early proposal of Guth, it was thought that the inflaton was the Higgs field, the field that explains the mass of the elementary particles. It is now believed by some that the inflaton cannot be the Higgs field although the recent discovery of the Higgs boson has increased the number of works considering the Higgs field as inflaton. One problem of this identification is the current tension with experimental data at the electroweak scale, which is currently under study at the Large Hadron Collider (LHC). Other models of inflation relied on the properties of grand unified theories. Since the simplest models of grand unification have failed, it is now thought by many physicists that inflation will be included in a supersymmetric theory like string theory or a supersymmetric grand unified theory. At present, while inflation is understood principally by its detailed predictions of the initial conditions for the hot early universe, the particle physics is largely "ad hoc" modelling. As such, though predictions of inflation have been consistent with the results of observational tests, there are many open questions about the theory.
Fine-tuning problem.
One of the most severe challenges for inflation arises from the need for fine tuning in inflationary theories. In new inflation, the "slow-roll conditions" must be satisfied for inflation to occur. The slow-roll conditions say that the inflaton potential must be flat (compared to the large vacuum energy) and that the inflaton particles must have a small mass. In order for the new inflation theory of Linde, Albrecht and Steinhardt to be successful, therefore, it seemed that the universe must have a scalar field with an especially flat potential and special initial conditions. However, there are ways to explain these fine-tunings. For example, classically scale invariant field theories, where scale invariance is broken by quantum effects, provide an explanation of the flatness of inflationary potentials, as long as the theory can be studied through perturbation theory.
Andrei Linde.
Andrei Linde proposed a theory known as "chaotic inflation" in which he suggested that the conditions for inflation are actually satisfied quite generically and inflation will occur in virtually any universe that begins in a chaotic, high energy state and has a scalar field with unbounded potential energy. However, in his model the inflaton field necessarily takes values larger than one Planck unit: for this reason, these are often called "large field" models and the competing new inflation models are called "small field" models. In this situation, the predictions of effective field theory are thought to be invalid, as renormalization should cause large corrections that could prevent inflation. This problem has not yet been resolved and some cosmologists argue that the small field models, in which inflation can occur at a much lower energy scale, are better models of inflation. While inflation depends on quantum field theory (and the semiclassical approximation to quantum gravity) in an important way, it has not been completely reconciled with these theories. The BICEP2 experiment detected evidence for primordial gravitational waves consistent with Linde's model.
Robert Brandenberger has commented on fine-tuning in another situation. The amplitude of the primordial inhomogeneities produced in inflation is directly tied to the energy scale of inflation. There are strong suggestions that this scale is around 1016 GeV or 10−3 times the Planck energy. The natural scale is naïvely the Planck scale so this small value could be seen as another form of fine-tuning (called a hierarchy problem): the energy density given by the scalar potential is down by 10−12 compared to the Planck density. This is not usually considered to be a critical problem, however, because the scale of inflation corresponds naturally to the scale of gauge unification.
Eternal inflation.
In many models of inflation, the inflationary phase of the universe's expansion lasts forever in at least some regions of the universe. This occurs because inflating regions expand very rapidly, reproducing themselves. Unless the rate of decay to the non-inflating phase is sufficiently fast, new inflating regions are produced more rapidly than non-inflating regions. In such models most of the volume of the universe at any given time is inflating. All models of eternal inflation produce an infinite multiverse, typically a fractal.
Although new inflation is classically rolling down the potential, quantum fluctuations can sometimes bring it back up to previous levels. These regions in which the inflaton fluctuates upwards expand much faster than regions in which the inflaton has a lower potential energy, and tend to dominate in terms of physical volume. This steady state, which first developed by Vilenkin, is called "eternal inflation". It has been shown that any inflationary theory with an unbounded potential is eternal. It is a popular conclusion among physicists that this steady state cannot continue forever into the past. The inflationary spacetime, which is similar to de Sitter space, is incomplete without a contracting region. However, unlike de Sitter space, fluctuations in a contracting inflationary space will collapse to form a gravitational singularity, a point where densities become infinite. Therefore, it is necessary to have a theory for the universe's initial conditions. Linde, however, believes inflation may be past eternal.
In eternal inflation, regions with inflation have an exponentially growing volume, while regions that are not inflating don't. This suggests that the volume of the inflating part of the universe in the global picture is always unimaginably larger than the part that has stopped inflating, even though inflation eventually ends as seen by any single pre-inflationary observer. Scientists disagree about how to assign a probability distribution to this hypothetical anthropic landscape. If the probability of different regions is counted by volume, one should expect that inflation will never end, or applying boundary conditions that a local observer exists to observe it, that inflation will end as late as possible. Some physicists believe this paradox can be resolved by weighting observers by their pre-inflationary volume.
Initial conditions.
Some physicists have tried to avoid the initial conditions problem by proposing models for an eternally inflating universe with no origin. These models propose that while the universe, on the largest scales, expands exponentially it was, is and always will be, spatially infinite and has existed, and will exist, forever.
Other proposals attempt to describe the ex nihilo creation of the universe based on quantum cosmology and the following inflation. Vilenkin put forth one such scenario. Hartle and Hawking offered the no-boundary proposal for the initial creation of the universe in which inflation comes about naturally.
Alan Guth has described the inflationary universe as the "ultimate free lunch": new universes, similar to our own, are continually produced in a vast inflating background. Gravitational interactions, in this case, circumvent (but do not violate) the first law of thermodynamics (energy conservation) and the second law of thermodynamics (entropy and the arrow of time problem). However, while there is consensus that this solves the initial conditions problem, some have disputed this, as it is much more likely that the universe came about by a quantum fluctuation. Donald Page was an outspoken critic of inflation because of this anomaly. He stressed that the thermodynamic arrow of time necessitates low entropy initial conditions, which would be highly unlikely. According to them, rather than solving this problem, the inflation theory further aggravates it – the reheating at the end of the inflation era increases entropy, making it necessary for the initial state of the Universe to be even more orderly than in other Big Bang theories with no inflation phase.
Hawking and Page later found ambiguous results when they attempted to compute the probability of inflation in the Hartle-Hawking initial state. Other authors have argued that, since inflation is eternal, the probability doesn't matter as long as it is not precisely zero: once it starts, inflation perpetuates itself and quickly dominates the universe. However, Albrecht and Lorenzo Sorbo have argued that the probability of an inflationary cosmos, consistent with today's observations, emerging by a random fluctuation from some pre-existent state, "compared" with a non-inflationary cosmos overwhelmingly favours the inflationary scenario, simply because the "seed" amount of non-gravitational energy required for the inflationary cosmos is so much less than any required for a non-inflationary alternative, which outweighs any entropic considerations.
Another problem that has occasionally been mentioned is the trans-Planckian problem or trans-Planckian effects. Since the energy scale of inflation and the Planck scale are relatively close, some of the quantum fluctuations that have made up the structure in our universe were smaller than the Planck length before inflation. Therefore, there ought to be corrections from Planck-scale physics, in particular the unknown quantum theory of gravity. There has been some disagreement about the magnitude of this effect: about whether it is just on the threshold of detectability or completely undetectable.
Hybrid inflation.
Another kind of inflation, called "hybrid inflation", is an extension of new inflation. It introduces additional scalar fields, so that while one of the scalar fields is responsible for normal slow roll inflation, another triggers the end of inflation: when inflation has continued for sufficiently long, it becomes favorable to the second field to decay into a much lower energy state.
In hybrid inflation, one of the scalar fields is responsible for most of the energy density (thus determining the rate of expansion), while the other is responsible for the slow roll (thus determining the period of inflation and its termination). Thus fluctuations in the former inflaton would not affect inflation termination, while fluctuations in the latter would not affect the rate of expansion. Therefore hybrid inflation is not eternal. When the second (slow-rolling) inflaton reaches the bottom of its potential, it changes the location of the minimum of the first inflaton's potential, which leads to a fast roll of the inflaton down its potential, leading to termination of inflation.
Inflation and string cosmology.
The discovery of flux compactifications have opened the way for reconciling inflation and string theory. A new theory, called "brane inflation" suggests that inflation arises from the motion of D-branes in the compactified geometry, usually towards a stack of anti-D-branes. This theory, governed by the "Dirac-Born-Infeld action", is very different from ordinary inflation. The dynamics are not completely understood. It appears that special conditions are necessary since inflation occurs in tunneling between two vacua in the string landscape. The process of tunneling between two vacua is a form of old inflation, but new inflation must then occur by some other mechanism.
Inflation and loop quantum gravity.
When investigating the effects the theory of loop quantum gravity would have on cosmology, a loop quantum cosmology model has evolved that provides a possible mechanism for cosmological inflation. Loop quantum gravity assumes a quantized spacetime. If the energy density is larger than can be held by the quantized spacetime, it is thought to bounce back. 
Inflation and generalized uncertainty principle (GUP).
The effects of generalized uncertainty principle (GUP) on the inflationary dynamics and the thermodynamics of the early Universe are studied. Using the GUP approach, et al. evaluated the tensorial and scalar density fluctuations in the inflation era and compared them with the standard case. They found a good agreement with the Wilkinson Microwave Anisotropy Probe data. Assuming that a quantum gas of scalar particles is confined within a thin layer near the apparent horizon of the Friedmann-Lemaitre-Robertson-Walker Universe that satisfies the boundary condition, et al. calculated the number and entropy densities and the free energy arising from the quantum states using the GUP approach. Furthermore, a qualitative estimation for effects of the quantum gravity on all these thermodynamic quantities was introduced.
Alternatives to inflation.
The flatness and horizon problems are naturally solved in the Einstein-Cartan-Sciama-Kibble theory of gravity, without needing an exotic form of matter and introducing free parameters. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. The minimal coupling between torsion and Dirac spinors generates a spin-spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction averts the unphysical Big Bang singularity, replacing it with a cusp-like bounce at a finite minimum scale factor, before which the Universe was contracting. The rapid expansion immediately after the Big Bounce explains why the present Universe at largest scales appears spatially flat, homogeneous and isotropic. As the density of the Universe decreases, the effects of torsion weaken and the Universe smoothly enters the radiation-dominated era.
There are models that explain some of the observations explained by inflation. However none of these "alternatives" has the same breadth of explanation as inflation, and still require inflation for a more complete fit with observation; they should therefore be regarded as adjuncts to inflation, rather than as alternatives.
String theory requires that, in addition to the three observable spatial dimensions, there exist additional dimensions that are curled up or compactified (see also Kaluza–Klein theory). Extra dimensions appear as a frequent component of supergravity models and other approaches to quantum gravity. This raised the contingent question of why four space-time dimensions became large and the rest became unobservably small. An attempt to address this question, called "string gas cosmology", was proposed by Robert Brandenberger and Cumrun Vafa. This model focuses on the dynamics of the early universe considered as a hot gas of strings. Brandenberger and Vafa show that a dimension of spacetime can only expand if the strings that wind around it can efficiently annihilate each other. Each string is a one-dimensional object, and the largest number of dimensions in which two strings will generically intersect (and, presumably, annihilate) is three. Therefore, one argues that the most likely number of non-compact (large) spatial dimensions is three. Current work on this model centers on whether it can succeed in stabilizing the size of the compactified dimensions and produce the correct spectrum of primordial density perturbations. For a recent review, see The authors admits that their model "does not solve the entropy and flatness problems of standard cosmology ... and we can provide no explanation for why the current universe is so close to being spatially flat".
The ekpyrotic and cyclic models are also considered adjuncts to inflation. These models solve the horizon problem through an expanding epoch well "before" the Big Bang, and then generate the required spectrum of primordial density perturbations during a contracting phase leading to a Big Crunch. The universe passes through the Big Crunch and emerges in a hot Big Bang phase. In this sense they are reminiscent of the oscillatory universe proposed by Richard Chace Tolman: however in Tolman's model the total age of the universe is necessarily finite, while in these models this is not necessarily so. Whether the correct spectrum of density fluctuations can be produced, and whether the universe can successfully navigate the Big Bang/Big Crunch transition, remains a topic of controversy and current research. Ekpyrotic models avoid the magnetic monopole problem as long as the temperature at the Big Crunch/Big Bang transition remains below the Grand Unified Scale, as this is the temperature required to produce magnetic monopoles in the first place. As things stand, there is no evidence of any 'slowing down' of the expansion, but this is not surprising as each cycle is expected to last on the order of a trillion years.
Another adjunct, the varying speed of light model has also been theorized by Jean-Pierre Petit in 1988, John Moffat in 1992 as well Andreas Albrecht and João Magueijo in 1999, instead of superluminal expansion the speed of light was 60 orders of magnitude faster than its current value solving the horizon and homogeneity problems in the early universe.
Criticisms.
Since its introduction by Alan Guth in 1980, the inflationary paradigm has become widely accepted. Nevertheless, many physicists, mathematicians, and philosophers of science have voiced criticisms, claiming untestable predictions and an alleged lack of serious empirical support. In 1999, John Earman and Jesús Mosterín published a thorough critical review of inflationary cosmology, concluding, "we do not think that there are, as yet, good grounds for admitting any of the models of inflation into the standard core of cosmology."
In order to work, and as pointed out by Roger Penrose from 1986 on, inflation requires extremely specific initial conditions of its own, so that the problem (or pseudo-problem) of initial conditions is not solved: "There is something fundamentally misconceived about trying to explain the uniformity of the early universe as resulting from a thermalization process. [...] For, if the thermalization is actually doing anything [...] then it represents a definite increasing of the entropy. Thus, the universe would have been even more special before the thermalization than after." The problem of specific or "fine-tuned" initial conditions would not have been solved; it would have gotten worse.
A recurrent criticism of inflation is that the invoked inflation field does not correspond to any known physical field, and that its potential energy curve seems to be an ad hoc contrivance to accommodate almost any data obtainable. Paul J. Steinhardt, one of the founding fathers of inflationary cosmology, has recently become one of its sharpest critics. He calls 'bad inflation' a period of accelerated expansion whose outcome conflicts with observations, and 'good inflation' one compatible with them: "Not only is bad inflation more likely than good inflation, but no inflation is more likely than either... Roger Penrose considered all the possible configurations of the inflaton and gravitational fields. Some of these configurations lead to inflation ... Other configurations lead to a uniform, flat universe directly – without inflation. Obtaining a flat universe is unlikely overall. Penrose's shocking conclusion, though, was that obtaining a flat universe without inflation is much more likely than with inflation – by a factor of 10 to the googol (10 to the 100) power!"

</doc>
<doc id="5385" url="http://en.wikipedia.org/wiki?curid=5385" title="Candela">
Candela

The candela ( or ; symbol: cd) is the SI base unit of luminous intensity; that is, power emitted by a light source in a particular direction, weighted by the luminosity function (a standardized model of the sensitivity of the human eye to different wavelengths, also known as the "luminous efficiency function"). A common candle emits light with a luminous intensity of roughly one candela. If emission in some directions is blocked by an opaque barrier, the emission would still be approximately one candela in the directions that are not obscured.
The word "candela" means "candle" in Latin, as well as in some Romance languages.
Definition.
Like most other SI base units, the candela has an operational definition—it is defined by a description of a physical process that will produce one candela of luminous intensity. Since the 16th General Conference on Weights and Measures (CGPM) in 1979, the candela has been defined as:
The candela is the luminous intensity, in a given direction, of a source that emits monochromatic radiation of frequency 540 hertz and that has a radiant intensity in that direction of  watt per steradian.
The definition describes how to produce a light source that (by definition) emits one candela. Such a source could then be used to calibrate instruments designed to measure luminous intensity.
The candela is sometimes still called by the old name "candle", such as in "foot-candle" and the modern definition of "candlepower".
Explanation.
The frequency chosen is in the visible spectrum near green, corresponding to a wavelength of about 555 nanometers. The human eye is most sensitive to this frequency, when adapted for bright conditions. At other frequencies, more radiant intensity is required to achieve the same luminous intensity, according to the frequency response of the human eye. The luminous intensity for light of a particular wavelength λ is given by
where "I"v(λ) is the luminous intensity in candelas, "I"e(λ) is the radiant intensity in W/sr and formula_2 is the standard luminosity function. If more than one wavelength is present (as is usually the case), one must sum or integrate over the spectrum of wavelengths present to get the total luminous intensity.
Examples.
A common candle emits light with roughly 1 cd luminous intensity. A 25 W compact fluorescent light bulb puts out around 1700 lumens; if that light is radiated equally in all directions, it will have an intensity of around 135 cd. Focused into a 20° beam, it will have an intensity of around .
The luminous intensity of light-emitting diodes is measured in millicandela (mcd), or thousandths of a candela. Indicator LEDs are typically in the 50 mcd range; "ultra-bright" LEDs can reach (15 cd), or higher.
Origin.
Prior to 1948, various standards for luminous intensity were in use in a number of countries. These were typically based on the brightness of the flame from a "standard candle" of defined composition, or the brightness of an incandescent filament of specific design. One of the best-known of these was the English standard of candlepower. One candlepower was the light produced by a pure spermaceti candle weighing one sixth of a pound and burning at a rate of 120 grains per hour. Germany, Austria and Scandinavia used the Hefnerkerze, a unit based on the output of a Hefner lamp.
It became clear that a better-defined unit was needed. The Commission Internationale de l'Éclairage (International Commission on Illumination) and the CIPM proposed a “new candle” based on the luminance of a Planck radiator (a black body) at the temperature of freezing platinum. The value of the new unit was chosen to make it similar to the earlier unit candlepower. The decision was promulgated by the CIPM in 1946:
The value of the new candle is such that the brightness of the full radiator at the temperature of solidification of platinum is 60 new candles per square centimetre.
It was then ratified in 1948 by the 9th CGPM which adopted a new name for this unit, the "candela". In 1967 the 13th CGPM removed the term "new candle" and gave an amended version of the candela definition, specifying the atmospheric pressure applied to the freezing platinum:
The candela is the luminous intensity, in the perpendicular direction, of a surface of square metre of a black body at the temperature of freezing platinum under a pressure of  newtons per square metre.
In 1979, because of the difficulties in realizing a Planck radiator at high temperatures and the new possibilities offered by radiometry, the 16th CGPM adopted the modern definition of the candela. The arbitrary (1/683) term was chosen so that the new definition would exactly match the old definition. Although the candela is now defined in terms of the second (an SI base unit) and the watt (a derived SI unit), the candela remains a base unit of the SI system, by definition.
SI photometric light units.
Relationship between luminous intensity and luminous flux.
If a source emits a known luminous intensity "I"v (in candelas) in a well-defined cone, the total luminous flux "Φ"v in lumens is given by
where "A" is the "radiation angle" of the lamp—the full vertex angle of the emission cone. For example, a lamp that emits 590 cd with a radiation angle of 40° emits about 224 lumens. See MR16 for emission angles of some common lamps.
If the source emits light uniformly in all directions, the flux can be found by multiplying the intensity by 4π: a uniform 1 candela source emits 12.6 lumens.

</doc>
<doc id="5387" url="http://en.wikipedia.org/wiki?curid=5387" title="Condensed matter physics">
Condensed matter physics

Condensed matter physics is a branch of physics that deals with the physical properties of condensed phases of matter. Condensed matter physicists seek to understand the behavior of these phases by using physical laws. In particular, these include the laws of quantum mechanics, electromagnetism and statistical mechanics.
The most familiar condensed phases are solids and liquids, while more exotic condensed phases include the superconducting phase exhibited by certain materials at low temperature, the ferromagnetic and antiferromagnetic phases of spins on atomic lattices, and the Bose–Einstein condensate found in cold atomic systems. The study of condensed matter physics involves measuring various material properties via experimental probes along with using techniques of theoretical physics to develop mathematical models that help in understanding physical behavior.
The diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists identify themselves as condensed matter physicists, and The Division of Condensed Matter Physics (DCMP) is the largest division of the American Physical Society. The field overlaps with chemistry, materials science, and nanotechnology, and relates closely to atomic physics and biophysics. Theoretical condensed matter physics shares important concepts and techniques with theoretical particle and nuclear physics.
A variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas, until the 1940s when they were grouped together as "Solid state physics". Around the 1960s, the study of physical properties of liquids was added to this list, and it came to be known as condensed matter physics. According to physicist Phil Anderson, the term was coined by him and Volker Heine when they changed the name of their group at the Cavendish Laboratories, Cambridge from "Solid state theory" to "Theory of Condensed Matter", as they felt it did not exclude their interests in the study of liquids, nuclear matter and so on. The Bell Labs (then known as the "Bell Telephone Laboratories") was one of the first institutes to conduct a research program in condensed matter physics.
References to "condensed" state can be traced to earlier sources. For example, in the introduction to his 1947 "Kinetic theory of liquids" book, Yakov Frenkel proposed that "The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies. As a matter of fact, it would be more correct to unify them under the title of "condensed bodies".
History.
Classical physics.
One of the first studies of condensed states of matter was by English chemist Humphry Davy, in the first decades of the 19th century. Davy observed that of the 40 chemical elements known at the time, 26 had metallic properties such as lustre, ductility and high electrical and thermal conductivity. This indicated that the atoms in Dalton's atomic theory were not indivisible as Dalton claimed, but had inner structure. Davy further claimed that elements that were then believed to be gases, such as nitrogen and hydrogen could be liquefied under the right conditions and would then behave as metals.
In 1823, Michael Faraday, then an assistant in Davy's lab, successfully liquefied chlorine and went on to liquefy all known gaseous elements, with the exception of nitrogen, hydrogen and oxygen. Shortly after, in 1869, Irish chemist Thomas Andrews studied the phase transition from a liquid to a gas and coined the term critical point to describe the condition where a gas and a liquid were indistinguishable as phases, and Dutch physicist Johannes van der Waals supplied the theoretical framework which allowed the prediction of critical behavior based on measurements at much higher temperatures. By 1908, James Dewar and H. Kamerlingh Onnes were successfully able to liquefy hydrogen and then newly discovered helium, respectively.
Paul Drude proposed the first theoretical model for a classical electron moving through a metallic solid. Drude's model described properties of metals in terms of a gas of free electrons, and was the first microscopic model to explain empirical observations such as the Wiedemann–Franz law. However, despite the success of Drude's free electron model, it had one notable problem, in that it was unable to correctly explain the electronic contribution to the specific heat of metals, as well as the temperature dependence of resistivity at low temperatures. 
In 1911, three years after helium was first liquefied, Onnes working at University of Leiden discovered superconductivity in mercury, when he observed the electrical resistivity of mercury to vanish at temperatures below a certain value. The phenomenon completely surprised the best theoretical physicists of the time, and it remained unexplained for several decades. Albert Einstein, in 1922, said regarding contemporary theories of superconductivity that “with our far-reaching ignorance of the quantum mechanics of composite systems we are very far from being able to compose a theory out of these vague ideas”.
Advent of quantum mechanics.
Drude's classical model was augmented by Felix Bloch, Arnold Sommerfeld, and independently by Wolfgang Pauli, who used quantum mechanics to describe the motion of a quantum electron in a periodic lattice. In particular, Sommerfeld's theory accounted for the Fermi–Dirac statistics satisfied by electrons and was better able to explain the heat capacity and resistivity. The structure of crystalline solids was studied by Max von Laue and Paul Knipping, when they observed the X-ray diffraction pattern of crystals, and concluded that crystals get their structure from periodic lattices of atoms. The mathematics of crystal structures developed by Auguste Bravais, Yevgraf Fyodorov and others was used to classify crystals by their symmetry group, and tables of crystal structures were the basis for the series "International Tables of Crystallography", first published in 1935. Band structure calculations was first used in 1930 to predict the properties of new materials, and in 1947 John Bardeen, Walter Brattain and William Shockley developed the first semiconductor-based transistor, heralding a revolution in electronics.
In 1879, Edwin Herbert Hall working at the Johns Hopkins University discovered the development of a voltage across conductors transverse to an electric current in the conductor and magnetic field perpendicular to the current. This phenomenon arising due to the nature of charge carriers in the conductor came to be known as the Hall effect, but it was not properly explained at the time, since the electron was experimentally discovered 18 years later. After the advent of quantum mechanics, Lev Landau in 1930 predicted the quantization of the Hall conductance for electrons confined to two dimensions.
Magnetism as a property of matter has been known since pre-historic times. However, the first modern studies of magnetism only started with the development of electrodynamics by Faraday, Maxwell and others in the nineteenth century, which included the classification of materials as ferromagnetic, paramagnetic and diamagnetic based on their response to magnetization. Pierre Curie studied the dependence of magnetization on temperature and discovered the Curie point phase transition in ferromagnetic materials. In 1906, Pierre Weiss introduced the concept of magnetic domains to explain the main properties of ferromagnets. The first attempt at a microscopic description of magnetism was by Wilhelm Lenz and Ernst Ising through the Ising model that described magnetic materials as consisting of a periodic lattice of spins that collectively acquired magnetization. The Ising model was solved exactly to show that spontaneous magnetization cannot occur in one dimension but is possible in higher-dimensional lattices. Further research such as by Bloch on spin waves and Néel on antiferromagnetism led to the development of new magnetic materials with applications to magnetic storage devices.
Modern many-body physics.
The Sommerfeld model and spin models for ferromagnetism illustrated the successful application of quantum mechanics to condensed matter problems in the 1930s. However, there still were several unsolved problems, most notably the description of superconductivity and the Kondo effect. After World War II, several ideas from quantum field theory were applied to condensed matter problems. These included recognition of collective modes of excitation of solids and the important notion of a quasiparticle. Russian physicist Lev Landau used the idea for the Fermi liquid theory wherein low energy properties of interacting fermion systems were given in terms of what are now known as Landau-quasiparticles. Landau also developed a mean field theory for continuous phase transitions, which described ordered phases as spontaneous breakdown of symmetry. The theory also introduced the notion of an order parameter to distinguish between ordered phases. Eventually in 1965, John Bardeen, Leon Cooper and John Schrieffer developed the so-called BCS theory of superconductivity, based on the discovery that arbitrarily small attraction between two electrons can give rise to a bound state called a Cooper pair.
The study of phase transition and the critical behavior of observables, known as critical phenomena, was a major field of interest in the 1960s. Leo Kadanoff, Benjamin Widom and Michael Fisher developed the ideas of critical exponents and scaling. These ideas were unified by Kenneth Wilson in 1972, under the formalism of the renormalization group in the context of quantum field theory.
The quantum Hall effect was discovered by Klaus von Klitzing in 1980 when he observed the Hall conductivity to be integer multiples of a fundamental constant. (see figure) The effect was observed to be independent of parameters such as the system size and impurities, and in 1981, theorist Robert Laughlin proposed a theory describing the integer states in terms of a topological invariant called the Chern number. Shortly after, in 1982, Horst Störmer and Daniel Tsui observed the fractional quantum Hall effect where the conductivity was now a rational multiple of a constant. Laughlin, in 1983, realized that this was a consequence of quasiparticle interaction in the Hall states and formulated a variational solution, known as the Laughlin wavefunction. The study of topological properties of the fractional Hall effect remains an active field of research.
In 1987, Karl Müller and Johannes Bednorz discovered the first high temperature superconductor, a material which was superconducting at temperatures as high as 50 Kelvin. It was realized that the high temperature superconductors are examples of strongly correlated materials where the electron–electron interactions play an important role. A satisfactory theoretical description of high-temperature superconductors is still not known and the field of strongly correlated materials continues to be an active research topic.
In 2009, David Field and researchers at Aarhus University discovered spontaneous electric fields when creating prosaic films of various gases. This has more recently expanded to form the research area of spontelectrics.
Theoretical.
Theoretical condensed matter physics involves the use of theoretical models to understand properties of states of matter. These include models to study the electronic properties of solids, such as the Drude model, the Band structure and the density functional theory. Theoretical models have also been developed to study the physics of phase transitions, such as the Ginzburg–Landau theory, critical exponents and the use of mathematical techniques of quantum field theory and the renormalization group. Modern theoretical studies involve the use of numerical computation of electronic structure and mathematical tools to understand phenomena such as high-temperature superconductivity, topological phases and gauge symmetries.
Emergence.
Theoretical understanding of condensed matter physics is closely related to the notion of emergence, wherein complex assemblies of particles behave in ways dramatically different from their individual constituents. For example, a range of phenomena related to high temperature superconductivity are not well understood, although the microscopic physics of individual electrons and lattices is well known. Similarly, models of condensed matter systems have been studied where collective excitations behave like photons and electrons, thereby describing electromagnetism as an emergent phenomenon. Emergent properties can also occur at the interface between materials: one example is the lanthanum-aluminate-strontium-titanate interface, where two non-magnetic insulators are joined to create conductivity, superconductivity, and ferromagnetism.
Electronic theory of solids.
The metallic state has historically been an important building block for studying properties of solids.<ref name=ashcroft/mermin></ref> The first theoretical description of metals was given by Paul Drude in 1900 with the Drude model, which explained electrical and thermal properties by describing a metal as an ideal gas of then-newly discovered electrons. This classical model was then improved by Arnold Sommerfeld who incorporated the Fermi–Dirac statistics of electrons and was able to explain the anomalous behavior of the specific heat of metals in the Wiedemann–Franz law. In 1913, X-ray diffraction experiments revealed that metals possess periodic lattice structure. Swiss physicist Felix Bloch provided a wave function solution to the Schrödinger equation with a periodic potential, called the Bloch wave.
Calculating electronic properties of metals by solving the many-body wavefunction is often computationally hard, and hence, approximation techniques are necessary to obtain meaningful predictions. The Thomas–Fermi theory, developed in the 1920s, was used to estimate electronic energy levels by treating the local electron density as a variational parameter. Later in the 1930s, Douglas Hartree, Vladimir Fock and John Slater developed the so-called Hartree–Fock wavefunction as an improvement over the Thomas–Fermi model. The Hartree–Fock method accounted for exchange statistics of single particle electron wavefunctions, but not for their Coulomb interaction. Finally in 1964–65, Walter Kohn, Pierre Hohenberg and Lu Jeu Sham proposed the density functional theory which gave realistic descriptions for bulk and surface properties of metals. The density functional theory (DFT) has been widely used since the 1970s for band structure calculations of variety of solids.
Symmetry breaking.
Certain states of matter exhibit symmetry breaking, where the relevant laws of physics possess some symmetry that is broken. A common example is crystalline solids, which break continuous translational symmetry. Other examples include magnetized ferromagnets, which break rotational symmetry, and more exotic states such as the ground state of a BCS superconductor, that breaks U(1) rotational symmetry.
Goldstone's theorem in quantum field theory states that in a system with broken continuous symmetry, there may exist excitations with arbitrarily low energy, called the Goldstone bosons. For example, in crystalline solids, these correspond to phonons, which are quantized versions of lattice vibrations.
Phase transition.
The study of critical phenomena and phase transitions is an important part of modern condensed matter physics. Phase transition refers to the change of phase of a system, which is brought about by change in an external parameter such as temperature. In particular, quantum phase transitions refer to transitions where the temperature is set to zero, and the phases of the system refer to distinct ground states of the Hamiltonian. Systems undergoing phase transition display critical behavior, wherein several of their properties such as correlation length, specific heat and susceptibility diverge. Continuous phase transitions are described by the Ginzburg–Landau theory, which works in the so-called mean field approximation. However, several important phase transitions, such as the Mott insulator–superfluid transition, are known that do not follow the Ginzburg–Landau paradigm. The study of phase transitions in strongly correlated systems is an active area of research.
Experimental.
Experimental condensed matter physics involves the use of experimental probes to try to discover new properties of materials. Experimental probes include effects of electric and magnetic fields, measurement of response functions, transport properties and thermometry. Commonly used experimental techniques include spectroscopy, with probes such as X-rays, infrared light and inelastic neutron scattering; study of thermal response, such as specific heat and measurement of transport via thermal and heat conduction.
Scattering.
Several condensed matter experiments involve scattering of an experimental probe, such as X-ray, optical photons, neutrons, etc., on constituents of a material. The choice of scattering probe depends on the observation energy scale of interest. Visible light has energy on the scale of 1 eV and is used as a scattering probe to measure variations in material properties such as dielectric constant and refractive index. X-rays have energies of the order of 10 keV and hence are able to probe atomic length scales, and are used to measure variations in electron charge density. Neutrons can also probe atomic length scales and are used to study scattering off nuclei and electron spins and magnetization (as neutrons themselves have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes, and similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is used as a tool for studying phenomena with energy in the range of visible light, for example, to study non-linear optics and forbidden transitions in media.
External magnetic fields.
In experimental condensed matter physics, external magnetic fields act as thermodynamic variables that control the state, phase transitions and properties of material systems. Nuclear magnetic resonance (NMR) is a technique by which external magnetic fields can be used to find resonance modes of individual electrons, thus giving information about the atomic, molecular and bond structure of their neighborhood. NMR experiments can be made in magnetic fields with strengths up to 65 Tesla. Quantum oscillations is another experimental technique where high magnetic fields are used to study material properties such as the geometry of the Fermi surface. The quantum hall effect is another example of measurements with high magnetic fields where topological properties such as Chern–Simons angle can be measured experimentally.
Cold atomic gases.
Cold atom trapping in optical lattices is an experimental tool commonly used in condensed matter as well as atomic, molecular, and optical physics. The technique involves using optical lasers to create an interference pattern, which acts as a "lattice", in which ions or atoms can be placed at very low temperatures. Cold atoms in optical lattices are used as "quantum simulators", that is, they act as controllable systems that can model behavior of more complicated systems, such as frustrated magnets. In particular, they are used to engineer one-, two- and three-dimensional lattices for a Hubbard model with pre-specified parameters. and to study phase transitions for Néel and spin liquid ordering.
In 1995, a gas of rubidium atoms cooled down to a temperature of 170 nK was used to experimentally realize the Bose–Einstein condensate, a novel state of matter originally predicted by S. N. Bose and Albert Einstein, wherein a large number of atoms occupy a single quantum state.
Applications.
Research in condensed matter physics has given rise to several device applications, such as the development of the semiconductor transistor, and laser technology. Several phenomena studied in the context of nanotechnology come under the purview of condensed matter physics. Techniques such as scanning-tunneling microscopy can be used to control processes at the nanometer scale, and have given rise to the study of nanofabrication. Several condensed matter systems are being studied with potential applications to quantum computation, including experimental systems like quantum dots, SQUIDs, and theoretical models like the toric code and the quantum dimer model. Condensed matter systems can be tuned to provide the conditions of coherence and phase-sensitivity that are essential ingredients for quantum information storage. Spintronics is a new area of technology that can be used for information processing and transmission, and is based on spin, rather than electron transport. Condensed matter physics also has important applications to biophysics, for example, the experimental technique of magnetic resonance imaging, which is widely used in medical diagnosis.

</doc>
<doc id="5388" url="http://en.wikipedia.org/wiki?curid=5388" title="Cultural anthropology">
Cultural anthropology

Cultural anthropology is a branch of anthropology focused on the study of cultural variation among humans and is in contrast to social anthropology which perceives cultural variation as a subset of the anthropological constant. 
A variety of methods are part of anthropological methodology, including participant observation (often called fieldwork because it involves the anthropologist spending an extended period of time at the research location), interviews, and surveys.
One of the earliest articulations of the anthropological meaning of the term "culture" came from Sir Edward Tylor who writes on the first page of his 1897 book: "Culture, or civilization, taken in its broad, ethnographic sense, is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society." The term "civilization" later gave way to definitions by V. Gordon Childe, with culture forming an umbrella term and civilization becoming a particular kind of culture.
The anthropological concept of "culture" reflects in part a reaction against earlier Western discourses based on an opposition between "culture" and "nature", according to which some human beings lived in a "state of nature". Anthropologists have argued that culture "is" "human nature", and that all people have a capacity to classify experiences, encode classifications symbolically (i.e. in language), and teach such abstractions to others.
Since humans acquire culture through the learning processes of enculturation and socialization, people living in different places or different circumstances develop different cultures. Anthropologists have also pointed out that through culture people can adapt to their environment in non-genetic ways, so people living in different environments will often have different cultures. Much of anthropological theory has originated in an appreciation of and interest in the tension between the local (particular cultures) and the global (a universal human nature, or the web of connections between people in distinct places/circumstances).
The rise of cultural anthropology occurred within the context of the late 19th century, when questions regarding which cultures were "primitive" and which were "civilized" occupied the minds of not only Marx and Freud, but many others. Colonialism and its processes increasingly brought European thinkers in contact, directly or indirectly with "primitive others." The relative status of various humans, some of whom had modern advanced technologies that included engines and telegraphs, while others lacked anything but face-to-face communication techniques and still lived a Paleolithic lifestyle, was of interest to the first generation of cultural anthropologists.
Parallel with the rise of cultural anthropology in the United States, social anthropology, in which "sociality" is the central concept and which focuses on the study of social statuses and roles, groups, institutions, and the relations among them—developed as an academic discipline in Britain and in France. An umbrella term socio-cultural anthropology makes reference to both cultural and social anthropology traditions.
Theoretical foundations.
The critique of evolutionism.
Anthropology is concerned with the lives of people within different parts of the world, particularly in relation to the discourse of beliefs and practices. In addressing this question, ethnologists in the 19th century divided into two schools of thought. Some, like Grafton Elliot Smith, argued that different groups must somehow have learned from one another, however indirectly; in other words, they argued that cultural traits spread from one place to another, or "diffused".
Other ethnologists argued that different groups had the capability of creating similar beliefs and practices independently. Some of those who advocated "independent invention", like Lewis Henry Morgan, additionally supposed that similarities meant that different groups had passed through the same stages of cultural evolution (See also classical social evolutionism). Morgan, in particular, acknowledged that certain forms of society and culture could not possibly have arisen before others. For example, industrial farming could not have been invented before simple farming, and metallurgy could not have developed without previous non-smelting processes involving metals (such as simple ground collection or mining). Morgan, like other 19th century social evolutionists, believed there was a more or less orderly progression from the primitive to the civilized.
20th-century anthropologists largely reject the notion that all human societies must pass through the same stages in the same order, on the grounds that such a notion does not fit the empirical facts. Some 20th-century ethnologists, like Julian Steward, have instead argued that such similarities reflected similar adaptations to similar environments. Although 19th-century ethnologists saw "diffusion" and "independent invention" as mutually exclusive and competing theories, most ethnographers quickly reached a consensus that both processes occur, and that both can plausibly account for cross-cultural similarities. But these ethnographers also pointed out the superficiality of many such similarities. They noted that even traits that spread through diffusion often were given different meanings and function from one society to another. Analyses of large human concentrations in big cities, in multidisciplinary studies by Ronald Daus, show how new methods may be applied to the understanding of man living in a global world and how it was caused by the action of extra-European nations, so high-lighting the role of Ethics in modern anthropology.
Accordingly, most of these anthropologists showed less interest in comparing cultures, generalizing about human nature, or discovering universal laws of cultural development, than in understanding particular cultures in those cultures' own terms. Such ethnographers and their students promoted the idea of "cultural relativism", the view that one can only understand another person's beliefs and behaviors in the context of the culture in which he or she lived or lives.
Others, such as Claude Lévi-Strauss (who was influenced both by American cultural anthropology and by French Durkheimian sociology), have argued that apparently similar patterns of development reflect fundamental similarities in the structure of human thought (see structuralism). By the mid-20th century, the number of examples of people skipping stages, such as going from hunter-gatherers to post-industrial service occupations in one generation, were so numerous that 19th-century evolutionism was effectively disproved.
Cultural relativism.
Cultural relativism is a principle that was established as axiomatic in anthropological research by Franz Boas and later popularized by his students. Boas first articulated the idea in 1887: "...civilization is not something absolute, but ... is relative, and ... our ideas and conceptions are true only so far as our civilization goes." Although, Boas did not coin the term, it became common among anthropologists after Boas' death in 1942, to express their synthesis of a number of ideas Boas had developed. Boas believed that the sweep of cultures, to be found in connection with any sub-species, is so vast and pervasive that there cannot be a relationship between culture and race. Cultural relativism involves specific epistemological and methodological claims. Whether or not these claims require a specific ethical stance is a matter of debate. This principle should not be confused with moral relativism.
Cultural relativism was in part a response to Western ethnocentrism. Ethnocentrism may take obvious forms, in which one consciously believes that one's people's arts are the most beautiful, values the most virtuous, and beliefs the most truthful. Franz Boas, originally trained in physics and geography, and heavily influenced by the thought of Kant, Herder, and von Humboldt, argued that one's culture may mediate and thus limit one's perceptions in less obvious ways. This understanding of culture confronts anthropologists with two problems: first, how to escape the unconscious bonds of one's own culture, which inevitably bias our perceptions of and reactions to the world, and second, how to make sense of an unfamiliar culture. The principle of cultural relativism thus forced anthropologists to develop innovative methods and heuristic strategies.
Foundational thinkers.
Boas and his students realized that if they were to conduct scientific research in other cultures, they would need to employ methods that would help them escape the limits of their own ethnocentrism. One such method is that of ethnography: basically, they advocated living with people of another culture for an extended period of time, so that they could learn the local language and be enculturated, at least partially, into that culture.
In this context, cultural relativism is of fundamental methodological importance, because it calls attention to the importance of the local context in understanding the meaning of particular human beliefs and activities. Thus, in 1948 Virginia Heyer wrote, "Cultural relativity, to phrase it in starkest abstraction, states the relativity of the part to the whole. The part gains its cultural significance by its place in the whole, and cannot retain its integrity in a different situation."
Lewis Henry Morgan.
Lewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from "savagery", to "barbarism", to "civilization". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.
Franz Boas, founder of the modern discipline.
Franz Boas established academic anthropology in the United States in opposition to this sort of evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.
Influenced by the German tradition, Boas argued that the world was full of distinct "cultures," rather than societies whose evolution could be measured by how much or how little "civilization" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.
In doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called "Four Field Approach" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.
Kroeber, Mead and Benedict.
Boas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.
The publication of Alfred Kroeber's textbook, "Anthropology," marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.
Though such works as "Coming of Age in Samoa" and "The Chrysanthemum and the Sword" remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined by Ralph Linton, and Mead was limited to her offices at the AMNH.
Wolf, Sahlins, Mintz and political economy.
In the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche—an approach popularized by Marvin Harris.
Economic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated Lévi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in 1960s and 1970s, including cognitive anthropology and componential analysis.
In keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as "Reinventing Anthropology" worried about anthropology's relevance.
Since the 1980s issues of power, such as those examined in Eric Wolf's "Europe and the People Without History", have been central to the discipline. In the 1980s books like "Anthropology and the Colonial Encounter" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins (again), who drew on Lévi-Strauss and Fernand Braudel to examine the relationship between social structure and individual agency. Jean and John Comaroff produced a whole generation of anthropologists at the University of Chicago that focused on these themes. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.
Geertz, Schneider and interpretive anthropology.
Many anthropologists reacted against the renewed emphasis on materialism and scientific modelling derived from Marx by emphasizing the importance of the concept of culture. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. Geertz was to state:
Geertz's interpretive method involved what he called "thick description." The cultural symbols of rituals, political and economic action, and of kinship, are "read" by the anthropologist as if they are a document in a foreign language. The interpretation of those symbols must be re-framed for their anthropological audience, i.e. transformed from the "experience-near" but foreign concepts of the other culture, into the "experience-distant" theoretical concepts of the anthropologist. These interpretations must then be reflected back to its originators, and its adequacy as a translation fine-tuned in a repeated way, a process called the hermeneutic circle. Geertz applied his method in a number of areas, creating programs of study that were very productive. His analysis of "religion as a cultural system" was particularly influential outside of anthropology. David Schnieder's cultural analysis of American kinship has proven equally influential. Schneider demonstrated that the American folk-cultural emphasis on "blood connections" had an undue influence on anthropological kinship theories, and that kinship is not a biological characteristic but a cultural relationship established on very different terms in different societies.
Prominent British symbolic anthropologists include Victor Turner and Mary Douglas.
The post-modern turn.
In the late 1980s and 1990s authors such as George Marcus and James Clifford pondered ethnographic authority, in particular how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by Feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theorizing and methods became "de rigueur" as part of the 'post-modern moment' in anthropology: Ethnographies became more interpretative and reflexive, explicitly addressing the author's methodology, cultural, gender and racial positioning, and their influence on his or her ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.
Methods.
Modern cultural anthropology has its origins in, and developed in reaction to, 19th century "ethnology", which involves the organized comparison of human societies. Scholars like E.B. Tylor and J.G. Frazer in England worked mostly with materials collected by others – usually missionaries, traders, explorers, or colonial officials – earning them the moniker of "arm-chair anthropologists".
Participant observation.
Participant observation is a widely used methodology in many disciplines, particularly cultural anthropology, less so in sociology, communication studies, and social psychology. Its aim is to gain a close and intimate familiarity with a given group of individuals (such as a religious, occupational, sub cultural group, or a particular community) and their practices through an intensive involvement with people in their cultural environment, usually over an extended period of time. The method originated in the field research of social anthropologists, especially Bronislaw Malinowski in Britain, the students of Franz Boas in the United States, and in the later urban research of the Chicago School of sociology.
Such research involves a range of well-defined, though variable methods: informal interviews, direct observation, participation in the life of the group, collective discussions, analyses of personal documents produced within the group, self-analysis, results from activities undertaken off or online, and life-histories. Although the method is generally characterized as qualitative research, it can (and often does) include quantitative dimensions. Traditional participant observation is usually undertaken over an extended period of time, ranging from several months to many years, and even generations. An extended research time period means that the researcher is able to obtain more detailed and accurate information about the individuals, community, and/or population under study. Observable details (like daily time allotment) and more hidden details (like taboo behavior) are more easily observed and interpreted over a longer period of time. A strength of observation and interaction over extended periods of time is that researchers can discover discrepancies between what participants say—and often believe—should happen (the formal system) and what actually does happen, or between different aspects of the formal system; in contrast, a one-time survey of people's answers to a set of questions might be quite consistent, but is less likely to show conflicts between different aspects of the social system or between conscious representations and behavior.
Ethnography.
In the 20th century, most cultural and social anthropologists turned to the crafting of ethnographies. An ethnography is a piece of writing about a people, at a particular place and time. Typically, the anthropologist lives among people in another society for a period of time, simultaneously participating in and observing the social and cultural life of the group.
Numerous other ethnographic techniques have resulted in ethnographic writing or details being preserved, as cultural anthropologists also curate materials, spend long hours in libraries, churches and schools poring over records, investigate graveyards, and decipher ancient scripts. A typical ethnography will also include information about physical geography, climate and habitat. It is meant to be a holistic piece of writing about the people in question, and today often includes the longest possible timeline of past events that the ethnographer can obtain through primary and secondary research.
Bronisław Malinowski developed the ethnographic method, and Franz Boas taught it in the United States. Boas' students such as Alfred L. Kroeber, Ruth Benedict and Margaret Mead drew on his conception of culture and cultural relativism to develop cultural anthropology in the United States. Simultaneously, Malinowski and A.R. Radcliffe Brown´s students were developing social anthropology in the United Kingdom. Whereas cultural anthropology focused on symbols and values, social anthropology focused on social groups and institutions. Today socio-cultural anthropologists attend to all these elements.
In the early 20th century, socio-cultural anthropology developed in different forms in Europe and in the United States. European "social anthropologists" focused on observed social behaviors and on "social structure", that is, on relationships among social roles (for example, husband and wife, or parent and child) and social institutions (for example, religion, economy, and politics).
American "cultural anthropologists" focused on the ways people expressed their view of themselves and their world, especially in symbolic forms, such as art and myths. These two approaches frequently converged and generally complemented one another. For example, kinship and leadership function both as symbolic systems and as social institutions. Today almost all socio-cultural anthropologists refer to the work of both sets of predecessors, and have an equal interest in what people do and in what people say.
Cross-cultural comparison.
One means by which anthropologists combat ethnocentrism is to engage in the process of cross-cultural comparison. It is important to test so-called "human universals" against the ethnographic record. Monogamy, for example, is frequently touted as a universal human trait, yet comparative study shows that it is not. 
The Human Relations Area Files, Inc. (HRAF) is a research agency based at Yale University. Since 1949, its mission has been to encourage and facilitate worldwide comparative studies of human culture, society, and behavior in the past and present. The name came from the Institute of Human Relations, an interdisciplinary program/building at Yale at the time. The Institute of Human Relations had sponsored HRAF’s precursor, the "Cross-Cultural Survey" (see George Peter Murdock), as part of an effort to develop an integrated science of human behavior and culture. The two eHRAF databases on the Web are expanded and updated annually. "eHRAF World Cultures" includes materials on cultures, past and present, and covers nearly 400 cultures. The second database, "eHRAF Archaeology", covers major archaeological traditions and many more sub-traditions and sites around the world.
Comparison across cultures includies the industrialized (or de-industrialized) West. Cultures in the more traditional standard cross-cultural sample of small scale societies are:
Multi-sited ethnography.
Ethnography dominates socio-cultural anthropology. Nevertheless, many contemporary socio-cultural anthropologists have rejected earlier models of ethnography as treating local cultures as bounded and isolated. These anthropologists continue to concern themselves with the distinct ways people in different locales experience and understand their lives, but they often argue that one cannot understand these particular ways of life solely from a local perspective; they instead combine a focus on the local with an effort to grasp larger political, economic, and cultural frameworks that impact local lived realities. Notable proponents of this approach include Arjun Appadurai, James Clifford, George Marcus, Sidney Mintz, Michael Taussig, Eric Wolf and Ronald Daus.
A growing trend in anthropological research and analysis is the use of multi-sited ethnography, discussed in George Marcus' article, "Ethnography In/Of the World System: the Emergence of Multi-Sited Ethnography". Looking at culture as embedded in macro-constructions of a global social order, multi-sited ethnography uses traditional methodology in various locations both spatially and temporally. Through this methodology, greater insight can be gained when examining the impact of world-systems on local and global communities.
Also emerging in multi-sited ethnography are greater interdisciplinary approaches to fieldwork, bringing in methods from cultural studies, media studies, science and technology studies, and others. In multi-sited ethnography, research tracks a subject across spatial and temporal boundaries. For example, a multi-sited ethnography may follow a "thing," such as a particular commodity, as it is transported through the networks of global capitalism.
Multi-sited ethnography may also follow ethnic groups in diaspora, stories or rumours that appear in multiple locations and in multiple time periods, metaphors that appear in multiple ethnographic locations, or the biographies of individual people or groups as they move through space and time. It may also follow conflicts that transcend boundaries. An example of multi-sited ethnography is Nancy Scheper-Hughes' work on the international black market for the trade of human organs. In this research, she follows organs as they are transferred through various legal and illegal networks of capitalism, as well as the rumours and urban legends that circulate in impoverished communities about child kidnapping and organ theft.
Sociocultural anthropologists have increasingly turned their investigative eye on to "Western" culture. For example, Philippe Bourgois won the Margaret Mead Award in 1997 for "In Search of Respect", a study of the entrepreneurs in a Harlem crack-den. Also growing more popular are ethnographies of professional communities, such as laboratory researchers, Wall Street investors, law firms, or information technology (IT) computer employees.

</doc>
<doc id="5390" url="http://en.wikipedia.org/wiki?curid=5390" title="Conversion of units">
Conversion of units

Conversion of units is the conversion between different units of measurement for the same quantity, typically through multiplicative conversion factors.
Techniques.
Process.
The process of conversion depends on the specific situation and the intended purpose. This may be governed by regulation, contract, Technical specifications or other published standards. Engineering judgment may include such factors as:
Some conversions from one system of units to another need to be exact, without increasing or decreasing the precision of the first measurement. This is sometimes called "soft conversion". It does not involve changing the physical configuration of the item being measured.
By contrast, a "hard conversion" or an "adaptive conversion" may not be exactly equivalent. It changes the measurement to convenient and workable numbers and units in the new system. It sometimes involves a slightly different configuration, or size substitution, of the item. Nominal values are sometimes allowed and used.
Multiplication factors.
Conversion between units in the metric system can be discerned by their prefixes (for example, 1 kilogram = 1000 grams, 1 milligram = 0.001 grams) and are thus not listed in this article. Exceptions are made if the unit is commonly known by another name (for example, 1 micron = 10−6 metre).
Table ordering.
Within each table, the units are listed alphabetically, and the SI units (base or derived) are highlighted.
Tables of conversion factors.
This article gives lists of conversion factors for each of a number of physical quantities, which are listed in the index. For each physical quantity, a number of different units (some only of historical interest) are shown and expressed in terms of the corresponding SI unit.
Mass.
Notes:
Speed or velocity.
A velocity consists of a speed combined with a direction; the speed part of the velocity takes units of speed.
Force.
"See also:" Conversion between weight (force) and mass
Information entropy.
Often, information entropy is measured in shannons, whereas the (discrete) storage space of digital devices is measured in bits. Thus, uncompressed redundant data occupy more than one bit of storage per shannon of information entropy. The multiples of a bit listed above are usually used with this meaning. Other times the bit is used as a measure of information entropy and is thus a synonym of shannon.
Luminous intensity.
The candela is the preferred nomenclature for the SI unit.
Radiation - source activity.
Please note that although becquerel (Bq) and hertz (Hz) both ultimately refer to the same SI base unit (s−1), Hz is used only for periodic phenomena, and Bq is only used for stochastic processes associated with radioactivity.
Radiation - exposure.
The roentgen is not an SI unit and the NIST strongly discourages its continued use.
Radiation - equivalent dose.
Although the definitions for sievert (Sv) and gray (Gy) would seem to indicate that they measure the same quantities, this is not the case. The effect of receiving a certain dose of radiation (given as Gy) is variable and depends on many factors, thus a new unit was needed to denote the biological effectiveness of that dose on the body; this is known as the equivalent dose and is shown in Sv. The general relationship between absorbed dose and equivalent dose can be represented as
where "H" is the equivalent dose, "D" is the absorbed dose, and "Q" is a dimensionless quality factor. Thus, for any quantity of "D" measured in Gy, the numerical value for "H" measured in Sv may be different.
Software tools.
Home and office computers come with converters in bundled spreadsheet applications or can access free converters via the Internet. Units and measurements can be easily converted using these tools, but only if the units are explicitly defined and the conversion is compatible (e.g., cmHg to kPa).
In fiction.
When a measure is converted to another unit in a fiction work like a novel or film, the value often loses meaning (as in numerology) or gets too many significant digits ("40 miles" turns into "64 km"). For this reason, authors sometimes choose numbers that can be converted easily. Most notably, distances measured in miles often begin with 3 or 5, so the value converted to km also has one significant digit (5 or 8).

</doc>
<doc id="5391" url="http://en.wikipedia.org/wiki?curid=5391" title="City">
City

A city is a relatively large and permanent human settlement. Although there is no agreement on how a city is distinguished from a town within general English language meanings, many cities have a particular administrative, legal, or historical status based on local law.
Cities generally have complex systems for sanitation, utilities, land usage, housing, and transportation. The concentration of development greatly facilitates interaction between people and businesses, benefiting both parties in the process, but it also presents challenges to managing urban growth. A big city or metropolis usually has associated suburbs and exurbs. Such cities are usually associated with metropolitan areas and urban areas, creating numerous business commuters traveling to urban centers for employment. Once a city expands far enough to reach another city, this region can be deemed a conurbation or megalopolis. In terms of population, the largest city proper is Shanghai, while the fastest growing is Dubai.
Origins.
There is not enough evidence to assert what conditions gave rise to the first cities. Some theorists have speculated on what they consider suitable pre-conditions, and basic mechanisms that might have been important driving forces.
The conventional view holds that cities first formed after the Neolithic revolution. The Neolithic revolution brought agriculture, which made denser human populations possible, thereby supporting city development. The advent of farming encouraged hunter-gatherers to abandon nomadic lifestyles and to settle near others who lived by agricultural production. The increased population-density encouraged by farming and the increased output of food per unit of land created conditions that seem more suitable for city-like activities. In his book, "Cities and Economic Development", Paul Bairoch takes up this position in his argument that agricultural activity appears necessary before true cities can form.
According to Vere Gordon Childe, for a settlement to qualify as a city, it must have enough surplus of raw materials to support trade and a relatively large population. Bairoch points out that, due to sparse population densities that would have persisted in pre-Neolithic, hunter-gatherer societies, the amount of land that would be required to produce enough food for subsistence and trade for a large population would make it impossible to control the flow of trade. To illustrate this point, Bairoch offers an example: "Western Europe during the pre-Neolithic, [where] the density must have been less than 0.1 person per square kilometre". Using this population density as a base for calculation, and allotting 10% of food towards surplus for trade and assuming that city dwellers do no farming, he calculates that "...to maintain a city with a population of 1,000, and without taking the cost of transport into account, an area of 100,000 square kilometres would have been required. When the cost of transport is taken into account, the figure rises to 200,000 square kilometres ...". Bairoch noted that this is roughly the size of Great Britain.
The urban theorist Jane Jacobs suggests that city-formation preceded the birth of agriculture, but this view is not widely accepted.
In his book "City Economics", Brendan O'Flaherty asserts "Cities could persist—as they have for thousands of years—only if their advantages offset the disadvantages" . O'Flaherty illustrates two similar attracting advantages known as increasing returns to scale and economies of scale, which are concepts usually associated with firms. Their applications are seen in more basic economic systems as well. Increasing returns to scale occurs when "doubling all inputs more than doubles the output [and] an activity has economies of scale if doubling output less than doubles cost" . To offer an example of these concepts, O'Flaherty makes use of "one of the oldest reasons why cities were built: military protection" . In this example, the inputs are anything that would be used for protection (e.g., a wall) and the output is the area protected and everything of value contained in it. O'Flaherty then asks that we suppose the protected area is square, and each hectare inside it has the same value of protection. The advantage is expressed as: 
The inputs depend on the length of the perimeter:
So there are increasing returns to scale:
Also, economies of scale:
"Cities, then, economize on protection, and so protection against marauding barbarian armies is one reason why people have come together to live in cities ..." .
Similarly, "Are Cities Dying?", a paper by Harvard economist Edward L. Glaeser, delves into similar reasons for city formation: reduced transport costs for goods, people and ideas. Discussing the benefits of proximity, Glaeser claims that if a city is doubled in size, workers get a ten percent increase in earnings. Glaeser furthers his argument by stating that bigger cities do not pay more for equal productivity than in a smaller city, so it is reasonable to assume that workers become more productive if they move to a city twice the size as they initially worked in. The workers do not benefit much from the ten percent wage increase, because it is recycled back into the higher cost of living in a larger city. They do gain other benefits from living in cities, though.
Geography.
City planning has seen many different schemes for how a city should look. The most commonly seen pattern is the grid, used for thousands of years in China, independently invented by Alexander the Great's city planner Dinocrates of Rhodes and favoured by the Romans, while almost a rule in parts of pre-Columbian America. Derry, begun in 1613, was the first planned city in Ireland, with the walls being completed five years later. The central diamond within a walled city with four gates was considered a good design for defence. The grid pattern was widely copied in the colonies of British North America.
The ancient Greeks often gave their colonies around the Mediterranean a grid plan. One of the best examples is the city of Priene. This city had different specialised districts, much as is seen in modern city planning today. Fifteen centuries earlier, the Indus Valley Civilisation was using grids in such cities as Mohenjo-Daro. In medieval times there was evidence of a preference for linear planning. Good examples are the cities established by various rulers in the south of France and city expansions in old Dutch and Flemish cities.
Grid plans were popular among planners in the 19th century, particularly after the redesign of Paris. They cut through the meandering, organic streets that followed old paths. The United States imposed grid plans in new territories and towns, as the American West was rapidly established, in places such as Salt Lake City and San Francisco.
Other forms may include a radial structure, in which main roads converge on a central point. This was often a historic form, the effect of successive growth over long time with concentric traces of town walls and citadels. In more recent history, such forms were supplemented by ring-roads that take traffic around the outskirts of a town. Many Dutch cities are structured this way: a central square surrounded by concentric canals. Every city expansion would imply a new circle (canals together with town walls). In cities such as Amsterdam, Haarlem and also Moscow, this pattern is still clearly visible.
History.
Towns and cities have a long history, although opinions vary on whether any particular ancient settlement can be considered a city. A city formed as central places of trade for the benefit of the members living in close proximity to others facilitates interaction of all kinds. These interactions generate both positive and negative externalities between others' actions. Benefits include reduced transport costs, exchange of ideas, sharing of natural resources, large local markets, and later in their development, amenities such as running water and sewage disposal. Possible costs would include higher rate of crime, higher mortality rates, higher cost of living, worse pollution, traffic and high commuting times. Cities grow when the benefits of proximity between people and firms are higher than the cost.
The first true towns are sometimes considered large settlements where the inhabitants were no longer simply farmers of the surrounding area, but began to take on specialized occupations, and where trade, food storage and power were centralized. In 1950 Gordon Childe attempted to define a historic city with 10 general metrics. These are:
This categorisation is descriptive, and it is used as a general touchstone when considering ancient cities, although not all have each of its characteristics.
One characteristic that can be used to distinguish a small city from a large town is organized government. A town accomplishes common goals through informal agreements between neighbors or the leadership of a chief. A city has professional administrators, regulations, and some form of taxation (food and other necessities or means to trade for them) to feed the government workers. The governments may be based on heredity, religion, military power, work projects (such as canal building), food distribution, land ownership, agriculture, commerce, manufacturing, finance, or a combination of those. Societies that live in cities are often called civilizations.
Ancient times.
Early cities developed in a number of regions of the ancient world. Uruk is the world's first city. After Mesopotamia, this culture arose in Syria and Anatolia, as shown by the city of Çatalhöyük (7500–5700BC). It is the largest Neolithic site found to date. Although it has sometimes been claimed that ancient Egypt lacked urbanism, several types of urban settlements were found in ancient times.
The Indus Valley Civilization and ancient China are two other areas with major indigenous urban traditions. Among the early Old World cities, Mohenjo-daro of the Indus Valley Civilization in present-day Pakistan, existing from about 2600 BC, was one of the largest, with a population of 50,000 or more.
In ancient Greece, beginning in the early 1st millennium BC, there emerged independent city-states that evolved for the first time the notion of citizenship, becoming in the process the archetype of the free city, the polis. The Agora, meaning "gathering place" or "assembly", was the center of athletic, artistic, spiritual and political life of the polis. These Greek city-states reached great levels of prosperity that resulted in an unprecedented cultural boom, that of classical Greece, expressed in architecture, drama, science, mathematics and philosophy, and nurtured in Athens under a democratic government. The Greek Hippodamus of Miletus (c. 407 BC) has been dubbed the "Father of City Planning" for his design of Miletus; the Hippodamian, or grid plan, was the basis for subsequent Greek and Roman cities. In the 4th century BC, Alexander the Great commissioned Dinocrates of Rhodes to lay out his new city of Alexandria, the grandest example of idealized urban planning of the ancient Mediterranean world, where the city's regularity was facilitated by its level site near a mouth of the Nile.
This roster of early urban traditions is notable for its diversity. Excavations at early urban sites show that some cities were sparsely populated political capitals, others were trade centers, and still other cities had a primarily religious focus. Some cities had large dense populations, whereas others carried out urban activities in the realms of politics or religion without having large associated populations. Theories that attempt to explain ancient urbanism by a single factor, such as economic benefit, fail to capture the range of variation documented by archaeologists.
The growth of the population of ancient civilizations, the formation of ancient empires concentrating political power, and the growth in commerce and manufacturing led to ever greater capital cities and centres of commerce and industry, with Alexandria, Antioch and Seleucia of the Hellenistic civilization, Pataliputra (now Patna) in India, Chang'an (now Xi'an) in China, Carthage, ancient Rome, its eastern successor Constantinople (later Istanbul).
Keith Hopkins estimates that ancient Rome had a population of about a million people by the end of the 1st century BC, after growing continually during the 3rd, 2nd, and 1st centuries BC, making it the largest city in the world at the time. Alexandria's population was also close to Rome's population at around the same time, the historian Rostovtzeff estimates a total population close to a million based on a census dated from 32 AD that counted 180,000 adult male citizens in Alexandria.
Cities of Late Antiquity underwent transformations as the urban power base shrank and was transferred to the local bishop (see Late Roman Empire). Cities essentially disappeared, earliest in Roman Britain and Germania and latest in the Eastern Roman Empire and Visigothic Spain.
In the ancient Americas, early urban traditions developed in the Andes and Mesoamerica. In the Andes, the first urban centers developed in the Norte Chico civilization (also Caral or Caral-Supe civilization), Chavin and Moche cultures, followed by major cities in the Huari, Chimu and Inca cultures. The Norte Chico civilization included as many as 30 major population centers in what is now the Norte Chico region of north-central coastal Peru. It is the oldest known civilization in the Americas, flourishing between the 30th century BC and the 18th century BC. Mesoamerica saw the rise of early urbanism in several cultural regions, including the Preclassic Maya, the Zapotec of Oaxaca, and Teotihuacan in central Mexico. Later cultures such as the Aztec drew on these earlier urban traditions.
In the first millennium AD, an urban tradition developed in the Khmer region of Cambodia, where Angkor grew into one of the largest cities (in area) of the world. The closest rival to Angkor, the Mayan city of Tikal in Guatemala, was between in total size. Although its population remains a topic of research and debate, newly identified agricultural systems in the Angkor area may have supported up to one million people.
Agriculture was practiced in sub-Saharan Africa since the third millennium BC. Because of this, cities could develop as centers of non-agricultural activity. Exactly when this first happened is still a topic of archeological and historical investigation. Western scholarship has tended to focus on cities in Europe and Mesopotamia, but emerging archeological evidence indicates that urbanization occurred south of the Sahara well before the influence of Arab urban culture. One of the oldest sites documented thus far, Jenné-Jeno in what is today Mali, has in fact been dated back to the third century BC. According to Roderick and Susan McIntosh, Jenné-Jeno did not fit into traditional Western conceptions of urbanity as it lacked monumental architecture and a distinctive elite social class, but it should indeed be considered a city based on more a more functional redefinition of urban development. In particular, Jenné-Jeno featured settlement mounds arranged according to a horizontal, rather than vertical, power hierarchy, and served as a center of specialized production and exhibited functional interdependence with the surrounding hinterland. Archaeological evidence from Jenné-Jeno, specifically the presence of non-West African glass beads dated from the third century BC to the fourth century AD, indicates that pre-Arabic trade contacts probably existed between Jenné-Jeno and North Africa. Additionally, other early urban centers in sub-Saharan Africa, dated to around 500 AD, include Awdaghust, Kumbi-Saleh the ancient capital of Ghana, and Maranda a center located on a trade rout between Egypt and Gao.
Middle Ages.
While David Kessler and Peter Temin consider ancient Rome the largest city before the 19th century, London was the first to exceed a population of 1 million. George Modelski considers medieval Baghdad, with an estimated population of 1.2 million at its peak, the largest city before 19th century London and the first with a population of over one million. Others estimate that Baghdad's population may have been as large as 2 million in the 9th century.
From the 9th through the end of the 12th century, the city of Constantinople, capital of the Byzantine Empire, was the largest and wealthiest city in Europe, with a population approaching 1 million.
During the European Middle Ages, a town was as much a political entity as a collection of houses. City residence brought freedom from customary rural obligations to lord and community: "Stadtluft macht frei" ("City air makes you free") was a saying in Germany. In Continental Europe cities with a legislature of their own were not unheard of, the laws for towns as a rule other than for the countryside, the lord of a town often being another than for surrounding land. In the Holy Roman Empire, some cities had no other lord than the emperor. In Italy medieval communes had quite a statelike power. In exceptional cases like Venice, Genoa or Lübeck, cities themselves became powerful states, sometimes taking surrounding areas under their control or establishing extensive maritime empires. Similar phenomena existed elsewhere, as in the case of Sakai, which enjoyed a considerable autonomy in late medieval Japan.
Early modern.
While the city-states, or poleis, of the Mediterranean and Baltic Sea languished from the 16th century, Europe's larger capitals benefited from the growth of commerce following the emergence of an Atlantic trade. By the early 19th century, London had become the largest city in the world with a population of over a million, while Paris rivaled the well-developed regionally traditional capital cities of Baghdad, Beijing, Istanbul and Kyoto.
During the Spanish colonization of the Americas the old Roman city concept was extensively used. Cities were founded in the middle of the newly conquered territories, and were bound to several laws about administration, finances and urbanism.
Most towns remained far smaller, so that in 1500 only some two dozen places in the world contained more than 100,000 inhabitants. As late as 1700, there were fewer than forty, a figure that rose to 300 in 1900.
Industrial age.
The growth of modern industry from the late 18th century onward led to massive urbanization and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In the United States from 1860 to 1910, the introduction of railroads reduced transportation costs, and large manufacturing centers began to emerge, thus allowing migration from rural to city areas. Cities during this period were deadly places to live in, due to health problems resulting from contaminated water and air, and communicable diseases. In the Great Depression of the 1930s cities were hard hit by unemployment, especially those with a base in heavy industry. In the U.S. urbanization rate increased forty to eighty percent during 1900–1990. Today the world's population is slightly over half urban, with millions still streaming annually into the growing cities of Asia, Africa and Latin America.
External effects.
Modern cities are known for creating their own microclimates. This is due to the large clustering of heat absorbent surfaces that heat up in sunlight and that channel rainwater into underground ducts.
Waste and sewage are two major problems for cities, as is air pollution from various forms of combustion, including fireplaces, wood or coal-burning stoves, other heating systems, and internal combustion engines. The impact of cities on places elsewhere, be it hinterlands or places far away, is considered in the notion of city footprinting ("ecological footprint").
Other negative external effects include health consequences such as communicable diseases, crime, and high traffic and commuting times. Cities cause more interaction with more people than rural areas, thus a higher probability to contracting contagious diseases. However, many inventions such as inoculations, vaccines, and water filtration systems have also lowered health concerns. Crime is also a concern in the cities. Studies have shown that crime rates in cities are higher and the chance of punishment after getting caught is lower. In cases such as burglary, the higher concentration of people in cities create more items of higher value worth the risk of crime. The high concentration of people also makes using auto mobiles inconvenient and pedestrian traffic is more prominent in metropolitan areas than a rural or suburban one.
Cities also generate positive external effects. The close physical proximity facilitates knowledge spillovers, helping people and firms exchange information and generate new ideas. A thicker labor market allows for better skill matching between firms and individuals. Population density enables also sharing of common infrastructure and production facilities, however in very dense cities, increased crowding and waiting times may lead to some negative effects.
Cities may have a positive influence on the environment. UN-HABITAT stated in its reports that city living can be the best solution for dealing with the rising population numbers (and thus still be a good approach on dealing with overpopulation) This is because cities concentrate human activity into one place, making the environmental damage on other places smaller. However, this can only be achieved if urban planning is improved and if city services are properly maintained.
Distinction between cities and towns.
The difference between "towns" and "cities" is differently understood in different parts of the world.
Even within the English-speaking world there is no one standard definition of a city: the term may be used either for a town possessing city status; for an urban locality exceeding an arbitrary population size; for a town dominating other towns with particular regional economic or administrative significance. United Kingdom city status was historically conferred on settlements with a diocesan cathedral; in more recent times towns apply to receive city status at times of national celebration. Larger settlements which are not designated as cities are towns, smaller settlements are villages and very small settlements are called hamlets. In the US "city" is used for much smaller settlements.
Although "city" can refer to an agglomeration including suburban and satellite areas, the term is not usually applied to a conurbation (cluster) of distinct urban places, nor for a wider metropolitan area including more than one city, each acting as a focus for parts of the area. And the word "town" (also "downtown") may mean the center of the city.
Global cities.
A global city, also known as a "world city", is a prominent centre of trade, banking, finance, innovation, and markets. The term "global city", as opposed to megacity, was coined by Saskia Sassen in a seminal 1991 work. Whereas "megacity" refers to any city of enormous size, a global city is one of enormous power or influence. Global cities, according to Sassen, have more in common with each other than with other cities in their host nations.
The notion of global cities is rooted in the concentration of power and capabilities within all cities. The city is seen as a container where skills and resources are concentrated: the better able a city is to concentrate its skills and resources, the more successful and powerful the city. This makes the city itself more powerful in the sense that it can influence what is happening around the world. Following this view of cities, it is possible to rank the world's cities hierarchically.
Critics of the notion point to the different realms of power. The term "global city" is heavily influenced by economic factors and, thus, may not account for places that are otherwise significant.
In 1995, Kanter argued that successful cities can be identified by three elements: good thinkers (concepts), good makers (competence) or good traders (connections). The interplay of these three elements, Kanter argued, means that good cities are not planned but managed.
Inner city.
In Paris, the inner city is the richest part of the metropolitan area, where housing is the most expensive, and where elites and high-income individuals dwell. In the developing world, economic modernization brings poor newcomers from the countryside to build haphazardly at the edge of current settlement (see favelas, shacks and shanty towns).
The United States, in particular, has a culture of anti-urbanism that dates back to colonial times. The American City Beautiful architecture movement of the late 19th century was a reaction to perceived urban decay and sought to provide stately civic buildings and boulevards to inspire civic pride in the motley residents of the urban core. Modern anti-urban attitudes are found in the United States in the form of a planning profession that continues to develop land on a low-density suburban basis, where access to amenities, work and shopping is provided almost exclusively by car rather than by foot or transit.
There is a growing movement in North America called "New Urbanism" that calls for a return to traditional city planning methods where mixed-use zoning allows people to walk from one type of land-use to another. The idea is that housing, shopping, office space, and leisure facilities are all provided within walking distance of each other, thus reducing the demand for road-space and also improving the efficiency and effectiveness of mass transit.
21st century.
There is a debate about whether technology and instantaneous communications are making cities obsolete, or reinforcing the importance of big cities as centres of the knowledge economy. Knowledge-based development of cities, globalization of innovation networks, and broadband services are driving forces of a new city planning paradigm towards intelligent cities. Intelligent / smart cities use technology and communication to create more efficient agglomerations in terms of competitiveness, innovation, environment, energy, utilities, governance, and delivery of services to the citizen. Some companies are building brand new masterplanned cities from scratch on greenfield sites.

</doc>
<doc id="5394" url="http://en.wikipedia.org/wiki?curid=5394" title="Chervil">
Chervil

Chervil (Anthriscus cerefolium), sometimes called garden chervil to distinguish it from similar plants also called chervil, or French parsley, is a delicate annual herb related to parsley. It is commonly used to season mild-flavoured dishes and is a constituent of the French herb mixture fines herbes.
Biology.
A member of the Apiaceae, chervil is native to the Caucasus but was spread by the Romans through most of Europe, where it is now naturalised.
The plants grow to , with tripinnate leaves that may be curly. The small white flowers form small umbels, across. The fruit is about 1 cm long, oblong-ovoid with a slender, ridged beak.
Uses and impact.
Culinary arts.
Chervil is used, particularly in France, to season poultry, seafood, young spring vegetables (such as carrots), soups, and sauces. More delicate than parsley, it has a faint taste of liquorice or aniseed.
Chervil is one of the four traditional French "fines herbes", along with tarragon, chives, and parsley, which are essential to French cooking. Unlike the more pungent, robust herbs, thyme, rosemary, etc., which can take prolonged cooking, the "fines herbes" are added at the last minute, to salads, omelettes, and soups. 
Horticulture.
According to some, slugs are attracted to chervil and the plant is sometimes used to bait them.
Health.
Chervil has had various uses in folk medicine. It was claimed to be useful as a digestive aid, for lowering high blood pressure, and, infused with vinegar, for curing hiccups. Besides its digestive properties, it is used as a mild stimulant.
Chervil has also been implicated in "strimmer dermatitis", or phytophotodermatitis, due to spray from weed trimmers and other forms of contact. Other plants in the family "Apiaceae" can have similar effects.
Cultivation.
Transplanting chervil can be difficult, due to the long taproot. It prefers a cool and moist location, otherwise it rapidly goes to seed (also known as bolting). It is usually grown as a cool season crop, like lettuce and should be planted in early spring and late fall or in a winter greenhouse. Regular harvesting of leaves also helps to prevent bolting. If plants bolt despite precautions, the plant can be periodically re-sown throughout the growing season, thus producing fresh plants as older plants bolt and go out of production.
Chervil grows to a height of , and a width of .

</doc>
<doc id="5395" url="http://en.wikipedia.org/wiki?curid=5395" title="Chives">
Chives

Chives is the common name of "Allium schoenoprasum", the smallest species of the edible onion genus. A perennial plant, it is native to Europe, Asia and North America. "A. schoenoprasum" is the only species of "Allium" native to both the New and the Old Worlds.
The name of the species derives from the Greek "skhoínos" (sedge) and "práson" (leek). Its English name, "chives", derives from the French word "cive", from "cepa", the Latin word for onion.
Chives are a commonly used herb and can be found in grocery stores or grown in home gardens. In culinary use, the scapes and the unopened, immature flower buds are diced and used as an ingredient for fish, potatoes, soups, and other dishes. Chives have insect-repelling properties that can be used in gardens to control pests.
Biology.
Chives are a bulb-forming herbaceous perennial plant, growing to tall. The bulbs are slender, conical, long and broad, and grow in dense clusters from the roots. The scapes (or stems) are hollow and tubular, up to long and across, with a soft texture, although, prior to the emergence of a flower, they may appear stiffer than usual. The leaves, which are shorter than the scapes, are also hollow and tubular, or terete, (round in cross-section) which distinguishes it at a glance from Garlic Chives. The flowers are pale purple, and star-shaped with six petals, wide, and produced in a dense inflorescence of 10-30 together; before opening, the inflorescence is surrounded by a papery bract. The seeds are produced in a small three-valved capsule, maturing in summer. The herb flowers from April to May in the southern parts of its habitat zones and in June in the northern parts.
Chives are the only species of "Allium" native to both the Old World and the New World. Sometimes, the plants found in North America are classified as "A. schoenoprasum" var. "sibiricum", although this is disputed. Differences among specimens are significant. One example was found in northern Maine growing solitary, instead of in clumps, also exhibiting dingy grey flowers.
Although chives are repulsive to insects in general, due to their sulfur compounds, their flowers attract bees, and they are at times kept to increase desired insect life.
Uses.
Culinary arts.
Chives are grown for their scapes, which are used for culinary purposes as a flavoring herb, and provide a somewhat milder flavor than those of other "Allium" species.
Chives have a wide variety of culinary uses, such as in traditional dishes in France and Sweden, among others. In his 1806 book "Attempt at a Flora" ("Försök til en flora"), Retzius describes how chives are used with pancakes, soups, fish and sandwiches. They are also an ingredient of the "gräddfil" sauce served with the traditional herring dish served at Swedish midsummer celebrations. The flowers may also be used to garnish dishes. In Poland, chives are served with quark cheese.
Chives are one of the "fines herbes" of French cuisine, which also include tarragon, chervil and/or parsley.
Chives can be found fresh at most markets year-round, making them readily available; they can also be dry-frozen without much impairment to the taste, giving home growers the opportunity to store large quantities harvested from their own gardens.
Uses in plant cultivation.
Retzius also describes how farmers would plant chives between the rocks making up the borders of their flowerbeds, to keep the plants free from pests (such as Japanese beetles). The growing plant repels unwanted insect life, and the juice of the leaves can be used for the same purpose, as well as fighting fungal infections, mildew and scab.
Its flowers are attractive to bees, which are important for gardens with an abundance of plants in need of pollination.
Medicine.
The medicinal properties of chives are similar to those of garlic, but weaker; the faint effects in comparison with garlic are probably the main reason for their limited use as a medicinal herb. Containing numerous organosulfur compounds such as allyl sulfides and alkyl sulfoxides, chives are reported to have a beneficial effect on the circulatory system. They also have mild stimulant, diuretic, and antiseptic properties. As chives are usually served in small amounts and never as the main dish, negative effects are rarely encountered, although digestive problems may occur following overconsumption.
Chives are also rich in vitamins A and C, contain trace amounts of sulfur, and are rich in calcium and iron.
Cultivation.
Chives are cultivated both for their culinary uses and their ornamental value; the violet flowers are often used in ornamental dry bouquets.
Chives thrive in well-drained soil, rich in organic matter, with a pH of 6-7 and full sun. They can be grown from seed and mature in summer, or early the following spring. Typically, chives need to be germinated at a temperature of 15 to 20°C (60-70°F) and kept moist. They can also be planted under a cloche or germinated indoors in cooler climates, then planted out later. After at least four weeks, the young shoots should be ready to be planted out. They are also easily propagated by division.
In cold regions, chives die back to the underground bulbs in winter, with the new leaves appearing in early spring. Chives starting to look old can be cut back to about 2–5 cm. When harvesting, the needed number of stalks should be cut to the base. During the growing season, the plant will continually regrow leaves, allowing for a continuous harvest.
History and cultural importance.
Chives have been cultivated in Europe since the Middle Ages (5th until the 15th centuries), although their usage dates back 5000 years. They were sometimes referred to as "rush leeks" (from the Greek "schoinos" meaning rush and "prason" meaning leek).
The Romans believed chives could relieve the pain from sunburn or a sore throat. They believed eating chives could increase blood pressure and act as a diuretic.
Romanian Gypsies have used chives in fortune telling. It was believed that bunches of dried chives hung around a house would ward off disease and evil.

</doc>
<doc id="5397" url="http://en.wikipedia.org/wiki?curid=5397" title="Chris Morris (satirist)">
Chris Morris (satirist)

Christopher Morris (born 15 June 1962) is an English satirist, writer, director, actor, voice actor, and producer, known for his black humour, surrealism, and controversial subject matter. He has been hailed for his "uncompromising, moralistic drive" by the British Film Institute. His tendency to avoid the media spotlight has seen him become one of the more enigmatic figures in British comedy.
In the early 1990s, Morris teamed up with his radio producer, Armando Iannucci, to create "On The Hour", a satire of news programmes. This was expanded into a television spin off, "The Day Today", which launched the career of Steve Coogan, and has since been hailed as one of the most important satirical shows of the 1990s. Morris further developed the satirical news format with "Brass Eye", which lampooned celebrities whilst focusing on themes such as crime and drugs. For many, the apotheosis of Morris's career was a "Brass Eye" special, which dealt with the moral panic surrounding paedophilia. It quickly became one of the most complained about programmes in British television history, leading the "Daily Mail" to describe him as "the most loathed man on TV".
Meanwhile, Morris's postmodern sketch and ambient music radio show "Blue Jam" helped him to gain a cult following. He went on to win a BAFTA for Best Short Film after expanding a "Blue Jam" sketch into "My Wrongs 8245–8249 & 117", which starred Paddy Considine. This was followed by "Nathan Barley", a sitcom written in collaboration with a then little-known Charlie Brooker that satirised hipsters, and although a ratings bomb, found success upon its DVD release. Morris followed this by joining the cast of the Graham Linehan sitcom "The IT Crowd", his first project in which he did not have writing or producing input.
In 2010, Morris directed his first feature-length film, "Four Lions", which satirised Islamic terrorism through a group of inept British Pakistanis. Reception of the film was largely positive, earning Morris his second BAFTA, for "Outstanding Debut". Since 2012, he has directed four episodes of Iannucci's political comedy "Veep".
Biography.
Early life.
Morris was born in Colchester, Essex, to father Michael Morris, a GP, and mother Rosemary Parrington and grew up in a Victorian farmhouse in the village Buckden, Huntingdonshire, which he describes as "very dull".
He has two younger brothers, including theatre director Tom Morris. From an early age he was a prankster, and also had a passion for radio. From the age of 10 he was educated at Stonyhurst College, an independent Jesuit boarding school in Lancashire. He went to study zoology at the University of Bristol, where he gained a 2:1.
Radio career.
On graduating, Morris pursued a career as a musician in various bands, for which he played the bass guitar. He then went to work for Radio West, a local radio station in Bristol. He then took up a news traineeship with BBC Radio Cambridgeshire, where he took advantage of access to editing and recording equipment to create elaborate spoofs and parodies. He also spent time in early 1987 hosting a 2–4pm afternoon show and finally ended up presenting Saturday morning show "I.T.".
In July 1987, he moved on to BBC Radio Bristol to present his own show "No Known Cure", broadcast on Saturday and Sunday mornings. The show was surreal and satirical, with odd interviews conducted with unsuspecting members of the public. He was fired from Bristol in 1990 after "talking over the news bulletins and making silly noises". In 1988 he also joined, from its launch, Greater London Radio (GLR). He presented "The Chris Morris Show" on GLR until 1993, when one show got suspended after a sketch was broadcast involving a child "outing" celebrities.
In 1991, Morris joined Armando Iannucci's spoof news project "On the Hour". Broadcast on BBC Radio 4, it saw him work alongside Iannucci, Steve Coogan, Stewart Lee, Richard Herring and Rebecca Front. In 1994, Morris began a weekly evening show, the "Chris Morris Music Show", on BBC Radio 1 alongside Peter Baynham and 'man with a mobile phone' Paul Garner. In the shows, Morris perfected the spoof interview style that would become a central component of his "Brass Eye" programme. In the same year, Morris teamed up with Peter Cook, as Sir Arthur Streeb-Greebling, in a series of improvised conversations for BBC Radio 3, entitled "Why Bother?".
Move into television and film.
In 1994, a BBC 2 television series based on "On the Hour" was broadcast under the name "The Day Today". "The Day Today" made a star of Morris, and marked the television debut of Steve Coogan's Alan Partridge character. The programme ended on a high after just one series, with Morris winning the 1994 British Comedy Award for Best Newcomer for his lead role as the Paxmanesque news anchor.
In 1997, the black humour which had featured in "On the Hour" and "The Day Today" became more prominent in "Brass Eye", another spoof current affairs television documentary, shown on Channel 4. "Brass Eye" became known for tricking celebrities and politicians into throwing support behind public awareness campaigns for made-up issues that were often absurd or surreal (such as a drug called "cake" and an elephant with its trunk stuck up its anus).
From 1997 to 1999 Morris created "Blue Jam" for BBC Radio 1, a surreal taboo-breaking radio show set to an ambient soundtrack. In 2000 this was followed by "Jam", a television reworking. Morris released a 'remix' version of this, entitled "Jaaaaam".
In 2001, a reprise of "Brass Eye" on the moral panic that surrounds paedophilia led to a record-breaking number of complaints – it still remains the third highest on UK television after "Celebrity Big Brother 2007" and "" – as well as heated discussion in the press. Many complainants, some of whom later admitted to not having seen the programme (notably Beverley Hughes, a government minister), felt the satire was directed at the victims of paedophilia, which Morris denies. Channel 4 defended the show, insisting the target was the media and its hysterical treatment of paedophilia, and not victims of crime.
In 2002, Morris ventured into film, directing the short "My Wrongs#8245–8249 & 117", adapted from a "Blue Jam" monologue about a man led astray by a sinister talking dog. It was the first film project of Warp Films, a branch of Warp Records. In 2002 this won the BAFTA for best short film. In 2005 Morris worked on a sitcom entitled "Nathan Barley", based on the character created by Charlie Brooker for his website TVGoHome. Co-written by Brooker and Morris, the series was broadcast on Channel 4 in early 2005.
Post 2005.
Morris was a cast member in "The IT Crowd", a Channel 4 sitcom which focused on the information technology department of the fictional company Reynholm Industries. The series was written and directed by Graham Linehan (writer of "Father Ted" and "Black Books", with whom Morris collaborated on "The Day Today", "Brass Eye" and "Jam") and produced by Ash Atalla ("The Office"). Morris played Denholm Reynholm, the eccentric managing director of the company. This marked the first time Morris has acted in a substantial role in a project which he has not developed himself. Morris's character appeared to leave the series during episode two of the second series. His character made a brief return in the first episode of the third series.
In November 2007, Morris wrote an article for "The Observer" in response to Ronan Bennett's article published six days earlier in "The Guardian". Bennett's article, "Shame on us", accused the novelist Martin Amis of racism. Morris's response, "The absurd world of Martin Amis", was also highly critical of Amis; although he did not accede to Bennett's accusation of racism, Morris likened Amis to the Muslim cleric Abu Hamza (who was jailed for inciting racial hatred in 2006), suggesting that both men employ "mock erudition, vitriol and decontextualised quotes from the Qu'ran" to incite hatred.
Morris served as script editor for the 2009 series "Stewart Lee's Comedy Vehicle", working with former colleagues Stewart Lee, Kevin Eldon and Armando Iannucci. He maintained this role for the second (2011) and third series (2014).
Morris completed his debut feature film "Four Lions" in late 2009, a satire based on a group of Islamist terrorists in Sheffield.
It premiered at the Sundance Film Festival in January 2010 and was short-listed for the festival's World Cinema Narrative prize. The film (working title "Boilerhouse") was picked up by Film Four. Morris told "The Sunday Times" that the film sought to do for Islamic terrorism what "Dad's Army", the classic BBC comedy, did for the Nazis by showing them as "scary but also ridiculous".
In 2012, Morris directed the seventh and penultimate episode of the first season of "Veep", an Armando Iannucci-devised American version of "The Thick of It". In 2013, he returned to direct two episodes for the second season of "Veep", and a further episode for season three in 2014.
In 2013, Morris appeared briefly in Richard Ayoade's "The Double", a black comedy film based on the Fyodor Dostoyevsky novella of the same name. Morris had previously worked with Ayoade on "Nathan Barley" and "The IT Crowd".
In February 2014, Morris made a surprise appearance at the beginning of a Stewart Lee live show, introducing the comedian with fictional anecdotes about their work together. The following month, Morris appeared in the third series of "Stewart Lee's Comedy Vehicle" as a "hostile interrogator", a role previously occupied by Armando Iannucci.
Music.
Morris often co-writes and performs incidental music for his television shows, notably with "Jam" and the 'extended remix' version, "Jaaaaam". In the early 1990s Morris contributed a Pixies parody track entitled "Motherbanger" to a flexi-disc given away with an edition of Select music magazine. Morris supplied sketches for British band Saint Etienne's 1993 single "You're in a Bad Way" (the sketch 'Spongbake' appears at the end of the 4th track on the CD single).
In 2000, he collaborated by mail with Amon Tobin to create the track "Bad Sex", which was released as a B-side on the Tobin single "Slowly".
British band Stereolab's song "Nothing to Do with Me" from their 2001 album "Sound-Dust" featured various lines from Chris Morris sketches as lyrics.
Recognition.
In 2003, Morris was listed in "The Observer" as one of the 50 funniest acts in British comedy. In 2005, Channel 4 aired a show called "The Comedian's Comedian" in which foremost writers and performers of comedy ranked their 50 favourite acts. Morris was at number eleven. Morris won the BAFTA for outstanding debut with his film "Four Lions". Adeel Akhtar and Nigel Lindsay collected the award in his absence. Lindsay stated that Morris had sent him a text message before they collected the award reading, 'Doused in petrol, Zippo at the ready'. In June 2012 Morris was placed at number 16 in the Top 100 People in UK Comedy.
In 2010, a biography, "Disgusting Bliss: The Brass Eye of Chris Morris", was published. Written by Lucian Randall, the book depicted Morris as "brilliant but uncompromising", and a "frantic-minded perfectionist".
Personal life.
Morris lives in Brixton, with his wife, actress turned literary agent Jo Unwin. The pair met in 1984 at the Edinburgh Festival, when he was playing bass guitar for the Cambridge Footlights Revue and she was in a comedy troupe called the Millies. They have two sons, Charles and Frederick, both of whom were born in Lambeth. Until the release of "Four Lions" he gave very few interviews and little had been published about Morris's personal life. Since 2009 he has made numerous media appearances to promote and support the film, both in the UK and USA, at one point appearing as a guest on "Late Night with Jimmy Fallon".
Morris can be heard as himself in a podcast for CERN.

</doc>
<doc id="5399" url="http://en.wikipedia.org/wiki?curid=5399" title="Colorado">
Colorado

Colorado (, or ) is a U.S. state encompassing most of the Southern Rocky Mountains as well as the northeastern portion of the Colorado Plateau and the western edge of the Great Plains. Colorado is part of the Western United States, the Southwestern United States, and the Mountain States. Colorado is the 8th most extensive and the 22nd most populous of the 50 United States. The United States Census Bureau estimates that the population of Colorado was 5,268,367 on July 1, 2013, an increase of 4.76% since the 2010 United States Census.
The state was named for the Colorado River, which Spanish explorers named the "Río Colorado" for the () silt the river carried from the mountains. On August 1, 1876, U.S. President Ulysses S. Grant signed a proclamation admitting Colorado to the Union as the 38th state. Colorado is nicknamed the "Centennial State" because it became a state in the centennial year of the United States Declaration of Independence.
Colorado is bordered by the northwest state of Wyoming to the north, the Midwest states of Nebraska and Kansas to the northeast and east, on the south by New Mexico and Oklahoma, on the west by Utah, and Arizona to the southwest. The four states of Colorado, New Mexico, Utah, and Arizona meet at one common point known as the Four Corners, which is known as the heart of the American Southwest. Colorado is noted for its vivid landscape of mountains, forests, high plains, mesas, canyons, plateaus, rivers, and desert lands.
Denver is the capital and the most populous city of Colorado. Residents of the state are properly known as "Coloradans", although the archaic term "Coloradoan" is still used, albeit rarely.
Geography.
Colorado is notable for its diverse geography, ranging from alpine mountains, arid plains and deserts with huge sand dunes, deep canyons, sandstone and granite rock formations, rivers, lakes, and lush forests. 
The borders of Colorado were originally defined to be lines of latitude and longitude, making its shape a latitude-longitude* quadrangle which stretches from 37°N to 41°N latitude and from 102°03'W to 109°03'W longitude (25°W to 32°W from the Washington Meridian). Colorado, Wyoming and Utah are the only states which have boundaries defined solely by lines of latitude and longitude. When placing the border markers for the Territory of Colorado, minor surveying errors resulted in several small kinks, most notably along the with the Territory of Utah. Once agreed upon by the federal, state, and territorial governments, those surveyors' benchmarks became the legal boundaries for the Colorado Territory, kinks and all.
Mountains.
Colorado is also known as "The Switzerland of America". The summit of Mount Elbert at in elevation in Lake County is the highest point of Colorado and the Rocky Mountains. Colorado is the only U.S. state that lies entirely above elevation. The point where the Arikaree River flows out of Yuma County, Colorado, and into Cheyenne County, Kansas, is the lowest point in Colorado at elevation. This point, which holds the distinction of being the highest low elevation point of any state, is higher than the high elevation points of 18 states and the District of Columbia.
Plains.
A little less than one half of the area of Colorado is flat and rolling land. East of the Rocky Mountains are the Colorado Eastern Plains of the High Plains, the section of the Great Plains within Nebraska at elevations ranging from roughly . The Colorado plains are usually thought of as prairies, but actually they have many patches of deciduous forests, buttes, and canyons, much like the high plains in New Mexico as well. Eastern Colorado is presently mainly covered in farmland, along with small farming villages and towns. Precipitation is fair, averaging from annually.
Corn, wheat, hay, soybeans, and oats are all typical crops, and most of the villages and towns in this region boast both a water tower and a grain elevator. As well as the farming of crops, Eastern Colorado has a good deal of livestock raising, such as at cattle ranches and hog farms and irrigation water is available from the South Platte, the Arkansas River, and a few other streams, and also from subterranean sources, including artesian wells. However, heavy use of ground water from wells for irrigation has caused underground water reserves to decline.
Front range.
Most of Colorado's population resides along the eastern edge of the Rocky Mountains in the Front Range Urban Corridor between Cheyenne, Wyoming, and Pueblo, Colorado. This region is partially protected from prevailing storms that blow in from the Pacific Ocean region by the high Rockies in the middle of Colorado. The only other significant population centers are at Grand Junction and Durango in western and southwestern Colorado, respectively.
Continental Divide.
The Continental Divide of the Americas extends along the crest of the Rocky Mountains. The area of Colorado to the west of the Continental Divide is called the Western Slope of Colorado. Drainage water west of the Continental Divide flows to the southwest via the Colorado River and the Green River into the Gulf of California.
Within the interior of the Rocky Mountains are several large so-called "parks" or high broad basins. In the north, on the east side of the Continental Divide is the North Park of Colorado. The North Park is drained by the North Platte River, which flows north into Wyoming and Nebraska. Just to the south of North Park, but on the western side of the Continental Divide, is the Middle Park of Colorado, which is drained by the Colorado River. The South Park of Colorado is the region of the headwaters of the South Platte River.
Southern region.
In southmost Colorado is the large San Luis Valley, where the headwaters of the Rio Grande are located. The valley sits between the Sangre De Cristo Mountains and San Juan Mountains, and consists of large desert lands that eventually run into the mountains. The Rio Grande drains due south into New Mexico, Mexico, and Texas. Across the Sangre de Cristo Range to the east of the San Luis Valley lies the Wet Mountain Valley. These basins, particularly the San Luis Valley, lie along the Rio Grande Rift, a major geological formation of the Rocky Mountains, and its branches.
Peaks.
To the west of the Great Plains of Colorado rises the eastern slope of the Rocky Mountains. Notable peaks of the Rocky Mountains include Longs Peak, Mount Evans, Pikes Peak, and the Spanish Peaks near Walsenburg, in southern Colorado. This area drains to the east and the southeast, ultimately either via the Mississippi River or the Rio Grande into the Gulf of Mexico.
The Rocky Mountains within Colorado contain about 53 peaks that are or higher in elevation above sea level, known as fourteeners. These mountains are largely covered with trees such as conifers and aspens up to the tree line, at an elevation of about in southern Colorado to about in northern Colorado. Above this only alpine vegetation grows. Only small parts of the Colorado Rockies are snow-covered year round.
Much of the alpine snow melts by mid-August with the exception of a few snowcapped peaks and a few small glaciers. The Colorado Mineral Belt, stretching from the San Juan Mountains in the southwest to Boulder and Central City on the front range, contains most of the historic gold- and silver-mining districts of Colorado. Mount Elbert is the highest summit of the Rocky Mountains. The 30 highest major summits of the Rocky Mountains of North America all lie within the state.
Colorado Western Slope.
The Western Slope of Colorado is drained by the Colorado River and its tributaries (primarily the Green River and the San Juan River), or by evaporation in its arid areas. Prominent in the southwestern area of the Western Slope is the Grand Mesa and the high San Juan Mountains, a rugged mountain range, and to the west of the San Juan Mountains, the Colorado Plateau, a high arid region that borders Southern Utah. The Colorado River flows through Glenwood Canyon and then through an arid valley made up of desert from Rifle to Parachute, through the desert canyon of De Beque Canyon, and into the arid desert of Grand Valley, of which the city of Grand Junction is located.
The city of Grand Junction, Colorado, is the largest city on the Western Slope, Grand Junction and Durango are the only major centers of television broadcasting west of the Continental Divide in Colorado. Most mountain resort communities publish daily newspapers. Higher education on the Western Slope can be found at Colorado Mesa University in Grand Junction, Western State College of Colorado in Gunnison, Fort Lewis College in Durango and Colorado Mountain College in Glenwood Springs and Steamboat Springs.
Grand Junction is located along Interstate 70, the only major highway of Western Colorado. Grand Junction is also along the major railroad of the Western Slope, the Union Pacific, which also provides the tracks for Amtrak's California Zephyr passenger train, which crosses the Rocky Mountains between Denver and Grand Junction via a route on which there are no continuous highways.
To the southeast of Grand Junction is the Grand Mesa, said to be the world's largest flat-topped mountain. Other towns of the Western Slope include Glenwood Springs with its resort hot springs, and the ski resorts of Aspen, Breckenridge, Vail, Crested Butte, Steamboat Springs, and Telluride.
The northwestern corner of Colorado is a sparsely populated region, and it contains part of the noted Dinosaur National Monument, which is not only a paleontological area, but is also a scenic area of rocky hills, canyons, arid desert, and streambeads. Here, the Green River briefly crosses over into Colorado.
From west to east, the land of Colorado consists of desert lands, desert plateaus, alpine mountains, National Forests, relatively flat grasslands, scattered forests, buttes, and canyons in the western edge of the Great Plains. The famous Pikes Peak is located just west of Colorado Springs. Its isolated peak is visible from nearly the Kansas border on clear days, and also far to the north and the south.
The desert lands in Colorado are located in and around areas such as, the Pueblo, Canon City, Florence, Great Sand Dunes National Park and Preserve, San Luis Valley, Cortez, Canyon of the Ancients National Monument, Hovenweep National Monument, Ute Mountain, Delta, Grand Junction, Colorado National Monument, and other areas surrounding the Uncompahgre Plateau and Uncompahgre National Forest.
Colorado is one of four states in the United States that share a common geographic point the Four Corners together with Arizona, New Mexico, and Utah. At this intersection, it is possible to stand in four states at once.
Climate.
The climate of Colorado is more complex than states outside of the Mountain States region. Unlike most other states, southern Colorado is not always warmer than northern Colorado. Most of Colorado is made up of mountains, foothills, high plains, and desert lands. Mountains and surrounding valleys greatly affect local climate.
As a general rule, with an increase in elevation comes a decrease in temperature and an increase in precipitation. Northeast, east, and southeast Colorado are mostly the high plains, while Northern Colorado is a mix of high plains, foothills, and mountains. Northwest and west Colorado are predominantly mountainous, with some desert lands mixed in. Southwest and southern Colorado are a complex mixture of desert and mountain areas.
Eastern Plains.
The climate of the Eastern Plains is semiarid (Köppen climate classification: "BSk") with low humidity and moderate precipitation, usually from annually. The area is known for its abundant sunshine and cool, clear nights, which give this area a great average diurnal temperature range. The difference between the highs of the days and the lows of the nights can be considerable as warmth dissipates to the space during clear nights, the heat radiation not being trapped by clouds.
In summer, this area can have many days above 95 °F (35 °C) and often 100 °F (38 °C). On the plains, the winter lows usually range from 25 to −10 °F (−4 to −23 °C). About 75% of the precipitation falls within the growing season, from April to September, but this area is very prone to droughts. Most of the precipitation comes from thunderstorms, which can be severe, and from major snowstorms that occur in the winter and early spring. Otherwise, winters tend to be mostly dry and cold.
In much of the region, March is the snowiest month. April and May are normally the rainiest months, while April is the wettest month overall. The Front Range cities closer to the mountains tend to be warmer in the winter due to chinook winds which warm the area, sometimes bringing temperatures of 70 °F or higher in the winter. The average July temperature is 55 °F (13 °C) in the morning and 90 °F (32 °C) in the afternoon. The average January temperature is 1 °F (−8 °C) in the morning and 48 °F (9 °C) in the afternoon, although variation between consecutive days can be 40 °F (4 °C).
West of the plains and foothills.
West of the plains and foothills, the weather of Colorado is much less uniform. Even places a few miles apart can experience entirely different weather depending on the topography of the area. Most valleys have a semi-arid climate, which becomes an alpine climate at higher elevations. Humid microclimates also exist in some areas. Generally, the wettest season in western Colorado is winter while June is the driest month.
The mountains have mild summers with many days of high temperatures between 60 and 85 °F (16 and 29 °C), although thunderstorms can cause sudden but brief drops in temperature. The winters bring abundant, powdery snowfall to the mountains with plenty of sunshine between major storms. The western slope has high summer temperatures similar to those found on the plains, while the winters tend to be slightly cooler due to the lack of warming winds common to the plains and Front Range. Other areas in the west have their own unique climate.
Extreme weather.
Extreme weather changes are common in Colorado, although the majority of extreme weather occurs in the least populated areas of the state. Thunderstorms are common east of the Continental Divide in the spring and summer, yet are usually brief. Hail is a common sight in the mountains east of the divide and in the northwest part of the state. The Eastern Plains have had some of the biggest hail storms in North America.
The Eastern Plains are part of the extreme western portion of Tornado Alley; some damaging tornadoes in the Eastern Plains include the 1990 Limon F3 tornado and the 2008 Windsor EF3 tornado, which devastated the small town.
The plains are also susceptible to occasional floods, which are caused both by thunderstorms and by the rapid melting of snow in the mountains during warm weather. Denver's record in 1901 for the number of consecutive days above 90 °F (32 °C) was broken during the summer of 2008. The new record of 24 consecutive days surpassed the previous record by almost a week.
Much of Colorado is a very dry state averaging only of precipitation per year statewide and rarely experiences a time when some portion of the state is not in some degree of drought. The lack of precipitation contributes to the severity of wildfires in the state, such as the Hayman Fire, one of the largest wildfires in American history, and the Fourmile Canyon Fire of 2010, which until the Waldo Canyon Fire of June 2012, and the Black Forest Fire about a year later, was the most destructive wildfire in Colorado's recorded history.
However, some of the mountainous regions of Colorado receive a huge amount of moisture from winter snowfalls. The spring melts of these snows often cause great waterflows in the Yampa River, the Colorado River, the Rio Grande, the Arkansas River, Cherry Creek, the North Platte River, and the South Platte River.
Water flowing out of the Colorado Rocky Mountains is a very significant source of water for the farms, towns, and cities of the southwest states of New Mexico, Arizona, Utah, and Nevada, as well as the Midwest, such as Nebraska and Kansas, and the southern states of Oklahoma and Texas. A significant amount of water is also diverted for use in California; occasionally (formerly naturally and consistently), the flow of water reaches northern Mexico.
Records.
The highest ambient air temperature ever recorded in Colorado was on July 11, 1888, at Bennett. The lowest air temperature was on February 1, 1985, at Maybell.
Earthquakes.
Despite its mountainous terrain, Colorado is relatively quiescent seismically. The U.S. National Earthquake Information Center is located in Golden.
On August 22, 2011, a 5.3 magnitude earthquake occurred nine miles WSW of the city of Trinidad. No casualties and only small damage was reported. It was the second largest earthquake in Colorado. A magnitude 5.7 earthquake was recorded in 1973.
History.
The region that is today the state of Colorado has been inhabited by Native Americans for more than 13,000 years. The Lindenmeier Site in Larimer County contains artifacts dating from approximately 11200 BC to 3000 BC. The eastern edge of the Rocky Mountains was a major migration route that was important to the spread of early peoples throughout the Americas. The Ancient Pueblo peoples lived in the valleys and mesas of the Colorado Plateau. The Ute Nation inhabited the mountain valleys of the Southern Rocky Mountains and the Western Rocky Mountains, even as far east as the Front Range of present day. The Apache and the Comanche also inhabited Eastern and Southeastern parts of the state. At times, the Arapaho Nation and the Cheyenne Nation moved west to hunt across the High Plains.
The U.S. acquired a territorial claim to the eastern Rocky Mountains with the Louisiana Purchase from France in 1803. This U.S. claim conflicted with the claim by Spain to the upper Arkansas River Basin as the exclusive trading zone of its colony of Santa Fé de Nuevo Méjico. In 1806, Zebulon Pike led a U.S. Army reconnaissance expedition into the disputed region. Colonel Pike and his men were arrested by Spanish cavalrymen in the San Luis Valley the following February, taken to Chihuahua, and expelled from Mexico the following July.
The U.S. relinquished its claim to all land south and west of the Arkansas River and south of 42nd parallel north and west of the 100th meridian west as part of its purchase of Florida from Spain with the Adams-Onís Treaty of 1819. The treaty took effect February 22, 1821. Having settled its border with Spain, the U.S. admitted the southeastern portion of the Territory of Missouri to the Union as the state of Missouri on August 10, 1821. The remainder of Missouri Territory, including what would become northeastern Colorado, became unorganized territory, and remained so for 33 years over the question of slavery. After 11 years of war, Spain finally recognized the independence of Mexico with the Treaty of Córdoba signed on August 24, 1821. Mexico eventually ratified the Adams-Onís Treaty in 1831. The Texian Revolt of 1835–1836 fomented a dispute between the U.S. and Mexico which eventually erupted into the Mexican-American War in 1846. Mexico surrendered its northern territory to the U.S. with the Treaty of Guadalupe Hidalgo at the conclusion of the war in 1848.
Most American settlers traveling overland west to the Oregon Country, namely the new goldfields of California, or the new Mormon settlements of the State of Deseret in the Salt Lake Valley, avoided the rugged Southern Rocky Mountains, and instead followed the North Platte River and Sweetwater River to South Pass (Wyoming), the lowest crossing of the Continental Divide between the Southern Rocky Mountains and the Central Rocky Mountains. In 1849, the Mormons of the Salt Lake Valley organized the extralegal State of Deseret, claiming the entire Great Basin and all lands drained by the rivers Green, Grand, and Colorado. The federal government of the U.S. flatly refused to recognize the new Mormon government, because it was theocratic and sanctioned plural marriage. Instead, the Compromise of 1850 divided the Mexican Cession and the northwestern claims of Texas into a new state and two new territories, the state of California, the Territory of New Mexico, and the Territory of Utah. On April 9, 1851, Mexican American settlers from the area of Taos settled the village of San Luis, then in the New Mexico Territory, later to become Colorado's first permanent Euro-American settlement.
In 1854, Senator Stephen A. Douglas persuaded the U.S. Congress to divide the unorganized territory east of the Continental Divide into two new organized territories, the Territory of Kansas and the Territory of Nebraska, and an unorganized southern region known as the Indian territory. Each new territory was to decide the fate of slavery within its boundaries, but this compromise merely served to fuel animosity between free soil and pro-slavery factions.
The gold seekers organized the Provisional Government of the Territory of Jefferson on August 24, 1859, but this new territory failed to secure approval from the Congress of the United States embroiled in the debate over slavery. The election of Abraham Lincoln for the President of the United States on November 6, 1860, led to the secession of nine southern slave states and the threat of civil war among the states. Seeking to augment the political power of the Union states, the Republican Party dominated Congress quickly admitted the eastern portion of the Territory of Kansas into the Union as the free State of Kansas on January 29, 1861, leaving the western portion of the Kansas Territory, and its gold-mining areas, as unorganized territory.
Territory act.
Thirty days later on February 28, 1861, outgoing U.S. President James Buchanan signed an Act of Congress organizing the free Territory of Colorado. The original boundaries of Colorado remain unchanged today. The name Colorado was chosen because it was commonly believed that the Colorado River originated in the territory. In 1776, Spanish priest Silvestre Vélez de Escalante recorded that Native Americans in the area knew the river as " for the red-brown silt that the river carried from the mountains. In 1859, a U.S. Army topographic expedition led by Captain John Macomb located the confluence of the Green River with the Grand River in what is now Canyonlands National Park in Utah. The Macomb party designated the confluence as the source of the Colorado River.
On April 12, 1861, South Carolina artillery opened fire on Fort Sumter to start the American Civil War. While many gold seekers held sympathies for the Confederacy, the vast majority remained fiercely loyal to the Union cause.
In 1862, a force of Texas cavalry invaded the Territory of New Mexico and captured Santa Fe on March 10. The object of this Western Campaign was to seize or disrupt the gold fields of Colorado and California and to seize ports on the Pacific Ocean for the Confederacy. A hastily organized force of Colorado volunteers force-marched from Denver City, Colorado Territory, to Glorieta Pass, New Mexico Territory, in an attempt to block the Texans. On March 28, the Coloradans and local New Mexico volunteers stopped the Texans at the Battle of Glorieta Pass, destroyed their cannon and supply wagons, and ran off 500 of their horses and mules. The Texans were forced to retreat to Santa Fe. Having lost the supplies for their campaign and finding little support in New Mexico, the Texans abandoned Santa Fe and returned to San Antonio in defeat. The Confederacy made no further attempts to seize the Southwestern United States.
In 1864, Territorial Governor John Evans appointed the Reverend John Chivington as Colonel of the Colorado Volunteers with orders to protect white settlers from Cheyenne and Arapaho warriors who were accused of stealing cattle. Colonel Chivington ordered his men to attack a band of Cheyenne and Arapaho encamped along Sand Creek. Chivington reported that his troops killed more than 500 warriors. The militia returned to Denver City in triumph, but several officers reported that the so-called battle was a blatant massacre of Indians at peace, that most of the dead were women and children, and that bodies of the dead had been hideously mutilated and desecrated. Three U.S. Army inquiries condemned the action, and incoming President Andrew Johnson asked Governor Evans for his resignation, but none of the perpetrators was ever punished. This event is now known as the Sand Creek massacre.
In the midst and aftermath of Civil War, many discouraged prospectors returned to their homes, but a few stayed and developed mines, mills, farms, ranches, roads, and towns in Colorado Territory. On September 14, 1864, James Huff discovered silver near Argentine Pass, the first of many silver strikes. In 1867, the Union Pacific Railroad laid its tracks west to Weir, now Julesburg, in the northeast corner of the Territory. The Union Pacific linked up with the Central Pacific Railroad at Promontory Summit, Utah, on May 10, 1869, to form the First Transcontinental Railroad. The Denver Pacific Railway reached Denver in June the following year, and the Kansas Pacific arrived two months later to forge the second line across the continent. In 1872, rich veins of silver were discovered in the San Juan Mountains on the Ute Indian reservation in southwestern Colorado. The Ute people were removed from the San Juans the following year.
Statehood.
The United States Congress passed an enabling act on March 3, 1875, specifying the requirements for the Territory of Colorado to become a state. On August 1, 1876 (28 days after the Centennial of the United States), U.S. President Ulysses S. Grant signed a proclamation admitting Colorado to the Union as the 38th state and earning it the moniker "Centennial State".
The discovery of a major silver lode near Leadville in 1878 triggered the Colorado Silver Boom. The Sherman Silver Purchase Act of 1890 invigorated silver mining, and Colorado's last, but greatest, gold strike at Cripple Creek a few months later lured a new generation of gold seekers. Colorado women were granted the right to vote beginning on November 7, 1893, making Colorado the second state to grant universal suffrage and the first one by a popular vote (of Colorado men). The repeal of the Sherman Silver Purchase Act in 1893 led to a staggering collapse of the mining and agricultural economy of Colorado, but the state slowly and steadily recovered.
Colorado became the first western state to host a major political convention when the Democratic Party met in Denver in 1908. By the U.S. Census in 1930, the population of Colorado first exceeded one million residents. Colorado suffered greatly through the Great Depression and the Dust Bowl of the 1930s, but a major wave of immigration following World War II boosted Colorado's fortune. Tourism became a mainstay of the state economy, and high technology became an important economic engine. The United States Census Bureau estimated that the population of Colorado exceeded five million in 2009.
Three warships of the U.S. Navy have been named the USS "Colorado". The first USS "Colorado" was named for the Colorado River. The later two ships were named in honor of the state, including the battleship USS "Colorado" which served in World War II in the Pacific beginning in 1941. At the time of the Attack on Pearl Harbor, this USS "Colorado" was located at the naval base in San Diego, Calif. and hence went unscathed.
Demographics.
The United States Census Bureau estimates that the population of Colorado was 5,268,367 on July 1, 2013, a 4.8% increase since the 2010 United States Census. Colorado's most populous city, and capital, is Denver. The Denver-Aurora-Boulder Combined Statistical Area with an estimated 2011 population of 3,157,520, has 61.90% of the state's residents.
The largest increases are expected in the Front Range Urban Corridor, especially in the Denver metropolitan area. The state's fastest-growing counties are Douglas and Weld. The center of population of Colorado is located just north of the village of Critchell in Jefferson County.
According to the 2010 United States Census, Colorado had a population of 5,029,196. Racial composition of the state's population was:
People of Hispanic and Latino American (of any race made) heritage, made up 20.7% of the population. According to the 2000 Census, the largest ancestry groups in Colorado are German (22%) including of Swiss and Austrian nationalities, Mexican (18%), Irish (12%), and English (12%). Persons reporting German ancestry are especially numerous in the Front Range, the Rockies (west-central counties) and Eastern parts/High Plains.
Colorado has a high proportion of Hispanic, mostly Mexican-American, citizens in Metropolitan Denver, Colorado Springs, as well as the smaller cities of Greeley and Pueblo, and elsewhere. Colorado is well known for its strong Latino culture and presence. Southern, Southwestern, and Southeastern Colorado has a large number of Hispanos, the descendants of the early Mexican settlers of colonial Spanish origin. In 1940, the Census Bureau reported Colorado's population as 8.2% Hispanic and 90.3% non-Hispanic white. The Hispanic population of Colorado has continued to grow quickly over the past decades. By 2012, Hispanics made up 21% of Colorado's population, and Non-Hispanic Whites made up 69%.
The 2000 United States Census found that 10.5% of people aged five and over in Colorado speak only Spanish at home, with the 2009 estimate being roughly 14%. Colorado also has a large immigration presence all throughout the state, which has led to Colorado cities being referred to as "Sanctuary Cities" for illegal immigrants as well. Colorado has the 4th highest percentage of undocumented people in the U.S., only behind Nevada, Arizona, California, and tied with Texas. An estimated 5.5–6.0% of the state's population is composed of illegal immigrants. Also, over 20% of the state's prisoners are undocumented inmates. Colorado, like New Mexico, is very rich in archaic Spanish idioms.
Colorado also has some large African-American communities located in Denver, in the neighborhoods of Montbello, Five Points, Whittier, and many other East Denver areas. A relatively large population of African Americans are also found in Colorado Springs on the east and southeast side of the city. The state has sizable numbers of Asian-Americans of Mongolian, Chinese, Filipino, Korean, Southeast Asian and Japanese descent. The highest population of Asian Americans can be found on the south and southeast side of Denver, as well as some on Denver's southwest side. The Denver metropolitan area is considered more liberal and diverse than much of the state when it comes to political issues and environmental concerns.
There were a total of 70,331 births in Colorado in 2006. (Birth Rate of 14.6). In 2007, non-Hispanic whites were involved in 59.1% of all the births. Some 14.06% of those births involved a non-Hispanic white person and someone of a different race, most often with a couple including one Hispanic. A birth where at least one Hispanic person was involved counted for 43% of the births in Colorado. As of the 2010 Census, Colorado has the seventh highest percentage of Hispanics (20.7%) in the U.S. behind New Mexico (46.3%), California (37.6%), Texas (37.6%), Arizona (29.6%), Nevada (26.5%), and Florida (22.5%). Per the 2000 census, the Hispanic population is estimated to be 918,899 or approximately 20% of the state total population. Colorado has the 5th largest population of Mexican-Americans behind California, Texas, Arizona, and Illinois. In percentages, Colorado has the 6th highest percentage of Mexican-Americans behind New Mexico, California, Texas, Arizona, and Nevada.
Religion.
Major religious affiliations of the people of Colorado are 64% Christian, of whom there are 44% Protestants, 19% Roman Catholics, 3% Latter Day Saint/Mormon, 2% Jews, 1% Muslim, 1% Buddhist and 0.5% Hindu. The religiously unaffiliated make up 25% of the population.
The largest denominations by number of adherents in 2010 were the Catholic Church with 811,630; non-denominational Evangelical Protestants with 229,981; and The Church of Jesus Christ of Latter-day Saints with 142,473.
Health.
Colorado also has a reputation for being a state of active and athletic people. According to several studies, Coloradans have the lowest rates of obesity of any state in the US. As of 2007, 18% of the population was considered medically obese, and while the lowest in the nation, the percentage had increased from 17% from 2004. Former Colorado Governor Bill Ritter commented: “As an avid fisherman and bike rider, I know first-hand that Colorado provides a great environment for active, healthy lifestyles,” although he highlighted the need for continued education and support to slow the growth of obesity in the state.
Culture.
Fine arts.
Film.
A number of film productions have shot on location in Colorado, especially prominent Westerns like "True Grit", "The Searchers" and "Butch Cassidy and the Sundance Kid." A number of historic military forts, railways with trains still operating, mining ghost towns have been utilized and transformed for historical accuracy in well known films. There are also a number of scenic highways and mountain passes that helped to feature the open road in films such as "Vanishing Point", "Bingo" and "Starman". Some Colorado landmarks have been featured in films, such as the The Stanley Hotel in "Dumb and Dumber" and the Sculptured House in "Sleeper". The Colorado Office of Film and Television has noted that over 400 films have been shot in Colorado.
There are also a number of established film festivals in Colorado, including Aspen Shortsfest, Boulder International Film Festival, Castle Rock Film Festival, Denver Film Festival, Festivus film festival, Mile High Horror Film Festival, Moondance International Film Festival, Mountainfilm in Telluride, Rocky Mountain Women's Film Festival, and Telluride Film Festival.
Cuisine.
Colorado is known for its Southwest and Rocky Mountain cuisine. Mexican restaurants are prominent throughout the state.
Boulder, Colorado was named America’s Foodiest Town 2010 by Bon Appétit. Boulder, and Colorado in general, is home to a number of national food and beverage companies, top-tier restaurants and farmers' markets. Boulder, Colorado also has more Master Sommeliers per capita than any other city, including San Francisco and New York.
The Food & Wine Classic held annually each June in Aspen, Colorado. Aspen also has a reputation as the culinary capital of the Rocky Mountain region.
Denver is known for steak, but now has a diverse culinary scene with many top-tier restaurants.
Wine and beer.
Colorado wines include award-winning varietals that have attracted favorable notice from outside the state. With wines made from traditional "Vitis vinifera" grapes along with wines made from cherries, peaches, plums and honey, Colorado wines have won top national and international awards for their quality. Colorado's grape growing regions contain the highest elevation vineyards in the United States, with most viticulture in the state practiced between feet above sea level. The mountain climate ensures warm summer days and cool nights. Colorado is home to two designated American Viticultural Areas of the Grand Valley AVA and the West Elks AVA, where most of the vineyards in the state are located. However, an increasing number of wineries are located along the Front Range.
Colorado is home to many nationally praised microbreweries, including New Belgium Brewing Company, Odell Brewing Company, Great Divide Brewing Company, and Oskar Blues Brewery. The area of northern Colorado near the city of Fort Collins is known as the "Napa Valley of Beer" due to its high density of craft breweries.
Cannabis.
Colorado is one of two states to legalize both the medicinal (2000) and recreational (2014) use of marijuana.
Amendment 64 also requires the Colorado state legislature to enact regulations for industrial hemp making Colorado the only source of hemp and hemp products within the United States.
Medicinal use.
On November 7, 2000, 54% of Colorado voters passed Amendment 20, which amends the Colorado State constitution to allow the medical use of cannabis. Patients can possess no more than two ounces of "usable cannabis" and not more than six cannabis plants.
Currently Colorado has listed "eight medical conditions for which patients can use cannabis – cancer, glaucoma, HIV/AIDS, muscle spasms, seizures, severe pain, severe nausea and cachexia or dramatic weight loss and muscle atrophy." Colorado Governor John Hickenlooper has allocated about half of the state's $13 million "Medical Marijuana Program Cash Fund" to medical research in the 2014 budget.
Recreational use.
On November 6, 2012, voters amended the state constitution to protect "personal use" of marijuana for adults, establishing a framework to regulate cannabis in a manner similar to alcohol. The first recreational marijuana shops in Colorado, and by extension the United States, opened their doors on January 1, 2014.
Economy.
CNBC's list of "Top States for Business for 2010" has recognized Colorado as the third best state in the nation, falling short to only Texas and Virginia.
The Bureau of Economic Analysis estimates that the total state product in 2010 was $257.6 billion. Per capita personal income in 2010 was $51 940, ranking Colorado 11th in the nation. The state's economy broadened from its mid-19th century roots in mining when irrigated agriculture developed, and by the late 19th century, raising livestock had become important. Early industry was based on the extraction and processing of minerals and agricultural products. Current agricultural products are cattle, wheat, dairy products, corn, and hay.
The federal government is also a major economic force in the state with many important federal facilities including NORAD (North American Aerospace Defense Command), United States Air Force Academy, Schriever Air Force Base located approximately 10 miles (16 kilometers) east of Peterson Air Force Base, and Fort Carson, both located in Colorado Springs within El Paso County; NOAA, the National Renewable Energy Laboratory (NREL) in Golden, and the National Institute of Standards and Technology in Boulder; U.S. Geological Survey and other government agencies at the Denver Federal Center near Lakewood; the Denver Mint, Buckley Air Force Base, and Tenth Circuit Court of Appeals in Denver; and a federal Supermax Prison and other federal prisons near Cañon City. In addition to these and other federal agencies, Colorado has abundant National Forest land and four National Parks that contribute to federal ownership of of land in Colorado, or 37% of the total area of the state.
In the second half of the 20th century, the industrial and service sectors have expanded greatly. The state's economy is diversified and is notable for its concentration of scientific research and high-technology industries. Other industries include food processing, transportation equipment, machinery, chemical products, the extraction of metals such as gold (see Gold mining in Colorado), silver, and molybdenum. Colorado now also has the largest annual production of beer of any state. Denver is an important financial center.
A number of nationally known brand names have originated in Colorado factories and laboratories. From Denver came the forerunner of telecommunications giant Qwest in 1879, Samsonite luggage in 1910, Gates belts and hoses in 1911, and Russell Stover Candies in 1923. Kuner canned vegetables began in Brighton in 1864. From Golden came Coors beer in 1873, CoorsTek industrial ceramics in 1920, and Jolly Rancher candy in 1949. CF&I railroad rails, wire, nails and pipe debuted in Pueblo in 1892. Holly Sugar was first milled from beets in Holly in 1905, and later moved its headquarters to Colorado Springs. The present-day Swift packed meat of Greeley evolved from Monfort of Colorado, Inc., established in 1930. Estes model rockets were launched in Penrose in 1958. Fort Collins has been the home of Woodward Governor Company's motor controllers (governors) since 1870, and Waterpik dental water jets and showerheads since 1962. Celestial Seasonings herbal teas have been made in Boulder since 1969. Rocky Mountain Chocolate Factory made its first candy in Durango in 1981.
Colorado has a flat 4.63% income tax, regardless of income level. Unlike most states, which calculate taxes based on federal "adjusted gross income", Colorado taxes are based on "taxable income" – income after federal exemptions and federal itemized (or standard) deductions. Colorado's state sales tax is 2.9% on retail sales. When state revenues exceed state constitutional limits, full-year Colorado residents can claim a sales tax refund on their individual state income tax return. Many counties and cities charge their own rates in addition to the base state rate. There are also certain county and special district taxes that may apply.
Real estate and personal business property are taxable in Colorado. The state's senior property tax exemption was temporarily suspended by the Colorado Legislature in 2003. The tax break is scheduled to return for assessment year 2006, payable in 2007.
As of August 2014, the state's unemployment rate is 5.3%.
Philanthropy.
Major philanthropic organizations based in Colorado, include the Daniels Fund, the Anschutz Family Foundation, the Gates Family Foundation, the El Pomar Foundation and the Boettcher Foundation grant each year from approximately $7 billion of assets.
Natural resources.
Colorado has significant hydrocarbon resources. According to the Energy Information Administration, Colorado hosts seven of the Nation’s 100 largest natural gas fields and two of its 100 largest oil fields. Conventional and unconventional natural gas output from several Colorado basins typically account for more than 5 percent of annual U.S. natural gas production. Colorado’s oil shale deposits hold an estimated of oil – nearly as much oil as the entire world’s proven oil reserves; the economic viability of the oil shale, however, has not been demonstrated. Substantial deposits of bituminous, subbituminous, and lignite coal are found in the state.
Colorado's high Rocky Mountain ridges and eastern plains offer wind power potential, and geologic activity in the mountain areas provides potential for geothermal power development. Much of the state is sunny and could produce solar power. Major rivers flowing from the Rocky Mountains offer hydroelectric power resources. Corn grown in the flat eastern part of the state offers potential resources for ethanol production.
Transportation.
Colorado's primary mode of transportation (in terms of passengers) is its highway system. Interstate 25 (I-25) is the primary north–south highway in the state, connecting Pueblo, Colorado Springs, Denver, and Fort Collins, and extending north to Wyoming and south to New Mexico. I-70 is the primary east–west corridor. It connects Grand Junction and the mountain communities with Denver, and enters Utah and Kansas. The state is home to a network of US and Colorado highways that provide access to all principal areas of the state. Smaller communities are only connected to this network via county roads.
Denver International Airport (DIA) is the fourth busiest domestic U.S. airport and thirteenth busiest world airport DIA handles by far the largest volume of commercial air traffic in Colorado, and is the busiest U.S. hub airport between Chicago and the Pacific coast, making Denver the most important airport for connecting passenger traffic in the western U.S.
Denver International Airport is the primary hub for low-cost carrier Frontier Airlines, with routes throughout North America. It is also the fourth-largest hub of the world's largest airline, United Airlines. DIA is a focus city for Southwest Airlines, which since commencing service to Denver in January 2006, has added over 50 destinations, making Denver its fastest-growing market. Denver International Airport is the only airport in the United States to have implemented an ISO 14001-certified environmental management system covering the entire airport.
Extensive public transportation bus services are offered both intra-city and inter-city—including the Denver metro area's extensive RTD services. The Regional Transportation District (RTD) operates the popular RTD Bus & Light Rail transit system in the Denver Metropolitan Area. As of January 2013 the RTD rail system had 170 light rail vehicles, serving of track.
Amtrak operates two rail passenger lines through Colorado, including the Glenwood Canyon route of the famed California Zephyr, one of America's legendary passenger trains.
Colorado's contribution to world railroad history was forged principally by the Denver and Rio Grande Western Railroad which began in 1870 and wrote the book on mountain railroading. In 1988 the "Rio Grande" acquired, but was merged into, the Southern Pacific Railroad by their joint owner Philip Anschutz. On September 11, 1996, Anschutz sold the combined company to the Union Pacific Railroad, creating the largest railroad network in the United States. The Anschutz sale was partly in response to the earlier merger of Burlington Northern and Santa Fe which formed the large Burlington Northern and Santa Fe Railway (BNSF), Union Pacific's principal competitor in western U.S. railroading. Both Union Pacific and BNSF have extensive freight operations in Colorado.
Colorado's freight railroad network consists of 2,688 miles of Class I trackage. It is integral to the U.S. economy, being a critical artery for the movement of energy, agriculture, mining, and industrial commodities as well as general freight and manufactured products between the East and Midwest and the Pacific coast states.
Government and politics.
State government.
Like the federal government and all other U.S. states, Colorado's state constitution provides for three branches of government: the legislative, the executive, and the judicial branches.
The Governor of Colorado heads the state's executive branch. The current governor is John Hickenlooper, a Democrat. Colorado's other statewide elected executive officers are the Lieutenant Governor of Colorado (elected on a ticket with the Governor), Secretary of State of Colorado, Colorado State Treasurer, and Attorney General of Colorado, all of whom serve four-year terms.
The seven-member Colorado Supreme Court is the highest judicial court in the state.
The state legislative body is the Colorado General Assembly, which is made up of two houses, the House of Representatives and the Senate. The House has 65 members and the Senate has 35. , the Democratic Party holds an 18 to 17 majority in the Senate and a 37 to 28 majority in the House.
Most Coloradans are native to other states (nearly 60% according to the 2000 census), and this is illustrated by the fact that the state did not have a native-born governor from 1975 (when John David Vanderhoof left office) until 2007, when Bill Ritter took office; his election the previous year marked the first electoral victory for a native-born Coloradan in a gubernatorial race since 1958 (Vanderhoof had ascended from the Lieutenant Governorship when John Arthur Love was given a position in Richard Nixon's administration in 1973). 
Counties.
The State of Colorado is divided into 64 counties. Counties are important units of government in Colorado since the state has no secondary civil subdivisions such as townships. Two of these counties, the City and County of Denver and the City and County of Broomfield, have consolidated city and county governments.
Nine Colorado counties have a population in excess of 250,000 each, while eight Colorado counties have a population of less than 2,500 each. The ten most populous Colorado counties are all located in the Front Range Urban Corridor.
Metropolitan areas.
The United States Office of Management and Budget (OMB) has defined one Combined Statistical Area (CSA), seven Metropolitan Statistical Areas (MSAs), and seven Micropolitan Statistical Areas (μSAs) in the state of Colorado.
The most populous of the 14 Core Based Statistical Areas in Colorado is the Denver-Aurora-Broomfield, CO Metropolitan Statistical Area. This area had an estimated population of 2,599,504 on July 1, 2011, an increase of +2.20% since the 2010 United States Census.
The more extensive Denver-Aurora-Boulder, CO Combined Statistical Area had an estimated population of 3,157,520 on July 1, 2011, an increase of +2.16% since the 2010 United States Census.
The most populous extended metropolitan region in Rocky Mountain Region is the Front Range Urban Corridor along the northeast face of the Southern Rocky Mountains. This region with Denver at its center had an estimated population of 4,423,936 on July 1, 2011, an increase of +2.06% since the 2010 United States Census.
Municipalities.
The state of Colorado currently has 271 active incorporated municipalities, including 196 towns, 73 cities, and two consolidated city and county governments.
Colorado municipalities operate under one of five types of municipal governing authority. Colorado has one town with a territorial charter, 160 statutory towns, 12 statutory cities, 96 home rule municipalities (61 cities and 35 towns), and 2 consolidated city and county governments.
Unincorporated communities.
In addition to its 271 municipalities, Colorado has 187 unincorporated United States census designated places and many other small communities.
Special districts.
The state of Colorado has more than 3,000 districts with taxing authority. These districts may provide schools, law enforcement, fire protection, water, sewage, drainage, irrigation, transportation, recreation, infrastructure, cultural facilities, business support, redevelopment, or other services.
Some of these districts have authority to levy sales tax and well as property tax and use fees. This has led to a hodgepodge of sales tax and property tax rates in Colorado. There are some street intersections in Colorado with a different sales tax rate on each corner, sometimes substantially different.
Some of the more notable Colorado districts are:
Federal politics.
Colorado is considered a swing state in both state and federal elections. Coloradans have elected 17 Democrats and 12 Republicans to the governorship in the last 100 years. In presidential politics, Colorado was considered a reliably Republican state during the post-World War II era, only voting for the Democratic candidate in 1964 and 1992. However, it became a competitive swing state by the turn of the century, and voted consecutively for Democrat Barack Obama in 2008 and 2012.
Colorado politics has the contrast of conservative cities such as Colorado Springs and liberal cities such as Boulder and Denver. Democrats are strongest in metropolitan Denver, the college towns of Fort Collins and Boulder, southern Colorado (including Pueblo), and a few western ski resort counties. The Republicans are strongest in the Eastern Plains, Colorado Springs, Greeley, and far Western Colorado near Grand Junction.
The state of Colorado is represented by its two United States Senators:
Colorado is represented by seven Representatives to the United States House of Representatives:
Significant bills passed in Colorado.
On the November 8, 1932 ballot, Colorado approved the repeal of alcohol prohibition more than a year before the federal government passed the Twenty-first Amendment to the United States Constitution.
In 2012, voters amended the state constitution to protect "personal use" of marijuana for adults, establishing a framework to regulate cannabis in a manner similar to alcohol. The first recreational marijuana shops in Colorado, and by extension the United States, opened their doors on January 1, 2014.
Education.
Colleges and universities in Colorado:
Military installations.
Colorado is currently the home of seven major military bases and installations. 
Former Military installations and outposts include
Protected areas.
Colorado is home to four national parks, seven national monuments, two national recreation areas, two national historic sites, three national historic trails, one national scenic trail, 11 national forests, two national grasslands, 41 national wilderness areas, two national conservation areas, eight national wildlife refuges, 44 state parks, 307 state wildlife areas, and numerous other scenic, historic, and recreational areas.
Units of the National Park System in Colorado:
Sports.
Colorado has five major professional sports leagues, all based in the Denver metropolitan area. Colorado is the least populous state with a franchise in each of the major professional sports leagues.
College athletics.
The following universities and colleges participate in the National Collegiate Athletic Association Division I. The most popular college sports program is the University of Colorado Buffaloes, who used to play in the Big-12 but now play in the Pac-12.

</doc>
<doc id="5401" url="http://en.wikipedia.org/wiki?curid=5401" title="Carboniferous">
Carboniferous

The Carboniferous is a geologic period and system that extends from the end of the Devonian Period, about 358.9 ± 0.4 million years ago, to the beginning of the Permian Period, about 298.9 ± 0.15 Ma. The name "Carboniferous" means "coal-bearing" and derives from the Latin words "carbō" (“coal”) and "ferō" (“I bear, I carry”), and was coined by geologists William Conybeare and William Phillips in 1822. Based on a study of the British rock succession, it was the first of the modern 'system' names to be employed, and reflects the fact that many coal beds were formed globally during this time. The Carboniferous is often treated in North America as two geological periods, the earlier Mississippian and the later Pennsylvanian.
Terrestrial life was well established by the Carboniferous period. Amphibians were the dominant land vertebrates, of which one branch would eventually evolve into reptiles, the first fully terrestrial vertebrates. Arthropods were also very common, and many (such as "Meganeura"), were much larger than those of today. Vast swaths of forest covered the land, which would eventually be laid down and become the coal beds characteristic of the Carboniferous system. The atmospheric content of oxygen also reached their highest levels in history during the period, 35% compared with 21% today. This increased the atmospheric density by a third over today’s value. A minor marine and terrestrial extinction event occurred in the middle of the period, caused by a change in climate. The later half of the period experienced glaciations, low sea level, and mountain building as the continents collided to form Pangaea.
Subdivisions.
In the United States the Carboniferous is usually broken into Mississippian (earlier) and Pennsylvanian (later) Periods. The Mississippian is about twice as long as the Pennsylvanian, but due to the large thickness of coal bearing deposits with Pennsylvanian ages in Europe and North America, the two subperiods were long thought to have been more or less equal. The faunal stages from youngest to oldest, together with some of their subdivisions, are:
Late Pennsylvanian: Gzhelian (most recent)
Late Pennsylvanian: Kasimovian
Middle Pennsylvanian: Moscovian
Early Pennsylvanian: Bashkirian / Morrowan
Late Mississippian: Serpukhovian
Middle Mississippian: Visean
Early Mississippian: Tournaisian (oldest)
Paleogeography.
A global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread epicontinental seas and carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwanaland was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush coal swamps flourished within 30 degrees of the northernmost glaciers.
A mid-Carboniferous drop in sea level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea level drop and the associated unconformity in North America separate the Mississippian subperiod from the Pennsylvanian subperiod. This happened about 318 million years ago, at the onset of the Permo-Carboniferous Glaciation.
The Carboniferous was a time of active mountain-building, as the supercontinent Pangaea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America–Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural mountains. Most of the Mesozoic supercontinent of Pangea was now assembled, although North China (which would collide in the Latest Carboniferous), and South China continents were still separated from Laurasia. The Late Carboniferous Pangaea was shaped like an "O."
There were two major oceans in the Carboniferous—Panthalassa and Paleo-Tethys, which was inside the "O" in the Carboniferous Pangaea. Other minor oceans were shrinking and eventually closed - Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean (closed by North China collision with Siberia/Kazakhstania).
Climate and weather.
The early part of the Carboniferous was mostly warm; in the later part of the Carboniferous, the climate cooled. Glaciations in Gondwana, triggered by Gondwana's southward movement, continued into the Permian and because of the lack of clear markers and breaks, the deposits of this glacial period are often referred to as Permo-Carboniferous in age.
The thicker atmosphere and stronger coriolis effect due to Earth's faster rotation (a day lasted for less than 23 hours in early Carboniferous) created significantly stronger winds than today.
The cooling and drying of the climate led to the Carboniferous Rainforest Collapse (CRC). Tropical rainforests fragmented and then were eventually devastated by climate change.
Rocks and coal.
Carboniferous rocks in Europe and eastern North America largely consist of a repeated sequence of limestone, sandstone, shale and coal beds. In North America, the early Carboniferous is largely marine limestone, which accounts for the division of the Carboniferous into two periods in North American schemes. The Carboniferous coal beds provided much of the fuel for power generation during the Industrial Revolution and are still of great economic importance.
The large coal deposits of the Carboniferous primarily owe their existence to two factors. The first of these is the appearance of bark-bearing trees (and in particular the evolution of the bark fiber lignin). The second is the lower sea levels that occurred during the Carboniferous as compared to the Devonian period. This allowed for the development of extensive lowland swamps and forests in North America and Europe. Based on a genetic analysis of mushroom fungi, David Hibbett and colleagues proposed that large quantities of wood were buried during this period because animals and decomposing bacteria had not yet evolved that could effectively digest the tough lignin. It is assumed that fungi that could break it down did not arise before the end of the period, making future coal formation much more rare. The Carboniferous trees made extensive use of lignin. They had bark to wood ratios of 8 to 1, and even as high as 20 to 1. This compares to modern values less than 1 to 4. This bark, which must have been used as support as well as protection, probably had 38% to 58% lignin. Lignin is insoluble, too large to pass through cell walls, too heterogeneous for specific enzymes, and toxic, so that few organisms other than Basidiomycetes fungi can degrade it. It can not be oxidized in an atmosphere of less than 5% oxygen. It can linger in soil for thousands of years and inhibits decay of other substances. Probably the reason for its high percentages is protection from insect herbivory in a world containing very effective insect herbivores, but nothing remotely as effective as modern insectivores and probably many fewer poisons than currently. In any case coal measures could easily have made thick deposits on well drained soils as well as swamps. The extensive burial of biologically produced carbon led to a buildup of surplus oxygen in the atmosphere; estimates place the peak oxygen content as high as 35%, compared to 21% today. This oxygen level probably increased wildfire activity, as well as resulted in gigantism of insects and amphibians—creatures whose size is constrained by respiratory systems that are limited in their ability to diffuse oxygen.
In eastern North America, marine beds are more common in the older part of the period than the later part and are almost entirely absent by the late Carboniferous. More diverse geology existed elsewhere, of course. Marine life is especially rich in crinoids and other echinoderms. Brachiopods were abundant. Trilobites became quite uncommon. On land, large and diverse plant populations existed. Land vertebrates included large amphibians.
Life.
Plants.
Early Carboniferous land plants, some of which were preserved in coal balls, were very similar to those of the preceding Late Devonian, but new groups also appeared at this time.
The main Early Carboniferous plants were the Equisetales (horse-tails), Sphenophyllales (vine-like plants), Lycopodiales (club mosses), Lepidodendrales (scale trees), Filicales (ferns), Medullosales (informally included in the "seed ferns", an artificial assemblage of a number of early gymnosperm groups) and the Cordaitales. These continued to dominate throughout the period, but during late Carboniferous, several other groups, Cycadophyta (cycads), the Callistophytales (another group of "seed ferns"), and the Voltziales (related to and sometimes included under the conifers), appeared.
The Carboniferous lycophytes of the order Lepidodendrales, which are cousins (but not ancestors) of the tiny club-moss of today, were huge trees with trunks 30 meters high and up to 1.5 meters in diameter. These included "Lepidodendron" (with its fruit cone called Lepidostrobus), "Halonia", "Lepidophloios" and "Sigillaria". The roots of several of these forms are known as Stigmaria. Unlike present day trees, their secondary growth took place in the cortex, which also provided stability, instead of the xylem. The Cladoxylopsids were large trees, that were ancestors of ferns, first arising in the Carboniferous.
The fronds of some Carboniferous ferns are almost identical with those of living species. Probably many species were epiphytic. Fossil ferns and "seed ferns" include "Pecopteris", "Cyclopteris", "Neuropteris", "Alethopteris", and "Sphenopteris"; "Megaphyton" and "Caulopteris" were tree ferns.
The Equisetales included the common giant form "Calamites", with a trunk diameter of 30 to and a height of up to . "Sphenophyllum" was a slender climbing plant with whorls of leaves, which was probably related both to the calamites and the lycopods.
"Cordaites", a tall plant (6 to over 30 meters) with strap-like leaves, was related to the cycads and conifers; the catkin-like inflorescence, which bore yew-like berries, is called "Cardiocarpus". These plants were thought to live in swamps and mangroves. True coniferous trees ("Walchia", of the order Voltziales) appear later in the Carboniferous, and preferred higher drier ground.
Marine invertebrates.
In the oceans the most important marine invertebrate groups are the Foraminifera, corals, Bryozoa, Ostracoda, brachiopods, ammonoids, hederelloids, microconchids and echinoderms (especially crinoids). For the first time foraminifera take a prominent part in the marine faunas. The large spindle-shaped genus "Fusulina" and its relatives were abundant in what is now Russia, China, Japan, North America; other important genera include "Valvulina", "Endothyra", "Archaediscus", and "Saccammina" (the latter common in Britain and Belgium). Some Carboniferous genera are still extant.
The microscopic shells of radiolarians are found in cherts of this age in the Culm of Devon and Cornwall, and in Russia, Germany and elsewhere. Sponges are known from spicules and anchor ropes, and include various forms such as the Calcispongea "Cotyliscus" and "Girtycoelia", the demosponge "Chaetetes", and the genus of unusual colonial glass sponges "Titusvillia".
Both reef-building and solitary corals diversify and flourish; these include both rugose (for example, "Caninia", "Corwenia", "Neozaphrentis"), heterocorals, and tabulate (for example, "Chladochonus", "Michelinia") forms. Conularids were well represented by "Conularia"
Bryozoa are abundant in some regions; the fenestellids including "Fenestella", "Polypora", and "Archimedes", so named because it is in the shape of an Archimedean screw. Brachiopods are also abundant; they include productids, some of which (for example, "Gigantoproductus") reached very large (for brachiopods) size and had very thick shells, while others like "Chonetes" were more conservative in form. Athyridids, spiriferids, rhynchonellids, and terebratulids are also very common. Inarticulate forms include "Discina" and "Crania". Some species and genera had a very wide distribution with only minor variations.
Annelids such as "Serpulites" are common fossils in some horizons. Among the mollusca, the bivalves continue to increase in numbers and importance. Typical genera include "Aviculopecten", "Posidonomya", "Nucula", "Carbonicola", "Edmondia", and "Modiola" Gastropods are also numerous, including the genera "Murchisonia", "Euomphalus", "Naticopsis". Nautiloid cephalopods are represented by tightly coiled nautilids, with straight-shelled and curved-shelled forms becoming increasingly rare. Goniatite ammonoids are common.
Trilobites are rarer than in previous periods, on a steady trend towards extinction, represented only by the proetid group. Ostracoda, a class of crustaceans, were abundant as representatives of the meiobenthos; genera included "Amphissites", "Bairdia", "Beyrichiopsis", "Cavellina", "Coryellina", "Cribroconcha", "Hollinella", "Kirkbya", "Knoxiella", and "Libumella".
Amongst the echinoderms, the crinoids were the most numerous. Dense submarine thickets of long-stemmed crinoids appear to have flourished in shallow seas, and their remains were consolidated into thick beds of rock. Prominent genera include "Cyathocrinus", "Woodocrinus", and "Actinocrinus". Echinoids such as "Archaeocidaris" and "Palaeechinus" were also present. The blastoids, which included the Pentreinitidae and Codasteridae and superficially resembled crinoids in the possession of long stalks attached to the seabed, attain their maximum development at this time.
Freshwater and lagoonal invertebrates.
Freshwater Carboniferous invertebrates include various bivalve molluscs that lived in brackish or fresh water, such as "Anthraconaia", "Naiadites", and "Carbonicola"; diverse crustaceans such as "Candona", "Carbonita", "Darwinula", "Estheria", "Acanthocaris", "Dithyrocaris", and "Anthrapalaemon".
The Eurypterids were also diverse, and are represented by such genera as "Eurypterus", "Glyptoscorpius", "Anthraconectes", "Megarachne" (originally misinterpreted as a giant spider) and the specialised very large "Hibbertopterus". Many of these were amphibious.
Frequently a temporary return of marine conditions resulted in marine or brackish water genera such as "Lingula", Orbiculoidea, and "Productus" being found in the thin beds known as marine bands.
Terrestrial invertebrates.
Fossil remains of air-breathing insects, myriapods and arachnids are known from the late Carboniferous, but so far not from the early Carboniferous. The first true priapulids appeared during this period. Their diversity when they do appear, however, shows that these arthropods were both well developed and numerous. Their large size can be attributed to the moistness of the environment (mostly swampy fern forests) and the fact that the oxygen concentration in the Earth's atmosphere in the Carboniferous was much higher than today. This required less effort for respiration and allowed arthropods to grow larger with the up to 2.6 metres long millipede-like "Arthropleura" being the largest known land invertebrate of all time. Among the insect groups are the huge predatory Protodonata (griffinflies), among which was "Meganeura", a giant dragonfly-like insect and with a wingspan of ca. — the largest flying insect ever to roam the planet. Further groups are the Syntonopterodea (relatives of present-day mayflies), the abundant and often large sap-sucking Palaeodictyopteroidea, the diverse herbivorous Protorthoptera, and numerous basal Dictyoptera (ancestors of cockroaches). Many insects have been obtained from the coalfields of Saarbrücken and Commentry, and from the hollow trunks of fossil trees in Nova Scotia. Some British coalfields have yielded good specimens: "Archaeoptitus", from the Derbyshire coalfield, had a spread of wing extending to more than 35 cm; some specimens ("Brodia") still exhibit traces of brilliant wing colors. In the Nova Scotian tree trunks land snails ("Archaeozonites", "Dendropupa") have been found.
Fish.
Many fish inhabited the Carboniferous seas; predominantly Elasmobranchs (sharks and their relatives). These included some, like "Psammodus", with crushing pavement-like teeth adapted for grinding the shells of brachiopods, crustaceans, and other marine organisms. Other sharks had piercing teeth, such as the Symmoriida; some, the petalodonts, had peculiar cycloid cutting teeth. Most of the sharks were marine, but the Xenacanthida invaded fresh waters of the coal swamps. Among the bony fish, the Palaeonisciformes found in coastal waters also appear to have migrated to rivers. Sarcopterygian fish were also prominent, and one group, the Rhizodonts, reached very large size.
Most species of Carboniferous marine fish have been described largely from teeth, fin spines and dermal ossicles, with smaller freshwater fish preserved whole.
Freshwater fish were abundant, and include the genera "Ctenodus", "Uronemus", "Acanthodes", "Cheirodus", and "Gyracanthus".
Sharks (especially the "Stethacanthids") underwent a major evolutionary radiation during the Carboniferous. It is believed that this evolutionary radiation occurred because the decline of the placoderms at the end of the Devonian period caused many environmental niches to become unoccupied and allowed new organisms to evolve and fill these niches. As a result of the evolutionary radiation carboniferous sharks assumed a wide variety of bizarre shapes including "Stethacanthus" which possessed a flat brush-like dorsal fin with a patch of denticles on its top. "Stethacanthus"' unusual fin may have been used in mating rituals.
Tetrapods.
Carboniferous amphibians were diverse and common by the middle of the period, more so than they are today; some were as long as 6 meters, and those fully terrestrial as adults had scaly skin. They included a number of basal tetrapod groups classified in early books under the Labyrinthodontia. These had long bodies, a head covered with bony plates and generally weak or undeveloped limbs. The largest were over 2 meters long. They were accompanied by an assemblage of smaller amphibians included under the Lepospondyli, often only about long. Some Carboniferous amphibians were aquatic and lived in rivers ("Loxomma", "Eogyrinus", "Proterogyrinus"); others may have been semi-aquatic ("Ophiderpeton", "Amphibamus", "Hyloplesion") or terrestrial ("Dendrerpeton", "Tuditanus", "Anthracosaurus").
The Carboniferous Rainforest Collapse slowed the evolution of amphibians who could not survive as well in the cooler, drier conditions. Reptiles, however prospered due to specific key adaptations. One of the greatest evolutionary innovations of the Carboniferous was the amniote egg, which allowed for the further exploitation of the land by certain tetrapods. These included the earliest sauropsid reptiles ("Hylonomus"), and the earliest known synapsid ("Archaeothyris"). These small lizard-like animals quickly gave rise to many descendants. The amniote egg allowed these ancestors of all later birds, mammals, and reptiles to reproduce on land by preventing the desiccation, or drying-out, of the embryo inside.
Reptiles underwent a major evolutionary radiation in response to the drier climate that preceded the rainforest collapse. By the end of the Carboniferous period, amniotes had already diversified into a number of groups, including protorothyridids, captorhinids, araeoscelids, and several families of pelycosaurs.
Fungi.
Because plants and animals were growing in size and abundance in this time (for example, "Lepidodendron"), land fungi diversified further. Marine fungi still occupied the oceans. All modern classes of fungi were present in the Late Carboniferous (Pennsylvanian Epoch).
Extinction events.
Romer's gap.
The first 15 million years of the Carboniferous had very limited terrestrial fossils. This gap in the fossil record is called Romer's gap after the American palaentologist Alfred Romer. While it has long been debated whether the gap is a result of fossilisation or relates to an actual event, recent work indicates the gap period saw a drop in atmospheric oxygen levels, indicating some sort of ecological collapse. The gap saw the demise of the Devonian fish-like ichthyostegalian labyrinthodonts, and the rise of the more advanced temnospondyl and reptiliomorphan amphibians that so typify the Carboniferous terrestrial vertebrate fauna.
Carboniferous rainforest collapse.
Before the end of the Carboniferous Period, an extinction event occurred. On land this event is referred to as the Carboniferous Rainforest Collapse (CRC). Vast tropical rainforests collapsed suddenly as the climate changed from hot and humid to cool and arid. This was likely caused by intense glaciation and a drop in sea levels.
The new climatic conditions were not favorable to the growth of rainforest and the animals within them. Rainforests shrank into isolated islands, surrounded by seasonally dry habitats. Towering lycopsid forests with a heterogeneous mixture of vegetation were replaced by much less diverse tree-fern dominated flora.
Amphibians, the dominant vertebrates at the time, fared poorly through this event with large losses in biodiversity; reptiles continued to diversify due to key adaptations that let them survive in the drier habitat, specifically the hard-shelled egg and scales both of which retain water better than their amphibian counterparts.

</doc>
<doc id="5403" url="http://en.wikipedia.org/wiki?curid=5403" title="Comoros">
Comoros

The Comoros (; , '), officially the Union of the Comoros (Comorian: "Udzima wa Komori," , '), is a sovereign archipelago island nation in the Indian Ocean, located at the northern end of the Mozambique Channel off the eastern coast of Africa, between northeastern Mozambique and northwestern Madagascar. Other countries near the Comoros are Tanzania to the northwest and the Seychelles to the northeast. Its capital is Moroni, on Grande Comore.
At , excluding the contested island of Mayotte, the Comoros is the third-smallest African nation by area. The population, excluding Mayotte, is estimated at 798,000. The name "Comoros" derives from the Arabic word ("moon"). As a nation formed at a crossroads of many civilizations, the archipelago is noted for its diverse culture and history. The Union of the Comoros has three official languagesthough French is the sole official language on Mayotte.
Officially, in addition to many smaller islands, the country consists of the four major islands in the volcanic Comoros archipelago: northwesternmost Grande Comore (Ngazidja); Mohéli (Mwali); Anjouan (Nzwani); and southeasternmost Mayotte (Maore). Mayotte, however, has never been administered by an independent Comoros government and continues to be administered by France (currently as an overseas department) as it was the only island in the archipelago that voted against independence in 1974. France has since vetoed United Nations Security Council resolutions that would affirm Comorian sovereignty over the island. In addition, a referendum on the question of Mayotte becoming an overseas department of France in 2011 was held on 29 March 2009 and passed overwhelmingly.
The Comoros is the only state to be a member of the African Union, Francophonie, Organisation of Islamic Cooperation, Arab League (of which it is the southernmost state, being the only member of the Arab League which is entirely within the Southern Hemisphere) and the Indian Ocean Commission. Since independence in 1975, the country has experienced numerous "coups d'état" and, as of 2008, about half the population lives below the international poverty line of US$1.25 a day.
History.
Precolonial peoples.
The first human inhabitants of the Comoros Islands are thought to have been Arab, African and Austronesian settlers who traveled to the islands by boat. These people arrived no later than the sixth century AD, the date of the earliest known archaeological site, found on Nzwani, although settlement beginning as early as the first century has been postulated. The islands of Comoros were populated by a succession of diverse groups from the coast of Africa, the Arabian Peninsula and the Persian Gulf, the Malay Archipelago, and Madagascar. Bantu-speaking settlers reached the islands as a part of the greater Bantu expansion that took place in Africa throughout the first millennium.
According to pre-Islamic mythology, a jinni (spirit) dropped a jewel, which formed a great circular inferno. This became the Karthala volcano, which created the island of Comoros.
Development of the Comoros was divided into phases. The earliest reliably recorded phase is the Dembeni phase (ninth to tenth centuries), during which each island maintained a single, central village. From the eleventh to the fifteenth centuries, trade with the island of Madagascar and merchants from the Middle East flourished, smaller villages emerged, and existing towns expanded. Many Comorians can trace their genealogies to ancestors from Yemen, mainly Hadramawt, and Oman.
Medieval Comoros.
According to legend, in 632, upon hearing of Islam, islanders are said to have dispatched an emissary, Mtswa-Mwindza, to Mecca—but by the time he arrived there, the Islamic prophet Muhammad had died. Nonetheless, after a stay in Mecca, he returned to Ngazidja and led the gradual conversion of his islanders to Islam.
Among the earliest accounts of East Africa, the works of Al-Masudi describe early Islamic trade routes, and how the coast and islands were frequently visited by Muslims including Persian and Arab merchants and sailors in search of coral, ambergris, ivory, tortoiseshell, gold and slaves. They also brought Islam to the people of the Zanj including Comoros. As the importance of Comoros grew along the East African coast, both small and large mosques were constructed. Despite its distance from the coast, Comoros is situated along the "Swahili Coast" in East Africa. It was a major hub of trade and an important location in a network of trading towns that included Kilwa, in presnt-day Tanzania, Sofala (an outlet for Zimbabwean gold), in Mozambique, and Mombasa in Kenya.
After the arrival of the Portuguese and the collapse of East African sultanates, the powerful Omani Sultan Saif bin Sultan began to defeat the Dutch and the Portuguese. His successor Said bin Sultan increased Omani Arab influence in region, moving his administration to nearby Zanzibar, which came under Omani rule. Nevertheless, the Comoros remained independent, and although the three smaller islands were usually politically unified, the largest island, Ngazidja, was divided into a number of autonomous kingdome ("ntsi").
By the time Europeans showed interest in the Comoros, the islanders were well placed to take advantage of their needs, initially supplying ships of the route to India and, later, slaves to the plantation islands in the Mascarenes.
European contact and French colonization.
Portuguese explorers first visited the archipelago in 1505 and the islands provided provisions to the Portuguese fort at Mozambique throughout the 16th century.
In 1793, Malagasy warriors from Madagascar first started raiding the islands for slaves. On Comoros, it was estimated in 1865 that as much as 40% of the population consisted of slaves. France first established colonial rule in the Comoros in 1841. The first French colonists landed in Mayotte, and Andrian Tsouli, the Malagasy King of Mayotte, signed the Treaty of April 1841, which ceded the island to the French authorities.
The Comoros served as a way station for merchants sailing to the Far East and India until the opening of the Suez Canal significantly reduced traffic passing through the Mozambique Channel. The native commodities exported by the Comoros were coconuts, cattle and tortoiseshell. French settlers, French-owned companies, and wealthy Arab merchants established a plantation-based economy that used about one-third of the land for export crops. After its annexation, France converted Mayotte into a sugar plantation colony. The other islands were soon transformed as well, and the major crops of ylang-ylang, vanilla, coffee, cocoa bean, and sisal were introduced.
In 1886, Mohéli was placed under French protection by its Sultan Mardjani Abdou Cheikh. That same year, despite having no authority to do so, Sultan Said Ali of Bambao, one of the sultanates on Ngazidja, placed the island under French protection in exchange for French support of his claim to the entire island, which he retained until his abdication in 1910. In 1908 The islands were unified under a single administration ("Colonie de Mayotte et Dépendances") and placed under the authority of the French colonial governor general of Madagascar. In 1909, Sultan Said Muhamed of Anjouan abdicated in favor of French rule. In 1912 the colony and the protectorates were abolished and the islands became a province of the colony of Madagascar.
Agreement was reached with France in 1973 for Comoros to become independent in 1978. The deputies of Mayotte abstained. Referendums were held on all four of the islands. Three voted for independence by large margins, while Mayotte voted against, and remains under French administration. On 6 July 1975, however, the Comorian parliament passed a unilateral resolution declaring independence. Ahmed Abdallah proclaimed the independence of the Comorian State ("État comorien"; دولة القمر) and became its first president.
Independence.
The next 30 years were a period of political turmoil. On 3 August 1975, president Ahmed Abdallah was removed from office in an armed coup and replaced with United National Front of the Comoros (UNF) member Prince Said Mohammed Jaffar. Months later, in January 1976, Jaffar was ousted in favor of his Minister of Defense Ali Soilih.
At this time, the population of Mayotte voted against independence from France in two referenda. The first, held in December 1974, won 63.8% support for maintaining ties with France, while the second, held in February 1976, confirmed that vote with an overwhelming 99.4%. The three remaining islands, ruled by President Soilih, instituted a number of socialist and isolationist policies that soon strained relations with France. On 13 May 1978, Bob Denard returned to overthrow President Soilih and reinstate Abdallah with the support of the French, Rhodesian and South African governments. During Soilih's brief rule, he faced seven additional coup attempts until he was finally forced from office and killed.
In contrast to Soilih, Abdallah's presidency was marked by authoritarian rule and increased adherence to traditional Islam and the country was renamed the Federal Islamic Republic of Comoros ("République Fédérale Islamique des Comores"; جمهورية القمر الإتحادية الإسلامية ). Abdallah continued as president until 1989 when, fearing a probable coup d'état, he signed a decree ordering the Presidential Guard, led by Bob Denard, to disarm the armed forces. Shortly after the signing of the decree, Abdallah was allegedly shot dead in his office by a disgruntled military officer, though later sources claim an antitank missile was launched into his bedroom and killed him. Although Denard was also injured, it is suspected that Abdallah's killer was a soldier under his command.
A few days later, Bob Denard was evacuated to South Africa by French paratroopers. Said Mohamed Djohar, Soilih's older half-brother, then became president, and served until September 1995, when Bob Denard returned and attempted another coup. This time France intervened with paratroopers and forced Denard to surrender. The French removed Djohar to Reunion, and the Paris-backed Mohamed Taki Abdulkarim became president by election. He led the country from 1996, during a time of labor crises, government suppression, and secessionist conflicts, until his death November 1998. He was succeeded by Interim President Tadjidine Ben Said Massounde.
The islands of Anjouan and Mohéli declared their independence from the Comoros in 1997, in an attempt to restore French rule. But France rejected their request, leading to bloody confrontations between federal troops and rebels. In April 1999, Colonel Azali Assoumani, Army Chief of Staff, seized power in a bloodless coup, overthrowing the Interim President Massounde, citing weak leadership in the face of the crisis. This was the Comoros' 18th coup, or attempted coup d'état since independence in 1975. Azali, however, failed to consolidate power and reestablish control over the islands, which was the subject of international criticism. The African Union, under the auspices of President Thabo Mbeki of South Africa, imposed sanctions on Anjouan to help broker negotiations and effect reconciliation. The official name of the country was changed to the Union of the Comoros and a new system of political autonomy was instituted for each island, plus a union government for the three islands was added.
Azali stepped down in 2002 to run in the democratic election of the President of the Comoros, which he won. Under ongoing international pressure, as a military ruler who had originally come to power by force, and was not always democratic while in office, Azali led the Comoros through constitutional changes that enabled new elections. A "Loi des compétences" law was passed in early 2005 that defines the responsibilities of each governmental body, and is in the process of implementation. The elections in 2006 were won by Ahmed Abdallah Mohamed Sambi, a Sunni Muslim cleric nicknamed the "Ayatollah" for his time spent studying Islam in Iran. Azali honored the election results, thus allowing the first peaceful and democratic exchange of power for the archipelago.
Colonel Mohammed Bacar, a French-trained former gendarme, seized power as President in Anjouan in 2001. He staged a vote in June 2007 to confirm his leadership that was rejected as illegal by the Comoros federal government and the African Union. On 25 March 2008 hundreds of soldiers from the African Union and Comoros seized rebel-held Anjouan, generally welcomed by the population: there have been reports of hundreds, if not thousands, of people tortured during Bacar's tenure.
Some rebels were killed and injured, but there are no official figures. At least 11 civilians were wounded. Some officials were imprisoned. Bacar fled in a speedboat to the French Indian Ocean territory of Mayotte to seek asylum. Anti-French protests followed in Comoros (see 2008 invasion of Anjouan).
Since independence from France, the Comoros experienced more than 20 coups or attempted coups.
Following elections in late 2010, former Vice-President Ikililou Dhoinine was inaugurated as President on 26 May 2011. A member of the ruling party, Dhoinine was supported in the election by the incumbent President Ahmed Abdallah Mohamed Sambi. Dhoinine, a pharmacist by training, is the first President of Comoros from the island of Mohéli.
Geography.
The Comoros is formed by Ngazidja (Grande Comore), Mwali (Mohéli), and Nzwani (Anjouan), three major islands in the Comoros Archipelago, as well as many minor islets. The islands are officially known by their Comorian language names, though international sources still use their French names (given in parentheses above). The capital and largest city, Moroni, is located on Ngazidja. The archipelago is situated in the Indian Ocean, in the Mozambique Channel, between the African coast (nearest to Mozambique and Tanzania) and Madagascar, with no land borders.
At , it is one of the smallest countries in the world. The Comoros also has claim to of territorial seas. The interiors of the islands vary from steep mountains to low hills. The climate is generally tropical and mild, and the two major seasons are distinguishable by their relative raininess. The temperature reaches an average of in March, the hottest month in the rainy season (called kashkazi, December to April), and an average low of in the cool, dry season (kusi, May to November). The islands are rarely subject to cyclones.
Ngazidja is the largest of the Comoros Archipelago, approximately equal in area to the other islands combined. It is also the most recent island, and therefore has rocky soil. The island's two volcanoes, Karthala (active) and La Grille (dormant), and the lack of good harbors are distinctive characteristics of its terrain. Mwali, with its capital at Fomboni, is the smallest of the four major islands. Nzwani, whose capital is Mutsamudu, has a distinctive triangular shape caused by three mountain chains, Sima, Nioumakele, and Jimilime, emanating from a central peak, Ntringi ().
The islands of the Comoros Archipelago were formed by volcanic activity. Mount Karthala, an active shield volcano located on Ngazidja, is the country's highest point, at 2,361 m or It contains the Comoros' largest patch of its disappearing rainforest. Karthala is currently one of the most active volcanoes in the world, with a minor eruption in May 2006, and prior eruptions as recently as April 2005 and 1991. In the 2005 eruption, which lasted from 17 to 19 April, 40,000 citizens were evacuated, and the crater lake in the volcano's caldera was destroyed.
The Comoros also lays claim to the Glorioso Islands, comprising Grande Glorieuse, Île du Lys, Wreck Rock, South Rock, Verte Rocks (three islets), and three unnamed islets, one of France's "Îles Éparses or Îles éparses de l'océan indien" (Scattered islands in the Indian Ocean) possessions. The Glorioso Islands were administered by the colonial Comoros before 1975, and are therefore sometimes considered part of the Comoros Archipelago. Banc du Geyser, a former island in the Comoros Archipelago, now submerged, is geographically located in the "Îles Éparses", but was annexed by Madagascar in 1976 as an unclaimed territory. The Comoros now claims it as part of its exclusive economic zone.
The Comoros constitute an ecoregion in their own right, Comoros forests.
Government.
Politics of the Comoros takes place in a framework of a federal presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. The Constitution of the Union of the Comoros was ratified by referendum on 23 December 2001, and the islands' constitutions and executives were elected in the following months. It had previously been considered a military dictatorship, and the transfer of power from Azali Assoumani to Ahmed Abdallah Mohamed Sambi in May 2006 was the first peaceful transfer in Comorian history.
Executive power is exercised by the government. Federal legislative power is vested in both the government and parliament. The preamble of the constitution guarantees an Islamic inspiration in governance, a commitment to human rights, and several specific enumerated rights, democracy, "a common destiny" for all Comorians. Each of the islands (according to Title II of the Constitution) has a great amount of autonomy in the Union, including having their own constitutions (or Fundamental Law), president, and Parliament. The presidency and Assembly of the Union are distinct from each of the Islands' governments. The presidency of the Union rotates between the islands. Mohéli holds the current presidency rotation, and so Ikililou Dhoinine is President of the Union; Grand Comore and Anjouan follow in four-year terms.
The Comorian legal system rests on Islamic law, an inherited French (Napoleonic Code) legal code, and customary law (mila na ntsi). Village elders, kadis or civilian courts settle most disputes. The judiciary is independent of the legislative and the executive. The Supreme Court acts as a Constitutional Council in resolving constitutional questions and supervising presidential elections. As High Court of Justice, the Supreme Court also arbitrates in cases where the government is accused of malpractice. The Supreme Court consists of two members selected by the president, two elected by the Federal Assembly, and one by the council of each island.
Around 80 percent of the central government's annual budget is spent on the country's complex electoral system which provides for a semi-autonomous government and president for each of the three islands and a rotating presidency for the overarching Union government. A referendum took place on 16 May 2009 to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. The referendum would cause each island's president to become a governor and the ministers to become councilors.
As of 2008, Comoros and Mauritania are considered by US-based organization Freedom House as the only real "electoral democracies" of the Arab world.
Also in 2008, the Comoros were ranked 14th out of 48 sub-Saharan African countries in the Ibrahim Index of African Governance. The Ibrahim Index is a comprehensive measure of African governance, based on a number of different variables which reflect the success with which governments deliver essential political goods to its citizens.
Military.
The military resources of the Comoros consist of a small standing army and a 500-member police force, as well as a 500-member defense force. A defence treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains a few senior officers presence in Comoros at government request. France maintains a small maritime base and a Foreign Legion Detachment (DLEM) on Mayotte.
Once the new government was installed in May–June 2011, an expert mission from UNREC (Lomé) came to Comoros and produced guidelines for the elaboration of a national security policy, which were discussed by different actors, notably the national defence authorities and civil society. By the end of the programme in end March 2012, a normative framework agreed upon by all entities involved in SSR will have been established. This will then have to be adopted by Parliament and implemented by the authorities.
Foreign relations.
In November 1975, Comoros became the 143rd member of the United Nations. The new nation was defined as comprising the entire archipelago, although France continues to maintain control over the island of Mayotte as an overseas department. Comoros has repeatedly pressed its claim to the island before the United Nations General Assembly, which adopted a series of resolutions under the caption "Question of the Comorian Island of Mayotte", opining that Mayotte belongs to Comoros under the principle that the territorial integrity of colonial territories should be preserved upon independence. As a practical matter, however, these resolutions have little effect and there is no foreseeable likelihood that Mayotte will become "de facto" part of Comoros without its people's consent. More recently, the Assembly has maintained this item on its agenda but deferred it from year to year without taking action. Other bodies, including the UN General Assembly, the Organization of African Unity, the Movement of Non-Aligned Countries and the Organisation of Islamic Cooperation, have similarly questioned French sovereignty over Mayotte.
Comoros also is a member of the African Union, the Arab League, the European Development Fund, the World Bank, the International Monetary Fund, the Indian Ocean Commission and the African Development Bank. On 10 April 2008, Comoros became the 179th nation to accept the Kyoto Protocol to the United Nations Framework Convention on Climate Change.
Economy.
Comoros is one of the world's poorest countries. Economic growth and poverty reduction are major priorities for the government. With a rate of 14.3%, unemployment is considered very high. Agriculture, including fishing, hunting, and forestry, is the leading sector of the economy, and 38.4% of the working population is employed in the primary sector. High population densities, as much as 1000 per square kilometer in the densest agricultural zones, for what is still a mostly rural, agricultural economy may lead to an environmental crisis in the near future, especially considering the high rate of population growth. In 2004 Comoros' real GDP growth was a low 1.9% and real GDP per capita continued to decline. These declines are explained by factors including declining investment, drops in consumption, rising inflation, and an increase in trade imbalance due in part to lowered cash crop prices, especially vanilla.
Fiscal policy is constrained by erratic fiscal revenues, a bloated civil service wage bill, and an external debt that is far above the HIPC threshold. Membership in the franc zone, the main anchor of stability, has nevertheless helped contain pressures on domestic prices.
Comoros has an inadequate transportation system, a young and rapidly increasing population, and few natural resources. The low educational level of the labor force contributes to a subsistence level of economic activity, high unemployment, and a heavy dependence on foreign grants and technical assistance. Agriculture contributes 40% to GDP, employs 80% of the labor force, and provides most of the exports. Comoros is the world's largest producer of ylang-ylang, and a large producer of vanilla.
The government is struggling to upgrade education and technical training, to privatize commercial and industrial enterprises, to improve health services, to diversify exports, to promote tourism, and to reduce the high population growth rate.
The Comoros claims the Banc du Geyser and the Glorioso Islands as part of its exclusive economic zone.
Comoros is a member of the Organization for the Harmonization of Business Law in Africa (OHADA).
Demographics.
With fewer than a million people, the Comoros is one of the least populous countries in the world, but is also one of the most densely populated, with an average of . In 2001, 34% of the population was considered urban, but that is expected to grow, since rural population growth is negative, while overall population growth is still relatively high. Almost half the population of Comoros is under the age of 15. Major urban centers include Moroni, Mutsamudu, Domoni, Fomboni, and Tsémbéhou. There are between 200,000 and 350,000 Comorians in France.
The islands of the Comoros share mostly African-Arab origins. Sunni Islam is the dominant religion, representing as much as 98% of the population. A minority of the population of Mayotte, mostly immigrants from metropolitan France, are Roman Catholic. Malagasy (Christian) and Indian (mostly Ismaili) minorities also exist, as well as minorities mostly descended from early French settlers. Chinese people are also present on Mayotte and parts of Grande Comore (especially Moroni). A small white minority of French with other European (i.e. Dutch, British and Portuguese) ancestry lives in Comoros. Most French left after independence in 1975.
The most common language in Comoros is Comorian, or "Shikomor", a language related to Swahili although with less Arabic influence than Swahili, with four different variants (Shingazidja, Shimwali, Shinzwani, and Shimaore) being spoken on each of the four islands. No official alphabet existed in 1992, but Arabic and Latin scripts are both used. French and Arabic are also official languages, along with Comorian. Arabic is widely known as a second language, being the language of Quranic teaching. French is the administrative language and the language of all non-Quranic formal education. A Malagasy language, Kibushi, is spoken by approximately a third of the population of Mayotte. About fifty-seven percent of the population is literate in the Latin script while more than 90% are literate in the Arabic script; total literacy is incorrectly estimated at 62.5%. Comorian has no native script, but both Arabic and Latin scripts are used.
There are 15 physicians per 100,000 people. The fertility rate was 4.7 per adult woman in 2004. Life expectancy at birth is 67 for females and 62 for males.
Culture.
Media.
There is a government-owned national newspaper in Comoros, Al-Watwan, published in Moroni. Radio Comoros is the national radio service and Comoros National TV is the television service.
Education.
Almost all of the educated populace of the Comoros have attended Quranic schools at some point in their lives, often before regular schooling. Here, boys and girls are taught about the Qur'an, and memorize it. Some parents specifically choose this early schooling to offset French schools children usually attend later. Since independence and the ejection of French teachers, the education system has been plagued by poor teacher training and poor results, though recent stability may allow for substantial improvements. In 2000, 44.2% of children ages 5 to 14 years were attending school. There is a general lack of facilities, equipment, qualified teachers, textbooks and other resources. Salaries for teachers are often so far in arrears that many refuse to work.
References.
This article incorporates text from the Library of Congress Country Studies, which is in the public domain.

</doc>
<doc id="5404" url="http://en.wikipedia.org/wiki?curid=5404" title="Critical philosophy">
Critical philosophy

Attributed to Immanuel Kant, the critical philosophy movement sees the primary task of philosophy as criticism rather than justification of knowledge; criticism, for Kant, meant judging as to the possibilities of knowledge before advancing to knowledge itself (from the Greek "kritike (techne)", or "art of judgment"). The initial, and perhaps even sole task of philosophers, according to this view, is not to establish and demonstrate theories about reality, but rather to subject all theories—including those about philosophy itself—to critical review, and measure their validity by how well they withstand criticism.
"Critical philosophy" is also used as another name for Kant's philosophy itself. Kant said that philosophy's proper enquiry is not about what is out there in reality, but rather about the character and foundations of experience itself. We must first judge how human reason works, and within what limits, so that we can afterwards correctly apply it to sense experience and determine whether it can be applied at all to metaphysical objects.

</doc>
<doc id="5405" url="http://en.wikipedia.org/wiki?curid=5405" title="China">
China

China (; ), officially the People's Republic of China (PRC), is a sovereign state located in East Asia, and one of two countries officially named China. It is the world's most populous country, with a population of over 1.35 billion. The PRC is a single-party state governed by the Communist Party, with its seat of government in the capital city of Beijing. It exercises jurisdiction over 22 provinces, five autonomous regions, four direct-controlled municipalities (Beijing, Tianjin, Shanghai, and Chongqing), and two mostly self-governing special administrative regions (Hong Kong and Macau). The PRC also claims the territories governed by Taiwan, a separate political entity officially known as the Republic of China (ROC), as its 23rd province, a claim which is controversial due to the complex political status of Taiwan.
Covering approximately 9.6 million square kilometers, China is the world's second-largest country by land area, and either the third or fourth-largest by total area, depending on the method of measurement. China's landscape is vast and diverse, ranging from forest steppes and the Gobi and Taklamakan deserts in the arid north to subtropical forests in the wetter south. The Himalaya, Karakoram, Pamir and Tian Shan mountain ranges separate China from South and Central Asia. The Yangtze and Yellow Rivers, the third- and sixth-longest in the world, run from the Tibetan Plateau to the densely populated eastern seaboard. China's coastline along the Pacific Ocean is long, and is bounded by the Bohai, Yellow, East and South China Seas.
The history of China goes back to the ancient civilization – one of the world's earliest – that flourished in the fertile basin of the Yellow River in the North China Plain. For millennia, China's political system was based on hereditary monarchies, known as dynasties, beginning with the semi-mythological Xia of the Yellow River basin (c. 2000 BCE). Since 221 BCE, when the Qin Dynasty first conquered several states to form a Chinese empire, the country has expanded, fractured and been reformed numerous times. The Republic of China (ROC) overthrew the last dynasty in 1911, and ruled the Chinese mainland until 1949. After the defeat of the Empire of Japan in World War II, the Communist Party defeated the nationalist Kuomintang in mainland China and established the People's Republic of China in Beijing on 1 October 1949, while the Kuomintang relocated the ROC government to its present capital of Taipei.
China had the largest and most complex economy in the world for most of the past two thousand years, during which it has seen cycles of prosperity and decline. Since the introduction of economic reforms in 1978, China has become one of the world's fastest-growing major economies. As of 2013, it is the world's second-largest economy by both nominal total GDP and purchasing power parity (PPP), and is also the world's largest exporter and importer of goods. China is a recognized nuclear weapons state and has the world's largest standing army, with the second-largest defence budget. The PRC has been a United Nations member since 1971, when it replaced the ROC as a permanent member of the U.N. Security Council. China is also a member of numerous formal and informal multilateral organizations, including the WTO, APEC, BRICS, the Shanghai Cooperation Organization, the BCIM and the G-20. China is a regional power within Asia and has been characterized as a potential superpower by a number of commentators.
Etymology.
The word "China" is derived from the Persian word "Chin" (), which is from the Sanskrit word "Cīna" (). It is first recorded in 1516 in the journal of the Portuguese explorer Duarte Barbosa. The journal was translated and published in England in 1555. The traditional theory, proposed in the 17th century by Martino Martini, is that "Cīna" is derived from "Qin" (), the westernmost of the Chinese kingdoms during the Zhou Dynasty. However, the word was used in early Hindu scripture, including the "Mahābhārata" (5th century BC) and the "Laws of Manu" (2nd century BC).
The official name of the present country is the People's Republic of China (). The common Chinese names for the country are ' (, from ', "central" or "middle", and ', "state" or "states," and in modern times, "nation") and ' (), although the country's official name has been changed numerous times by successive dynasties and modern governments. The term "" appeared in various ancient texts, such as the "Classic of History" of the 6th century BCE, and in pre-imperial times it was often used as a cultural concept to distinguish the Huaxia tribes from perceived "barbarians". The term, which can be either singular or plural, referred to the group of states or provinces in the central plain, but was not used as a name for the country as a whole until the nineteenth century. The Chinese were not unique in regarding their country as "central", with other civilizations having the same view of themselves.
History.
Prehistory.
Archaeological evidence suggests that early hominids inhabited China between 250,000 and 2.24 million years ago. A cave in Zhoukoudian (near present-day Beijing) exhibits hominid fossils dated at between 680,000 and 780,000 BCE. The fossils are of Peking Man, an example of "Homo erectus" who used fire. The Peking Man site has also yielded remains of "Homo sapiens" dating back to 18,000–11,000 BCE. Some scholars assert that a form of proto-writing existed in China as early as 3000 BCE.
According to Chinese tradition, the first imperial dynasty was the Xia, which emerged around 2070 BCE. However, the dynasty was considered mythical by historians until scientific excavations found early Bronze Age sites at Erlitou, Henan in 1959. It remains unclear whether these sites are the remains of the Xia Dynasty or of another culture from the same period.
Early dynastic rule.
The first Chinese dynasty that left historical records, the loosely feudal Shang, settled along the Yellow River in eastern China from the 17th to the 11th century BCE. The oracle bone script of the Shang Dynasty represents the oldest form of Chinese writing yet found, and is a direct ancestor of modern Chinese characters. The Shang were conquered by the Zhou, who ruled between the 12th and 5th centuries BCE, until its centralized authority was slowly eroded by feudal warlords. Many independent states eventually emerged from the weakened Zhou state and continually waged war with each other in the 300-year Spring and Autumn Period, only occasionally deferring to the Zhou king. By the time of the Warring States period of the 5th–3rd centuries BCE, there were seven powerful sovereign states in what is now China, each with its own king, ministry and army.
Imperial China.
The Warring States period ended in 221 BCE, after the state of Qin conquered the other six kingdoms and established the first unified Chinese state. Qin Shi Huang, the emperor of Qin, proclaimed himself the "First Emperor" () and imposed reforms throughout China, notably the forced standardization of the Chinese language, measurements, length of cart axles, and currency. The Qin Dynasty lasted only fifteen years, falling soon after Qin Shi Huang's death, as its harsh legalist and authoritarian policies led to widespread rebellion.
The subsequent Han Dynasty ruled China between 206 BCE and 220 CE, and created a lasting Han cultural identity among its populace that has endured to the present day. The Han Dynasty expanded the empire's territory considerably with military campaigns reaching Korea, Vietnam, Mongolia and Central Asia, and also helped establish the Silk Road in Central Asia. Han China gradually became the largest economy of the ancient world. The Han Dynasty adopted Confucianism, a philosophy developed in the Spring and Autumn period, as its official state ideology. Despite the Han's official abandonment of Legalism, the official ideology of the Qin, Legalist institutions and policies remained and formed the basis of the Han government.
After the collapse of Han, a period of disunion known as the period of the Three Kingdoms followed. In 581 CE, China was reunited under the Sui. However, the Sui Dynasty declined following its defeat in the Goguryeo–Sui War (598–614).
Under the succeeding Tang and Song dynasties, Chinese technology and culture entered a golden age. The An Shi Rebellion in the 8th century devastated the country and weakened the dynasty. The Song Dynasty was the first government in world history to issue paper money and the first Chinese polity to establish a permanent standing navy. Between the 10th and 11th centuries, the population of China doubled in size to around 100 million people, mostly due to the expansion of rice cultivation in central and southern China, and the production of abundant food surpluses. The Song Dynasty also saw a flourishing of philosophy and the arts, as landscape art and portrait painting were brought to new levels of maturity and complexity, and social elites gathered to view art, share their own and trade precious artworks. The Song Dynasty saw a revival of Confucianism, in response to the growth of Buddhism during the Tang.
In the 13th century, China was gradually conquered by the Mongol empire. In 1271, the Mongol leader Kublai Khan established the Yuan Dynasty; the Yuan conquered the last remnant of the Song Dynasty in 1279. Before the Mongol invasion, the population of Song China was 120 million citizens; this was reduced to 60 million by the time of the census in 1300. A peasant named Zhu Yuanzhang overthrew the Yuan Dynasty in 1368 and founded the Ming Dynasty.
Under the Ming Dynasty, China enjoyed another golden age, developing one of the strongest navies in the world and a rich and prosperous economy amid a flourishing of art and culture. It was during this period that Zheng He led explorations throughout the world, reaching as far as Africa. In the early years of the Ming Dynasty, China's capital was moved from Nanjing to Beijing. During the Ming Dynasty, philosophers such as Wang Yangming further critiqued and expanded Neo-Confucianism with concepts of individualism and innate morality.
In 1644, Beijing was captured by a coalition of rebel forces led by Li Zicheng, a minor Ming official who led the peasant revolt. The last Ming Chongzhen Emperor committed suicide when the city fell. The Manchu Qing Dynasty then allied with Ming Dynasty general Wu Sangui and overthrew Li's short-lived Shun Dynasty, and subsequently seized control of Beijing, which became the new capital of the Qing Dynasty.
End of dynastic rule.
The Qing dynasty, which lasted from 1644 until 1912, was the last imperial dynasty of China. In the 19th century, the dynasty experienced Western imperialism following the First Opium War (1839–42) and the Second Opium War (1856–60) with Britain. China was forced to sign unequal treaties, pay compensation, allow extraterritoriality for foreign nationals, and cede Hong Kong to the British under the 1842 Treaty of Nanking. The First Sino-Japanese War (1894–95) resulted in Qing China's loss of influence in the Korean Peninsula, as well as the cession of Taiwan to Japan.
The Qing dynasty also began experiencing internal unrest in which millions of people died. In the 1850s and 1860s, the failed Taiping Rebellion ravaged southern China. Other major rebellions included the Punti-Hakka Clan Wars (1855–67), the Nien Rebellion (1851–68), the Miao Rebellion (1854–73), the Panthay Rebellion (1856–73) and the Dungan Revolt (1862–77).
In the 19th century, the great Chinese Diaspora began. Losses due to emigration were added to by conflicts and catastrophes such as the Northern Chinese Famine of 1876–79, in which between 9 and 13 million people died. In 1898, the Guangxu Emperor drafted a reform plan to establish a modern constitutional monarchy, but he was overthrown by the Empress Dowager Cixi in a coup d'état. The ill-fated anti-Western Boxer Rebellion of 1899–1901 further weakened the Qing dynasty. The Xinhai Revolution of 1911–12 brought an end to the Qing dynasty and established the Republic of China.
Republic of China (1912–1949).
On 1 January 1912, the Republic of China was established, and Sun Yat-sen of the Kuomintang (the KMT or Nationalist Party) was proclaimed provisional president. However, the presidency was later given to Yuan Shikai, a former Qing general who in 1915 proclaimed himself Emperor of China. In the face of popular condemnation and opposition from his own Beiyang Army, he was forced to abdicate and reestablish the republic.
After Yuan Shikai's death in 1916, China was politically fragmented. Its Beijing-based government was internationally recognized but virtually powerless; regional warlords controlled most of its territory. In the late 1920s, the Kuomintang, under Chiang Kai-shek, was able to reunify the country under its own control with a series of deft military and political manoeuvrings, known collectively as the Northern Expedition. The Kuomintang moved the nation's capital to Nanjing and implemented "political tutelage", an intermediate stage of political development outlined in Sun Yat-sen's San-min program for transforming China into a modern democratic state. The political division in China made it difficult for Chiang to battle the Communists, against whom the Kuomintang had been warring since 1927 in the Chinese Civil War. This war continued successfully for the Kuomintang, especially after the Communists retreated in the Long March, until Japanese aggression and the 1936 Xi'an Incident forced Chiang to confront Imperial Japan.
The Second Sino-Japanese War (1937–1945), a theatre of World War II, forced an uneasy alliance between the Kuomintang and the Communists. Japanese forces committed numerous war atrocities against the civilian population; in all, as many as 20 million Chinese civilians died. An estimated 200,000 Chinese were massacred in the city of Nanjing alone during the Japanese occupation. Japan surrendered unconditionally to China in 1945. Taiwan, including the Pescadores, was put under the administrative control of the Republic of China, which immediately claimed sovereignty. China emerged victorious but war-ravaged and financially drained. The continued distrust between the Kuomintang and the Communists led to the resumption of civil war. In 1947, constitutional rule was established, but because of the ongoing unrest, many provisions of the ROC constitution were never implemented in mainland China.
People's Republic of China (1949–present).
Major combat in the Chinese Civil War ended in 1949 with the Communist Party in control of most of mainland China, and the Kuomintang retreating offshore, reducing the ROC's territory to only Taiwan, Hainan, and their surrounding islands. On 1 October 1949, Communist Party Chairman Mao Zedong proclaimed the establishment of the People's Republic of China. In 1950, the People's Liberation Army succeeded in capturing Hainan from the ROC and occupying Tibet. However, remaining Nationalist forces continued to wage an insurgency in western China throughout the 1950s.
Mao encouraged population growth, and under his leadership the Chinese population almost doubled from around 550 million to over 900 million. However, Mao's Great Leap Forward, a large-scale economic and social reform project, resulted in an estimated 45 million deaths between 1958 and 1961, mostly from starvation. Between 1 and 2 million landlords were executed as "counterrevolutionaries." In 1966, Mao and his allies launched the Cultural Revolution, sparking a period of political recrimination and social upheaval which lasted until Mao's death in 1976. In October 1971, the PRC replaced the Republic of China in the United Nations, and took its seat as a permanent member of the Security Council.
After Mao's death in 1976 and the arrest of the faction known as the Gang of Four, who were blamed for the excesses of the Cultural Revolution, Deng Xiaoping took power and led the country to significant economic reforms. The Communist Party subsequently loosened governmental control over citizens' personal lives and the communes were disbanded in favour of private land leases. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment. China adopted its current constitution on 4 December 1982. In 1989, the violent suppression of student protests in Tiananmen Square brought condemnation and sanctions against the Chinese government from various countries.
Jiang Zemin, Li Peng and Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. The country formally joined the World Trade Organization in 2001, and maintained its high rate of economic growth under Hu Jintao and Wen Jiabao's leadership in the 2000s. However, rapid growth also severely impacted the country's resources and environment, and caused major social displacement. Living standards continued to improve rapidly despite the late-2000s recession, but centralized political control remained tight.
Preparations for a decadal Communist Party leadership change in 2012 were marked by factional disputes and political scandals. During China's 18th National Communist Party Congress in November 2012, Hu Jintao was replaced as General Secretary of the Communist Party by Xi Jinping. Under Xi, the Chinese government began large-scale efforts to reform its economy, which has suffered from structural instabilities and slowing growth. The Xi-Li Administration also announced major reforms to the one-child policy and prison system.
Geography.
Political geography.
The People's Republic of China is the second-largest country in the world by land area after Russia, and is either the third- or fourth-largest by total area, after Russia, Canada and, depending on the definition of total area, the United States. China's total area is generally stated as being approximately . Specific area figures range from according to the "Encyclopædia Britannica", according to the UN Demographic Yearbook, to according to the CIA World Factbook.
China has the longest combined land border in the world, measuring from the mouth of the Yalu River to the Gulf of Tonkin. China borders 14 nations, more than any other country except Russia, which also borders 14. China extends across much of East Asia, bordering Vietnam, Laos, and Burma in Southeast Asia; India, Bhutan, Nepal and Pakistan in South Asia; Afghanistan, Tajikistan, Kyrgyzstan and Kazakhstan in Central Asia; and Russia, Mongolia, and North Korea in Inner Asia and Northeast Asia. Additionally, China shares maritime boundaries with South Korea, Japan, Vietnam, the Philippines and Taiwan.
Landscape and climate.
The territory of China lies between latitudes 18° and 54° N, and longitudes 73° and 135° E. China's landscapes vary significantly across its vast width. In the east, along the shores of the Yellow Sea and the East China Sea, there are extensive and densely populated alluvial plains, while on the edges of the Inner Mongolian plateau in the north, broad grasslands predominate. Southern China is dominated by hills and low mountain ranges, while the central-east hosts the deltas of China's two major rivers, the Yellow River and the Yangtze River. Other major rivers include the Xi, Mekong, Brahmaputra and Amur. To the west sit major mountain ranges, most notably the Himalayas. High plateaus feature among the more arid landscapes of the north, such as the Taklamakan and the Gobi Desert. The world's highest point, Mount Everest (8,848m), lies on the Sino-Nepalese border. The country's lowest point, and the world's third-lowest, is the dried lake bed of Ayding Lake (−154m) in the Turpan Depression.
China's climate is mainly dominated by dry seasons and wet monsoons, which lead to pronounced temperature differences between winter and summer. In the winter, northern winds coming from high-latitude areas are cold and dry; in summer, southern winds from coastal areas at lower latitudes are warm and moist. The climate in China differs from region to region because of the country's highly complex topography.
A major environmental issue in China is the continued expansion of its deserts, particularly the Gobi Desert. Although barrier tree lines planted since the 1970s have reduced the frequency of sandstorms, prolonged drought and poor agricultural practices have resulted in dust storms plaguing northern China each spring, which then spread to other parts of East Asia, including Korea and Japan. China's environmental watchdog, Sepa, stated in 2007 that China is losing a million acres (4,000 km²) per year to desertification. Water quality, erosion, and pollution control have become important issues in China's relations with other countries. Melting glaciers in the Himalayas could potentially lead to water shortages for hundreds of millions of people.
Biodiversity.
China is one of 17 megadiverse countries, lying in two of the world's major ecozones: the Palearctic and the Indomalaya. By one measure, China has over 34,687 species of animals and vascular plants, making it the third-most biodiverse country in the world, after Brazil and Colombia. The country signed the Rio de Janeiro Convention on Biological Diversity on 11 June 1992, and became a party to the convention on 5 January 1993. It later produced a National Biodiversity Strategy and Action Plan, with one revision that was received by the convention on 21 September 2010.
China is home to at least 551 species of mammals (the third-highest such number in the world), 1,221 species of birds (eighth), 424 species of reptiles (seventh) and 333 species of amphibians (seventh). China is the most biodiverse country in each category outside of the tropics. Wildlife in China share habitat with and bear acute pressure from the world's largest population of "homo sapiens". At least 840 animal species are threatened, vulnerable or in danger of local extinction in China, due mainly to human activity such as habitat destruction, pollution and poaching for food, fur and ingredients for traditional Chinese medicine. Endangered wildlife is protected by law, and as of 2005, the country has over 2,349 nature reserves, covering a total area of 149.95 million hectares, 15 percent of China's total land area.
China has over 32,000 species of vascular plants, and is home to a variety of forest types. Cold coniferous forests predominate in the north of the country, supporting animal species such as moose and Asian black bear, along with over 120 bird species. The understorey of moist conifer forests may contain thickets of bamboo. In higher montane stands of juniper and yew, the bamboo is replaced by rhododendrons. Subtropical forests, which are predominate in central and southern China, support as many as 146,000 species of flora. Tropical and seasonal rainforests, though confined to Yunnan and Hainan Island, contain a quarter of all the animal and plant species found in China. China has over 10,000 recorded species of fungi, and of them, nearly 6,000 are higher fungi.
Environmental issues.
In recent decades, China has suffered from severe environmental deterioration and pollution. While regulations such as the 1979 Environmental Protection Law are fairly stringent, they are poorly enforced, as they are frequently disregarded by local communities and government officials in favour of rapid economic development. Urban air pollution is a severe health issue in the country; the World Bank estimated in 2013 that 16 of the world's 20 most-polluted cities are located in China. China is the world's largest carbon dioxide emitter. The country also has water problems. Roughly 298 million Chinese in rural areas do not have access to safe drinking water, and 40% of China's rivers had been polluted by industrial and agricultural waste by late 2011. This crisis is compounded by increasingly severe water shortages, particularly in the north-east of the country.
However, China is the world's leading investor in renewable energy commercialization, with $52 billion invested in 2011 alone; it is a major manufacturer of renewable energy technologies and invests heavily in local-scale renewable energy projects. By 2009, over 17% of China's energy was derived from renewable sources – most notably hydroelectric power plants, of which China has a total installed capacity of 197 GW. In 2011, the Chinese government announced plans to invest four trillion yuan (US$618.55 billion) in water infrastructure and desalination projects over a ten-year period, and to complete construction of a flood prevention and anti-drought system by 2020. In 2013, China began a five-year, US$277-billion effort to reduce air pollution, particularly in the north of the country.
Politics.
The People's Republic of China is one of the world's few remaining socialist states openly endorsing communism (see Ideology of the Communist Party of China). The Chinese government has been variously described as communist and socialist, but also as authoritarian and corporatist, with heavy restrictions in many areas, most notably against free access to the Internet, freedom of the press, freedom of assembly, the right to have children, free formation of social organizations and freedom of religion. Its current political, ideological and economic system has been termed by its leaders as the "people's democratic dictatorship", "socialism with Chinese characteristics" (which is Marxism adapted to Chinese circumstances) and the "socialist market economy" respectively.
Communist Party.
The country is ruled by the Communist Party of China (CPC), whose power is enshrined in China's constitution. The Chinese electoral system is hierarchical, whereby local People's Congresses are directly elected, and all higher levels of People's Congresses up to the National People's Congress (NPC) are indirectly elected by the People's Congress of the level immediately below. The political system is decentralized, and provincial and sub-provincial leaders have a significant amount of autonomy. There are other political parties in China, referred to in China as democratic parties, which participate in the National People's Congress and the Chinese People's Political Consultative Conference (CPPCC).
Compared to its closed-door policies until the mid-1970s, the liberalization of China has resulted in the administrative climate being less restrictive than before. China supports the Leninist principle of "democratic centralism", but the elected National People's Congress has been described as a "rubber stamp" body. As a single-party state, the General Secretary of the Communist Party of China holds ultimate power and authority over state and government.
Government.
The President of China is the titular head of state, serving as the ceremonial figurehead under National People's Congress. The Premier of China is the head of government, presiding over the State Council composed of four vice premiers and the heads of ministries and commissions. The incumbent President is Xi Jinping, who is also the General Secretary of the Communist Party of China and the Chairman of the Central Military Commission, making him China's paramount leader. The incumbent Premier is Li Keqiang, who is also a senior member of the CPC Politburo Standing Committee, China's "de facto" top decision-making body.
There have been some moves toward political liberalization, in that open contested elections are now held at the village and town levels. However, the Party retains effective control over government appointments: in the absence of meaningful opposition, the CPC wins by default most of the time. Political concerns in China include the growing gap between rich and poor and government corruption. Nonetheless, the level of public support for the government and its management of the nation is high, with 80–95% of Chinese citizens expressing satisfaction with the central government, according to a 2011 survey.
Administrative divisions.
The People's Republic of China has administrative control over 22 provinces and considers Taiwan to be its 23rd province, although Taiwan is currently and independently governed by the Republic of China, which disputes the PRC's claim. China also has five subdivisions officially termed autonomous regions, each with a designated minority group; four municipalities; and two Special Administrative Regions (SARs), which enjoy a degree of political autonomy. These 22 provinces, five autonomous regions, and four municipalities can be collectively referred to as "mainland China", a term which usually excludes the SARs of Hong Kong and Macau. None of these divisions are recognized by the ROC government, which claims the entirety of the PRC's territory.
Foreign relations.
The PRC has diplomatic relations with 171 countries and maintains embassies in 162. Its legitimacy is disputed by the Republic of China and a few other countries; it is thus the largest and most populous state with limited recognition. In 1971, the PRC replaced the Republic of China as the sole representative of China in the United Nations and as one of the five permanent members of the United Nations Security Council. China was also a former member and leader of the Non-Aligned Movement, and still considers itself an advocate for developing countries. Along with Brazil, Russia, India and South Africa, China is a member of the BRICS group of emerging major economies and hosted the group's third official summit at Sanya, Hainan in April 2011.
Under its interpretation of the One-China policy, Beijing has made it a precondition to establishing diplomatic relations that the other country acknowledges its claim to Taiwan and severs official ties with the government of the Republic of China. Chinese officials have protested on numerous occasions when foreign countries have made diplomatic overtures to Taiwan, especially in the matter of armament sales. Political meetings between foreign government officials and the 14th Dalai Lama are also opposed by China, as the latter considers Tibet to be formally part of China.
Much of current Chinese foreign policy is reportedly based on Premier Zhou Enlai's Five Principles of Peaceful Coexistence, and is also driven by the concept of "harmony without uniformity", which encourages diplomatic relations between states despite ideological differences. This policy may have led China to support states that are regarded as dangerous or repressive by Western nations, such as Zimbabwe, North Korea and Iran. China has a close economic and military relationship with Russia, and the two states often vote in unison in the UN Security Council.
Trade relations.
In recent decades, China has played an increasing role in calling for free trade areas and security pacts amongst its Asia-Pacific neighbours. In 2004, it proposed an entirely new East Asia Summit (EAS) framework as a forum for regional security issues. The EAS, which includes ASEAN Plus Three, India, Australia and New Zealand, held its inaugural summit in 2005. China is also a founding member of the Shanghai Cooperation Organization (SCO), along with Russia and the Central Asian republics. China became a member of the World Trade Organization (WTO) on 11 December 2001.
In 2000, the United States Congress approved "permanent normal trade relations" (PNTR) with China, allowing Chinese exports in at the same low tariffs as goods from most other countries. China has a significant trade surplus with the United States, its most important export market. In the early 2010s, US politicians argued that the Chinese yuan was significantly undervalued, giving China an unfair trade advantage. In recent decades, China has followed a policy of engaging with African nations for trade and bilateral co-operation; in 2012, Sino-African trade totalled over US$160 billion. China has furthermore strengthened its ties with major South American economies, becoming the largest trading partner of Brazil and building strategic links with Argentina.
Territorial disputes.
In addition to claiming all of Taiwan, China has been involved in a number of other international territorial disputes. Since the 1990s, China has been involved in negotiations to resolve its disputed land borders, including a disputed border with India and an undefined border with Bhutan. China is additionally involved in multilateral disputes over the ownership of several small islands in the East and South China Seas, such as the Senkaku Islands and the Scarborough Shoal. On 21 May 2014 President Xi, speaking at a conference in Shanghai, pledged to settle China's territorial disputes peacefully. "China stays committed to seeking peaceful settlement of disputes with other countries over territorial sovereignty and maritime rights and interests," he said.
Emerging superpower status.
China is regularly hailed as a potential new superpower, with certain commentators citing its rapid economic progress, growing military might, very large population, and increasing international influence as signs that it will play a prominent global role in the 21st century. Others, however, warn that economic bubbles and demographic imbalances could slow or even halt China's growth as the century progresses.
Some authors also question the definition of "superpower", arguing that China's large economy alone would not qualify it as a superpower, and noting that it lacks the military and cultural influence of the United States.
Sociopolitical issues, human rights and reform.
The Chinese democracy movement, social activists, and some members of the Communist Party of China have all identified the need for social and political reform. While economic and social controls have been significantly relaxed in China since the 1970s, political freedom is still tightly restricted. The Constitution of the People's Republic of China states that the "fundamental rights" of citizens include freedom of speech, freedom of the press, the right to a fair trial, freedom of religion, universal suffrage, and property rights. However, in practice, these provisions do not afford significant protection against criminal prosecution by the state. Censorship of political speech and information, most notably on the Internet, is openly and routinely used in China to silence criticism of the government and the ruling Communist Party. In 2005, Reporters Without Borders ranked China 159th out of 167 states in its Annual World Press Freedom Index, indicating a very low level of perceived press freedom.
Rural migrants to China's cities often find themselves treated as second-class citizens by the "hukou" household registration system, which controls access to state benefits. Property rights are often poorly protected, and taxation disproportionately affects poorer citizens. However, a number of rural taxes have been reduced or abolished since the early 2000s, and additional social services provided to rural dwellers.
A number of foreign governments, foreign press agencies and NGOs also routinely criticize China's human rights record, alleging widespread civil rights violations such as detention without trial, forced abortions, forced confessions, torture, restrictions of fundamental rights, and excessive use of the death penalty. The government has suppressed demonstrations by organizations that it considers a potential threat to "social stability", as was the case with the Tiananmen Square protests of 1989. The Chinese state is regularly accused of large-scale repression and human rights abuses in Tibet and Xinjiang, including violent police crackdowns and religious suppression.
The Chinese government has responded to foreign criticism by arguing that the notion of human rights should take into account a country's present level of economic development and the "people's rights to subsistence and development". It emphasizes the rise in the Chinese standard of living, literacy rate and average life expectancy since the 1970s, as well as improvements in workplace safety and efforts to combat natural disasters such as the perennial Yangtze River floods. Furthermore, some Chinese politicians have spoken out in support of democratization, although others remain more conservative. Some major reform efforts have been conducted; for an instance in November 2013, the government announced its plans to the abolish the much-criticized re-education through labour program. Although during the 2000s and early 2010s, the Chinese government was increasingly tolerant of NGOs that offer practical, efficient solutions to social problems, such "third sector" activity remained heavily regulated.
Military.
With 2.3 million active troops, the People's Liberation Army (PLA) is the largest standing military force in the world, commanded by the Central Military Commission (CMC). The PLA consists of the People's Liberation Army Ground Force (PLAGF), the People's Liberation Army Navy (PLAN), the People's Liberation Army Air Force (PLAAF), and a strategic nuclear force, the Second Artillery Corps. According to the Chinese government, China's military expenditure in 2012 totalled US$100 billion, constituting the world's second-largest military budget. However, other nations, such as the United States, have argued that China does not report its real level of military spending, which is allegedly much higher than the official budget.
As a recognized nuclear weapons state, China is considered both a major regional military power and a potential military superpower. According to a 2013 report by the US Department of Defense, China fields between 50 and 75 nuclear ICBMs, along with a number of SRBMs. However, compared with the other four UN Security Council Permanent Members, China has a relatively limited power projection capabilities. To offset this, it has developed numerous power projection assets – its first aircraft carrier entered service in 2012, and it maintains a substantial fleet of submarines, including several nuclear-powered attack and ballistic missile submarines. China has furthermore established a network of foreign military relationships along critical sea lanes.
China has made significant progress in modernising its air force since the early 2000s, purchasing Russian fighter jets such as the Sukhoi Su-30, and also manufacturing its own modern fighters, most notably the Chengdu J-10 and the Shenyang J-11, J-15 and J-16. China is furthermore engaged in developing an indigenous stealth aircraft and numerous combat drones. China has also updated its ground forces, replacing its ageing Soviet-derived tank inventory with numerous variants of the modern Type 99 tank, and upgrading its battlefield C3I and C4I systems to enhance its network-centric warfare capabilities. In addition, China has developed or acquired numerous advanced missile systems, including anti-satellite missiles, cruise missiles and submarine-launched nuclear ICBMs.
Economy.
As of 2013, China has the world's second-largest economy in terms of nominal GDP, totalling approximately US$9.3253 trillion according to the International Monetary. If purchasing power parity (PPP) is taken into account (US$13.395 trillion in 2013), China's economy is again second only to the United States. In 2013, its PPP GDP per capita was US$9,844, while nominal GDP per capita was US$6,747. Both cases put China behind around ninety countries (out of 183 countries on the IMF list) in global GDP per capita rankings.
Economic history and growth.
From its founding in 1949 until late 1978, the People's Republic of China was a Soviet-style centrally planned economy. Following Mao's death in 1976 and the consequent end of the Cultural Revolution, Deng Xiaoping and the new Chinese leadership began to reform the economy and move towards a more market-oriented mixed economy under one-party rule. Agricultural collectivization was dismantled and farmlands privatized, while foreign trade became a major new focus, leading to the creation of Special Economic Zones (SEZs). Inefficient state-owned enterprises (SOEs) were restructured and unprofitable ones were closed outright, resulting in massive job losses. Modern-day China is mainly characterized as having a market economy based on private property ownership, and is one of the leading examples of state capitalism. The state still dominates in strategic "pillar" sectors such as energy production and heavy industries, but private enterprise has expanded enormously, with around 30 million private businesses recorded in 2008.
Since economic liberalization began in 1978, China has been among the world's fastest-growing economies, relying largely on investment- and export-led growth. According to the IMF, China's annual average GDP growth between 2001 and 2010 was 10.5%. Between 2007 and 2011, China's economic growth rate was equivalent to all of the G7 countries' growth combined. According to the Global Growth Generators index announced by Citigroup in February 2011, China has a very high 3G growth rating. Its high productivity, low labour costs and relatively good infrastructure have made it a global leader in manufacturing. However, the Chinese economy is highly energy-intensive and inefficient; China became the world's largest energy consumer in 2010, relies on coal to supply over 70% of its energy needs, and surpassed the US to become the world's largest oil importer in September 2013. However, China's economic growth and industrialization has damaged its environment, and in the early 2010s, China's economic growth rate began to slow amid domestic credit troubles—international demand for Chinese exports has weakened and this has led to turmoil in the global economy.
In the online realm, China's e-commerce industry has grown more slowly than the EU and the US, with a significant period of development occurring from around 2009 onwards. According to Credit Suisse, the total value of online transactions in China grew from an insignificant size in 2008 to around RMB 4 trillion (US$660 billion) in 2012. Alipay has the biggest market share in China with 300 million users and control of just under half of China's online payment market in February 2014, while Tenpay's share is around 20 percent, and China UnionPay's share is slightly greater than 10 percent.
China in the global economy.
China is a member of the WTO and is the world's largest trading power, with a total international trade value of US$3.87 trillion in 2012. Its foreign exchange reserves reached US$2.85 trillion by the end of 2010, an increase of 18.7% over the previous year, making its reserves by far the world's largest. As of 2009, China owns an estimated $1.6 trillion of US securities. China, holding over US$1.16 trillion in US Treasury bonds, is the largest foreign holder of US public debt. In 2012, China was the world's largest recipient of inward foreign direct investment (FDI), attracting $253 billion. China also invests abroad, with a total outward FDI of $62.4 billion in 2012, and a number of major takeovers of foreign firms by Chinese companies. China's undervalued exchange rate has caused friction with other major economies, and it has also been widely criticized for manufacturing large quantities of counterfeit goods.
China ranked 29th in the Global Competitiveness Index in 2009, although it is only ranked 136th among the 179 countries measured in the 2011 Index of Economic Freedom. In 2011, 61 Chinese companies were listed in the Fortune Global 500. Measured by total revenues, three of the world's top ten most valuable companies in 2011 were Chinese, including fifth-ranked Sinopec Group, sixth-ranked China National Petroleum and seventh-ranked State Grid (the world's largest electric utilities company).
Class and income equality.
China's middle-class population (if defined as those with annual income of between US$10,000 and US$60,000) had reached more than 300 million by 2012. According to the Hurun Report, the number of US dollar billionaires in China increased from 130 in 2009 to 251 in 2012, giving China the world's second-highest number of billionaires. China's domestic retail market was worth over 20 trillion yuan (US$3.2 trillion) in 2012 and is growing at over 12% annually as of 2013, while the country's luxury goods market has expanded immensely, with 27.5% of the global share. However, in recent years, China's rapid economic growth has contributed to severe consumer inflation, leading to increased government regulation. China has a high level of economic inequality, which has increased in the past few decades. In 2012, China's Gini coefficient was 0.474.
Internationalization of the renminbi.
Since 2008 global financial crisis, China realized the dependency of US Dollar and the weakness of the international monetary system. The RMB Internationalization accelerated in 2009 when China established dim sum bond market and expanded Cross-Border Trade RMB Settlement Pilot Project, which helps establish pools of offshore RMB liquidity.
In November 2010, Russia began using the Chinese renminbi in its bilateral trade with China. This was soon followed by Japan, Australia, Singapore, and the United Kingdom. As a result of the rapid internationalization of the renminbi, it became the eighth-most-traded currency in the world in 2013.
Science and technology.
Historical.
China was a world leader in science and technology until the Ming Dynasty. Ancient Chinese discoveries and inventions, such as papermaking, printing, the compass, and gunpowder (the Four Great Inventions), later became widespread in Asia and Europe. Chinese mathematicians were the first to use negative numbers. However, by the 17th century, the Western world had surpassed China in scientific and technological development. The causes of this Great Divergence continue to be debated.
After repeated military defeats by Western nations in the 19th century, Chinese reformers began promoting modern science and technology as part of the Self-Strengthening Movement. After the Communists came to power in 1949, efforts were made to organize science and technology based on the model of the Soviet Union, in which scientific research was part of central planning. After Mao's death in 1976, science and technology was established as one of the Four Modernizations, and the Soviet-inspired academic system was gradually reformed.
Modern era.
Since the end of the Cultural Revolution, China has made significant investments in scientific research, spending over US$100 billion on scientific research and development in 2011 alone. Science and technology are seen as vital for achieving economic and political goals, and are held as a source of national pride to a degree sometimes described as "techno-nationalism". While Chinese-born scientists have won the Nobel Prize in Physics four times and the Nobel Prize in Chemistry once, these scientists had all earned their doctorates and conducted their award-winning research in the West.
China is rapidly developing its education system with an emphasis on science, mathematics and engineering; in 2009, it produced over 10,000 Ph.D. engineering graduates, and as many as 500,000 BSc graduates, more than any other country. China is also the world's second-largest publisher of scientific papers, producing 121,500 in 2010 alone, including 5,200 in leading international scientific journals. Chinese technology companies such as Huawei and Lenovo have become world leaders in telecommunications and personal computing, and Chinese supercomputers are consistently ranked among the world's most powerful. Currently China is experiencing a significant growth in the use of industrial robots; from 2008 to 2011, the installation of multi-role robots has risen by 136 percent.
The Chinese space program is one of the world's most active, and is a major source of national pride. In 1970, China launched its first satellite, Dong Fang Hong I, becoming the fifth country to do so independently. In 2003, China became the third country to independently send humans into space, with Yang Liwei's spaceflight aboard Shenzhou 5; as of June 2013, ten Chinese nationals have journeyed into space. In 2011, China's first space station module, Tiangong-1, was launched, marking the first step in a project to assemble a large manned station by the early 2020s. In 2013, China successfully landed the Chang'e 3 probe and Yutu rover onto the moon. The rover is expected to last 3 months and the lander up to one year. China plans to collect lunar soil samples by 2017.
Infrastructure.
Telecommunications.
China currently has the largest number of active cellphones of any country in the world, with over 1 billion users by February 2012. It also has the world's largest number of internet and broadband users, with over 591 million internet users as of 2013, equivalent to around 44% of its population. A 2013 report found that the national average internet connection speed is 3.14 MB/s. As of July 2013, China accounts for 24% of the world's internet-connected devices.
China Telecom and China Unicom, the world's two largest broadband providers, accounted for 20% of global broadband subscribers. China Telecom alone serves more than 50 million broadband subscribers, while China Unicom serves more than 40 million. Several Chinese telecommunications companies, most notably Huawei and ZTE, have been accused of spying for the Chinese military.
China is developing its own satellite navigation system, dubbed Beidou, which began offering commercial navigation services across Asia in 2012, and is planned to offer global coverage by 2020.
Transport.
Since the late 1990s, China's national road network has been significantly expanded through the creation of a network of national highways and expressways. In 2011 China's highways had reached a total length of , making it the longest highway system in the world. China has the world's largest market for automobiles, having surpassed the United States in both auto sales and production. Auto sales in 2009 exceeded 13.6 million and reach 40 million by 2020. A side-effect of the rapid growth of China's road network has been a significant rise in traffic accidents, with poorly enforced traffic laws cited as a possible cause—in 2011 alone, around 62,000 Chinese died in road accidents. In urban areas, bicycles remain a common mode of transport, despite the increasing prevalence of automobiles – as of 2012, there are approximately 470 million bicycles in China.
China's railways, which are state-owned, are among the busiest in the world, handling a quarter of the world's rail traffic volume on only 6 percent of the world's tracks in 2006. As of 2013, the country had of railways, the third longest network in the world. All provinces and regions are connected to the rail network except Macau. The railways strain to meet enormous demand particularly during the Chinese New Year holiday, when the world's largest annual human migration takes place. In 2013, Chinese railways delivered 2.106 billion passenger trips, generating 1,059.56 billion passenger-kilometers and carried 3.967 billion tons of freight, generating 2,917.4 billion cargo tons-kilometers.
China's high-speed rail (HSR) system, built entirely since the early 2000s, had of track in 2013 and was the longest HSR network in the world. The network includes the Beijing–Guangzhou–Shenzhen High-Speed Railway, the single longest HSR line in the world, and the Beijing–Shanghai High-Speed Railway, which has three of longest railroad bridges in the world. The HSR track network is set to reach approximately by 2020. The Shanghai Maglev Train, which reaches , is the fastest commercial train service in the world.
As of May 2014, 20 Chinese cities have urban mass transit systems in operation, with a dozen more to join them by 2020. The Shanghai Metro, Beijing Subway, Guangzhou Metro, Hong Kong MTR and Shenzhen Metro are among the longest and busiest in the world.
There were 182 commercial airports in China in 2012. With 82 new airports planned to open by 2015, more than two-thirds of the airports under construction worldwide in 2013 were in China, and Boeing expects that China's fleet of active commercial aircraft in China will grow from 1,910 in 2011 to 5,980 in 2031. With rapid expansion in civil aviation, the largest airports in China have also joined the ranks of the busiest in the world. In 2013, Beijing's Capital Airport ranked second in the world by passenger traffic (it was 26th in 2002). Since 2010, the Hong Kong International Airport and Shanghai Pudong International Airport have ranked first and third in air cargo tonnage.
Some 80% of China's airspace remains restricted for military use, and Chinese airlines made up eight of the 10 worst-performing Asian airlines in terms of delays.
China has over 2,000 river and seaports, about 130 of which are open to foreign shipping. In 2012, the Ports of Shanghai, Hong Kong, Shenzhen, Ningbo-Zhoushan, Guangzhou, Qingdao, Tianjin, Dalian ranked in the top in the world in in container traffic and cargo tonnage .
Demographics.
The national census of 2010 recorded the population of the People's Republic of China as approximately 1,370,536,875. About 16.60% of the population were 14 years old or younger, 70.14% were between 15 and 59 years old, and 13.26% were over 60 years old. The population growth rate for 2013 is estimated to be 0.46%.
Although a middle-income country by Western standards, China's rapid growth has pulled hundreds of millions of its people out of poverty since 1978. Today, about 10% of the Chinese population lives below the poverty line of US$1 per day, down from 64% in 1978. Urban unemployment in China reportedly declined to 4% by the end of 2007. At present, urban unemployment rate of China is about 4.1%.
With a population of over 1.3 billion and dwindling natural resources, the government of China is very concerned about its population growth rate and has attempted since 1979, with mixed results, to implement a strict family planning policy, known as the "one-child policy." Before 2013, this policy sought to restrict families to one child each, with exceptions for ethnic minorities and a degree of flexibility in rural areas. A major loosening of the policy was enacted in December 2013, allowing families to have two children if one parent is an only child. China's family planning minister indicated in 2008 that the one-child policy would be maintained until at least 2020. The one-child policy is resisted, particularly in rural areas, primarily because of the need for agricultural labour and a traditional preference for boys. Families who breach the policy often lie during the census. Data from the 2010 census implies that the total fertility rate may now be around 1.4.
The policy, along with traditional preference for boys, may be contributing to an imbalance in the sex ratio at birth. According to the 2010 census, the sex ration at birth was 118.06 boys for every 100 girls, which is beyond the normal range of around 105 boys for every 100 girls. The 2010 census found that males accounted for 51.27 percent of the total population. However, China's sex ratio is more balanced than it was in 1953, when males accounted for 51.82 percent of the total population.
Ethnic groups.
China officially recognizes 56 distinct ethnic groups, the largest of
which are the Han Chinese, who constitute about 91.51% of the total
population. The Han Chinese – the world's largest single ethnic group – outnumber other ethnic groups in every provincial-level division except Tibet and Xinjiang. Ethnic minorities account for about 8.49% of the population of China, according to
the 2010 census. Compared with the 2000 population census, the Han population increased by 66,537,177 persons, or 5.74%, while the population of the 55 national minorities combined increased by 7,362,627 persons, or 6.92%. The 2010 census recorded a total of 593,832 foreign citizens living in China. The largest such groups were from South Korea (120,750), the
United States (71,493) and Japan (66,159).
Languages.
There are as many as 292 living languages in China. The languages most commonly spoken belong to the Sinitic branch of the Sino-Tibetan language family, which contains Mandarin (spoken natively by 70% of the population), and other Chinese languages: Wu (including Shanghainese), Yue (including Cantonese and Taishanese), Min (including Hokkien and Teochew), Xiang, Gan, and Hakka. Languages of the Tibeto-Burman branch, including Tibetan, Qiang, Naxi and Yi, are spoken are spoken across the Tibetan and Yunnan–Guizhou Plateau. Other ethnic minority languages in southwest China include Zhuang, Thai, Dong and Sui of the Tai-Kadai family, Miao and Yao of the Hmong–Mien family, and Wa of the Austroasiatic family. Across northeastern and northwestern China, minority ethnic groups speak Altaic languages including Manchu, Mongolian and several Turkic languages: Uyghur, Kazakh, Kyrgyz, Salar and Western Yugur. Korean is spoken natively along the border with North Korea. Sarikoli, the language of Tajiks in western Xinjiang, is an Indo-European language. Taiwanese aborigines, including a small population on the mainland, speak Austronesian languages.
Standard Mandarin, a variety of Mandarin based on the Beijing dialect, is the official national language of China and is used as a lingua franca in the country between people of different linguistic backgrounds.
Chinese characters have been used as the written script for the Sinitic languages for thousands of years. They allow speakers of mutually unintelligible Chinese languages and dialects to communicate with each other through writing. In 1956, the government introduced simplified characters, which have supplanted the older traditional characters in mainland China. Chinese characters are romanized using the Pinyin system. Tibetan uses an alphabet based on an Indic script. Uyghur is most commonly written in a Perseo-Arabic script. The Mongolian script used in China and the Manchu script are both derived from the Old Uyghur alphabet. Modern Zhuang uses the Latin alphabet.
Urbanization.
China has urbanized significantly in the past few decades. The percent of the country's population living in urban areas increased from 20% in 1990 to 46% in 2007. It is estimated that China's urban population will reach one billion by 2030. As of 2012, there are more than 262 million migrant workers in China. Most of them are from rural areas and seek work in the cities.
China has over 160 cities with a population of over one million, including the seven megacities (cities with a population of over 10 million) of Chongqing, Shanghai, Beijing, Guangzhou, Tianjin, Shenzhen, and Wuhan. By 2025, it is estimated that the country will be home to 221 cities with over a million inhabitants. The figures in the table below are from the 2010 census, and are only estimates of the urban populations within administrative city limits; a different ranking exists when considering the total municipal populations (which includes suburban and rural populations). The large "floating populations" of migrant workers make conducting censuses in urban areas difficult; the figures below include only long-term residents.
Education.
Since 1986, compulsory education in China comprises primary and junior secondary school, which together last for nine years. In 2010, about 82.5 percent of students continued their education at a three-year senior secondary school. The Gaokao, China's national university entrance exam, is a prerequisite for entrance into most higher education institutions. In 2010, 27 percent of secondary school graduates are enrolled in higher education. Vocational education is available to students at the secondary and tertiary level.
In February 2006, the government pledged to provide completely free nine-year education, including textbooks and fees. Annual education investment went from less than US$50 billion in 2003 to more than US$250 billion in 2011. However, there remains an inequality in education spending. In 2010, the annual education expenditure per secondary school student in Beijing totalled ¥20,023, while in Guizhou, one of the poorest provinces in China, only totalled ¥3,204. Free compulsory education in China consists of primary school and junior secondary school between the ages of 6 and 15. In 2011, around 81.4% of Chinese have received secondary education. By 2007, there were 396,567 primary schools, 94,116 secondary schools, and 2,236 higher education institutions in China.
, 94% of the population over age 15 are literate, compared to only 20% in 1950. In 2009, Chinese students from Shanghai achieved the world's best results in mathematics, science and literacy, as tested by the Programme for International Student Assessment (PISA), a worldwide evaluation of 15-year-old school pupils' scholastic performance.
Health.
The Ministry of Health, together with its counterparts in the provincial health bureaux, oversees the health needs of the Chinese population. An emphasis on public health and preventive medicine has characterized Chinese health policy since the early 1950s. At that time, the Communist Party started the Patriotic Health Campaign, which was aimed at improving sanitation and hygiene, as well as treating and preventing several diseases. Diseases such as cholera, typhoid and scarlet fever, which were previously rife in China, were nearly eradicated by the campaign. After Deng Xiaoping began instituting economic reforms in 1978, the health of the Chinese public improved rapidly due to better nutrition, although many of the free public health services provided in the countryside disappeared along with the People's Communes. Healthcare in China became mostly privatized, and experienced a significant rise in quality. In 2009, the government began a 3-year large-scale healthcare provision initiative worth US$124 billion. By 2011, the campaign resulted in 95% of China's population having basic health insurance coverage. In 2011, China was estimated to be the world's third-largest supplier of pharmaceuticals, but its population has suffered from the development and distribution of counterfeit medications.
Life expectancy at birth in China is 75 years, and the infant mortality rate is 12 per thousand. Both have improved significantly since the 1950s. Rates of stunting, a condition caused by malnutrition, have declined from 33.1% in 1990 to 9.9% in 2010. Despite significant improvements in health and the construction of advanced medical facilities, China has several emerging public health problems, such as respiratory illnesses caused by widespread air pollution, hundreds of millions of cigarette smokers, and an increase in obesity among urban youths. China's large population and densely populated cities have led to serious disease outbreaks in recent years, such as the 2003 outbreak of SARS, although this has since been largely contained. In 2010, air pollution caused 1.2 million premature deaths in China.
Religion.
Freedom of religion is guaranteed by China's constitution, although religious organizations that lack official approval can be subject to state persecution. Estimates of religious demographics in China vary. A 2007 survey found that 31.4 percent of Chinese above the age of 16 were religious, while a 2006 study found that 46% of the Chinese population were religious.
Over the millennia, the Chinese civilization has been influenced by various religious movements. China's "San Jiao" ("three doctrines" or "three religions") include Confucianism, Buddhism, and Taoism, and historically have had a significant impact in shaping Chinese culture. Elements of these three belief systems are often incorporated into popular or folk religious traditions. A 2008 survey of rural villagers in six provinces found that
A 2007 survey by the Horizon Research Consultancy Group found that individuals who self-identify as Buddhists made up 11–16% of China's adult population, while Christians comprised around 3–4%, and Muslims comprised approximately 1%. Some of the ethnic minorities of China practice unique ethnic religions – Dongbaism is the traditional religion of the Nakhi people, Moism that of the Zhuang people, and Ruism that of the Qiang people. The traditional indigenous religion of Tibet is Bön, while most Tibetans follow Tibetan Buddhism, a form of Vajrayana.
Culture.
Since ancient times, Chinese culture has been heavily influenced by Confucianism and conservative philosophies. For much of the country's dynastic era, opportunities for social advancement could be provided by high performance in the prestigious imperial examinations, which have their origins in the Han Dynasty. The literary emphasis of the exams affected the general perception of cultural refinement in China, such as the belief that calligraphy, poetry and painting were higher forms of art than dancing or drama. Chinese culture has long emphasized a sense of deep history and a largely inward-looking national perspective. Examinations and a culture of merit remain greatly valued in China today.
The first leaders of the People's Republic of China were born into the traditional imperial order, but were influenced by the May Fourth Movement and reformist ideals. They sought to change some traditional aspects of Chinese culture, such as rural land tenure, sexism, and the Confucian system of education, while preserving others, such as the family structure and culture of obedience to the state. Some observers see the period following the establishment of the PRC in 1949 as a continuation of traditional Chinese dynastic history, while others claim that the Communist Party's rule has damaged the foundations of Chinese culture, especially through political movements such as the Cultural Revolution of the 1960s, where many aspects of traditional culture were destroyed, having been denounced as "regressive and harmful" or "vestiges of feudalism". Many important aspects of traditional Chinese morals and culture, such as Confucianism, art, literature, and performing arts like Peking opera, were altered to conform to government policies and propaganda at the time. Access to foreign media remains heavily restricted; only 34 foreign films a year are allowed to be shown in Chinese cinemas.
Today, the Chinese government has accepted numerous elements of traditional Chinese culture as being integral to Chinese society. With the rise of Chinese nationalism and the end of the Cultural Revolution, various forms of traditional Chinese art, literature, music, film, fashion and architecture have seen a vigorous revival, and folk and variety art in particular have sparked interest nationally and even worldwide. China is now the third-most-visited country in the world, with 55.7 million inbound international visitors in 2010. It also experiences an enormous volume of domestic tourism; an estimated 740 million Chinese holidaymakers travelled within the country in October 2012 alone.
Cuisine.
Chinese cuisine is highly diverse, drawing on several millennia of culinary history. The dynastic emperors of ancient China were known to have many dining chambers in their palaces, with each chamber divided into several departments, each responsible for a specific type of dish. China's staple food is rice. Pork is the most popular meat in China, accounting for about three-fourths of the country's total meat consumption. Spices are central to Chinese cuisine. Numerous foreign offshoots of Chinese food, such as Hong Kong cuisine and American Chinese food, have emerged in the various nations that play host to the Chinese diaspora.
Sports.
China has one of the oldest sporting cultures in the world. There is evidence that archery "(Shèjiàn)" was practised during the Western Zhou Dynasty. Swordplay "(Jiànshù)" and a form of association football "(Cùjū)" date back to China's early dynasties as well. Today, some of the most popular sports in the country include martial arts, basketball, football, table tennis, badminton, swimming and snooker. Board games such as go (known as "weiqi" in China), xiangqi, and more recently chess, are also played at a professional level.
Physical fitness is widely emphasized in Chinese culture, with morning exercises such as "qigong" and "t'ai chi ch'uan" widely practised, and commercial gyms and fitness clubs gaining popularity in the country. Young people in China are also enjoy soccer and basketball, especially in urban centres with limited space and grass areas. The American National Basketball Association has a huge following among the Chinese youth, with ethnic or native Chinese players such as Yao Ming and Jeremy Lin held in high esteem. In addition, China is home to a huge number of cyclists, with an estimated 470 million bicycles as of 2012. Many more traditional sports, such as dragon boat racing, Mongolian-style wrestling and horse racing are also popular.
China has participated in the Olympic Games since 1932, although it has only participated as the PRC since 1952. China hosted the 2008 Summer Olympics in Beijing, where its athletes received 51 gold medals – the highest number of gold medals of any participating nation that year. China also won the most medals of any nation at the 2012 Summer Paralympics, with 231 overall, including 95 gold medals. In 2011, Shenzhen in Guandgong, China hosted the 2011 Summer Universiade. China hosted the 2013 East Asian Games in Tianjin and the 2014 Summer Youth Olympics in Nanjing.

</doc>
<doc id="5407" url="http://en.wikipedia.org/wiki?curid=5407" title="California">
California

California () is a state located on the West Coast of the United States. It is the most populous U.S. state, home to one out of eight people who live in the U.S., with a total of 38 million people, and it is the third largest state by area (after Alaska and Texas). California is bordered by Oregon to the north, Nevada to the east, Arizona to the southeast, and the Mexican State of Baja California to the south. It is home to the nation's second and fifth most populous census statistical areas (Greater Los Angeles Area and San Francisco Bay Area, respectively), and eight of the nation's 50 most populated cities (Los Angeles, San Diego, San Jose, San Francisco, Fresno, Sacramento, Long Beach, and Oakland). Sacramento is the state capital.
What is now California was first settled by various Native American tribes before being explored by a number of European expeditions throughout the 16th and 17th centuries. It was then claimed by the Spanish Empire as part of Alta California in the larger territory of New Spain. Alta California became a part of Mexico in 1821 following its successful war for independence, but would later be ceded to the United States in 1848 after the Mexican-American War. The western portion of Alta California was soon organized as the State of California, which was admitted as the 31st state on September 9, 1850. The California Gold Rush starting in 1848 led to dramatic social and demographic change, with large-scale immigration from the U.S. and abroad and an accompanying economic boom.
California's diverse geography ranges from the Pacific Coast in the west, to the Sierra Nevada in the east – from the Redwood–Douglas fir forests of the northwest, to the Mojave Desert areas in the southeast. The center of the state is dominated by the Central Valley, a major agricultural area. California contains both the highest and lowest points in the contiguous United States (Mount Whitney and Death Valley), and has the 3rd longest coastline of all states (after Alaska and Florida). Earthquakes are a common occurrence because of the state's location along the Pacific Ring of Fire: about 37,000 are recorded annually, but most are too small to feel.
At least half of the fruit produced in the United States is now cultivated in California, and the state also leads in the production of vegetables. Other important contributors to its economy include aerospace, education, manufacturing, and high-tech industry. If it were a country, it would be the 8th or 9th largest economy in the world and the 34th most populous.
Etymology.
The word "California" originally referred to the entire region composed of the Baja California peninsula of Mexico, the current U.S. states of California, Nevada, and Utah, and parts of Arizona, New Mexico, Texas and Wyoming.
The name "California" is most commonly believed to have derived from a fictional paradise peopled by Black Amazons and ruled by Queen Calafia. The story of Calafia is recorded in a 1510 work "The Adventures of Esplandián", written as a sequel to "Amadis de Gaula" by Spanish adventure writer Garci Rodríguez de Montalvo. The kingdom of Queen Calafia, according to Montalvo, was said to be a remote land inhabited by griffins and other strange beasts, and rich in gold.
Shortened forms of the state's name include CA, Cal., Cali., Calif. and us-CA.
Geography.
California is the 3rd largest state in the United States in area, after Alaska and Texas.
In the middle of the state lies the California Central Valley, bounded by the coastal mountain ranges in the west, the Sierra Nevada to the east, the Cascade Range in the north and the Tehachapi Mountains in the south. The Central Valley is California's agricultural heartland.
Divided in two by the Sacramento-San Joaquin River Delta, the northern portion, the Sacramento Valley serves as the watershed of the Sacramento River, while the southern portion, the San Joaquin Valley is the watershed for the San Joaquin River; both areas derive their names from the rivers that transit them. With dredging, the Sacramento and the San Joaquin Rivers have remained sufficiently deep that several inland cities are seaports.
The Sacramento-San Joaquin River Delta serves as a critical water supply hub for the state. Water is routed through an extensive network of canals and pumps out of the delta, that traverse nearly the length of the state, including the Central Valley Project and the State Water Project. Water from the Delta provides drinking water for nearly 23 million people, almost two-thirds of the state's population, and provides water to farmers on the west side of the San Joaquin Valley. The Channel Islands are located off the Southern coast.
The Sierra Nevada (Spanish for "snowy range") includes the highest peak in the contiguous 48 states, Mount Whitney, at . The range embraces Yosemite Valley, famous for its glacially carved domes, and Sequoia National Park, home to the giant sequoia trees, the largest living organisms on Earth, and the deep freshwater lake, Lake Tahoe, the largest lake in the state by volume.
To the east of the Sierra Nevada are Owens Valley and Mono Lake, an essential migratory bird habitat. In the western part of the state is Clear Lake, the largest freshwater lake by area entirely in California. Though Lake Tahoe is larger, it is divided by the California/Nevada border. The Sierra Nevada falls to Arctic temperatures in winter and has several dozen small glaciers, including Palisade Glacier, the southernmost glacier in the United States.
About 45 percent of the state's total surface area is covered by forests, and California's diversity of pine species is unmatched by any other state. California contains more forestland than any other state except Alaska. Many of the trees in the California White Mountains are the oldest in the world; an individual Bristlecone pine has an age over 5,000 years.
In the south is a large inland salt lake, the Salton Sea. The south-central desert is called the Mojave; to the northeast of the Mojave lies Death Valley, which contains the lowest and hottest place in North America, the Badwater Basin at . The horizontal distance from the nadir of Death Valley to the summit of Mount Whitney is less than . Indeed, almost all of southeastern California is arid, hot desert, with routine extreme high temperatures during the summer. The southeastern border of California with Arizona is entirely formed by the Colorado River, from which the southern part of the state gets about half of its water.
Along the California coast are several major metropolitan areas, including Greater Los Angeles Area, the San Francisco Bay Area, and the San Diego metropolitan area.
As part of the Ring of Fire, California is subject to tsunamis, floods, droughts, Santa Ana winds, wildfires, landslides on steep terrain, and has several volcanoes. It sees numerous earthquakes due to several faults, in particular the San Andreas Fault.
Climate.
Much of the state has a Mediterranean climate, with cool, rainy winters and dry summers. The cool California Current offshore often creates summer fog near the coast. Farther inland, one encounters colder winters and hotter summers. The maritime moderation results in the shoreline summertime temperatures of Los Angeles and San Francisco being the coolest of all major metropolitan areas of the United States and uniquely cool compared to areas on the same parallels in the interior and on the east coast of the North American continent. Even the San Diego shoreline bordering Mexico is cooler in summer than most areas in contiguous United States. Just a few miles inland, summer temperature extremes are significantly higher, with downtown Los Angeles being several degrees warmer than at the coast. The same microclimate phenomenon is seen in the climate of the Bay Area, where areas sheltered from the sea sees significantly hotter summers than nearby areas close to the ocean.
Northern parts of the state average higher annual rainfall than the south. California's mountain ranges influence the climate as well: some of the rainiest parts of the state are west-facing mountain slopes. Northwestern California has a temperate climate, and the Central Valley has a Mediterranean climate but with greater temperature extremes than the coast. The high mountains, including the Sierra Nevada, have an alpine climate with snow in winter and mild to moderate heat in summer.
The east side of California's mountains produce a rain shadow, creating expansive deserts. The higher elevation deserts of eastern California see hot summers and cold winters, while the low deserts east of the Southern California mountains experience hot summers and nearly frostless mild winters. Death Valley, a desert with large expanses below sea level, is considered the hottest location in the world; the highest temperature in the world, , was recorded there on July 10, 1913. The lowest temperature in California was in 1937 in Boca.
Ecology.
California is one of the richest and most diverse parts of the world, and includes some of the most endangered ecological communities. California is part of the Nearctic ecozone and spans a number of terrestrial ecoregions.
California's large number of endemic species includes relict species, which have died out elsewhere, such as the Catalina Ironwood ("Lyonothamnus floribundus"). Many other endemics originated through differentiation or adaptive radiation, whereby multiple species develop from a common ancestor to take advantage of diverse ecological conditions such as the California lilac ("Ceanothus"). Many California endemics have become endangered, as urbanization, logging, overgrazing, and the introduction of exotic species have encroached on their habitat.
Flora and fauna.
California boasts several superlatives in its collection of flora: the largest trees, the tallest trees, and the oldest trees. California's native grasses are perennial plants. After European contact, these were generally replaced by invasive species of European annual grasses; and, in modern times, California's hills turn a characteristic golden-brown in summer.
Because California has the greatest diversity of climate and terrain, the state has six life zones which are the lower Sonoran (desert); upper Sonoran (foothill regions and some coastal lands), transition (coastal areas and moist northeastern counties); and the Canadian, Hudsonian, and Arctic Zones, comprising the state's highest elevations.
Plant life in the dry climate of the lower Sonoran zone contains a diversity of native cactus, mesquite, and paloverde. The Joshua tree is found in the Mojave Desert. Flowering plants include the dwarf desert poppy and a variety of asters. Fremont cottonwood and valley oak thrive in the Central Valley. The upper Sonoran zone includes the chaparral belt, characterized by forests of small shrubs, stunted trees, and herbaceous plants. "Nemophila", mint, "Phacelia", "Viola", and the California poppy ("Eschscholzia californica") – the state flower – also flourish in this zone, along with the lupine, more species of which occur here than anywhere else in the world.
The transition zone includes most of California's forests with the redwood ("Sequoia sempervirens") and the "big tree" or giant sequoia ("Sequoiadendron giganteum"), among the oldest living things on earth (some are said to have lived at least 4,000 years). Tanbark oak, California laurel, sugar pine, madrona, broad-leaved maple, and Douglas-fir also grow here. Forest floors are covered with swordfern, alumnroot, barrenwort, and trillium, and there are thickets of huckleberry, azalea, elder, and wild currant. Characteristic wild flowers include varieties of mariposa, tulip, and tiger and leopard lilies.
The high elevations of the Canadian zone allow the Jeffrey pine, red fir, and lodgepole pine to thrive. Brushy areas are abundant with dwarf manzanita and ceanothus; the unique Sierra puffball is also found here. Right below the timeberline, in the Hudsonian zone, the whitebark, foxtail, and silver pines grow. At about , begins the Arctic zone, a treeless region whose flora include a number of wildflowers, including Sierra primrose, yellow columbine, alpine buttercup, and alpine shooting star.
Common plants that have been introduced to the state include the eucalyptus, acacia, pepper tree, geranium, and Scotch broom. The species that are federally classified as endangered are the Contra Costa wallflower, Antioch Dunes evening primrose, Solano grass, San Clemente Island larkspur, salt marsh bird's beak, McDonald's rock-cress, and Santa Barbara Island liveforever. , 85 plant species were listed as threatened or endangered.
In the deserts of the lower Sonoran zone, the mammals include the jackrabbit, kangaroo rat, squirrel, and opossum. Common birds include the owl, roadrunner, cactus wren, and various species of hawk. The area's reptilian life include the sidewinder viper, desert tortoise, and horned toad. The upper Sonoran zone boasts mammals such as the antelope, brown-footed woodrat, and ring-tailed cat. Birds unique to this zone are the California thrasher, bushtit, and California condor.
In the transition zone, there are Colombian black-tailed deer, black bears, gray foxes, cougars, bobcats, and Roosevelt elk. Reptiles such as the garter snakes and rattlesnakes inhabit the zone. In addition, amphibians such as the water puppy and redwood salamander are common too. Birds such as the kingfisher, chickadee, towhee, and hummingbird thrive here as well.
The Canadian zone mammals include the mountain weasel, snowshoe hare, and several species of chipmunks. Conspicuous birds include the blue-fronted jay, Sierra chickadee. Sierra hermit thrush, water ouzel, and Townsend's solitaire. As one ascends into the Hudsonian zone, birds become scarcer. While the Sierra rosy finch is the only bird native to the high Arctic region, other bird species such as the hummingbird and Clark's nutcracker. Principal mammals found in this region include the Sierra coney, white-tailed jackrabbit, and the bighorn sheep. , the bighorn sheep was listed as endangered by the US Fish and Wildlife Service. The fauna found throughout several zones are the mule deer, coyote, mountain lion, northern flicker, and several species of hawk and sparrow.
Aquatic life in California thrives, from the state's mountain lakes and streams to the rocky Pacific coastline. Numerous trout species are found, among them rainbow, golden, and cutthroat. Migratory species of salmon are common as well. Deep-sea life forms include sea bass, yellowfin tuna, barracuda, and several types of whale. Native to the cliffs of northern California are seals, sea lions, and many types of shorebirds, including migratory species.
As of April 2003, 118 California animals were on the federal endangered list; 181 plants were listed as endangered or threatened. Endangered animals include the San Joaquin kitfox, Point Arena mountain beaver, Pacific pocket mouse, salt marsh harvest mouse, Morro Bay kangaroo rat (and five other species of kangaroo rat), Amargosa vole, California least tern, California condor, loggerhead shrike, San Clemente sage sparrow, San Francisco garter snake, five species of salamander, three species of chub, and two species of pupfish. Eleven butterflies are also endangered and two that are threatened are on the federal list. Among threatened animals are the coastal California gnatcatcher, Paiute cutthroat trout, southern sea otter, and northern spotted owl. California has a total of of National Wildlife Refuges. , 123 California animals were listed as either endangered or threatened on the federal list provided by the US Fish & Wildlife Service. Also, , 178 species of California plants were listed either as endangered or threatened on this federal list.
Rivers.
The vast majority of rivers in California are dammed as part of two massive water projects: the Central Valley Project, providing water to the agricultural central valley, the California State Water Project diverting water from northern to southern California. The state's coasts, rivers, and other bodies of water are regulated by the California Coastal Commission.
The two most prominent rivers within California are the Sacramento River and the San Joaquin River, which drain the Central Valley and the west slope of the Sierra Nevada and flow to the Pacific Ocean through San Francisco Bay. Several major tributaries feed into the Sacramento and the San Joaquin, including the Pit River, the Tuolumne River, and the Feather River.
The Eel River and Salinas River each drain portions of the California coast, north and south of San Francisco Bay, respectively, and the Eel River is the largest river in the state to remain in its natural un-dammed state. The Mojave River is the primary watercourse in the Mojave Desert, and the Santa Ana River drains much of the Transverse Ranges as it bisects Southern California. Some other important rivers are the Klamath River and the Trinity River in the far north coast, and the Colorado River on the southeast border with Arizona.
History.
Settled by successive waves of arrivals during the last 10,000 years, California was one of the most culturally and linguistically diverse areas in pre-Columbian North America. Various estimates of the native population range from 100,000 to 300,000, which was about one-third of all native Americans in what is now the United States. The Indigenous peoples of California included more than 70 distinct groups of Native Americans, ranging from large, settled populations living on the coast to groups in the interior. California groups also were diverse in their political organization with bands, tribes, villages, and on the resource-rich coasts, large chiefdoms, such as the Chumash, Pomo and Salinan. Trade, intermarriage and military alliances fostered many social and economic relationships among the diverse groups.
The first European effort to explore the coast as far north as the Russian River was a Spanish sailing expedition, led by Portuguese captain Juan Rodríguez Cabrillo, in 1542. Some 37 years later English explorer Francis Drake also explored and claimed an undefined portion of the California coast in 1579. Spanish traders made unintended visits with the Manila Galleons on their return trips from the Philippines beginning in 1565. Sebastián Vizcaíno explored and mapped the coast of California in 1602 for New Spain.
Finally, after the Portola expedition of 1769-70, Spanish missionaries began setting up 21 California Missions on or near the coast of Alta (Upper) California, beginning in San Diego. During the same period, Spanish military forces built several forts ("presidios") and three small towns ("pueblos"). Two of the pueblos grew into the cities of Los Angeles and San Jose.
19th century.
In 1821 the Mexican War of Independence gave Mexico (including California) independence from Spain; for the next 25 years, Alta California remained a remote northern province of the nation of Mexico.
Cattle ranches, or "ranchos", emerged as the dominant institutions of Mexican California. After Mexican independence from Spain, the chain of missions became the property of the Mexican government and were secularized by 1834. The ranchos developed under ownership by Californios (Spanish-speaking Californians) who had received land grants, and traded cowhides and tallow with Boston merchants.
Beginning in the 1820s, trappers and settlers from the U.S. and Canada began to arrive in Northern California. These new arrivals used the Siskiyou Trail, California Trail, Oregon Trail and Old Spanish Trail to cross the rugged mountains and harsh deserts surrounding California. In this period, Imperial Russia explored the California coast and established a trading post at Fort Ross.
In between 1831 to 1836, California experienced a series of revolts against Mexico; this culminated in the 1836 California revolt lead by Juan Bautista Alvarado, which ended after Mexico appointed him governor of the department. The revolt, which had momentarily declared California an independent state, was successful with the assistance of American and British residents of California, including Isaac Graham; after 1840, 100 of those residents who did not have passports were arrested, leading to the Graham affair in 1840.
In 1846 settlers rebelled against Mexican rule during the Bear Flag Revolt. Afterwards, rebels raised the Bear Flag (featuring a bear, a star, a red stripe and the words "California Republic") at Sonoma. The Republic's only president was William B. Ide, who played a pivotal role during the Bear Flag Revolt.
The California Republic was short lived; the same year marked the outbreak of the Mexican-American War (1846–1848). When Commodore John D. Sloat of the United States Navy sailed into Monterey Bay and began the military occupation of California by the United States, Northern California capitulated in less than a month to the U.S. forces. After a series of defensive battles in Southern California, the Treaty of Cahuenga was signed by the Californios on January 13, 1847, securing American control in California.
Following the Treaty of Guadalupe Hidalgo that ended the war, the region was divided between Mexico and the U.S.; the western territory of Alta California, was to become the U.S. state of California, and Arizona, Nevada, Colorado and Utah became U.S. Territories, while the lower region of California, the Baja Peninsula, remained in the possession of Mexico.
In 1846 the non-native population of California was estimated to be no more than 8,000, plus about 100,000 Native Americans down from about 300,000 before Hispanic settlement in 1769. After gold was discovered, the population burgeoned with U.S. citizens, Europeans, Chinese and other immigrants during the great California Gold Rush. By 1854 over 300,000 settlers had come. Between 1847 and 1870, the population of San Francisco increased from 500 to 150,000. On September 9, 1850, as part of the Compromise of 1850, California was admitted to the United States undivided as a free state, denying the expansion of slavery to the Pacific Coast.
California's native population precipitously declined, above all, from Eurasian diseases to which they had no natural immunity. Like in other states, the native inhabitants were forcefully removed from their lands by incoming miners, ranchers, and farmers. And although California entered the union as a free state, the "loitering or orphaned Indians" were de facto enslaved by Mexican and Anglo-American masters under the 1853 "Act for the Government and Protection of Indians". There were several massacres in which hundreds of indigenous people were killed. Between 1850 and 1860, California paid around 1.5 million dollars (some 250,000 of which was reimbursed by the federal government) to hire militias whose purpose was to protect settlers from the indigenous populations. In subsequent decades, the native population was placed in reservations and rancherias, which were often small and isolated and lacked adequate natural resources or funding from the government to sustain the populations living on them. As a result, the rise of California brought great hardship for the native inhabitants. Several scholars and Native American activists, including Benjamin Madley and Ed Castillo, have described the actions of the California government as a genocide.
The seat of government for California under Spanish and later Mexican rule was located at Monterey from 1777 until 1845. Pio Pico, last Mexican governor of Alta California, moved the capital to Los Angeles in 1845. The United States consulate was also located in Monterey, under consul Thomas O. Larkin.
In 1849, the Constitutional Convention was first held in Monterey. Among the tasks was a decision on a location for the new state capital. The first legislative sessions were held in San Jose (1850–1851). Subsequent locations included Vallejo (1852–1853), and nearby Benicia (1853–1854); these locations eventually proved to be inadequate as well. The capital has been located in Sacramento since 1854 with only a short break in 1861 when legislative sessions were held in San Francisco due to flooding in Sacramento.
Initially, travel between California and the rest of the continental U.S. was time consuming and dangerous. A more direct connection came in 1869 with the completion of the First Transcontinental Railroad through Donner Pass in the Sierra Nevada mountains. Once completed, hundreds of thousands of U.S. citizens came west, where new Californians were discovering that land in the state, if irrigated during the dry summer months, was extremely well suited to fruit cultivation and agriculture in general. Vast expanses of wheat, other cereal crops, vegetable crops, cotton, and nut and fruit trees were grown (including oranges in Southern California), and the foundation was laid for the state's prodigious agricultural production in the Central Valley and elsewhere.
20th century.
Migration to California accelerated during the early 20th century with the completion of major transcontinental highways like the Lincoln Highway and Route 66. In the period from 1900 to 1965, the population grew from fewer than one million to become the most populous state in the Union. In 1940, the Census Bureau reported California's population as 6.0% Hispanic, 2.4% Asian, and 89.5% non-Hispanic white. The 1906 San Francisco earthquake and 1928 St. Francis Dam flood remain the deadliest in U.S history.
To meet the population's needs, major engineering feats like the California and Los Angeles Aqueducts; the Oroville and Shasta Dams; and the Bay and Golden Gate Bridges were built across the state. The state government also adopted the California Master Plan for Higher Education in 1960 to develop a highly efficient system of public education.
Meanwhile, attracted to the mild Mediterranean climate, cheap land, and the state's wide variety of geography, filmmakers established the studio system in Hollywood in the 1920s. California manufactured 8.7 percent of total United States military armaments produced during World War II, ranking third (behind New York and Michigan) among the 48 states. After World War II, California's economy greatly expanded due to strong aerospace and defense industries, whose size decreased following the end of the Cold War. Stanford University and its Dean of Engineering Frederick Terman began encouraging faculty and graduates to stay in California instead of leaving the state, and develop a high-tech region in the area now known as Silicon Valley. As a result of these efforts, California is regarded as a world center of the entertainment and music industries, of technology, engineering, and the aerospace industry, and as the U.S. center of agricultural production. Just before the "Dot Com Bust" California had the 5th largest economy in the world among nations.
Demographics.
Population.
The United States Census Bureau estimates that the population of California was 38,332,521 on July 1, 2013, a 2.9% increase since the 2010 United States Census. Between 2000 and 2009, there was a natural increase of 3,090,016 (5,058,440 births minus 2,179,958 deaths). During this time period, international migration produced a net increase of 1,816,633 people while domestic migration produced a net decrease of 1,509,708, resulting in a net in-migration of 306,925 people. The State of California's own statistics show a population of 38,292,687 for January 1, 2009. However, according to the Manhattan Institute for Policy Research, since 1990 almost 3.4 million Californians have moved to other states, with most leaving to Texas, Nevada, and Arizona.
California is the second-most-populous sub-national entity in the Western Hemisphere and the Americas, with a population second to that of State of São Paulo, Brazil. California's population is greater than that of all but 34 countries of the world. Also, Los Angeles County has held the title of most populous U.S. county for decades, and it alone is more populous than 42 U.S. states. In addition, California is home to eight of the 50 most populous cities in the United States: Los Angeles (2nd), San Diego (8th), San Jose (10th), San Francisco (13th), Fresno (34th), Sacramento (35th), Long Beach (36th), and Oakland (47th). The center of population of California is located in the town of Buttonwillow, Kern County.
Immigration.
Starting in the year 2010, for the first time since the California Gold Rush, California-born residents make up the majority of the state's population. In 2011, California saw a shift in its immigration pattern, with more coming from Asia and less from Latin America. In total for 2011, there were 277,304 immigrants. 57% came from Asian countries vs. 22% from Latin American countries.
The state's population of illegal immigrants has been shrinking in recent years, due to increased enforcement and a slumping economy. The number of migrants arrested attempting to cross the Mexican border in the Southwest plunged from a high of 1.1 million in 2005 to just 367,000 in 2011. Illegal alien constituted an estimated 7.3 percent of the state's population, the third highest percentage of any state in the country, totaling nearly 2.6 million. More than half originate from Mexico. Illegal aliens make up more than ten percent of the population in Los Angeles, Monterey, San Benito, Imperial, and Napa Counties – the latter four of which have significant agricultural industries that depend on manual labor.
Racial and ancestral makeup.
According to the U.S. Census Bureau in 2011:
The principal ancestries of California's residents in 2009 has been surveyed to be:
With regard to demographics, California has the largest population of White Americans in the U.S., an estimated 22,200,000 residents, although most demographic surveys do not measure actual genetic ancestry. The state has the 5th largest population of African Americans in the U.S., an estimated 2,250,000 residents. California's Asian American population is estimated at 4.4 million, about a third of the nation's 13 million Asian Americans. California's Native American population of 285,000 is the most of any state. While the population of minorities accounts for 102 million of 301 million U.S. residents, 20% of the national total live in California.
According to estimates from 2011, California has the largest minority population in the United States by numbers, making up 60% of the state population. In 2000, Hispanics constituted 32% of the population; that number grew to 38% in 2011. Non-Hispanic whites decreased from 80% of the state's population in 1970 to 40% in 2011.
The population younger than age 1 was 75% minority in 2011. Approximately 26% of California's public school students in the 2011–12 school year identified themselves as white (non-Hispanic), and 52% of the state's students identified themselves as Hispanic or Latino. The following ethnic groups that made up the statewide public school student body were Asians (11%), African Americans (7%), Native Americans (0.7%), and Pacific Islanders (0.6%). Students of mixed race made up 2% of the public schools. Hispanics made up the majority of the state's public schools since 2010. Los Angeles Unified School District, the largest school district in California and second largest in the nation, is 73% Hispanic, 10% African American, 9% non-Hispanic Caucasian, 6% Asian, 0.5% Native American, and 0.4% Pacific Islander.
Languages.
In 2010, the Modern Language Association of America estimated that 57.02% (19,429,309) of California residents age 5 and older spoke English at home as a primary language, while 42.98% did not. The most common language spoken besides English was Spanish which was spoken by 28.46% (9,696,638) of the population. In total, 16 languages other than English were spoken as primary languages at home by more than 100,000 persons, more than any other state in the nation.
According to the 2007 American Community Survey, 42.6% of California's population older than age 5 spoke a language other than English at home, with 73% of those persons also speaking English well or very well, and 9.8% not speaking English at all.
California had the highest concentration of Vietnamese or Chinese speakers in the United States, second highest concentration of Korean or Spanish speakers in the United States, and third highest concentration of Tagalog speakers in the United States. California was historically one of the most linguistically diverse areas in the world, and is home to more than 70 indigenous languages derived from 64 root languages in 6 language families. A survey conducted between 2007 and 2009 identified 23 different indigenous languages of Mexico that are spoken among California farmworkers.
Over 200 languages are known to be spoken and read in California, with Spanish used as the state's "alternative" language. California has more than 100 indigenous languages, making California one of the most linguistically diverse areas in the world. All of California's indigenous languages are endangered, although there are now efforts toward language revitalization.
The official language of California has been English since the passage of Proposition 63 in 1986. However, many state, city, and local government agencies still continue to print official public documents in numerous languages. For example, the California Department of Motor Vehicles offers the written exam for the standard C Class driver's license in 31 languages along with English, and the audio exam in 11 languages.
As a result of the state's increasing diversity and migration from other areas across the country and around the globe, linguists began noticing a noteworthy set of emerging characteristics of spoken English in California since the late 20th century. This dialect, known as California English, has a vowel shift and several other phonological processes that are different from the dialects used in other regions of the country.
Armed forces.
In California, , the U.S. Department of Defense had a total of 117,806 active duty servicemembers of which 88,370 were Sailors or Marines, 18,339 were Airmen, and 11,097 were Soldiers, with 61,365 Department of Defense civilian employees. Additionally, there were a total of 57,792 Reservists and Guardsman in California.
In 2010, Los Angeles County was the largest origin of military recruits in the United States by county, with 1,437 individuals enlisting in the military. However, as of 2002, Californians were relatively under-represented in the military as a proportion to its population.
In 2000, California, had 2,569,340 veterans of U.S. military service: 504,010 served in World War II, 301,034 in the Korean War, 754,682 during the Vietnam War, and 278,003 during 1990–2000 (including the Persian Gulf War). , there were 1,942,775 veterans living in California, of which 1,457,875 served during a period of armed conflict, and just over four thousand served before World War II (the largest population of this group of any state).
California's military forces consist of the Army and Air National Guard, the naval and state military reserve (militia), and the California Cadet Corps.
Culture.
The culture of California is a Western culture and most clearly has its modern roots in the culture of the United States, but also, historically, many Hispanic influences. As a border and coastal state, Californian culture has been greatly influenced by several large immigrant populations, especially those from Latin America.
California has long been a subject of interest in the public mind and has often been promoted by its boosters as a kind of paradise. In the early 20th century, fueled by the efforts of state and local boosters, many Americans saw the Golden State as an ideal resort destination, sunny and dry all year round with easy access to the ocean and mountains. In the 1960s, popular music groups such as The Beach Boys promoted the image of Californians as laid-back, tanned beach-goers.
The California Gold Rush of the 1850s is still seen as a symbol of California's economic style, which tends to generate technology, social, entertainment, and economic fads and booms and related busts.
Religion.
The largest religious denominations by number of adherents as a percentage of California's population in 2008 were the Catholic Church with 31 percent; Evangelical Protestants with 18 percent; and Mainline Protestants with 14 percent. Those unaffiliated with any religion represented 21 percent of the population. The breakdown of other religions is 0.5% Muslim, 1% Hindu and 2% Buddhist. The "American Jewish Year Book" placed the total Jewish population of California at about 1,194,190 in 2006. According to the Association of Religion Data Archives(ARDA) the largest denominations by adherents in 2010 were the Roman Catholic Church with 10,233,334; The Church of Jesus Christ of Latter-day Saints with 763,818; and the Southern Baptist Convention with 489,953.
The first priests to come to California were Roman Catholic missionaries from Spain. Roman Catholics founded 21 missions along the California coast, as well as the cities of Los Angeles and San Francisco. California continues to have a large Roman Catholic population due to the large numbers of Mexicans and Central Americans living within its borders. California has twelve dioceses and two archdioceses, the Archdiocese of Los Angeles and the Archdiocese of San Francisco, the former being the largest archdiocese in the United States.
A Pew Research Center survey revealed that California is somewhat less religious than the rest of the US: 62 percent of Californians say they are "absolutely certain" of their belief in God, while in the nation 71 percent say so. The survey also revealed 48 percent of Californians say religion is "very important," compared to 56 percent nationally.
Economy.
The economy of California is large enough to be comparable to that of the largest of countries. , the gross state product (GSP) is about $2.203 trillion, the largest in the United States. California is responsible for 13.2 percent of the United States' approximate $16.7 trillion gross domestic product (GDP). California's GDP is larger than that of all but 7 countries in dollar terms (the United States, China, Japan, Germany, France, Brazil, and the United Kingdom), larger than Russia, Italy, India, Canada, Australia, and Spain. In Purchasing Power Parity, it is larger than all but 10 countries (the United States, China, India, Japan, Germany, Russia, Brazil, France, the United Kingdom, and Indonesia), larger than Italy, Mexico, Spain, South Korea, Saudi Arabia, and Canada.
The five largest sectors of employment in California are trade, transportation, and utilities; government; professional and business services; education and health services; and leisure and hospitality. In output, the five largest sectors are financial services, followed by trade, transportation, and utilities; education and health services; government; and manufacturing. , California has the 5th highest unemployment rate in the nation at 7.6%.
California's economy is dependent on trade and international related commerce accounts for approximately one-quarter of the state's economy. In 2008, California exported $144 billion worth of goods, up from $134 billion in 2007 and $127 billion in 2006.
Computers and electronic products are California's top export, accounting for 42 percent of all the state's exports in 2008.
Agriculture is an important sector in California's economy. Farming-related sales more than quadrupled over the past three decades, from $7.3 billion in 1974 to nearly $31 billion in 2004. This increase has occurred despite a 15 percent decline in acreage devoted to farming during the period, and water supply suffering from chronic instability. Factors contributing to the growth in sales-per-acre include more intensive use of active farmlands and technological improvements in crop production. In 2008, California's 81,500 farms and ranches generated $36.2 billion products revenue. In 2011, that number grew to $43.5 billion products revenue. According to the USDA in 2011, the three largest California agricultural products by value were milk and cream, shelled almonds, and grapes.
Per capita GDP in 2007 was $38,956, ranking eleventh in the nation. Per capita income varies widely by geographic region and profession. The Central Valley is the most impoverished, with migrant farm workers making less than minimum wage. According to a 2005 report by the Congressional Research Service, the San Joaquin Valley was characterized as one of the most economically depressed regions in the U.S., on par with the region of Appalachia. California has a poverty rate of 23.5%, the highest of any state in the country. Many coastal cities include some of the wealthiest per-capita areas in the U.S. The high-technology sectors in Northern California, specifically Silicon Valley, in Santa Clara and San Mateo counties, have emerged from the economic downturn caused by the dot-com bust.
In 2010, there were more than 663,000 millionaires in the state, more than any other state in the nation. In 2010, California residents were ranked first among the states with the best average credit score of 754.
State finances.
State spending increased from $56 billion in 1998 to $127 billion in 2011. California, with 12% of the U.S. population, has one-third of the nation's welfare recipients. California has the third highest per capita spending on welfare among the states, as well as the highest spending on welfare at $6.67 billion. In January 2011 the California's total debt was at least $265 billion. On June 27, 2013, Governor Jerry Brown signed a balanced budget (no deficit) for the state, its first in decades; however the state's debt remains at $132 billion.
With the passage of Proposition 30 in 2012, California now levies a 13.3% maximum marginal income tax rate with ten tax brackets, ranging from 1% at the bottom tax bracket of $0 annual individual income to 13.3% for annual individual income over $1,000,000. California has a state sales tax of 7.5%, though local governments can and do levy additional sales taxes. Many of these taxes are temporary for a seven-year period (as stipulated in Proposition 30) and afterwards will revert to a previous maximum marginal income tax bracket of 10.3% and state sales tax rate of 7.25%.
All real property is taxable annually; the tax is based on the property's fair market value at the time of purchase or new construction. Property tax increases are capped at 2% per year (see Proposition 13).
Infrastructure.
Energy.
Because it is the most populous U.S. state, California is one of the country's largest users of energy. However because of its high energy rates, conservation mandates, mild weather in the largest population centers and strong environmental movement, its "per capita" energy use is one of the smallest of any U.S. state. Due to the high electricity demand, California imports more electricity than any other state, primarily hydroelectric power from states in the Pacific Northwest (via Path 15 and Path 66) and coal- and natural gas-fired production from the desert Southwest via Path 46.
As a result of the state's strong environmental movement, California has some of the most aggressive renewable energy goals in the United States, with a target for California to obtain a third of its electricity from renewables by 2020. Currently, several solar power plants such as the Solar Energy Generating Systems facility are located in the Mojave Desert. California's wind farms include Altamont Pass, San Gorgonio Pass, and Tehachapi Pass. Several dams across the state provide hydro-electric power.
The state's crude oil and natural gas deposits are located in the Central Valley and along the coast, including the large Midway-Sunset Oil Field. Natural gas-fired power plants typically account for more than one-half of State electricity generation.
California is also home to two major nuclear power plants: Diablo Canyon and San Onofre. However, voters banned the approval of new nuclear power plants since the late 1970s because of concerns over radioactive waste disposal. In addition, several cities such as Oakland, Berkeley and Davis have declared themselves as nuclear-free zones.
Transportation.
California's vast terrain is connected by an extensive system of controlled-access highways ('freeways'), limited-access roads ('expressways'), and highways. California is known for its car culture, giving California's cities a reputation for severe traffic congestion. Construction and maintenance of state roads and statewide transportation planning are primarily the responsibility of the California Department of Transportation, nicknamed "Caltrans". The rapidly growing population of the state is straining all of its transportation networks, and California has some of the worst roads in the United States. The Reason Foundation's 19th Annual Report on the Performance of State Highway Systems ranked California's highways the third-worst of any state, with Alaska second, and Rhode Island first.
The state has been a pioneer in road construction. One of the state's more visible landmarks, the Golden Gate Bridge, was once the longest suspension bridge main span in the world at when it opened in 1937. With its orange paint and panoramic views of the bay, this highway bridge is a popular tourist attraction and also accommodates pedestrians and bicyclists. The San Francisco – Oakland Bay Bridge (often abbreviated the "Bay Bridge"), completed in 1936, transports approximately 280,000 vehicles per day on two-decks. Its two sections meet at Yerba Buena Island through the world's largest diameter transportation bore tunnel, at wide by high. The Arroyo Seco Parkway, connecting Los Angeles and Pasadena, opened in 1940 as the first freeway in the Western United States. It was later extended south to the Four Level Interchange in downtown Los Angeles, regarded as the first stack interchange ever built.
Los Angeles International Airport (LAX), the 6th busiest airport in the world, and San Francisco International Airport (SFO), the 21st busiest airport in the world, are major hubs for trans-Pacific and transcontinental traffic. There are about a dozen important commercial airports and many more general aviation airports throughout the state.
California also has several important seaports. The giant seaport complex formed by the Port of Los Angeles and the Port of Long Beach in Southern California is the largest in the country and responsible for handling about a fourth of all container cargo traffic in the United States. The Port of Oakland, fourth largest in the nation, also handles trade entering from the Pacific Rim to the rest of the country.
The California Highway Patrol is the largest statewide police agency in the United States in employment with over 10,000 employees. They are responsible for providing any police-sanctioned service to anyone on California's state maintained highways and on state property.
The California Department of Motor Vehicles is by far the largest in North America. By the end of 2009, the California DMV had 26,555,006 driver's licenses and ID cards on file. In 2010, there were 1.17 million new vehicle registrations in force.
Intercity rail travel is provided by Amtrak California, which manages the three busiest intercity rail lines in the U.S. outside the Northeast Corridor, all of which are funded by Caltrans. This service is becoming increasingly popular over flying and ridership is continuing to set records, especially on the LAX-SFO route. Integrated subway and light rail networks are found in Los Angeles (Metro Rail) and San Francisco (MUNI Metro). Light rail systems are also found in San Jose (VTA), San Diego (San Diego Trolley), Sacramento (RT Light Rail), and Northern San Diego County (Sprinter). Furthermore, commuter rail networks serve the San Francisco Bay Area (ACE, BART, Caltrain), Greater Los Angeles (Metrolink), and San Diego County (Coaster).
The California High-Speed Rail Authority was created in 1996 by the state to implement an extensive rail system. Construction was approved by the voters during the November 2008 general election, a $9.95 billion state bond will go toward its construction. Nearly all counties operate bus lines, and many cities operate their own city bus lines as well. Intercity bus travel is provided by Greyhound and Amtrak Thruway Coach.
Water.
California's interconnected water system is the world's largest, managing over 40,000,000 acre feet of water per year, centered on six main systems of aqueducts and infrastructure projects. Water use and conservation in California is a politically divisive issue, as state experiences periodic droughts and has to balance the demands of its large agricultural and urban sectors, especially in the arid southern portion of the state. The state's widespread redistribution of water also invites the frequent scorn of environmentalists.
The California Water Wars, a conflict between Los Angeles and the Owens Valley over water rights, is one of the most well-known examples of the struggle to secure adequate water supplies. Former California Governor Arnold Schwarzenegger said: "We've been in crisis for quite some time because we're now 38 million people and not anymore 18 million people like we were in the late 60s. So it developed into a battle between environmentalists and farmers and between the south and the north and between rural and urban. And everyone has been fighting for the last four decades about water."
Government and politics.
Government.
The state's capital is Sacramento.
California is organized into three branches of government – the executive branch consisting of the Governor and the other independently elected constitutional officers; the legislative branch consisting of the Assembly and Senate; and the judicial branch consisting of the Supreme Court of California and lower courts. The state also allows ballot propositions: direct participation of the electorate by initiative, referendum, recall, and ratification. Before the passage of California Proposition 14 (2010), California allowed each political party to choose whether to have a closed primary or a primary where only party members and independents vote. After June 8, 2010 when Proposition 14 was approved, excepting only the U.S. President and county central committee offices, all candidates in the primary elections are listed on the ballot with their preferred party affiliation, but they are not the official nominee of that party. At the primary election, the two candidates with the top votes will advance to the general election regardless of party affiliation. If at the primary, one candidate receives more than 50% of all the votes cast, no general election will be held, and they are declared the winner.
The California executive branch consists of the Governor of California and seven other elected constitutional officers: Lieutenant Governor, Attorney General, Secretary of State, State Controller, State Treasurer, Insurance Commissioner, and State Superintendent of Public Instruction. They serve four-year terms and may be re-elected only once.
The California State Legislature consists of a 40-member Senate and 80-member Assembly. Senators serve four-year terms and Assembly members two. Members of the Assembly are subject to term limits of three terms, and members of the Senate are subject to term limits of two terms.
California's legal system is explicitly based upon English common law (as is the case with all other states except Louisiana) but carries a few features from Spanish civil law, such as community property. California's prison population grew from 25,000 in 1980 to over 170,000 in 2007. Capital punishment is a legal form of punishment and the state has the largest "Death Row" population in the country (though Texas is far more active in carrying out executions).
California's judiciary system is the largest in the United States (with a total of 1,600 judges, while the federal system has only about 840). At the apex is the seven Justices of the Supreme Court of California, while the California Courts of Appeal serve as the primary appellate courts and the California Superior Courts serve as the primary trial courts. Justices of the Supreme Court and Courts of Appeal are appointed by the Governor, but are subject to retention by the electorate every 12 years. The administration of the state's court system is controlled by the Judicial Council, composed of the Chief Justice of the California Supreme Court, 14 judicial officers, four representatives from the State Bar of California, and one member from each house of the state legislature.
Local government.
Counties.
California is divided into 58 counties. Per Article 11, Section 1, of the Constitution of California, they are the legal subdivisions of the state. The county government provides countywide services such as law enforcement, jails, elections and voter registration, vital records, property assessment and records, tax collection, public health, health care, social services, libraries, flood control, fire protection, animal control, agricultural regulations, building inspections, ambulance services, and education departments in charge of maintaining statewide standards. In addition, the county serves as the local government for all unincorporated areas.
Cities and towns.
The state has 482 incorporated cities and towns; of which 460 are cities and 22 are towns. Under California law, the terms "city" and "town" are explicitly interchangeable; the name of an incorporated municipality in the state can either be "City of (Name)" or "Town of (Name)".
Sacramento became California's first incorporated city on February 27, 1850. San Jose, San Diego and Benicia tied for California's second incorporated city, each receiving incorporation on March 27, 1850. Jurupa Valley became the state's most recent and 482nd incorporated municipality on July 1, 2011.
The majority of these cities and towns are within one of five metropolitan areas: the Los Angeles Metropolitan Area, the San Francisco Bay Area, the Riverside-San Bernardino Area, the San Diego metropolitan area and the Sacramento metropolitan area.
The state recognizes two kinds of cities: charter and general law. General law cities owe their existence to state law and are consequently governed by it; charter cities are governed by their own city charters. Cities incorporated in the 19th century tend to be charter cities. All ten of the state's most populous cities are charter cities.
School districts and special districts.
About 1,102 school districts, independent of cities and counties, handle California's public education. California school districts may be organized as elementary districts, high school districts, unified school districts combining elementary and high school grades, or community college districts.
There are about 3,400 special districts in California. A special district, defined by California Government Code § 16271(d) as "any agency of the state for the local performance of governmental or proprietary functions within limited boundaries", provides a limited range of services within a defined geographic area. Most of California's special districts are "single-purpose districts", and provide one service.
Federal representation.
The State of California sends 53 members to the House of Representatives, the nation's largest congressional state delegation. Consequently California also has the largest number of electoral votes in national presidential elections, with 55. California's U.S. Senators are Dianne Feinstein and Barbara Boxer.
Politics.
California has an idiosyncratic political culture compared to the rest of the country, and is sometimes regarded as a trendsetter. In socio-cultural mores and national politics, Californians are perceived as more liberal than other Americans, especially those who live in the inland states.
Among the political idiosyncrasies and trendsetting, California was the second state to recall their state governor, the second state to legalize abortion, and the only state to ban marriage for gay couples twice by voters (including Proposition 8 in 2008). Voters also passed Proposition 71 in 2004 to fund stem cell research, and Proposition 14 in 2010 to completely change the state's primary election process. California has also experienced disputes over water rights; and a tax revolt, culminating with the passage of Proposition 13 in 1978, limiting state property taxes.
The state's trend towards the Democratic Party and away from the Republican Party can be seen in state elections. From 1899 to 1939, California had Republican governors. Since 1990, California has generally elected Democratic candidates to federal, state and local offices, including current Governor Jerry Brown; however, the state has elected Republican Governors, though many of its Republican Governors, such as Arnold Schwarzenegger, tend to be considered moderate Republicans and more centrist than the national party.
The Democrats also now hold a majority in both houses of the state legislature. There are 56 Democrats and 24 Republicans in the Assembly; and 26 Democrats and 12 Republicans in the Senate.
The trend towards the Democratic Party is most obvious in presidential elections; Republicans have not won California's electoral votes since 1988. Additionally, both the state's current Democratic U.S. Senators, Dianne Feinstein, a native and former mayor of San Francisco, and Barbara Boxer, a former congresswoman from Marin County, have held onto their seats since they were first elected in 1992.
In the U.S. House, the Democrats held a 34–19 edge in the CA delegation of the 110th United States Congress in 2007. As the result of gerrymandering, the districts in California were usually dominated by one or the other party, and few districts were considered competitive. In 2008, Californians passed Proposition 20 to empower a 14-member independent citizen commission to redraw districts for both local politicians and Congress. After the 2012 elections, when the new system took effect, Democrats gained 4 seats and held a 38-15 majority in the delegation.
In general, Democratic strength is centered in the populous coastal regions of the Los Angeles metropolitan area and the San Francisco Bay Area. Republican strength is still greatest in eastern parts of the state. Orange County also remains mostly Republican. One study ranked Berkeley, Oakland, Inglewood and San Francisco in the top 20 most liberal American cities; and Bakersfield, Orange, Escondido, Garden Grove, and Simi Valley in the top 20 most conservative cities.
In October 2012, out of the 23,802,577 people eligible to vote, 18,245,970 people were registered to vote. Of the people registered, the three largest registered groups were Democrats (7,966,422), Republicans (5,356,608), and Decline to State (3,820,545). Los Angeles County had the largest number of registered Democrats (2,430,612) and Republicans (1,037,031) of any county in the state.
LGBT.
California is considered generally liberal in its policies regarding the LGBT community, and the rights of lesbian, gay, bisexual, and transgender people have received greater recognition since 1960 at both the state and municipal level. California is home to a number of gay villages such as the Castro District in San Francisco, Hillcrest in San Diego, and West Hollywood. Through the Domestic Partnership Act of 1999, California became the first state in the United States to recognize same-sex relationships in any legal capacity. In 2000, voters passed Proposition 22, which restricted state recognition of marriage to opposite-sex couples. This was struck down by the California Supreme Court in May 2008, effectively legalizing same-sex marriage; however, this was overruled later that same year when California voters passed Proposition 8. After further judicial cases, in 2013 the U.S. Supreme Court rendered the law void, allowing same-sex marriages in California to resume.
Education.
Public secondary education consists of high schools that teach elective courses in trades, languages, and liberal arts with tracks for gifted, college-bound and industrial arts students. California's public educational system is supported by a unique constitutional amendment that requires a minimum annual funding level for grades K–12 and community colleges that grows with the economy and student enrollment figures.
California had over 6.2 million school students in the 2005–06 school year. Funding and staffing levels in California schools lag behind other states. In expenditure per pupil, California ranked 29th (of the 50 states and the District of Columbia) in 2005–06. In teaching staff expenditure per pupil, California ranked 49th of 51. In overall teacher-pupil ratio, California was also 49th, with 21 students per teacher. Only Arizona and Utah were lower.
A 2007 study concluded that California's public school system was "broken".
California's public postsecondary education offers a unique three tiered system:
California is also home to such notable private universities as Stanford University, the University of Southern California, the California Institute of Technology, and the Claremont Colleges. California has hundreds of other private colleges and universities, including many religious and special-purpose institutions.
Sports.
California has nineteen major professional sports league franchises, far more than any other state. The San Francisco Bay Area has seven major league teams spread in its three major cities: San Francisco, San Jose, and Oakland. While the Greater Los Angeles Area is home to ten major league franchises, it is also the largest metropolitan area not to have a team from the National Football League. San Diego has two major league teams, and Sacramento has one. The NFL Super Bowl has been hosted in California 11 times at four different stadiums: Los Angeles Memorial Coliseum, the Rose Bowl, Stanford Stadium, and San Diego's Qualcomm Stadium. A twelfth, Super Bowl 50, is scheduled to be held at the new Levi's Stadium in Santa Clara on February 7, 2016.
California has long had many respected collegiate sports programs. California is home to the oldest college bowl game, the annual Rose Bowl, among others.
California is the only US state to have hosted both the Summer and Winter Olympics. The 1932 and 1984 Summer Olympics were held in Los Angeles. Squaw Valley Ski Resort in the Lake Tahoe region hosted the 1960 Winter Olympics. Multiple games during the 1994 FIFA World Cup took place in California, with the Rose Bowl hosting eight matches including the final, while Stanford Stadium hosted six matches.
Below is a list of major league sports teams in California:

</doc>
<doc id="5408" url="http://en.wikipedia.org/wiki?curid=5408" title="Columbia River">
Columbia River

The Columbia River is the largest river in the Pacific Northwest region of North America. The river rises in the Rocky Mountains of British Columbia, Canada. It flows northwest and then south into the US state of Washington, then turns west to form most of the border between Washington and the state of Oregon before emptying into the Pacific Ocean. The river is long, and its largest tributary is the Snake River. Its drainage basin is roughly the size of France and extends into seven U.S. states and a Canadian province.
By volume, the Columbia is the fourth-largest river in the United States; it has the greatest flow of any North American river draining into the Pacific. The river's heavy flow and its relatively steep gradient gives it tremendous potential for the generation of electricity. The 14 hydroelectric dams on the Columbia's main stem and many more on its tributaries produce more hydroelectric power than those of any other North American river.
The Columbia and its tributaries have been central to the region's culture and economy for thousands of years. They have been used for transportation since ancient times, linking the many cultural groups of the region. The river system hosts many species of anadromous fish, which migrate between freshwater habitats and the saline Pacific Ocean. These fish—especially the salmon species—provided the core subsistence for natives; in past centuries, traders from across western North America traveled to the Columbia to trade for fish.
In the late 18th century, a private American ship became the first non-indigenous vessel to enter the river; it was followed by a British explorer, who navigated past the Oregon Coast Range into the Willamette Valley. In the following decades, fur trading companies used the Columbia as a key transportation route. Overland explorers entered the Willamette Valley through the scenic but treacherous Columbia River Gorge, and pioneers began to settle the valley in increasing numbers, following both routes to enter it. Steamships along the river linked communities and facilitated trade; the arrival of railroads in the late 19th century, many running along the river, supplemented these links.
Since the late 19th century, public and private sectors have heavily developed the river. The development, commonly referred to as taming or harnessing of the river, has been massive and multi-faceted. To aid ship and barge navigation, locks have been built along the lower Columbia and its tributaries, and dredging has opened, maintained, and enlarged shipping channels. Since the early 20th century, dams have been built across the river for the purposes of power generation, navigation, irrigation, and flood control. Today, a dam-impounded reservoir lies along nearly every U.S. mile of the once free-flowing river, and much of the Canadian stretch has been impounded as well. Production of nuclear power has taken place at two sites along the river. Plutonium for nuclear weapons was produced for decades at the Hanford Site, which is now the most contaminated nuclear site in the U.S. All these developments have had a tremendous impact on river environments, perhaps most notably through industrial pollution and barriers to fish migration.
Course.
The Columbia begins its journey in the southern Rocky Mountain Trench in British Columbia (BC). Columbia Lake – above sea level – and the adjoining Columbia Wetlands form the river's headwaters. The trench is a broad, deep, and long glacial valley between the Canadian Rockies and the Columbia Mountains in BC. For its first , the Columbia flows northwest along the trench through Windermere Lake and the town of Invermere, a region known in British Columbia as the Columbia Valley, then northwest to Golden and into Kinbasket Lake. Rounding the northern end of the Selkirk Mountains, the river turns sharply south through a region known as the Big Bend Country, passing through Revelstoke Lake and the Arrow Lakes. Revelstoke, the Big Bend, and the Columbia Valley combined are referred to in BC parlance as the Columbia Country. Below the Arrow Lakes, the Columbia passes the cities of Castlegar, located at the Columbia's confluence with the Kootenay River, and Trail, two major population centers of the West Kootenay region. The Pend Oreille River joins the Columbia about north of the U.S.–Canada border.
The Columbia enters eastern Washington flowing south and turning to the west at the Spokane River confluence. It marks the southern and eastern borders of the Colville Indian Reservation and the western border of the Spokane Indian Reservation. The river turns south after the Okanogan River confluence, then southeasterly near the confluence with the Wenatchee River in central Washington. This C‑shaped segment of the river is also known as the "Big Bend". During the Missoula Floods 10,000 to 15,000 years ago, much of the floodwater took a more direct route south, forming the ancient river bed known as the Grand Coulee. After the floods, the river found its present course, and the Grand Coulee was left dry. The construction of the Grand Coulee Dam in the mid-20th century impounded the river, forming Lake Roosevelt, from which water was pumped into the dry coulee, forming the reservoir of Banks Lake.
The river flows past The Gorge Amphitheatre, a prominent concert venue in the Northwest, then through Priest Rapids Dam, and then through the Hanford Nuclear Reservation. Entirely within the reservation is Hanford Reach, the only U.S. stretch of the river that is completely free-flowing, unimpeded by dams and not a tidal estuary. The Snake River and Yakima River join the Columbia in the Tri‑Cities population center. The Columbia makes a sharp bend to the west at the Washington–Oregon border. The river defines that border for the final of its journey.
The Deschutes River joins the Columbia near The Dalles. Between The Dalles and Portland, the river cuts through the Cascade Range, forming the dramatic Columbia River Gorge. No other river except for the Klamath completely breaches the Cascades—the other rivers that flow through the range also originate in or very near the mountains. The headwaters and upper course of the Pit River flows through much of the Cascades; in contrast the Columbia cuts through the range nearly a thousand miles from its source in the Rocky Mountains. The gorge is known for its strong and steady winds, scenic beauty, and its role as an important transportation link. The river continues west, bending sharply to the north-northwest near Portland and Vancouver, Washington, at the Willamette River confluence. Here the river slows considerably, dropping sediment that might otherwise form a river delta. Near Longview, Washington and the Cowlitz River confluence, the river turns west again. The Columbia empties into the Pacific Ocean just west of Astoria, Oregon, over the Columbia Bar, a shifting sandbar that makes the river's mouth one of the most hazardous stretches of water to navigate in the world. Because of the danger and the many shipwrecks near the mouth, it acquired a reputation as the "Graveyard of Ships".
The Columbia drains an area of about . Its drainage basin covers nearly all of Idaho, large portions of British Columbia, Oregon, and Washington, ultimately all of Montana west of the Continental Divide, and small portions of Wyoming, Utah, and Nevada; the total area is similar to the size of France. Roughly of the river's length and 85 percent of its drainage basin are in the U.S. The Columbia is the twelfth-longest river and has the sixth-largest drainage basin in the United States. In Canada, where the Columbia flows for and drains , the river ranks 23rd in length, and its basin ranks 13th in size. 
The Columbia shares its name with nearby places, such as British Columbia, as well as with landforms and bodies of water.
Discharge.
With an average flow at the mouth of about , the Columbia is the largest river by volume flowing into the Pacific from North America and is the fourth-largest by volume in the U.S. The average flow where the river crosses the international border between Canada and the United States is from a drainage basin of . This amounts to about 15 percent of the entire Columbia watershed. The Columbia's highest recorded flow, measured at The Dalles, was in June 1894, before the river was dammed. The lowest flow recorded at The Dalles was on April 16, 1968, and was caused by the initial closure of the John Day Dam, upstream. The Dalles is about from the mouth; the river at this point drains about or about 91 percent of the total watershed. Flow rates on the Columbia are affected by many large upstream reservoirs, many diversions for irrigation, and, on the lower stretches, reverse flow from the tides of the Pacific Ocean. The National Weather Service issues tide forecasts for eight places along the river between Astoria and the base of Bonneville Dam.
Geology.
When the rifting of Pangea, due to the process of plate tectonics, pushed North America away from Europe and Africa and into the Panthalassic Ocean (ancestor to the modern Pacific Ocean), the Pacific Northwest was not part of the continent. As the North American continent moved westward, the Farallon Plate subducted under its western margin. As the plate subducted, it carried along island arcs which were accreted to the North American continent, resulting in the creation of the Pacific Northwest between 150 and 90 million years ago. The general outline of the Columbia Basin was not complete until between 60 and 40 million years ago, but it lay under a large inland sea later subject to uplift. Between 40 and 20 million years ago, in the Eocene and Miocene eras, tremendous volcanic eruptions frequently modified much of the landscape traversed by the Columbia. The lower reaches of the ancestral river passed through a valley near where Mount Hood later arose. Carrying sediments from erosion and erupting volcanoes, it built a thick delta that underlies the foothills on the east side of the Coast Range near Vernonia in northwestern Oregon. Between 17 million and 6 million years ago, huge outpourings of flood basalt lava covered the Columbia River Plateau and forced the lower Columbia into its present course. The Cascade Range began to uplift during the early Pleistocene era (two million to 700,000 years ago). Cutting through the uplifting mountains, the Columbia River created the Columbia River Gorge.
The river and its drainage basin experienced some of the world's greatest known catastrophic floods toward the end of the last ice age. The periodic rupturing of ice dams at Glacial Lake Missoula resulted in the Missoula Floods, with discharges 10 times the combined flow of all the rivers of the world, dozens of times over thousands of years. The exact number of floods is unknown, but geologists have documented at least 40; evidence suggests that they occurred between about 19,000 and 13,000 years ago.
The floodwaters rushed across eastern Washington, creating the channeled scablands, which are a complex network of dry canyon-like channels, or coulees that are often braided and sharply gouged into the basalt rock underlying the region's deep topsoil. Numerous flat-topped buttes with rich soil stand high above the chaotic scablands. Constrictions at several places caused the floodwaters to pool into large temporary lakes, such as Lake Lewis, in which sediments were deposited. Water depths have been estimated at at Wallula Gap, at Bonneville Dam, and over modern Portland, Oregon. Sediments were also deposited when the floodwaters slowed in the broad flats of the Quincy, Othello, and Pasco Basins. The floods' periodic inundation of the lower Columbia River Plateau deposited rich sediments; 21st-century farmers in the Willamette Valley "plow fields of fertile Montana soil and clays from Washington's Palouse".
Over the last several thousand years a series of large landslides have occurred on the north side of the Columbia River Gorge, sending massive amounts of debris south from Table Mountain and Greenleaf Peak into the gorge near the present site of Bonneville Dam. The most recent and significant is known as the Bonneville Slide, which formed a massive earthen dam, filling of the river's length. Various studies have placed the date of the Bonneville Slide anywhere between 1060 and 1760 AD; the idea that the landslide debris present today was formed by more than one slide is relatively recent and may explain the large range of estimates. It has been suggested that if the later dates are accurate there may be a link with the 1700 Cascadia earthquake. The pile of debris resulting from the Bonneville Slide blocked the river until rising water finally washed away the sediment. It is not known how long it took the river to break through the barrier; estimates range from several months to several years. Much of the landslide's debris remained, forcing the river about south of its previous channel and forming the Cascade Rapids. In 1938, the construction of Bonneville Dam inundated the rapids as well as the remaining trees that could be used to refine the estimated date of the landslide.
In 1980, the eruption of Mount St. Helens deposited large amounts of sediment in the lower Columbia, temporarily reducing the depth of the shipping channel by .
Indigenous peoples.
Humans have inhabited the Columbia's watershed for more than 15,000 years, with a transition to a sedentary lifestyle based mainly on salmon starting about 3,500 years ago. In 1962, archaeologists found evidence of human activity dating back 11,230 years at the Marmes Rockshelter, near the confluence of the Palouse and Snake rivers in eastern Washington. In 1996 the skeletal remains of a 9,000-year-old prehistoric man (dubbed Kennewick Man) were found near Kennewick, Washington. The discovery rekindled debate in the scientific community over the origins of human habitation in North America and sparked a protracted controversy over whether the scientific or Native American community was entitled to possess and/or study the remains.
Many different Native Americans and First Nations peoples have a historical and continuing presence on the Columbia. South of the Canada–U.S. border, the Colville, Spokane, Coeur d'Alene, Yakama, Nez Perce, Cayuse, Palus, Umatilla, Cowlitz, and the Confederated Tribes of Warm Springs live along the U.S. stretch. Along the upper Snake River and Salmon River, the Shoshone Bannock tribes are present. The Sinixt or Lakes people lived on the lower stretch of the Canadian portion, while above that the Shuswap people (Secwepemc in their own language) reckon the whole of the upper Columbia east to the Rockies as part of their territory. The Canadian portion of the Columbia Basin outline the Traditional homelands of the Canadian Kootenay- Ktunaxa.
The Chinook tribe, which is not federally recognized, who live near the lower Columbia River, call it "Wimahl" in the Chinookan language. and "Nch’i-Wàna" to the Sahaptin-speaking peoples of its middle course in present-day Washington; The river is known as "swah'netk'qhu" by the Sinixt people, who live in the area of the Arrow Lakes in the river's upper reaches in Canada. All three terms essentially mean "the big river".
Oral histories describe the formation and destruction of the Bridge of the Gods, a land bridge that connected the Oregon and Washington sides of the river in the Columbia River Gorge. The bridge, which aligns with geological records of the Bonneville Slide, was described in some stories as the result of a battle between gods, represented by Mount Adams and Mount Hood, in their competition for the affection of a goddess, represented by Mount St. Helens. Native American stories about the bridge differ in their details but agree in general that the bridge permitted increased interaction between tribes on the north and south sides of the river.
Horses, originally acquired from Spanish New Mexico, spread widely via native trade networks, reaching the Shoshone of the Snake River Plain by 1700. The Nez Perce, Cayuse, and Flathead people acquired their first horses around 1730. Along with horses came aspects of the emerging plains culture, such as equestrian and horse training skills, greatly increased mobility, hunting efficiency, trade over long distances, intensified warfare, the linking of wealth and prestige to horses and war, and the rise of large and powerful tribal confederacies. The Nez Perce and Cayuse kept large herds and made annual long-distance trips to the Great Plains for bison hunting, adopted the plains culture to a significant degree, and became the main conduit through which horses and the plains culture diffused into the Columbia River region. Other peoples acquired horses and aspects of the plains culture unevenly. The Yakama, Umatilla, Palus, Spokane, and Coeur d'Alene maintained sizable herds of horses and adopted some of the plains cultural characteristics, but fishing and fish-related economies remained important. Less affected groups included the Molala, Klickitat, Wenatchi, Okanagan, and Sinkiuse-Columbia peoples, who owned small numbers of horses and adopted few plains culture features. Some groups remained essentially unaffected, such as the Sanpoil and Nespelem people, whose culture remained centered on fishing.
Natives of the region encountered foreigners at several times and places during the 18th and 19th centuries. European and American vessels explored the coastal area around the mouth of the river in the late 18th century, trading with local natives. The contact would prove devastating to the Indian tribes; a large portion of their population was wiped out by a smallpox epidemic. Canadian explorer Alexander Mackenzie crossed what is now interior British Columbia in 1793. From 1805 to 1807, the Lewis and Clark Expedition entered the Oregon Country along the Clearwater and Snake rivers, and encountered numerous small settlements of natives. Their records recount tales of hospitable traders who were not above stealing small items from the visitors. They also noted brass teakettles, a British musket, and other artifacts that had been obtained in trade with coastal tribes. From the earliest contact with westerners, the natives of the mid- and lower Columbia were not tribal, but instead congregated in social units no larger than a village, and more often at a family level; these units would shift with the season as people moved about, following the salmon catch up and down the river's tributaries.
Sparked by the 1848 Whitman Massacre, a number of violent battles were fought between American settlers and the region's natives. The subsequent Indian Wars, notably the Yakima War, decimated the native population and removed much land from native control. As years progressed, the right of natives to fish along the Columbia became the central issue of contention with the states, commercial fishers, and private property owners. The U.S. Supreme Court upheld fishing rights in landmark cases in 1905 and 1918, as well as the 1974 case United States v. Washington, commonly called the Boldt Decision.
Fish were central to the culture of the region's natives, both as sustenance and as part of their religious beliefs. Natives drew fish from the Columbia at several major sites, which also served as trading posts. Celilo Falls, located east of the modern city of The Dalles, was a vital hub for trade and the interaction of different cultural groups, being used for fishing and trading for 11,000 years. Prior to contact with westerners, villages along this stretch may have at times had a population as great as 10,000. The site drew traders from as far away as the Great Plains. The Cascades Rapids of the Columbia River Gorge, and Kettle Falls and Priest Rapids in eastern Washington, were also major fishing and trading sites.
In prehistoric times the Columbia's salmon and steelhead runs numbered an estimated annual average of 10 to 16 million fish. In comparison, the largest run since 1938 was in 1986, with 3.2 million fish entering the Columbia. The annual catch by natives has been estimated at . The most important and productive native fishing site was located at Celilo Falls, which was perhaps the most productive inland fishing site in North America. The falls were located at the border between Chinookan- and Sahaptian-speaking peoples and served as the center of an extensive trading network across the Pacific Plateau. Celilo was the oldest continuously inhabited community on the North American continent.
Salmon canneries established by white settlers beginning in 1866 had a strong negative impact on the salmon population, and in 1908 U.S. President Theodore Roosevelt observed that the salmon runs were but a fraction of what they had been 25 years prior.
As river development continued in the 20th century, each of these major fishing sites was flooded by a dam, beginning with Cascades Rapids in 1938. The development was accompanied by extensive negotiations between natives and U.S. government agencies. The Confederated Tribes of Warm Springs, a coalition of various tribes, adopted a constitution and incorporated after the 1938 completion of the Bonneville Dam flooded Cascades Rapids; Still, in the 1930s, there were natives who lived along the river and fished year round, moving along with the fish's migration patterns throughout the seasons. The Yakama were slower to do so, organizing a formal government in 1944. In the 21st century, the Yakama, Nez Perce, Umatilla, and Warm Springs tribes all have treaty fishing rights along the Columbia and its tributaries.
In 1957 Celilo Falls was submerged by the construction of The Dalles Dam, and the native fishing community was displaced. The affected tribes received a $26.8 million settlement for the loss of Celilo and other fishing sites submerged by The Dalles Dam. The Confederated Tribes of Warm Springs used part of its $4 million settlement to establish the Kah-Nee-Ta resort south of Mount Hood.
New waves of explorers.
Some historians believe that Japanese or Chinese vessels blown off course reached the Northwest Coast long before Europeans—possibly as early as 219 BCE. It is unknown whether they landed near the Columbia. Evidence exists that Spanish castaways reached the shore in 1679 and traded with the Clatsop; if these were indeed the first Europeans to see the Columbia, they failed to send word home to Spain.
In the 18th century, there was strong interest in discovering a Northwest Passage that would permit navigation between the Atlantic (or inland North America) and the Pacific Ocean. Many ships in the area, especially those under Spanish and British command, searched the northwest coast for a large river that might connect to Hudson Bay or the Missouri River. The first documented European discovery of the Columbia River was that of Bruno de Heceta, who in 1775 sighted the river's mouth. On the advice of his officers, he did not explore it, as he was short-staffed and the current was strong. He considered it a bay, and called it "Ensenada de Asunción". Later Spanish maps based on his discovery showed a river, labeled "Rio de San Roque", or an entrance, called "Entrada de Hezeta". Following Heceta's reports, British maritime fur trader Captain John Meares searched for the river in 1788 but concluded that it did not exist. He named Cape Disappointment for the non-existent river, not realizing the cape marks the northern edge of the river's mouth.
What happened next would form the basis for decades of both cooperation and dispute between British and American exploration of, and ownership claim to, the region. Royal Navy commander George Vancouver sailed past the mouth in April 1792 and observed a change in the water's color, but he accepted Meares' report and continued on his journey northward. Later that month, Vancouver encountered the American captain Robert Gray at the Strait of Juan de Fuca. Gray reported that he had seen the entrance to the Columbia and had spent nine days trying but failing to enter.
On May 12, 1792, Gray returned south and crossed the Columbia Bar, becoming the first explorer to enter the river. Gray's fur trading mission had been financed by Boston merchants, who outfitted him with a private vessel named "Columbia Rediviva"; he named the river after the ship on May 18. Gray spent nine days trading near the mouth of the Columbia, then left without having gone beyond upstream. The farthest point reached was Grays Bay at the mouth of Grays River. Gray's discovery of the Columbia River was later used by the United States to support its claim to the Oregon Country, which was also claimed by Russia, Great Britain, Spain and other nations.
In October 1792, Vancouver sent Lieutenant William Robert Broughton, his second-in-command, up the river. Broughton got as far as the Sandy River at the western end of the Columbia River Gorge, about upstream, sighting and naming Mount Hood. Broughton formally claimed the river, its drainage basin, and the nearby coast for Britain. In contrast, Gray had not made any formal claims on behalf of the United States.
Because the Columbia was at the same latitude as the headwaters of the Missouri River, there was some speculation that Gray and Vancouver had discovered the long-sought Northwest Passage. A 1798 British map showed a dotted line connecting the Columbia with the Missouri. However, when the American explorers Meriwether Lewis and William Clark charted the vast, unmapped lands of the American West in their overland expedition (1803–05), they found no passage between the rivers. After crossing the Rocky Mountains, Lewis and Clark built dugout canoes and paddled down the Snake River, reaching the Columbia near the present-day Tri-Cities, Washington. They explored a few miles upriver, as far as Bateman Island, before heading down the Columbia, concluding their journey at the river's mouth and establishing Fort Clatsop, a short-lived establishment that was occupied for less than three months.
Canadian explorer David Thompson, of the North West Company, spent the winter of 1807–08 at Kootenae House near the source of the Columbia at present-day Invermere, British Columbia. Over the next few years he explored much of the river and its northern tributaries. In 1811 he traveled down the Columbia to the Pacific Ocean, arriving at the mouth just after John Jacob Astor's Pacific Fur Company had founded Astoria. On his return to the north, Thompson explored the one remaining part of the river he had not yet seen, becoming the first European-American to travel the entire length of the river.
In 1825 the Hudson's Bay Company (HBC) established Fort Vancouver on the bank of the Columbia, in what is now Vancouver, Washington, as the headquarters of the company's Columbia District, which encompassed everything west of the Rocky Mountains. John McLoughlin, a physician, was appointed Chief Factor of the Columbia District. The HBC reoriented its Columbia District operations toward the Pacific Ocean via the Columbia, which became the region's main trunk route. In the early 1840s Americans began to colonize the Oregon country in large numbers via the Oregon Trail, despite the HBC's efforts to discourage American settlement in the region. For many the final leg of the journey involved travel down the lower Columbia River to Fort Vancouver. This part of the Oregon Trail, from The Dalles to Fort Vancouver, was the trail's most treacherous stretch, which prompted the 1846 construction of the Barlow Road.
In the Treaty of 1818 the United States and Britain agreed that both nations were to enjoy equal rights in Oregon Country for 10 years. By 1828, when the so-called "joint occupation" was renewed for an indefinite period, it seemed probable that the lower Columbia River would in time become the border. For years the Hudson's Bay Company successfully maintained control of the Columbia River and American attempts to gain a foothold were fended off. In the 1830s, however, American religious missions were established at several locations in the lower Columbia River region. And in the 1840s a mass migration of American settlers undermined British control. The Hudson's Bay Company tried to maintain dominance by shifting from the fur trade, which was in sharp decline, to exporting other goods such as salmon and lumber. Colonization schemes were attempted, but failed to match the scale of American settlement. Americans generally settled south of the Columbia, mainly in the Willamette Valley. The Hudson's Bay Company tried to establish settlements north of the river, but nearly all the British colonists moved south to the Willamette Valley. The hope that the British colonists might dilute the American flavor of the valley failed in the face of the overwhelming number of American settlers. These developments rekindled the issue of "joint occupation" and the boundary dispute. While some British interests, especially the Hudson's Bay Company, fought for a boundary along the Columbia River, the Oregon Treaty of 1846 set the boundary at the 49th parallel. The Columbia River did become the border between the U.S. territories of Oregon and Washington. Oregon became a U.S. state in 1859, Washington in 1889.
By the turn of the 20th century, the difficulty of navigating the Columbia was seen as an impediment to the economic development of the Inland Empire region east of the Cascades. The dredging and dam building that followed would permanently alter the river, disrupting its natural flow but also providing electricity, irrigation, navigability and other benefits to the region.
Navigation.
American captain Robert Gray and British captain George Vancouver, who explored the river in 1792, proved that it was possible to cross the Columbia Bar. Many of the challenges associated with that feat remain today; even with modern engineering alterations to the mouth of the river, the strong currents and shifting sandbar make it dangerous to pass between the river and the Pacific Ocean.
The use of steamboats along the river, beginning with the British "Beaver" in 1836 and followed by American vessels in 1850, contributed to the rapid settlement and economic development of the region. Steamboats operated in several distinct stretches of the river: on its lower reaches, from the Pacific Ocean to Cascades Rapids; from the Cascades to Celilo Falls; from Celilo to the confluence with the Snake River; on the Wenatchee Reach of eastern Washington; on British Columbia's Arrow Lakes; and on tributaries like the Willamette, the Snake and Kootenay Lake. The boats, initially powered by burning wood, carried passengers and freight throughout the region for many years. Early railroads served to connect steamboat lines interrupted by waterfalls on the river's lower reaches. In the 1880s, railroads maintained by companies such as the Oregon Railroad and Navigation Company and the Shaver Transportation Company began to supplement steamboat operations as the major transportation links along the river.
Opening the passage to Lewiston.
As early as 1881, industrialists proposed altering the natural channel of the Columbia to improve navigation. Changes to the river over the years have included the construction of jetties at the river's mouth, dredging, and the construction of canals and navigation locks. Today, ocean freighters can travel upriver as far as Portland and Vancouver, and barges can reach as far inland as Lewiston, Idaho.
The shifting Columbia Bar makes passage between the river and the Pacific Ocean difficult and dangerous, and numerous rapids along the river hinder navigation. "Pacific Graveyard," a 1964 book by James A. Gibbs, describes the numerous shipwrecks near the mouth of the Columbia.
Jetties, first constructed in 1886, extend the river's channel into the ocean. Strong currents and the shifting sandbar remain a threat to ships entering the river and necessitate continuous maintenance of the jetties.
In 1891 the Columbia was dredged to enhance shipping. The channel between the ocean and Portland and Vancouver was deepened from to . "The Columbian" called for the channel to be deepened to as early as 1905, but that depth was not attained until 1976.
Cascade Locks and Canal were first constructed in 1896 around the Cascades Rapids, enabling boats to travel safely through the Columbia River Gorge. The Celilo Canal, bypassing Celilo Falls, opened to river traffic in 1915. In the mid-20th century, the construction of dams along the length of the river submerged the rapids beneath a series of reservoirs. An extensive system of locks allowed ships and barges to pass easily from one reservoir to the next. A navigation channel reaching to Lewiston, Idaho, along the Columbia and Snake rivers, was completed in 1975. One of the main commodities is wheat, mainly for export. More than 40 percent of all US wheat exports are barged on the Columbia River.
The 1980 eruption of Mount St. Helens caused mudslides in the area, which reduced the Columbia's depth by for a stretch, disrupting Portland's economy.
Deeper shipping channel.
Efforts to maintain and improve the navigation channel have continued to the present day. In 1990 a new round of studies examined the possibility of further dredging on the lower Columbia. The plans were controversial from the start because of economic and environmental concerns.
In 1999, Congress authorized deepening the channel between Portland and Astoria from , which will make it possible for large container and grain ships to reach Portland and Vancouver. However, the project has met opposition because of concerns about stirring up toxic sediment on the riverbed. Portland-based Northwest Environmental Advocates brought a lawsuit against the Army Corps of Engineers, but it was rejected by the Ninth U.S. Circuit Court of Appeals in August 2006. The project includes measures to mitigate environmental damage; for instance, the U.S. Army Corps of Engineers must restore 12 times the area of wetland damaged by the project. In early 2006, the Corps spilled of hydraulic oil into the Columbia, drawing further criticism from environmental organizations.
Work on the project began in 2005 and concluded in 2010. The project's cost is estimated at $150 million. The federal government is paying 65 percent, Oregon and Washington are paying $27 million each, and six local ports are also contributing to the cost.
Dams.
In 1902, the United States Bureau of Reclamation was established to aid in the economic development of arid western states. One of its major undertakings was building Grand Coulee Dam to provide irrigation for the of the Columbia Basin Project in central Washington. With the onset of World War II, the focus of dam construction shifted to production of hydroelectricity. Irrigation efforts resumed after the war.
River development occurred within the structure of the 1909 International Boundary Waters Treaty between the U.S. and Canada. The United States Congress passed the Rivers and Harbors Act of 1925, which directed the Army Corps of Engineers and the Federal Power Commission to explore the development of the nation's rivers. This prompted agencies to conduct the first formal financial analysis of hydroelectric development; the reports produced by various agencies were presented in House Document 308. Those reports, and subsequent related reports, are referred to as 308 Reports.
In the late 1920s, political forces in the Northwestern United States generally favored private development of hydroelectric dams along the Columbia. But the overwhelming victories of gubernatorial candidate George W. Joseph in the 1930 Oregon Republican Party primary, and later his law partner Julius Meier, were understood to demonstrate strong public support for public ownership of dams. In 1933, President Franklin D. Roosevelt signed a bill that enabled the construction of the Bonneville and Grand Coulee dams as public works projects. The legislation was attributed to the efforts of Oregon Senator Charles McNary, Washington Senator Clarence Dill, and Oregon Congressman Charles Martin, among others.
In 1948 floods swept through the Columbia watershed, destroying Vanport, then the second largest city in Oregon, and impacting cities as far north as Trail, British Columbia. The flooding prompted the United States Congress to pass the Flood Control Act of 1950, authorizing the federal development of additional dams and other flood control mechanisms. By that time, however, local communities had become wary of federal hydroelectric projects, and sought local control of new developments; a public utility district in Grant County, Washington ultimately began construction of the dam at Priest Rapids.
In the 1960s, the United States and Canada signed the Columbia River Treaty, which focused on flood control and the maximization of downstream power generation. Canada agreed to build dams and provide reservoir storage, and the United States agreed to deliver to Canada one-half of the increase in U.S. downstream power benefits as estimated five years in advance. Canada's obligation was met by building three dams (two on the Columbia, and one on the Duncan River), the last of which was completed in 1973.
Today the main stem of the Columbia River has 14 dams, of which three are in Canada and 11 in the U.S. Four mainstem dams and four lower Snake River dams contain navigation locks to allow ship and barge passage from the ocean as far as Lewiston, Idaho. The river system as a whole has more than 400 dams for hydroelectricity and irrigation. The dams address a variety of demands, including flood control, navigation, stream flow regulation, storage and delivery of stored waters, reclamation of public lands and Indian reservations, and the generation of hydroelectric power.
The larger U.S. dams are owned and operated by the federal government (some by the Army Corps of Engineers and some by the Bureau of Reclamation), while the smaller dams are operated by public utility districts, and private power companies. The federally operated system is known as the Federal Columbia River Power System, which includes 31 dams on the Columbia and its tributaries. The system has altered the seasonal flow of the river in order to meet higher electricity demands during the winter. At the beginning of the 20th century, roughly 75 percent of the Columbia's flow occurred in the summer, between April and September. By 1980, the summer proportion had been lowered to about 50 percent, essentially eliminating the seasonal pattern.
The installation of dams dramatically altered the landscape and ecosystem of the river. At one time, the Columbia was one of the top salmon-producing river systems in the world. Previously active fishing sites, most notably Celilo Falls in the eastern Columbia River Gorge, have exhibited a sharp decline in fishing along the Columbia in the last century, and salmon populations have been dramatically reduced. Fish ladders have been installed at some dam sites to help the fish journey to spawning waters. Chief Joseph Dam has no fish ladders and completely blocks fish migration to the upper half of the Columbia River system.
Irrigation.
The Bureau of Reclamation's Columbia Basin Project focused on the generally dry region of central Washington known as the Columbia Basin, which features rich loess soil. Several groups developed competing proposals, and in 1933, President Franklin D. Roosevelt authorized the Columbia Basin Project. The Grand Coulee Dam was the project's central component; upon completion, it pumped water up from the Columbia to fill the formerly dry Grand Coulee, forming Banks Lake. By 1935, the intended height of the dam was increased from a range between to , a height that would extend the lake impounded by the dam all the way to the Canadian border; the project had grown from a local New Deal relief measure to a major national project.
The project's initial purpose was irrigation, but the onset of World War II created a high demand for electricity, mainly for aluminum production and for the development of nuclear weapons at the Hanford Site. Irrigation began in 1951. The project provides water to more than of fertile but arid land in central Washington, transforming the region into a major agricultural center. Important crops include orchard fruit, potatoes, alfalfa, mint, beans, beets, and wine grapes.
Since 1750, the Columbia has experienced six multi-year droughts. The longest, lasting 12 years in the mid‑19th century, reduced the river's flow to 20 percent below average. Scientists have expressed concern that a similar drought would have grave consequences in a region so dependent on the Columbia. In 1992–1993, a lesser drought affected farmers, hydroelectric power producers, shippers, and wildlife managers.
Many farmers in central Washington build dams on their property for irrigation and to control frost on their crops. The Washington Department of Ecology, using new techniques involving aerial photographs, estimated there may be as many as a hundred such dams in the area, most of which are illegal. Six such dams have failed in recent years, causing hundreds of thousands of dollars of damage to crops and public roads. Fourteen farms in the area have gone through the permitting process to build such dams legally.
Hydroelectricity.
The Columbia's heavy flow and extreme elevation drop over a short distance, , give it tremendous capacity for hydroelectricity generation. In comparison, the Mississippi drops less than . The Columbia alone possesses one-third of the United States's hydroelectric potential.
The largest of the 150 hydroelectric projects, the Grand Coulee Dam and the Chief Joseph Dam, are also the largest in the United States and among the largest in the world.
Inexpensive hydropower supported the location of a large aluminum industry in the region, because the reduction of metallic aluminum from bauxite ore requires large amounts of electricity. Until 2000, the Northwestern United States produced up to 17 percent of the world's aluminum and 40 percent of the aluminum produced in the U.S. But the commoditization of power in the early 21st century, coupled with drought that reduced the generation capacity of the river, damaged the industry. By 2001, Columbia River aluminum producers had idled 80 percent of its production capacity, and by 2003, the entire United States produced only 15 percent of the world's aluminum, with many smelters along the Columbia having gone dormant or out of business.
Power remains relatively inexpensive along the Columbia, and in recent years high-tech companies like Google have begun to move server farm operations into the area to avail themselves of cheap power.
Downriver of Grand Coulee, each dam's reservoir is closely regulated by the Bonneville Power Administration (BPA), the U.S. Army Corps of Engineers, and various Washington public utility districts to ensure flow, flood control, and power generation objectives are met. Increasingly, hydro-power operations are required to meet standards under the U.S. Endangered Species Act and other agreements to manage operations to minimize impacts on salmon and other fish, and some conservation and fishing groups support removing four dams on the lower Snake River, the largest tributary of the Columbia.
In 1941, the BPA hired Oklahoma folksinger Woody Guthrie to write songs for a documentary film promoting the benefits of hydropower. In the month he spent traveling the region Guthrie wrote 26 songs, which have become an important part of the cultural history of the region.
Ecology and environment.
Fish migration.
The Columbia supports several species of anadromous fish that migrate between the Pacific Ocean and fresh water tributaries of the river. Sockeye salmon, Coho and Chinook (also known as "king") salmon, and steelhead, all of the genus Oncorhynchus, are ocean fish that migrate up the rivers at the end of their life cycles to spawn. White sturgeon, which take 15 to 25 years to mature, typically migrate between the ocean and the upstream habitat several times during their lives.
Salmon populations declined dramatically after the establishment of canneries in 1867. By 1908, there was widespread concern about the decline of salmon and sturgeon. In that year, the people of Oregon passed two laws under their newly instituted program of Citizens' Initiatives limiting fishing on the Columbia and other rivers. Then in 1948, another initiative banned the use of seine nets (devices already used by Native Americans, and refined by later settlers) altogether.
Dams interrupt the migration of anadromous fish. Salmon and steelhead return to the streams in which they were born to spawn; where dams prevent their return, entire populations of salmon die. Some of the Columbia and Snake River dams employ fish ladders, which are effective to varying degrees at allowing these fish to travel upstream. Another problem exists for the juvenile salmon headed downstream to the ocean. Previously, this journey would have taken two to three weeks. With river currents slowed by the dams, and the Columbia converted from wild river to a series of slackwater pools, the journey can take several months, which increases the mortality rate. In some cases, the Army Corps of Engineers transports juvenile fish downstream by truck or river barge. The Chief Joseph Dam and several dams on the Columbia's tributaries entirely block migration, and there are no migrating fish on the river above these dams. Sturgeon have different migration habits and can survive without ever visiting the ocean. In many upstream areas cut off from the ocean by dams, sturgeon simply live upstream of the dam.
Not all fish have suffered from the modifications to the river; the northern pikeminnow (formerly known as the "squawfish") thrives in the warmer, slower water created by the dams. Research in the mid-1980s found that juvenile salmon were suffering substantially from the predatory pikeminnow, and in 1990, in the interest of protecting salmon, a "bounty" program was established to reward anglers for catching pikeminnow.
In 1994, the salmon catch was smaller than usual in the rivers of Oregon, Washington, and British Columbia, causing concern among commercial fishermen, government agencies, and tribal leaders. U.S. government intervention, to which the states of Alaska, Idaho, and Oregon objected, included an 11-day closure of an Alaska fishery. In April 1994 the Pacific Fisheries Management Council unanimously approved the strictest regulations in 18 years, banning all commercial salmon fishing for that year from Cape Falcon north to the Canadian border. In the winter of 1994, the return of coho salmon far exceeded expectations, which was attributed in part to the fishing ban.
Also in 1994, United States Secretary of the Interior Bruce Babbitt first proposed the removal of several Pacific Northwest dams because of their impact on salmon spawning. The Northwest Power Planning Council approved a plan that provided more water for fish and less for electricity, irrigation, and transportation. Environmental advocates have called for the removal of certain dams in the Columbia system in the years since. Of the 227 major dams in the Columbia River drainage basin, the four Washington dams on the lower Snake River are often identified for removal, notably in an ongoing lawsuit concerning a Bush administration plan for salmon recovery. These dams and reservoirs currently limit the recovery of upriver salmon runs to Idaho's Salmon and Clearwater rivers. Historically, the Snake produced over 1.5 million spring and summer Chinook Salmon, a number that has dwindled to several thousand in recent years. Idaho Power Company's Hells Canyon dams have no fish ladders (and do not pass juvenile salmon downstream), and thus allow no steelhead or salmon to migrate above Hells Canyon. In 2007, the destruction of the Marmot Dam on the Sandy River was the first dam removal in the system. Other Columbia Basin dams that have been removed include Condit Dam on Washington's White Salmon River, and the Milltown Dam on the Clark Fork in Montana.
Pollution.
In southeastern Washington, a stretch of the river passes through the Hanford Site, established in 1943 as part of the Manhattan Project. The site served as a plutonium production complex, with nine nuclear reactors and related facilities located on the banks of the river. From 1944 to 1971, pump systems drew cooling water from the river and, after treating this water for use by the reactors, returned it to the river. Before being released back into the river, the used water was held in large tanks known as retention basins for up to six hours. Longer-lived isotopes were not affected by this retention, and several terabecquerels entered the river every day. By 1957, the eight plutonium production reactors at Hanford dumped a daily average of 50,000 curies of radioactive material into the Columbia. These releases were kept secret by the federal government until the release of declassified documents in the late 1980s. Radiation was measured downstream as far west as the Washington and Oregon coasts.
The nuclear reactors were decommissioned at the end of the Cold War, and the Hanford site is now the focus of the world's largest environmental cleanup, managed by the Department of Energy under the oversight of the Washington Department of Ecology and the Environmental Protection Agency. Nearby aquifers contain an estimated 270 billion US gallons (1 billion m3) of groundwater contaminated by high-level nuclear waste that has leaked out of Hanford's massive underground storage tanks. , 1 million US gallons (3,785 m3) of highly radioactive waste is traveling through groundwater toward the Columbia River. This waste is expected to reach the river in 12 to 50 years if cleanup does not proceed on schedule.
In addition to concerns about nuclear waste, numerous other pollutants are found in the river. These include chemical pesticides, bacteria, arsenic, dioxins, and polychlorinated biphenyl (PCB).
Studies have also found significant levels of toxins in fish and the waters they inhabit within the basin. Accumulation of toxins in fish threatens the survival of fish species, and human consumption of these fish can lead to health problems. Water quality is also an important factor in the survival of other wildlife and plants that grow in the Columbia River drainage basin. The states, Indian tribes, and federal government are all engaged in efforts to restore and improve the water, land, and air quality of the Columbia River drainage basin and have committed to work together to enhance and accomplish critical ecosystem restoration efforts. A number of cleanup efforts are currently underway, including Superfund projects at Portland Harbor, Hanford, and Lake Roosevelt.
Timber industry activity further contaminates river water, notably in the increased sediment runoff that results from clearcuts. The Northwest Forest Plan, a piece of federal legislation from 1994, mandated that timber companies consider the environmental impacts of their practices on rivers like the Columbia.
On July 1, 2003, Christopher Swain of Portland, Oregon, became the first person to swim the Columbia River's entire length, in an effort to raise public awareness about the river's environmental health.
Watershed.
Most of the Columbia's drainage basin (which, at , is about the size of France) lies roughly between the Rocky Mountains on the east and the Cascade Mountains on the west. In the United States and Canada the term watershed is often used to mean drainage basin. The term "Columbia Basin" is used to refer not only to the entire drainage basin but also to subsets of the river's full watershed, such as the relatively flat and unforested area in eastern Washington bounded by the Cascades, the Rocky Mountains, and the Blue Mountains. Within the watershed are diverse landforms including mountains, arid plateaus, river valleys, rolling uplands, and deep gorges. Grand Teton National Park lies in the watershed, as well as parts of Yellowstone National Park, Glacier National Park, Mount Rainier National Park, and North Cascades National Park. Canadian National Parks in the watershed include Kootenay National Park, Yoho National Park, Glacier National Park, and Mount Revelstoke National Park. Hells Canyon, the deepest gorge in North America, and the Columbia Gorge are in the watershed. Vegetation varies widely, ranging from Western hemlock and Western redcedar in the moist regions to sagebrush in the arid regions. The watershed provides habitat for 609 known fish and wildlife species, including the bull trout, Bald Eagle, gray wolf, grizzly bear, and Canada lynx.
The World Wide Fund for Nature (WWF) divides the waters of the Columbia and its tributaries into three freshwater ecoregions, naming them Columbia Glaciated, Columbia Unglaciated, and Upper Snake. The Columbia Glaciated ecoregion, making up about a third of the total watershed, lies in the north and was covered with ice sheets during the Pleistocene. The ecoregion includes the mainstem Columbia north of the Snake River and tributaries such as the Yakima, Okanagan, Pend Oreille, Clark Fork, and Kootenay rivers. The effects of glaciation include a number of large lakes and a relatively low diversity of freshwater fish. The Upper Snake ecoregion is defined as the Snake River watershed above Shoshone Falls, which totally blocks fish migration. This region has 14 species of fish, many of which are endemic. The Columbia Unglaciated ecoregion makes up the rest of the watershed. It includes the mainstem Columbia below the Snake River and tributaries such as the Salmon, John Day, Deschutes, and lower Snake Rivers. Of the three ecoregions it is the richest in terms of freshwater species diversity. There are 35 species of fish, of which four are endemic. There are also high levels of mollusk endemism.
In 2000, about six million people lived within the Columbia's drainage basin. Of this total about 2.4 million people lived in Oregon, 1.7 million in Washington, 1 million in Idaho, half a million in British Columbia, and 0.4 million in Montana. Population in the watershed has been rising for many decades and is projected to rise to about 10 million by 2030. The highest population densities are found west of the Cascade Mountains along the I-5 corridor, especially in the Portland-Vancouver urban area. High densities are also found around Spokane, Washington, and Boise, Idaho. Although much of the watershed is rural and sparsely populated, areas with recreational and scenic values are growing rapidly. The central Oregon county of Deschutes is the fastest-growing in the state. Populations have also been growing just east of the Cascades in central Washington around the city of Yakima and the Tri-Cities area. Projections for the coming decades assume growth throughout the watershed, including the interior. The Canadian part of the Okanagan subbasin is also growing rapidly.
Climate varies greatly from place to place within the watershed. Elevation ranges from sea level at the river mouth to more than in the mountains, and temperatures vary with elevation. The highest peak is Mount Rainier, at . High elevations have cold winters and short cool summers; interior regions are subject to great temperature variability and severe droughts. Over some of the watershed, especially west of the Cascade Mountains, precipitation maximums occur in winter, when Pacific storms come ashore. Atmospheric conditions block the flow of moisture in summer, which is generally dry except for occasional thunderstorms in the interior. In some of the eastern parts of the watershed, especially shrub-steppe regions with Continental climate patterns, precipitation maximums occur in early summer. Annual precipitation varies from more than a year in the Cascades to less than in the interior. Much of the watershed gets less than a year.
Several major North American drainage basins and many minor ones share a common border with the Columbia River's drainage basin. To the east, in northern Wyoming and Montana, the Continental Divide separates the Columbia watershed from the Mississippi-Missouri watershed, which empties into the Gulf of Mexico. To the northeast, mostly along the southern border between British Columbia and Alberta, the Continental Divide separates the Columbia watershed from the Nelson-Lake Winnipeg-Saskatchewan watershed, which empties into Hudson Bay. The Mississippi and Nelson watersheds are separated by the Laurentian Divide, which meets the Continental Divide at Triple Divide Peak near the headwaters of the Columbia's Flathead River tributary. This point marks the meeting of three of North America's main drainage patterns, to the Pacific Ocean, to Hudson Bay, and to the Atlantic Ocean via the Gulf of Mexico.
Further north along the Continental Divide, a short portion of the combined Continental and Laurentian divides separate the Columbia watershed from the MacKenzie-Slave-Athabasca watershed, which empties into the Arctic Ocean. The Nelson and Mackenzie watersheds are separated by a divide between streams flowing to the Arctic Ocean and those of the Hudson Bay watershed. This divide meets the Continental Divide at Snow Dome (also known as Dome), near the northernmost bend of the Columbia River.
To the southeast, in western Wyoming, another divide separates the Columbia watershed from the Colorado–Green watershed, which empties into the Gulf of California. The Columbia, Colorado, and Mississippi watersheds meet at Three Waters Mountain in the Wind River Range of To the south, in Oregon, Nevada, Utah, Idaho, and Wyoming, the Columbia watershed is divided from the Great Basin, whose several watersheds are endorheic, not emptying into any ocean but rather drying up or sinking into sumps. Great Basin watersheds that share a border with the Columbia watershed include Harney Basin, Humboldt River, and Great Salt Lake. The associated triple divide points are Commissary Ridge North, Wyoming, and Sproats Meadow Northwest, Oregon. To the north, mostly in British Columbia, the Columbia watershed borders the Fraser River watershed. To the west and southwest the Columbia watershed borders a number of smaller watersheds that drain to the Pacific Ocean, such as the Klamath River in Oregon and California and the Puget Sound Basin in Washington.
Major tributaries.
The Columbia receives more than 60 significant tributaries. The four largest that empty directly into the Columbia (measured either by discharge or by size of watershed) are the Snake River (mostly in Idaho), the Willamette River (in northwest Oregon), the Kootenay River (mostly in British Columbia), and the Pend Oreille River (mostly in northern Washington and Idaho, also known as the lower part of the Clark Fork). Each of these four averages more than and drains an area of more than .
The Snake is by far the largest tributary. Its watershed of is larger than the state of Idaho. Its discharge is roughly a third of the Columbia's at the rivers' confluence, but compared to the Columbia upstream of the confluence the Snake is longer (113%) and has a larger drainage basin (104%).
The Pend Oreille river system (including its main tributaries, the Clark Fork and Flathead rivers) is also similar in size to the Columbia at their confluence. Compared to the Columbia River above the two rivers' confluence, the Pend Oreille-Clark-Flathead is nearly as long (about 86%), its basin about three-fourths as large (76%), and its discharge over a third (37%).

</doc>
<doc id="5409" url="http://en.wikipedia.org/wiki?curid=5409" title="Commelinales">
Commelinales

Commelinales is the botanical name of an order of flowering plants. It comprises five families: Commelinaceae, Haemodoraceae, Hanguanaceae, Philydraceae, and Pontederiaceae. All the families combined contain over 800 species in about 70 genera; the majority of species are in the Commelinaceae. Plants in the order share a number of synapomorphies that tie them together, such as a lack of mycorrhizal associations and tapetal raphides. Estimates differ as to when the Comminales evolved, but most suggest an origin and diversification sometime during the mid- to late Cretaceous. Depending on the methods used, studies suggest a range of origin between 123 to 73 million years, with diversification occurring within the group 110 to 66 million years ago. The order's closest relatives are in the Zingiberales, which includes ginger, bananas, cardamom, and others.
According to the most recent classification scheme, the APG III of 2009, the order includes five families: Commelinaceae, Haemodoraceae, Hanguanaceae, Philydraceae, and Pontederiaceae. This is unchanged from the APG II of 2003, but different from the older APG system of 1998, which did not include Hanguanaceae. The still older Cronquist system of 1981, which was not based on molecular data, placed the order in subclass Commelinidae of class Liliopsida and included the families Commelinaceae, Mayacaceae, Rapateaceae and Xyridaceae. These families are now known to be only distantly related.

</doc>
<doc id="5410" url="http://en.wikipedia.org/wiki?curid=5410" title="Cyperales">
Cyperales

Cyperales is a name for an order of flowering plants. As used in the Engler system (update, of 1964) and in the Wettstein system it consisted of only the single family. In the Cronquist system it is used for an order (placed in subclass "Commelinidae") and circumscribed as (1981):
The APG II system, used here, assigns the plants involved to the order "Poales".

</doc>
<doc id="5411" url="http://en.wikipedia.org/wiki?curid=5411" title="Cucurbitales">
Cucurbitales

The Cucurbitales are an order of flowering plants, included in the rosid group of dicotyledons. This order mostly belongs to tropical areas, with limited presence in subtropic and temperate regions. The order includes shrubs and trees, together with many herbs and climbers. One of major characteristics of the Cucurbitales is the presence of unisexual flowers, mostly pentacyclic, with thick pointed petals (whenever present). The pollination is usually performed by insects, but wind pollination is also present (in Coriariaceae and Datiscaceae).
The order consists of roughly 2600 species in eight families. The largest families are Begoniaceae (begonia family) with 1400 species and Cucurbitaceae (gourd family) with 825 species. The large families of Cucurbitales include several economically important plants. Specifically, the Cucurbitaceae (gourd family) are responsible for some food species, such as squash, pumpkin (both from "Cucurbita"), melons including watermelon ("Citrullus vulgaris"), and cucumber ("Cucumis"). The Begoniaceae are known for their horticultural species, of which there are over 130.
Overview.
The Cucurbitales are an order of cosmopolitan plants with distribution, particularly diverse in the tropics. Most are herbs, climber herbs, woody lianas or shrubs but some genus are canopy evergreen lauroid trees. Cucurbitales form an important component of tropical forests from low to montane forests with greater representation in terms of number of species. Although not known with certainty the total number of species in the family, conservative estimates indicate about 2600 species worldwide, distributed in 109 genera. Compared to other plant families, the taxonomy of the family is poorly understood due to their great diversity, difficulty in identification, and reduced taxonomic work done on it.
The order Cucurbitales in the eurosid I clade comprises almost 2600 species in 109 or 110 genera in eight families, tropical and temperate, of very different sizes, morphology, and ecology. It is a case of divergent evolution. In contrast, there is convergent evolution with other groups not related due to ecological or physical drivers toward a similar solution, including analogous structures.
Some botanical species are trees have similar foliage to the Lauraceae due to convergent evolution.
The patterns of speciation in the Cucurbitales is diversified in a high number of species. They have a pantropical distribution with centers of diversity in Africa, South America, and Southeast Asia. They most likely originated in West Gondwana and are 67–107 million years old, so the oldest split could relate to the break-up of Gondwana in the middle Eocene to late Oligocene, 45–24 million years ago, and reached their current distribution by multiple intercontinental dispersal events. One factor was product of aridification, others groups responded to favorable climatic periods and expanded across the available habitat, occur as opportunistic species across wide distribution; other groups diverged over long times in isolated areas.
The Cucurbitales comprise the families: Apodanthaceae, Anisophylleaceae, Begoniaceae, Coriariaceae, Corynocarpaceae, Cucurbitaceae, Tetramelaceae, and Datiscaceae. Some of the synapomorphies of the order are: leaves in spiral, secondary veins palmated, calyx or perianth valvate, elevated stomatal calyx/perianth with separate styles. The two whorls are similar in texture.
"Tetrameles nudiflora" is a tree of immense proportions of height and width; Tetramelaceae, Anisophylleaceae, and Corynocarpaceae are tall canopy trees in temperate and tropical forests. The genus "Dendrosicyos", with the only species being the cucumber tree, is adapted to the arid semidesert island of Socotra. Deciduous perennial Cucurbitales lose all of their leaves for part of the year depending on variations in rainfall. The leaf loss coincides with the dry season in tropical, subtropical and arid regions. In temperate or polar climates, the dry season is due to the inability of the plant to absorb water available in the form of ice. Apodanthaceae are obligatory endoparasites that only emerge once a year in the form of small flowers that develop into small berries.
Half of the species known are in the greatly diverse begonia group of Begoniaceae, with two genera, and 1500 species. Because of the lack of knowledge about the order in general, very little is known about the order diversity. The increase in the number of species is expected for the genera, bringing an expected considerable increase in the total number of species of the family. Before DNA-molecular classifications, the species placed now in Cucurbitales order, were assigned to orders as diverse as Ranunculales, Malpighiales, Violales, and Rafflesiales. Early molecular studies revealed several surprises, such as the nonmonophyly of the traditional Datiscaceae, including "Tetrameles" and "Octomeles", but the exact relationships among the families remained unclear.
Classification.
Under the Cronquist system, the first four families (including Begoniaceae, Cucurbitaceae, and Datiscaceae), were placed in the order Violales, within the Dilleniidae, with the Tetramelaceae subsumed within the Datiscaceae. The other families were distributed throughout various orders. The present classification is due to APG III (2009).

</doc>
<doc id="5412" url="http://en.wikipedia.org/wiki?curid=5412" title="Contra dance">
Contra dance

Contra dance (also contradance, contra-dance and other variant spellings) refers to several partnered folk dance styles in which couples dance in two facing lines or in a group of four. It has mixed origins from English country dance and French dance styles in the 17th century. Sometimes described as New England folk dance, contra dances can be found around the world and have some popularity in North America and the United Kingdom where weekly or monthly dances and annual dance weekends are common.
Considered a type of social dance, contra dancing involves steps that require a partner, where couples can be variously arranged in lines up and down the dance hall. Throughout the course of a dance, couples progress up and down these lines, typically interacting with different couples as they progress. The dance is led by a caller who facilitates a walkthrough before the actual dance begins. Callers describe a series of steps called "figures", and in a single dance, a caller may include anywhere from 6-12 individuals figures which are repeated as couples progress.
The accompanying music for contra dances includes, but is not limited to Irish, Scottish, old-timey and French-Canadian folk tunes. The fiddle is considered the core instrument, though other stringed instruments such as the guitar and mandolin are used as well. Music in a dance can consist of one or multiple songs, and key changes during the course of a dance are common.
History.
At the end of the 17th century, English country dances were taken up by French dance masters. The French called these dances "contra-dances" or "contredanses". As time progressed, these dances returned to England and were spread and reinterpreted in the United States, and eventually the French form of the name came to be associated with the American folk dances, where they were alternatively called "country dances" or in some parts of New England such as New Hampshire, "contradances."
Contra dances were fashionable in the United States and were considered one of the most popular social dances across class lines in the late 18th century, though these events were usually referred to as "country dances" until the 1780s, when the term contra dance became more common to describe these events. In the mid-19th century, group dances started to decline in popularity in favor of quadrilles, lancers, and couple dances such as the waltz and polka. By the late 19th century, square dances too had fallen out of favor, except in rural areas. When squares were revived (around 1925 to 1940, depending on the region), contra dances were generally not included. In the 1930s and 1940s, contra dances appear to have been done only in small towns in widely scattered parts of northeastern North America, such as Ohio, the Maritime provinces of Canada, and particularly northern New England. Ralph Page almost single-handedly maintained the New England tradition until it was revitalized in the 1950s and 1960s, particularly by Ted Sannella and Dudley Laufman. The New England contra dance tradition has been maintained in its purest form by the Ed Larkin Old Time Contra Dancers, formed by Vermont fiddler and prompter Edwin Loyal Larkin in 1934. Ed Larkin was born in Tunbridge, Vermont in 1867, and by age 21 was fiddling and prompting for contra dances all over the area. In 1934 he formed the group to maintain what he saw even then as a dying tradition. He died in 1954, but the group he founded is still performing, teaching the dances, and holding monthly open house dances in Tunbridge. Their primary fiddler, Harold Luce of Chelsea, Vermont, was born in October 1918 and continued fiddling until his death in August 2014. He was the last of the original members of the group. The Larkin Dancers are careful to teach and perform the dances in the traditional manner. They performed at the 1940 and 1964 World's Fairs in New York City, have demonstrated their art at the Smithsonian Institution, the New England Folk Festival, the Eastern States Exposition, the Vermont History Expo, and many other events. They have performed at the Tunbridge World's Fair every year since 1933, a year before their formal organization.
By then, early dance camps, retreats, and weekends had emerged, such as Pinewoods Camp, in Plymouth, Massachusetts, which became primarily a music and dance camp in 1933, and NEFFA, the New England Folk Festival, also in Massachusetts, which began in 1944. These and others continue to be popular and some offer other dances and activities besides contra dancing.
In the 1970s, Sannella and other callers introduced movements from English Country Dance, such as heys and gypsies, to the contra dances. New dances, such as "Shadrack's Delight" by Tony Parkes, featured symmetrical dancing by all couples. (Previously, the actives and inactives —see "Progression" below— had significantly different roles). Double progression dances, popularized by Herbie Gaudreau, added to the aerobic nature of the dances, and one caller, Gene Hubert, wrote a quadruple progression dance, "Contra Madness". Becket formation was introduced, with partners next to each other in the line instead of opposite. The Brattleboro Dawn Dance started in 1976, and continues to run semiannually.
In the early 1980s, Tod Whittemore started the first Saturday dance in the Peterborough Town House, which remains one of the more popular regional dances. The Peterborough dance influenced Bob McQuillen who became a notable musician in New England. As musicians and callers moved to other locations, they founded contra dances in Michigan, Washington, Oregon, California, Texas, and elsewhere.
In the 1970s, the Boston-area Lesbian and Gay Folk Dance was the first group regularly contra dancing without gender roles; its dances were referred to as "gender-free" or "gender-neutral" dances. In 1981, a group in Minneapolis/St. Paul, MN, called "Les be Gay and Dance" was started, in which callers deliberately eliminated references to gender by avoiding the terms "ladies" or "gents." In 1987, Chris Ricciotti started a gay dance group in Providence, RI, using the terms "ladies" and "gents," although dancers were not lining up according to gender. Other gender-free dance groups started in the area after that, and in 1989, at the gender-free dance group in Jamaica Plain, MA, a group of dancers led by Janet Dillon protested the use of these terms, and the armband system was devised: the traditionally male-role dancers would wear armbands and be called "armbands" or just "bands," and the traditionally female-role dancers would be called "bare arms" or just "bares."
Events.
Unless explicitly labeled otherwise, contra dance events are open to all, regardless of experience. They are family-friendly, and alcohol consumption is not part of the culture. Many events offer beginner-level instructions prior to the dance. A typical evening of contra dance is three hours long, including an intermission. The event consists of a number of individual "contra dances", divided by a scattering of other partner dances, perhaps one or more waltzes, schottisches, polkas, or Swedish hambos. In some places, square dances are thrown into the mix. Music for the evening is typically performed by a live band, playing jigs and reels from Ireland, Scotland, Canada, or the USA. The tunes are traditional and more than a century old, or modern compositions which follow the same form as the traditional pieces.
Generally, a leader, known as a caller, will teach each individual dance just before the music for that dance begins. During this introductory walkthrough, participants learn the dance by walking through the steps and formations, following the caller's instructions. The caller gives the instructions orally, and sometimes augments them with demonstrations of steps by experienced dancers in the group. The walkthrough usually proceeds in the order of the moves as they will be done with the music; in some dances, the caller may vary the order of moves during the dance, a fact that is usually explained as part of the caller's instructions.
After the walkthrough, the music begins and the dancers repeat that sequence some number of times before that dance ends, often 10 to 15 minutes, depending on the length of the contra lines. Calls are normally given at least the first few times through, and often for the last. At the end of each dance, the dancers thank their partners. The contra dance tradition in North America is to change partners for every dance, while in the United Kingdom typically people dance with the same partner the entire evening. One who attends an evening of contra dances in North America does not need to bring his or her own partner. In the short break between individual dances, women and men invite each other to dance. Booking ahead (lining up a partner or partners ahead of time for each individual dance) is common at some venues and other times discouraged.
Most contra dances do not have a strict dress code. No special outfits are worn, but comfortable and loose-fitting clothing that does not restrict movement is usually recommended. Lightweight skirts are often worn, at some dances by men as well as women, as these have a very pretty effect when swinging or twirling. However, low, broken-in, soft-soled, non-marking shoes, such as dance shoes, sneakers, or sandals, are recommended and, in some places, required. As dancing can be aerobic, dancers are sometimes encouraged to bring a change of clothes.
As in any social dance, cooperation is vital to contra dancing. Since over the course of any single dance, individuals interact with not just their partners but everyone else in the set, contra dancing might be considered a group activity. As will necessarily be the case when beginners are welcomed in by more practiced dancers, mistakes are made; most dancers are willing to help beginners in learning the steps. However, because the friendly, social nature of the dances can be misinterpreted or even abused, some groups have created anti-harassment policies.
Form.
Formations.
Contra dances are arranged in long paired lines of couples. A pair of lines is called a "set". Sets are generally arranged so they run the length of the hall, with the "top" or "head" of the set being the end closest to the band and caller. Correspondingly, the "bottom" or "foot" of the set is the end farthest from the caller.
Couples consist of two people, traditionally but not necessarily one male and one female, typically referred to as the gent, gentleman or man, and lady or woman. Couples interact primarily with an adjacent couple for each round of the dance. Each sub-group of two interacting couples is known to choreographers as a "minor set" and to dancers as a "foursome" or "hands four". Couples in the same minor set are "neighbors". Minor sets originate at the head of the set, starting with the topmost dancers as the 1s (the "active couple" or "actives"); the other couple are "2s" (or "inactives"). The 1s are said to be "above" their neighboring 2s; 2s are "below". If there is an uneven number of couples dancing, the bottom-most couple will "wait out" the first time through the dance.
There are three common ways of arranging couples in the minor sets: "proper" formation, "improper" formation, and "Becket" formation. There are many additional forms a contra dance may take. Five of them are: "triple minor", "triplet", "indecent", "four-face-four", and "whole-set". (For diagrams and full descriptions, see Contra Dance Form main article.)
Progression.
A fundamental aspect of contra dancing is that the same dance, one time through which lasts roughly 30 seconds, is repeated over and over - but each time you dance with new neighbors. This change is effected by "progressing" the 1s "down" the set and progressing the 2s "up" (also "up the hall" and "down the hall"; see the article on contra dance form for full characterizations of the progression in the eight dance forms mentioned above).
A single dance runs around ten minutes, long enough to progress 15-20 times. If the sets are short to medium length the caller will often try to run the dance until each couple has danced with every other couple both as a 1 and a 2 and returned to where they started. With longer sets (more than ~40 people) this would require long enough sets that the caller will usually only run the dance all the way around on (rare) non equal-turn dances.
Choreography.
Contra dance choreography specifies the dance formation, the "figures", and the sequence of those figures in a dance. Notably, contra dance figures (with a few exceptions) do not have defined footwork; within the limits of the music and the comfort of their fellow dancers, individuals move according to their own taste.
Most contra dances consist of a sequence of about 6 to 12 individual figures, prompted by the caller in time to the music as the figures are danced. As the sequence repeats, the caller may cut down his or her prompting, and eventually drop out, leaving the dancers to each other and the music.
A figure is a pattern of movement that typically takes eight "counts", although figures with four or 16 counts are also common. Each dance is a collection of figures assembled to allow the dancers to progress along the set (see "Progression," above).
A count (as used above) is one half of a musical measure, such as one quarter note in 2/4 time or three eighth notes in 6/8 time. A count may also be called a "step", as contra dance is a walking form, and each count of a dance typically matches a single physical step in a figure.
Typical contra dance choreography comprises four parts, each 16 counts (8 measures) long. The parts are called A1, A2, B1 and B2. This nomenclature stems from the music: Most contra dance tunes (as written) have two parts (A and B), each 8 measures long, and each fitting one part of the dance. The A and B parts are each played twice in a row, hence, A1, A2, B1, B2. While the same music is generally played in, for example, parts A1 and A2, distinct choreography is followed in those parts. Thus, a contra dance is typically 64 "counts", and goes with a 32 "measure" tune. Tunes of this form are called "square"; tunes that deviate from this form are called "crooked".
Sample contra dances:
Many modern contra dances have these characteristics:
An event which consists primarily (or solely) of dances in this style is sometimes referred to as a Modern Urban Contra Dance.
Music.
The most common contra dance repertoire is rooted in the Anglo-Celtic tradition as it developed in North America. Irish, Scottish, French Canadian, and Old-time tunes are common, and Klezmer tunes have also been used. The old-time repertoire includes very few of the jigs common in the others.
Tunes used for a contra dance are nearly always "square" 64-beat tunes, in which one time through the tune is each of two 16-beat parts played twice (this is notated AABB). However, any 64-beat tune will do; for instance, three 8-beat parts could be played AABB AACC, or two 8-beat parts and one 16-beat part could be played AABB CC. Tunes not 64 beats long are called "crooked" and are almost never used for contra dancing, although a few crooked dances have been written as novelties.
In terms of instrumentation, fiddles are considered to be the primary instrument in contra dancing, though other stringed instruments can also be used, such as the mandolin, banjo, or guitar. Occasionally, percussion instruments are also used in contra dancing, such as the Irish bodhran or less frequently, the dumbek or washboard.
Until the 1970s it was traditional to play a single tune for the duration of a contra dance (about 5 to 10 minutes). Since then, contra dance musicians have typically played tunes in sets of two or three related (and sometimes contrasting) tunes, though single-tune dances are again becoming popular with some northeastern bands. In the Celtic repertoires it is common to change keys with each tune. A set might start with a tune in G, switch to a tune in D, and end with a tune in Bm. Here, D is related to G as its dominant (5th), while D and Bm share a key signature of two sharps. In the old-time tradition the musicians will either play the same tune for the whole dance, or switch to tunes in the same key. This is because the tunings of the banjo are key-specific. An old-time band might play a set of tunes in D, then use the time between dances to retune for a set of tunes in A. (Fiddlers also may take this opportunity to retune; tune- or key-specific fiddle tunings are uncommon in American Anglo-Celtic traditions other than old-time.)
In the Celtic repertoires it is most common for bands to play sets of reels and sets of jigs. However, since the underlying beat structure of jigs and reels is the same (two "counts" per bar) bands will occasionally mix jigs and reels in a set.
In recent years, younger contra dancers have begun establishing "crossover contra" or "techno contra" - contra dancing to techno, hip-hop, and other modern forms of music. While challenging for DJs and callers, the fusion of contra patterns with moves from hip-hop, tango, and other forms of dance has made this form of contra dance a rising trend since 2008; it has become especially prevalent in Asheville, NC, but regular techno contra dance series are spreading up the East Coast to locales such as Charlottesville, VA, Washington, DC, Amherst, MA, and Greenfield, MA, with one-time or annual events cropping up in locations further West, including California and Washington state.

</doc>
<doc id="5413" url="http://en.wikipedia.org/wiki?curid=5413" title="Coin collecting">
Coin collecting

Coin collecting is the collecting of coins or other forms of minted legal tender.
Coins of interest to collectors often include those that circulated for only a brief time, coins with mint errors and especially beautiful or historically significant pieces. Coin collecting can be differentiated from numismatics in that the latter is the systematic study of currency. Though closely related, the two disciplines are not necessarily the same. A numismatist may or may not be a coin collector, and vice versa.
History.
People have hoarded coins for their bullion value for as long as coins have been minted. However, the collection of coins for their artistic value was a later development. Evidence from the archaeological and historical record of Ancient Rome and medieval Mesopotamia indicates that coins were collected and catalogued by scholars and state treasuries. It also seems probable that individual citizens collected old, exotic or commemorative coins as an affordable, portable form of art. According to Suetonius in his "De vita Caesarum" ("The Lives of the Twelve Caesars"), written in the first century CE, the emperor Augustus sometimes presented old and exotic coins to friends and courtiers during festivals and other special occasions.
Contemporary coin collecting and appreciation began around the fourteenth century. During the Renaissance, it became a fad among some members of the privileged classes, especially kings and queens. The Italian scholar and poet Petrarch is credited with being the pursuit's first and most famous aficionado. Following his lead, many European kings, princes, and other nobility kept collections of ancient coins. Some notable collectors were Pope Boniface VIII, Emperor Maximilian I of the Holy Roman Empire, Louis XIV of France, Ferdinand I, Henry IV of France and Elector Joachim II of Brandenburg, who started the Berlin Coin Cabinet (German: "Münzkabinett Berlin"). Perhaps because only the very wealthy could afford the pursuit, in Renaissance times coin collecting became known as the "Hobby of Kings."
During the 17th and 18th centuries coin collecting remained a pursuit of the well-to-do. But rational, Enlightenment thinking led to a more systematic approach to accumulation and study. Numismatics as an academic discipline emerged in these centuries at the same time as coin collecting became a leisure pursuit of a growing middle class, eager to prove their wealth and sophistication. During the 19th and 20th centuries, coin collecting increased further in popularity. The market for coins expanded to include not only antique coins, but foreign or otherwise exotic currency. Coin shows, trade associations, and regulatory bodies emerged during these decades. The first international convention for coin collectors was held 15–18 August 1962, in Detroit, Michigan, and was sponsored by the American Numismatic Association and the Royal Canadian Numismatic Association. Attendance was estimated at 40,000. As one of the oldest and most popular world pastimes, coin collecting is now often referred to as the "King of Hobbies".
Motivations for coin collecting.
“Art in the form of coins is not only what we study but the emotion when we hold a piece of history” - Geoffrey Cope.
The motivations for collecting are varied. Possibly the most common type of collector is the hobbyist, who amasses a collection purely for fun with no real expectation of profit. This is especially true of casual collectors and children who collect items on the basis of chance and personal interest.
Another frequent reason for purchasing coins is as an investment. As with stamps, precious metals or other commodities, coin prices are cyclical based on supply and demand. Prices drop for coins that are not in long-term demand, and increase along with a coin's perceived or intrinsic value. Investors buy with the expectation that the value of their purchase will increase over the long term. As with all types of investment, the principle of "caveat emptor" applies and study is recommended before buying. Likewise, as with most collectibles, a coin collection does not produce income until it is sold, and may even incur costs (for example, the cost of safe deposit box storage) in the interim.
Coin hoarders may be similar to investors in the sense that they accumulate coins for potential long-term profit. However, unlike investors, they typically do not take into account aesthetic considerations; rather they gather whatever quantity of coins they can and hold them. This is most common with coins whose metal value exceeds their spending value.
Speculators, be they amateurs or commercial buyers, generally purchase coins in bulk and often act with the expectation of short-term profit. They may wish to take advantage of a spike in demand for a particular coin (for example, during the annual release of Canadian numismatic collectibles from the Royal Canadian Mint). The speculator might hope to buy the coin in large lots and sell at profit within weeks or months. Speculators may also buy common circulation coins for their intrinsic metal value. Coins without collectible value may be melted down or distributed as bullion for commercial purposes. Typically they purchase coins that are composed of rare or precious metals, or coins that have a high purity of a specific metal.
A final type of collector is the inheritor, an accidental collector who acquires coins (a collection, hoard or investment) from another person as part of an inheritance. The inheritor may not necessarily have an interest in or know anything about numismatics at the time of the acquisition.
Collector types.
Casual coin collectors often begin the hobby by saving notable coins found by chance. These coins may be pocket change left from an international trip or an old coin found in circulation.
Usually, if the enthusiasm of the novice increases over time, random coins found in circulation are not enough to satisfy their interest. The hobbyist may then trade coins in a coin club or buy coins from dealers or mints. Their collection then takes on a more specific focus.
Some enthusiasts become "generalists" and accumulate a few examples from a broad variety of historical or geographically significant coins. Given enough resources, this can result in a vast collection. King Farouk of Egypt was a generalist with a collection famous for its scope and variety.
Most collectors decide to focus their financial resources on a narrower, "specialist" interest. Some collectors focus on coins of a certain nation or historic period. Some collect coins by themes (or 'subjects') that are featured on the artwork displayed on the coin. Others will seek error coins. Still others might focus on exonumia such as medlas, tokens or challenge coins. For example, John Yarwood of Melbourne is the first person to take a serious interest in British military money (especially tokens).
Some collectors are "completists" and seek an example of every type of coin within a certain category. Perhaps the most famous of these is Louis Eliasberg, the only collector thus far to assemble a complete set of known coins of the United States.
Coin collecting can become a "competitive" activity, as prompted by the recent emergence of PCGS (Professional Coin Grading Service) and NGC (Numismatic Guarantee Corporation) Registry Sets. Registry Sets are private collections of coins verified for ownership and quality by numismatic grading services. The grading services assess collections, seal the coins in clear plastic holders, then register and publish the results. This can lead to very high prices as dedicated collectors compete for the very best specimens of, for example, each date and mint mark combination.
Common collection themes.
A few common themes are often combined into a goal for a collection:
Collecting counterfeits and forgeries is a controversial area because of the possibility that counterfeits might someday reenter the coin market as authentic coins, but US statutory and case law do not explicitly prohibit possession of counterfeit coins.
Coin condition and value.
In coin collecting, the condition of a coin is paramount to its value; a high-quality example is often worth many times more than a poor example. Collectors have created systems to describe the overall condition of coins.
In the early days of coin collecting—before the development of a large international coin market—extremely precise grades were not needed. Coins were described using only three adjectives: "good," "fine" or "uncirculated". By the mid 20th century, with the growing market for rare coins, the American Numismatic Association helps identify most coins in North America. It uses a 1–70 numbering scale, where 70 represents a perfect specimen and I represents a barely identifiable coin. Descriptions and numeric grades for coins (from highest to lowest) is as follows:
In addition to the rating of coins by their wear, Proof coinage occurs as a separate category. These are specimens struck from polished dies and are often packaged and sold by mints. This is frequently done for Commemorative coins, though annual proof sets of circulating coinage may be issued as well. Unless mishandled, they will stay in Mint State. Collectors often desire both the proof and regular ("business strike") issues of a coin, though the difference in price between the two may be significant.
Coin experts in Europe and elsewhere often shun the numerical system, preferring to rate specimens on a purely descriptive, or adjectival, scale. Nevertheless, most grading systems use similar terminology and values and remain mutually intelligible.
When evaluating a coin, the following—often subjective—factors may be considered: 1) "eye appeal" or the aesthetic interest of the coin; 2) dents on the rim; 3) unsightly scratches or other blemishes on the surface of the coin; 4) luster; 5) toning; 6) level of detail retained, where a coin with full details obviously is valued higher than one with worn details. If the coin is judged favorably in all of these criteria, it will generally be awarded a higher grade.
Damage of any sort (e.g., holes, edge dents, repairs, cleaning, re-engraving or gouges) can substantially reduce the value of a coin. Specimens are occasionally cleaned or polished in an attempt to pass them off as higher grades or as uncirculated strikes. Because of the substantially lower prices for cleaned or damaged coins, some enthusiasts specialize in their collection.
Coin grading services.
Coin grading services are a relatively recent phenomenon, having emerged in the 1980s as a response to the need for buyers and sellers to agree on common measures of a coin's value. 
Such services are an effort to bring more confidence to investors in rare coins under the guise of being "official." Grading services certify the authenticity and rate the quality of individual coins, thus—it is hoped—establishing the worth of the coin relative to all others of its kind. Many coin grading services will also seal coins in a labeled, air-tight plastic holder, ensuring the coin is protected from deterioration. This process is commonly known as coin slabbing and is most prevalent in the US market. The two most prominent grading services at present are the Numismatic Guaranty Corporation (NGC) and the Professional Coin Grading Service (PCGS).
While professional grading has reduced the number of counterfeits foisted upon investors and has improved buyer confidence substantially, the goal of creating a sight-unseen market for coins remains somewhat elusive. Professional grading services remain the subject of controversy because grading is ultimately subjective; coins may receive different grades by different services or even upon resubmission to the same service. In addition, the numeric grade alone cannot represent other factors, such as toning, which are often important considerations to buyers. Due to potentially large differences in value over slight differences in a coin's condition, some commercial coin dealers will repeatedly resubmit a coin to a grading service in the hope of a higher grade. Furthermore, grading services are not free, and by using them collectors funnel money into an ancillary aspect of the hobby, instead of purchasing additional material for their collections. Reputable coin dealers eschew the growing tendency to sell higher graded coins via sales pitches asserting that grading removes subjectivity as to the value of mint state coins. Dealers that sponsor infomercials on shortwave radio are notorious for such sales pitches, most notably Discount Gold & Silver Trading on WWCR, which proclaims graded coins, in direct contradiction to their pricing history, as having appreciated more than non-graded coins, or as not likewise being "opinion coins."

</doc>
<doc id="5415" url="http://en.wikipedia.org/wiki?curid=5415" title="Crokinole">
Crokinole

Crokinole ( ) is a dexterity board game similar in various ways to pitchnut, carrom, marbles, and shove ha'penny, with elements of shuffleboard and curling reduced to table-top size. Players take turns shooting discs across the circular playing surface, trying to have their discs land in the higher-scoring regions of the board, while also attempting to knock away opposing discs.
Equipment.
Board dimensions vary with a playing surface typically of polished wood or laminate approximately in diameter. The arrangement is 3 concentric rings worth 5, 10, and 15 points as you move in from the outside. There is a shallow 20-point hole at the center. The inner 15-point ring is guarded with 8 small bumpers or posts. The outer ring of the board is divided into four quadrants. The outer edge of the board is raised a bit to keep errant shots from flying out, with a gutter between the playing surface and the edge to collect discarded pieces. Crokinole boards are typically octagonal or round in shape. The discs are roughly checker-sized, slightly smaller in diameter than the board's central hole, and may have concave faces to reduce sliding friction. Alternatively, the game may be played with ring-shaped pieces with a central hole.
Powder.
The use of any lubricating powder in crokinole is controversial, with some purists reviling the practice.
Powder is used to ensure pieces slide smoothly on the surface. According to Carrom rules, the powder must be of high quality to keep the surface smooth and dry, and shall not be wet. Pouches and containers are used to spread the powder over the playing surface. There must be no impurity in the powder. Boric acid powder is mostly used for this purpose.
In the UK, many players use a version of anti-set-off spray powder, from the printing industry, which has specific electrostatic properties, with particles of 50-micrometre diameter (). The powder is made of pure food-grade plant/vegetable starch.
Gameplay.
Crokinole is most commonly played by two players, or by four players in teams of two, with partners sitting across the board from each other. Players take turns flicking their discs from the outer edge of their quadrant of the board onto the playfield. Shooting is usually done by flicking the disc with a finger, though sometimes small cue sticks may be used. If there are any enemy discs on the board, a player must make contact, directly or indirectly, with an enemy disc during the shot. If unsuccessful, the shot disc is "fouled" and removed from the board, along with any of the player's other discs that were moved during the shot.
When there are no enemy discs on the board, many (but not all) rules also state that a player must shoot for the centre of the board, and a shot disc must finish either completely inside the 15-point guarded ring line, or (depending on the specifics of the rules) be inside or touching this line. This is often called the "no hiding" rule, since it prevents players from placing their first shots where their opponent must traverse completely though the guarded centre ring to hit them and avoid fouling. When playing without this rule, a player may generally make any shot desired, and as long as a disc remains completely inside the outer line of the playfield, it remains on the board. During any shot, any disc that falls completely into the recessed central "20" hole (aka the "Toad") is removed from play, and counts as twenty points for the owner of the disc at the end of the round, assuming the shot is valid.
Scoring occurs after all pieces (generally 12 per player or team) have been played, and is differential: i.e., the player or team with higher score is awarded the difference between the higher and lower scores for the round, thus only one team or player each round gains points. Play continues until a predetermined winning score is reached.
History of the game.
The earliest known crokinole board was made by craftsman Eckhardt Wettlaufer in 1876 in Perth County, Ontario, Canada. Several other home-made boards of southwestern Ontario origin, and dating from the 1870s, have been discovered since the 1990s. It seems to have been patented on April 20, 1880, in New York City by Joshua K. Ingalls.
Crokinole is often believed to be of Mennonite or Amish origins, but there is no factual data to support such a claim. The reason for this misconception may be due to its popularity in Mennonite and Amish groups. The game was viewed as a rather innocuous pastime – unlike the perception that diversions such as card playing or dancing were considered "works of the Devil" as held by many 19th-century Protestant groups.
The oldest roots of crokinole, from the 1860s, suggest the British and South Asian games are the most likely antecedents of what became crokinole.
In 2006, a documentary film called "Crokinole" was released. The world premiere occurred at the Princess Cinema in Waterloo, Ontario, in early 2006. The movie follows some of the competitors of the 2004 World Crokinole Championship as they prepare for the event.
Origins of the name.
The name "Crokinole" derives from "croquignole", a French word today designating:
It also used to designate of the action of flicking with the finger (Molière, "Le malade imaginaire"; or Voltaire, "Lettre à Frédéric II Roi de Prusse"; etc.), and this seems the most likely origin of the name of the game. "Croquignole" was also a synonym of "pichenotte", a word that gave its name to the different but related games of pichenotte and pitchnut.
Crokinole is called "knipsbrat" ("flick-board") in the Low German spoken by Mennonites.
World Crokinole Championship.
The World Crokinole Championship (WCC) tournament has been held annually since 1999 on the first Saturday of June in Tavistock, Ontario. Tavistock was chosen as the host city because it was the home of Eckhardt Wettlaufer, the maker of the earliest known board. The reigning world Adult single's crokinole champion is Brian Cook from Toronto, Ontario. The reigning world Adult doubles champions are Fred Slater and Justin Slater of Toronto.
The WCC begins with a qualifying round in which competitors play 10 matches against randomly assigned opponents. The qualifying round is divided into two separate sessions to accommodate the large number of players. At the end of the opening round, the top 16 competitors move on to the playoffs. The top four in the playoffs advance to the semifinals to play each other, and the top two compete in the finals.
The WCC has multiple divisions, including a singles finger-shooting category for competitive players (Adult Singles), novices (Recreational), and younger players (Intermediate, 11-14 yrs; Junior, 6-10 yrs). The WCC also awards a prize for the top 20-hole shooter in the Competitive Singles qualifying round, in the Recreational Singles qualifying round, in the Intermediate Singles, and in the Junior Singles.
Chinese Championships of Crokinole.
Originating in Dalian in 2005, the Chinese Championships of Crokinole (CCC) has been heavily contested since. Dalian was chosen due to the large number of Canadian expatriates in the area.
In 2005, the CCC established that the winner of the Chinese event was to be given a seat at the World Championships. This, of course, gave great prestige to the Chinese version and in the first four years of play only one winner has emerged from China. The current 4-time champion is Adrian Conradi; however, due to his return to Canada a new champion will be declared. The tournament is a double-knockout formula, best of three series, games to 100.

</doc>
<doc id="5416" url="http://en.wikipedia.org/wiki?curid=5416" title="Capitalism">
Capitalism

Capitalism is an economic system in which trade, industry, and the means of production are largely or entirely privately owned and operated for profit. Central characteristics of capitalism include capital accumulation, competitive markets and wage labour. In a capitalist economy, the parties to a transaction typically determine the prices at which assets, goods, and services are exchanged.
The degree of competition, role of intervention and regulation, and scope of public ownership varies across different models of capitalism. Economists, political economists, and historians have taken different perspectives in their analysis of capitalism and recognized various forms of it in practice. These include "laissez-faire" capitalism, welfare capitalism, crony capitalism and state capitalism; each highlighting varying degrees of dependency on markets, public ownership, and inclusion of social policies. The extent to which different markets are free, as well as the rules defining private property, is a matter of politics and policy. Many states have what are termed capitalist mixed economies, referring to a mix between planned and market-driven elements. Capitalism has existed under many forms of government, in many different times, places, and cultures. Following the demise of feudalism, capitalism became the dominant economic system in the Western world.
Capitalism was carried across the world by broader processes of globalization such as imperialism and, by the end of the nineteenth century, became the dominant "global" economic system, in turn intensifying processes of economic and other globalization. Later, in the 20th century, capitalism overcame a challenge by centrally-planned economies and is now "the" encompassing system worldwide, with the mixed economy being its dominant form in the industrialized Western world. Barry Gills and Paul James write:
Different economic perspectives emphasize specific elements of capitalism in their preferred definition. "Laissez-faire" and liberal economists emphasize the degree to which government does not have control over markets and the importance of property rights. Neoclassical and Keynesian macro-economists emphasize the need for government regulation to prevent monopolies and to soften the effects of the boom and bust cycle. Marxian economists emphasize the role of capital accumulation, exploitation and wage labor. Most political economists emphasize private property as well, in addition to power relations, wage labor, class, and the uniqueness of capitalism as a historical formation.
Proponents of capitalism argue that it creates more prosperity than any other economic system, and that its benefits are mainly to the ordinary person. Critics of capitalism variously associate it with economic instability, an inability to provide for the well-being of all people, and an unsustainable danger to the natural environment. Socialists maintain that, although capitalism is superior to all previously existing economic systems (such as feudalism or slavery), the contradiction between class interests will only be resolved by advancing into a completely social system of production and distribution in which all persons have an equal relationship to the means of production.
The term "capitalism", in its modern sense, is often attributed to Karl Marx. In his magnum opus "Capital", Marx analysed the "capitalist mode of production" using a method of understanding today known as Marxism. However, Marx himself rarely used the term “capitalism”, while it was used twice in the more political interpretations of his work, primarily authored by his collaborator Friedrich Engels. In the 20th century defenders of the capitalist system often replaced the term "capitalism" with phrases such as "free enterprise" and "private enterprise" and replaced "capitalist" with "rentier" and "investor" in reaction to the negative connotations associated with capitalism.
History.
Economic trade for profit has existed since at least the second millennium BC. Early Islam promulgated capitalist economic policies, which migrated to Europe through trade partners from cities such as Venice. However, capitalism in its modern form is usually traced to the emergence of agrarian capitalism and mercantilism in the Early Modern era.
Agrarian capitalism.
The economic foundations of the feudal agricultural system began to shift substantially in 16th century England; the manorial system had broken down by this time, and land began to be concentrated in the hands of fewer landlords with increasingly large estates. Instead of a serf-based system of labor, workers were increasingly being employed as part of a broader and expanding money economy. The system put pressure on both the landlords and the tenants to increase the productivity of the agriculture to make profit; the weakened coercive power of the aristocracy to extract peasant surpluses encouraged them to try out better methods, and the tenants also had incentive to improve their methods, in order to flourish in an increasingly competitive labor market. Terms of rent for the land were becoming subject to economic market forces rather than the previous stagnant system of custom and feudal obligation.
By the early 17th-century, England was a centralized state, in which much of the feudal order of Medieval Europe had been swept away. This centralization was strengthened by a good system of roads and a disproportionately large capital city, London. The capital acted as a central market hub for the entire country, creating a very large internal market for goods, instead of the fragmented feudal holdings that prevailed in most parts of the Continent.
Mercantilism.
The economic doctrine that held sway between the sixteenth and eighteenth centuries is commonly described as mercantilism. This period, the Age of Discovery, was associated with the geographic exploration of foreign lands by merchant traders, especially from England and the Low Countries. Mercantilism was a system of trade for profit, although commodities were still largely produced by non-capitalist production methods. Most scholars consider the era of merchant capitalism and mercantilism as the origin of modern capitalism, although Karl Polanyi argued that the hallmark of capitalism is the establishment of generalized markets for what he referred to as the "fictitious commodities": land, labor, and money. Accordingly, he argued that "not until 1834 was a competitive labor market established in England, hence industrial capitalism as a social system cannot be said to have existed before that date."
England began a large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). A systematic and coherent explanation of balance of trade was made public through Thomas Mun's argument "England's Treasure by Forraign Trade, or the Balance of our Forraign Trade is The Rule of Our Treasure." It was written in the 1620s and published in 1664.
Among the major tenets of mercantilist theory was bullionism, a doctrine stressing the importance of accumulating precious metals. Mercantilists argued that a state should export more goods than it imported so that foreigners would have to pay the difference in precious metals. Mercantilists argued that only raw materials that could not be extracted at home should be imported; and promoted government subsidies, such as the granting of monopolies and protective tariffs, which mercantilists thought were necessary to encourage home production of manufactured goods.
European merchants, backed by state controls, subsidies, and monopolies, made most of their profits from the buying and selling of goods. In the words of Francis Bacon, the purpose of mercantilism was "the opening and well-balancing of trade; the cherishing of manufacturers; the banishing of idleness; the repressing of waste and excess by sumptuary laws; the improvement and husbanding of the soil; the regulation of prices ..."
The British East India Company and the Dutch East India Company inaugurated an expansive era of commerce and trade. These companies were characterized by their colonial and expansionary powers given to them by nation-states. During this era, merchants, who had traded under the previous stage of mercantilism, invested capital in the East India Companies and other colonies, seeking a return on investment.
Industrial capitalism.
A new group of economic theorists, led by David Hume and Adam Smith, in the mid-18th century, challenged fundamental mercantilist doctrines such as the belief that the amount of the world's wealth remained constant and that a state could only increase its wealth at the expense of another state.
During the Industrial Revolution, the industrialist replaced the merchant as a dominant factor in the capitalist system and affected the decline of the traditional handicraft skills of artisans, guilds, and journeymen. Also during this period, the surplus generated by the rise of commercial agriculture encouraged increased mechanization of agriculture. Industrial capitalism marked the development of the factory system of manufacturing, characterized by a complex division of labor between and within work process and the routine of work tasks; and finally established the global domination of the capitalist mode of production.
Britain also abandoned its protectionist policy, as embraced by mercantilism. In the 19th century, Richard Cobden and John Bright, who based their beliefs on the Manchester School, initiated a movement to lower tariffs. In the 1840s, Britain adopted a less protectionist policy, with the repeal of the Corn Laws and the Navigation Acts. Britain reduced tariffs and quotas, in line with David Ricardo's advocacy for free trade.
Globalization.
Industrialization allowed cheap production of household items using economies of scale, while rapid population growth created sustained demand for commodities. Globalization in this period was decisively shaped by nineteenth-century imperialism.
After the First and Second Opium Wars and the completion of British conquest of India, vast populations of these regions became ready consumers of European exports. It was in this period that areas of sub-Saharan Africa and the Pacific islands were incorporated into the world system. Meanwhile, the conquest of new parts of the globe, notably sub-Saharan Africa, by Europeans yielded valuable natural resources such as rubber, diamonds and coal and helped fuel trade and investment between the European imperial powers, their colonies, and the United States.
The global financial system was mainly tied to the gold standard in this period. The United Kingdom first formally adopted this standard in 1821. Soon to follow was Canada in 1853, Newfoundland in 1865, and the United States and Germany ("de jure") in 1873. New technologies, such as the telegraph, the transatlantic cable, the Radiotelephone, the steamship and railway allowed goods and information to move around the world at an unprecedented degree.
Keynesianism and Monetarism.
In the period following the global depression of the 1930s, the state played an increasingly prominent role in the capitalistic system throughout much of the world. The post war era was greatly influenced by Keynesian economic stabilization policies. The postwar boom ended in the late 1960s and early 1970s, and the situation was worsened by the rise of stagflation.
Monetarism, a theoretical alternative to Keynesianism that is more compatible with laissez-faire, gained increasing prominence in the capitalist world, especially under the leadership of Ronald Reagan in the US and Margaret Thatcher in the UK in the 1980s. Public and political interest began shifting away from the so-called collectivist concerns of Keynes's managed capitalism to a focus on individual choice, called "remarketized capitalism." 
Economic elements.
The essential feature of capitalism is the investment of money in order to make a profit.
In a capitalist economic system capital assets can be owned and controlled by private persons, labor is purchased for money wages, capital gains accrue to private owners, and the price mechanism is utilized to allocate capital goods between competing uses. The extent to which the price mechanism is used, the degree of competitiveness, the balance between the public sector and the private sector, and the extent of government intervention in markets are the factors which distinguish several forms of capitalism in the modern world.
In free-market and "laissez-faire" forms of capitalism, markets are utilized most extensively with minimal or no regulation over the pricing mechanism. In mixed economies, which are almost universal today, markets continue to play a dominant role but are regulated to some extent by government in order to correct market failures, promote social welfare, conserve natural resources, fund defense and public safety or for other reasons. In state capitalist systems, markets are relied upon the least, with the state relying heavily on state-owned enterprises or indirect economic planning to accumulate capital.
Capitalism and capitalist economics is often contrasted with socialism, though the meaning of the word "socialism" has changed over time. The original meaning of socialism was state ownership of the means of production. Today, the word is often used to mean any state control of economic decision-making.
Money, capital, and accumulation.
Money is primarily a standardized medium of exchange, and final means of payment, that serves to measure the value of all goods and commodities in a standard of value. It is an abstraction of economic value and medium of exchange that eliminates the cumbersome system of barter by separating the transactions involved in the exchange of products, thus greatly facilitating specialization and trade through encouraging the exchange of commodities. Capitalism involves the further abstraction of money into other exchangeable assets and the accumulation of money through ownership, exchange, interest and various other financial instruments.
The accumulation of capital refers to the process of "making money", or growing an initial sum of money through investment in production. Capitalism is based around the accumulation of capital, whereby financial capital is invested in order to realize a profit and then reinvested into further production in a continuous process of accumulation. In Marxian economic theory, this dynamic is called the law of value.
Capital and financial markets.
The defining feature of capitalist markets, in contrast to markets and exchange in pre-capitalist societies like feudalism, is the existence of a market for capital goods (the means of production), meaning exchange-relations (business relationships) exist within the production process. Additionally, capitalism features a market for labor. This distinguishes the capitalist market from pre-capitalist societies which generally only contained market exchange for final goods and secondary goods. The "market" in capitalism refers to capital markets and financial markets. Thus, there are three main markets in a typical capitalistic economy: labor, goods and services, and financial.
Wage labor and class structure.
Wage labor refers to the class-structure of capitalism, whereby workers receive either a wage or a salary, and owners receive the profits generated by the factors of production employed in the production of economic value. Individuals who possess and supply financial capital to productive ventures become owners, either jointly (as shareholders) or individually. In Marxian economics these owners of the means of production and suppliers of capital are generally called "capitalists". The description of the role of the "capitalist" has shifted, first referring to a useless intermediary between producers to an employer of producers, and eventually came to refer to owners of the means of production. The term "capitalist" is not generally used by supporters of mainstream economics.
"Workers" includes those who expend both manual and mental (or creative) labor in production, where production does not simply mean physical production but refers to the production of both tangible and intangible economic value. "Capitalists" are individuals who derive income from investments.
Labor includes all physical and mental human resources, including entrepreneurial capacity and management skills, which are needed to produce products and services. Production is the act of making goods or services by applying labor power.
Macroeconomics.
Macroeconomics keeps its eyes on things such as inflation: a general increase in prices and fall in the purchasing value of money; growth: how much money a government has and how quickly it accrues money; unemployment, and rates of trade between other countries. Whereas microeconomics deals with individual firms, people, and other institutions that work within a set frame work of rules to balance prices and the workings of a singular government.
Both micro and macroeconomics work together to form a single set of evolving rules and regulations. Governments (the macroeconomic side) set both national and international regulations that keep track of prices and corporations' (microeconomics) growth rates, set prices, and trade, while the corporations influence what federal laws are set.
Types of capitalism.
There are many variants of capitalism in existence that differ according to country and region. They vary in their institutional makeup and by their economic policies. The common features among all the different forms of capitalism is that they are based on the production of goods and services for profit, predominately market-based allocation of resources, and they are structured upon the accumulation of capital. The major forms of capitalism are listed below:
Mercantilism.
Mercantilism is a nationalist form of early capitalism that came into existence approximately in the late 16th century. It is characterized by the intertwining of national business interests to state-interest and imperialism, and consequently, the state apparatus is utilized to advance national business interests abroad. An example of this is colonists living in America who were only allowed to trade with and purchase goods from their respective mother countries (Britain, France, etc.). Mercantilism holds that the wealth of a nation is increased through a positive balance of trade with other nations, and corresponds to the phase of capitalist development called the Primitive accumulation of capital.
Free-market economy.
Free-market economy refers to a capitalist economic system where prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy. It typically entails support for highly competitive markets, private ownership of productive enterprises. "Laissez-faire" is a more extensive form of free-market economy where the role of the state is limited to protecting property rights.
Social-market economy.
A social-market economy is a nominally free-market system where government intervention in price formation is kept to a minimum but the state provides significant services in the area of social security, unemployment benefits and recognition of labor rights through national collective bargaining arrangements. This model is prominent in Western and Northern European countries, and Japan, albeit in slightly different configurations. The vast majority of enterprises are privately owned in this economic model.
Rhine capitalism refers to the contemporary model of capitalism and adaptation of the social market model that exists in continental Western Europe today.
State capitalism.
State capitalism consists of state ownership of the means of production within a state, and the organization of state enterprises as commercial, profit-seeking businesses. The debate between proponents of private versus state capitalism is centered around questions of managerial efficacy, productive efficiency, and fair distribution of wealth.
According to Aldo Musacchio, a professor at Harvard Business School, it is a system in which governments, whether democratic or autocratic, exercise a widespread influence on the economy, through either direct ownership or various subsidies. Musacchio also emphasizes the difference between today's state capitalism and its predecessors. Gone are the days when governments appointed bureaucrats to run companies. The world's largest state-owned enterprises are traded on the public markets and kept in good health by large institutional investors.
Corporate capitalism.
Corporate capitalism is a free or mixed-market economy characterized by the dominance of hierarchical, bureaucratic corporations.
Mixed economy.
A mixed economy is a largely market-based economy consisting of both private and public ownership of the means of production and economic interventionism through macroeconomic policies intended to correct market failures, reduce unemployment and keep inflation low. The degree of intervention in markets varies among different countries. Some mixed economies, such as France under dirigisme, also featured a degree of indirect economic planning over a largely capitalist-based economy.
Most capitalist economies are defined as "mixed economies" to some degree.
Other.
Other variants of capitalism include:
Etymology and early usage.
The term "capitalist" as referring to an owner of capital (rather than its meaning of someone adherent to the economic system) shows earlier recorded use than the term "capitalism", dating back to the mid-17th century. "Capitalist" is derived from "capital", which evolved from "capitale", a late Latin word based on "caput", meaning "head" — also the origin of "chattel" and "cattle" in the sense of movable property (only much later to refer only to livestock). Capitale emerged in the 12th to 13th centuries in the sense of referring to funds, stock of merchandise, sum of money, or money carrying interest. By 1283 it was used in the sense of the capital assets of a trading firm. It was frequently interchanged with a number of other words — wealth, money, funds, goods, assets, property, and so on.
The "Hollandische Mercurius" uses "capitalists" in 1633 and 1654 to refer to owners of capital. In French, Étienne Clavier referred to "capitalistes" in 1788, six years before its first recorded English usage by Arthur Young in his work "Travels in France" (1792). David Ricardo, in his "Principles of Political Economy and Taxation" (1817), referred to "the capitalist" many times. Samuel Taylor Coleridge, an English poet, used "capitalist" in his work "Table Talk" (1823). Pierre-Joseph Proudhon used the term "capitalist" in his first work, "What is Property?" (1840) to refer to the owners of capital. Benjamin Disraeli used the term "capitalist" in his 1845 work "Sybil". Karl Marx and Friedrich Engels used the term "capitalist" ("Kapitalist") in "The Communist Manifesto" (1848) to refer to a private owner of capital.
According to the "Oxford English Dictionary" (OED), the term "capitalism" was first used by novelist William Makepeace Thackeray in 1854 in "The Newcomes", where he meant "having ownership of capital". Also according to the OED, Carl Adolph Douai, a German-American socialist and abolitionist, used the term "private capitalism" in 1863.
The initial usage of the term "capitalism" in its modern sense has been attributed to Louis Blanc in 1850 and Pierre-Joseph Proudhon in 1861. Karl Marx and Friedrich Engels referred to the "capitalistic system" ("kapitalistisches System") and to the capitalist mode of production ("kapitalistische Produktionsform") in "Das Kapital" (1867). The use of the word "capitalism" in reference to an economic system appears twice in Volume I of "Das Kapital", p. 124 (German edition), and in "Theories of Surplus Value", tome II, p. 493 (German edition). Marx did not extensively use the form "capitalism", but instead those of "capitalist" and "capitalist mode of production", which appear more than 2600 times in the trilogy "Das Kapital".
Marx's notion of the capitalist mode of production is characterised as a system of primarily private ownership of the means of production in a mainly market economy, with a legal framework on commerce and a physical infrastructure provided by the state. He believed that no legal framework was available to protect the laborers, and so exploitation by the companies was rife. Engels made more frequent use of the term "capitalism"; volumes II and III of "Das Kapital", both edited by Engels after Marx's death, contain the word "capitalism" four and three times, respectively. The three combined volumes of "Das Kapital" (1867, 1885, 1894) contain the word "capitalist" more than 2,600 times.
An 1877 work entitled "Better Times" by Hugh Gabutt and an 1884 article in the "Pall Mall Gazette" also used the term "capitalism". A later use of the term "capitalism" to describe the production system was by the German economist Werner Sombart, in his 1902 book "Modern Capitalism" ("Der moderne Kapitalismus"). Sombart's close friend and colleague, Max Weber, also used "capitalism" in his 1904 book "The Protestant Ethic and the Spirit of Capitalism" ("Die protestantische Ethik und der Geist des Kapitalismus").
Perspectives.
Classical political economy.
The classical school of economic thought emerged in Britain in the late 18th century. The classical political economists Adam Smith, David Ricardo, Jean-Baptiste Say, and John Stuart Mill published analyses of the production, distribution and exchange of goods in a market that have since formed the basis of study for most contemporary economists.
In France, 'Physiocrats' like François Quesnay promoted free trade based on a conception that wealth originated from land. Quesnay's "Tableau Économique" (1759), described the economy analytically and laid the foundation of the Physiocrats' economic theory, followed by Anne Robert Jacques Turgot who opposed tariffs and customs duties and advocated free trade. Richard Cantillon defined long-run equilibrium as the balance of flows of income, and argued that the supply and demand mechanism around land influenced short-term prices.
Smith's attack on mercantilism and his reasoning for "the system of natural liberty" in "The Wealth of Nations" (1776) are usually taken as the beginning of classical political economy. Smith devised a set of concepts that remain strongly associated with capitalism today. His theories regarding the "invisible hand" are commonly interpreted to mean individual pursuit of self-interest unintentionally producing collective good for society. It was necessary for Smith to be so forceful in his argument in favor of free markets because he had to overcome the popular mercantilist sentiment of the time period.
He criticized monopolies, tariffs, duties, and other state enforced restrictions of his time and believed that the market is the most fair and efficient arbitrator of resources. This view was shared by David Ricardo, second most important of the classical political economists and one of the most influential economists of modern times.
In "On the Principles of Political Economy and Taxation" (1817), he developed the law of comparative advantage, which explains why it is profitable for two parties to trade, even if one of the trading partners is more efficient in every type of economic production. This principle supports the economic case for free trade. Ricardo was a supporter of Say's Law and held the view that full employment is the normal equilibrium for a competitive economy. He also argued that inflation is closely related to changes in quantity of money and credit and was a proponent of the law of diminishing returns, which states that each additional unit of input yields less and less additional output.
The values of classical political economy are strongly associated with the classical liberal doctrine of minimal government intervention in the economy, though it does not necessarily oppose the state's provision of a few basic public goods. Classical liberal thought has generally assumed a clear division between the economy and other realms of social activity, such as the state.
While economic liberalism favors markets unfettered by the government, it maintains that the state has a legitimate role in providing public goods. For instance, Adam Smith argued that the state has a role in providing roads, canals, schools and bridges that cannot be efficiently implemented by private entities. However, he preferred that these goods should be paid proportionally to their consumption (e.g. putting a toll). In addition, he advocated retaliatory tariffs to bring about free trade, and copyrights and patents to encourage innovation.
Marxist political economy.
Karl Marx considered capitalism to be a historically specific mode of production (the way in which the productive property is owned and controlled, combined with the corresponding social relations between individuals based on their connection with the process of production) in which capitalism has become the dominant mode of production.
The capitalist stage of development or "bourgeois society," for Marx, represented the most advanced form of social organization to date, but he also thought that the working classes would come to power in a worldwide socialist or communist transformation of human society as the end of the series of first aristocratic, then capitalist, and finally working class rule was reached.
Following Adam Smith, Marx distinguished the use value of commodities from their exchange value in the market. Capital, according to Marx, is created with the purchase of commodities for the purpose of creating new commodities with an exchange value higher than the sum of the original purchases. For Marx, the use of labor power had itself become a commodity under capitalism; the exchange value of labor power, as reflected in the wage, is less than the value it produces for the capitalist.
This difference in values, he argues, constitutes surplus value, which the capitalists extract and accumulate. In his book "Capital", Marx argues that the capitalist mode of production is distinguished by how the owners of capital extract this surplus from workers—all prior class societies had extracted surplus labor, but capitalism was new in doing so via the sale-value of produced commodities. He argues that a core requirement of a capitalist society is that a large portion of the population must not possess sources of self-sustenance that would allow them to be independent, and must instead be compelled, to survive, to sell their labor for a living wage.
In conjunction with his criticism of capitalism was Marx's belief that the working class, due to its relationship to the means of production and numerical superiority under capitalism, would be the driving force behind the socialist revolution. This argument is intertwined with Marx's version of the labor theory of value arguing that labor is the source of all value, and thus of profit.
Vladimir Lenin, in "Imperialism, the Highest Stage of Capitalism" (1916), further developed Marxist theory and argued that capitalism necessarily led to monopoly capitalism and the export of capital—which he also called "imperialism"—to find new markets and resources, representing the last and highest stage of capitalism. Some 20th-century Marxian economists consider capitalism to be a social formation where capitalist class processes dominate, but are not exclusive.
Capitalist class processes, to these thinkers, are simply those in which surplus labor takes the form of surplus value, usable as capital; other tendencies for utilization of labor nonetheless exist simultaneously in existing societies where capitalist processes are predominant. However, other late Marxian thinkers argue that a social formation as a whole may be classed as capitalist if capitalism is the mode by which a surplus is "extracted", even if this surplus is not "produced" by capitalist activity, as when an absolute majority of the population is engaged in non-capitalist economic activity.
In "Limits to Capital" (1982), David Harvey outlines an overdetermined, "spatially restless" capitalism coupled with the spatiality of crisis formation and resolution. Harvey used Marx's theory of crisis to aid his argument that capitalism must have its "fixes" but that we cannot predetermine what fixes will be implemented, nor in what form they will be. His work on contractions of capital accumulation and international movements of capitalist modes of production and money flows has been influential. According to Harvey, capitalism creates the conditions for volatile and geographically uneven development 
Weberian political sociology.
In social science, the understanding of the defining characteristics of capitalism has been strongly influenced by the German sociologist, Max Weber. Weber considered market exchange, a voluntary supply of labor and a planned division of labor within the enterprises as defining features of capitalism. Capitalist enterprises, in contrast to their counterparts in prior modes of economic activity, were directed toward the rationalization of production, maximizing efficiency and productivity – a tendency embedded in a sociological process of enveloping rationalization that formed modern legal bureaucracies in both public and private spheres. According to Weber, workers in pre-capitalist economies understood work in terms of a personal relationship between master and journeyman in a guild, or between lord and peasant in a manor.
For these developments of capitalism to emerge, Weber argued, it was necessary the development of a "capitalist spirit"; that is, ideas and habits that favor a rational pursuit of economic gain. These ideas, in order to propagate a certain manner of life and come to dominate others, "had to originate somewhere ... as a way of life common to whole groups of men". In his book "The Protestant Ethic and the Spirit of Capitalism" (1904–1905), Weber sought to trace how a particular form of religious spirit, infused into traditional modes of economic activity, was a condition of possibility of modern western capitalism. For Weber, the 'spirit of capitalism' was, in general, that of ascetic Protestantism; this ideology was able to motivate extreme rationalization of daily life, a propensity to accumulate capital by a religious ethic to advance economically through hard and diligent work, and thus also the propensity to reinvest capital. This was sufficient, then, to create "self-mediating capital" as conceived by Marx.
This is pictured in the Protestant understanding of "beruf" – whose meaning encompass at the same time profession, vocation, and calling – as exemplified in Proverbs 22:29, "Seest thou a man diligent in his calling? He shall stand before kings". In the "Protestant Ethic", Weber describes the developments of this idea of calling from its religious roots, through the understanding of someone's economic success as a sign of his salvation, until the conception that moneymaking is, within the modern economic order, the result and the expression of diligence in one's calling.
Finally, as the social mores critical for its development became no longer necessary for its maintenance, modern western capitalism came to represent the order "now bound to the technical and economic conditions of machine production which today determine the lives of all the individuals who are born into this mechanism, not only those directly concerned with economic acquisition, with irresistible force. Perhaps it will so determine them until the last ton of fossilized coal is burnt" (p. 123). This is further seen in his criticism of "specialists without spirit, hedonists without a heart" that were developing, in his opinion, with the fading of the original Puritan "spirit" associated with capitalism.
Institutional economics.
Institutional economics, once the main school of economic thought in the United States, holds that capitalism cannot be separated from the political and social system within which it is embedded. It emphasizes the legal foundations of capitalism (see John R. Commons) and the evolutionary, habituated, and volitional processes by which institutions are erected and then changed.
One key figure in institutional economics was Thorstein Veblen who in his book, The Theory of the Leisure Class (1899), analyzed the motivations of wealthy people in capitalism who conspicuously consumed their riches as a way of demonstrating success. The concept of conspicuous consumption was in direct contradiction to the neoclassical view that capitalism was efficient.
In The Theory of Business Enterprise (1904) Veblen distinguished the motivations of industrial production for people to use things from business motivations that used, or misused, industrial infrastructure for profit, arguing that the former often is hindered because businesses pursue the latter. Output and technological advance are restricted by business practices and the creation of monopolies. Businesses protect their existing capital investments and employ excessive credit, leading to depressions and increasing military expenditure and war through business control of political power.
German Historical School and Austrian School.
From the perspective of the German Historical School, capitalism is primarily identified in terms of the organization of production for markets. Although this perspective shares similar theoretical roots with that of Weber, its emphasis on markets and money lends it different focus. For followers of the German Historical School, the key shift from traditional modes of economic activity to capitalism involved the shift from medieval restrictions on credit and money to the modern monetary economy combined with an emphasis on the profit motive.
In the late 19th century, the German Historical School of economics diverged, with the emerging Austrian School of economics, led at the time by Carl Menger. Later generations of followers of the Austrian School continued to be influential in Western economic thought in the early part of the 20th century.
Austrian-born economist Joseph Schumpeter, sometimes associated with the School, emphasized the "creative destruction" of capitalism—the fact that market economies undergo constant change. Schumpeter argued that at any moment in time there are rising industries and declining industries. Schumpeter, and many contemporary economists influenced by his work, argue that resources should flow from the declining to the expanding industries for an economy to grow, but they recognized that sometimes resources are slow to withdraw from the declining industries because of various forms of institutional resistance to change.
The Austrian economists Ludwig von Mises and Friedrich Hayek were among the leading defenders of market economy against 20th century proponents of socialist planned economies. Mises and Hayek argued that only market capitalism could manage a complex, modern economy.
Since a modern economy produces such a large array of distinct goods and services, and consists of such a large array of consumers and enterprises, argued Mises and Hayek, the information problems facing any other form of economic organization other than market capitalism would exceed its capacity to handle information. Thinkers within Supply-side economics built on the work of the Austrian School, and particularly emphasize Say's Law: "supply creates its own demand." Capitalism, to this school, is defined by lack of state restraint on the decisions of producers.
Keynesian economics.
In his 1937 "The General Theory of Employment, Interest and Money", the British economist John Maynard Keynes argued that capitalism suffered a basic problem in its ability to recover from periods of slowdowns in investment. Keynes argued that a capitalist economy could remain in an indefinite equilibrium despite high unemployment.
Essentially rejecting Say's law, he argued that some people may have a liquidity preference that would see them rather hold money than buy new goods or services, which therefore raised the prospect that the Great Depression would not end without what he termed in the "General Theory" "a somewhat comprehensive socialization of investment."
Keynesian economics challenged the notion that "laissez-faire" capitalist economics could operate well on their own, without state intervention used to promote aggregate demand, fighting high unemployment and deflation of the sort seen during the 1930s. He and his followers recommended "pump-priming" the economy to avoid recession: cutting taxes, increasing government borrowing, and spending during an economic down-turn. This was to be accompanied by trying to control wages nationally partly through the use of inflation to cut real wages and to deter people from holding money.
John Maynard Keynes tried to provide solutions to many of Marx's problems without completely abandoning the classical understanding of capitalism. His work attempted to show that regulation can be effective, and that economic stabilizers can rein in the aggressive expansions and recessions that Marx disliked. These changes sought to create more stability in the business cycle, and reduce the abuses of laborers. Keynesian economists argue that Keynesian policies were one of the primary reasons capitalism was able to recover following the Great Depression. The premises of Keynes's work have, however, since been challenged by neoclassical and supply-side economics and the Austrian School.
Another challenge to Keynesian thinking came from his colleague Piero Sraffa, and subsequently from the Neo-Ricardian school that followed Sraffa. In Sraffa's highly technical analysis, capitalism is defined by an entire system of social relations among both producers and consumers, but with a primary emphasis on the demands of production. According to Sraffa, the tendency of capital to seek its highest rate of profit causes a dynamic instability in social and economic relations.
Neoclassical economics and the Chicago School.
Today, the majority of academic research on capitalism in the English-speaking world draws on neoclassical economic thought. It favors extensive market coordination and relatively neutral patterns of governmental market regulation aimed at maintaining property rights; deregulated labor markets; corporate governance dominated by financial owners of firms; and financial systems depending chiefly on capital market-based financing rather than state financing.
Milton Friedman took many of the basic principles set forth by Adam Smith and the classical economists and gave them a new twist. One example of this is his article in the September 1970 issue of "The New York Times" Magazine, where he argues that the social responsibility of business is "to use its resources and engage in activities designed to increase its profits ... (through) open and free competition without deception or fraud." This is similar to Smith's argument that self-interest in turn benefits the whole of society. Work like this helped lay the foundations for the coming marketization (or privatization) of state enterprises and the supply-side economics of Ronald Reagan and Margaret Thatcher.
The Chicago School of economics is best known for its free market advocacy and monetarist ideas. According to Friedman and other monetarists, market economies are inherently stable if left to themselves and depressions result only from government intervention.
Friedman, for example, argued that the Great Depression was result of a contraction of the money supply, controlled by the Federal Reserve, and not by the lack of investment as John Maynard Keynes had argued. Ben Bernanke, former Chairman of the Federal Reserve, is among the economists today generally accepting Friedman's analysis of the causes of the Great Depression.
Neoclassical economists, who by 1998 constituted a majority of academic economists, subscribe to a subjective theory of value, according to which the value derived from consumption of a good, rather than being objective and static, varies widely from person to person and for the same person at different times. Adherence to a subjective theory of value compels Neoclassical thinkers to reject the labor theory of value upheld by Adam Smith and other classical liberal thinkers, which was grounded upon a conception of objective value.
Neoclassical models typically adopt the assumptions of Marginalism, according to which economic value results from marginal utility and marginal cost (the marginal concepts). Marginalist theory implies that capitalists earn profits not by exploiting workers, but by forgoing current consumption, taking risks, and organizing production.
Neoclassical economic theory.
Neoclassical economics explain capitalism as made up of individuals, enterprises, markets and government. According to their theories, individuals engage in a capitalist economy as consumers, laborers, and investors. As laborers, individuals may decide which jobs to prepare for, and in which markets to look for work. As investors they decide how much of their income to save and how to invest their savings. These savings, which become investments, provide much of the money that businesses need to grow.
Business firms decide what to produce and where this production should occur. They also purchase inputs (materials, labor, and capital). Businesses try to influence consumer purchase decisions through marketing and advertisement, as well as the creation of new and improved products.
Driving the capitalist economy is the search for profits (revenues minus expenses). This is known as the profit motive, and it helps ensure that companies produce the goods and services that consumers desire and are able to buy. To be profitable, firms must sell a quantity of their product at a certain price to yield a profit. A business may lose money if sales fall too low or if its costs become too high. The profit motive encourages firms to operate more efficiently. By using less materials, labor or capital, a firm can cut its production costs, which can lead to increased profits.
An economy grows when the total value of goods and services produced rises. This growth requires investment in infrastructure, capital and other resources necessary in production. In a capitalist system, businesses decide when and how much they want to invest.
Income in a capitalist economy depends primarily on what skills are in demand and what skills are being supplied. Skills that are in scarce supply are worth more in the market and can attract higher incomes. Competition among workers for jobs — and among employers for skilled workers — help determine wage rates. Firms need to pay high enough wages to attract the appropriate workers; when jobs are scarce, workers may accept lower wages than they would when jobs are plentiful. Trade union and governments influence wages in capitalist systems. Unions act to represent their members in negotiations with employers over such things as wage rates and acceptable working conditions.
The market.
Supply is the amount of a good or service produced by a firm and which is available for sale. Demand is the amount that people are willing to buy at a specific price. Prices tend to rise when demand exceeds supply, and fall when supply exceeds demand. In theory, the market is able to coordinate itself when a new equilibrium price and quantity is reached.
Competition arises when more than one producer is trying to sell the same or similar products to the same buyers. In capitalist theory, competition leads to innovation and more affordable prices. Without competition, a monopoly or cartel may develop. A monopoly occurs when a firm supplies the total output in the market; the firm can therefore limit output and raise prices because it has no fear of competition. A cartel is a group of firms that act together in a monopolistic manner to control output and raise prices.
Role of government.
In a capitalist system, the government does not prohibit private property or prevent individuals from working where they please. The government does not prevent firms from determining what wages they will pay and what prices they will charge for their products. Many countries, however, have minimum wage laws and minimum safety standards.
Under some versions of capitalism, the government carries out a number of economic functions, such as issuing money, supervising public utilities and enforcing private contracts. Many countries have competition laws that prohibit monopolies and cartels from forming. Despite anti-monopoly laws, large corporations can form near-monopolies in some industries. Such firms can temporarily drop prices and accept losses to prevent competition from entering the market, and then raise them again once the threat of entry is reduced. In many countries, public utilities (e.g. electricity, heating fuel, communications) are able to operate as a monopoly under government regulation, due to high economies of scale.
Government agencies regulate the standards of service in many industries, such as airlines and broadcasting, as well as financing a wide range of programs. In addition, the government regulates the flow of capital and uses financial tools such as the interest rate to control factors such as inflation and unemployment.
Democracy, the state, and legal frameworks.
Private property.
The relationship between the state, its formal mechanisms, and capitalist societies has been debated in many fields of social and political theory, with active discussion since the 19th century. Hernando de Soto is a contemporary economist who has argued that an important characteristic of capitalism is the functioning state protection of property rights in a formal property system where ownership and transactions are clearly recorded.
According to de Soto, this is the process by which physical assets are transformed into capital, which in turn may be used in many more ways and much more efficiently in the market economy. A number of Marxian economists have argued that the Enclosure Acts in England, and similar legislation elsewhere, were an integral part of capitalist primitive accumulation and that specific legal frameworks of private land ownership have been integral to the development of capitalism.
Institutions.
New institutional economics, a field pioneered by Douglass North, stresses the need of a legal framework in order for capitalism to function optimally, and focuses on the relationship between the historical development of capitalism and the creation and maintenance of political and economic institutions. In new institutional economics and other fields focusing on public policy, economists seek to judge when and whether governmental intervention (such as taxes, welfare, and government regulation) can result in potential gains in efficiency. According to Gregory Mankiw, a New Keynesian economist, governmental intervention can improve on market outcomes under conditions of "market failure", or situations in which the market on its own does not allocate resources efficiently.
Market failure occurs when an externality is present and a market will either under-produce a product with a positive externalization or overproduce a product that generates a negative externalization. Air pollution, for instance, is a negative externalization that cannot be incorporated into markets as the world's air is not owned and then sold for use to polluters. So, too much pollution could be emitted and people not involved in the production pay the cost of the pollution instead of the firm that initially emitted the air pollution. Critics of market failure theory, like Ronald Coase, Harold Demsetz, and James M. Buchanan argue that government programs and policies also fall short of absolute perfection. Market failures are often small, and government failures are sometimes large. It is therefore the case that imperfect markets are often better than imperfect governmental alternatives. While all nations currently have some kind of market regulations, the desirable degree of regulation is disputed.
Democracy.
The relationship between democracy and capitalism is a contentious area in theory and popular political movements. The extension of universal adult male suffrage in 19th century Britain occurred along with the development of industrial capitalism, and democracy became widespread at the same time as capitalism, leading many theorists to posit a causal relationship between them, or that each affects the other. However, in the 20th century, according to some authors, capitalism also accompanied a variety of political formations quite distinct from liberal democracies, including fascist regimes, absolute monarchies, and single-party states.
While some thinkers argue that capitalist development more-or-less inevitably eventually leads to the emergence of democracy, others dispute this claim. Research on the democratic peace theory indicates that capitalist democracies rarely make war with one another and have little internal violence. However, critics of the democratic peace theory note that democratic capitalist states may fight infrequently and or never with other democratic capitalist states because of political similarity or stability rather than because they are democratic or capitalist.
Some commentators argue that though economic growth under capitalism has led to democratization in the past, it may not do so in the future, as authoritarian regimes have been able to manage economic growth without making concessions to greater political freedom.
States that have highly capitalistic economic systems have thrived under authoritarian or oppressive political systems. Singapore, which maintains a highly open market economy and attracts lots of foreign investment, does not protect civil liberties such as freedom of speech and expression. The private (capitalist) sector in the People's Republic of China has grown exponentially and thrived since its inception, despite having an authoritarian government. Augusto Pinochet's rule in Chile led to economic growth and high levels of inequality by using authoritarian means to create a safe environment for investment and capitalism. Thomas Piketty of the Paris School of Economics asserts that rising economic inequality is a natural consequence of capitalist activity, and is destabilizing to democratic societies and undermines the ideals of social justice upon which they are built.
In response to criticism of the system, some proponents of capitalism have argued that its advantages are supported by empirical research. Indices of Economic Freedom show a correlation between nations with more economic freedom (as defined by the indices) and higher scores on variables such as income and life expectancy, including the poor, in these nations.
Advocacy for capitalism.
Economic growth.
Many theorists and policymakers in predominantly capitalist nations have emphasized capitalism's ability to promote economic growth, as measured by Gross Domestic Product (GDP), capacity utilization or standard of living. This argument was central, for example, to Adam Smith's advocacy of letting a free market control production and price, and allocate resources. Many theorists have noted that this increase in global GDP over time coincides with the emergence of the modern world capitalist system.
Between 1000 and 1820, the world economy grew sixfold, a faster rate than the population growth, so each individual enjoyed, on the average, a 50% increase in wealth. Between 1820 and 1998, world economy grew 50-fold, a much faster rate than the population growth, so each individual enjoyed, on the average, a 9-fold increase in wealth. In most capitalist economic regions such as Europe, the United States, Canada, Australia and New Zealand, the economy grew 19-fold per person, even though these countries already had a higher starting level, and in Japan, which was poor in 1820, the increase per person was 31-fold. In the third world there was an increase, but only 5-fold per person.
Proponents argue that increasing GDP (per capita) is empirically shown to bring about improved standards of living, such as better availability of food, housing, clothing, and health care. The decrease in the number of hours worked per week and the decreased participation of children and the elderly in the workforce have been attributed to capitalism.
Proponents also believe that a capitalist economy offers far more opportunities for individuals to raise their income through new professions or business ventures than do other economic forms. To their thinking, this potential is much greater than in either traditional feudal or tribal societies or in socialist societies.
Political freedom.
In his book "The Road to Serfdom", Freidrich Hayek asserts that the economic freedom of capitalism is a requisite of political freedom. He argues that the market mechanism is the only way of deciding what to produce and how to distribute the items without using coercion. Milton Friedman, Andrew Brennan and Ronald Reagan also promoted this view. Friedman claimed that centralized economic operations are always accompanied by political repression. In his view, transactions in a market economy are voluntary, and that the wide diversity that voluntary activity permits is a fundamental threat to repressive political leaders and greatly diminish their power to coerce. Some of Friedman's views were shared by John Maynard Keynes, who believed that capitalism is vital for freedom to survive and thrive.
The novelist and philosopher Ayn Rand made positive moral defences of "laissez-faire" capitalism, most notably in her 1957 novel "Atlas Shrugged", and in her 1966 collection of essays "". She argued that capitalism should be supported on moral grounds, not just on the basis of practical benefits. She has significantly influenced conservative and libertarian supporters of capitalism, especially in the American Tea Party movement.
Self-organization.
Austrian School economists have argued that capitalism can organize itself into a complex system without an external guidance or central planning mechanism. Friedrich Hayek considered the phenomenon of self-organization as underpinning capitalism. Prices serve as a signal as to the urgent and unfilled wants of people, and the opportunity to earn profits if successful, or absorb losses if resources are used poorly or left idle, gives entrepreneurs incentive to use their knowledge and resources to satisfy those wants. Thus the activities of millions of people, each seeking his own interest, are coordinated.
Criticism.
Critics of capitalism associate the economic system with social inequality; unfair distribution of wealth and power; a tendency toward market monopoly or oligopoly (and government by oligarchy); imperialism; counter-revolutionary wars; various forms of economic and cultural exploitation; materialism; repression of workers and trade unionists; social alienation; economic inequality; unemployment; and economic instability. Notable critics of capitalism have included: socialists, anarchists, communists, national socialists, social democrats, environmentalists, technocrats, some types of conservatives, Luddites, Narodniks, Shakers, and some types of nationalists.
Many socialists consider capitalism to be irrational, in that production and the direction of the economy are unplanned, creating many inconsistencies and internal contradictions. Capitalism and individual property rights have been associated with the tragedy of the anticommons. Marxian economist Richard D. Wolff postulates that capitalist economies prioritize profits and capital accumulation over the social needs of communities, and capitalist enterprises rarely include the workers in the basic decisions of the enterprise. Following the banking crisis of 2007, Alan Greenspan told the United States Congress on October 23, 2008, "The whole intellectual edifice collapsed. I made a mistake in presuming that the self-interests of organizations, specifically banks and others, were such that they were best capable of protecting their own shareholders. ... I was shocked."
Some labor historians and scholars have argued that unfree labor — by slaves, indentured servants, prisoners or other coerced persons — is compatible with capitalist relations. Tom Brass argued that unfree labor is acceptable to capital. Historian Greg Grandin argues that capitalism has its origins in slavery: "when historians talk about the Atlantic market revolution, they are talking about capitalism. And when they are talking about capitalism, they are talking about slavery."
According to Immanuel Wallerstein, institutional racism has been "one of the most significant pillars" of the capitalist system and serves as "the ideological justification for the hierarchization of the work-force and its highly unequal distributions of reward."
Many aspects of capitalism have come under attack from the anti-globalization movement, which is primarily opposed to corporate capitalism. Environmentalists have argued that capitalism requires continual economic growth, and that it will inevitably deplete the finite natural resources of the Earth. Such critics argue that while this neoliberalism or contemporary capitalism has indeed increased global trade, it has also destroyed traditional ways of life, exacerbated inequality and increased global poverty - with more living today in abject poverty than before neoliberalism, and that environmental indicators indicate massive environmental degradation since the late 1970s.
Many religions have criticized or opposed specific elements of capitalism. Traditional Judaism, Christianity, and Islam forbid lending money at interest, although alternative methods of banking have been developed. Some Christians have criticized capitalism for its materialist aspects and its inability to account for the wellbeing of all people. Many of Jesus' parables deal with economic concerns: farming, shepherding, being in debt, doing hard labor, being excluded from banquets and the houses of the rich, and have implications for wealth and power distribution. In his 84-page apostolic exhortation "Evangelii Gaudium," Pope Francis described unfettered capitalism as "a new tyranny" and called upon world leaders to fight rising poverty and inequality:

</doc>
<doc id="5420" url="http://en.wikipedia.org/wiki?curid=5420" title="Cross ownership">
Cross ownership

Cross ownership is a method of reinforcing business relationships by owning stock in the companies with which a given company does business. Heavy cross ownership is referred to as circular ownership.
In the US, "cross ownership" also refers to a type of investment in different mass-media properties in one market.
Cross ownership of stock.
Some countries where cross ownership of shares is a major part of the business culture are:
Positives of cross ownership:
Cross ownership of shares is criticized for:
A major factor in perpetuating cross ownership of shares is a high capital gains tax rate. A company has less incentive to sell cross owned shares if taxes are high because of the immediate reduction in the value of the assets.
For example, a company owns $1000 of stock in another company that was originally purchased for $200. If the capital gains tax rate is 50% (like Germany) and the company sells the stock,
the company has $600 which is 40 percent less than before it sold the stock.
Long term cross ownership of shares combined with a high capital tax rate greatly increases periods of asset deflation both in time and in severity.
Media cross ownership.
Cross ownership also refers to a type of media ownership in which one type of communications (say a newspaper) owns or is the sister company of another type of medium (such as a radio or TV station). One example is "The New York Times" 's former ownership of WQXR Radio and the "Chicago Tribune"'s similar relationship with WGN Radio (WGN-AM) and Television (WGN-TV).
The Federal Communications Commission generally does not allow cross ownership, to keep from one license holder having too much local media ownership, unless the license holder obtains a waiver, such as News Corporation and the Tribune Company have in New York.
The mid-1970s cross-ownership guidelines grandfathered already-existing crossownerships, such as "Tribune"-WGN, "New York Times"-WQXR and the "New York Daily News" ownership of WPIX Television and Radio.

</doc>
<doc id="5421" url="http://en.wikipedia.org/wiki?curid=5421" title="Cardiology">
Cardiology

Cardiology (from Greek "kardiā", "heart" and "-logia", "study") is a medical specialty dealing with disorders of the heart be it human or animal. The field includes medical diagnosis and treatment of congenital heart defects, coronary artery disease, heart failure, valvular heart disease and electrophysiology. Physicians who specialize in this field of medicine are called cardiologists. Physicians who specialize in cardiac surgery are called cardiac surgeons.
Specialization.
Cardiology is a specialty of internal medicine. To be a cardiologist in the United States, a three year residency in internal medicine is followed by a three year fellowship in cardiology. It is possible to specialize further in a sub-specialty. Recognized sub-specialties in the United States by the ACGME are:
Recognized subspecialties in the United States by the American Osteopathic Association Bureau of Osteopathic Specialists (AOABOS) include:
The heart.
As the center focus of cardiology, the heart has numerous anatomical features (e.g., atria, ventricles, heart valves) and numerous physiological features (e.g., systole, heart sounds, afterload) that have been encyclopedically documented for many centuries.
Disorders of the heart lead to heart disease and cardiovascular disease and can lead to a significant number of deaths: cardiovascular disease is the leading cause of death and caused 29.34% of all deaths in 2002.
The primary responsibility of the heart is to pump blood throughout the body.
It pumps blood from the body — called the systemic circulation — through the lungs — called the pulmonary circulation — and then back out to the body.
This means that the heart is connected to and affects the entirety of the body. Simplified, the heart is a circuit of the Circulation.
While plenty is known about the healthy heart, the bulk of study in cardiology is in disorders of the heart and restoration, and where possible, of function.
The heart is a muscle that squeezes blood and functions like a pump.
Each part of the heart is susceptible to failure or dysfunction and the heart can be divided into the mechanical and the electrical parts.
The electrical part of the heart is centered on the periodic contraction (squeezing) of the muscle cells that is caused by the cardiac pacemaker located in the sinoatrial node.
The study of the electrical aspects is a sub-field of electrophysiology called cardiac electrophysiology and is epitomized with the electrocardiogram (ECG/EKG).
The action potentials generated in the pacemaker propagate throughout the heart in a specific pattern. The system that carries this potential is called the electrical conduction system.
Dysfunction of the electrical system manifests in many ways and may include Wolff–Parkinson–White syndrome, ventricular fibrillation, and heart block.
The mechanical part of the heart is centered on the fluidic movement of blood and the functionality of the heart as a pump.
The mechanical part is ultimately the purpose of the heart and many of the disorders of the heart disrupt the ability to move blood.
Failure to move sufficient blood can result in failure in other organs and may result in death if severe.
Heart failure is one condition in which the mechanical properties of the heart have failed or are failing, which means insufficient blood is being circulated.
Disorders.
Cardiology is concerned with the normal functionality of the heart and the deviation from a healthy heart.
Many disorders involve the heart itself but some are outside of the heart.
Disorders of the coronary circulation.
Contrary to a basic understanding of the cardiovascular system, the heart cannot itself receive enough oxygen and nutrients from the blood it pumps and it must be supplied with blood as if it were any other organ in the body. Unlike the systemic organs the heart receives perfusion in the phase of diastole rather than systole. This circulation of blood is called the coronary circulation.
The coronary circulation consists of coronary arteries and coronary veins.
Disorders of the coronary circulation can have devastating effects to the heart since damage to the heart can reduce coronary circulation which causes further damage. A few examples are presented, as follows:
Cardiac arrest.
Cardiac arrest refers to the cessation(to cease)of normal systemic circulation due to failure in proper contraction of the heart.
There are several conditions that can cause cardiac arrest.
Treatment of cardiac arrest includes cardiopulmonary resuscitation (CPR) and defibrillation depending on the exact cause of cardiac arrest.
Disorders of the pericardium (outer lining of the heart).
The pericardium is a double-walled sac — fibrous pericardium and serous pericardium — that contains the heart.
Disorders of the heart valves.
The heart contains four valves that direct the flow of blood in a single direction.
Failure to prevent reverse-flow is called regurgitation, or insufficiency.
Narrowing of the valves obstructs flow and is called stenosis.
Congenital heart defect.
Congenital heart defects are defects in the structure of the heart which are present at birth.
Diagnostic tests and procedures.
Various cardiology diagnostic tests and procedures.

</doc>
<doc id="5422" url="http://en.wikipedia.org/wiki?curid=5422" title="Capcom">
Capcom

, or Capcom, is a Japanese developer and publisher of video games, known for creating multi-million-selling franchises such as "Mega Man", "Monster Hunter", "Resident Evil", "Devil May Cry" and "Street Fighter". Originally established in 1983, it has since become an international enterprise with branches and subsidiaries in North America, Europe, and East Asia.
History.
The original companies that spawned Capcom's Japanese branch were I.R.M Corporation founded on May 30, 1979, as well as its subsidiary Japan Capsule Computers Co., Ltd., both of which were devoted to the manufacturing and distribution of electronic game machines. The two companies underwent a name change to Sambi Co., Ltd. in September 1981, while Capcom Co., Ltd. itself was first established on June 11, 1983, for the purpose of taking over the internal sales department.
In January 1989, the old affiliate company Capcom Co., Ltd. merged with Sambi Co., Ltd., resulting in the current Japanese branch. The name Capcom is a portmanteau of "Capsule Computers", a term coined by the company to describe the arcade machines it solely manufactured in its early years, designed to set themselves apart from personal computers that were becoming widespread at that time. The word capsule alludes to how Capcom likened its game software to "a capsule packed to the brim with gaming fun", as well as to the company's desire to protect its intellectual property with a hard outer shell, preventing illegal copies and inferior imitations.
While Capcom's first product was the coin-operated "Little League" from July 1983, its first real video game, the arcade title "Vulgus", was released in May 1984. Beginning with a Nintendo Entertainment System port of "1942" published in December 1985, the company started to venture into the market of home console video games, which became its main business segment a few years later. Since then, Capcom created 15 multi-million-selling game series, the most successful of which is "Resident Evil".
In 1994, Capcom adapted its "Street Fighter" series of fighting games into a film of the same name. While commercially successful, the production received almost universal criticism. A 2002 adaptation of its "Resident Evil" series faced similar criticism but was also successful in theaters. The company sees films as a way to build sales for its video games.
Corporate structure.
Development studios.
In the first few years after its establishment, the Japanese branch of Capcom had three development groups referred to as "Planning Rooms", led by Tokuro Fujiwara, Takashi Nishiyama and Yoshiki Okamoto, respectively. Later, games developed internally used to be created by several numbered "Production Studios", each assigned to different games. Starting in 2002, the development process was reformed to better share technologies and expertise, and all of the individual studios were gradually restructured into bigger departments responsible for different tasks. While there are self-contained departments for the creation of arcade, pachinko and pachislo, online, and mobile games, the Consumer Games R&D Division instead is an amalgamation of subsections in charge of various game development stages.
In addition to these internal teams, Capcom also commissions outside development studios to ensure a steady output of titles. However, following poor sales of "Dark Void" and "Bionic Commando", the company's management has decided to limit outsourcing to sequels and newer versions of installments in existing franchises, reserving the development of original titles for its in-house teams. The production of games, budgets, and platforms supported are decided upon in development approval meetings, attended by the company management and the marketing, sales, and quality control departments.
Branches and subsidiaries.
Apart from the head office building and the R&D building of Capcom Co., Ltd., both located in Chūō-ku, Osaka, the Japanese parent company also has a branch office in the Shinjuku Mitsui Building in Nishi-Shinjuku, Shinjuku, Tokyo. It also has the Ueno Facility, a branch office in Iga, Mie Prefecture.
The international Capcom Group currently encompasses 15 subsidiaries in Japan, North America, Europe, and East Asia. Affiliated companies include Koko Capcom Co., Ltd. in South Korea, Street Fighter Film, LLC in the United States, and Dellgamadas Co., Ltd.
Game-related media.
In addition to the development and publishing of home, online, mobile, arcade, pachinko, and pachislo games, the company publishes strategy guides, maintains its own arcade centers, and licenses its franchise and character properties for use in tie-in products, movies, television series, and stage performances.
Suleputer, an in-house marketing and music label established in cooperation with Sony Music Entertainment Intermedia in 1998, publishes CDs, DVDs, and other media based on Capcom's games.
An annual private media summit called Captivate, renamed from Gamers Day in 2008, is traditionally used as a platform for new game and business announcements.
Games.
Capcom launched its "Street Fighter" franchise in 1987. The series of fighting games are among the most popular in their genre, and have sold over 30 million units. That same year, 1987, the company introduced its "Mega Man" series. Selling nearly 30 million units, the series serves as Capcom's flagship franchise.
The company developed the inaugural entry in its "Resident Evil" survival horror series in 1996. The series has achieved enormous success, selling nearly 50 million units. Following work on the second entry in the "Resident Evil" series, Capcom began work on a "Resident Evil" game for the new PlayStation 2. Radically different from the existing series, Capcom decided to spin off the game into its own series, "Devil May Cry". While it released the first two entries exclusively for the PS2 the company brought further entries to non-Sony consoles. The series as a whole has seen sales in excess of 10 million units. Capcom also initiated its "Monster Hunter" series in 2004. The games have seen sales of over 20 million units on a variety of consoles.
While often criticized for its reliance on existing franchises, the company developed several titles for the Xbox 360 and PlayStation 3 based on original intellectual property: "" and "Dead Rising". Each title has since spawned at least two sequels. Also of note are the titles "Ōkami" and "Ōkamiden".
Controversies.
In recent years, Capcom has been criticized for controversial sales tactics, such as having to pay for additional content which is already available within the game's files, most notably in "Street Fighter X Tekken". Capcom has defended the practice. The company has also been criticized for other questionable business decisions, such as not releasing certain games outside of Japan, abruptly cancelling anticipated projects, and shutting down Clover Studio.

</doc>
<doc id="5428" url="http://en.wikipedia.org/wiki?curid=5428" title="History of Cambodia">
History of Cambodia

People have been living within the area covered by the present-day country of Cambodia at least since the 5th millennium BC. The ancient Kingdom of Funan occupied a wider area, and it was during that period that the culture became heavily influenced by Hinduism. The state of Chenla then arose. The Khmer Empire had its golden age in the 9th to the 13th centuries, when huge temple complexes were built, most notably Angkor Wat. 
Spanish and Portuguese missionaries visited from the 16th century, and Cambodia became a protectorate of France in the 19th century, being ruled as part of French Indochina. Cambodia became an independent kingdom in 1953 under Norodom Sihanouk. The Vietnam War extended into Cambodia, giving rise to the Khmer Rouge, which took Phnom Penh in 1975 and carried out a campaign of mass killing. Following an invasion by Vietnam, the Khmer Rouge were deposed and the People's Republic of Kampuchea was established. After years of isolation, the war-ravaged nation was reunited under the monarchy in 1993 and has seen rapid economic progress while rebuilding from decades of civil war.
Prehistory and early history.
Carbon 14 dating of a cave at Laang Spean in northwest Cambodia revealed stone tools from 6000-7000 BC, and pottery from 4200 BC. Further archaeological evidence indicates that other parts of the region now called Cambodia were inhabited from around 2000-1000 BC by a Neolithic culture. Skulls and human bones found at Samrong Sen date from 1500 BC. These people may have migrated from South Eastern China to the Indochinese Peninsula. Scholars trace the first cultivation of rice and the first bronze making in Southeast Asia to these people. By the 1st century AD, the inhabitants had developed relatively stable, organized societies and spoke languages very much related to the Cambodian or Khmer of the present day. The culture and technical skills of these people in the 1st century AD far surpassed the primitive stage. The most advanced groups lived along the coast and in the lower Mekong River valley and delta regions in houses constructed on stilts where they cultivated rice, fished and kept domesticated animals. Recent research has unlocked the discovery of artificial circular earthworks dating to Cambodia's Neolithic era.
The Khmer people were among the first inhabitants of South East Asia. They were also among the first in South East Asia to adopt religious ideas and political institutions from India and to establish centralized kingdoms comprising large territories. The earliest known kingdom in the area, Funan, flourished from around the 1st to the 6th century. This was succeeded by Chenla, which controlled large parts of modern Cambodia, Vietnam, Laos, and Thailand.
Funan Kingdom (1st century AD – 550).
The Funanese Empire rose to eminence from its affluent and powerful home city of Óc Eo (in present-day Vietnam), known in the Roman Empire as Kattigara, meaning the Renowned City. Contacts with the distant Roman Empire are evidenced by the fact that Roman coins have been found at archeological sites dating from the 2nd and 3rd centuries. However, most of the foreign trade of the Funan Empire was carried on much closer to home with India, especially the Bengal area of India. Trade with India commenced well before 500 BC (before the widespread use of Sanskrit as a language in India). Trade with China began after the southward expansion of the Han Dynasty around the 2nd century BC. Remnants of Chinese influence can be seen in axes excavated in Cambodia, which display Han-style elements. With the Indian trade came the Indianization of the culture of Funan and the religion of Hinduism. Hinduism produced a syncretism phenomenon with other previous religions and beliefs already present in the Khmer culture. Funan and its succeeding societies which occupied this section of Southeast Asia would remain mainly Hindu in religion for about 900 years. Some cultural features and customs of Hinduism continue to exist within the current society. 
The empire reached its greatest extent under the rule of Fan Shih-man in the early 3rd century, extending as far south as Malaysia and as far west as Burma. The Funanese established a strong system of mercantilism and commercial monopolies that would become a pattern for empires in the region. Exports from the Funan Empire were largely forest products and precious metals—including accessories such as gold elephants, ivory, rhinoceros horn, kingfisher feathers, wild spices like cardamom, lacquer hides and aromatic wood. Fan Shih-man expanded the fleet and improved the Funanese bureaucracy, creating a quasi-feudal pattern that left local customs and identities largely intact, particularly in the empire's farther reaches.
Chenla Kingdom (6th century – 802).
The Khmers, vassals of Funan, reached the Mekong river from the northern Menam River via the Mun River Valley. Chenla, their first independent state developed out of Funanese influence.
Ancient Chinese records mention two kings, Shrutavarman and Shreshthavarman who ruled at the capital Shreshthapura located in modern day southern Laos. The immense influence on the identity of Cambodia to come was wrought by the Khmer Kingdom of Bhavapura, in the modern day Cambodian city of Kampong Thom. Its legacy was its most important sovereign, Ishanavarman who completely conquered the kingdom of Funan during 612-628. He chose his new capital at the Sambor Prei Kuk, naming it Ishanapura.
After the death of Jayavarman I in 681, turmoil came upon the kingdom and at the start of the 8th century, the kingdom broke up into many principalities. Pushkaraksha, the ruler of Shambhupura announced himself as king of the entire Kambuja. Chinese chronicles proclaim that, in the 8th century, Chenla was split into land Chenla and water Chenla. During this time, Shambhuvarman son of Pushkaraksha controlled most of water Chenla until the 8th century which the Malayans and Javanese dominated over many Khmer principalities.
Khmer Empire (802–1431).
The golden age of Khmer civilization, however, was the period from the 9th to the 13th centuries, when Khmer Empire, which gave Kampuchea, or Cambodia, its name, ruled large territories from its capital in the region of Angkor in western Cambodia.
Legend has it that in 802 AD, Jayavarman II, king of the Khmers, first came to the Kuhlen hills, the future site of Angkor Wat. Later, under Jayavarman VII (1181 – c. 1218), Khmer reached its zenith of political power and cultural creativity. Jayavarman VII gained power and territory in a series of successful wars. Khmer conquests were almost unstoppable as they raided home cities of powerful seafaring Chams. However, territorial expansion stopped after a defeat by Dai Viet. The battle also witnessed Suryavarman II's death. Following Jayavarman VII's death, Khmer experienced a gradual decline. Important factors were the aggressiveness of neighboring peoples (especially the Thai, or Siamese), chronic interdynastic strife, and the gradual deterioration of the complex irrigation system that had ensured rice surpluses. The Angkorian monarchy survived until 1431, when the Thai captured Angkor Thom and the Cambodian king fled to the southern part of the country
Dark ages of Cambodia (1431–1863).
The 15th to the 19th centuries were a period of continued decline and territorial loss. Cambodia enjoyed a brief period of prosperity during the 16th century because its kings, who built their capitals in the region southeast of the Tonlé Sap along the Mekong river, promoted trade with other parts of Asia. This was the period when Spanish and Portuguese adventurers and missionaries first visited the country. It is known that the Portuguese visited Cambodia as early as 1555. However, the Thai conquest of the new capital at Lovek in 1594 marked a downturn in the country's fortunes and Cambodia. Becoming a pawn in power struggles between its two increasingly powerful neighbors, Siam and Vietnam. Cambodia remained a protectorate of Siam. Vietnam's settlement of the Mekong Delta led to its annexation of that area at the end of the 17th century. Vietnam employed a strategy similar to those of North American pilgrims and pioneers: settle and claim. Such foreign encroachments continued through the first half of the 19th century. A successful invasion by Vietnam further limited Thai protectorship in Cambodia and established the kingdom under full Vietnamese suzerainty.
French colonial period (1863–1953).
In 1863, King Norodom signed an agreement with the French to establish a protectorate over his kingdom. The state gradually came under French colonial rule.
During World War II, the 1940–41 Franco-Thai War left the French Indochinese colonial authorities in a position of weakness. The Vichy government signed an agreement with Japan to allow the Japanese military transit through French Indochina.
Meanwhile the Thai government, under the pro-Japanese leadership of Field Marshal Plaek Phibunsongkhram, took advantage of its position and invaded Cambodia's western provinces.
Cambodia's situation at the end of the war was chaotic. The Free French, under General Charles de Gaulle, were determined to recover Indochina, though they offered Cambodia and the other Inchochinese protectorates a carefully circumscribed measure of self-government. Convinced that they had a "civilizing mission", they envisioned Indochina's participation in a French Union of former colonies that shared the common experience of French culture.
Administration of Sihanouk (1953–70).
On 9 March 1945, during the Japanese occupation of Cambodia, young king Norodom Sihanouk proclaimed an independent Kingdom of Kampuchea, following a formal request by the Japanese. Shortly thereafter the Japanese government nominally ratified the independence of Cambodia and established a consulate in Phnom Penh. The new government did away with the romanization of the Khmer language that the French colonial administration was beginning to enforce and officially reinstated the Khmer script. This measure taken by the short-lived governmental authority would be popular and long-lasting, for since then no government in Cambodia has tried to romanize the Khmer language again.
After Allied military units entered Cambodia, the Japanese military forces present in the country were disarmed and repatriated. The French were able to reimpose the colonial administration in Phnom Penh in October the same year.
Sihanouk's "royal crusade for independence" resulted in grudging French acquiescence to his demands for a transfer of sovereignty. A partial agreement was struck in October 1953. Sihanouk then declared that independence had been achieved and returned in triumph to Phnom Penh. As a result of the Geneva Conference on Indochina, Cambodia was able to bring about the withdrawal of the Viet Minh troops from its territory and to withstand any residual impingement upon its sovereignty by external powers.
Neutrality was the central element of Cambodian foreign policy during the 1950s and 1960s. By the mid-1960s, parts of Cambodia's eastern provinces were serving as bases for North Vietnamese Army and National Liberation Front (NVA/NLF) forces operating against South Vietnam, and the port of Sihanoukville was being used to supply them. As NVA/VC activity grew, the United States and South Vietnam became concerned, and in 1969, the United States began a 14 month long series of bombing raids targeted at NVA/VC elements, contributing to destabilization. The bombing campaign took place no further than ten, and later twenty miles (32 km) inside the Cambodian border, areas where the Cambodian population had been evicted by the NVA. Prince Sihanouk, fearing that the conflict between communist North Vietnam and South Vietnam might spill over to Cambodia, publicly opposed the idea of a bombing campaign by the United States along the Vietnam-Cambodia border and inside Cambodian territory. However Peter Rodman claimed, "Prince Sihanouk complained bitterly to us about these North Vietnamese bases in his country and invited us to attack them". In December 1967 Washington Post journalist Stanley Karnow was told by Sihanouk that if the US wanted to bomb the Vietnamese communist sanctuaries, he would not object, unless Cambodians were killed. The same message was conveyed to US President Johnson's emissary Chester Bowles in January 1968. So the US had no real motivation to overthrow Sihanouk. However Prince Sihanouk wanted Cambodia to stay out of the North Vietnam-South Vietnam conflict and was very critical of the United States government and its allies (the South Vietnamese government). Prince Sihanouk, facing internal struggles of his own, due to the rise of the Khmer Rouge, did not want Cambodia to be involved in the conflict. Sihanouk wanted the United States and its allies (South Vietnam) to keep the war away from the Cambodian border. Sihanouk did not allow the United States to use Cambodian air space and airports for military purposes. This upset the United States greatly and contributed to their view that of Prince Sihanouk as a North Vietnamese sympathizer and a thorn on the United States. However, declassified documents indicate that, as late as March 1970, the Nixon administration was hoping to garner "friendly relations" with Sihanouk.
Throughout the 1960s, domestic Cambodian politics became polarized. Opposition to the government grew within the middle class and leftists including Paris-educated leaders like Son Sen, Ieng Sary, and Saloth Sar (later known as Pol Pot), who led an insurgency under the clandestine Communist Party of Kampuchea (CPK). Sihanouk called these insurgents the Khmer Rouge, literally the "Red Khmer". But the 1966 national assembly elections showed a significant swing to the right, and General Lon Nol formed a new government, which lasted until 1967. During 1968 and 1969, the insurgency worsened. However members of the government and army, who resented Sihanouk's ruling style as well as his tilt away from the United States, did have a motivation to overthrow him.
Khmer Republic and the War (1970–75).
While visiting Beijing in 1970 Sihanouk was ousted by a military coup led by Prime Minister General Lon Nol and Prince Sisowath Sirik Matak in the early hours of March 18, 1970.
Despite Sihanouk's allegations, there is no evidence that this coup was planned by the United States Central Intelligence Agency. However, as early as March 12, 1970, the CIA Station Chief told Washington that based on communications from Sirik Matak, Lon Nol's cousin, that "the (Cambodian) army was ready for a coup". Lon Nol assumed power after the military coup and immediately allied Cambodia with the United States. Son Ngoc Thanh, an opponent of Pol Pot, announced his support for the new government. On October 9, the Cambodian monarchy was abolished, and the country was renamed the Khmer Republic. The new regime immediately demanded that the Vietnamese communists leave Cambodia.
Hanoi rejected the new republic's request for the withdrawal of NVA troops. In response, the United States moved to provide material assistance to the new government's armed forces, which were engaged against both CPK insurgents and NVA forces. The North Vietnamese and Viet Cong forces, desperate to retain their sanctuaries and supply lines from North Vietnam, immediately launched armed attacks on the new government. The North Vietnamese quickly overran large parts of eastern Cambodia reaching to within of Phnom Penh. The North Vietnamese turned the newly won territories over to the Khmer Rouge. The king urged his followers to help in overthrowing this government, hastening the onset of civil war.
In April 1970, U.S. President Nixon announced to the American public that US and South Vietnamese ground forces had entered Cambodia in a campaign aimed at destroying NVA base areas in Cambodia (see Cambodian Incursion). The US had already been bombing Vietnamese positions in Cambodia for well over a year by that point. Although a considerable quantity of equipment was seized or destroyed by US and South Vietnamese forces, containment of North Vietnamese forces proved elusive. 
The Khmer Republic's leadership was plagued by disunity among its three principal figures: Lon Nol, Sihanouk's cousin Sirik Matak, and National Assembly leader In Tam. Lon Nol remained in power in part because none of the others were prepared to take his place. In 1972, a constitution was adopted, a parliament elected, and Lon Nol became president. But disunity, the problems of transforming a 30,000-man army into a national combat force of more than 200,000 men, and spreading corruption weakened the civilian administration and army.
The Khmer Rouge insurgency inside Cambodia continued to grow, aided by supplies and military support from North Vietnam. Pol Pot and Ieng Sary asserted their dominance over the Vietnamese-trained communists, many of whom were purged. At the same time, the Khmer Rouge (CPK) forces became stronger and more independent of their Vietnamese patrons. By 1973, the CPK were fighting battles against government forces with little or no North Vietnamese troop support, and they controlled nearly 60% of Cambodia's territory and 25% of its population.
The government made three unsuccessful attempts to enter into negotiations with the insurgents, but by 1974, the CPK were operating openly as divisions, and some of the NVA combat forces had moved into South Vietnam. Lon Nol's control was reduced to small enclaves around the cities and main transportation routes. More than 2 million refugees from the war lived in Phnom Penh and other cities.
On New Year's Day 1975, Communist troops launched an offensive which, in 117 days of the hardest fighting of the war, collapsed the Khmer Republic. Simultaneous attacks around the perimeter of Phnom Penh pinned down Republican forces, while other CPK units overran fire bases controlling the vital lower Mekong resupply route. A US-funded airlift of ammunition and rice ended when Congress refused additional aid for Cambodia. The Lon Nol government in Phnom Penh surrendered on April 17, 1975, just five days after the US mission evacuated Cambodia.
The relationship between the massive carpet bombing of Cambodia by the United States and the growth of the Khmer Rouge, in terms of recruitment and popular support, has been a matter of interest to historians. Some historians have cited the U.S. intervention and bombing campaign (spanning 1965–1973) as a significant factor leading to increased support of the Khmer Rouge among the Cambodian peasantry. However, Pol Pot biographer David Chandler argues that the bombing "had the effect the Americans wanted – it broke the Communist encirclement of Phnom Penh". Peter Rodman and Michael Lind claimed that the US intervention saved Cambodia from collapse in 1970 and 1973. Craig Etcheson agreed that it was "untenable" to assert that US intervention caused the Khmer Rouge victory while acknowledging that it may have played a small role in boosting recruitment for the insurgents. William Shawcross, however, wrote that the US bombing and ground incursion plunged Cambodia into the chaos Sihanouk had worked for years to avoid. 
The Vietnamese intervention in Cambodia, launched at the request of the Khmer Rouge, has also been cited as a major factor in their eventual victory, including by Shawcross. Vietnam later admitted that it played "a decisive role" in their seizure of power. China "armed and trained" the Khmer Rouge during the civil war and continued to aid them years afterward.
Democratic Kampuchea (Khmer Rouge era) (1975–79).
Immediately after its victory, the CPK ordered the evacuation of all cities and towns, sending the entire urban population into the countryside to work as farmers, as the CPK was trying to reshape society into a model that Pol Pot had conceived.
The new government sought to completely restructure Cambodian society. Remnants of the old society were abolished and religion, particularly Buddhism and Catholicism, was suppressed. Agriculture was collectivized, and the surviving part of the industrial base was abandoned or placed under state control. Cambodia had neither a currency nor a banking system.
Democratic Kampuchea's relations with Vietnam and Thailand worsened rapidly as a result of border clashes and ideological differences. While communist, the CPK was fiercely nationalistic, and most of its members who had lived in Vietnam were purged. Democratic Kampuchea established close ties with the People's Republic of China, and the Cambodian-Vietnamese conflict became part of the Sino-Soviet rivalry, with Moscow backing Vietnam. Border clashes worsened when the Democratic Kampuchea military attacked villages in Vietnam. The regime broke off relations with Hanoi in December 1977, protesting Vietnam's alleged attempt to create an Indochina Federation. In mid-1978, Vietnamese forces invaded Cambodia, advancing about before the arrival of the rainy season.
The reasons for Chinese support of the CPK was to prevent a pan-Indochina movement, and maintain Chinese military superiority in the region. The Soviet Union supported a strong Vietnam to maintain a second front against China in case of hostilities and to prevent further Chinese expansion. Since Stalin's death, relations between Mao-controlled China and the Soviet Union had been lukewarm at best. In February to March 1979, China and Vietnam would fight the brief Sino-Vietnamese War over the issue.
In December 1978, Vietnam announced formation of the Kampuchean United Front for National Salvation (KUFNS) under Heng Samrin, a former DK division commander. It was composed of Khmer Communists who had remained in Vietnam after 1975 and officials from the eastern sector—like Heng Samrin and Hun Sen—who had fled to Vietnam from Cambodia in 1978. In late December 1978, Vietnamese forces launched a full invasion of Cambodia, capturing Phnom Penh on January 7, 1979 and driving the remnants of Democratic Kampuchea's army westward toward Thailand.
Within the CPK, the Paris-educated leadership—Pol Pot, Ieng Sary, Nuon Chea, and Son Sen—were in control. A new constitution in January 1976 established Democratic Kampuchea as a Communist People's Republic, and a 250-member Assembly of the Representatives of the People of Kampuchea (PRA) was selected in March to choose the collective leadership of a State Presidium, the chairman of which became the head of state.
Prince Sihanouk resigned as head of state on April 4. On April 14, after its first session, the PRA announced that Khieu Samphan would chair the State Presidium for a 5-year term. It also picked a 15-member cabinet headed by Pol Pot as prime minister. Prince Sihanouk was put under virtual house arrest.
Social and cultural implications of the regime.
Thousands starved or died of disease during the evacuation and its aftermath. Many of those forced to evacuate the cities were resettled in newly created villages, which lacked food, agricultural implements, and medical care. Many who lived in cities had lost the skills necessary for survival in an agrarian environment. Thousands starved before the first harvest. Hunger and malnutrition—bordering on starvation—were constant during those years. Most military and civilian leaders of the former regime who failed to disguise their pasts were executed.
Some of the ethnicities in Cambodia, such as the Cham suffered specific and targeted and violent persecutions. To the point of some international sources referring to it as the "Cham genocide". Entire families and towns were targeted and attacked with the goal of significantly diminishing their numbers and eventually eliminated them. 
Life in 'Democratic Kampuchea' was strict and brutal. In many areas of the country people were rounded up and executed for speaking a foreign language, wearing glasses, scavenging for food, and even crying for dead loved ones. Former businessmen and bureaucrats were hunted down and killed along with their entire families; the Khmer Rouge feared that they held beliefs that could lead them to oppose their regime. A few Khmer Rouge loyalists were even killed for failing to find enough 'counter-revolutionaries' to execute.
Modern research has located 20,000 mass graves from the Khmer Rouge era all over Cambodia. Various studies have estimated the death toll at between 740,000 and 3,000,000, most commonly between 1.4 million and 2.2 million, with perhaps half of those deaths being due to executions, and the rest from starvation and disease.
The U.S. State Department-funded Yale Cambodian Genocide Project estimates approximately 1.7 million. R. J. Rummel, an analyst of historical political killings, gives a figure of 2 million.
A UN investigation reported 2–3 million dead, while UNICEF estimated 3 million had been killed. Demographic analysis by Patrick Heuveline suggests that between 1.17 and 3.42 million Cambodians were killed, while Marek Sliwinski estimates that 1.8 million is a conservative figure. Researcher Craig Etcheson of the Documentation Center of Cambodia suggests that the death toll was between 2 and 2.5 million, with a "most likely" figure of 2.2 million. After 5 years of researching grave sites, he concluded that "these mass graves contain the remains of 1,386,734 victims of execution".
Vietnamese occupation and the PRK (1979–93).
On January 10, 1979, after the Vietnamese army and the KUFNS invaded Cambodia, the new People's Republic of Kampuchea (PRK) was established with Heng Samrin as head of state. Pol Pot's Khmer Rouge forces retreated rapidly to the Thai border. The Khmer Rouge and the PRK began a costly struggle that played into the hands of the larger powers China, the United States and the Soviet Union. A civil war was imposed on impoverished Cambodia that displaced 600,000 Cambodians to refugee camps along the border between Thailand and Cambodia. The new regime murdered tens of thousands of people.
Peace efforts began in Paris in 1989 under the State of Cambodia, culminating two years later in October 1991 in a comprehensive peace settlement. The United Nations was given a mandate to enforce a ceasefire, and deal with refugees and disarmament known as the United Nations Transitional Authority in Cambodia (UNTAC).
Modern Cambodia (1993-present).
On October 23, 1991, the Paris Conference reconvened to sign a comprehensive settlement giving the UN full authority to supervise a cease-fire, repatriate the displaced Khmer along the border with Thailand, disarm and demobilize the factional armies, and prepare the country for free and fair elections. Prince Sihanouk, President of the Supreme National Council of Cambodia (SNC), and other members of the SNC returned to Phnom Penh in November 1991, to begin the resettlement process in Cambodia. The UN Advance Mission for Cambodia (UNAMIC) was deployed at the same time to maintain liaison among the factions and begin demining operations to expedite the repatriation of approximately 370,000 Cambodians from Thailand.
On March 16, 1992, the UN Transitional Authority in Cambodia (UNTAC) arrived in Cambodia to begin implementation of the UN Settlement Plan. The UN High Commissioner for Refugees began fullscale repatriation in March 1992. UNTAC grew into a 22,000-strong civilian and military peacekeeping force to conduct free and fair elections for a constituent assembly.
Over 4 million Cambodians (about 90% of eligible voters) participated in the May 1993 elections, although the Khmer Rouge or Party of Democratic Kampuchea (PDK), whose forces were never actually disarmed or demobilized, barred some people from participating. Prince Ranariddh's royalist FUNCINPEC Party was the top vote recipient with 45.5% of the vote, followed by Hun Sen's Cambodian People's Party and the Buddhist Liberal Democratic Party, respectively. FUNCINPEC then entered into a coalition with the other parties that had participated in the election. The parties represented in the 120-member assembly proceeded to draft and approve a new constitution, which was promulgated September 24, 1993. It established a multiparty liberal democracy in the framework of a constitutional monarchy, with the former Prince Sihanouk elevated to King. Prince Ranariddh and Hun Sen became First and Second Prime Ministers, respectively, in the Royal Cambodian Government (RGC). The constitution provides for a wide range of internationally recognized human rights.
On October 4, 2004, the Cambodian National Assembly ratified an agreement with the United Nations on the establishment of a tribunal to try senior leaders responsible for the atrocities committed by the Khmer Rouge. Donor countries have pledged the $43 million international share of the three-year tribunal budget, while the Cambodian government’s share of the budget is $13.3 million. The tribunal started trials of senior Khmer Rouge leaders in 2008. Cambodia is also recovering from the land mines which were used heavily by the Khmer Rouge and Vietnamese; it will take approximately a decade to remove most of the land mines from Cambodia.

</doc>
<doc id="5429" url="http://en.wikipedia.org/wiki?curid=5429" title="Geography of Cambodia">
Geography of Cambodia

Cambodia is a country in Southeastern Asia, bordering the Gulf of Thailand, between Thailand, Vietnam, and Laos. Its approximate geographical coordinates are . Its 2,572 km border is split among Vietnam (1,228 km), Thailand (803 km) and Laos (541 km), as well as 443 km of coastline. Cambodia covers 181,035 square kilometres in the southwestern part of the Indochina peninsula. It lies completely within the tropics; its southernmost points are only slightly more than 10° above the equator. Roughly square in shape, the country is bounded on the north by Thailand and by Laos, on the east and southeast by Vietnam, and on the west by the Gulf of Thailand and by Thailand. Much of the country's area consists of rolling plains. Dominant features are the large, almost centrally located, Tonle Sap (Great Lake) and the Mekong River, which traverses the country from north to south and is the 12th longest river in the world.
The climate is monsoonal and has marked wet and dry seasons of relatively equal length. Both temperature and humidity generally are high throughout the year. Forest covers about two-thirds of the country, but it has been somewhat degraded in the more readily accessible areas by burning (a method called slash-and-burn agriculture), and by shifting agriculture.
Topography.
Cambodia falls within several well-defined geographic regions. The largest part of the country, about 75 percent, consists of the Tonle Sap Basin and the Mekong Lowlands. To the southeast of this great basin is the Mekong Delta, which extends through Vietnam to the South China Sea. The basin and delta regions are rimmed with mountain ranges to the southwest by the Cardamom Mountains and the Elephant Range and to the north by the Dangrek Mountains. Higher land to the northeast and to the east merges into the Central Highlands of southern Vietnam.
The Tonle Sap Basin-Mekong Lowlands region consists chiefly of plains with elevations generally of less than 100 meters. As the elevation increases, the terrain becomes more rolling and dissected.
The Cardamom Mountains in the southwest, oriented generally in a northwest-southeast direction, rise to more than 1,500 meters. The highest mountain in Cambodia--Phnom Aural, at 1,771 meters—is in the eastern part of this range. The Elephant Range, an extension running toward the south and the southeast from the Cardamom Mountains, rises to elevations of between 500 and 1,000 meters. These two ranges are bordered on the west by a narrow coastal plain that contains Kampong Saom Bay, which faces the Gulf of Thailand. This area was largely isolated until the opening of the port of Sihanoukville (formerly called Kampong Saom) and the construction of a road and railroad connecting Sihanoukville, Kampot, Takev, and Phnom Penh in the 1960s.
The Dangrek Mountains at the northern rim of the Tonle Sap Basin consist of a steep escarpment with an average elevation of about 500 meters, the highest points of which reach more than 700 meters. The escarpment faces southward and is the southern edge of the Korat Plateau in Thailand. The watershed along the escarpment marks the boundary between Thailand and Cambodia. The main road through a pass in the Dangrek Mountains at O Smach connects northwestern Cambodia with Thailand. Despite this road and those running through a few other passes, in general the escarpment impedes easy communication between the two countries. Between the western part of the Dangrek and the northern part of the Cardamom ranges, however, lies an extension of the Tonle Sap Basin that merges into lowlands in Thailand, which allows easy access from the border to Bangkok.
The Mekong Valley, which offers a communication route between Cambodia and Laos, separates the eastern end of the Dangrek Mountains and the northeastern highlands. To the southeast, the basin joins the Mekong Delta, which, extending into Vietnam, provides both water and land communications between the two countries.
Climate.
Cambodia's climate, like that of much the rest of mainland Southeast Asia is dominated by monsoons, which are known as tropical wet and dry because of the distinctly marked seasonal differences. The monsoonal airflows are caused by annual alternating high pressure and low pressure over the Central Asian landmass. In summer, moisture-laden air—the southwest monsoon—is drawn landward from the Indian Ocean. The flow is reversed during the winter, and the northeast monsoon sends back dry air. The southwest monsoon brings the rainy season from mid-May to mid-September or to early October, and the northeast monsoon flow of drier and cooler air lasts from early November to March. The southern third of the country has a two-month dry season; the northern two-thirds, a four-month one. Short transitional periods, which are marked by some difference in humidity but by little change in temperature, intervene between the alternating seasons. Temperatures are fairly uniform throughout the Tonle Sap Basin area, with only small variations from the average annual mean of around . The maximum mean is about ; the minimum mean, about . Maximum temperatures of higher than , however, are common and, just before the start of the rainy season, they may rise to more than . Minimum temperatures rarely fall below . January is the coolest month, and April is the warmest. Tropical cyclones that often devastate coastal Vietnam rarely cause damage in Cambodia.
The total annual rainfall average is between , and the heaviest amounts fall in the southeast. Rainfall from April to September in the Tonle Sap Basin-Mekong Lowlands area averages annually, but the amount varies considerably from year to year. Rainfall around the basin increases with elevation. It is heaviest in the mountains along the coast in the southwest, which receive from to more than of precipitation annually as the southwest monsoon reaches the coast. This area of greatest rainfall, however, drains mostly to the sea; only a small quantity goes into the rivers flowing into the basin. The relative humidity is high at night throughout the year; usually it exceeds 90 percent. During the daytime in the dry season, humidity averages about 50 percent or slightly lower, but it may remain about 60 percent in the rainy period.
Drainage.
Except for the smaller rivers in the southeast, most of the major rivers and river systems in Cambodia drain into the Tonle Sap or into the Mekong River. The Cardamom Mountains and Elephant Range form a separate drainage divide. To the east the rivers flow into the Tonle Sap, while on the west they flow into the Gulf of Thailand. Toward the southern end of the Elephant Mountains, however, because of the topography, some small rivers flow southward on the eastern side of the divide.
The Mekong River in Cambodia flows southward from the Cambodia-Laos border to a point below Kracheh city, where it turns west for about 50 kilometers and then turns southwest to Phnom Penh. Extensive rapids run above Kracheh city. From Kampong Cham the gradient slopes very gently, and inundation of areas along the river occurs at flood stage—June through November—through breaks in the natural levees that have built up along its course. At Phnom Penh four major water courses meet at a point called the Chattomukh (Four Faces). The Mekong River flows in from the northeast and the Tonle Sab—a river emanating from the Tonle Sap—flows in from the northwest. They divide into two parallel channels, the Mekong River proper and the Basak River, and flow independently through the delta areas of Cambodia and Vietnam to the South China Sea.
The flow of water into the Tonle Sap is seasonal. In September or in October, the flow of the Mekong River, fed by monsoon rains, increases to a point where its outlets through the delta cannot handle the enormous volume of water. At this point, the water pushes northward up the Tonle Sab and empties into the Tonle Sap, thereby increasing the size of the lake from about 2,590 square kilometers to about 24,605 square kilometers at the height of the flooding. After the Mekong's waters crest—when its downstream channels can handle the volume of water—the flow reverses, and water flows out of the engorged lake.
As the level of the Tonle Sap retreats, it deposits a new layer of sediment. The annual flooding, combined with poor drainage immediately around the lake, transforms the surrounding area into marshlands unusable for agricultural purposes during the dry season. The sediment deposited into the lake during the Mekong's flood stage appears to be greater than the quantity carried away later by the Tonle Sap River. Gradual silting of the lake would seem to be occurring; during low-water level, it is only about 1.5 meters deep, while at flood stage it is between 10 and 15 meters deep.
Regional divisions.
Cambodia's boundaries were for the most part based upon those recognized by France and by neighboring countries during the colonial period. The 800-kilometre boundary with Thailand, coincides with a natural feature, the watershed of the Dangrek Mountains, only in its northern sector. The 541-kilometer border with Laos and the 1,228-kilometer border with Vietnam result largely from French administrative decisions and do not follow major natural features. Border disputes have broken out in the past between Cambodia and Thailand as well as between Cambodia and Vietnam.
Area and boundaries.
Area:
<br>"total:"
181,035 km²
<br>"land:"
181,035 km²
<br>"water:"
4,520 km²
Maritime claims:
<br>"contiguous zone:"
<br>"continental shelf:"
<br>"exclusive economic zone:"
<br>"territorial sea:"
Elevation extremes:
<br>"lowest point:"
Gulf of Thailand 0 m
<br>"highest point:"
Phnum Aoral 1,810 m
Resources and land use.
Natural resources:
oil and natural gas, timber, gemstones, iron ore, manganese, phosphates, hydropower potential
Land use:
<br>"arable land:"
20.44%
<br>"permanent crops:"
0.59%
<br>"other:"
78.97% (2005)
"'Total renewable water resources:"
 (1999)
Freshwater withdrawal (domestic/industrial/agricultural):
<br>"total:" /yr (1%/0%/98%)
<br>"per capita:" /yr (2000)
Irrigated land:
2800 km² (2003)
Environmental concerns.
Natural hazards:
monsoonal rains (June to November); flooding; occasional droughts
Environment - current issues:
illegal logging activities throughout the country and strip mining for gems in the western region along the border with Thailand have resulted in habitat loss and declining biodiversity (in particular, destruction of mangrove swamps threatens natural fisheries); soil erosion; in rural areas, most of the population does not have access to potable water; declining fish stocks because of illegal fishing and overfishing
Environment - international agreements:
<br>"party to:"
Biodiversity, Climate Change, Desertification, Endangered Species, Marine Life Conservation, Ship Pollution (MARPOL 73/78), Tropical Timber 94, Wetlands
<br>"signed, but not ratified:"
Law of the Sea, Marine Dumping
Geography - note:
a land of paddies and forests dominated by the Mekong River and Tonle Sap
Lakes

</doc>
<doc id="5430" url="http://en.wikipedia.org/wiki?curid=5430" title="Demographics of Cambodia">
Demographics of Cambodia

This article is about the demographic features of the population of Cambodia, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
Population.
Between 1874 and 1921, the total population of Cambodia increased from about 946,000 to 2.4 million. By 1950, it had increased to between 3,710,107 and 4,073,967, and in 1962 it had reached 5.7 million. From the 1960s until 1975, the population of Cambodia increased by about 2.2% yearly, the lowest increase in Southeast Asia. By 1975 when the Khmer Rouge took power, it was estimated at 7.3 million. Of this total an estimated one to two million reportedly died between 1975 and 1978. In 1981, the PRK gave the official population figure as nearly 6.7 million, although approximately 6.3 million to 6.4 million is probably more accurate. The average annual rate of population growth from 1978 to 1985 was 2.3% (see table 2, Appendix A). A post-Khmer Rouge baby boom pushed the population above 10 million, although growth has slowed in recent years.
In 1959, about 45% of the population was under 15 years of age. By 1962, this had increased slightly to 46%. In 1962, an estimated 52% of the population was between 15 and 64 years of age, while 2% were older than 65. The percentage of males and females in the three groups was almost the same.
Vital statistics.
Fertility.
According to 2010 Demographic and Health Survey, the total fertility rate in Cambodia was 3.0 children per woman in 2010. In 2000, this was 4.0 children and in 2005 3.4. Women in urban areas have 2.2 children on average, compared with 3.3 children per woman in rural areas. Fertility is highest in Mondol Kiri/Rattanak Kiri Province, where women have an average of 4.5 children, and lowest in Phnom Penh where women have an average of 2.0 children. According to the survey, Cambodian women want about three children, on average. Ideal family size is slightly higher among women in rural areas than urban areas (3.2 versus 2.9). Women with secondary and higher education desire fewer children than women with no schooling (2.8 versus 3.5).
Fertility and Births.
Total Fertility Rate (TFR) and Crude Birth Rate (CBR):
Infant and childhood mortality.
Childhood mortality rates are decreasing in Cambodia. Currently, the infant mortality rate is 45 deaths per 1,000 live births for the five-year period before the survey compared with 66 deaths reported in the 2005 CDHS and 95 in the 2000 CDHS. Under-five mortality rates have also decreased from 124 deaths per 1,000 live births in 2000, 83 deaths in 2005 to 54 deaths per 1,000 in 2010.
Childhood mortality decreases markedly with mother’s education and wealth. Infant mortality, for example, is twice as high among children whose mothers have no schooling compared to those with secondary or higher education (72 versus 31). The association with wealth is even stronger. There are 77 deaths per 1,000 live births among infants from the poorest households compared to only 23 deaths per 1,000 live births among infants from the richest households.
Mortality rates are much higher in rural than urban areas. Infant mortality, for example, is 64 deaths per 1,000 live births in rural areas compared to only 22 in urban areas.
Mortality also differs by province. Infant mortality ranges from only 13 deaths per 1,000 live births in Phnom Penh to 78 deaths per 1,000 live births in Kampong Chhnang and Svay Rieng.
Life expectancy.
In 1959, life expectancy at birth was 44.2 years for males and 43.3 years for females. By 1970, life expectancy had increased by about 2.5 years since 1945. The greater longevity for females apparently reflected improved health practices during maternity and childbirth.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
HIV/AIDS.
adult prevalence rate
people living with HIV/AIDS
deaths
Overseas Population.
Countries with notable populations of overseas Cambodians are:

</doc>
<doc id="5431" url="http://en.wikipedia.org/wiki?curid=5431" title="Politics of Cambodia">
Politics of Cambodia

The politics of Cambodia takes place in a frame work of a constitutional monarchy, where by the Prime Minister is the head of government and a Monarch is head of state. The kingdom formally operates according to the nation's constitution (enacted in 1993) in a framework of a parliamentary, representative democracy. Executive power is exercised by the government. Legislative power is vested in the two chambers of parliament, the National Assembly and the Senate.
Executive branch.
The Prime Minister of Cambodia is a representative from the ruling party of the National Assembly. He or she is appointed by the King on the recommendation of the President and Vice Presidents of the National Assembly. In order for a person to become Prime Minister, he or she must first be given a vote of confidence by the National Assembly.
The Prime Minister is officially the Head of Government in Cambodia. Upon entry into office, he or she appoints a Council of Ministers who are responsible to the Prime Minister. Officially, the Prime Minister's duties include chairing meetings of the Council of Ministers (Cambodia's version of a Cabinet) and appointing and leading a government. The Prime Minister and his government make up Cambodia's executive branch of government.
The current Prime Minister is Cambodian People's Party (CPP) member Hun Sen. He has held this position since the criticized 1998 election, one year after the CPP staged a bloody coup in Phnom Penh to overthrow elected Prime Minister Prince Norodom Ranariddh, president of the FUNCINPEC party. Hun Sen has vowed to rule until he is 74. Hun Sen is a former Khmer Rouge member who defected and oversaw Cambodia's rise from the ashes of war. His government is regularly accused of ignoring human rights and suppressing political dissent. After the 2013 election results, disputed by Hun Sen's opposition, demonstrators were injured and killed in Cambodia in protests in the capital where a reported 20,000 protesters gathered, some clashing with riot police. From a humble farming background, Hun Sen was just 33 when he took power in 1985 and is now in the unenviable company of enduring dictators such as Zimbabwe's Robert Mugabe and Kazakhstan's Nursultan Nazarbayev'.
Legislative branch.
The legislative branch of the Cambodian government is made up of a bicameral parliament.
The official duty of the Parliament is to legislate and make laws. Bills passed by the Parliament are given to the King who gives the proposed bills Royal Assent. The King does not have veto power over bills passed by the National Assembly and thus, cannot withhold Royal Assent. The National Assembly also has the power to dismiss the Prime Minister and his government by a two-thirds vote of no confidence.
Senate.
The upper house of the Cambodian legislature is called the "Senate". It consists of sixty-one members. Two of these members are appointed by the King, two are elected by the lower house of the government, and the remaining fifty-seven are elected popularly by electors from provincial and local governments, in a similar fashion to the Senate of France. Members in this house serve six-year terms.
Prior to 2006, elections had last been held for the Senate in 1999. New elections were supposed to have occurred in 2004, but these elections were initially postponed. On January 22, 2006, 11,352 possible voters went to the poll and chose their candidates. This election was criticized by local monitoring non-governmental organizations as being undemocratic .
, the Cambodian People's Party holds forty-three seats in the Senate, constituting a significant majority. The two other major parties holding seats in the Senate are the Funcinpec party (holding twelve seats) and the Sam Rainsy Party (holding two seats).
National Assembly.
The lower house of the legislature is called the "National Assembly". It is made up of 123 members, elected by popular vote to serve a five-year term. Elections were last held for the National Assembly in July 2008.
In order to vote in legislative elections, one must be at least eighteen years of age. However, in order to be elected to the Legislature, one must be at least twenty-five years of age.
The National Assembly is led by a President and two Vice Presidents who are selected by Assembly members prior to each session.
, the Cambodian People's Party holds a majority of the seats in the National Assembly, controlling 90 out of the 123 seats. The Sam Rainsy Party holds 26 seats and other parties hold the other 7 seats.
Judicial branch.
The judicial branch is independent from the rest of the government, as specified by the Cambodian Constitution. The highest court of judicial branch is the Supreme Council of the Magistracy. Other, lower courts also exist. Until 1997, Cambodia did not have a judicial branch of government despite the nation's Constitution requiring one.
The main duties of the judiciary are to prosecute criminals, settle lawsuits, and, most importantly, protect the freedoms and rights of Cambodian citizens. However, in reality, the judicial branch in Cambodia is highly corrupt and often serves as a tool of the executive branch to silence civil society and its leaders . There are currently 17 justices on the Supreme Council.
Monarchy.
Cambodia is a constitutional monarchy, i.e. the King reigns but does not rule, in similar fashion to Queen Elizabeth II of the United Kingdom. The King is officially the Head of State and is the symbol of unity and "eternity" of the nation, as defined by Cambodia's constitution.
From September 24, 1993 through October 7, 2004, Norodom Sihanouk reigned as King, after having previously served in a number of offices (including King) since 1941. Under the Constitution, the King has no political power, but as Norodom Sihanouk was revered in the country, his word often carried much influence in the government. For example, in February 2004, he issued a proclamation stating that since Cambodia is a "liberal democracy," the Kingdom ought to allow gay marriage. While such views aren't prevalent in Cambodia, his word was respected by his subjects. The King, often irritated over the conflicts in his government, several times threatened to abdicate unless the political factions in the government got along. This put pressure on the government to solve their differences. This influence of the King was often used to help mediate differences in government.
After the abdication of King Norodom Sihanouk in 2004, he was succeeded by his son Norodom Sihamoni. While the retired King was highly revered in his country for dedicating his lifetime to Cambodia, the current King has spent most of his life abroad in France. Thus, it remains to be seen whether the new King's views will be as highly respected as his father's.
Although in the Khmer language there are many words meaning "king", the word officially used in Khmer (as found in the 1993 Cambodian Constitution) is "preahmâhaksat" (Khmer regular script:), which literally means: "preah"- ("sacred", cognate of the Indian word Brahmin) -"mâha"- (from Sanskrit, meaning "great", cognate with "maha-" in maharaja) -"ksat" ("warrior, ruler", cognate of the Indian word Kshatriya).
On the occasion of HM King Norodom Sihanouk's retirement in September 2004, the Cambodian National Assembly coined a new word for the retired king: "preahmâhaviraksat" (Khmer regular script:), where "vira" comes from Sanskrit "vīra", meaning "brave or eminent man, hero, chief", cognate of Latin "vir", "viris", English "virile". "Preahmâhaviraksat" is translated in English as "King-Father" (), although the word "father" does not appear in the Khmer noun.
As "preahmâhaviraksat", Norodom Sihanouk retained many of the prerogatives he formerly held as "preahmâhaksat" and was a highly respected and listened-to figure. Thus, in effect, Cambodia could be described as a country with two Kings during Sihanouk's lifetime: the one who was the Head of State, the "preahmâhaksat" Norodom Sihamoni, and the one who was not the Head of State, the "preahmâhaviraksat" Norodom Sihanouk.
Sihanouk died of a pulmonary infarction on October 15, 2012.
Succession to the throne.
Unlike most monarchies, Cambodia's monarchy is not necessarily hereditary and the King is not allowed to select his own heir. Instead, a new King is chosen by a Royal Council of the Throne, consisting of the president of the National Assembly, the Prime Minister, the Chiefs of the orders of Mohanikay and Thammayut, and the First and Second Vice-President of the Assembly. The Royal Council meets within a week of the King's death or abdication and selects a new King from a pool of candidates with royal blood.
It has been suggested that Cambodia's ability to peacefully appoint a new King shows that Cambodia's government has stabilized incredibly from the situation the country was in during the 1970s (see History of Cambodia).
International organization participation.
ACCT, AsDB, ASEAN, CP, ESCAP, FAO, G-77, IAEA, IBRD, ICAO, ICC, ICRM, IDA, IFAD, IFC, IFRCS, ILO, IMF, IMO, Intelsat (nonsignatory user), International Monetary Fund, Interpol, IOC, ISO (subscriber), ITU, NAM, OPCW, PCA, UN, UNCTAD, UNESCO, UNIDO, UPU, WB, WFTU, WHO, WIPO, WMO, WTO, WToO, WTrO (applicant)
Provincial and local governments.
Below the central government are 24 provincial and municipal administration. (In rural areas, first-level administrative divisions are called provinces; in urban areas, they are called municipalities.) The administrations are a part of the Ministry of the Interior and their members are appointed by the central government. Provincial and municipal administrations participate in the creation of nation budget; they also issue land titles and license businesses.
Since 2002, commune-level governments (commune councils) have been composed of members directly elected by commune residents every five years.
In practice, the allocation of responsibilities between various levels of government is uncertain. This uncertainty has created additional opportunities for corruption and increased costs for investors.

</doc>
<doc id="5432" url="http://en.wikipedia.org/wiki?curid=5432" title="Economy of Cambodia">
Economy of Cambodia

The economy of Cambodia at present follows an open market system (Market economy) and has seen rapid economic progress in the last decade. Per capita income, although rapidly increasing, is low compared with most neighboring countries. Cambodia's two largest industries are textiles and tourism, while agricultural activities remain the main source of income for many Cambodians living in rural areas. The service sector is heavily concentrated on trading activities and catering-related services. Recently, Cambodia has reported that oil and natural gas reserves have been found off-shore.
In 1995, the government transformed the country's economic system from a Planned economy to its present market-driven system. Following those changes, growth was estimated at a value of 7% while inflation dropped from 26% in 1994 to only 6% in 1995. Imports increased due to the influx of foreign aid, and exports, particularly from the country's garment industry, also increased.
After four years of improving economic performance, Cambodia's economy slowed in 1997-98 due to the regional economic crisis, civil unrest, and political infighting. Foreign investments declined during this period. Also, in 1998 the main harvest was hit by drought. But in 1999, the first full year of relative peace in 30 years, progress was made on economic reforms and growth resumed at 4%.
Currently, Cambodia's foreign policy focuses on establishing friendly borders with its neighbors (such as Thailand and Vietnam), as well as integrating itself into regional (ASEAN) and global (WTO) trading systems. Some of the obstacles faced by this emerging economy are the need for a better education system and the lack of a skilled workforce; particularly in the poverty-ridden countryside, which struggles with inadequate basic infrastructure. Nonetheless, Cambodia continues to attract investors because of its low wages, plentiful labor, proximity to Asian raw materials, and favorable tax treatment.
History.
Following its independence from France in 1953, the Cambodian state underwent five periods of political, social, and economic transformation:
In 1989, the State of Cambodia implemented reform policies that transformed the Cambodian economic system from a Command economy to an open market one. In line with the economic reformation, private property rights were introduced and state-owned enterprises were privatized. Cambodia also focused on integrating itself into regional and international economic blocs, such as the Association of South East Asian Nations and the World Trade Organization respectively. These policies triggered a growth in the economy, with its national GDP growing at an average of 6.1% before a period of domestic unrest and regional economic instability in 1997 (1997 Asian Financial Crisis). However, conditions improved and since 1999, the Cambodian economy has continued to grow at an average pace of approximately 6-8% per annum.
Foreign aid.
Cambodia's emerging democracy has received strong international support. Under the mandate of the United Nations Transitional Authority in Cambodia (UNTAC), $1.72 billion (1.72 G$) was spent in an effort to bring basic security, stability and democratic rule to the country.
With regards to economic assistance, official donors had pledged $880 million at the Ministerial Conference on the Rehabilitation of Cambodia (MCRRC) in Tokyo in June 1992. In addition to that figure, $119 million was pledged in September 1993 at the International Committee on the Reconstruction of Cambodia (ICORC) meeting in Paris, and $643 million at the March 1994 ICORC meeting in Tokyo.
Cambodia experienced a shortfall in foreign aid in the year 2005 due to the government's failure in passing anti-corruption laws, opening up a single import/export window, increasing its spending on education, and complying with policies of good governance. In response, the government adopted the National Strategic Development Plan for 2006–10 (also known as the “Third Five-Year Plan”). The plan focused on three major areas:
Recent developments.
In 2007, Cambodia's Gross domestic product grew by an estimated 18.6%. Garment exports rose by almost 8%, while tourist arrivals increased by nearly 35%. With exports decreasing, the 2007 GDP growth was driven largely by consumption and investment. Foreign direct investment(FDI) inflows reached US$600 million (7 percent of GDP), slightly more than what the country received in official aid. Domestic investment, driven largely by the private sector, accounted for 23.4 percent of GDP. Export growth, especially to the
US, began to slow in late 2007 accompanied by stiffer competition from Vietnam and emerging risks (a slowdown in the US economy and lifting of safeguards on China’s exports). US companies were the fifth largest investors in Cambodia, with more than $1.2 billion in investments over the last decade.
Cambodia was severely hit by the 2008 economic crisis (refer to table below), and its main economic sector, the garment industry, suffered a 23% drop in exports to the United States of America and Europe. As a result, 60,000 workers were laid off. However, in the last quarter of 2009 and early 2010, conditions were beginning to improve and the Cambodian economy is recovering. Cambodian exports to the US for the first 11 months of 2012 reached $2.49 billion, a 1 per cent increase year-on-year. Its imports of US goods grew 26 per cent for that period, reaching $213 million. Another factor underscoring the potential of Cambodia economic engine is the fall of poverty that decreased half the size. However, at another hand, people’s proceeds let them to do so by a small margin. Thus, the poverty rate is 20,5 per cent making approximately 2,8 million people live below the poverty line, besides 90 per cent of them live in the countryside.
The table below represents the fluctuations in Cambodia's economy over the past 8 years (2012 data is not yet available). 
Industries.
The garment industry represents the largest portion of Cambodia's manufacturing sector, accounting for 80% of the country's exports. In 2012, the exports grew to $4.61 billion up 8% over 2011. In the first half of 2013, the garment industry reported exports worth $1.56 billion. The sector employs 335,400 workers, of which 91% are female.
Cambodia is a country with a GDP of just $13 billion. The sector operates largely on the final phase of garment production, that is turning yarns and fabrics into garments, as the country lacks a strong textile manufacturing base. In 2005, there were fears that the end of the Multi Fibre Arrangement would threaten Cambodia's garment industry; exposing it to stiff competition with China's strong manufacturing capabilities. On the contrary, Cambodia's garment industry at present continues to grow rapidly. This is can be attributed to the country's open economic policy which has drawn in large amounts of foreign investment into this sector of the economy.
Garment Factories by Ownership Nationality in 2010:
note: In 2010, 236 garment export-oriented factories are currently operating and registered with GMAC,
with 93% being foreign direct investment (FDI).
As seen in the table above, Cambodia's garment industry is characterized by a small percentage of local ownership. This is a reflection of the deficiency of skilled workers in the country as well as the limited leverage and autonomy Cambodian factories have in strategic decisions. Another characteristic of the industry is the country's competitive advantage as the only country where garment factories are monitored and reported according to national and international standards. 
This has allowed Cambodia to secure its share of quotas for exports to the US through the US-Cambodia Trade Agreement on Textiles and Apparel (1999-2004), which linked market access to labor standards. However, the Cambodian garment industry remains vulnerable to global competition due to a lack of adequate infrastructure, labor unrest, the absence of a domestic textile industry, and almost complete dependence on imported textile material.
In the 1960s, Cambodia was a prominent tourist destination in the Southeast Asian region. But due to protracted periods of civil war, insurgencies, and especially the genocidal regime of the Khmer Rouge (see Khmer Rouge Genocide), Cambodia's tourism industry was close to non-existent. However, since the late 1990s, tourism is fast becoming Cambodia's second largest industry, just behind the garment manufacturing. In 2006, Cambodia's tourism sector generated a revenue of US$1.594 billion, which made up approximately 16% of the country's GDP.
Cultural heritage tourism is especially popular in the country, with many foreign tourists visiting the ancient Hindu temple of Angkor Wat located in the Siem Reap province. Other popular tourist attractions include the Royal Palace, Phnom Penh, as well as ecotourism spots such as Tonlé Sap Lake and the Mekong River.
The tourism industry in Cambodia has been perpetuated by the development of important transportation infrastructure; in particular Cambodia's two international airports in Phnom Penh and Siem Reap respectively. To the Cambodian economy, tourism has been a means for accumulation of foreign currency earnings and employment for the Cambodian workforce, with about 250,000 jobs generated in 2006. Meanwhile, challenges to the industry include a leakage of revenue to foreign markets due to a dependence on foreign goods as well as the prevalence of the Child sex tourism industry.
The gambling industry of Cambodia supports its tourism industry, which is mostly concentrated around the Siem Reap province. The introduction of casino on border cities and towns created an industry that has thrived and contributed to the generation of employment and a steady stream of revenue for the government. However, the issue of corruption in relation to the government bureaucratic process involved in the gambling sector has been raised. It has likewise spur growth in different parts of the country at border crossing towns like Poipet, Bavet and Koh Kong. The growth of the gambling industry in Cambodia is due to its proximity to Thailand where gambling is forbidden.
The increase in tourist arrivals has led to growing demand for hotels and other forms of accommodation surrounding tourist hotspots. Siem Reap in particular has seen a construction boom in recent years. The capital Phnom Penh has also witnessed a growth in the construction and real estate sector. Recently, planned projects that have been on the pipeline for several years have been shelved temporarily due to a reduction in foreign investment.
From 2009, the Cambodian government has allowed foreigners to own condominiums. This has helped in attracting real estate investors from Thailand, Malaysia, Singapore and other countries.
The construction sector has attracted investment of $2.1 billion in 2012 which is a 72 per cent rise compared with 2011. Construction licenses issued stood at 1,694 projects in 2012, which was 20% lower than 2011 but they were of higher in value.
Oil seeps were discovered in Cambodia as early as the 1950s by Russian and Chinese geologists. Development of the industry was delayed, however, by the Vietnam and Cambodian Civil Wars and the political uncertainty that followed. Further discoveries of oil and natural gas deposits offshore in the early 2000s led to renewed domestic and international interest in Cambodia's production possibilities. As of 2013, the US company Chevron, Japanese JOGMEC and other international companies maintain production sites both on shore and off. Chevron alone has invested over 160 million USD and drilled 18 wells. Sok Khavan, acting director general of the Cambodian National Petroleum Authority, estimates that once the contracts are finalized and legal issues resolved, the Cambodian government will receive approximately 70% of the revenues, contributing to an economy in which the GDP is projected to increase five-fold by 2030. In addition, there are 10,000 square miles offshore in the Gulf of Thailand that holds potential reserves of 12-14 trillion cubic feet of natural gas and an unspecified amount of oil. The rights to this territory are currently a subject of dispute between Cambodia and Thailand, further delaying any possible production developments. In early 2013 it was reported that the two countries were close to a deal that would allow joint production to begin.
Statistics.
 

</doc>
<doc id="5433" url="http://en.wikipedia.org/wiki?curid=5433" title="Telecommunications in Cambodia">
Telecommunications in Cambodia

Telacommunications in Cambodia include telephone, radio, television, and Internet services, which are regulated by the Ministry of Posts and Telecommunications. Transport and posts were restored throughout most of the country in the early 1980s during the People's Republic of Kampuchea regime after being disrupted under the Khmer Rouge.
In January 1987, the Soviet-aided Intersputnik space communications station began operation in Phnom Penh and established two-way telecommunication links between the Cambodian capital and the cities of Moscow, Hanoi, Vientiane and Paris. The completion of the earth satellite station restored the telephone and telex links among Phnom Penh, Hanoi, and other countries for the first time since 1975. Although telecommunications services were initially limited to the government, these advances in communications helped break down the country's isolation, both internally and internationally.
Today, with the availability of mobile phones, communications are open to all, though the country's Prime Minister Hun Sen decreed that 3G mobile phones would not be allowed to support video calling.
Telephones.
The government state communications corporation is Telecom Cambodia, founded in 2006 as an expansion of the telecom operating department of the Ministry of Posts and Telecommunications. 
Fixed line service in Phnom Penh and other provincial cities is available. Mobile-phone systems are widely used in urban areas to bypass deficiencies in the fixed-line network. Mobile phone coverage is rapidly expanding in rural areas. Mobile-cellular usage, aided by increasing competition among service providers, is increasing.
International calling access is adequate, but expensive. Fixed line and mobile service is available to all countries from Phnom Penh and major provincial cities.
Radio and television.
In 2009 Cambodian broadcasters were a mixture of state-owned, joint public-private, and privately owned companies.
Radio stations.
In 2009 there were roughly 50 radio broadcast stations - 1 state-owned broadcaster with multiple stations and a large mixture of public and private broadcasters. Several international broadcasters are also available.
Provincial stations.
There are radio stations in each of the following provinces: Banteay Meanchey, Battambang, Kampong Cham, Kampong Thom, Kampot, Kandal, Pailin, Preah Vihear, Siem Reap, Sihanoukville and Svay Rieng.
Television.
In 2009 there were 9 TV broadcast stations with most operating on multiple channels, including 1 state-operated station broadcasting from multiple locations, 6 stations either jointly operated or privately owned with some broadcasting from several locations, and 2 TV relay stations - one relaying a French TV station and the other relaying a Vietnamese TV station. Multi-channel cable and satellite systems are also available.
Internet.
During 2012 Internet access was somewhat available to the people of Cambodia, particularly in urban centers, and some 50 percent of Cambodians were able to access the Internet through their mobile phones, according to the Ministry of Posts and Telecommunications.
Internet censorship and surveillance.
In its "Freedom on the Net 2013" report, Freedom House gives Cambodia a "Freedom on the Net Status" of "partly free". 
Compared to traditional media in Cambodia, new media, including online news, social networks and personal blogs, enjoy more freedom and independence from government censorship and restrictions. However, the government does proactively block blogs and websites, either on moral grounds, or for hosting content deemed critical of the government. The government restricts access to sexually explicit content, but does not systematically censor online political discourse. Since 2011 three blogs hosted overseas have been blocked for perceived antigovernment content. In 2012, government ministries threatened to shutter internet cafes too near schools—citing moral concerns—and instituted surveillance of cafe premises and cell phone subscribers as a security measure.
Early in 2011, very likely at the urging of the Ministry of Posts and Telecommunications, all Cambodian ISPs blocked the hosting service Blogspot, apparently in reaction to a December 2010 post on KI-Media, a blog run by Cambodians from both inside and outside the country. The site, which is often critical of the administration, described the prime minister and other officials as "traitors" after opposition leader Sam Rainsy alleged they had sold land to Vietnam at a contested national border. All ISPs but one subsequently restored service to the sites following customer complaints. In February 2011, however, multiple ISPs reinstated blocks on individual Blogspot sites, including KI-Media, Khmerization—another critical citizen journalist blog—and a blog by the Khmer political cartoonist Sacrava. 
There are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms without appropriate legal authority. During 2012 NGOs expressed concern about potential online restrictions. In February and November, the government published two circulars, which, if implemented fully, would require Internet cafes to install surveillance cameras and restrict operations within major urban centers. Activists also reported concern about a draft “cybercrimes” law, noting that it could be used to restrict online freedoms. The government maintained it would only regulate criminal activity.
The constitution provides for freedom of speech and press; however, these rights were not always respected in practice. The 1995 press law prohibits prepublication censorship or imprisonment for expressing opinions; however, the government uses the penal code to prosecute citizens on defamation, disinformation, and incitement charges. The penal code does not prescribe imprisonment for defamation, but does for incitement or spreading disinformation, which carry prison sentences of up to three years. Judges also can order fines, which may lead to jail time if not paid. The constitution requires that free speech not adversely affect public security. 
The constitution declares that the king is “inviolable,” and a Ministry of Interior directive conforming to the defamation law reiterates these limits and prohibits publishers and editors from disseminating stories that insult or defame government leaders and institutions. The continued criminalization of defamation and disinformation and a broad interpretation of criminal incitement constrains freedom of expression. 
The law provides for the privacy of residence and correspondence and prohibits illegal searches; however, NGOs report that police routinely conduct searches and seizures without warrants. 
Corruption remains pervasive and governmental human rights bodies are generally ineffective. A weak judiciary that sometimes fails to provide due process or fair trial procedures is a serious problem. The courts lack human and financial resources and, as a result, are not truly independent and are subject to corruption and political influence.

</doc>
<doc id="5434" url="http://en.wikipedia.org/wiki?curid=5434" title="Transport in Cambodia">
Transport in Cambodia

War and continuing fighting severely damaged Cambodia's transportation system — a system that had been inadequately developed in peacetime. The country's weak infrastructure hindered emergency relief efforts and created tremendous problems of procurement of supplies in general and of distribution. Cambodia received Soviet technical assistance and equipment to support the maintenance of the transportation network.
Railways.
After decades of neglect and damage from wartime, Cambodia's rail network is currently being reconstructed; this includes part of the Trans-Asian Railway project with modern trains replacing the current open-access system of "bamboo trains", homemade bamboo mats powered by go-kart or water pump engines. Two rail lines exist, both originating in Phnom Penh and totaling about 612 kilometers of single track. A third line is planned to connect Phnom Penh with Vietnam, the last missing link of the planned rail corridor between Singapore and the city of Kunming, China. A new north-south line is also planned.
Highways.
Of the current total, only about 50 percent of the roads and highways were covered with asphalt and were in good condition; about 50 percent of the roads were made of crushed stone, gravel, or improved earth; and the remaining approximately 30 percent were unimproved earth or were little more than tracks. In 1981 Cambodia opened a newly repaired section of National Route 1 which runs southeast from Phnom Penh to the Vietnamese border. The road, which suffered damage during the war years, was restored most probably by Vietnamese army engineers. In the late 1980s, Cambodia's road network was both underutilized and unable to meet even the modest demands placed upon it by an unindustrialized and agriculture society (see fig. 8.). Commercial vehicles, such as trucks and buses, were insufficient in number and lacked the spare parts necessary to keep them running. Road construction and maintenance were ignored by a financially hard-pressed government, while insurgents regularly destroyed bridges and rendered some routes unsafe for travel.
Cambodia is upgrading the main highways to international standards and most are vastly improved from 2006. Most main roads are now paved. And now road construction is on going from the Thailand border at Poipet to Siem Reap (Angkor Wat).
Chart of 01/2014
Waterways.
The nation's extensive inland waterways were important historically in domestic trade. The Mekong and the Tonlé Sap Rivers, their numerous tributaries, and the Tonlé Sap provided avenues of considerable length, including 3,700 kilometers navigable all year by craft drawing 0.6 meters and another 282 kilometers navigable to craft drawing 1.8 meters. In some areas, especially west of the Mekong River and north of the Tonle Sap River, the villages were completely dependent on waterways for communications. Launches, junks, or barges transport passengers, rice, and other food in the absence of roads and railways.
According to the Ministry of Communications, Transport, and Posts, Cambodia's main ferry services crossing the Bassac River and the middle Mekong River at Neak Leung Ferry Service, Tonle Bet Ferry Service, Sre Ambel Ferry Service, Kampong Cham Ferry Service, and Stoeng Treng Ferry Service were restored in 1985. The major Mekong River navigation routes also were cleared for traffic.
Also seaplane service to all waterways and islands in now offered by Aero Cambodia Airline.
Seaports and harbors.
Cambodia has two major ports, Phnom Penh Port and Sihanoukville Port, also known as Kampong Som, and five minor ones. Phnom Penh, located at the junction of the Bassac, the Mekong, and the Tonle Sap rivers, is the only river port capable of receiving 8,000-ton ships during the wet season and 5,000-ton ships during the dry season. It remains an important port for international commerce as well as for domestic communications.
Sihanoukville port reopened in late 1979. It had been built in 1960 with French assistance. In 1980 some 180 Soviet dockworkers, having brought with them forklifts and trucks, were reportedly working at Kampong Som as longshoremen or as instructors of unskilled Cambodian port workers. By 1984 approximately 1,500 Cambodian port workers were handling 2,500 tons of cargo per day. According to official statistics, Sihanoukville had handled only 769,500 tons in the four prior years (1979 to 1983), a level that contrasted sharply with the port's peacetime capacity of about 1 million tons of cargo per year.
Airports.
The country possesses twenty-six airfields, of which only thirteen were usable in the mid-1980s. Eight airfields had permanent-surface runways. Phnom Penh International Airport in Phnom Penh is the largest airport; it also serves as the main base for the renascent Cambodian Air Force.
Cambodia's second largest airport is Angkor International Airport in the major tourist city of Siem Reap. Tourist traffic into Angkor International Airport saw passenger numbers overtake those of Phnom Penh in 2006, the airport now being the country's busiest.
Cambodia also opened a new Soviet-built airfield at Ream, Sihanoukville International Airport in late 1983, which never saw commercial air traffic until now. There are additional airports in Battambang and Stung Treng.
The new national airline Cambodia Angkor Air was launched in 2009, with a large financial investment from Vietnam Airlines. And Aero Cambodia Airline started business in 2011 offering flights to all airports and water ways with seaplanes.
Airports - with paved runways.
"Total:"
6
"2,500 to 3,000 m:"
3
"1,500 to 2,2500 m:"
2
"1000 to 1,500 m:"
1 (2010)
Airports - with unpaved runways.
"Total:"
11
<br>"1,500 to 2,500 m:"
1
<br>"1000 to 1,500 m:"
9
<br>"under 1000 m:"
1 (2010)
Heliports.
1 (2010)

</doc>
<doc id="5436" url="http://en.wikipedia.org/wiki?curid=5436" title="Foreign relations of Cambodia">
Foreign relations of Cambodia

The Royal Cambodian Government (RGC) has established diplomatic relations with most countries, including the United States, the United Kingdom, and France, as well as all of its Asian neighbors, to include the People's Republic of China, India, Vietnam, Laos, South Korea, North Korea, and Thailand. The RGC is a member of most major international organizations, including the United Nations and its specialized agencies such as the World Bank and International Monetary Fund. The RGC is an Asian Development Bank (ADB) member, a member of ASEAN, also a member of the WTO. In 2005 Cambodia attended the inaugural East Asia Summit.
International disputes.
Cambodia is involved in a dispute regarding offshore islands and sections of the boundary with Vietnam. In addition, the maritime boundary Cambodia has with Vietnam is undefined. Parts of Cambodia's border with Thailand are indefinite, and the maritime boundary with Thailand is not clearly defined.
Illicit drugs.
Cambodia is a transshipment site for Golden Triangle heroin, and possibly a site of money laundering. Reportedly, there is corruption related to narcotics in parts of the government, military and police. Cambodia is also a possible site of small-scale opium, heroin, and amphetamine production. The country is a large producer of cannabis for the international market.

</doc>
