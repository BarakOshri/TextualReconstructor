<doc id="627" url="http://en.wikipedia.org/wiki?curid=627" title="Agriculture">
Agriculture

Agriculture is the cultivation of animals, plants, fungi, and other life forms for food, fiber, biofuel, medicinals and other products used to sustain and enhance human life. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that nurtured the development of civilization. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years, and its development has been driven and defined by greatly different climates, cultures, and technologies. However, all farming generally relies on techniques to expand and maintain the lands that are suitable for raising domesticated species. For plants, this usually requires some form of irrigation, although there are methods of dryland farming. Livestock are raised in a combination of grassland-based and landless systems, in an industry that covers almost one-third of the world's ice- and water-free area. In the developed world, industrial agriculture based on large-scale monoculture has become the dominant system of modern farming, although there is growing support for sustainable agriculture, including permaculture and organic agriculture.
Until the Industrial Revolution, the vast majority of the human population labored in agriculture. Pre-industrial agriculture was typically subsistence agriculture/self-sufficiency in which farmers raised most of their crops for their own consumption instead of cash crops for trade. A remarkable shift in agricultural practices has occurred over the past century in response to new technologies, and the development of world markets. This also has led to technological improvements in agricultural techniques, such as the Haber-Bosch method for synthesizing ammonium nitrate which made the traditional practice of recycling nutrients with crop rotation and animal manure less important.
Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological improvements have sharply increased yields from cultivation, but at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. Genetically modified organisms are an increasing component of agriculture, although they are banned in several countries. Agricultural food production and water management are increasingly becoming global issues that are fostering debate on a number of fronts. Significant degradation of land and water resources, including the depletion of aquifers, has been observed in recent decades, and the effects of global warming on agriculture and of agriculture on global warming are still not fully understood.
The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. Specific foods include cereals (grains), vegetables, fruits, oils, meats and spices. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are produced by plants, such as resins, dyes, drugs, perfumes, biofuels and ornamental products such as cut flowers and nursery plants. Over one third of the world's workers are employed in agriculture, second only to the services' sector, although the percentages of agricultural workers in developed countries has decreased significantly over the past several centuries.
Etymology and terminology.
The word "agriculture" is a late Middle English adaptation of Latin "agricultūra", from "ager", "field", and "cultūra", "cultivation" or "growing". Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle. To practice agriculture means to use natural resources to "produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services." This definition includes arable farming or agronomy, and horticulture, all terms for the growing of plants, animal husbandry and forestry. A distinction is sometimes made between forestry and agriculture, based on the former's longer management rotations, extensive versus intensive management practices and development mainly by nature, rather than by man. Even then, it is acknowledged that there is a large amount of knowledge transfer and overlap between silviculture (the management of forests) and agriculture. In traditional farming, the two are often combined even on small landholdings, leading to the term agroforestry.
History.
Agricultural practices such as irrigation, crop rotation, application of fertilizers and pesticides, and the domestication of livestock were developed long ago, but have made great progress in the past century. The history of agriculture has played a major role in human history, as agricultural progress has been a crucial factor in worldwide socio-economic change. Division of labour in agricultural societies made commonplace specializations rarely seen in hunter-gatherer cultures, which allowed the growth of towns and cities, and the complex societies we call civilizations. When farmers became capable of producing food beyond the needs of their own families, others in their society were free to devote themselves to projects other than food acquisition. Historians and anthropologists have long argued that the development of agriculture made civilization possible. According to geographer Jared Diamond, the costs of agriculture were: "the average daily number of work hours increased, nutrition deteriorated, infectious disease and body wear increased, and lifespan shortened."
Prehistoric origins.
Forest gardening, a plant-based food production system, is thought to be the world's oldest agroecosystem. Forest gardens originated in prehistoric times along jungle-clad river banks and in the wet foothills of monsoon regions. In the gradual process of a family improving their immediate environment, useful tree and vine species were identified, protected and improved whilst undesirable species were eliminated. Eventually superior foreign species were selected and incorporated into the family's garden.
Neolithic.
The Fertile Crescent of Western Asia first saw the domestication of animals, starting the Neolithic Revolution. Between 10,000 and 13,000 years ago, the ancestors of modern cattle, sheep, goats and pigs were domesticated in this area. The gradual transition from wild harvesting to deliberate cultivation happened independently in several areas around the globe. Agriculture allowed for the support of an increased population, leading to larger societies and eventually the development of cities. It also created the need for greater organization of political power (and the creation of social stratification), as decisions had to be made regarding labor and harvest allocation and access rights to water and land. Agriculture bred immobility, as populations settled down for long periods of time, which led to the accumulation of material goods.
Early Neolithic villages show evidence of the ability to process grain, and the Near East is the ancient home of the ancestors of wheat, barley and peas. There is evidence of the cultivation of figs in the Jordan Valley as long as 11,300 years ago, and cereal (grain) production in Syria approximately 9,000 years ago. During the same period, farmers in China began to farm rice and millet, using man-made floods and fires as part of their cultivation regimen. Fiber crops were domesticated as early as food crops, with China domesticating hemp, cotton being developed independently in Africa and South America, and the Near East domesticating flax. The use of soil amendments, including manure, fish, compost and ashes, appears to have begun early, and developed independently in several areas of the world, including Mesopotamia, the Nile Valley and Eastern Asia.
Squash was grown in Mexico nearly 10,000 years ago, while maize-like plants, derived from the wild teosinte, began to be seen at around 9,000 years ago. The derivation of teosinte into modern corn was slow, however, and it took until 5,500 to 6,000 years ago to turn into what we know today as maize. It then gradually spread across North America and was the major crop of Native Americans at the time of European exploration. Beans were domesticated around the same time, and together these three plants formed the Three Sisters nutritional foundation of many native populations in North and Central America. Combined with peppers, these crops provided a balanced diet for much of the continent. Grapes were first grown for wine approximately 8,000 years ago, in the Southern Caucasus, and by 3000 BC had spread to the Fertile Crescent, the Jordan Valley and Egypt.
Agriculture advanced to Europe slightly later, reaching the northeast of the continent from the east around 4000 BC. The idea that agriculture spread to Europe, rather than independently developing there, has led to two main hypotheses. The first is a "wave of advance", which holds that agriculture traveled slowly and steadily across the continent, while the second, "population pulse" theory, holds that it moved in jumps. Also around 6000 years ago, horses first began to be domesticated in the Eurasian steppes. Initially used for food, it was quickly discovered that they were useful for field work and carrying goods and people. Around 5,000 years ago, sunflowers were first cultivated in North America, while South America's Andes region was developing the potato. A minor center of domestication, the indigenous peoples of the eastern United States appear to have domesticated numerous crops, including tobacco.
Bronze and Iron Ages.
Beginning around 3000 BC, nomadic pastoralism, with societies focused on the care of livestock for subsistence, appeared independently in several areas in Europe and Asia. The main region was the steppes stretching from the Hungarian Plain to Manchuria, where cattle, sheep, horses, and to a lesser extent yaks and bactrian camels provided sustenance. The second was in Arabia, where one-humped camels were the main animal, with sheep, goats and horses also seen. The third area was a band of societies in areas of eastern and central Africa with a tropical savannah climate. Cattle and goats were found most often in this area, with smaller numbers of sheep, horses and camels. A fourth area, more minor than the others, was found in northern Europe and Asia and was focused on reindeer herding.
Between 2500 and 2000 BC, the simplest form of the plough, called the ard, spread throughout Europe, replacing the hoe. This change in equipment significantly increased cultivation ability, and affected the demand for land, as well as ideas about property, inheritance and family rights. Before this period, simple digging sticks or hoes were used. These tools would have also been easier to transport, which was a benefit as people only stayed until the soil's nutrients were depleted. However, as the continuous cultivating of smaller pieces of land became a sustaining practice throughout the world, ards were much more efficient than digging sticks. As humanity became more stationary, empires, such as the New Kingdom of Egypt and the Ancient Romans, arose, dependent upon agriculture to feed their growing populations, and slavery, which was used to provide the labor needed for continually intensifying agricultural processes. Agricultural technology continued to improve, allowing the expansion of available crop varieties, including a wide range of fruits, vegetables, oil crops, spices and other products. China was also an important center for agricultural technology development during this period. During the Zhou dynasty (1666–221 BC), the first canals were built, and irrigation was used extensively. The later Three Kingdoms and Northern and Southern dynasties (221–581 AD) brought the first biological pest control, extensive writings on agricultural topics and technological innovations such as steel and the wheelbarrow.
In the ancient world, fresh products, such as meats, dairy products and fresh fruits and vegetables, were likely consumed relatively close to where they were produced. Less perishable products, such as grains, preserved foods, olive oil and wine, were often traded over an extensive network of land and sea routes. The ancient trade in agricultural goods was well established, with wine traded in the Mediterranean region in the 6th century BC and Rome receiving extensive shipments of grain as tax payments by the 2nd century BC. Huge amounts of grain were transported, mainly by sea, and it was during this period that the subsidization of grain farming began, for the prevention of famine. Ancient Rome was a major center for agricultural trade. Trade routes stretched from Britain and Scandinavia in the west to India and China in the east, and included major crops, such as grain, wine and olive oil (also a fuel for oil lamps), as well as additional products, including spices, fabrics and drugs.
In Ancient Greece and Rome, many scholars documented farming techniques, including the use of fertilizers. Much of what was believed about farming and plant nutrition at this time was later found to be incorrect, but their theories provided the scientific foundation for the development of agricultural theories through the Middle Ages. Ideas about soil fertility and fertilization remained much the same from the time of Greco-Roman scholars until the 19th century, with correspondingly low crop yields. By the time of Alexander the Great's conquests (330–323 BC), the role of horses had developed, and they played a huge role in warfare and agriculture. Innovations continued to be developed which allowed them to work longer, harder and more efficiently. By medieval times they became the primary source of power for agriculture, transport and warfare, a position they held until the development of the steam and internal combustion engines. The Mayan culture developed several innovations in agriculture during its peak, which ranged from 400 BC to 900 AD and was heavily dependent upon agriculture to support its population. The Mayans used extensive canal and raised field systems to farm the large portions of swampland on the Yucatán Peninsula.
Middle Ages.
The Middle Ages saw significant improvements in the agricultural techniques and technology. During this time period, monasteries spread throughout Europe and became important centers for the collection of knowledge related to agriculture and forestry. The manorial system, which existed under different names throughout Europe and Asia, allowed large landowners significant control over both their land and its laborers, in the form of peasants or serfs. During the medieval period, the Arab world was critical in the exchange of crops and technology between the European, Asia and African continents. Besides transporting numerous crops, they introduced the concept of summer irrigation to Europe and developed the beginnings of the plantation system of sugarcane growing through the use of slaves for intensive cultivation. Population continued to increase along with land use. From 100 BC to 1600 AD, methane emissions, produced by domesticated animals and rice growing, increased substantially.
By 900 AD in Europe, developments in iron smelting allowed for increased production, leading to developments in the production of agricultural implements such as ploughs, hand tools and horse shoes. The plough was significantly improved, developing into the mouldboard plough, capable of turning over the heavy, wet soils of northern Europe. This led to the clearing of forests in that area and a significant increase in agricultural production, which in turn led to an increase in population. A similar plough, which may have developed independently, was also found in China as early as the 9th century. At the same time, farmers in Europe moved from a two field crop rotation to a three field crop rotation in which one field of three was left fallow every year. This resulted in increased productivity and nutrition, as the change in rotations led to different crops being planted, including legumes such as peas, lentils and beans. Inventions such as improved horse harnesses and the whippletree also changed methods of cultivation. Watermills were initially developed by the Romans, but were improved throughout the Middle Ages, along with windmills, and used to grind grains into flour, cut wood and process flax and wool, among other uses.
Crops included wheat, rye, barley and oats. Peas, beans, and vetches became common from the 13th century onward as a fodder crop for animals and also for their nitrogen-fixation fertilizing properties. Crop yields peaked in the 13th century, and stayed more or less steady until the 18th century. Though the limitations of medieval farming were once thought to have provided a ceiling for the population growth in the Middle Ages, recent studies have shown that the technology of medieval agriculture was always sufficient for the needs of the people under normal circumstances, and that it was only during exceptionally harsh times, such as the terrible weather of 1315–17, that the needs of the population could not be met. The Medieval Warm Period, between 900–1300 AD, brought generally warmer global temperatures, leading to increased harvests throughout Europe and a greater northern range for subtropical crops such as figs and olives. Greenland and Iceland were settled by Europeans during this period, and supported agricultural activities. The long-term warming period is generally thought to have occurred mainly in Europe, but other areas of the world experienced shorter warming periods at different times during this period, including China in the 11th and 12th centuries, with similar effects on agriculture. The climate variations found in Europe during the Medieval Warm Period returned to more moderate levels in the 15th century, and terminated in the Little Ice Age of the 16th-mid 19th centuries.
Global exchange.
After 1492, a global exchange of previously local crops and livestock breeds occurred. Key crops involved in this exchange included maize, potatoes, sweet potatoes and manioc traveling from the New World to the Old, and several varieties of wheat, barley, rice and turnips going from the Old World to the New. There were very few livestock species in the New World, with horses, cattle, sheep and goats being completely unknown before their arrival with Old World settlers. Crops moving in both directions across the Atlantic Ocean caused population growth around the world, and had a lasting effect on many cultures.
After its introduction from South America to Spain in the late 1500s, the potato became an important staple crop throughout Europe by the late 1700s. The potato allowed farmers to produce more food, and initially added variety to the European diet. The nutrition boost caused by increased potato consumption resulted in lower disease rates, higher birth rates and lower mortality rates, causing a population boom throughout the British Empire, the US and Europe. The introduction of the potato also brought about the first intensive use of fertilizer, in the form of guano imported to Europe from Peru, and the first artificial pesticide, in the form of an arsenic compound used to fight Colorado potato beetles. Before the adoption of the potato as a major crop, the dependence on grain caused repetitive regional and national famines when the crops failed: 17 major famines in England alone between 1523 and 1623. Although initially almost eliminating the danger of famine, the resulting dependence on the potato eventually caused the European Potato Failure, a disastrous crop failure from disease resulting in widespread famine, and the death of over one million people in Ireland alone.
Modern developments.
The British Agricultural Revolution, with its massive increases in agricultural productivity and net output, is a topic of ongoing debate among historians and agricultural scholars. The changes in agriculture in Britain between the 16th and 19th centuries would subsequently affect agriculture around the world. Major points of development included enclosure, mechanization, crop rotation and selective breeding. Prior to the 1960s, historians viewed the British Agricultural Revolution of having been "largely facilitated by a small number of key innovators," including Robert Bakewell, Thomas Coke and Charles Townshend. However, modern historians disperse much of the importance surrounding these individual men, and instead point to them holding a smaller position within a major societal shift regarding agriculture in Britain.
The agricultural changes, along with industrialization and migration, allowed the population of Britain, as well as other countries who followed its model, such as the US, Germany and Belgium, to escape from the Malthusian trap and increase both their population and their standard of living. It is estimated that the productivity of wheat in England went up from about 19 bushels per acre in 1720 to 21–22 bushels by the middle of the century and finally stabilized at around 30 bushels by 1840.
Premodern agriculture across Europe was characterized by the feudal open field system, where farmers worked on strips of land in fields that were held in common; this was inefficient and reduced the incentive to improve productivity. Many farms began to be enclosed by yeomen who improved the use of their land. This process of land reform accelerated in the 18th century with special acts of Parliament to expedite the legal process. The consolidation of large, privately owned holdings, encouraged the improvement of productivity through experimentation by enterprising landowners. By the 1750s, the market for agriculture was substantially commercialized - crop surpluses were routinely sold by the producers on the market or exported elsewhere.
These social changes were coupled with technical improvements. New methods of crop rotation and land use resulted in large additions to the amount of arable land. The four-field crop rotation was popularized by Charles Townshend in the 18th century. The system (wheat, turnips, barley and clover), opened up a fodder crop and grazing crop allowing livestock to be bred year-round. Yields of cereal crops increased as farmers utilized nitrogen-rich manure and nitrogen fixing-crops such as clover, increasing the available nitrogen in the soil and removing the limiting factor on cereal productions that had existed prior to the early 19th century. This improved production per farmer led to an increase in population and in the available workforce, creating the labor force needed for the Industrial Revolution.
The development of agriculture into its modern form was made possible through a continuing process of mechanization. Prior to this, basic agricultural tools had slowly been improved over centuries of use. The plough, for example, was a heavy implement with wheels in the 1500s. By the 1600s it was lighter, and by 1730, the Rotherham plough dramatically changed farming with no wheels, interchangeable parts, stronger construction and less weight. During the early 1800s, cast iron replaced wood for many parts, leading to longer-lasting implements. Seed drills had been under development since the early 1500s, but it was Jethro Tull's 1731 invention of a horse-drawn seed drill and horse hoe (a small plough to hoe between crop rows) that would eventually revolutionize planting in Britain, although they would not become popular until the early 1800s. Andrew Meikle patented the first practical threshing machine in 1784.
The Industrial Revolution caused a boom in international trade and shipping. Increased production caused a rise in the need for raw materials, with European merchants purchasing the majority of the goods. The value of goods traded worldwide increased by five times between 1750 and 1914, with annual shipping tonnages increasing from 4 million to 30 million tons between 1800 and 1900. In the second half of the 19th century, trade also expanded in the food (including grain and meat) and wool markets, and England (with the repeal of the Corn Laws in 1846) began to trade quantities of industrial products for wheat from around the world. The vast expansion of railroads that followed the invention of the steam engine further revolutionized world trade, especially in the Americas and East Asia, as goods could now be more easily traded across vast land distances. The developments of heat processing and refrigeration in the 19th century led to a similar revolution in the meat industry, as they allowed meat to be shipped long distances without spoiling. Countries in tropical locations, such as Australia and South America, were at the forefront of this effort.
In the mid-1800s, horse drawn machinery, such as the McCormick reaper, revolutionized harvesting, while inventions such as the cotton gin made possible the processing of large amounts of crops. During this same period, farmers began to use steam-powered threshers and tractors, although they were found to be expensive, dangerous and a fire hazard. The first gasoline-powered tractors were successfully developed around 1900, and in 1923, the International Harvester Farmall tractor became the first all-purpose tractor, and marked a major point in the replacement of draft animals (particularly horses) with machines. Since that time, self-propelled mechanical harvesters (combines), planters, transplanters and other equipment have been developed, further revolutionizing agriculture. These inventions allowed farming tasks to be done with a speed and on a scale previously impossible, leading modern farms to output much greater volumes of high-quality produce per land unit.
The scientific investigation of fertilization began at the Rothamsted Experimental Station in 1843 by John Bennet Lawes. He developed the first commercial process for fertilizer production - the obtaining of "phosphate" from the dissolution of coprolites in sulphuric acid. In 1909 the revolutionary Haber-Bosch method to synthesize ammonium nitrate was first demonstrated; it represented a major breakthrough and allowed crop yields to overcome previous constraints. In the years after World War II, the use of synthetic fertilizer increased rapidly, in sync with the increasing world population.
Recent.
Despite the tremendous gains in agricultural productivity, famines continued to sweep the globe through the 20th century. Through the effects of climatic events, government policy, war and crop failure, millions of people died in each of at least ten famines between the 1920s and the 1990s.
The Green Revolution refers to a series of research, development, and technology transfer initiatives, occurring between the 1940s and the late 1970s, that increased agriculture production around the world, beginning most markedly in the late 1960s. It involved the development of high-yielding varieties of cereal grains, expansion of irrigation infrastructure, modernization of management techniques, distribution of hybridized seeds, synthetic fertilizers, and pesticides to farmers. The initiatives, led by Norman Borlaug, the "Father of the Green Revolution", are credited with saving hundreds of millions of people from starvation. Demographer Thomas Malthus in 1798 famously predicted that the Earth would not be able to support its growing population, but technologies such as those promoted by the Green Revolution have thus far allowed the world to produce a surplus of food.
Although the Green Revolution significantly increased rice yields in Asia, yield increases have not occurred in the past 15–20 years. The genetic yield potential has increased for wheat, but the yield potential for rice has not increased since 1966, and the yield potential for maize has "barely increased in 35 years". It takes a decade or two for herbicide-resistant weeds to emerge, and insects become resistant to insecticides within about a decade. Crop rotation helps to prevent resistances.
The cereals rice, corn, and wheat provide 60% of human food supply. Between 1700 and 1980, "the total area of cultivated land worldwide increased 466%" and yields increased dramatically, particularly because of selectively bred high-yielding varieties, fertilizers, pesticides, irrigation, and machinery. However, concerns have been raised over the sustainability of intensive agriculture. Intensive agriculture has become associated with decreased soil quality in India and Asia, and there has been increased concern over the effects of fertilizers and pesticides on the environment, particularly as population increases and food demand expands. The monocultures typically used in intensive agriculture increase the number of pests, which are controlled through pesticides. Integrated pest management (IPM), which "has been promoted for decades and has had some notable successes" has not significantly affected the use of pesticides because policies encourage the use of pesticides and IPM is knowledge-intensive. In the 21st century, plants have been used to grow biofuels, pharmaceuticals (including biopharmaceuticals), and bioplastics.
Contemporary agriculture.
In the past century agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food.
In 2007, higher incentives for farmers to grow non-food biofuel crops combined with other factors, such as overdevelopment of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth, caused food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes. The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security. They in part base this on the experience of Vietnam, which went from a food importer to large food exporter and saw a significant drop in poverty, due mainly to the development of smallholder agriculture in the country.
Disease and land degradation are two of the major concerns in agriculture today. For example, an epidemic of stem rust on wheat caused by the Ug99 lineage is currently spreading across Africa and into Asia and is causing major concerns due to crop losses of 70% or more under some conditions. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.
In 2009, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States, according to the International Monetary Fund ("see below"). Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.
Workforce.
As of 2011, the International Labour Organization states that approximately one billion people, or over 1/3 of the available work force, are employed in the global agricultural sector. Agriculture constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry. The service sector only overtook the agricultural sector as the largest global employer in 2007. Between 1997 and 2007, the percentage of people employed in agriculture fell by over four percentage points, a trend that is expected to continue. The number of people employed in agriculture varies widely on a per-country basis, ranging from less than 2% in countries like the US and Canada to over 80% in many African nations. In developed countries, these figures are significantly lower than in previous centuries. During the 16th century in Europe, for example, between 55 and 75 percent of the population was engaged in agriculture, depending on the country. By the 19th century in Europe, this had dropped to between 35 and 65 percent. In the same countries today, the figure is less than 10%.
Safety.
Agriculture remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Common causes of fatal injuries among young farm workers include drowning, machinery and motor vehicle-related accidents.
The International Labour Organization considers agriculture "one of the most hazardous of all economic sectors." It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.
Agricultural production systems.
Crop cultivation systems.
Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.
Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.
Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.
In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual cropping is the dominant farming system.
Crop statistics.
Important categories of crops include cereals and pseudocereals, pulses (legumes), forage, and fruits and vegetables. Specific crops are cultivated in distinct growing regions throughout the world. In millions of metric tons, based on FAO estimate.
Livestock production systems.
Animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship.
Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. As of 2010, 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.
During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.
Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.
Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development(OECD) member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.
Production practices.
Tillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.
Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.
Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.
Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.
According to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
Crop alteration and biotechnology.
Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.
Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.
The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating "high-yielding varieties". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).
Genetic engineering.
Genetically Modified Organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import and/or use of GMO foods and crops, which have been put in place due to concerns over potential health issues, declining agricultural diversity and contamination of non-GMO crops. Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.
Herbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosates. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.
Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium "Bacillus thuringiensis" (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.
Environmental impact.
Agriculture imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, loss of natural environment and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society. In 2010, the International Resource Panel of the United Nations Environment Programme published a report assessing the environmental impacts of consumption and production. The study found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions.
Livestock issues.
A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2,) and 37% of all human-induced methane (which is 23 times as warming as CO2.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity.
Land and water issues.
Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39 to 50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).
Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.
Agriculture accounts for 70% of withdrawals of freshwater resources. Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.
Pesticides.
Pesticide use has increased since 1950 to 2.5 million tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that 3 million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the 'pesticide treadmill' in which pest resistance warrants the development of a new pesticide.
An alternative argument is that the way to 'save the environment' and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation.
Climate change.
Climate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO2, solar radiation and the interaction of these elements. Extreme events, such as droughts and floods, are forecast to increase as climate change takes hold. Agriculture is among sectors most vulnerable to the impacts of climate change; water supply for example, will be critical to sustain agricultural production and provide the increase in food output required to sustain the world's growing population. Fluctuations in the flow of rivers are likely to increase in the twenty-first century. Based on the experience of countries in the Nile river basin (Ethiopia, Kenya and Sudan) and other developing countries, depletion of water resources during seasons crucial for agriculture can lead to a decline in yield by up to 50%. Transformational approaches will be needed to manage natural resources in the future. For example, policies, practices and tools promoting climate-smart agriculture will be important, as will better use of scientific information on climate for assessing risks and vulnerability. Planners and policy-makers will need to help create suitable policies that encourage funding for such agricultural transformation.
Agriculture can both mitigate or worsen global warming. Some of the increase in CO2 in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddies, as well as the normal digestive activities of farm animals. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gases nitric oxide and nitrous oxide. Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO2 in the atmosphere.
There are several factors within the field of agriculture that contribute to the large amount of CO2 emissions. The diversity of the sources ranges from the production of farming tools to the transport of harvested produce. Approximately 8% of the national carbon footprint is due to agricultural sources. Of that, 75% is of the carbon emissions released from the production of crop assisting chemicals. Factories producing insecticides, herbicides, fungicides, and fertilizers are a major culprit of the greenhouse gas. Productivity on the farm itself and the use of machinery is another source of the carbon emission. Almost all the industrial machines used in modern farming are powered by fossil fuels. These instruments are burning fossil fuels from the beginning of the process to the end. Tractors are the root of this source. The tractor is going to burn fuel and release CO2 just to run. The amount of emissions from the machinery increase with the attachment of different units and need for more power. During the soil preparation stage tillers and plows will be used to disrupt the soil. During growth watering pumps and sprayers are used to keep the crops hydrated. And when the crops are ready for picking a forage or combine harvester is used. These types of machinery all require additional energy which leads to increased carbon dioxide emissions from the basic tractors. The final major contribution to CO2 emissions in agriculture is in the final transport of produce. Local farming suffered a decline over the past century due to large amounts of farm subsidies. The majority of crops are shipped hundreds of miles to various processing plants before ending up in the grocery store. These shipments are made using fossil fuel burning modes of transportation. Inevitably these transport adds to carbon dioxide emissions.
Sustainability.
Some major organizations are hailing farming within agroecosystems as the way forward for mainstream agriculture. Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. According to a report by the International Water Management Institute and UNEP, there is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. The report suggested assigning value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for
more productive farmland, or the preservation of a wetland system that limits fishing rights.
Technological advancements help provide farmers with tools and resources to make farming more sustainable. New technologies have given rise to innovations like conservation tillage, a farming process which helps prevent land loss to erosion, water pollution and enhances carbon sequestration.
According to a report by the International Food Policy Research Institute (IFPRI), agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
Agricultural economics.
Agricultural economics refers to economics as it relates to the "production, distribution and consumption of [agricultural] goods and services". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.
National government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures. Since at least the 1960s, a combination of import/export restrictions, exchange rate policies and subsidies have affected farmers in both the developing and developed world. In the 1980s, it was clear that non-subsidized farmers in developing countries were experiencing adverse affects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements were put into place that limited agricultural tariffs, subsidies and other trade restrictions.
However, as of 2009, there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities (i.e. corn, soybeans, cows) are generally graded to indicate quality. The quality affects the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.
Energy and agriculture.
Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960–65 measuring cycle and the cycle from 1986 to 1990, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages.
Modern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1) direct consumption on the farm and 2) indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gasoline, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1% of the nation's total energy.
Indirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002. The natural gas and coal consumed by the production of nitrogen fertilizer can account for over half of the agricultural energy usage. China utilizes mostly coal in the production of nitrogen fertilizer, while most of Europe uses large amounts of natural gas and small amounts of coal. According to a 2010 report published by The Royal Society, agriculture is increasingly dependent on the direct and indirect input of fossil fuels. Overall, the fuels used in agriculture vary based on several factors, including crop, production system and location. The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years. Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.
Mitigation of effects of petroleum shortages.
In the event of a petroleum shortage (see peak oil for global concerns), organic agriculture can be more attractive than conventional practices that use petroleum-based pesticides, herbicides, or fertilizers. Some studies using modern organic-farming methods have reported yields as high as those available from conventional farming. In the aftermath of the fall of the Soviet Union, with shortages of conventional petroleum-based inputs, Cuba made use of mostly organic practices, including biopesticides, plant-based pesticides and sustainable cropping practices, to feed its populace. However, organic farming may be more labor-intensive and would require a shift of the workforce from urban to rural areas. The reconditioning of soil to restore nutrients lost during the use of monoculture agriculture techniques also takes time.
It has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural "waste" to provide charcoal fertilizer, some fuel "and" food, instead of the normal food vs. fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion.
It has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops. The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds. While there has been some research on sustainability using GMO crops, at least one prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop.
Policy.
Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.
There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.

</doc>
<doc id="628" url="http://en.wikipedia.org/wiki?curid=628" title="Aldous Huxley">
Aldous Huxley

Aldous Leonard Huxley (26 July 1894 – 22 November 1963) was an English writer and a prominent member of the Huxley family. Best known for his novels including "Brave New World", set in a dystopian London, "The Doors of Perception", which recalls experiences when taking a psychedelic drug, and a wide-ranging output of essays, Huxley also edited the magazine "Oxford Poetry", and published short stories, poetry, travel writing, film stories and scripts. He spent the later part of his life in the United States, living in Los Angeles from 1937 until his death.
Huxley was a humanist, pacifist, and satirist. He became deeply concerned that humans might become subjugated through the sophisticated use of the mass media or mood-altering drugs, or tragically impacted by misunderstanding or the misapplication of increasingly sophisticated technology.
Huxley later became interested in spiritual subjects such as parapsychology and philosophical mysticism, in particular, Universalism. He is also well known for his use of psychedelic drugs. By the end of his life, Huxley was widely acknowledged as one of the pre-eminent intellectuals of his time.
Early life.
Aldous Huxley was born in Godalming, Surrey, England, in 1894. He was the third son of the writer and schoolmaster Leonard Huxley and his first wife, Julia Arnold, who founded Prior's Field School. Julia was the niece of poet and critic Matthew Arnold and the sister of Mrs. Humphrey Ward. Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic and controversialist ("Darwin's Bulldog"). His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists. Aldous had another brother, Noel Trevelyan Huxley (1891–1914), who committed suicide after a period of clinical depression.
Huxley began his learning in his father's well-equipped botanical laboratory, then went to Hillside School, Malvern. His teacher was his mother, who supervised him for several years until she became terminally ill. After Hillside, he was educated at Eton College. Huxley's mother died in 1908 when he was 14. In 1911, he suffered an illness (keratitis punctata) which "left [him] practically blind for two to three years". Aldous volunteered to join the army at the outbreak of the First World War, but was rejected on health grounds: he was half-blind in one eye. Once his eyesight recovered sufficiently, he was able to study English literature at Balliol College, Oxford. In 1916 he edited "Oxford Poetry" and later graduated (B.A.) with first class honours. His brother Julian wrote:
I believe his blindness was a blessing in disguise. For one thing, it put paid to his idea of taking up medicine as a career ... His uniqueness lay in his universalism. He was able to take all knowledge for his province.
Following his education at Balliol, Huxley was financially indebted to his father and had to earn a living. He taught French for a year at Eton, where Eric Blair (later to become George Orwell) and Steven Runciman were among his pupils, but was remembered as an incompetent and hopeless teacher who couldn't keep discipline. Nevertheless, Blair and others were impressed by his use of words. For a short while in 1918, he was employed acquiring provisions at the Air Ministry.
Significantly, Huxley also worked for a time in the 1920s at the technologically advanced Brunner and Mond chemical plant in Billingham, Teesside, and the most recent introduction to his famous science fiction novel "Brave New World" (1932) states that this experience of "an ordered universe in a world of planless incoherence" was one source for the novel.
Career.
Huxley completed his first (unpublished) novel at the age of 17 and began writing seriously in his early 20s. His first published novels were social satires, beginning with "Crome Yellow" (1921).
Bloomsbury Set.
During the First World War, Huxley spent much of his time at Garsington Manor, home of Lady Ottoline Morrell, working as a farm labourer. Here he met several Bloomsbury figures including Bertrand Russell, Alfred North Whitehead and Clive Bell. Later, in "Crome Yellow" (1921) he caricatured the Garsington lifestyle. Jobs were very scarce, but in 1919 John Middleton Murry was reorganising the "Athenaeum" and invited Huxley to join the staff. He accepted immediately, and quickly married the Belgian refugee Maria Nys, also at Garsington. They lived with their young son in Italy part of the time in the 1920s, where Huxley would visit his friend D. H. Lawrence. Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1933).
Works of this period included important novels on the dehumanising aspects of scientific progress, most famously "Brave New World", and on pacifist themes (for example, "Eyeless in Gaza"). In "Brave New World" Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning. Huxley was strongly influenced by F. Matthias Alexander and included him as a character in "Eyeless in Gaza".
Starting from this period, Huxley began to write and edit non-fiction works on pacifist issues, including "Ends and Means", "An Encyclopedia of Pacifism", and "Pacifism and Philosophy", and was an active member of the Peace Pledge Union.
United States.
In 1937, Huxley moved to Hollywood, with his wife Maria, son Matthew, and friend Gerald Heard. He lived in the US, mainly in southern California, until his death, but also for a time in Taos, New Mexico, where he wrote "Ends and Means" (published in 1937). In this work he examines the fact that although most people in modern civilisation agree that they want a world of "liberty, peace, justice, and brotherly love", they have not been able to agree on how to achieve it.
Heard introduced Huxley to Vedanta (Upanishad-centered philosophy), meditation, and vegetarianism through the principle of ahimsa. In 1938, Huxley befriended J. Krishnamurti, whose teachings he greatly admired. He also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle. Not long after, Huxley wrote his book on widely held spiritual values and ideas, "The Perennial Philosophy", which discussed the teachings of renowned mystics of the world. Huxley's book affirmed a sensibility that insists there are realities beyond the generally accepted "five senses" and that there is genuine meaning for humans beyond both sensual satisfactions and sentimentalities.
Huxley became a close friend of Remsen Bird, president of Occidental College. He spent much time at the college, which is in the Eagle Rock neighbourhood of Los Angeles. The college appears as "Tarzana College" in his satirical novel "After Many a Summer" (1939). The novel won Huxley that year's James Tait Black Memorial Prize for fiction. Huxley also incorporated Bird into the novel.
During this period, Huxley earned some income as a Hollywood writer. In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for "Madame Curie" which was originally to star Greta Garbo and be directed by George Cukor. (The film was eventually completed by MGM in 1943 with a different director and cast.) Huxley received screen credit for "Pride and Prejudice" (1940) and was paid for his work on a number of other films, including "Jane Eyre" (1944).
However, his success in Hollywood was minimal. When he wrote a synopsis of "Alice in Wonderland", Walt Disney rejected it on the grounds that "he could only understand every third word". Huxley's leisurely development of ideas, it seemed, was not suitable for the movie moguls, who demanded fast, dynamic dialogue above all else. For Dick Huemer, during the 1940s, Huxley went to the first of a five meetings' session to elaborate the script of "Alice in Wonderland" but never came again. For author John Grant, although the movie's character the Caterpillar displays some characteristics familiar from Huxley's discussion of his experiments with hallucinogens, Huxley's contribution to the movie is nonexistent.
Huxley wrote an introduction to the posthumous publication of J.D. Unwin's 1940 book "Hopousia or The Sexual and Economic Foundations of a New Society".
On 21 October 1949, Huxley wrote to George Orwell, author of "Nineteen Eighty-Four", congratulating him on "how fine and how profoundly important the book is". In his letter to Orwell, he predicted:
Within the next generation I believe that the world's leaders will discover that infant conditioning and narco-hypnosis are more efficient, as instruments of government, than clubs and prisons, and that the lust for power can be just as completely satisfied by suggesting people into loving their servitude as by flogging them and kicking them into obedience.
Huxley had deeply felt apprehensions about the future the developed world might make for itself. From these, he made some warnings in his writings and talks. In a 1958 televised interview conducted by journalist Mike Wallace, Huxley outlined several major concerns: the difficulties and dangers of world overpopulation; the tendency toward distinctly hierarchical social organisation; the crucial importance of evaluating the use of technology in mass societies susceptible to wily persuasion; the tendency to promote modern politicians, to a naive public, as well-marketed commodities.
Post World War II.
After the Second World War, Huxley applied for United States citizenship. His application was continuously deferred on the grounds that he would not say he would take up arms to defend the U.S. He claimed a philosophical, rather than a religious objection, and therefore was not exempt under the McCarran Act. He withdrew his application. Nevertheless, he remained in the country; and in 1959 he turned down an offer of a Knight Bachelor by the Macmillan government. During the 1950s, Huxley's interest in the field of psychical research grew keener, and his later works are strongly influenced by both mysticism and his experiences with psychedelic drugs.
In October 1930, the English occultist Aleister Crowley had dined with Huxley in Berlin, and to this day rumours persist that Crowley introduced Huxley to peyote on that occasion. He was introduced to mescaline (the key active ingredient of peyote) by the psychiatrist Humphry Osmond in 1953, taking it for the first time in the afternoon of 5 May. Through Dr. Osmond, Huxley met millionaire Alfred Matthew Hubbard, who was by this point introducing creative and influential people to LSD on a wide-ranging basis. On 24 December 1955, Huxley took his first dose of LSD. Indeed, Huxley was a pioneer of self-directed psychedelic drug use "in a search for enlightenment". According to a letter written by his wife Laura, Huxley requested and received two intramuscular injections of 100 micrograms of LSD as he lay dying. His psychedelic drug experiences are described in the essays "The Doors of Perception" (the title deriving from some lines in the book "The Marriage of Heaven and Hell" by William Blake), and "Heaven and Hell". Some of his writings on psychedelics became frequent reading among early hippies. While living in Los Angeles, Huxley was a friend of Ray Bradbury. According to Sam Weller's biography of Bradbury, the latter was dissatisfied with Huxley, especially after Huxley encouraged Bradbury to take psychedelic drugs.
Association with Vedanta.
Beginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda. Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices.
In 1944, Huxley wrote the introduction to the "Bhagavad Gita: The Song of God", translated by Swami Prabhavanada and Christopher Isherwood, which was published by The Vedanta Society of Southern California.
From 1941 until 1960, Huxley contributed 48 articles to "Vedanta and the West", published by the Society. He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962.
Huxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples. Two of those lectures have been released on CD: "Knowledge and Understanding" and "Who Are We" from 1955.
After the publication of "The Doors of Perception", Huxley and the Swami disagreed about the meaning and importance of the LSD drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the Society's journal, lecture at the temple, and attend social functions. His agnosticism, together with his speculative propensity, made it difficult for him to fully embrace any form of institutionalized religion.
Eyesight.
There are differing accounts about the details of the quality of Huxley's eyesight at specific points in his life. Around 1939, Huxley encountered the Bates Method for better eyesight, and a teacher, Margaret Corbett, who was able to teach him in the method. In 1940, Huxley relocated from Hollywood to a "ranchito" in the high desert hamlet of Llano, California, in northernmost Los Angeles County. Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert. He reported that for the first time in over 25 years, he was able to read without glasses and without strain. He even tried driving a car along the dirt road beside the ranch. He wrote a book about his successes with the Bates Method, "The Art of Seeing", which was published in 1942 (US), 1943 (UK). The book contained some generally disputed theories, and its publication created a growing degree of popular controversy about Huxley's eyesight.
It was, and is, widely believed that Huxley was nearly blind since the illness in his teens, despite the partial recovery which had enabled him to study at Oxford. For example, some ten years after publication of "The Art of Seeing", in 1952, Bennett Cerf was present when Huxley spoke at a Hollywood banquet, wearing no glasses and apparently reading his paper from the lectern without difficulty: "Then suddenly he faltered—and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonising moment."
On the other hand, Huxley's second wife, Laura Archera Huxley, would later emphasise in her biographical account, "This Timeless Moment": "One of the great achievements of his life: that of having regained his sight". After revealing a letter she wrote to the "Los Angeles Times" disclaiming the label of Huxley as a "poor fellow who can hardly see" by Walter C. Alvarez, she tempered this: "Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision. For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens." Laura Huxley proceeded to elaborate a few nuances of inconsistency peculiar to Huxley's vision. Her account, in this respect, is discernibly congruent with the following sample of Huxley's own words from "The Art of Seeing". "The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable". Nevertheless, the topic of Huxley's eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.
American popular science author Steven Johnson, in his book "Mind Wide Open", quotes Huxley about his difficulties with visual encoding: "I am and, for as long as I can remember, I have always been a poor visualizer. Words, even the pregnant words of poets, do not evoke pictures in my mind. No hypnagogic visions greet me on the verge of sleep. When I recall something, the memory does not present itself to me as a vividly seen event or object. By an effort of the will, I can evoke a not very vivid image of what happened yesterday afternoon..." (Huxley, "The Doors of Perception and Heaven and Hell", HarperPerennial, 1963, p. 15.)
Personal life.
Huxley married Maria Nys (10 September 1899 – 12 February 1955), a Belgian he met at Garsington, in 1919. They had one child, Matthew Huxley (19 April 1920 – 10 February 2005), who had a career as an author, anthropologist, and prominent epidemiologist.
In 1956, Huxley married Laura Archera (1911–2007), also an author. She wrote "This Timeless Moment", a biography of Huxley. Laura felt inspired to illuminate the story of their provocative marriage through Mary Ann Braubach's 2010 documentary, "Huxley on Huxley".
In 1960, Huxley was diagnosed with laryngeal cancer and, in the years that followed, with his health deteriorating, he wrote the Utopian novel "Island", and gave lectures on "Human Potentialities" at the Esalen Institute, which were fundamental to the forming of the Human Potential Movement.
Despite his interest in spirituality and mysticism, Huxley called himself an agnostic.
Huxley was a close friend of Jiddu Krishnamurti and Rosalind Rajagopal and was involved in the creation of the Happy Valley School (now Besant Hill School of Happy Valley) in Ojai, California.
The most substantial collection of Huxley's few remaining papers (following the destruction of most in a fire) is at the Library of the University of California, Los Angeles. Some are also at the Stanford University Library.
Death.
On his deathbed, unable to speak, Huxley made a written request to his wife Laura for "LSD, 100 µg, intramuscular". According to her account of his death in "This Timeless Moment", she obliged with an injection at 11:45 am and a second dose a few hours later; Huxley died aged 69, at 5:20 pm on 22 November 1963.
Media coverage of Huxley's passing – as with that of the author C. S. Lewis – was overshadowed by the assassination of President John F. Kennedy on the same day. This coincidence was the inspiration for Peter Kreeft's book "Between Heaven and Hell: A Dialog Somewhere Beyond Death with John F. Kennedy, C. S. Lewis, & Aldous Huxley".
Huxley's ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, a village near Guildford, Surrey, England. On 26 July 2013 a commemorative bench was unveiled there, donated by the Aldous and Laura Huxley Literary Trust and the International Aldous Huxley Society.
Huxley had been a long-time friend of famous Russian composer Igor Stravinsky, who later dedicated his last orchestral composition to Huxley. Stravinsky began "Variations" in Santa Fé, New Mexico in July 1963, and completed the composition in Hollywood on 28 October 1964. It was first performed in Chicago on 17 April 1965, by the Chicago Symphony Orchestra conducted by Robert Craft (Spies 1965, 62; White 1979, 534)(White 1979, 536–37).
Huxley's literary legacy continues to be represented by the literary agency headed by Georges Borchardt.

</doc>
<doc id="630" url="http://en.wikipedia.org/wiki?curid=630" title="Ada">
Ada

Ada, ADA, or A.D.A. may refer to:

</doc>
<doc id="632" url="http://en.wikipedia.org/wiki?curid=632" title="Aberdeen (disambiguation)">
Aberdeen (disambiguation)

Aberdeen is a city in Scotland, United Kingdom.
Aberdeen may also refer to:

</doc>
<doc id="633" url="http://en.wikipedia.org/wiki?curid=633" title="Algae">
Algae

Algae ( or ; singular "alga" , Latin for "seaweed") are a very large and diverse group of eukaryotic organisms, ranging from unicellular genera such as "Chlorella" and the diatoms to multicellular forms such as the giant kelp, a large brown alga that may grow up to 50 meters in length. Most are autotrophic and lack many of the distinct cell and tissue types found in land plants such as stomata, xylem and phloem. The largest and most complex marine algae are called seaweeds, while the most complex freshwater forms are the Charophyta, a division of algae that includes "Spirogyra" and the stoneworts.
There is no generally accepted definition of algae. One definition is that algae "have chlorophyll as their primary photosynthetic pigment and lack a sterile covering of cells around their reproductive cells". Other authors exclude all prokaryotes and thus do not consider cyanobacteria (blue-green algae) as algae.
Algae constitute a polyphyletic group since they do not include a common ancestor, and although their plastids seem to have a single origin, from cyanobacteria, they were acquired in different ways. Green algae are examples of algae that have primary chloroplasts derived from endosymbiotic cyanobacteria. Diatoms are examples of algae with secondary chloroplasts derived from an endosymbiotic red alga.
Algae exhibit a wide range of reproductive strategies, from simple asexual cell division to complex forms of sexual reproduction.
Algae lack the various structures that characterize land plants, such as the phyllids (leaf-like structures) of bryophytes, rhizoids in nonvascular plants, and the roots, leaves, and other organs that are found in tracheophytes (vascular plants). Most are phototrophic, although some groups contain members that are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy. Some unicellular species such as the green algae "Prototheca" and "Helicosporidium", and many euglenids, dinoflagellates and other algae, have become heterotrophs (also called colorless or apochlorotic algae), sometimes parasitic, relying entirely on external energy sources and have limited or no photosynthetic apparatus. Some other heterotrophic organisms, like the apicomplexans, are also derived from cells whose ancestors possessed plastids, but are not traditionally considered as algae. Algae have photosynthetic machinery ultimately derived from cyanobacteria that produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria. Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago.
Etymology and study.
 The singular "alga" is the Latin word for a particular seaweed and retains that meaning in English. The etymology is obscure. Although some speculate that it is related to Latin "algēre", "be cold", there is no known reason to associate seaweed with temperature. A more likely source is "alliga", "binding, entwining."
The Ancient Greek word for seaweed was "φῦκος" (fūkos or phykos), which could mean either the seaweed (probably red algae) or a red dye derived from it. The Latinization, "fūcus", meant primarily the cosmetic rouge. The etymology is uncertain, but a strong candidate has long been some word related to the Biblical "פוך" (pūk), "paint" (if not that word itself), a cosmetic eye-shadow used by the ancient Egyptians and other inhabitants of the eastern Mediterranean. It could be any color: black, red, green, blue.
Accordingly the modern study of marine and freshwater, algae is called either phycology or algology, depending on whether the Greek or Latin root is used. The name "Fucus" appears in a number of taxa.
Classification.
Most algae contain chloroplasts that are similar in structure to cyanobacteria. Chloroplasts contain circular DNA like that in cyanobacteria and presumably represent reduced endosymbiotic cyanobacteria. However, the exact origin of the chloroplasts is different among separate lineages of algae, reflecting their acquisition during different endosymbiotic events. The table below describes the composition of the three major groups of algae. Their lineage relationships are shown in the figure in the upper right. Many of these groups contain some members that are no longer photosynthetic. Some retain plastids, but not chloroplasts, while others have lost plastids entirely.
Phylogeny based on plastid not nucleocytoplasmic genealogy:
Linnaeus, in Species Plantarum (1753), the starting point for modern botanical nomenclature, recognized 14 genera of algae, of which only 4 are currently considered among algae. In "Systema Naturae", Linnaeus described the genera "Volvox" and "Corallina", among the animals.
W.H.Harvey (1811—1866) and Lamouroux (1813) were the first to divide macroscopic algae into four divisions based on their pigmentation. This is the first use of a biochemical criterion in plant systematics. Harvey's four divisions are: red algae (Rhodophyta), brown algae (Heteromontophyta), green algae (Chlorophyta) and Diatomaceae.
At this time, microscopic algae were discovered and reported by a different group of workers (e.g., O. F. Müller and Ehrenberg) studying the Infusoria (microscopic organisms). Unlike macroalgae, which were clearly viewed as plants, microalgae were frequently considered animals because they are often motile.
Although used as a taxonomic category in some pre-Darwinian classifications, e.g., Linnaeus (1753), de Jussieu (1789), Horaninow (1843), Agassiz (1859), Wilson & Cassin (1864), in further classifications, the "algae" are seen as an artificial, polyphyletic group.
Relationship to higher plants.
The first plants on earth probably evolved from shallow freshwater charophyte algae much like "Chara" almost 500 million years ago. These probably had an isomorphic alternation of generations and were probably filamentous. Fossils of isolated land plant spores suggest land plants may have been around as long as 475 million years ago.
Morphology.
A range of algal morphologies are exhibited, and convergence of features in unrelated groups is common. The only groups to exhibit three-dimensional multicellular thalli are the reds and browns, and some chlorophytes. Apical growth is constrained to subsets of these groups: the florideophyte reds, various browns, and the charophytes. The form of charophytes is quite different from those of reds and browns, because they have distinct nodes, separated by internode 'stems'; whorls of branches reminiscent of the horsetails occur at the nodes. Conceptacles are another polyphyletic trait; they appear in the coralline algae and the Hildenbrandiales, as well as the browns.
Most of the simpler algae are unicellular flagellates or amoeboids, but colonial and non-motile forms have developed independently among several of the groups. Some of the more common organizational levels, more than one of which may occur in the life cycle of a species, are
In three lines even higher levels of organization have been reached, with full tissue differentiation. These are the brown algae,—some of which may reach 50 m in length (kelps)—the red algae, and the green algae. The most complex forms are found among the green algae (see Charales and Charophyta), in a lineage that eventually led to the higher land plants. The point where these non-algal plants begin and algae stop is usually taken to be the presence of reproductive organs with protective cell layers, a characteristic not found in the other alga groups.
Physiology.
Many algae, particularly members of the Characeae, have served as model experimental organisms to understand the mechanisms of the water permeability of membranes, osmoregulation, turgor regulation, salt tolerance, cytoplasmic streaming, and the generation of action potentials.
Phytohormones are found not only in higher plants, but in algae too.
Symbiotic algae.
Some species of algae form symbiotic relationships with other organisms. In these symbioses, the algae supply photosynthates (organic substances) to the host organism providing protection to the algal cells. The host organism derives some or all of its energy requirements from the algae. Examples are as follows.
Lichens.
"Lichens" are defined by the International Association for Lichenology to be "an association of a fungus and a photosynthetic symbiont resulting in a stable vegetative body having a specific structure." The fungi, or mycobionts, are mainly from the Ascomycota with a few from the Basidiomycota. They are not found alone in nature but when they began to associate is not known. One mycobiont associates with the same phycobiont species, rarely two, from the green algae, except that alternatively the mycobiont may associate with a species of cyanobacteria (hence "photobiont" is the more accurate term). A photobiont may be associated with many different mycobionts or may live independently; accordingly, lichens are named and classified as fungal species. The association is termed a morphogenesis because the lichen has a form and capabilities not possessed by the symbiont species alone (they can be experimentally isolated). It is possible that the photobiont triggers otherwise latent genes in the mycobiont.
Coral reefs.
 Coral reefs are accumulated from the calcareous exoskeletons of marine invertebrates of the order Scleractinia (stony corals). As animals they metabolize sugar and oxygen to obtain energy for their cell-building processes, including secretion of the exoskeleton, with water and carbon dioxide as byproducts. As the reef is the result of a favorable equilibrium between construction by the corals and destruction by marine erosion, the rate at which metabolism can proceed determines the growth or deterioration of the reef.
Dinoflagellates (algal protists) are often endosymbionts in the cells of marine invertebrates, where they accelerate host-cell metabolism by generating immediately available sugar and oxygen through photosynthesis using incident light and the carbon dioxide produced by the host. Stony corals that are reef-building corals (hermatypic corals) require endosymbiotic algae from the genus "Symbiodinium" to be in a healthy condition. The loss of "Symbiodinium" from the host is known as coral bleaching, a condition which leads to the deterioration of a reef.
Sea sponges.
Green algae live close to the surface of some sponges, for example, breadcrumb sponge ("Halichondria panicea"). The alga is thus protected from predators; the sponge is provided with oxygen and sugars which can account for 50 to 80% of sponge growth in some species.
Life-cycle.
Rhodophyta, Chlorophyta and Heterokontophyta, the three main algal phyla, have life-cycles which show tremendous variation with considerable complexity. In general there is an asexual phase where the seaweed's cells are diploid, a sexual phase where the cells are haploid followed by fusion of the male and female gametes. Asexual reproduction is advantageous in that it permits efficient population increases, but less variation is possible. Sexual reproduction allows more variation, but is more costly. Often there is no strict alternation between the sporophyte and also because there is often an asexual phase, which could include the fragmentation of the thallus.
Numbers.
The "Algal Collection of the US National Herbarium" (located in the National Museum of Natural History) consists of approximately 320,500 dried specimens, which, although not exhaustive (no exhaustive collection exists), gives an idea of the order of magnitude of the number of algal species (that number remains unknown). Estimates vary widely. For example, according to one standard textbook, in the British Isles the "UK Biodiversity Steering Group Report" estimated there to be 20000 algal species in the UK. Another checklist reports only about 5000 species. Regarding the difference of about 15000 species, the text concludes: "It will require many detailed field surveys before it is possible to provide a reliable estimate of the total number of species ..."
Regional and group estimates have been made as well:
and so on, but lacking any scientific basis or reliable sources, these numbers have no more credibility than the British ones mentioned above. Most estimates also omit microscopic algae, such as phytoplankton.
The most recent estimate suggests a total number of 72,500 algal species worldwide.
Distribution.
The distribution of algal species has been fairly well studied since the founding of phytogeography in the mid-19th century AD. Algae spread mainly by the dispersal of spores analogously to the dispersal of Plantae by seeds and spores. Spores are everywhere in all parts of the Earth: the waters fresh and marine, the atmosphere, free-floating and in precipitation or mixed with dust, the humus and in other organisms, such as humans. Whether a spore is to grow into an organism depends on the combination of the species and the environmental conditions of where the spore lands.
The spores of fresh-water algae are dispersed mainly by running water and wind, as well as by living carriers. The bodies of water into which they are transported are chemically selective. Marine spores are spread by currents. Ocean water is temperature selective, resulting in phytogeographic zones, regions and provinces.
To some degree the distribution of algae is subject to floristic discontinuities caused by geographical features, such as Antarctica, long distances of ocean or general land masses. It is therefore possible to identify species occurring by locality, such as "Pacific Algae" or "North Sea Algae". When they occur out of their localities, it is usually possible to hypothesize a transport mechanism, such as the hulls of ships. For example, "Ulva reticulata" and "Ulva fasciata" travelled from the mainland to Hawaii in this manner.
Mapping is possible for select species only: "there are many valid examples of confined distribution patterns." For example, "Clathromorphum" is an arctic genus and is not mapped far south of there. On the other hand, scientists regard the overall data as insufficient due to the "difficulties of undertaking such studies."
Locations.
Algae are prominent in bodies of water, common in terrestrial environments and are found in unusual environments, such as on snow and on ice. Seaweeds grow mostly in shallow marine waters, under ; however some have been recorded to a depth of .
The various sorts of algae play significant roles in aquatic ecology. Microscopic forms that live suspended in the water column (phytoplankton) provide the food base for most marine food chains. In very high densities (algal blooms) these algae may discolor the water and outcompete, poison, or asphyxiate other life forms.
Algae are variously sensitive to different factors, which has made them useful as biological indicators in the Ballantine Scale and its modification.
Cultural associations.
In Classical Chinese, the word is used both for "algae" and (in the modest tradition of the imperial scholars) for "literary talent". The third island in Kunming Lake beside the Summer Palace in Beijing is known as the Zaojian Tang Dao which thus simultaneously means "Island of the Algae-Viewing Hall" and "Island of the Hall for Reflecting on Literary Talent".
Uses.
Agar.
Agar, a gelatinous substance derived from red algae, has a number of commercial uses. It is a good medium on which to grow bacteria and fungi as most microorganisms cannot digest agar.
Alginates.
Alginic acid, or alginate, is extracted from brown algae. Its uses range from gelling agents in food, to medical dressings. Alginic acid also has been used in the field of biotechnology as a biocompatible medium for cell encapsulation and cell immobilization. Molecular cuisine is also a user of the substance for its gelling properties, by which it becomes a delivery vehicle for flavours.
Between 100,000 and 170,000 wet tons of "Macrocystis" are harvested annually in New Mexico for alginate extraction and abalone feed.
Energy source.
To be competitive and independent from fluctuating support from (local) policy on the long run, biofuels should equal or beat the cost level of fossil fuels. Here, algae based fuels hold great promise, directly related to the potential to produce more biomass per unit area in a year than any other form of biomass. The break-even point for algae-based biofuels is estimated to occur by 2025.
Fertilizer.
For centuries seaweed has been used as a fertilizer; George Owen of Henllys writing in the 16th century referring to drift weed in South Wales:This kind of ore they often gather and lay on great heapes, where it heteth and rotteth, and will have a strong and loathsome smell; when being so rotten they cast on the land, as they do their muck, and thereof springeth good corn, especially barley ... After spring-tydes or great rigs of the sea, they fetch it in sacks on horse backes, and carie the same three, four, or five miles, and cast it on the lande, which doth very much better the ground for corn and grass.
Today, algae are used by humans in many ways; for example, as fertilizers, soil conditioners and livestock feed. Aquatic and microscopic species are cultured in clear tanks or ponds and are either harvested or used to treat effluents pumped through the ponds. Algaculture on a large scale is an important type of aquaculture in some places. Maerl is commonly used as a soil conditioner.
Nutrition.
Naturally growing seaweeds are an important source of food, especially in Asia. They provide many vitamins including: A, B1, B2, B6, niacin and C, and are rich in iodine, potassium, iron, magnesium and calcium. In addition commercially cultivated microalgae, including both algae and cyanobacteria, are marketed as nutritional supplements, such as Spirulina, Chlorella and the Vitamin-C supplement, Dunaliella, high in beta-carotene.
Algae are national foods of many nations: China consumes more than 70 species, including "fat choy", a cyanobacterium considered a vegetable; Japan, over 20 species; Ireland, dulse; Chile, cochayuyo. Laver is used to make "laver bread" in Wales where it is known as "bara lawr"; in Korea, gim; in Japan, nori and aonori. It is also used along the west coast of North America from California to British Columbia, in Hawaii and by the Māori of New Zealand. Sea lettuce and badderlocks are a salad ingredient in Scotland, Ireland, Greenland and Iceland.
The oils from some algae have high levels of unsaturated fatty acids. For example, "Parietochloris incisa" is very high in arachidonic acid, where it reaches up to 47% of the triglyceride pool. Some varieties of algae favored by vegetarianism and veganism contain the long-chain, essential omega-3 fatty acids, Docosahexaenoic acid (DHA) and Eicosapentaenoic acid (EPA). Fish oil contains the omega-3 fatty acids, but the original source is algae (microalgae in particular), which are eaten by marine life such as copepods and are passed up the food chain. Algae has emerged in recent years as a popular source of omega-3 fatty acids for vegetarians who cannot get long-chain EPA and DHA from other vegetarian sources such as flaxseed oil, which only contains the short-chain Alpha-Linolenic acid (ALA).
Pollution control.
Agricultural Research Service scientists found that 60–90% of nitrogen runoff and 70–100% of phosphorus runoff can be captured from manure effluents using a horizontal algae scrubber, also called an algal turf scrubber (ATS). Scientists developed the ATS, which are shallow, 100-foot raceways of nylon netting where algae colonies can form, and studied its efficacy for three years. They found that algae can readily be used to reduce the nutrient runoff from agricultural fields and increase the quality of water flowing into rivers, streams, and oceans. The enriched algae itself also can be used as a fertilizer. Researchers collected and dried the nutrient-rich algae from the ATS and studied its potential as an organic fertilizer. They found that cucumber and corn seedlings grew just as well using ATS organic fertilizer as they did with commercial fertilizers. Algae scrubbers, using bubbling upflow or vertical waterfall versions, are now also being used to filter aquariums and ponds.
Bioremediation.
The algae Stichococcus bacillaris, has been seen to colonize silicone resins used at archaeological sites; biodeterriorating the synthetic substance.
Pigments.
The natural pigments (carotenoids and chlorophylls) produced by algae can be used as an alternative to chemical dyes and coloring agents.
Presence of some individual alga pigments, together with specific pigment concentrations ratios, are taxon-specific: analysis of their concentrations with various analytical methods, particularly HPLC, can therefore offer deep insight into the taxonomic composition and relative abundance of natural alga populations in sea water samples.
Stabilizing substances.
Carrageenan, from the red alga "Chondrus crispus", is used as a stabilizer in milk products.

</doc>
<doc id="634" url="http://en.wikipedia.org/wiki?curid=634" title="Analysis of variance">
Analysis of variance

Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences between group means and their associated procedures (such as "variation" among and between groups), developed by R.A. Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the "t"-test to more than two groups. As doing multiple two-sample t-tests would result in an increased chance of committing a statistical type I error, ANOVAs are useful in comparing (testing) three or more means (groups or variables) for statistical significance.
Motivating example.
The analysis of variance can be used as an exploratory tool to explain observations. A dog show provides an example. A dog show is not a random sampling of the breed: it is typically limited to dogs that are male, adult, pure-bred, and exemplary. A histogram of dog weights from a show might plausibly be rather complex, like the yellow-orange distribution shown in the illustrations. Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog. Before we could do that, we would need to "explain" the distribution of weights by dividing the dog population into groups based on those characteristics. A successful grouping will split dogs such that a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and b) the mean of each group is distinct (if two groups have the same mean, then it isn't reasonable to conclude that the groups are, in fact, separate in any meaningful way).
In the illustrations to the right, each group is identified as "X"1, "X"2, etc. In the first illustration, we divide the dogs according to the product (interaction) of two binary groupings: young vs old, and short-haired vs long-haired (thus, group 1 is young, short-haired dogs, group 2 is young, long-haired dogs, etc.). Since the distributions of dog weight within each of the groups (shown in blue) has a large variance, and since the means are very close across groups, grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights: knowing which group a dog is in does not allow us to make any reasonable statements as to what that dog's weight is likely to be. Thus, this grouping fails to "fit" the distribution we are trying to explain (yellow-orange).
An attempt to explain the weight distribution by grouping dogs as (pet vs working breed) and (less athletic vs more athletic) would probably be somewhat more successful (fair fit). The heaviest show dogs are likely to be big strong working breeds, while breeds kept as pets tend to be smaller and thus lighter. As shown by the second illustration, the distributions have variances that are considerably smaller than in the first case, and the means are more reasonably distinguishable. However, the significant overlap of distributions, for example, means that we cannot reliably say that "X"1 and "X"2 are truly distinct (i.e., it is perhaps reasonably likely that splitting dogs according to the flip of a coin—by pure chance—might produce distributions that look similar).
An attempt to explain weight by breed is likely to produce a very good fit. All Chihuahuas are light and all St Bernards are heavy. The difference in weights between Setters and Pointers does not justify separate breeds. The analysis of variance provides the formal tools to justify these intuitive judgments. A common use of the method is the analysis of experimental data or the development of models. The method has some advantages over correlation: not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship.
Background and terminology.
ANOVA is a particular form of statistical hypothesis testing heavily used in the analysis of experimental data. A statistical hypothesis test is a method of making decisions using data. A test result (calculated from the null hypothesis and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, "assuming the truth of the null hypothesis". A statistically significant result, when a probability (p-value) is less than a threshold (significance level), justifies the rejection of the null hypothesis, but only if the a priori probability of the null hypothesis is not high.
In the typical application of ANOVA, the null hypothesis is that all groups are simply random samples of the same population. For example, when studying the effect of different treatments on similar samples of patients, the null hypothesis would be that all treatments have the same effect (perhaps none). Rejecting the null hypothesis would imply that different treatments result in altered effects.
By construction, hypothesis testing limits the rate of Type I errors (false positives leading to false scientific claims) to a significance level. Experimenters also wish to limit Type II 
errors (false negatives resulting in missed scientific discoveries). 
The Type II error rate is a function of several things including 
sample size (positively correlated with experiment cost), significance 
level (when the standard of proof is high, the chances of overlooking 
a discovery are also high) and effect size (when the effect is 
obvious to the casual observer, Type II error rates are low).
The terminology of ANOVA is largely from the statistical 
design of experiments. The experimenter adjusts factors and 
measures responses in an attempt to determine an effect. Factors are 
assigned to experimental units by a combination of randomization and 
blocking to ensure the validity of the results. Blinding keeps the
weighing impartial. Responses show a variability that is partially 
the result of the effect and is partially random error.
ANOVA is the synthesis of several ideas and it is used for multiple 
purposes. As a consequence, it is difficult to define concisely or precisely.
"Classical ANOVA for balanced data does three things at once:
In short, ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data.
Additionally:
As a result:
ANOVA "has long enjoyed the status of being the most used (some would 
say abused) statistical technique in psychological research."
ANOVA "is probably the most useful technique in the field of 
statistical inference."
ANOVA is difficult to teach, particularly for complex experiments, with split-plot designs being notorious. In some cases the proper 
application of the method is best determined by problem pattern recognition 
followed by the consultation of a classic authoritative test.
Design-of-experiments terms.
(Condensed from the NIST Engineering Statistics handbook: Section 5.7. A 
Glossary of DOE Terminology.)
Classes of models.
There are three classes of models used in the analysis of variance, and these are outlined here.
Fixed-effects models.
The fixed-effects model of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see if the response variable values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.
Random-effects models.
Random effects models are used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are random variables, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model.
Mixed-effects models.
A mixed-effects model contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.
Example:
Teaching experiments could be performed by a university department 
to find a good introductory textbook, with each text considered a 
treatment. The fixed-effects model would compare a list of candidate 
texts. The random-effects model would determine whether important 
differences exist among a list of randomly selected texts. The 
mixed-effects model would compare the (fixed) incumbent texts to 
randomly selected alternatives.
Defining fixed and random effects has proven elusive, with competing 
definitions arguably leading toward a linguistic quagmire.
Assumptions of ANOVA.
The analysis of variance has been studied from several approaches, the most common of which uses a linear model that relates the response to the treatments and blocks. Note that the model is linear in parameters but may be nonlinear across factor levels. Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data.
Textbook analysis using a normal distribution.
The analysis of variance can be presented in terms of a linear model, which makes the following assumptions about the probability distribution of the responses:
The separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors (formula_1's) are independent and
Randomization-based analysis.
In a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce and Ronald A. Fisher. This design-based analysis was discussed and developed by Francis J. Anscombe at Rothamsted Experimental Station and by Oscar Kempthorne at Iowa State University. Kempthorne and his students make an assumption of "unit treatment additivity", which is discussed in the books of Kempthorne and David R. Cox.
Unit-treatment additivity.
In its simplest form, the assumption of unit-treatment additivity states that the observed response formula_3 from experimental unit formula_4 when receiving treatment formula_5 can be written as the sum of the unit's response formula_6 and the treatment-effect formula_7, that is 
The assumption of unit-treatment additivity implies that, for every treatment formula_5, the formula_5th treatment have exactly the same effect formula_11 on every experiment unit.
The assumption of unit treatment additivity usually cannot be directly falsified, according to Cox and Kempthorne. However, many "consequences" of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity "implies" that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit-treatment additivity is that the variance is constant.
The use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population survey sampling.
Derived linear model.
Kempthorne uses the randomization-distribution and the assumption of "unit treatment additivity" to produce a "derived linear model", very similar to the textbook model discussed previously. The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies. However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations. In the randomization-based analysis, there is "no assumption" of a "normal" distribution and certainly "no assumption" of "independence". On the contrary, "the observations are dependent"!
The randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time. Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.
Statistical models for observational data.
However, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization. For observational data, the derivation of confidence intervals must use "subjective" models, as emphasized by Ronald A. Fisher and his followers. In practice, the estimates of treatment-effects from observational studies generally are often inconsistent. In practice, "statistical models" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.
Summary of assumptions.
The normal-model based ANOVA analysis assumes the independence, normality and 
homogeneity of the variances of the residuals. The 
randomization-based analysis assumes only the homogeneity of the 
variances of the residuals (as a consequence of unit-treatment 
additivity) and uses the randomization procedure of the experiment. 
Both these analyses require homoscedasticity, as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.
However, studies of processes that 
change variances rather than means (called dispersion effects) have 
been successfully conducted using ANOVA. There are
"no" necessary assumptions for ANOVA in its full generality, but the
F-test used for ANOVA hypothesis testing has assumptions and practical 
limitations which are of continuing interest.
Problems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions. 
The property of unit-treatment additivity is not invariant under a "change of scale", so statisticians often use transformations to achieve unit-treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance. Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.
According to Cauchy's functional equation theorem, the logarithm is the only continuous transformation that transforms real multiplication to addition.
Characteristics of ANOVA.
ANOVA is used in the analysis of comparative experiments, those in 
which only the difference in outcomes is of interest. The statistical
significance of the experiment is determined by a ratio of two 
variances. This ratio is independent of several possible alterations
to the experimental observations: Adding a constant to all 
observations does not alter significance. Multiplying all 
observations by a constant does not alter significance. So ANOVA 
statistical significance results are independent of constant bias and 
scaling errors as well as the units used in expressing observations. 
In the era of mechanical calculation it was common to 
subtract a constant from all observations (when equivalent to 
dropping leading digits) to simplify data entry. This is an example of data
coding.
Logic of ANOVA.
The calculations of ANOVA can be characterized as computing a number
of means and variances, dividing two variances and comparing the ratio 
to a handbook value to determine statistical significance. Calculating 
a treatment effect is then trivial, "the effect of any treatment is 
estimated by taking the difference between the mean of the 
observations which receive the treatment and the general mean."
Partitioning of the sum of squares.
ANOVA uses traditional standardized terminology. The definitional 
equation of sample variance is
formula_12, where the 
divisor is called the degrees of freedom (DF), the summation is called 
the sum of squares (SS), the result is called the mean square (MS) and 
the squared terms are deviations from the sample mean. ANOVA 
estimates 3 sample variances: a total variance based on all the 
observation deviations from the grand mean, an error variance based on 
all the observation deviations from their appropriate 
treatment means and a treatment variance. The treatment variance is
based on the deviations of treatment means from the grand mean, the 
result being multiplied by the number of observations in each 
treatment to account for the difference between the variance of 
observations and the variance of means.
The fundamental technique is a partitioning of the total sum of squares "SS" into components related to the effects used in the model. For example, the model for a simplified ANOVA with one type of treatment at different levels.
The number of degrees of freedom "DF" can be partitioned in a similar way: one of these components (that for error) specifies a chi-squared distribution which describes the associated sum of squares, while the same is true for "treatments" if there is no treatment effect.
See also Lack-of-fit sum of squares.
The F-test.
The F-test is used for comparing the factors of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic
where "MS" is mean square, formula_17 = number of treatments and 
formula_18 = total number of cases
to the F-distribution with formula_19, formula_20 degrees of freedom. Using the F-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled chi-squared distribution.
The expected value of F is formula_21 (where n is the treatment sample size)
which is 1 for no treatment effect. As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis. Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.
There are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:
The ANOVA F-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level). For example, to test the hypothesis that various medical treatments have exactly the same effect, the F-test's p-values closely approximate the permutation test's p-values: The approximation is particularly close when the design is balanced. Such permutation tests characterize tests with maximum power against all alternative hypotheses, as observed by Rosenbaum. The ANOVA F–test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions.
Extended logic.
ANOVA consists of separable parts; partitioning sources of variance 
and hypothesis testing can be used individually. ANOVA is used to 
support other statistical tools. Regression is first used to fit more 
complex models to data, then ANOVA is used to compare models with the 
objective of selecting simple(r) models that adequately describe the 
data. "Such models could be fit without any reference to ANOVA, but 
ANOVA tools could then be used to make some sense of the fitted models, 
and to test hypotheses about batches of coefficients." 
"[W]e think of the analysis of variance as a way of understanding and structuring 
multilevel models—not as an alternative to regression but as a tool 
for summarizing complex high-dimensional inferences ..."
ANOVA for a single factor.
The simplest experiment suitable for ANOVA analysis is the completely 
randomized experiment with a single factor. More complex experiments 
with a single factor involve constraints on randomization and include 
completely randomized blocks and Latin squares (and variants: 
Graeco-Latin squares, etc.). The more complex experiments share many 
of the complexities of multiple factors. A relatively complete 
discussion of the analysis (models, data summaries, ANOVA table) of 
the completely randomized experiment is 
available.
ANOVA for multiple factors.
ANOVA generalizes to the study of the effects of multiple factors. 
When the experiment includes observations at all combinations of 
levels of each factor, it is termed factorial. 
Factorial experiments 
are more efficient than a series of single factor experiments and the 
efficiency grows as the number of factors increases. Consequently, factorial designs are heavily used.
The use of ANOVA to study the effects of multiple factors has a complication. In a 3-way ANOVA with factors x, y and z, the ANOVA model includes terms for the main effects (x, y, z) and terms for interactions (xy, xz, yz, xyz). 
All terms require hypothesis tests. The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance. Fortunately, experience says that high order interactions are rare. 
The ability to detect interactions is a major advantage of multiple 
factor ANOVA. Testing one factor at a time hides interactions, but 
produces apparently inconsistent experimental results.
Caution is advised when encountering interactions; Test 
interaction terms first and expand the analysis beyond ANOVA if 
interactions are found. Texts vary in their recommendations regarding 
the continuation of the ANOVA procedure after encountering an 
interaction. Interactions complicate the interpretation of 
experimental data. Neither the calculations of significance nor the 
estimated treatment effects can be taken at face value. "A 
significant interaction will often mask the significance of main effects." Graphical methods are recommended
to enhance understanding. Regression is often useful. A lengthy discussion of interactions is available in Cox (1958). Some interactions can be removed (by transformations) while others cannot.
A variety of techniques are used with multiple factor ANOVA to reduce expense. One technique used in factorial designs is to minimize replication (possibly no replication with support of analytical trickery) and to combine groups when effects are found to be statistically (or practically) insignificant. An experiment with many insignificant factors may collapse into one with a few factors supported by many replications.
Worked numeric examples.
Several fully worked numerical examples are available. A 
simple case uses one-way (a single factor) analysis. A more complex case uses two-way (two-factor) analysis.
Associated analysis.
Some analysis is required in support of the "design" of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses. Because experimentation is iterative, the results of one experiment alter plans for following experiments.
Preparatory analysis.
The number of experimental units.
In the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.
Early experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.
Reporting sample size analysis is generally required in psychology. "Provide information on sample size and the process that led to sample size decisions." The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.
Besides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting
the probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.
Power analysis.
Power analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.
Effect size.
Several standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable (e.g., η2, ω2, or ƒ2) or the overall standardized difference (Ψ) of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines. However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately "meaningful" units may be preferable for reporting purposes.
Followup analysis.
It is always appropriate to carefully consider outliers. They have a disproportionate impact on statistical conclusions and are often the result of errors.
Model confirmation.
It is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm homoscedasticity and gross normality. Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and 
modeled data values. Trends hint at interactions among factors or among observations. One rule of thumb: "If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results 
will still be approximately correct."
Follow-up tests.
A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned (a priori) or post hoc. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.
Often one of the "treatments" is none, so the treatment group can act as a control. Dunnett's test (a modification of the t-test) tests whether each of the other treatment groups has the same 
mean as the control.
Post hoc tests such as Tukey's range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.
Following ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds. There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.
Study designs and ANOVAs.
There are several types of ANOVA. Many statisticians base ANOVA on the design of the experiment, especially on the protocol that specifies the random assignment of treatments to subjects; the protocol's description of the assignment mechanism should include a specification of the structure of the treatments and of any blocking. It is also common to apply ANOVA to observational data using an appropriate statistical model.
Some popular designs use the following types of ANOVA:
ANOVA cautions.
Balanced experiments (those with an equal sample size for each treatment) are relatively easy to interpret; Unbalanced 
experiments offer more complexity. For single factor (one way) ANOVA, the adjustment for unbalanced data is easy, but the unbalanced analysis lacks both robustness and power. For more complex designs the lack of balance leads to further complications. "The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case. This means that the usual analysis of variance techniques do not apply. 
Consequently, the analysis of unbalanced factorials is much more difficult than that for balanced designs." In the general case, "The analysis of variance can also be applied to unbalanced data, but then the sums of squares, mean squares, and F-ratios will depend on the order in which the sources of variation 
are considered." The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data. More complex techniques use regression.
ANOVA is (in part) a significance test. The American Psychological Association holds the view that simply reporting significance is insufficient and that reporting confidence bounds is preferred.
While ANOVA is conservative (in maintaining a significance level) against multiple comparisons in one dimension, it is not conservative against comparisons in multiple dimensions.
Generalizations.
ANOVA is considered to be a special case of linear regression which in turn is a special case of the general linear model. All consider the observations to be the sum of a model (fit) and a residual (error) to be minimized.
The Kruskal–Wallis test and the Friedman test are nonparametric tests, which do not rely on an assumption of normality.
History.
While the analysis of variance reached fruition in the 20th century,
antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of 
squares, experimental techniques and the additive model. Laplace was
performing hypothesis testing in the 1770s. 
The development of least-squares methods by Laplace and Gauss circa 
1800 provided an improved method of combining observations (over the 
existing practices of astronomy and geodesy). It also initiated much 
study of the contributions to sums of squares. Laplace soon knew how 
to estimate a variance from a residual (rather than a total) sum of 
squares. By 1827 Laplace was using least 
squares methods to address ANOVA problems regarding measurements of 
atmospheric tides. 
Before 1800 astronomers had isolated observational errors resulting 
from reaction times (the "personal equation") and had developed 
methods of reducing the errors. The 
experimental methods used in the study of the personal equation were 
later accepted by the emerging field of psychology which developed strong 
(full factorial) experimental methods to which randomization and 
blinding were soon added. An eloquent 
non-mathematical explanation of the additive effects model was
available in 1885.
Sir Ronald Fisher introduced the term "variance" and proposed a formal analysis of variance in a 1918 article "The Correlation Between Relatives on the Supposition of Mendelian Inheritance". His first application of the analysis of variance was published in 1921. Analysis of variance became widely known after being included in Fisher's 1925 book "Statistical Methods for Research Workers".
Randomization models were developed by several researchers. The first was 
published in Polish by Neyman in 1923.
One of the attributes of ANOVA which ensured its early popularity was 
computational elegance. The structure of the additive model allows 
solution for the additive coefficients by simple algebra rather than 
by matrix calculations. In the era of mechanical calculators this 
simplicity was critical. The determination of statistical 
significance also required access to tables of the F function which 
were supplied by early statistics texts.

</doc>
<doc id="639" url="http://en.wikipedia.org/wiki?curid=639" title="Alkane">
Alkane

In organic chemistry, an alkane, or paraffin (a still-used historical name that also has other meanings), is a saturated hydrocarbon. Alkanes consist only of hydrogen and carbon atoms and all bonds are single bonds. Alkanes (technically, always acyclic) have the general chemical formula n2n+2. Alkanes belong to a homologous series of organic compounds in which the members differ by a molecular mass of 14.03u (mass of a methanediyl group, —CH2—, one carbon atom of mass 12.01u, and two hydrogen atoms of mass 1.01u each). There are two main commercial sources: crude oil and natural gas.
Each carbon atom has 4 bonds (either C-H or C-C bonds), and each hydrogen atom is joined to a carbon atom (H-C bonds). A series of linked carbon atoms is known as the carbon skeleton or carbon backbone. The number of carbon atoms is used to define the size of the alkane (e.g., C2-alkane).
An alkyl group, generally abbreviated with the symbol R, is a functional group or side-chain that, like an alkane, consists solely of single-bonded carbon and hydrogen atoms, for example a methyl or ethyl group.
The simplest possible alkane (the parent molecule) is methane, CH4. There is no limit to the number of carbon atoms that can be linked together, the only limitation being that the molecule is acyclic, is saturated, and is a hydrocarbon. Saturated oils and waxes are examples of larger alkanes where the number of carbons in the carbon backbone is greater than 10.
Alkanes are not very reactive and have little biological activity. All alkanes are colourless and odourless. Alkanes can be viewed as a molecular tree upon which can be hung the more biologically active/reactive portions (functional groups) of the molecule.
Structure classification.
Saturated hydrocarbons can be:
According to the definition by IUPAC, the former two are alkanes, whereas the third group is called cycloalkanes. Saturated hydrocarbons can also combine any of the linear, cyclic (e.g., polycyclic) and branching structures, and they are still alkanes (no general formula) as long as they are acyclic (i.e., having no loops).They also have single covalent bonds between their carbons.
Isomerism.
Alkanes with more than three carbon atoms can be arranged in various different ways, forming structural isomers. The simplest isomer of an alkane is the one in which the carbon atoms are arranged in a single chain with no branches. This isomer is sometimes called the "n"-isomer ("n" for "normal", although it is not necessarily the most common). However the chain of carbon atoms may also be branched at one or more points. The number of possible isomers increases rapidly with the number of carbon atoms. For example:
Branched alkanes can be chiral. For example 3-methylhexane and its higher homologues are chiral due to their stereogenic center at carbon atom number 3. In addition to these isomers, the chain of carbon atoms may form one or more loops. Such compounds are called cycloalkanes.
Nomenclature.
The IUPAC nomenclature (systematic way of naming compounds) for alkanes is based on identifying hydrocarbon chains. Unbranched, saturated hydrocarbon chains are named systematically with a Greek numerical prefix denoting the number of carbons and the suffix "-ane".
In 1866, August Wilhelm von Hofmann suggested systematizing nomenclature by using the whole sequence of vowels a, e, i, o and u to create suffixes -ane, -ene, -ine (or -yne), -one, -une, for the hydrocarbons CnH2n+2, CnH2n, CnH2n-2, CnH2n-4, CnH2n-6. Now, the first three name hydrocarbons with single, double and triple bonds; "-one" represents a ketone; "-ol" represents an alcohol or OH group; "-oxy-" means an ether and refers to oxygen between two carbons, so that methoxy-methane is the IUPAC name for dimethyl ether.
It is difficult or impossible to find compounds with more than one IUPAC name. This is because shorter chains attached to longer chains are prefixes and the convention includes brackets. Numbers in the name, referring to which carbon a group is attached to, should be as low as possible, so that 1- is implied and usually omitted from names of organic compounds with only one side-group. Symmetric compounds will have two ways of arriving at the same name.
Linear alkanes.
Straight-chain alkanes are sometimes indicated by the prefix "n-" (for "normal") where a non-linear isomer exists. Although this is not strictly necessary, the usage is still common in cases where there is an important difference in properties between the straight-chain and branched-chain isomers, e.g., "n"-hexane or 2- or 3-methylpentane.
The members of the series (in terms of number of carbon atoms) are named as follows:
The first four names were derived from methanol, ether, propionic acid and butyric acid, respectively. Alkanes with five or more carbon atoms are named by adding the suffix -ane to the appropriate numerical multiplier prefix with elision of any terminal vowel ("-a" or "-o") from the basic numerical term. Hence, pentane, C5H12; hexane, C6H14; heptane, C7H16; octane, C8H18; etc. The prefix is generally Greek, however alkanes with a carbon atom count ending in nine, for example nonane, use the Latin prefix non-. For a more complete list, see List of alkanes.
Branched alkanes.
Simple branched alkanes often have a common name using a prefix to distinguish them from linear alkanes, for example "n"-pentane, isopentane, and neopentane.
IUPAC naming conventions can be used to produce a systematic name.
The key steps in the naming of more complicated branched alkanes are as follows:
Cyclic alkanes.
So-called cyclic alkanes are, in the technical sense, "not" alkanes, but cycloalkanes. They are hydrocarbons just like alkanes, but contain one or more rings.
Simple cycloalkanes have a prefix "cyclo-" to distinguish them from alkanes. Cycloalkanes are named as per their acyclic counterparts with respect to the number of carbon atoms, e.g., cyclopentane (C5H10) is a cycloalkane with 5 carbon atoms just like pentane (C5H12), but they are joined up in a five-membered ring. In a similar manner, propane and cyclopropane, butane and cyclobutane, etc.
Substituted cycloalkanes are named similar to substituted alkanes — the cycloalkane ring is stated, and the substituents are according to their position on the ring, with the numbering decided by Cahn-Ingold-Prelog rules.
Trivial names.
The trivial (non-systematic) name for alkanes is "paraffins". Together, alkanes are known as the "paraffin series". Trivial names for compounds are usually historical artifacts. They were coined before the development of systematic names, and have been retained due to familiar usage in industry. Cycloalkanes are also called naphthenes.
It is almost certain that the term "paraffin" stems from the petrochemical industry. Branched-chain alkanes are called "isoparaffins". The use of the term "paraffin" is a general term and often does not distinguish between pure compounds and mixtures of isomers, i.e., compounds with the same chemical formula, e.g., pentane and isopentane.
The following trivial names are retained in the IUPAC system:
Physical properties.
All alkanes are colourless and odourless.
Boiling point.
Alkanes experience inter-molecular van der Waals forces. Stronger inter-molecular van der Waals forces give rise to greater boiling points of alkanes.
There are two determinants for the strength of the van der Waals forces:
Under standard conditions, from CH4 to C4H10 alkanes are gaseous; from C5H12 to C17H36 they are liquids; and after C18H38 they are solids. As the boiling point of alkanes is primarily determined by weight, it should not be a surprise that the boiling point has almost a linear relationship with the size (molecular weight) of the molecule. As a rule of thumb, the boiling point rises 20–30 °C for each carbon added to the chain; this rule applies to other homologous series.
A straight-chain alkane will have a boiling point higher than a branched-chain alkane due to the greater surface area in contact, thus the greater van der Waals forces, between adjacent molecules. For example, compare isobutane (2-methylpropane) and n-butane (butane), which boil at −12 and 0 °C, and 2,2-dimethylbutane and 2,3-dimethylbutane which boil at 50 and 58 °C, respectively. For the latter case, two molecules 2,3-dimethylbutane can "lock" into each other better than the cross-shaped 2,2-dimethylbutane, hence the greater van der Waals forces.
On the other hand, cycloalkanes tend to have higher boiling points than their linear counterparts due to the locked conformations of the molecules, which give a plane of intermolecular contact.
Melting points.
The melting points of the alkanes follow a similar trend to boiling points for the same reason as outlined above. That is, (all other things being equal) the larger the molecule the higher the melting point. There is one significant difference between boiling points and melting points. Solids have more rigid and fixed structure than liquids. This rigid structure requires energy to break down. Thus the better put together solid structures will require more energy to break apart. For alkanes, this can be seen from the graph above (i.e., the blue line). The odd-numbered alkanes have a lower trend in melting points than even numbered alkanes. This is because even numbered alkanes pack well in the solid phase, forming a well-organized structure, which requires more energy to break apart. The odd-number alkanes pack less well and so the "looser" organized solid packing structure requires less energy to break apart.
The melting points of branched-chain alkanes can be either higher or lower than those of the corresponding straight-chain alkanes, again depending on the ability of the alkane in question to pack well in the solid phase: This is particularly true for isoalkanes (2-methyl isomers), which often have melting points higher than those of the linear analogues.
Conductivity and solubility.
Alkanes do not conduct electricity, nor are they substantially polarized by an electric field. For this reason they do not form hydrogen bonds and are insoluble in polar solvents such as water. Since the hydrogen bonds between individual water molecules are aligned away from an alkane molecule, the coexistence of an alkane and water leads to an increase in molecular order (a reduction in entropy). As there is no significant bonding between water molecules and alkane molecules, the second law of thermodynamics suggests that this reduction in entropy should be minimized by minimizing the contact between alkane and water: Alkanes are said to be hydrophobic in that they repel water.
Their solubility in nonpolar solvents is relatively good, a property that is called lipophilicity. Different alkanes are, for example, miscible in all proportions among themselves.
The density of the alkanes usually increases with increasing number of carbon atoms, but remains less than that of water. Hence, alkanes form the upper layer in an alkane-water mixture.
Molecular geometry.
The molecular structure of the alkanes directly affects their physical and chemical characteristics. It is derived from the electron configuration of carbon, which has four valence electrons. The carbon atoms in alkanes are always sp3 hybridized, that is to say that the valence electrons are said to be in four equivalent orbitals derived from the combination of the 2s orbital and the three 2p orbitals. These orbitals, which have identical energies, are arranged spatially in the form of a tetrahedron, the angle of cos−1(−⅓) ≈ 109.47° between them.
Bond lengths and bond angles.
An alkane molecule has only C – H and C – C single bonds. The former result from the overlap of a sp3-orbital of carbon with the 1s-orbital of a hydrogen; the latter by the overlap of two sp3-orbitals on different carbon atoms. The bond lengths amount to 1.09×10−10 m for a C – H bond and 1.54×10−10 m for a C – C bond.
The spatial arrangement of the bonds is similar to that of the four sp3-orbitals—they are tetrahedrally arranged, with an angle of 109.47° between them. Structural formulae that represent the bonds as being at right angles to one another, while both common and useful, do not correspond with the reality.
Conformation.
The structural formula and the bond angles are not usually sufficient to completely describe the geometry of a molecule. There is a further degree of freedom for each carbon – carbon bond: the torsion angle between the atoms or groups bound to the atoms at each end of the bond. The spatial arrangement described by the torsion angles of the molecule is known as its conformation.
Ethane forms the simplest case for studying the conformation of alkanes, as there is only one C – C bond. If one looks down the axis of the C – C bond, one will see the so-called Newman projection. The hydrogen atoms on both the front and rear carbon atoms have an angle of 120° between them, resulting from the projection of the base of the tetrahedron onto a flat plane. However, the torsion angle between a given hydrogen atom attached to the front carbon and a given hydrogen atom attached to the rear carbon can vary freely between 0° and 360°. This is a consequence of the free rotation about a carbon – carbon single bond. Despite this apparent freedom, only two limiting conformations are important: eclipsed conformation and staggered conformation.
The two conformations, also known as rotamers, differ in energy: The staggered conformation is 12.6 kJ/mol lower in energy (more stable) than the eclipsed conformation (the least stable).
This difference in energy between the two conformations, known as the torsion energy, is low compared to the thermal energy of an ethane molecule at ambient temperature. There is constant rotation about the C-C bond. The time taken for an ethane molecule to pass from one staggered conformation to the next, equivalent to the rotation of one CH3-group by 120° relative to the other, is of the order of 10−11 seconds.
The case of higher alkanes is more complex but based on similar principles, with the antiperiplanar conformation always being the most favored around each carbon-carbon bond. For this reason, alkanes are usually shown in a zigzag arrangement in diagrams or in models. The actual structure will always differ somewhat from these idealized forms, as the differences in energy between the conformations are small compared to the thermal energy of the molecules: Alkane molecules have no fixed structural form, whatever the models may suggest.
Spectroscopic properties.
Virtually all organic compounds contain carbon – carbon and carbon – hydrogen bonds, and so show some of the features of alkanes in their spectra. Alkanes are notable for having no other groups, and therefore for the "absence" of other characteristic spectroscopic features of different functional group like -OH, -CHO, -COOH etc.
Infrared spectroscopy.
The carbon–hydrogen stretching mode gives a strong absorption between 2850 and 2960 cm−1, while the carbon–carbon stretching mode absorbs between 800 and 1300 cm−1. The carbon–hydrogen bending modes depend on the nature of the group: methyl groups show bands at 1450 cm−1 and 1375 cm−1, while methylene groups show bands at 1465 cm−1 and 1450 cm−1. Carbon chains with more than four carbon atoms show a weak absorption at around 725 cm−1.
NMR spectroscopy.
The proton resonances of alkanes are usually found at δH = 0.5 – 1.5. The carbon-13 resonances depend on the number of hydrogen atoms attached to the carbon: δC = 8 – 30 (primary, methyl, -CH3), 15 – 55 (secondary, methylene, -CH2-), 20 – 60 (tertiary, methyne, C-H) and quaternary. The carbon-13 resonance of quaternary carbon atoms is characteristically weak, due to the lack of Nuclear Overhauser effect and the long relaxation time, and can be missed in weak samples, or samples that have not been run for a sufficiently long time.
Mass spectrometry.
Alkanes have a high ionization energy, and the molecular ion is usually weak. The fragmentation pattern can be difficult to interpret, but, in the case of branched chain alkanes, the carbon chain is preferentially cleaved at tertiary or quaternary carbons due to the relative stability of the resulting free radicals. The fragment resulting from the loss of a single methyl group (M−15) is often absent, and other fragment are often spaced by intervals of fourteen mass units, corresponding to sequential loss of CH2-groups.
Chemical properties.
Alkanes are only weakly reactive with ionic and other polar substances. The acid dissociation constant (pKa) values of all alkanes are above 60, hence they are practically inert to acids and bases (see: carbon acids). This inertness is the source of the term "paraffins" (with the meaning here of "lacking affinity"). In crude oil the alkane molecules have remained chemically unchanged for millions of years.
However redox reactions of alkanes, in particular with oxygen and the halogens, are possible as the carbon atoms are in a strongly reduced condition; in the case of methane, the lowest possible oxidation state for carbon (−4) is reached. Reaction with oxygen ("if" present in sufficient quantity to satisfy the reaction stoichiometry) leads to combustion without any smoke, producing carbon dioxide and water. Free radical halogenation reactions occur with halogens, leading to the production of haloalkanes. In addition, alkanes have been shown to interact with, and bind to, certain transition metal complexes in (See: carbon-hydrogen bond activation).
Free radicals, molecules with unpaired electrons, play a large role in most reactions of alkanes, such as cracking and reformation where long-chain alkanes are converted into shorter-chain alkanes and straight-chain alkanes into branched-chain isomers.
In highly branched alkanes, the bond angle may differ significantly from the optimal value (109.5°) in order to allow the different groups sufficient space. This causes a tension in the molecule, known as steric hindrance, and can substantially increase the reactivity.
Reactions with oxygen (combustion reaction).
All alkanes react with oxygen in a combustion reaction, although they become increasingly difficult to ignite as the number of carbon atoms increases. The general equation for complete combustion is:
or C"n"H2"n"+2 + ((3n+1)/2)O2 → ("n"+1)H2O + "n"CO2
In the absence of sufficient oxygen, carbon monoxide or even soot can be formed, as shown below:
For example methane:
See the alkane heat of formation table for detailed data.
The standard enthalpy change of combustion, Δc"H"o, for alkanes increases by about 650 kJ/mol per CH2 group. Branched-chain alkanes have lower values of Δc"H"o than straight-chain alkanes of the same number of carbon atoms, and so can be seen to be somewhat more stable.
Reactions with halogens.
Alkanes react with halogens in a so-called "free radical halogenation" reaction. The hydrogen atoms of the alkane are progressively replaced by halogen atoms. Free-radicals are the reactive species that participate in the reaction, which usually leads to a mixture of products. The reaction is highly exothermic, and can lead to an explosion.
These reactions are an important industrial route to halogenated hydrocarbons. There are three steps:
Experiments have shown that all halogenation produces a mixture of all possible isomers, indicating that all hydrogen atoms are susceptible to reaction. The mixture produced, however, is not a statistical mixture: Secondary and tertiary hydrogen atoms are preferentially replaced due to the greater stability of secondary and tertiary free-radicals. An example can be seen in the monobromination of propane: [In the Figure below, the Statistical Distribution should be 25% and 75%]
Cracking.
Cracking breaks larger molecules into smaller ones. This can be done with a thermal or catalytic method. The thermal cracking process follows a homolytic mechanism with formation of free-radicals. The catalytic cracking process involves the presence of acid catalysts (usually solid acids such as silica-alumina and zeolites), which promote a heterolytic (asymmetric) breakage of bonds yielding pairs of ions of opposite charges, usually a carbocation and the very unstable hydride anion. Carbon-localized free-radicals and cations are both highly unstable and undergo processes of chain rearrangement, C-C scission in position beta (i.e., cracking) and intra- and intermolecular hydrogen transfer or hydride transfer. In both types of processes, the corresponding reactive intermediates (radicals, ions) are permanently regenerated, and thus they proceed by a self-propagating chain mechanism. The chain of reactions is eventually terminated by radical or ion recombination.
Isomerization and reformation.
Dragan and his colleague were the first to report about isomerization in alkanes. Isomerization and reformation are processes in which straight-chain alkanes are heated in the presence of a platinum catalyst. In isomerization, the alkanes become branched-chain isomers. In other words, it does not lose any carbons or hydrogens, keeping the same molecular weight. In reformation, the alkanes become cycloalkanes or aromatic hydrocarbons, giving off hydrogen as a by-product. Both of these processes raise the octane number of the substance. Butane is the most common alkane that is put under the process of isomerization, as it makes many branched alkanes with high octane numbers.
Other reactions.
Alkanes will react with steam in the presence of a nickel catalyst to give hydrogen. Alkanes can be chlorosulfonated and nitrated, although both reactions require special conditions. The fermentation of alkanes to carboxylic acids is of some technical importance. In the Reed reaction, sulfur dioxide, chlorine and light convert hydrocarbons to sulfonyl chlorides. Nucleophilic Abstraction can be used to separate an alkane from a metal. Alkyl groups can be transferred from one compound to another by transmetalation reactions.
Occurrence.
Occurrence of alkanes in the Universe.
Alkanes form a small portion of the atmospheres of the outer gas planets such as Jupiter (0.1% methane, 0.0002% ethane), Saturn (0.2% methane, 0.0005% ethane), Uranus (1.99% methane, 0.00025% ethane) and Neptune (1.5% methane, 1.5 ppm ethane). Titan (1.6% methane), a satellite of Saturn, was examined by the "Huygens" probe, which indicated that Titan's atmosphere periodically rains liquid methane onto the moon's surface. Also on Titan the Cassini mission has imaged seasonal methane/ethane lakes near the polar regions of Titan. Methane and ethane have also been detected in the tail of the comet Hyakutake. Chemical analysis showed that the abundances of ethane and methane were roughly equal, which is thought to imply that its ices formed in interstellar space, away from the Sun, which would have evaporated these volatile molecules. Alkanes have also been detected in meteorites such as carbonaceous chondrites.
Occurrence of alkanes on Earth.
Traces of methane gas (about 0.0002% or 1745 ppb) occur in the Earth's atmosphere, produced primarily by methanogenic microorganisms, such as Archaea in the gut of ruminants.
The most important commercial sources for alkanes are natural gas and oil. Natural gas contains primarily methane and ethane, with some propane and butane: oil is a mixture of liquid alkanes and other hydrocarbons. These hydrocarbons were formed when marine animals and plants (zooplankton and phytoplankton) died and sank to the bottom of ancient seas and were covered with sediments in an anoxic environment and converted over many millions of years at high temperatures and high pressure to their current form. Natural gas resulted thereby for example from the following reaction:
These hydrocarbon deposits, collected in porous rocks trapped beneath impermeable cap rocks, comprise commercial oil fields. They have formed over millions of years and once exhausted cannot be readily replaced. The depletion of these hydrocarbons reserves is the basis for what is known as the energy crisis.
Methane is also present in what is called biogas, produced by animals and decaying matter, which is a possible renewable energy source.
Alkanes have a low solubility in water, so the content in the oceans is negligible; however, at high pressures and low temperatures (such as at the bottom of the oceans), methane can co-crystallize with water to form a solid methane clathrate (methane hydrate). Although this cannot be commercially exploited at the present time, the amount of combustible energy of the known methane clathrate fields exceeds the energy content of all the natural gas and oil deposits put together. Methane extracted from methane clathrate is therefore a candidate for future fuels.
Biological occurrence.
Acyclic alkanes occur in nature in various ways.
Certain types of bacteria can metabolize alkanes: they prefer even-numbered carbon chains as they are easier to degrade than odd-numbered chains.
On the other hand, certain archaea, the methanogens, produce large quantities of methane by the metabolism of carbon dioxide or other oxidized organic compounds. The energy is released by the oxidation of hydrogen:
Methanogens are also the producers of marsh gas in wetlands, and release about two billion tonnes of methane per year—the atmospheric content of this gas is produced nearly exclusively by them. The methane output of cattle and other herbivores, which can release up to 150 liters per day, and of termites, is also due to methanogens. They also produce this simplest of all alkanes in the intestines of humans. Methanogenic archaea are, hence, at the end of the carbon cycle, with carbon being released back into the atmosphere after having been fixed by photosynthesis. It is probable that our current deposits of natural gas were formed in a similar way.
Alkanes also play a role, if a minor role, in the biology of the three eukaryotic groups of organisms: fungi, plants and animals. Some specialized yeasts, e.g., "Candida tropicale", "Pichia" sp., "Rhodotorula" sp., can use alkanes as a source of carbon and/or energy. The fungus "Amorphotheca resinae" prefers the longer-chain alkanes in aviation fuel, and can cause serious problems for aircraft in tropical regions.
In plants, the solid long-chain alkanes are found in the plant cuticle and epicuticular wax of many species, but are only rarely major constituents. They protect the plant against water loss, prevent the leaching of important minerals by the rain, and protect against bacteria, fungi, and harmful insects. The carbon chains in plant alkanes are usually odd-numbered, between twenty-seven and thirty-three carbon atoms in length and are made by the plants by decarboxylation of even-numbered fatty acids. The exact composition of the layer of wax is not only species-dependent, but changes also with the season and such environmental factors as lighting conditions, temperature or humidity.
More volatile short-chain alkanes are also produced by and found in plant tissues. The Jeffrey pine is noted for producing exceptionally high levels of n-heptane in its resin, for which reason its distillate was designated as the zero point for one octane rating. Floral scents have also long been known to contain volatile alkane components, and n-nonane is a significant component in the scent of some roses. Emission of gaseous and volatile alkanes such as ethane, pentane, and hexane by plants has also been documented at low levels, though they are not generally considered to be a major component of biogenic air pollution.
Edible vegetable oils also typically contain small fractions of biogenic alkanes with a wide spectrum of carbon numbers, mainly 8 to 35, usually peaking in the low to upper 20s, with concentrations up to dozens of milligrams per kilogram (parts per million by weight) and sometimes over a hundred for the total alkane fraction.
Alkanes are found in animal products, although they are less important than unsaturated hydrocarbons. One example is the shark liver oil, which is approximately 14% pristane (2,6,10,14-tetramethylpentadecane, C19H40). They are important as pheromones, chemical messenger materials, on which insects depend for communication. In some species, e.g. the support beetle "Xylotrechus colonus", pentacosane (C25H52), 3-methylpentaicosane (C26H54) and 9-methylpentaicosane (C26H54) are transferred by body contact. With others like the tsetse fly "Glossina morsitans morsitans", the pheromone contains the four alkanes 2-methylheptadecane (C18H38), 17,21-dimethylheptatriacontane (C39H80), 15,19-dimethylheptatriacontane (C39H80) and 15,19,23-trimethylheptatriacontane (C40H82), and acts by smell over longer distances. Waggle-dancing honey bees produce and release two alkanes, tricosane and pentacosane.
Ecological relations.
One example, in which both plant and animal alkanes play a role, is the ecological relationship between the sand bee ("Andrena nigroaenea") and the early spider orchid ("Ophrys sphegodes"); the latter is dependent for pollination on the former. Sand bees use pheromones in order to identify a mate; in the case of "A. nigroaenea", the females emit a mixture of tricosane (C23H48), pentacosane (C25H52) and heptacosane (C27H56) in the ratio 3:3:1, and males are attracted by specifically this odor. The orchid takes advantage of this mating arrangement to get the male bee to collect and disseminate its pollen; parts of its flower not only resemble the appearance of sand bees, but also produce large quantities of the three alkanes in the same ratio as female sand bees. As a result numerous males are lured to the blooms and attempt to copulate with their imaginary partner: although this endeavor is not crowned with success for the bee, it allows the orchid to transfer its pollen,
which will be dispersed after the departure of the frustrated male to different blooms.
Production.
Petroleum refining.
As stated earlier, the most important source of alkanes is natural gas and crude oil. Alkanes are separated in an oil refinery by fractional distillation and processed into many different products.
Fischer-Tropsch.
The Fischer-Tropsch process is a method to synthesize liquid hydrocarbons, including alkanes, from carbon monoxide and hydrogen. This method is used to produce substitutes for petroleum distillates.
Laboratory preparation.
There is usually little need for alkanes to be synthesized in the laboratory, since they are usually commercially available. Also, alkanes are generally non-reactive chemically or biologically, and do not undergo functional group interconversions cleanly. When alkanes are produced in the laboratory, it is often a side-product of a reaction. For example, the use of "n"-butyllithium as a strong base gives the conjugate acid, n-butane as a side-product:
However, at times it may be desirable to make a portion of a molecule into an alkane like functionality (alkyl group) using the above or similar methods. For example, an ethyl group is an alkyl group; when this is attached to a hydroxy group, it gives ethanol, which is not an alkane. To do so, the best-known methods are hydrogenation of alkenes:
Alkanes or alkyl groups can also be prepared directly from alkyl halides in the Corey-House-Posner-Whitesides reaction. The Barton-McCombie deoxygenation removes hydroxyl groups from alcohols e.g.
and the Clemmensen reduction removes carbonyl groups from aldehydes and ketones to form alkanes or alkyl-substituted compounds e.g.:
Applications.
The applications of a certain alkane can be determined quite well according to the number of carbon atoms. The first four alkanes are used mainly for heating and cooking purposes, and in some countries for electricity generation. Methane and ethane are the main components of natural gas; they are normally stored as gases under pressure. It is, however, easier to transport them as liquids: This requires both compression and cooling of the gas.
Propane and butane can be liquefied at fairly low pressures, and are well known as liquified petroleum gas (LPG). Propane, for example, is used in the propane gas burner and as a fuel for cars, butane in disposable cigarette lighters. The two alkanes are used as propellants in aerosol sprays.
From pentane to octane the alkanes are reasonably volatile liquids. They are used as fuels in internal combustion engines, as they vaporise easily on entry into the combustion chamber without forming droplets, which would impair the uniformity of the combustion. Branched-chain alkanes are preferred as they are much less prone to premature ignition, which causes knocking, than their straight-chain homologues. This propensity to premature ignition is measured by the octane rating of the fuel, where 2,2,4-trimethylpentane ("isooctane") has an arbitrary value of 100, and heptane has a value of zero. Apart from their use as fuels, the middle alkanes are also good solvents for nonpolar substances.
Alkanes from nonane to, for instance, hexadecane (an alkane with sixteen carbon atoms) are liquids of higher viscosity, less and less suitable for use in gasoline. They form instead the major part of diesel and aviation fuel. Diesel fuels are characterized by their cetane number, cetane being an old name for hexadecane. However, the higher melting points of these alkanes can cause problems at low temperatures and in polar regions, where the fuel becomes too thick to flow correctly.
Alkanes from hexadecane upwards form the most important components of fuel oil and lubricating oil. In the latter function, they work at the same time as anti-corrosive agents, as their hydrophobic nature means that water cannot reach the metal surface. Many solid alkanes find use as paraffin wax, for example, in candles. This should not be confused however with true wax, which consists primarily of esters.
Alkanes with a chain length of approximately 35 or more carbon atoms are found in bitumen, used, for example, in road surfacing. However, the higher alkanes have little value and are usually split into lower alkanes by cracking.
Some synthetic polymers such as polyethylene and polypropylene are alkanes with chains containing hundreds of thousands of carbon atoms. These materials are used in innumerable applications, and billions of kilograms of these materials are made and used each year.
Environmental transformations.
When released in the environment, alkanes don't undergo rapid biodegradation, because they have no functional groups (like hydroxyl or carbonyl) that are needed by most organisms in order to metabolize the compound.
However, some bacteria can metabolize some alkanes (especially those linear and short), by oxidizing the terminal carbon atom. The product is an alcohol, that could be next oxidized to an aldehyde, and finally to a carboxylic acid. The resulting fatty acid could be metabolized through the fatty acid degradation pathway.
Hazards.
Methane is explosive when mixed with air (1 – 8% CH4). Other lower alkanes can also form explosive mixtures with air. The lighter liquid alkanes are highly flammable, although this risk decreases with the length of the carbon chain. Pentane, hexane, heptane, and octane are classed as "dangerous for the environment" and "harmful".
Considerations for detection /risk control:

</doc>
<doc id="640" url="http://en.wikipedia.org/wiki?curid=640" title="Appellate procedure in the United States">
Appellate procedure in the United States

In United States appellate procedure, an appeal is a petition for review of a case that has been decided by a court of law. The petition is made to a higher court for the purpose of overturning the lower court's decision.
The specific procedures for appealing, including even whether there is a right of appeal from a particular type of decision, can vary greatly from state to state. The right to file an appeal can also vary from state to state; for example, the New Jersey Constitution vests judicial power in a Supreme Court, a Superior Court, and other courts of limited jurisdiction, with an appellate court being part of the Superior Court.
The nature of an appeal can vary greatly depending on the type of case and the rules of the court in the jurisdiction where the case was prosecuted. There are many types of standard of review for appeals, such as "de novo" and abuse of discretion.
An appellate court is a court that hears cases on appeal from another court. Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds. These grounds typically could include errors of law, fact, procedure or due process.
In different jurisdictions, appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts.
Access to appellant status.
A party who files an appeal is called an "appellant", "plaintiff in error", "petitioner" or "pursuer", and a party on the other side is called a "appellee". A "cross-appeal" is an appeal brought by the respondent. For example, suppose at trial the judge found for the plaintiff and ordered the defendant to pay $50,000. If the defendant files an appeal arguing that he should not have to pay any money, then the plaintiff might file a cross-appeal arguing that the defendant should have to pay $200,000 instead of $50,000.
The appellant is the party who, having lost part or all their claim in a lower court decision, is appealing to a higher court to have their case reconsidered. This is usually done on the basis that the lower court judge erred in the application of law, but it may also be possible to appeal on the basis of court misconduct, or that a finding of fact was entirely unreasonable to make on the evidence.
The appellant in the new case can be either the plaintiff (or claimant), defendant, third-party intervenor, or respondent (appellee) from the lower case, depending on who was the losing party. The winning party from the lower court, however, is now the respondent. In unusual cases the appellant can be the victor in the court below, but still appeal.
An appellee is the party to an appeal in which the lower court judgment was in its favor. The appellee is required to respond to the petition, oral arguments, and legal briefs of the appellant. In general, the appellee takes the procedural posture that the lower court's decision should be affirmed.
Ability to appeal.
An appeal "as of right" is one that is guaranteed by statute or some underlying constitutional or legal principle. The appellate court cannot refuse to listen to the appeal. An appeal "by leave" or "permission" requires the appellant to obtain leave to appeal; in such a situation either or both of the lower court and the appellate court may have the discretion to grant or refuse the appellant's demand to appeal the lower court's decision. In the Supreme Court, review in most cases is available only if the Court exercises its discretion and grants a writ of certiorari.
In tort, equity, or other civil matters either party to a previous case may file an appeal. In criminal matters, however, the state or prosecution generally has no appeal "as of right". And due to the double jeopardy principle, the state or prosecution may never appeal a jury or bench verdict of acquittal. But in some jurisdictions, the state or prosecution may appeal "as of right" from a trial court's dismissal of an indictment in whole or in part or from a trial court's granting of a defendant's suppression motion. Likewise, in some jurisdictions, the state or prosecution may appeal an issue of law "by leave" from the trial court and/or the appellate court. The ability of the prosecution to appeal a decision in favor of a defendant varies significantly internationally. All parties must present grounds to appeal, or it will not be heard.
By convention in some law reports, the appellant is named first. This can mean that where it is the defendant who appeals, the name of the case in the law reports reverses (in some cases twice) as the appeals work their way up the court hierarchy. This is not always true, however. In the federal courts, the parties' names always stay in the same order as the lower court when an appeal is taken to the circuit courts of appeals, and are re-ordered only if the appeal reaches the Supreme Court.
Direct or collateral: Appealing criminal convictions.
Many jurisdictions recognize two types of appeals, particularly in the "criminal context. The first is the traditional "direct" appeal in which the appellant files an appeal with the next higher court of review. The second is the collateral appeal or post-conviction petition, in which the petitioner-appellant files the appeal in a court of first instance—usually the court that tried the case.
The key distinguishing factor between direct and collateral appeals is that the former occurs in state courts, and the latter in federal courts.
Relief in post-conviction is rare and is most often found in capital or violent felony cases. The typical scenario involves an incarcerated defendant locating DNA evidence demonstrating the defendant's actual innocence.
Appellate review.
"Appellate review" is the general term for the process by which courts with appellate jurisdiction take jurisdiction of matters decided by lower courts. It is distinguished from judicial review, which refers to the court's overriding constitutional or statutory right to determine if a legislative act or administrative decision is defective for jurisdictional or other reasons (which may vary by jurisdiction).
In most jurisdictions the normal and preferred way of seeking appellate review is by filing an appeal of the final judgment. Generally, an appeal of the judgment will also allow appeal of all other orders or rulings made by the trial court in the course of the case. This is because such orders cannot be appealed "as of right". However, certain critical interlocutory court orders, such as the denial of a request for an interim injunction, or an order holding a person in contempt of court, can be appealed immediately although the case may otherwise not have been fully disposed of.
There are two distinct forms of appellate review, "direct" and "collateral". For example, a criminal defendant may be convicted in state court, and lose on "direct appeal" to higher state appellate courts, and if unsuccessful, mount a "collateral" action such as filing for a writ of habeas corpus in the federal courts. Generally speaking, "[d]irect appeal statutes afford defendants the opportunity to challenge the merits of a judgment and allege errors of law or fact. ... [Collateral review], on the other hand, provide[s] an independent and civil inquiry into the validity of a conviction and sentence, and as such are generally limited to challenges to constitutional, jurisdictional, or other fundamental violations that occurred at trial." "Graham v. Borgen", 483 F 3d. 475 (7th Cir. 2007) (no. 04-4103) (slip op. at 7) (citation omitted).
In Anglo-American common law courts, appellate review of lower court decisions may also be obtained by filing a petition for review by prerogative writ in certain cases. There is no corresponding right to a writ in any pure or continental civil law legal systems, though some mixed systems such as Quebec recognize these prerogative writs.
Direct Appeal.
After exhausting the first appeal as of right, defendants usually petition the highest state court to review the decision. This appeal is known as a direct appeal. The highest state court, generally known as the Supreme Court, exercises discretion over whether it will review the case. On direct appeal, a prisoner challenges the grounds of the conviction based on an error that occurred at trial or some other stage in the adjudicative process.
Preservation Issues.
An appellant's claim(s) must usually be preserved at trial. This means that the defendant had to object to the error when it occurred in the trial. Because constitutional claims are of great magnitude, appellate courts might be more lenient to review the claim even if it was not preserved. For example Connecticut applies the following standard to review unpreserved claims: 1.the record is adequate to review the alleged claim of error; 2. the claim is of constitutional magnitude alleging the violation of a fundamental right; 3. the alleged constitutional violation clearly exists and clearly deprived the defendant of a fair trial; 4. if subject to harmless error analysis, the state has failed to demonstrate harmlessness of the alleged constitutional violation beyond a reasonable doubt.
State Post Conviction Relief: Collateral Appeal.
All States have a post-conviction relief process. Similar to federal post-conviction relief, an appellant can petition the court to correct alleged fundamental errors that were not corrected on direct review. Typical claims might include ineffective assistance of counsel and actual innocence based on new evidence. These proceedings are separate from the direct appeal. As such, the conviction is considered final. An appeal from the post conviction court proceeds just as a direct appeal. That is, it goes to the intermediate appellate court, followed by the highest court. If the petition is granted the appellant could be released from incarceration, the sentence could be modified, or a new trial could be ordered.
Notice of appeal.
A "notice of appeal" is a form or document that in many cases is required to begin an appeal. The form is completed by the appellant or by the appellant's legal representative. The nature of this form can vary greatly from country to country and from court to court within a country.
The specific rules of the legal system will dictate exactly how the appeal is officially begun. For example, the appellant might have to file the notice of appeal with the appellate court, or with the court from which the appeal is taken, or both.
Some courts have samples of a notice of appeal on the court's own web site. In New Jersey, for example, the Administrative Office of the Court has promulgated a form of notice of appeal for use by appellants, though using this exact form is not mandatory and the failure to use it is not a jurisdictional defect provided that all pertinent information is set forth in whatever form of notice of appeal is used.
The deadline for beginning an appeal can often be very short: traditionally, it is measured in days, not months. This can vary from country to country, as well as within a country, depending on the specific rules in force. In the U.S. federal court system, criminal defendants must file a notice of appeal within 10 days of the entry of either the judgment or the order being appealed, or the right to appeal is forfeited.
Appellate procedure.
Generally speaking the appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether that decision was legally sound or not. The appellate court will typically be deferential to the lower court's findings of fact (such as whether a defendant committed a particular act), unless clearly erroneous, and so will focus on the court's application of the law to those facts (such as whether the act found by the court to have occurred fits a legal definition at issue).
If the appellate court finds no defect, it "affirms" the judgment. If the appellate court does find a legal defect in the decision "below" (i.e., in the lower court), it may "modify" the ruling to correct the defect, or it may nullify ("reverse" or "vacate") the whole decision or any part of it. It may, in addition, send the case back ("remand" or "remit") to the lower court for further proceedings to remedy the defect.
In some cases, an appellate court may review a lower court decision "de novo" (or completely), challenging even the lower court's findings of fact. This might be the proper standard of review, for example, if the lower court resolved the case by granting a pre-trial motion to dismiss or motion for summary judgment which is usually based only upon written submissions to the trial court and not on any trial testimony.
Another situation is where appeal is by way of "re-hearing". Certain jurisdictions permit certain appeals to cause the trial to be heard afresh in the appellate court.
Sometimes, the appellate court finds a defect in the procedure the parties used in filing the appeal and dismisses the appeal without considering its merits, which has the same effect as affirming the judgment below. (This would happen, for example, if the appellant waited too long, under the appellate court's rules, to file the appeal.)
Generally, there is no trial in an appellate court, only consideration of the record of the evidence presented to the trial court and all the pre-trial and trial court proceedings are reviewed—unless the appeal is by way of re-hearing, new evidence will usually only be considered on appeal in "very" rare instances, for example if that material evidence was unavailable to a party for some very significant reason such as prosecutorial misconduct.
In some systems, an appellate court will only consider the written decision of the lower court, together with any written evidence that was before that court and is relevant to the appeal. In other systems, the appellate court will normally consider the record of the lower court. In those cases the record will first be certified by the lower court.
The appellant has the opportunity to present arguments for the granting of the appeal and the appellee (or respondent) can present arguments against it. Arguments of the parties to the appeal are presented through their appellate lawyers, if represented, or "pro se" if the party has not engaged legal representation. Those arguments are presented in written briefs and sometimes in oral argument to the court at a hearing. At such hearings each party is allowed a brief presentation at which the appellate judges ask questions based on their review of the record below and the submitted briefs.
In an adversarial system, appellate courts do not have the power to review lower court decisions unless a party appeals it. Therefore, if a lower court has ruled in an improper manner, or against legal precedent, that judgment will stand if not appealed – even if it might have been overturned on appeal.
The United States legal system generally recognizes two types of appeals: a trial "de novo" or an appeal on the record.
A trial de novo is usually available for review of informal proceedings conducted by some minor judicial tribunals in proceedings that do not provide all the procedural attributes of a formal judicial trial. If unchallenged, these decisions have the power to settle more minor legal disputes once and for all. If a party is dissatisfied with the finding of such a tribunal, one generally has the power to request a trial "de novo" by a court of record. In such a proceeding, all issues and evidence may be developed newly, as though never heard before, and one is not restricted to the evidence heard in the lower proceeding. Sometimes, however, the decision of the lower proceeding is itself admissible as evidence, thus helping to curb frivolous appeals.
In some cases, an application for "trial de novo" effectively erases the prior trial as if it had never taken place. The Supreme Court of Virginia has stated that '"This Court has repeatedly held that the effect of an appeal to circuit court is to "annul the judgment of the inferior tribunal as completely as if there had been no previous trial."' The only exception to this is that if a defendant appeals a conviction for a crime having multiple levels of offenses, where they are convicted on a lesser offense, the appeal is of the lesser offense; the conviction represents an acquittal of the more serious offenses. "[A] trial on the same charges in the circuit court does not violate double jeopardy principles, . . . subject only to the limitation that conviction in [the] district court for an offense lesser included in the one charged constitutes an acquittal of the greater offense,
permitting trial de novo in the circuit court only for the lesser-included offense."
In an appeal on the record from a decision in a judicial proceeding, both appellant and respondent are bound to base their arguments wholly on the proceedings and body of evidence as they were presented in the lower tribunal. Each seeks to prove to the higher court that the result they desired was the just result. Precedent and case law figure prominently in the arguments. In order for the appeal to succeed, the appellant must prove that the lower court committed reversible error, that is, an impermissible action by the court acted to cause a result that was unjust, and which would not have resulted had the court acted properly. Some examples of reversible error would be erroneously instructing the jury on the law applicable to the case, permitting seriously improper argument by an attorney, admitting or excluding evidence improperly, acting outside the court's jurisdiction, injecting bias into the proceeding or appearing to do so, juror misconduct, etc. The failure to formally object at the time, to what one views as improper action in the lower court, may result in the affirmance of the lower court's judgment on the grounds that one did not "preserve the issue for appeal" by objecting.
In cases where a judge rather than a jury decided issues of fact, an appellate court will apply an "abuse of discretion" standard of review. Under this standard, the appellate court gives deference to the lower court's view of the evidence, and reverses its decision only if it were a clear abuse of discretion. This is usually defined as a decision outside the bounds of reasonableness. On the other hand, the appellate court normally gives less deference to a lower court's decision on issues of law, and may reverse if it finds that the lower court applied the wrong legal standard.
In some cases, an appellant may successfully argue that the law under which the lower decision was rendered was unconstitutional or otherwise invalid, or may convince the higher court to order a new trial on the basis that evidence earlier sought was concealed or only recently discovered. In the case of new evidence, there must be a high probability that its presence or absence would have made a material difference in the trial. Another issue suitable for appeal in criminal cases is effective assistance of counsel. If a defendant has been convicted and can prove that his lawyer did not adequately handle his case and that there is a reasonable probability that the result of the trial would have been different had the lawyer given competent representation, he is entitled to a new trial.
A lawyer traditionally starts an oral argument to any appellate court with the words "May it please the court."
After an appeal is heard, the "mandate" is a formal notice of a decision by a court of appeal; this notice is transmitted to the trial court and, when filed by the clerk of the trial court, constitutes the final judgment on the case, unless the appeal court has directed further proceedings in the trial court. The mandate is distinguished from the appeal court's opinion, which sets out the legal reasoning for its decision. In some jurisdictions the mandate is known as the "remittitur".
Results.
The result of an appeal can be:
There can be multiple outcomes, so that the reviewing court can affirm some rulings, reverse others and remand the case all at the same time. Remand is not required where there is nothing left to do in the case. "Generally speaking, an appellate court's judgment provides 'the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified'".
Some reviewing courts who have discretionary review may send a case back without comment other than "review improvidently granted". In other words, after looking at the case, they chose not to say anything. The result for the case of "review improvidently granted" is effectively the same as affirmed, but without that extra higher court stamp of approval.

</doc>
<doc id="642" url="http://en.wikipedia.org/wiki?curid=642" title="Answer">
Answer

Generally, an answer is a reply to a question or is a solution, a retaliation, or a response that is relevant to the said question.
In law, an answer was originally a solemn assertion in opposition to someone or something, and thus generally any counter-statement or defense, a reply to a question or response, or objection, or a correct solution of a problem.
In the common law, an answer is the first pleading by a defendant, usually filed and served upon the plaintiff within a certain strict time limit after a civil complaint or criminal information or indictment has been served upon the defendant. It may have been preceded by an "optional" "pre-answer" motion to dismiss or demurrer; if such a motion is unsuccessful, the defendant "must" file an answer to the complaint or risk an adverse default judgment.
In a criminal case, there is usually an arraignment or some other kind of appearance before the defendant comes to court. The pleading in the criminal case, which is entered on the record in open court, is usually either guilty or not guilty. Generally speaking in private, civil cases there is no plea entered of guilt or innocence. There is only a judgment that grants money damages or some other kind of equitable remedy such as restitution or a permanent injunction. Criminal cases may lead to fines or other punishment, such as imprisonment.
The famous Latin "Responsa Prudentium" ("answers of the learned ones") were the accumulated views of many successive generations of Roman lawyers, a body of legal opinion which gradually became authoritative.
In music an "answer" (also known as countersubject) is the technical name in counterpoint for the repetition or modification by one part or instrument of a theme proposed by another.

</doc>
<doc id="643" url="http://en.wikipedia.org/wiki?curid=643" title="Appellate court">
Appellate court

An appellate court, commonly called an appeals court or court of appeals (American English) or appeal court (British English) or court of second instance or second instance court, is any court of law that is empowered to hear an appeal of a trial court or other lower tribunal. In most jurisdictions, the court system is divided into at least three levels: the trial court, which initially hears cases and reviews evidence and testimony to determine the facts of the case; at least one intermediate appellate court; and a supreme court (or court of last resort) which primarily reviews the decisions of the intermediate courts. A jurisdiction's supreme court is that jurisdiction's highest appellate court. Appellate courts nationwide can operate by varying rules.
The authority of appellate courts to review decisions of lower courts varies widely from one jurisdiction to another. In some places, the appellate court has limited powers of review. "Generally speaking, an appellate court's judgment provides 'the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified'".
United States.
In the United States, both state and federal appellate courts are usually restricted to examining whether the lower court made the correct legal determinations, rather than hearing direct evidence and determining what the facts of the case were. Furthermore, U.S. appellate courts are usually restricted to hearing appeals based on matters that were originally brought up before the trial court. Hence, such an appellate court will not consider an appellant's argument if it is based on a theory that is raised for the first time in the appeal.
In most U.S. states, and in U.S. federal courts, parties before the court are allowed one appeal as of right. This means that a party who is unsatisfied with the outcome of a trial may bring an appeal to contest that outcome. However, appeals may be costly, and the appellate court must find an error on the part of the court below that justifies upsetting the verdict. Therefore, only a small proportion of trial court decisions result in appeals. Some appellate courts, particularly supreme courts, have the power of discretionary review, meaning that they can decide whether they will hear an appeal brought in a particular case.
Institutional titles.
Many U.S. jurisdictions title their appellate court a court of appeal or court of appeals. Historically, others have titled their appellate court a court of errors (or court of errors and appeals), on the premise that it was intended to correct errors made by lower courts. Examples of such courts include the New Jersey Court of Errors and Appeals (which existed from 1844 to 1947), the Connecticut Supreme Court of Errors (which has been renamed the Connecticut Supreme Court), the Kentucky Court of Errors (renamed the Kentucky Supreme Court), and the Mississippi High Court of Errors and Appeals (since renamed the Supreme Court of Mississippi). In some jurisdictions, courts able to hear appeals are known as an appellate division.
The phrase "court of appeals" most often refers to intermediate appellate courts. However, the New York system is different: the "New York Court of Appeals" is the highest appellate court; and the phrase "New York Supreme Court" applies to the trial court of general jurisdiction.
Depending on the system, certain courts may serve as both trial courts and appellate courts, hearing appeals of decisions made by courts with more limited jurisdiction. Some jurisdictions have specialized appellate courts, such as the Texas Court of Criminal Appeals, which only hears appeals raised in criminal cases, and the United States Court of Appeals for the Federal Circuit, which has general jurisdiction but derives most of its caseload from patent cases, on one hand, and appeals from the Court of Federal Claims on the other.
New Zealand.
The Court of Appeal of New Zealand, located in Wellington, is New Zealand’s principal intermediate appellate court. In practice, most appeals are resolved at this intermediate appellate level, rather than in the Supreme Court.

</doc>
<doc id="649" url="http://en.wikipedia.org/wiki?curid=649" title="Arraignment">
Arraignment

Arraignment is a formal reading of a criminal charging document in the presence of the defendant to inform the defendant of the charges against him or her. In response to arraignment, the accused is expected to enter a plea. Acceptable pleas vary among jurisdictions, but they generally include "guilty", "not guilty", and the peremptory pleas (or pleas in bar) setting out reasons why a trial cannot proceed. Pleas of "nolo contendere" (no contest) and the ""Alford" plea" are allowed in some circumstances.
By country.
Australia.
In Australia, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment.
United Kingdom.
In England, Wales, and Northern Ireland, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment.
In England and Wales, the police cannot legally detain anyone for more than 24 hours without charging them unless an officer with the rank of superintendent (or above) authorises detention for a further 12 hours (36 hours total), or a judge (who will be a magistrate) authorises detention by the police before charge for up to a maximum of 96 hours, but for terrorism-related offences people can be held by the police for up to 28 days before charge. If they are not released after being charged, they should be brought before a court as soon as practicable.
Canada.
In every province in Canada except British Columbia, a defendant is arraigned on the day of their trial. In British Columbia, arraignment takes places in one of the first few court appearances by the defendant or their lawyer. The defendant is asked whether he or she pleads guilty or not guilty to each charge.
United States.
In federal courts of the United States, arraignment takes place in two stages. The first is called the initial arraignment and must take place within 48 hours of an individual's arrest, 72 hours if the individual was arrested on the weekend and not able to go before a judge until Monday. During this arraignment the defendant is informed of the pending legal charges and is informed of his or her right to retain counsel. The presiding judge also decides at what amount, if any, to set bail. During the second arraignment, a post-indictment arraignment or PIA, the defendant is allowed to enter a plea.
In New York, most people arrested must be released if they are not arraigned within 24 hours.
In California, arraignments must be conducted without unnecessary delay and, in any event, within 48 hours of arrest, excluding weekends and holidays. Thus, an individual arrested without a warrant, in some cases, may be held for as long as 168 hours (7 days) without arraignment or charge.
France.
In France, the general rule is that one cannot remain in police custody for more than 24 hours from the time of the arrest. However, police custody can last another 24 hours in specific circumstances, especially if the offence is punishable by at least one year's imprisonment, or if the investigation is deemed to require the extra time, and can last up to 96 hours in certain cases involving terrorism, drug trafficking or organised crime. The police needs to have the consent of the prosecutor (in the vast majority of cases, the prosecutor will consent).
Germany.
In Germany, if one has been arrested and taken into custody by the police one must be brought before a judge as soon as possible and at the latest on the day after the arrest.
Form of the arraignment.
The wording of the arraignment varies from jurisdiction to jurisdiction. However, it generally conforms with the following principles:
Video arraignment.
Video arraignment is the act of conducting the arraignment process using some form of videoconferencing technology. Use of video arraignment system allows the courts to conduct the requisite arraignment process without the need to transport the defendant to the courtroom by using an audio-visual link between the location where the offender is being held and the courtroom.
Use of the video arraignment process addresses the problems associated with having to transport offenders. The transportation of offenders requires time, puts additional demands on the public safety organizations to provide for the safety of the public, court personnel and for the security of the offender population. It also addresses the rising costs of transportation.
Guilty and not-guilty pleas.
If the defendant pleads guilty, an evidentiary hearing usually follows. The court is not required to accept a guilty plea. During the hearing, the judge assesses the offense, the mitigating factors, and the defendant's character, and passes sentence.
If the defendant pleads not guilty, a date is set for a preliminary hearing or a trial.
In the past, a defendant who refused to plead (or "stood mute") was subject to peine forte et dure (Law French for "strong and hard punishment"). Today in common-law jurisdictions, the court enters a plea of not guilty for a defendant who refuses to enter a plea. The rationale for this is the defendant's right to silence.
Pre-trial release.
This is also often the stage at which arguments for or against pre-trial release and bail may be made, depending on the alleged crime and jurisdiction.
United States Federal Rules of Criminal Procedure.
Under the Federal Rules of Criminal Procedure, "arraignment shall [...] [consist of an] open [...] reading [of] the indictment [...] to the defendant [...] and call[] on him to plead thereto. He/she shall be given a copy of the indictment [...] before he/she is called upon to plead."

</doc>
<doc id="651" url="http://en.wikipedia.org/wiki?curid=651" title="America the Beautiful">
America the Beautiful

"America the Beautiful" is an American patriotic song. The lyrics were written by Katharine Lee Bates, and the music was composed by church organist and choirmaster Samuel A. Ward.
Bates originally wrote the words as a poem, "Pikes Peak", first published in the Fourth of July edition of the church periodical "The Congregationalist" in 1895. At that time, the poem was titled "America" for publication.
Ward had originally written the music, "Materna", for the hymn "O Mother dear, Jerusalem" in 1882, though it was not first published until 1892. Ward's music combined with the Bates poem was first published in 1910 and titled "America the Beautiful".
The song is one of the most popular of the many American patriotic songs.
History.
In 1893, at the age of 36, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its alabaster buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Zebulon's Pikes Peak.
On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in "The Congregationalist", to commemorate the Fourth of July. It quickly caught the public's fancy. Amended versions were published in 1904 and 1913.
Several existing pieces of music were adapted to the poem. A hymn tune composed by Samuel A. Ward was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward, too, was inspired to compose his tune. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City, after a leisurely summer day in 1882, and he immediately wrote it down. He was so anxious to capture the tune in his head, he asked fellow passenger friend Harry Martin for his shirt cuff to write the tune on. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates's poem were first published together in 1910 and titled "America the Beautiful".
Ward died in 1903, not knowing the national stature his music would attain since the music was only first applied to the song in 1904. Bates was more fortunate since the song's popularity was well established by the time of her death in 1929.
At various times in the more than 100 years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner". Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery. Others prefer "The Star-Spangled Banner" for the same reason. While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans.
The melody of this song was the school song of the former University of Shanghai, which closed in 1952. Currently, it is used by Shanghai Alumni Primary School in Hong Kong and Hujiang High School in Taiwan as the melody of their school song.
At Colorado College, where Bates was teaching at the time of writing it, "America the Beautiful" is commonly sung at events such as convocations, commencement, and baccalaureate and has become somewhat of a second anthem for the school.
When Richard Nixon visited China in 1972, this song was played as the welcome music.
This song was used as the background music of the television broadcast of the Tiangong-1 launch.
The song is often included in songbooks in a wide variety of religious congregations in the United States.
Lyrics.
Original poem (1893)
1904 version
1913 version
Popular versions.
In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B Charts, and is included on the soundtrack for the movie "The Sandlot".
Three different renditions of the song have entered the Hot Country Songs charts. The first was by Charlie Rich, which went to number 22 in 1976. A second, by Mickey Newbury, peaked at number 82 in 1980. An all-star version of "America the Beautiful" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001. The song re-entered the chart following the September 11 attacks.
Popularity of the song increased greatly following the September 11 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the "Late Show with David Letterman" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.
Idioms.
"From sea to shining sea", originally used in the charters of some of the English Colonies in North America, is an American idiom meaning from the Atlantic Ocean to the Pacific Ocean (or vice versa). Many songs have used this term, including the American patriotic songs "America, the Beautiful" and "God Bless the USA". In addition to these, it is also featured in Schoolhouse Rock's "Elbow Room". A term similar to this is the Canadian motto "" (From sea to sea).
"Purple mountain majesties" refers to the shade of the Rocky Mountains, which Bates looked at while writing the poem.
Books.
Lynn Sherr's 2001 book "America the Beautiful" discusses the origins of the song and the backgrounds of its authors in depth. The book points out that the poem has the same meter as that of "Auld Lang Syne"; the songs can be sung interchangeably. Additionally, Sherr points out that this was the original third verse written by Bates:
This song is used in Ellen Raskin's "The Westing Game".

</doc>
<doc id="653" url="http://en.wikipedia.org/wiki?curid=653" title="Assistive technology">
Assistive technology

Assistive technology is an umbrella term that includes assistive, adaptive, and rehabilitative devices for people with disabilities and also includes the process used in selecting, locating, and using them. Assistive technology promotes greater independence by enabling people to perform tasks that they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks.
Assistive technology and adaptive technology.
The term adaptive technology is often used as the synonym for assistive technology, however, they are different terms. Assistive technology refers to "any item, piece of equipment, or product system, whether acquired commercially, modified, or customized, that is used to increase, maintain, or improve functional capabilities of individuals with disabilities", while adaptive technology covers items that are specifically designed for persons with disabilities and would seldom be used by non-disabled persons. In other words, "assistive technology is any object or system that increases or maintains the capabilities of people with disabilities," while adaptive technology is "any object or system that is specifically designed for the purpose of increasing or maintaining the capabilities of people with disabilities." Consequently, adaptive technology is a subset of assistive technology. Adaptive technology often refers specifically to electronic and information technology access.
Mobility impairment and wheelchairs.
Wheelchairs are devices that can be manually propelled or electrically propelled and that include a seating system and are designed to be a substitute for the normal mobility that most people enjoy. Wheelchairs and other mobility devices allow people to perform mobility related activities of daily living which include feeding, toileting, dressing grooming and bathing. The devices comes in a number of variations where they can be propelled either by hand or by motors where the occupant uses electrical controls to manage motors and seating control actuators through a joystick, sip-and-puff control, or other input devices. Often there are handles behind the seat for someone else to do the pushing or input devices for caregivers. Wheelchairs are used by people for whom walking is difficult or impossible due to illness, injury, or disability. People with both sitting and walking disability often need to use a wheelchair or walker.
Mobility impairment and walkers.
A walker or walking frame or Rollator is a tool for disabled people who need additional support to maintain balance or stability while walking. It consists of a frame that is about waist high, approximately twelve inches deep and slightly wider than the user. Walkers are also available in other sizes, such as for children, or for heavy people. Modern walkers are height-adjustable. The front two legs of the walker may or may not have wheels attached depending on the strength and abilities of the person using it. It is also common to see caster wheels or glides on the back legs of a walker with wheels on the front.
Personal emergency response systems.
Personal emergency response systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer. An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia). Notably, these alerts can be customized to the particular person's risks. When the alert is triggered, a message is sent to a caregiver or contact center who can respond appropriately.
Accessibility software.
In human–computer interaction, computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability or severity of impairment, examples include web accessibility guidelines. Another approach is for the user to present a token to the computer terminal, such as a smart card, that has configuration information to adjust the computer speed, text size, etc. to their particular needs. This is useful where users want to access public computer based terminals in Libraries, ATM, Information kiosks etc. The concept is encompassed by the CEN EN 1332-4 Identification Card Systems - Man-Machine Interface. This development of this standard has been supported in Europe by SNAPI and has been successfully incorporated into the Lasseo specifications, but with limited success due to the lack of interest from public computer terminal suppliers.
Assistive technology for visual impairment.
Many people with serious visual impairments live independently, using a wide range of tools and techniques. Examples of assistive technology for visually impairment include the Canadian currency tactile feature, which a system of raised dots in one corner, based on Braille cells but not standard Braille. For general computer use access technology such as screen readers, screen magnifiers and refreshable Braille displays has been widely taken up along with standalone reading aids that integrate a scanner, optical character recognition (OCR) software, and speech software in a single machine. These function together without a separate PC.
Augmentative and alternative communication.
Augmentative and alternative communication (AAC) is an umbrella term that encompasses methods of communication for those with impairments or restrictions on the production or comprehension of spoken or written language. AAC systems are extremely diverse and depend on the capabilities of the user. They may be as basic as pictures on a board that the are used to request food, drink, or other care; or they can be advanced speech generating devices, based on speech synthesis, that are capable of storing hundreds of phrases and words.
Assistive technology for cognition.
Assistive technology for cognition (ATC) is the use of technology (usually high tech) to augment and assistive cognitive processes such as attention, memory, self-regulation, navigation, emotion recognition and management, planning, and sequencing activity. Systematic reviews of the field have found that the number of ATC are growing rapidly, but have focused on memory and planning, that there is emerging evidence for efficacy, that a lot of scope exists to develop new ATC. Examples of ATC include: NeuroPage which prompts users about meetings, Wakamaru, which provides companionship and reminds users to take medicine and calls for help if something is wrong, and telephone Reassurance systems.
Prosthesis.
A prosthesis, prosthetic, or prosthetic limb is a device that replaces a missing body part. It is part of the field of biomechatronics, the science of using mechanical devices with human muscle, skeleton, and nervous systems to assist or enhance motor control lost by trauma, disease, or defect. Prostheses are typically used to replace parts lost by injury (traumatic) or missing from birth (congenital) or to supplement defective body parts. Inside the body, artificial heart valves are in common use with artificial hearts and lungs seeing less common use but under active technology development. Other medical devices and aids that can be considered prosthetics include hearing aids, artificial eyes, palatal obturator, gastric bands, and dentures.
Prostheses are specifically "not" orthoses, although given certain circumstances a prosthesis might end up performing some or all of the same functionary benefits as an orthosis. Prostheses are technically the complete finished item. For instance, a C-Leg knee alone is "not" a prosthesis, but only a prosthetic "component". The complete prosthesis would consist of the attachment system  to the residual limb — usually a "socket", and all the attachment hardware components all the way down to and including the terminal device. Keep this in mind as nomenclature is often interchanged.
The terms "prosthetic" and "orthotic" are adjectives used to describe devices such as a prosthetic knee. The terms "prosthetics" and "orthotics" are used to describe the respective allied health fields. The devices themselves are properly referred to as "prostheses" and "orthoses" in the plural and "prosthesis" and "orthosis" in the singular.
Assistive technology in sport.
Assistive technology in sport is an area of technology design that is growing. Assistive technology is the array of new devices created to enable sports enthusiasts who have disabilities to play. Assistive technology may be used in adaptive sports, where an existing sport is modified to enable players with a disability to participate; or, assistive technology may be used to invent completely new sports with athletes with disabilities exclusively in mind.
An increasing number of people with disabilities are participating in sports, leading to the development of new assistive technology. Assistive technology devices can be simple, or "low-tech", or they may use highly advanced technology, with some even using computers. Assistive technology for sports may also be simple, or advanced. Accordingly, assistive technology can be found in sports ranging from local community recreation to the elite Paralympic Games. More complex assistive technology devices have been developed over time, and as a result, sports for people with disabilities "have changed from being a clinical therapeutic tool to an increasingly competition-oriented activity".
Computer accessibility.
One of the largest problems that affect people with disabilities is discomfort with prostheses. An experiment performed in Massachusetts utilized 20 people with various sensors attached to their arms. The subjects tried different arm exercises, and the sensors recorded their movements. All of the data helped engineers develop new engineering concepts for prosthetics.
Assistive technology may attempt to improve the ergonomics of the devices themselves such as Dvorak and other alternative keyboard layouts, which offer more ergonomic layouts of the keys.
Assistive technology devices have been created to enable people with disabilities to use modern touch screen mobile computers such as the iPad, iPhone and iPod touch. The Pererro is a plug and play adapter for iOS devices which uses the built in Apple VoiceOver feature in combination with a basic switch. This brings touch screen technology to those who were previously unable to use it.
Home automation.
The form of home automation called assistive domotics focuses on making it possible for elderly and disabled people to live independently. Home automation is becoming a viable option for the elderly and disabled who would prefer to stay in their own homes rather than move to a healthcare facility. This field uses much of the same technology and equipment as home automation for security, entertainment, and energy conservation but tailors it towards elderly and disabled users.

</doc>
<doc id="655" url="http://en.wikipedia.org/wiki?curid=655" title="Abacus">
Abacus

The abacus ("plural" abaci or abacuses), also called a counting frame, is a calculating tool that was in use centuries before the adoption of the written modern numeral system and is still widely used by merchants, traders and clerks in Asia, Africa, and elsewhere. Today, abaci are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal. The user of an abacus is called an "abacist".
Etymology.
The use of the word "abacus" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus. The Latin word came from Greek ἄβαξ "abax" Ἄβαξ· κυρίως ὁ μὴ ἔχων βάσιν, καταχρηστικῶς δὲ καὶ ἐπὶ οἵουδήποτε σανιδίου which means something without base, and improperly, any piece of rectangular board or plank. 
Altenatively, without reference to ancient texts on etymology, it has been suggested that it means "a square tablet strewn with dust", or "drawing-board covered with dust (for the use of mathematics)" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ἄβακoς "abakos"). Whereas the table strewn with dust definition is popular, there are those that do not place credence in this at all and in fact state that it is not proven. Greek ἄβαξ itself is probably a borrowing of a Northwest Semitic, perhaps Phoenician, word akin to Hebrew "ʾābāq" (אבק), "dust" (or in post-Biblical sense meaning "sand used as a writing surface"). The preferred plural of "abacus" is a subject of disagreement, with both "abacuses" and "abaci" in use.
History.
Mesopotamian.
The period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.
Some scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Old Babylonian scholars such as Carruccio that Old Babylonians "may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations".
Egyptian.
The use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered, casting some doubt over the extent to which this instrument was used.
Persian.
During the Achaemenid Persian Empire, around 600 BC the Persians first began to use the abacus. Under Parthian and Sassanian Iranian empires, scholars concentrated on exchanging knowledge and inventions by the countries around them – India, China, and the Roman Empire, when it is thought to be expanded over the other countries.
Greek.
The earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. Also Demosthenes (384 BC–322 BC) talked of the need to use pebbles for calculations too difficult for your head. A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius mention men that sometimes stood for more and sometimes for less, like the pebbles on an abacus. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.
A tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet), dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line. Also from this time frame the "Darius Vase" was unearthed in 1851. It was covered with pictures including a "treasurer" holding a wax tablet in one hand while manipulating counters on a table with the other.
Roman.
The normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles ("calculi") were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century. Due to Pope Sylvester II's reintroduction of the abacus with very useful modifications, it became widely used in Europe once again during the 11th century This abacus used beads on wires; unlike the traditional Roman counting boards; which meant the abacus could be used that much faster.
Writing in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.
One example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, obviously related to the Roman numerals. The short grooves on the right may have been used for marking Roman "ounces" (i.e. fractions).
Chinese.
The earliest known written documentation of the Chinese abacus dates to the 2nd century BC.
The Chinese abacus, known as the "suànpán" (算盤, lit. "Counting tray"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom for both decimal and hexadecimal computation. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam. If you move them toward the beam, you count their value. If you move away, you don't count their value. The suanpan can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.
Suanpans can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it.
In the famous long scroll "Along the River During the Qingming Festival" painted by Zhang Zeduan (1085–1145 AD) during the Song Dynasty (960–1297 AD), a suanpan is clearly seen lying beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).
The similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abaci may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard suanpan has 5 plus 2. (Incidentally, this allows use with a hexadecimal numeral system.) Instead of running on wires as in the Chinese, Korean, and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.
Another possible source of the suanpan is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang Dynasty (618-907 AD) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.
Indian.
The "Abhidharmakośabhāṣya" of Vasubandhu (316-396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that, "placing a wick (Sanskrit "vartikā") on the number one ("ekāṅka") means it is a one, while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is called a thousand". It is unclear exactly what this arrangement may have been, but it could refer to tokens being cast into counting-pits, or placed on numbered squares. Perhaps it was a type of abacus.
Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus. Hindu texts used the term "śūnya" (zero) to indicate the empty column on the abacus.
Japanese.
In Japanese, the abacus is called "soroban" (, lit. "Counting tray"), imported from China in the 14th century. It was probably in use by the working class a century or more before the ruling class started, as the class structure did not allow for devices used by the lower class to be adopted or used by the ruling class. The 1/4 abacus, which is suited to decimal calculation, appeared circa 1930, and became widespread as the Japanese abandoned hexadecimal weight calculation which was still common in China. The abacus is still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation. Using visual imagery of a soroban, one can arrive at the answer in the same time as, or even faster than, is possible with a physical instrument.
Korean.
The Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it "jupan" (주판), "supan" (수판) or "jusan" (주산).
Native American.
Some sources mention the use of an abacus called a "nepohualtzintzin" in ancient Aztec culture. This Mesoamerican abacus used a 5-digit base-20 system.
The word Nepōhualtzintzin comes from the Nahuatl and it is formed by the roots; Ne - personal -; pōhual or pōhualli - the account -; and tzintzin - small similar elements. And its complete meaning was taken as: counting with small similar elements by somebody. Its use was taught in the "Calmecac" to the "temalpouhqueh" , who were students dedicated to take the accounts of skies, from childhood. Unfortunately the Nepōhualtzintzin and its teaching were among the victims of the conquering destruction, when a diabolic origin was attributed to them after observing the tremendous properties of representation, precision and speed of calculations.
For the Aztec the count by 20s was completely natural. The amount of 4, 5, 13, 20 and other cyclees meant cycles. The Nepōhualtzintzin was divided in two main parts separated by a bar or intermediate cord. In the left part there were four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.
Altogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nepōhualtzintzin. This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens. One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximate a year (1 days short). The Nepōhualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed, when translated into modern computer arithmetic.
The rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his wanderings throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them made in gold, jade, encrustations of shell, etc. There have also been found very old Nepōhualtzintzin attributed to the Olmeca culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.
George I. Sanchez, "Arithmetic in Maya", Austin-Texas, 1961 found another base 5, base 4 abacus in the Yucatán that also computed calendar data. This was a finger abacus, on one hand 0 1,2, 3, and 4 were used; and on the other hand used 0, 1, 2 and 3 were used. Note the use of zero at the beginning and end of the two cycles. Sanchez worked with Sylvanus Morley, a noted Mayanist.
The quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (Quechua for "counting tool"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed by Italian mathematician Nicolino De Pasquale. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at minimum.
Russian.
The Russian abacus, the "schoty" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire, usually positioned near the user, with four beads for quarter-ruble fractions). Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916. The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different colour from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.
As a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. Today it is regarded as an archaism and replaced by the handheld calculator.
The Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid. The Turks and the Armenian people also used abaci similar to the Russian schoty. It was named a "coulba" by the Turks and a "choreb" by the Armenians.
School abacus.
Around the world, abaci have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.
In Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy.
This type of abacus uses a row of 10 beads to represent arithmetical columns; thus the top row represents units, the second, tens, the third, hundreds, and so on. Most of these abaci consist of 10 rows, thus count up to 11,111,111,110.
The red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons. The twenty bead version, referred to by its Dutch name rekenrek, is often used, sometimes on a string of beads, sometimes on a rigid framework.
Uses by the blind.
An adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cubic root.
Although blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.
Binary abacus.
The binary abacus is used to explain how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an 'on' or 'off' position.

</doc>
<doc id="656" url="http://en.wikipedia.org/wiki?curid=656" title="Acid">
Acid

An acid (from the Latin "acidus/acēre" meaning "sour") is a chemical substance whose aqueous solutions are characterized by a sour taste, the ability to turn blue litmus red, and the ability to react with bases and certain metals (like calcium) to form salts. Aqueous solutions of acids have a pH of less than 7. A lower pH means a higher acidity, and thus a higher concentration of positive hydrogen ions in the solution. Chemicals or substances having the property of an acid are said to be acidic.
There are three common definitions for acids: the Arrhenius definition, the Brønsted-Lowry definition, and the Lewis definition. The Arrhenius definition defines acids as substances which increase the concentration of hydrogen ions (H+), or more accurately, hydronium ions (H3O+), when dissolved in water. The Brønsted-Lowry definition is an expansion: an acid is a substance which can act as a proton donor. By this definition, any compound which can easily be deprotonated can be considered an acid. Examples include alcohols and amines which contain O-H or N-H fragments. A Lewis acid is a substance that can accept a pair of electrons to form a covalent bond. Examples of Lewis acids include all metal cations, and electron-deficient molecules such as boron trifluoride and aluminium trichloride.
Common examples of acids include hydrochloric acid (a solution of hydrogen chloride which is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute solution of this liquid), sulfuric acid (used in car batteries), and tartaric acid (a solid used in baking). As these examples show, acids can be solutions or pure substances, and can be derived from solids, liquids, or gases. Strong acids and some concentrated weak acids are corrosive, but there are exceptions such as carboranes and boric acid.
Definitions and concepts.
Modern definitions are concerned with the fundamental chemical reactions common to all acids.
Most acids encountered in everyday life are aqueous solutions, or can be dissolved in water, so the Arrhenius and Brønsted-Lowry definitions are the most relevant.
The Brønsted-Lowry definition is the most widely used definition; unless otherwise specified, acid-base reactions are assumed to involve the transfer of a proton (H+) from an acid to a base.
Hydronium ions are acids according to all three definitions. Interestingly, although alcohols and amines can be Brønsted-Lowry acids, they can also function as Lewis bases due to the lone pairs of electrons on their oxygen and nitrogen atoms.
Arrhenius acids.
The Swedish chemist Svante Arrhenius attributed the properties of acidity to hydrogen ions (H+) or protons in 1884. An Arrhenius acid is a substance that, when added to water, increases the concentration of H+ ions in the water. Note that chemists often write H+("aq") and refer to the hydrogen ion when describing acid-base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion, H3O+. Thus, an Arrhenius acid can also be described as a substance that increases the concentration of hydronium ions when added to water. This definition stems from the equilibrium dissociation of water into hydronium and hydroxide (OH−) ions:
In pure water the majority of molecules are H2O, but the molecules are constantly dissociating and re-associating, and at any time a small number of the molecules (always near 1 in 107) are hydronium and an equal number are hydroxide. Because the numbers are equal, pure water is neutral (not acidic or basic).
An Arrhenius base, on the other hand, is a substance which increases the concentration of hydroxide ions when dissolved in water, hence decreasing the concentration of hydronium.
The constant association and disassociation of H2O molecules forms an equilibrium in which any increase in the concentration of hydronium is accompanied by a decrease in the concentration of hydroxide, thus an Arrhenius acid could also be said to be one that decreases hydroxide concentration, with an Arrhenius base increasing it.
In an acid, the concentration of hydronium ions is greater than 10−7 moles per liter. Since pH is defined as the negative logarithm of the concentration of hydronium ions, acids thus have a pH of less than 7.
Brønsted-Lowry acids.
While the Arrhenius concept is useful for describing many reactions, it is also quite limited in its scope. In 1923 chemists Johannes Nicolaus Brønsted and Thomas Martin Lowry independently recognized that acid-base reactions involve the transfer of a proton. A Brønsted-Lowry acid (or simply Brønsted acid) is a species that donates a proton to a Brønsted-Lowry base. Brønsted-Lowry acid-base theory has several advantages over Arrhenius theory. Consider the following reactions of acetic acid (CH3COOH), the organic acid that gives vinegar its characteristic taste:
Both theories easily describe the first reaction: CH3COOH acts as an Arrhenius acid because it acts as a source of H3O+ when dissolved in water, and it acts as a Brønsted acid by donating a proton to water. In the second example CH3COOH undergoes the same transformation, in this case donating a proton to ammonia (NH3), but cannot be described using the Arrhenius definition of an acid because the reaction does not produce hydronium.
Brønsted-Lowry theory can also be used to describe molecular compounds, whereas Arrhenius acids must be ionic compounds. Hydrogen chloride (HCl) and ammonia combine under several different conditions to form ammonium chloride, NH4Cl. In aqueous solution HCl behaves as hydrochloric acid and exists as hydronium and chloride ions. The following reactions illustrate the limitations of Arrhenius's definition:
As with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed by the HCl solute. The next two reactions do not involve the formation of ions but are still proton transfer reactions. In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH3 combine to form the solid.
Lewis acids.
A third concept was proposed in 1923 by Gilbert N. Lewis which includes reactions with acid-base characteristics that do not involve a proton transfer. A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor. Brønsted acid-base reactions are proton transfer reactions while Lewis acid-base reactions are electron pair transfers. All Brønsted acids are also Lewis acids, but not all Lewis acids are Brønsted acids. Contrast how the following reactions are described in terms of acid-base chemistry.
In the first reaction a fluoride ion, F−, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate. Fluoride "loses" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion. BF3 is a Lewis acid because it accepts the electron pair from fluoride. This reaction cannot be described in terms of Brønsted theory because there is no proton transfer. The second reaction can be described using either theory. A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion. The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in H3O+ gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen. Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile.
Dissociation and equilibrium.
Reactions of acids are often generalized in the form HA H+ + A−, where HA represents the acid and A− is the conjugate base. Acid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA+ H+ + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant "K" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [H2O] means "the concentration of H2O". The acid dissociation constant "K"a is generally used in the context of acid-base reactions. The numerical value of "K"a is equal to the product of the concentrations of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H+.
The stronger of two acids will have a higher "K"a than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton. Because the range of possible values for "K"a spans many orders of magnitude, a more manageable constant, p"K"a is more frequently used, where p"K"a = -log10 "K"a. Stronger acids have a smaller p"K"a than weaker acids. Experimentally determined p"K"a at 25 °C in aqueous solution are often quoted in textbooks and reference material.
Nomenclature.
In the classical naming system, acids are named according to their anions. That ionic suffix is dropped and replaced with a new suffix (and sometimes prefix), according to the table below.
For example, HCl has chloride as its anion, so the -ide suffix makes it take the form hydrochloric acid. In the IUPAC naming system, "aqueous" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, the IUPAC name would be aqueous hydrogen chloride. The prefix "hydro-" is added only if the acid is made up of just hydrogen and one other element.
Classical naming system:
Acid strength.
The strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H+ and one mole of the conjugate base, A−, and none of the protonated acid HA. In contrast, a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H+. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base.
Stronger acids have a larger "K"a and a more negative p"K"a than weaker acids.
Sulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable.
Superacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. Superacids can permanently protonate water to give ionic, crystalline hydronium "salts". They can also quantitatively stabilize carbocations.
While "K"a measures the strength of an acid compound, the strength of an aqueous acid solution is measured by pH, which is an indication of the concentration of hydronium in the solution. The pH of a simple solution of an acid compound in water is determined by the dilution of the compound and the compound's "K"a.
Chemical characteristics.
Monoprotic acids.
Monoprotic acids are those acids that are able to donate one proton per molecule during the process of dissociation (sometimes called ionization) as shown below (symbolized by HA):
Common examples of monoprotic acids in mineral acids include hydrochloric acid (HCl) and nitric acid (HNO3). On the other hand, for organic acids the term mainly indicates the presence of one carboxylic acid group and sometimes these acids are known as monocarboxylic acid. Examples in organic acids include formic acid (HCOOH), acetic acid (CH3COOH) and benzoic acid (C6H5COOH).
Polyprotic acids.
Polyprotic acids, also known as polybasic acids, are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule. Specific types of polyprotic acids have more specific names, such as diprotic acid (two potential protons to donate) and triprotic acid (three potential protons to donate).
A diprotic acid (here symbolized by H2A) can undergo one or two dissociations depending on the pH. Each dissociation has its own dissociation constant, Ka1 and Ka2.
The first dissociation constant is typically greater than the second; i.e., "K"a1 > "K"a2. For example, sulfuric acid (H2SO4) can donate one proton to form the bisulfate anion (HSO4−), for which "K"a1 is very large; then it can donate a second proton to form the sulfate anion (SO42-), wherein the "K"a2 is intermediate strength. The large "K"a1 for the first dissociation makes sulfuric a strong acid. In a similar manner, the weak unstable carbonic acid (H2CO3) can lose one proton to form bicarbonate anion (HCO3−) and lose a second to form carbonate anion (CO32-). Both "K"a values are small, but "K"a1 > "K"a2 .
A triprotic acid (H3A) can undergo one, two, or three dissociations and has three dissociation constants, where "K"a1 > "K"a2 > "K"a3.
An inorganic example of a triprotic acid is orthophosphoric acid (H3PO4), usually just called phosphoric acid. All three protons can be successively lost to yield H2PO4−, then HPO42-, and finally PO43-, the orthophosphate ion, usually just called phosphate. An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion. Even though the positions of the protons on the original molecule may be equivalent, the successive "K"a values will differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged.
Although the subsequent loss of each hydrogen ion is less favorable, all of the conjugate bases are present in solution. The fractional concentration, "α" (alpha), for each species can be calculated. For example, a generic diprotic acid will generate 3 species in solution: H2A, HA-, and A2-. The fractional concentrations can be calculated as below when given either the pH (which can be converted to the [H+]) or the concentrations of the acid with all its conjugate bases:
A plot of these fractional concentrations against pH, for given "K"1 and "K"2, is known as a Bjerrum plot. A pattern is observed in the above equations and can be expanded to the general "n" -protic acid that has been deprotonated "i" -times:
where K0 = 1 and the other K-terms are the dissociation constants for the acid.
Neutralization.
Neutralization is the reaction between an acid and a base, producing a salt and neutralized base; for example, hydrochloric acid and sodium hydroxide form sodium chloride and water:
Neutralization is the basis of titration, where a pH indicator shows equivalence point when the equivalent number of moles of a base have been added to an acid. It is often wrongly assumed that neutralization should result in a solution with pH 7.0, which is only the case with similar acid and base strengths during a reaction.
Neutralization with a base weaker than the acid results in a weakly acidic salt. An example is the weakly acidic ammonium chloride, which is produced from the strong acid hydrogen chloride and the weak base ammonia. Conversely, neutralizing a weak acid with a strong base gives a weakly basic salt, e.g. sodium fluoride from hydrogen fluoride and sodium hydroxide.
Weak acid–weak base equilibrium.
In order for a protonated acid to lose a proton, the pH of the system must rise above the p"K"a of the acid. The decreased concentration of H+ in that basic solution shifts the equilibrium towards the conjugate base form (the deprotonated form of the acid). In lower-pH (more acidic) solutions, there is a high enough H+ concentration in the solution to cause the acid to remain in its protonated form.
Solutions of weak acids and salts of their conjugate bases form buffer solutions.
Applications of acids.
There are numerous uses for acids. Acids are often used to remove rust and other corrosion from metals in a process known as pickling. They may be used as an electrolyte in a wet cell battery, such as sulfuric acid in a car battery.
Strong acids, sulfuric acid in particular, are widely used in mineral processing. For example, phosphate minerals react with sulfuric acid to produce phosphoric acid for the production of phosphate fertilizers, and zinc is produced by dissolving zinc oxide into sulfuric acid, purifying the solution and electrowinning.
In the chemical industry, acids react in neutralization reactions to produce salts. For example, nitric acid reacts with ammonia to produce ammonium nitrate, a fertilizer. Additionally, carboxylic acids can be esterified with alcohols, to produce esters.
Acids are used as additives to drinks and foods, as they alter their taste and serve as preservatives. Phosphoric acid, for example, is a component of cola drinks. Acetic acid is used in day-to-day life as vinegar. Carbonic acid is an important part of some cola drinks and soda. Citric acid is used as a preservative in sauces and pickles.
Tartaric acid is an important component of some commonly used foods like unripened mangoes and tamarind. Natural fruits and vegetables also contain acids. Citric acid is present in oranges, lemon and other citrus fruits. Oxalic acid is present in tomatoes, spinach, and especially in carambola and rhubarb; rhubarb leaves and unripe carambolas are toxic because of high concentrations of oxalic acid.
Ascorbic acid (Vitamin C) is an essential vitamin for the human body and is present in such foods as amla, lemon, citrus fruits, and guava.
Certain acids are used as drugs. Acetylsalicylic acid (Aspirin) is used as a pain killer and for bringing down fevers.
Acids play important roles in the human body. The hydrochloric acid present in the stomach aids in digestion by breaking down large and complex food molecules. Amino acids are required for synthesis of proteins required for growth and repair of body tissues. Fatty acids are also required for growth and repair of body tissues. Nucleic acids are important for the manufacturing of DNA and RNA and transmitting of traits to offspring through genes. Carbonic acid is important for maintenance of pH equilibrium in the body.
Acid catalysis.
Acids are used as catalysts in industrial and organic chemistry; for example, sulfuric acid is used in very large quantities in the alkylation process to produce gasoline. Strong acids, such as sulfuric, phosphoric and hydrochloric acids also effect dehydration and condensation reactions. In biochemistry, many enzymes employ acid catalysis.
Biological occurrence.
Many biologically important molecules are acids. Nucleic acids, which contain acidic phosphate groups, include DNA and RNA. Nucleic acids contain the genetic code that determines many of an organism's characteristics, and is passed from parents to offspring. DNA contains the chemical blueprint for the synthesis of proteins which are made up of amino acid subunits. Cell membranes contain fatty acid esters such as phospholipids.
An α-amino acid has a central carbon (the α or "alpha" carbon) which is covalently bonded to a carboxyl group (thus they are carboxylic acids), an amino group, a hydrogen atom and a variable group. The variable group, also called the R group or side chain, determines the identity and many of the properties of a specific amino acid. In glycine, the simplest amino acid, the R group is a hydrogen atom, but in all other amino acids it is contains one or more carbon atoms bonded to hydrogens, and may contain other elements such as sulfur, oxygen or nitrogen. With the exception of glycine, naturally occurring amino acids are chiral and almost invariably occur in the . Peptidoglycan, found in some bacterial cell walls contains some -amino acids. At physiological pH, typically around 7, free amino acids exist in a charged form, where the acidic carboxyl group (-COOH) loses a proton (-COO−) and the basic amine group (-NH2) gains a proton (-NH3+). The entire molecule has a net neutral charge and is a zwitterion, with the exception of amino acids with basic or acidic side chains. Aspartic acid, for example, possesses one protonated amine and two deprotonated carboxyl groups, for a net charge of −1 at physiological pH.
Fatty acids and fatty acid derivatives are another group of carboxylic acids that play a significant role in biology. These contain long hydrocarbon chains and a carboxylic acid group on one end. The cell membrane of nearly all organisms is primarily made up of a phospholipid bilayer, a micelle of hydrophobic fatty acid esters with polar, hydrophilic phosphate "head" groups. Membranes contain additional components, some of which can participate in acid-base reactions.
In humans and many other animals, hydrochloric acid is a part of the gastric acid secreted within the stomach to help hydrolyze proteins and polysaccharides, as well as converting the inactive pro-enzyme, pepsinogen into the enzyme, pepsin. Some organisms produce acids for defense; for example, ants produce formic acid.
Acid-base equilibrium plays a critical role in regulating mammalian breathing. Oxygen gas (O2) drives cellular respiration, the process by which animals release the chemical potential energy stored in food, producing carbon dioxide (CO2) as a byproduct. Oxygen and carbon dioxide are exchanged in the lungs, and the body responds to changing energy demands by adjusting the rate of ventilation. For example, during periods of exertion the body rapidly breaks down stored carbohydrates and fat, releasing CO2 into the blood stream. In aqueous solutions such as blood CO2 exists in equilibrium with carbonic acid and bicarbonate ion.
It is the decrease in pH that signals the brain to breathe faster and deeper, expelling the excess CO2 and resupplying the cells with O2.
 Cell membranes are generally impermeable to charged or large, polar molecules because of the lipophilic fatty acyl chains comprising their interior. Many biologically important molecules, including a number of pharmaceutical agents, are organic weak acids which can cross the membrane in their protonated, uncharged form but not in their charged form (i.e. as the conjugate base). For this reason the activity of many drugs can be enhanced or inhibited by the use of antacids or acidic foods. The charged form, however, is often more soluble in blood and cytosol, both aqueous environments. When the extracellular environment is more acidic than the neutral pH within the cell, certain acids will exist in their neutral form and will be membrane soluble, allowing them to cross the phospholipid bilayer. Acids that lose a proton at the intracellular pH will exist in their soluble, charged form and are thus able to diffuse through the cytosol to their target. Ibuprofen, aspirin and penicillin are examples of drugs that are weak acids.
Common acids.
Halogenated carboxylic acids.
Halogenation at alpha position increases acid strength, so that the following acids are all stronger than acetic acid.
Vinylogous carboxylic acids.
Normal carboxylic acids are the direct union of a carbonyl group and a hydroxy group. In vinylogous carboxylic acids, a carbon-carbon double bond separates the carbonyl and hydroxyl groups.

</doc>
<doc id="657" url="http://en.wikipedia.org/wiki?curid=657" title="Asphalt">
Asphalt

Asphalt ( or ), also known as bitumen (), is a sticky, black and highly viscous liquid or semi-solid form of petroleum. It may be found in natural deposits or may be a refined product; it is a substance classed as a pitch. Until the 20th century, the term asphaltum was also used.
The primary use (70%) of asphalt/bitumen is in road construction, where it is used as the glue or binder mixed with aggregate particles to create asphalt concrete. Its other main uses are for bituminous waterproofing products, including production of roofing felt and for sealing flat roofs.
The terms "asphalt" and "bitumen" are often used interchangeably to mean both natural and manufactured forms of the substance. In American English, asphalt (or asphalt cement) is the carefully refined residue from the distillation process of selected crude oils. Outside the United States, the product is often called bitumen. Geological terminology often prefers the term "bitumen". Common usage often refers to various forms of asphalt/bitumen as "tar", such as at the La Brea Tar Pits. Another term, mostly archaic, refers to asphalt/bitumen as "pitch".
Naturally occurring asphalt/bitumen is sometimes specified by the term "crude bitumen". Its viscosity is similar to that of cold molasses while the material obtained from the fractional distillation of crude oil [boiling at is sometimes referred to as "refined bitumen".
Composition.
The components of asphalt are classified into four classes of compounds: 
The naphthene aromatics and polar aromatics are typically the majority components. Additionally, most natural bitumens contain organosulfur compounds, resulting in an overall sulfur content of up to 4%. Nickel and vanadium are found in the <10 ppm level, as is typical of some petroleum.
The substance is soluble in carbon disulfide. It is commonly modelled as a colloid, with asphaltenes as the dispersed phase and as the continuous phase. and "it is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large".
Asphalt/bitumen can sometimes be confused with "coal tar", which is a visually similar black, thermoplastic material produced by the destructive distillation of coal. During the early and mid-20th century when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates. The addition of tar to macadam roads led to the word tarmac, which is now used in common parlance to refer to road-making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt/bitumen has completely overtaken the use of coal tar in these applications. Other examples of this confusion include the asphalt/bitumen of the La Brea Tar Pits and the Canadian oil sands. Pitch is another term sometimes used at times to refer to asphalt/bitumen, as in Pitch Lake.
Occurrence.
The great majority of asphalt used commercially is obtained from petroleum. Nonetheless, large amounts of asphalt occur in concentrated form in nature. Naturally occurring deposits of asphalt/bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things. These remains were deposited in the mud on the bottom of the ocean or lake where the organisms lived. Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as asphalt/bitumen, kerogen, or petroleum. Deposits at the La Brea Tar Pits are an example.
Natural deposits of asphalt/bitumen include lakes such as the Pitch Lake in Trinidad and Tobago and Lake Bermudez in Venezuela. Natural seeps of asphalt/bitumen occur in the La Brea Tar Pits and in the Dead Sea.
Asphalt/bitumen also occurs as impregnated sandstones known as bituminous rock and the similar "tar sands" such as in Athabasca, Canada and Utah, USA. The Athabasca tar sands are located in the McMurray Formation, Alberta. This Formation is of early Cretaceous age, and is composed of numerous lenses of oil-bearing sand with up to 20% oil. Isotopic studies attribute the oil deposits to be about 110 Ma old. Heavy oil or bitumen deposits also occur in the Uinta Basin in Utah, USA. The Tar Sand Triangle deposit, for example, is roughly 6% bitumen.
Asphalt/bitumen occurs in hydrothermal veins. An example of this is within the Uinta Basin of Utah, USA, where there is a swarm of laterally and vertically extensive veins composed of a solid hydrocarbon termed Gilsonite. These veins formed by the polymerisation and solidification of hydrocarbons that were mobilized from the deeper oil shales of the Green River Formation during burial and diagenesis.
Asphalt/bitumen is similar to the organic matter in carbonaceous meteorites. However, detailed studies have shown these materials to be distinct.
History.
Ancient times.
The Bible mentions at Genesis 6:14 that Noah used bitumen for Ark waterproofing in 2370 B.C.E.
The use of asphalt/bitumen for waterproofing and as an adhesive dates at least to the fifth millennium B.C. in the early Indus community of Mehrgarh, where it was used to line the baskets in which they gathered crops.
In the ancient Middle East, the Sumerians used natural asphalt/bitumen deposits for mortar between bricks and stones, to cement parts of carvings, such as eyes, into place, for ship caulking, and for waterproofing. The Greek historian Herodotus said hot asphalt/bitumen was used as mortar in the walls of Babylon.
In some versions of the Book of Genesis in the Bible, the name of the substance used to bind the bricks of the Tower of Babel is translated as bitumen (see Gen 11:3), while other translations use the word "pitch". A one-kilometre tunnel beneath the river Euphrates at Babylon in the time of Queen Semiramis (ca. 800 B.C.) was reportedly constructed of burnt bricks covered with asphalt/bitumen as a waterproofing agent.
Asphalt/bitumen was used by ancient Egyptians to embalm mummies. The Persian word for asphalt is "moom", which is related to the English word mummy. The Egyptians' primary source of asphalt/bitumen was the Dead Sea, which the Romans knew as "Palus Asphaltites" (Asphalt Lake).
Approximately 40 AD, Dioscorides described the Dead Sea material as "Judaicum bitumen", and noted other places in the region where it could be found: 
"The Judaicum Bitumen is better than others; that is reckoned the best, which doth shine like purple, being of a strong scent & weightie, but the black and fowle is naught for it is adulterated with Pitch mixed with it. It growes in Phoenice also, and in Sidon, & in Babylon, & in Zacynthum. It is found also moyst swimming upon wells in the country of the Agrigentines of Sicilie, which they use for lamps instead of oyle, and which they call falsely Sicilian oyle, for it is a kinde of moyst Bitumen."
The Sidon bitumen is thought to refer to asphalt/bitumen found at Hasbeya. Pliny refers also to asphalt/bitumen being found in Epirus. It was a valuable strategic resource; the object of the first known battle for a hydrocarbon deposit, between the Seleucids and the Nabateans in 312 B.C.
In the ancient Far East, natural asphalt/bitumen was slowly boiled to get rid of the higher fractions, leaving a material of higher molecular weight which is thermoplastic and when layered on objects, became quite hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.
In North America, archaeological recovery has indicated asphalt/bitumen was sometimes used to adhere stone projectile points to wooden shafts.
Early use in Europe.
100 years after the fall of Constantinople in 1453, Pierre Belon described in his work "Observations" in 1553 that "pissasphalto", a mixture of pitch and bitumen, was used in Dubrovnik for tarring of ships from where it was exported to a market place in Venice where it could be bought by anyone.
An 1838 edition of "Mechanics Magazine" cites an early use of asphalt in France. A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways – "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth", which at that time made the water unusable. "He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation". But it was generally neglected in France until the revolution of 1830. Then, in the 1830s, there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes". Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (BasRhin), the Parc (l'Ain) and the Puy-de-la-Poix (Puy-de-Dome)", although it could also be made artificially. One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.
Photography and art.
Asphalt/bitumen was used in early photographic technology. In 1826 or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature. The asphalt/bitumen was thinly coated onto a pewter plate which was then exposed in a camera. Exposure to light hardened the asphalt/bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained. Many hours of exposure in the camera were required, making asphalt/bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.
Asphalt/bitumen was the nemesis of many artists during the 19th century. Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common dilutents, such as linseed oil, varnish and turpentine. Unless thoroughly diluted, asphalt/bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact. The use of asphalt/bitumen as a glaze to set in shadow or mixed with other colours to render a darker tone resulted in the eventual deterioration of a good many paintings, those of Delacroix being just one notable example. Perhaps the most famous example of the destructiveness of asphalt/bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of asphalt/bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.
Early use in the United Kingdom.
Among the earlier uses of asphalt/bitumen in the United Kingdom was for etching. William Salmon's "Polygraphice" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum. By the fifth edition in 1685, he had included more asphaltum recipes from other sources.
The first British patent for the use of asphalt/bitumen was Cassell's patent asphalte or bitumen' in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson claims that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)". Indeed, mastic pavements had been previously employed at Vauxhall by a competitor of Claridge, but without success.
In 1838, Claridge obtained patents in Scotland on 27 March, and Ireland on 23 April, and in 1851 extensions were sought for all three patents, by the trustees of a company previously formed by Claridge. This was "Claridge's Patent Asphalte Company", formed in 1838 for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France", and "laid one of the first asphalt pavements in Whitehall". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park". "The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry". "By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order".
Indeed in 1838, there was a flurry of entrepreneurial activity over asphalt/bitumen, which had uses beyond paving. For example, asphalt could also used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, with these latter themselves proliferating in the 19th century. On the London stockmarket, there were various claims as to the exclusivity of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, "Claridge's was the type most used in the 1840s and 50s"
In 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd. Two products resulted, namely "Clarmac", and "Clarphalte", with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although "Clarmac" was more widely used. However, the First World War impacted financially on the Clarmac Company, which entered into liquidation in 1915. The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset, and in a subsequent attempt to save the Clarmac Company.
Early use in the United States.
The first use of asphalt/bitumen in the New World was by indigenous peoples. On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring asphalt/bitumen that seeped to the surface above underlying petroleum deposits. All three used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphaltum to provide decorations. It was used as a sealant on baskets to make them watertight for carrying water. Asphaltum was used also to seal the planks on ocean-going canoes.
Roads in the US have been paved with materials that include asphalt/bitumen since at least 1870, when a street in front of the Newark, NJ City Hall was paved. In many cases, these early pavings were made from naturally occurring "bituminous rock", such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873. In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington, DC, in time for the celebration of the national centennial. Asphalt/bitumen was also used for flooring, paving and waterproofing of baths and swimming pools during the early 20th century, following similar trends in Europe.
Modern use.
Rolled asphalt concrete.
The largest use of asphalt/bitumen is for making asphalt concrete for road surfaces and accounts for approximately 85% of the asphalt consumed in the United States. Asphalt concrete pavement material is commonly composed of 5% asphalt/bitumen cement and 95% aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt/bitumen cement must be heated so it can be mixed with the aggregates at the asphalt mixing plant. There are about 4,000 asphalt concrete mixing plants in the U.S., and a similar number in Europe.
Asphalt concrete road surface is the most widely recycled material in the U.S., both by gross tonnage and by percentage. According to an industry survey conducted by the Federal Highway Administration and the National Asphalt Pavement Association and released in 2011, more than 99% of the asphalt removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments.
Roofing shingles account for most of the remaining asphalt/bitumen consumption. Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics.
Asphalt concrete paving is widely used in airports around the world. Due to the sturdiness and ability to be repaired quickly, it is widely used for runways dedicated to aircraft landing and taking off.
Mastic asphalt.
Mastic asphalt is a type of asphalt which differs from dense graded asphalt (asphalt concrete) in that it has a higher asphalt/bitumen (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% added asphalt/bitumen. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form an impervious barrier about thick.
Asphalt emulsion.
A number of technologies allow asphalt/bitumen to be mixed at much lower temperatures. These involve mixing with petroleum solvents to form "cutbacks" with reduced melting point, or mixtures with water to turn the asphalt/bitumen into an emulsion. Asphalt emulsions contain up to 70% asphalt/bitumen and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock, gravel or crushed slag. Slurry seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold-mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth and asphalt emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements.
Other uses.
Asphalt/bitumen is used to make Japan black, a lacquer known especially for its use on iron and steel. Asphalt/bitumen also is used in paint and marker inks by some graffiti supply companies (primarily Molotow) to increase the weather resistance and permanence of the paint and/or ink, and to make the color much darker. Asphalt/bitumen is also used to seal some alkaline batteries during the manufacturing process.
Production.
About 40,000,000 tons were produced in 1984. It is obtained as the "heavy" (i.e., difficult to distill) fraction. Material with a boiling point greater than around 500 °C is considered asphalt. Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel). The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications. In a de-asphalting unit, the crude asphalt is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated. Further processing is possible by "blowing" the product: namely reacting it with oxygen. This step makes the product harder and more viscous.
Asphalt/bitumen is typically stored and transported at temperatures around . Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called "bitumen feedstock", or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt/bitumen, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is no longer used as a release agent due to environmental concerns.
From oil sands.
Naturally occurring crude asphalt/bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from "Oil sands", currently under development in Alberta, Canada. Canada has most of the world's supply of natural asphalt/bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world. The Athabasca oil sands is the largest asphalt/bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by "in situ" methods. Because of oil price increases since 2003, upgrading bitumen to synthetic crude oil has become highly profitable. As of 2006, Canadian crude asphalt/bitumen production averaged about per day and was projected to rise to per day by 2020. The total amount of crude asphalt/bitumen in Alberta which could be extracted is estimated to be about , which at a rate of would last about 200 years.
Alternatives and bioasphalt.
Although uncompetitive economically, asphalt/bitumen can be made from nonpetroleum-based renewable resources such as sugar, molasses and rice, corn and potato starches. Asphalt/bitumen can also be made from waste material by fractional distillation of used motor oils, which is sometimes disposed by burning or dumping into landfills; use of these results in premature cracking in colder climates, resulting in roads that need to be repaved more frequently. Nonpetroleum-based asphalt/bitumen binders can be made light-colored. Lighter-colored roads absorb less heat from solar radiation, and have less surface heat than darker surfaces, reducing their contribution to the urban heat island effect.
Etymology.
The word "asphalt" is derived from the late Middle English, in turn from French "asphalte", based on Late Latin "asphalton", "asphaltum", which is the latinisation of the Greek ἄσφαλτος ("ásphaltos", "ásphalton"), a word meaning "asphalt/bitumen/pitch", which perhaps derives from ἀ-, "without" and σφάλλω ("sfallō"), "make fall". Note that in French, the term "asphalte" is used for naturally occurring bitumen-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads. It is a significant fact that the first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. Specifically Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall. From the Greek, the word passed into late Latin, and thence into French ("asphalte") and English ("asphaltum" and "asphalt").
The expression "bitumen" originated in the Sanskrit, where we find the words "jatu", meaning "pitch," and "jatu-krit", meaning "pitch creating", "pitch producing" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally "gwitu-men" (pertaining to pitch), and by others, "pixtumens" (exuding or bubbling pitch), which was subsequently shortened to "bitumen", thence passing via French into English. From the same root is derived the Anglo Saxon word "cwidu" (mastix), the German word "Kitt" (cement or mastic) and the old Norse word "kvada".
Neither of the terms asphalt or bitumen should be confused with tar or coal tars.
Modern usage.
In British English, the word 'asphalt' is used to refer to a mixture of mineral aggregate and asphalt/bitumen (also called tarmac in common parlance). The earlier word 'asphaltum' is now archaic and not commonly used. In American English, 'asphalt' is equivalent to the British 'bitumen'. However, 'asphalt' is also commonly used as a shortened form of 'asphalt concrete' (therefore equivalent to the British 'asphalt' or 'tarmac'). In Australian English, bitumen is often used as the generic term for road surfaces. In Canadian English, the word bitumen is used to refer to the vast Canadian deposits of extremely heavy crude oil, while asphalt is used for the oil refinery product used to pave roads and manufacture roof shingles and various waterproofing products. Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as dilbit in the Canadian petroleum industry, while bitumen "upgraded" to synthetic crude oil is known as syncrude and syncrude blended with bitumen as synbit.
Bitumen is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum. Bituminous rock is a form of sandstone impregnated with bitumen. The tar sands of Alberta, Canada are a similar material.

</doc>
<doc id="659" url="http://en.wikipedia.org/wiki?curid=659" title="American National Standards Institute">
American National Standards Institute

The American National Standards Institute (ANSI, ) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide. For example, standards ensure that people who own cameras can find the film they need for that camera anywhere around the globe.
ANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.
The organization's headquarters are in Washington, DC. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.
History.
ANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became the United States of America Standards Institute (USASI). The present name was adopted in 1969.
Prior to 1918, these five founding engineering societies:
had been members of the United Engineering Society (UES).
At the behest of the AIEE, they invited the U.S. government Departments of War, Navy (combined in 1947 to become the Department of Defense or DOD) and Commerce to join in founding a national standards organization.
According to Paul G. Agnew, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies.
In 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.
Members.
ANSI's membership comprises government agencies, organizations, corporations, academic and international bodies, and individuals. In total, the Institute represents the interests of more than 125,000 companies and 3.5 million professionals.
Process.
Though ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process.
ANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.
Voluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.
The American National Standards process involves:
International activities.
In addition to facilitating the formation of standards in the U.S., ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.
The Institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO), as a founding member, and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.
Standards panels.
The Institute administers nine standards panels:
Each of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.
In 2009, ANSI and the National Institute for Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC). NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.

</doc>
<doc id="661" url="http://en.wikipedia.org/wiki?curid=661" title="Argument (disambiguation)">
Argument (disambiguation)

In philosophy and logic, an argument is an attempt to persuade someone of something, or give evidence or reasons for accepting a particular conclusion. 
Argument may also refer to: 

</doc>
<doc id="662" url="http://en.wikipedia.org/wiki?curid=662" title="Apollo 11">
Apollo 11

Apollo 11 was the spaceflight that landed the first humans on the Moon, Americans Neil Armstrong and Buzz Aldrin, on July 20, 1969, at 20:18 UTC. Armstrong became the first to step onto the lunar surface six hours later on July 21 at 02:56 UTC. Armstrong spent about two and a half hours outside the spacecraft, Aldrin slightly less, and together they collected of lunar material for return to Earth. The third member of the mission, Michael Collins, piloted the command spacecraft alone in lunar orbit until Armstrong and Aldrin returned to it just under a day later for the trip back to Earth.
Launched by a Saturn V rocket from Kennedy Space Center in Merritt Island, Florida, on July 16, Apollo 11 was the fifth manned mission of NASA's Apollo program. The Apollo spacecraft had three parts: a Command Module (CM) with a cabin for the three astronauts, and the only part that landed back on Earth; a Service Module (SM), which supported the Command Module with propulsion, electrical power, oxygen, and water; and a Lunar Module (LM) for landing on the Moon. After being sent toward the Moon by the Saturn V's upper stage, the astronauts separated the spacecraft from it and traveled for three days until they entered into lunar orbit. Armstrong and Aldrin then moved into the Lunar Module and landed in the Sea of Tranquility. They stayed a total of about 21½ hours on the lunar surface. After lifting off in the upper part of the Lunar Module and rejoining Collins in the Command Module, they returned to Earth and landed in the Pacific Ocean on July 24.
Broadcast on live TV to a world-wide audience, Armstrong stepped onto the lunar surface and described the event as "one small step for [a] man, one giant leap for mankind." Apollo 11 effectively ended the Space Race and fulfilled a national goal proposed in 1961 by the U.S. President John F. Kennedy in a speech before the U.S. Congress: "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
Framework.
Crew.
Each crewman of Apollo 11 had made a spaceflight before this mission, making it only the second all-veteran crew (the other being Apollo 10) in human spaceflight history.
Collins was originally slated to be the Command Module Pilot (CMP) on Apollo 8 but was removed when he required surgery on his back and was replaced by Jim Lovell, his backup for that flight. After Collins was medically cleared, he took what would have been Lovell's spot on Apollo 11; as a veteran of Apollo 8, Lovell was transferred to Apollo 11's backup crew, but promoted to backup commander.
Backup crew.
In early 1969, Anders accepted a job with the National Space Council effective August 1969 and announced that he would retire as an astronaut on that date. At that point Ken Mattingly was moved from the support crew into parallel training with Anders as backup Command Module Pilot in case Apollo 11 was delayed past its intended July launch (at which point Anders would be unavailable if needed) and would later join Lovell's crew and ultimately be assigned as the original Apollo 13 CMP.
Call signs.
After the crew of Apollo 10 named their spacecraft "Charlie Brown" and "Snoopy", assistant manager for public affairs Julian Scheer wrote to Manned Spacecraft Center director George M. Low to suggest the Apollo 11 crew be less flippant in naming their craft. During early mission planning, the names "Snowcone" and "Haystack" were used and put in the news release, but the crew later decided to change them.
The Command Module was named "Columbia" after the "Columbiad", the giant cannon shell "spacecraft" fired by a giant cannon (also from Florida) in Jules Verne's 1865 novel "From the Earth to the Moon". The Lunar Module was named "Eagle" for the national bird of the United States, the bald eagle, which is featured prominently on the mission insignia.
Insignia.
The Apollo 11 mission insignia was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States". He chose an eagle as the symbol, put an olive branch in its beak, and drew a lunar background with the Earth in the distance. NASA officials said the talons of the eagle looked too "warlike" and after some discussion, the olive branch was moved to the claws. The crew decided the Roman numeral XI would not be understood in some nations and went with "Apollo 11"; they decided not to put their names on the patch, so it would "be representative of "everyone" who had worked toward a lunar landing". All colors are natural, with blue and gold borders around the patch. 
When the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side. The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979, ten years after the Apollo 11 mission.
Mementos.
Neil Armstrong's personal preference kit carried a piece of wood from the Wright brothers' 1903 airplane's left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Deke Slayton by the widows of the Apollo 1 crew. This pin had been intended to be flown on Apollo 1 and given to Slayton after the mission but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton and Armstrong took it on Apollo 11.
Mission highlights.
Launch and flight to lunar orbit.
In addition to throngs of people crowding highways and beaches near the launch site, millions watched the event on television, with NASA Chief of Public Information Jack King providing commentary. President Richard M. Nixon viewed the proceedings from the Oval Office of the White House.
A Saturn V launched Apollo 11 from Launch Pad 39A, part of the Launch Complex 39 site at the Kennedy Space Center on July 16, 1969 at 13:32:00 UTC (9:32:00 a.m. EDT local time). It entered Earth orbit, at an altitude of by , twelve minutes later. After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC. About 30 minutes later the command/service module pair separated from this last remaining Saturn V stage and docked with the Lunar Module still nestled in the Lunar Module Adaptor. After the Lunar Module was extracted, the combined spacecraft headed for the Moon, while the third stage booster flew on a trajectory past the Moon and into orbit around the Sun.
On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility (Mare Tranquillitatis) about southwest of the crater Sabine D (0.67408N, 23.47297E). The landing site was selected in part because it had been characterized as relatively flat and smooth by the automated "Ranger 8" and "Surveyor 5" landers along with the "Lunar Orbiter" mapping spacecraft and unlikely to present major landing or extra-vehicular activity (EVA) challenges.
Lunar descent.
On July 20, 1969, the Lunar Module "Eagle" separated from the Command Module "Columbia". Collins, alone aboard "Columbia", inspected "Eagle" as it pirouetted before him to ensure the craft was not damaged.
As the descent began, Armstrong and Aldrin found that they were passing landmarks on the surface four seconds early and reported that they were "long"; they would land miles west of their target point.
Five minutes into the descent burn, and above the surface of the Moon, the LM navigation and guidance computer distracted the crew with the first of several unexpected "1202" and "1201" program alarms. Inside Mission Control Center in Houston, Texas, computer engineer Jack Garman told guidance officer Steve Bales it was safe to continue the descent, and this was relayed to the crew. The program alarms indicated "executive overflows", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them.
Landing.
When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a diameter crater (later determined to be "West crater," named for its location in the western part of the originally planned landing ellipse). Armstrong took semi-automatic control and, with Aldrin calling out altitude and velocity data, landed at 20:17:40 UTC on July 20 with about 25 seconds of fuel left.
Apollo 11 landed with less fuel than other missions, and the astronauts encountered a premature low fuel warning. This was later found to be the result of greater propellant 'slosh' than expected, uncovering a fuel sensor. On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.
Throughout the descent Aldrin had called out navigation data to Armstrong, who was busy piloting the LM. A few moments before the landing, a light informed Aldrin that at least one of the probes hanging from "Eagle"'s footpads had touched the surface, and he said "Contact light!" Three seconds later, "Eagle" landed and Armstrong said "Shutdown." Aldrin immediately said "Okay, engine stop. ACA – out of detent." Armstrong acknowledged "Out of detent. Auto" and Aldrin continued "Mode control – both auto. Descent engine command override off. Engine arm – off. 413 is in."
Charles Duke, CAPCOM during the landing phase, acknowledged their landing by saying "We copy you down, Eagle."
Armstrong acknowledged Aldrin's completion of the post landing checklist with "Engine arm is off," before responding to Duke with the words, "Houston, Tranquility Base here. The "Eagle" has landed." Armstrong's unrehearsed change of call sign from "Eagle" to "Tranquility Base" emphasized to listeners that landing was complete and successful. Duke mispronounced his reply as he expressed the relief at Mission Control: "Roger, Twan— Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot."
Two and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:
 He then took communion privately. At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the Apollo 8 crew reading from the Book of Genesis) demanding that their astronauts refrain from broadcasting religious activities while in space. As such, Aldrin chose to refrain from directly mentioning taking communion on the Moon. Aldrin was an elder at the Webster Presbyterian Church, and his communion kit was prepared by the pastor of the church, the Rev. Dean Woodruff. Aldrin described communion on the Moon and the involvement of his church and pastor in the October 1970 edition of "Guideposts" magazine and in his book "Return to Earth". Webster Presbyterian possesses the chalice used on the Moon and commemorates the event each year on the Sunday closest to July 20.
The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, since they had been awake since early morning. However, they elected to forgo the sleep period and begin the preparations for the EVA early, thinking that they would be unable to sleep.
Lunar surface operations.
The astronauts planned placement of the Early Apollo Scientific Experiment Package (EASEP) and the U.S. flag by studying their landing site through "Eagle"'s twin triangular windows, which gave them a 60° field of view. Preparation required longer than the two hours scheduled. Armstrong initially had some difficulties squeezing through the hatch with his Portable Life Support System (PLSS). According to veteran Moon-walker John Young, a redesign of the LM to incorporate a smaller hatch had not been followed by a redesign of the PLSS backpack, so some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress.
At 02:39 UTC on Monday July 21, 1969, Armstrong opened the hatch, and at 02:51 UTC began his descent to the lunar surface. The Remote Control Unit controls on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the Modular Equipment Stowage Assembly (MESA) folded against "Eagle"'s side and activate the TV camera, and at 02:56:15 UTC he set his left foot on the surface. The first landing used slow-scan television incompatible with commercial TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor, significantly reducing the quality of the picture. The signal was received at Goldstone in the United States but with better fidelity by Honeysuckle Creek Tracking Station in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Although copies of this video in broadcast format were saved and are widely available, recordings of the original slow scan source transmission from the lunar surface were accidentally destroyed during routine magnetic tape re-use at NASA.
After describing the surface dust as "very fine-grained" and "almost like a powder," Armstrong stepped off "Eagle"'s footpad and uttered his famous line, "That's one small step for [a] man, one giant leap for mankind" six and a half hours after landing. Aldrin joined him, describing the view as "Magnificent desolation."
Armstrong claims to have said "That's one small step for "a" man, one giant leap for mankind" when he first set foot on the lunar surface. The "a" is not clear in NASA recordings, but the audio and video links back to Earth were somewhat intermittent, partly because of storms near Parkes Observatory. More recent digital analysis of the tape by NASA revealed the "a" may have been spoken but obscured by static.
About seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick. He then folded the bag and tucked it into a pocket on his right thigh. This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM.
In addition to fulfilling President Kennedy's mandate to land a man on the Moon before the end of the 1960s, Apollo 11 was an engineering test of the Apollo system; therefore, Armstrong snapped photos of the LM so engineers would be able to judge its post-landing condition. He removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod from the LM. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA.
Armstrong said that moving in the lunar gravity, one-sixth of Earth's, was "even perhaps easier than the simulations ... It's absolutely no trouble to walk around." Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backwards, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into "Eagle"'s shadow produced no temperature change inside the suit, though the helmet was warmer in sunlight, so he felt cooler in shadow.
The astronauts planted a specially designed U.S. flag on the lunar surface, in clear view of the TV camera. Some time later, President Richard Nixon spoke to them through a telephone-radio transmission which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief, to respect the lunar landing as Kennedy's legacy.
William Safire prepared a speech called "In Event of Moon Disaster" for President Nixon to read on television if the Apollo 11 astronauts were stranded on the Moon. According to the plans, Mission Control would "close down communications" with the LEM, and a clergyman would have commended their souls to "the deepest of the deep" in a public ritual likened to burial at sea. Presidential telephone calls to the astronauts' wives were also planned. The speech originated in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman in which Safire suggested a protocol the administration might follow in reaction to such a disaster. The last line of the prepared text contained an allusion to Rupert Brooke's First World War poem, "The Soldier."
The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust which soiled the outer part of their suits, the integrated thermal meteoroid garment.
They deployed the EASEP, which included a passive seismograph and a Lunar Ranging Retroreflector (LRRR). Then Armstrong walked from the LM to snap photos at the rim of Little West Crater while Aldrin collected two core tubes. He used the geological hammer to pound in the tubes - the only time the hammer was used on Apollo 11. The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes.
Three new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite. Armalcolite was named after Armstrong, Aldrin, and Collins.
During this period Mission Control used a coded phrase to warn Armstrong that his metabolic rates were high and that he should slow down. He was moving rapidly from task to task as time ran out. However, as metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension. In a 2010 interview, Armstrong, who had walked a maximum of from the LM, explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.
Lunar ascent and return.
Aldrin entered "Eagle" first. With some difficulty the astronauts lifted film and two sample boxes containing more than of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor. Armstrong reminded Aldrin of a bag of memorial items in his suit pocket sleeve, and Aldrin tossed the bag down; Armstrong then jumped to the ladder's third rung and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, one Hasselblad camera, and other equipment. They then pressurized the LM, and settled down to sleep.
While moving within the cabin, Aldrin accidentally broke the circuit breaker that would arm the main engine for lift off from the Moon. There was concern this would prevent firing the engine, stranding them on the Moon. Fortunately a felt-tip pen was sufficient to activate the switch. Had this not worked, the Lunar Module circuitry could have been reconfigured to allow firing the ascent engine.
After about seven hours of rest, the crew was awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54 UTC, they lifted off in "Eagle"'s ascent stage, carrying 21.5 kilograms of lunar samples with them, to rejoin CMP Michael Collins aboard "Columbia" in lunar orbit. During the launch Aldrin looked up in time to see the exhaust from the ascent module's engine knock over the American flag they had planted.
After more than 21½ total hours on the lunar surface, they had left behind scientific instruments that included a retroreflector array used for the Lunar Laser Ranging Experiment and a Passive Seismic Experiment Package used to measure moonquakes. They also left an American flag, an Apollo 1 mission patch, and a plaque (mounted on the LM Descent Stage ladder) bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon. The inscription read:
They also left behind a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace and a silicon message disk. The disk carries the goodwill statements by Presidents Eisenhower, Kennedy, Johnson, and Nixon and messages from leaders of 73 countries around the world. The disc also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and present top management. (In his 1989 book, "Men from Earth", Aldrin says that the items included Soviet medals commemorating Cosmonauts Vladimir Komarov and Yuri Gagarin.) Also, according to Deke Slayton's book "Moonshot", Armstrong carried with him a special diamond-studded astronaut pin from Slayton.
Film taken from the LM Ascent Stage upon liftoff from the Moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine. Buzz Aldrin witnessed it topple: "The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions usually planted the American flags at least from the LM to prevent its being blown over by the ascent engine exhaust.
After rendezvous with "Columbia", "Eagle"s ascent stage was jettisoned into lunar orbit on July 21, 1969 at 23:41 UTC. Just before the Apollo 12 flight, it was noted that "Eagle" was still likely to be orbiting the Moon. Later NASA reports mentioned that "Eagle"'s orbit had decayed, resulting in it impacting in an "uncertain location" on the lunar surface. The location is uncertain because the "Eagle" ascent stage was not tracked after it was jettisoned, and the lunar gravity field is sufficiently non-uniform to make the orbit of the spacecraft unpredictable after a short time. NASA estimated that the orbit had decayed within months and would have impacted on the Moon.
On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented,
Aldrin added,
Armstrong concluded,
On the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return. A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year old son Greg use his small hands to reach into the housing and pack it with grease. Greg later was thanked by Armstrong.
Splashdown and quarantine.
On July 24, the astronauts returned home aboard the Command Module "Columbia" just before dawn local time (16:51 UTC) at , in the Pacific Ocean east of Wake Island, south of Johnston Atoll, and from the recovery ship, USS "Hornet".
At 16:44 UTC the drogue parachutes had been deployed and seven minutes later the Command Module struck the water forcefully. During splashdown, the Command Module landed upside down but was righted within 10 minutes by flotation bags triggered by the astronauts. "Everything's okay. Our checklist is complete. Awaiting swimmers," was Armstrong's last official transmission from the "Columbia". A diver from the Navy helicopter hovering above attached a sea anchor to the Command Module to prevent it from drifting. Additional divers attached flotation collars to stabilize the module and position rafts for astronaut extraction. Though the chance of bringing back pathogens from the lunar surface was considered remote, it was considered a possibility and NASA took great precautions at the recovery site. Divers provided the astronauts with Biological Isolation Garments (BIGs) which were worn until they reached isolation facilities on board the "Hornet". Additionally astronauts were rubbed down with a sodium hypochlorite solution and the Command Module wiped with Betadine to remove any lunar dust that might be present. The raft containing decontamination materials was then intentionally sunk.
A second Sea King helicopter hoisted the astronauts aboard one by one, where a NASA flight surgeon gave each a brief physical check during the trip back to the "Hornet".
After touchdown on the "Hornet", the astronauts exited the helicopter, leaving the flight surgeon and three crewmen. The helicopter was then lowered into hangar bay #2 where the astronauts walked the to the Mobile Quarantine Facility (MQF) where they would begin their 21 days of quarantine. This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life and the quarantine process dropped.
President Richard Nixon was aboard "Hornet" to personally welcome the astronauts back to Earth. He told the astronauts, "As a result of what you've done, the world has never been closer together before." After Nixon departed, the "Hornet" was brought alongside the five-ton Command Module where it was placed aboard by the ship's crane, placed on a dolly and moved next to the MQF. The "Hornet" sailed for Pearl Harbor where the Command Module and MQF were airlifted to the Johnson Space Center.
In accordance with the recently passed Extra-Terrestrial Exposure Law, the astronauts were placed in quarantine for fear that the Moon might contain undiscovered pathogens and that the astronauts might have been exposed to them during their Moon walks. However, after almost three weeks in confinement (first in their trailer and later in the Lunar Receiving Laboratory at the Manned Spacecraft Center), the astronauts were given a clean bill of health. On August 10, 1969, the astronauts exited quarantine.
Celebration.
On August 13, they rode in parades in their honor in New York, Chicago, and Los Angeles. On the same evening in Los Angeles there was an official State Dinner to celebrate the flight, attended by members of Congress, 44 governors, the Chief Justice of the United States, and ambassadors from 83 nations at the Century Plaza Hotel. President Richard Nixon and Vice President Spiro T. Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom. This celebration was the beginning of a 45-day "Giant Leap" tour that brought the astronauts to 25 foreign countries and included visits with prominent leaders such as Queen Elizabeth II of the United Kingdom. Many nations honored the first manned Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.
On September 16, 1969, the three astronauts spoke before a joint session of Congress on Capitol Hill. They presented two US flags, one to the House of Representatives and the other to the Senate, that had been carried to the surface of the Moon with them.
Moon race.
The Soviet Union was secretly attempting to compete with the US in landing a man on the Moon but had been hampered by repeated failures in development of a launcher comparable to the Saturn V. Meanwhile, they tried to beat the US to return lunar material to the Earth by means of unmanned probes. On July 13, three days before Apollo 11's launch, they launched Luna 15, which reached lunar orbit before Apollo 11. During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the surface. The Jodrell Bank Observatory radio telescope in England was later discovered to have recorded transmissions from Luna 15 during its descent, and this was published in July 2009 on the 40th anniversary of Apollo 11.
Spacecraft location.
The Command Module is displayed at the National Air and Space Museum, Washington, D.C. It is in the central "Milestones of Flight" exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the Wright Flyer, the "Spirit of St. Louis", the Bell X-1, the North American X-15, Mercury spacecraft "Friendship 7", and Gemini 4. Armstrong's and Aldrin's space suits are displayed in the museum's "Apollo to the Moon" exhibit. The quarantine trailer, the flotation collar, and the righting spheres are displayed at the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Virginia.
In 2009 the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts.
In March 2012 Amazon founder Jeff Bezos located the F-1 engines that launched Apollo 11 into space. The engines were found below the Atlantic Ocean's surface through the use of advanced sonar scanning. His team brought at least one of the five engines to the surface. In July 2013, it was confirmed through serial numbers (2044) that F-1 engine parts brought up from the depths of the Atlantic Ocean were from the Apollo 11 launch.
Additional details.
Several books indicate early mission timelines had Buzz Aldrin rather than Neil Armstrong as the first man on the Moon.
40th anniversary events.
On July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by "Life" photographer Ralph Morse prior to the Apollo 11 launch.
From July 16–24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred.
In addition, it is in the process of restoring the video footage and has released a preview of key moments.
On July 20, 2009, the crew of Armstrong, Aldrin, and Collins met with U.S. President Barack Obama at the White House. "We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins and Aldrin," Obama said. "We want to make sure that NASA is going to be there for them when they want to take their journey."
The John F. Kennedy Presidential Library and Museum set up a Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.
A group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:
It was carried out in a technically brilliant way with risks taken ... that would be inconceivable in the risk-averse world of today ... The Apollo programme is arguably the greatest technical achievement of mankind to date ... nothing since Apollo has come close [to] the excitement that was generated by those astronauts - Armstrong, Aldrin and the 10 others who followed them.
On August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Sen. Bill Nelson and Florida Rep. Alan Grayson.
In July 2010, air to ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronised and released for the first time.
Further reading.
For young readers
External links.
NASA reports
Multimedia

</doc>
<doc id="663" url="http://en.wikipedia.org/wiki?curid=663" title="Apollo 8">
Apollo 8

Apollo 8, the second human spaceflight mission in the United States Apollo space program, was launched on December 21, 1968, and became the first manned spacecraft to leave Earth orbit, reach the Earth's Moon, orbit it and return safely to Earth. The three-astronaut crew — Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders — became the first humans to travel beyond low Earth orbit, the first to see Earth as a whole planet, the first to directly see the far side of the Moon, and then the first to witness Earthrise. The 1968 mission, the third flight of the Saturn V rocket and that rocket's first manned launch, was also the first human spaceflight launch from the Kennedy Space Center, Florida, located adjacent to Cape Canaveral Air Force Station.
The mission was originally planned as Apollo 9, to be performed in early 1969 as the second test of the complete Apollo spacecraft, including the Lunar Module and the Command/Service Module in an elliptical medium Earth orbit. But when the Lunar Module proved unready to make its first test in a lower Earth orbit in December 1968, it was decided in August to fly Apollo 8 in December as a more ambitious lunar orbital flight without the Lunar Module. This meant Borman's crew was scheduled to fly two to three months sooner than originally planned, leaving them a shorter time for training and preparation, thus placing more demands than usual on their time and discipline.
Apollo 8 took three days to travel to the Moon. It orbited ten times over the course of 20 hours, during which the crew made a Christmas Eve television broadcast where they read the first 10 verses from the Book of Genesis. At the time, the broadcast was the most watched TV program ever. Apollo 8's successful mission paved the way for Apollo 11 to fulfill U.S. President John F. Kennedy's goal of landing a man on the Moon before the end of the 1960s. The Apollo 8 astronauts returned to Earth on December 27, 1968, when their spacecraft splashed down in the Northern Pacific Ocean. The crew was named Time Magazine's "Men of the Year" for 1968 upon their return.
Crew.
This crew was unique among pre-shuttle era missions in that the commander was not the most experienced member of the crew. This was also the first case of the rarity of an astronaut who had flown as commander subsequently flying as a non-commander, as Lovell had previously commanded Gemini XII.
Backup crew.
On a lunar mission, the Command Module Pilot (CMP) was assigned the role of navigator, while the Lunar Module Pilot (LMP) was assigned the role of flight engineer, responsible for monitoring all spacecraft systems, even if the flight didn't include a Lunar Module.
Lovell was originally the CMP on the back-up crew, with Michael Collins as the prime crew's CMP. However, Collins was replaced in July 1968, after suffering a cervical disc herniation that required surgery to repair.
Edwin Aldrin was originally the backup LMP. When Lovell was rotated to the prime crew, no one with experience on CSM-103 (the specific spacecraft used for the mission) was available, so Aldrin was moved to CMP and Fred Haise brought in as backup LMP. Neil Armstrong went on to command Apollo 11, where Aldrin was returned to the LMP position and Collins was assigned as CMP.
Mission control.
The Earth-based mission control teams for Apollo 8 consisted of astronauts assigned to the support crew, as well as non-astronaut flight directors and their staffs. The support crew members were not trained to fly the mission, but were able to stand in for astronauts in meetings and be involved in the minutiae of mission planning, while the prime and backup crews trained. They also served as CAPCOMs during the mission. For Apollo 8, these crew members included astronauts John S. Bull, Vance D. Brand, Gerald P. Carr, and Ken Mattingly. The mission control teams on Earth rotated in three shifts, each led by a flight director. The directors for Apollo 8 included Clifford E. Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).
Mission insignia.
The triangular shape of the insignia symbolizes the shape of the Apollo Command Module (CM). It shows a red figure-8 looping around the Earth and Moon representing the mission number as well as the circumlunar nature of the mission. On the red number 8 are the names of the three astronauts.
The initial design of the insignia was developed by Jim Lovell. Lovell reportedly sketched the initial design while riding in the backseat of a T-38 flight from California to Houston, shortly after learning of the re-designation of the flight to become a lunar-orbital mission. The graphic design of the insignia was done by Houston artist and animator William Bradley.
Planning.
Apollo 4 and Apollo 6 had been "A" missions, unmanned tests of the Saturn V launch vehicle using an unmanned Block I production model of the Apollo Command and Service Module in Earth orbit. , scheduled for October 1968, would be a manned Earth-orbit flight of the CSM, completing the objectives for Mission "C."
Further missions depended on the readiness of the Lunar Module. Apollo 8 was planned as the "D" mission, to test the LM in a low Earth orbit in December 1968 by James McDivitt, David Scott and Russell Schweickart, while Borman's crew would fly the "E" mission, a more rigorous LM test in an elliptical medium Earth orbit as Apollo 9, in early 1969.
But production of the LM fell behind schedule, and when Apollo 8's LM arrived at Cape Canaveral in June 1968, significant defects were discovered, leading Grumman, the lead contractor for the LM, to predict that the first mission-ready LM would not be ready until at least February 1969. This would mean delaying the "D" and subsequent missions, endangering the program's goal of a lunar landing before the end of 1969.
George Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August to keep the program on track despite the LM delay. Since the Command/Service Module (CSM) would be ready three months before the Lunar Module, a CSM-only mission could be flown in December 1968. Instead of just repeating the "C" mission flight of Apollo 7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit. The new mission would also allow NASA to test lunar landing procedures that would otherwise have to wait until Apollo 10, the scheduled "F" mission. This also meant that the medium Earth orbit "E" mission could be dispensed with. The net result was that only the "D" mission had to be delayed.
Almost every senior manager at NASA agreed with this new mission, citing both confidence in the hardware and personnel, and the potential for a significant morale boost provided by a circumlunar flight. The only person who needed some convincing was James E. Webb, the NASA administrator. With the rest of his agency in support of the new mission, Webb eventually approved the mission change. The mission was officially changed from a "D" mission to a "C-Prime" lunar-orbit mission, but was still referred to in press releases as an Earth-orbit mission at Webb's direction. No public announcement was made about the change in mission until November 12, three weeks after Apollo 7's successful Earth-orbit mission and less than 40 days before launch.
With the change in mission for Apollo 8, Director of Flight Crew Operations Deke Slayton decided to swap the crews of the D and E missions. This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104.
On September 9, the crew entered the simulators to begin their preparation for the flight. By the time the mission flew, the crew had spent seven hours training for every actual hour of flight. Although all crew members were trained in all aspects of the mission, it was necessary to specialize. Borman, as commander, was given training on controlling the spacecraft during the re-entry. Lovell was trained on navigating the spacecraft in case communication was lost with the Earth. Anders was placed in charge of checking that the spacecraft was in working order.
Added pressure on the Apollo program to make its 1969 landing goal was provided by the Soviet Union's flight of some living creatures, including Russian tortoises, in a cislunar loop around the Moon on Zond 5 and return to Earth on September 21. There was speculation within NASA and the press that they might be preparing to launch cosmonauts on a similar circumlunar mission before the end of 1968.
The Apollo 8 crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch. They talked about how, before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight. The total was a tenth of the amount that the Saturn V would burn every second. The next day, the Lindberghs watched the launch of Apollo 8 from a nearby dune.
Saturn V.
The Saturn V rocket used by Apollo 8 was designated SA-503, or the "03rd" model of the Saturn V ("5") Rocket to be used in the Saturn-Apollo ("SA") program. When it was erected in the Vertical Assembly Building on December 20, 1967, it was thought that the rocket would be used for an unmanned Earth-orbit test flight carrying a boilerplate Command/Service Module. Apollo 6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second stage engine failures, and a third stage that failed to reignite in orbit. Without assurances that these problems had been rectified, NASA administrators could not justify risking a manned mission until additional unmanned test flights proved that the Saturn V was ready.
Teams from the Marshall Space Flight Center (MSFC) went to work on the problems. Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew. A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket. A system using helium gas to absorb some of these vibrations was installed.
Of equal importance was the failure of three engines during flight. Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two. When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it accidentally shut down engine three's liquid oxygen due to a miswired connection. As a result, engine three failed within one second of engine two's shutdown. Further investigation revealed the same problem for the third-stage engine—a faulty igniter line. The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches.
The teams tested their solutions in August 1968 at the Marshall Space Flight Center. A Saturn stage IC was equipped with shock absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions. Once NASA administrators were convinced that the problems were solved, they gave their approval for a manned mission using SA-503.
The Apollo 8 spacecraft was placed on top of the rocket on September 21 and the rocket made the slow 3-mile (5 km) journey to the launch pad on October 9. Testing continued all through December until the day before launch, including various levels of readiness testing from December 5 through 11. Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on December 18, a mere three days before the scheduled launch.
Mission.
Parameter summary.
As the first manned spacecraft to orbit more than one celestial body, Apollo 8's profile had two different sets of orbital parameters, separated by a translunar injection maneuver.
Apollo lunar missions would begin with a nominal circular Earth parking orbit. Apollo 8 was launched into an initial orbit with an apogee of and a perigee of , with an inclination of 32.51° to the Equator, and an orbital period of 88.19 minutes. Propellant venting increased the apogee by over the 2 hours, 44 minutes and 30 seconds spent in the parking orbit.
This was followed by a Trans-Lunar Injection (TLI) burn of the S-IVB third stage for 318 seconds, accelerating the spacecraft from an orbital velocity of to the injection velocity of , which set a record for the highest speed, relative to Earth, that humans had ever traveled. This speed was slightly less than the Earth's escape velocity of , but put Apollo 8 into an elongated elliptical Earth orbit, to a point where the Moon's gravity would capture it.
The standard lunar orbit for Apollo missions was planned as a nominal circular orbit above the Moon's surface. Initial lunar orbit insertion was an ellipse with a perilune of and an apolune of , at an inclination of 12° from the lunar equator. This was then circularized at by , with an orbital period of 128.7 minutes. The effect of lunar mass concentrations ("masscons") on the orbit was found to be greater than initially predicted; over the course of the twenty-hour mission, the orbit was perturbated to by .
Apollo 8 achieved a maximum distance from Earth of .
Launch and trans-lunar injection.
Apollo 8 launched at 7:51:00 a.m. Eastern Standard Time on December 21, 1968, using the Saturn V's three stages, S-IC, S-II, and S-IVB, to achieve Earth orbit. The launch phase experienced only three minor problems: The engines of the first stage, S-IC, underperformed by 0.75%, causing the engines to burn for 2.45 seconds longer than planned, and toward the end of the second stage burn, S-II, the rocket underwent pogo oscillations. Frank Borman estimated the oscillations were approximately and (±2.5 m/s2).
All three rocket stages fired during launch; the S-IC and S-II detached during launch. The S-IC impacted the Atlantic Ocean at and the S-II second stage at . The third stage of the rocket, S-IVB, assisted in driving the craft into Earth orbit but remained attached to later perform the TLI burn that would put the spacecraft on a trajectory to the Moon.
Once in Earth orbit, both the Apollo 8 crew and Mission Control spent the next 2 hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI. The proper operation of third stage of the rocket, S-IVB, was crucial: in the last unmanned test, the S-IVB had failed to re-ignite for TLI.
During the flight, three fellow astronauts served on the ground as Capsule Communicators (usually referred to as "CAPCOMs") on a rotating schedule. The CAPCOMs were the only people who regularly communicated with the crew. Michael Collins was the first CAPCOM on duty and at 2 hours, 27 minutes and 22 seconds after launch radioed, "Apollo 8. You are Go for TLI." This communication signified that Mission Control had given official permission for Apollo 8 to go to the Moon. Over the next 12 minutes before the TLI burn, the Apollo 8 crew continued to monitor the spacecraft and the S-IVB. The engine ignited on time and performed the TLI burn perfectly.
After the S-IVB had performed its required tasks, it was jettisoned. The crew then rotated the spacecraft to take some photographs of the spent stage and then practiced flying in formation with it. As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it. This marked the first time humans could view the whole Earth at once. Borman became worried that the S-IVB was staying too close to the Command/Service Module and suggested to Mission Control that the crew perform a separation maneuver. Mission Control first suggested pointing the spacecraft towards Earth and using the Reaction Control System (RCS) thrusters on the Service Module (SM) to add away from the Earth, but Borman did not want to lose sight of the S-IVB. After discussion, the crew and Mission Control decided to burn in this direction, but at instead. These discussions put the crew an hour behind their flight plan.
Five hours after launch, Mission Control sent a command to the S-IVB booster to vent its remaining fuel through its engine bell to change the booster's trajectory. This S-IVB would then pass the Moon and enter into a solar orbit, posing no further hazard to Apollo 8. The S-IVB subsequently went into a solar orbit with an inclination of 23.47° from the plane of the ecliptic, and an orbital period of 340.80 days.
The Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth. Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1 milligray (during a year, the average human receives a dose of 2 to 3 mGy). To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew. By the end of the mission, the crew experienced an average radiation dose of 1.6 mGy.
After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object where it would continue to orbit the Sun for many years. , it remains in orbit.
Lunar trajectory.
Jim Lovell's main job as Command Module Pilot was as navigator. Although Mission Control performed all the actual navigation calculations, it was necessary to have a crew member serving as navigator so that the crew could successfully return to Earth in case of communication loss with Mission Control. Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon. This task proved to be difficult, as a large cloud of debris around the spacecraft formed by the venting S-IVB made it hard to distinguish the stars.
By seven hours into the mission, the crew was about one hour and 40 minutes behind flight plan due to the issues of moving away from the S-IVB and Lovell's obscured star sightings. The crew now placed the spacecraft into Passive Thermal Control (PTC), also known as "barbecue" roll. PTC involved the spacecraft rotating about once per hour along its long axis to ensure even heat distribution across the surface of the spacecraft. In direct sunlight, the spacecraft could be heated to over while the parts in shadow would be . These temperatures could cause the heat shield to crack or propellant lines to burst. As it was impossible to get a perfect roll, the spacecraft actually swept out a cone as it rotated. The crew had to make minor adjustments every half-hour as the cone pattern got larger and larger.
The first mid-course correction came 11 hours into the flight. Testing on the ground had shown that the Service Propulsion System (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was "coated" first. Burning the engine for a short period would accomplish coating. This first correction burn was only 2.4 seconds and added about velocity prograde (in the direction of travel). This change was less than the planned due to a bubble of helium in the oxidizer lines causing lower than expected fuel pressure. The crew had to use the small RCS thrusters to make up the shortfall. Two later planned mid-course corrections were canceled as the Apollo 8 trajectory was found to be perfect.
11 hours into the flight, the crew had been awake for over 16 hours. Before launch, NASA had decided that at least one crew member should be awake at all times to deal with any issues that might arise. Borman started the first sleep shift, but between the constant radio chatter and mechanical noises, he found sleep difficult.
About an hour after starting his sleep shift, Borman requested clearance to take a Seconal sleeping pill. However, the pill had little effect. Borman eventually fell asleep but then awoke feeling ill. He vomited twice and had a bout of diarrhea that left the spacecraft full of small globules of vomit and feces that the crew cleaned up to the best of their ability. Borman initially decided that he did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control. The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed. After recording a description of Borman's illness they requested that Mission Control check the recording, stating that they "would like an evaluation of the voice comments."
The Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second floor control room (there were two identical control rooms in Houston on the second and third floor, only one of which was used during a mission). The conference participants decided that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill. Researchers now believe that he was suffering from space adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness. Space adaptation syndrome had not been an issue on previous spacecraft (Mercury and Gemini), as those astronauts were unable to move freely in the comparatively smaller cabins of those spacecraft. The increased cabin space in the Apollo Command Module afforded astronauts greater freedom of movement, contributing to symptoms of space sickness for Borman and, later, astronaut Russell Schweickart during Apollo 9.
The cruise phase was a relatively uneventful part of the flight, except for the crew checking that the spacecraft was in working order and that they were on course. During this time, NASA scheduled a television broadcast at 31 hours after launch. The Apollo 8 crew used a 2 kg camera that broadcast in black-and-white only, using a Vidicon tube. The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens.
During this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space. However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible. Additionally, the Earth image became saturated by any bright source without proper filters. In the end, all the crew could show the people watching back on Earth was a bright blob. After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday.
By this time, the crew had completely abandoned the planned sleep shifts. Lovell went to sleep 32½ hours into the flight—3½ hours before he had planned to. A short while later, Anders also went to sleep after taking a sleeping pill.
The crew was unable to see the Moon for much of the outward cruise. Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for the PTC. It was not until the crew had gone behind the Moon that they would be able to see it for the first time.
The Apollo 8 made a second television broadcast at 55 hours into the flight. This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens. Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth. The crew spent the transmission describing the Earth and what was visible and the colors they could see. The transmission lasted 23 minutes.
Lunar sphere of influence.
At about 55 hours and 40 minutes into the flight, the crew of Apollo 8 became the first humans to enter the gravitational sphere of influence of another celestial body. In other words, the effect of the Moon's gravitational force on Apollo 8 became stronger than that of the Earth. At the time it happened, Apollo 8 was from the Moon and had a speed of relative to the Moon. This historic moment was of little interest to the crew since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center. They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit. It was only 13 hours until they would be in lunar orbit.
The last major event before Lunar Orbit Insertion (LOI) was a second mid-course correction. It was in retrograde (against direction of travel) and slowed the spacecraft down by , effectively lowering the closest distance that the spacecraft would pass the moon. At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds. They would now pass from the lunar surface.
At 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion-1 (LOI-1). This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth. After Mission Control was polled for a "go/no go" decision, the crew was told at 68 hours, they were Go and "riding the best bird we can find." At 68 hours and 58 minutes, the spacecraft went behind the Moon and out of radio contact with the Earth.
With 10 minutes before the LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in the correct place. At that time, they finally got their first glimpses of the Moon. They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface. The LOI burn was only two minutes away, so the crew had little time to appreciate the view.
Lunar orbit.
The SPS ignited at 69 hours, 8 minutes, and 16 seconds after launch and burned for 4 minutes and 13 seconds, placing the Apollo 8 spacecraft in orbit around the Moon. The crew described the burn as being the longest four minutes of their lives. If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even flung off into space. If it lasted too long they could have struck the Moon. After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours.
On Earth, Mission Control continued to wait. If the crew had not burned the engine or the burn had not lasted the planned length of time, the crew would appear early from behind the Moon. However, this time came and went without Apollo 8 reappearing. Exactly at the calculated moment, the signal was received from the spacecraft, indicating it was in a orbit about the Moon.
After reporting on the status of the spacecraft, Lovell gave the first description of what the lunar surface looked like:
Lovell continued to describe the terrain they were passing over. One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that would be the Apollo 11 landing site. The launch time of Apollo 8 had been chosen to give the best lighting conditions for examining the site. A film camera had been set up in one of the spacecraft windows to record a frame every second of the Moon below. Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest. By the end of the mission the crew had taken 700 photographs of the Moon and 150 of the Earth.
Throughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked. He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary. He also asked that they receive a "go/no go" decision before they passed behind the Moon on each orbit.
As they reappeared for their second pass in front of the Moon, the crew set up the equipment to broadcast a view of the lunar surface. Anders described the craters that they were passing over. At the end of this second orbit they performed the 11-second LOI-2 burn of the SPS to circularize the orbit to .
Through the next two orbits, the crew continued to keep check of the spacecraft and to observe and photograph the Moon. During the third pass, Borman read a small prayer for his church. He had been scheduled to participate in a service at St. Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo 8 flight was unable. A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer which could be recorded and then replayed during the service.
In the foreword to the Millennial Edition of his novel "" Arthur C. Clarke says that crew told him "they had been tempted to radio back the discovery of a large black monolith," but discretion prevailed.
Earthrise.
When the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed "Earthrise" for the first time in human history (NASA's Lunar Orbiter 1 took the very first picture of an Earthrise from the vicinity of the Moon, on August 23, 1966). Borman saw the Earth emerging from behind the lunar horizon and called in excitement to the others, taking a black-and-white photo as he did so. In the ensuing scramble Anders took the more famous color photo, later picked by "Life" magazine as one of its hundred photos of the century. Due to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the lunar surface. Earthrise is generally only visible when orbiting the Moon, other than at selected places near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon.
Anders continued to take photographs while Lovell assumed control of the spacecraft so Borman could rest. Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status. Borman awoke fully, however, when he started to hear his fellow crew members make mistakes. They were beginning to not understand questions and would have to ask for the answers to be repeated. Borman realized that everyone was extremely tired having not had a good night's sleep in over three days. Taking command, he ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed. At first Anders protested saying that he was fine, but Borman would not be swayed. At last Anders agreed as long as Borman would set up the camera to continue to take automatic shots of the Moon. Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching he wanted the crew to be alert. For the next two orbits Anders and Lovell slept while Borman sat at the helm. On subsequent Apollo missions, crews would avoid this situation by sleeping on the same schedule.
As they rounded the Moon for the ninth time, the second television transmission began. Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon. Borman described it as being "a vast, lonely, forbidding expanse of nothing." Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth. Each man on board read a section from the Biblical creation story from the Book of Genesis. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth. His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit. Borman said, "And from the crew of Apollo 8, we close with good night, good luck, a Merry Christmas and God bless all of you - all of you on the good Earth."
The only task left for the crew at this point was to perform the Trans-Earth Injection (TEI), which was scheduled for 2½ hours after the end of the television transmission. The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in lunar orbit, with little hope of escape. As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth.
The burn occurred exactly on time. The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated. When voice contact was regained, Lovell announced, "Please be informed, there is a Santa Claus," to which Ken Mattingly, the current CAPCOM, replied, "That's affirmative, you are the best ones to know." The spacecraft began its journey back to Earth on December 25, Christmas Day.
Unplanned manual re-alignment.
Later, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard. However, he accidentally erased some of the computer's memory, which caused the Inertial Measurement Unit (IMU) to think the module was in the same relative position it had been in before lift-off and fire the thrusters to "correct" the module's attitude.
Once the crew realized why the computer had changed the module's attitude, they realized they would have to re-enter data that would tell the computer its real position. It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another 15 minutes to enter the corrected data into the computer.
16 months later, Lovell would once again have to perform a similar manual re-alignment, under more critical conditions, during the Apollo 13 mission, after that module's IMU had to be turned off to conserve energy. In his 1994 book, "Lost Moon: The Perilous Voyage of Apollo 13", Lovell wrote, "My training [on Apollo 8] came in handy!" In that book he dismissed the incident as a "planned experiment," requested by the ground crew. In subsequent interviews Lovell has acknowledged that the incident was an accident, caused by his mistake.
Cruise back to Earth and re-entry.
The cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft. As long as the trajectory specialists had calculated everything correctly, the spacecraft would re-enter two-and-half days after TEI and splashdown in the Pacific.
On Christmas afternoon, the crew made their fifth television broadcast. This time they gave a tour of the spacecraft, showing how an astronaut lived in space. When they finished broadcasting they found a small present from Deke Slayton in the food locker: a real turkey dinner with stuffing, in the same kind of pack that the troops in Vietnam received. Another Slayton surprise was a gift of three miniature bottles of brandy, that Borman ordered the crew to leave alone until after they landed. They remained unopened, even years after the flight. There were also small presents to the crew from their wives. The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the earth, in a four-minute broadcast.
After two uneventful days the crew prepared for re-entry. The computer would control the re-entry and all the crew had to do was put the spacecraft in the correct attitude, blunt end forward. If the computer broke down, Borman would take over.
Once the Command Module was separated from the Service Module, the astronauts were committed to re-entry. Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been predicted by the trajectory specialists. As they hit the thin outer atmosphere they noticed it was becoming hazy outside as glowing plasma formed around the spacecraft. The spacecraft started slowing down and the deceleration peaked at 6 g (59 m/s2). With the computer controlling the descent by changing the attitude of the spacecraft, Apollo 8 rose briefly like a skipping stone before descending to the ocean. At the drogue parachute stabilized the spacecraft and was followed at by the three main parachutes. The spacecraft splashdown position was officially reported as in the North Pacific Ocean south of Hawaii.
When it hit the water, the parachutes dragged the spacecraft over and left it upside down, in what was termed Stable 2 position. About six minutes later the Command Module was righted into its normal apex-up splashdown orientation by the inflatable bag uprighting system. As they were buffeted by a swell, Borman was sick, waiting for the three flotation balloons to right the spacecraft. It was 43 minutes after splashdown before the first frogman from the USS "Yorktown" arrived, as the spacecraft had landed before sunrise. Forty-five minutes later, the crew was safe on the deck of the aircraft carrier.
Historical importance.
Apollo 8 came at the end of 1968, a year that had seen much upheaval in the United States and most of the world. Even though the year saw political assassinations, political unrest in the streets of Europe and America, and the Prague Spring, "Time" magazine chose the crew of Apollo 8 as their Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year. They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body. They had survived a mission that even the crew themselves had rated as only having a fifty-fifty chance of fully succeeding. The effect of Apollo 8 can be summed up by a telegram from a stranger, received by Borman after the mission, that simply stated, "Thank you Apollo 8. You saved 1968."
One of the most famous aspects of the flight was the Earthrise picture that was taken as they came around for their fourth orbit of the Moon. This was the first time that humans had taken such a picture whilst actually behind the camera, and it has been credited with a role in inspiring the first Earth Day in 1970. It was selected as the first of "Life" magazine's "100 Photographs That Changed the World". Apollo 11 astronaut Michael Collins said, "Eight's momentous historic significance was foremost"; while many space historians, such as Robert K. Poole, see Apollo 8 as the most historically significant of all the Apollo missions.
The mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn in 1962. There were 1200 journalists covering the mission, with the BBC coverage being broadcast in 54 countries in 15 different languages. The Soviet newspaper "Pravda" featured a quote from Boris Nikolaevich Petrov, Chairman of the Soviet Interkosmos program, who described the flight as an "outstanding achievement of American space sciences and technology." It is estimated that a quarter of the people alive at the time saw—either live or delayed—the Christmas Eve transmission during the ninth orbit of the Moon. The Apollo 8 broadcasts won an Emmy Award, the highest honor given by the Academy of Television Arts & Sciences.
Madalyn Murray O'Hair, an atheist, later caused controversy by bringing a lawsuit against NASA over the reading from Genesis. O'Hair wished the courts to ban American astronauts—who were all government employees—from public prayer in space. Though the case was rejected by the Supreme Court of the United States for lack of jurisdiction, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program. Buzz Aldrin, on Apollo 11, self-communicated Presbyterian Communion on the surface of the Moon after landing; he refrained from mentioning this publicly for several years, and only obliquely referred to it at the time.
In 1969, the United States Postal Service issued a postage stamp (Scott catalogue #1371) commemorating the Apollo 8 flight around the Moon. The stamp featured a detail of the famous photograph of the Earthrise over the Moon taken by Anders on Christmas Eve, and the words, "In the beginning God..." Just 18 days after the crew's return to Earth, they were featured during the 1969 Super Bowl pre-game show reciting the Pledge of Allegiance prior to the national anthem being performed by Anita Bryant.
Spacecraft location.
In January 1970, the spacecraft was delivered to Osaka, Japan, for display in the U.S. pavilion at Expo '70. It is now displayed at the Chicago Museum of Science and Industry, along with a collection of personal items from the flight donated by Lovell and the space suit worn by Frank Borman. Jim Lovell's Apollo 8 space suit is on public display in the Visitor Center at NASA's Glenn Research Center. Bill Anders's space suit is on display at the Science Museum in London, England.
In film.
Apollo 8's historic mission has been shown and referred to in several forms, both documentary and fiction. The various television transmissions and 16 mm footage shot by the crew of Apollo 8 was compiled and released by NASA in the 1969 documentary, "Debrief: Apollo 8", which was hosted by Burgess Meredith. In addition, Spacecraft Films released, in 2003, a three-disc DVD set containing all of NASA's TV and 16 mm film footage related to the mission including all TV transmissions from space, training and launch footage, and motion pictures taken in flight. Portions of the Apollo 8 Mission can be seen in the 1989 documentary "For All Mankind", which won the Grand Jury Prize Documentary at the Sundance Film Festival. The Apollo 8 mission was well-covered in the 2007 British documentary "In the Shadow of the Moon".
Portions of the Apollo 8 mission are dramatized in the 1998 miniseries "From the Earth to the Moon" episode "1968". The S-IVB stage of Apollo 8 was also portrayed as the location of an alien device in the 1970 "UFO" episode "Conflict."
At the Kennedy Space Center Visitor Complex's Apollo/Saturn V Center, the history of the U.S. space program leading up to the launch of Apollo 8 is the subject of a multi-screen multimedia presentation which also features the actual control panels used in the Firing Room for the launch.

</doc>
<doc id="664" url="http://en.wikipedia.org/wiki?curid=664" title="Astronaut">
Astronaut

An astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft.
While generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.
Starting in the 1950s up until 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies. With the sub-orbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.
Following the end of the American Space Shuttle program in 2011, the only means of transportation for astronauts into space are the Russian Soyuz and Chinese Shenzhou spacecraft.
Definition.
The criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.
, a total of 532 people from 36 countries have reached or more in altitude, of which 529 reached low Earth orbit or beyond.
Of these, 24 people have traveled beyond Low Earth orbit, to either lunar or trans-lunar orbit or to the surface of the moon; three of the 24 did so twice: Jim Lovell, John Young and Eugene Cernan. The three astronauts who have not reached low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie.
, under the U.S. definition 538 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded in altitude, only one exceeded 100 kilometers (about 62 miles).
Space travelers have spent over 41,790 man-days (114.5 man-years) in space, including over 100 astronaut-days of spacewalks.
As of 2008, the man with the longest cumulative time in space is Sergei K. Krikalev, who has spent 803 days, 9 hours and 39 minutes, or 2.2 years, in space.
Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.
Terminology.
English.
In English-speaking nations, a professional space traveler is called an "astronaut". The term derives from the Greek words "ástron" (ἄστρον), meaning "star", and "nautes" (ναύτης), meaning "sailor". The first known use of the term "astronaut" in the modern sense was by Neil R. Jones in his short story "The Death's Head Meteor" in 1930. The word itself had been known earlier. For example, in Percy Greg's 1880 book "Across the Zodiac", "astronaut" referred to a spacecraft. In "Les Navigateurs de l'Infini" (1925) of J.-H. Rosny aîné, the word "astronautique" (astronautic) was used. The word may have been inspired by "aeronaut", an older term for an air traveler first applied (in 1784) to balloonists. An early use in a non-fiction publication is Eric Frank Russell's poem "The Astronaut" in the November 1934 "Bulletin of the British Interplanetary Society".
The first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950 and the subsequent founding of the International Astronautical Federation the following year.
NASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.
Russian.
By convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a "cosmonaut" in English texts. The word is an anglicisation of the Russian word "kosmonavt" ( ), one who works in space outside the Earth's atmosphere, a space traveler, which derives from the Greek words "kosmos" (κόσμος), meaning "universe", and "nautes" (ναύτης), meaning "sailor". Other countries of the former Eastern Bloc use variations of the Russian word "kosmonavt", such as the Polish "kosmonauta".
The Soviet Air Force pilot Yuri Gagarin was the first cosmonaut—indeed the first person—in space. Valentina Tereshkova, a Russian factory worker, was the first woman in space, as well as arguably the second civilian to make it there (see below for a further discussion of civilians in space). On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first "American cosmonaut".
Chinese.
Official English-language texts issued by the government of China use "astronaut" while texts in Russian use космонавт ("cosmonaut"). In official Chinese-language texts, "yǔ háng yuán" (, "space navigating personnel") is used for astronauts and cosmonauts, and "háng tiān yuán" (, "space navigating personnel") is used for Chinese astronauts. The phrase "tài kōng rén" (, "spaceman") is often used in Hong Kong and Taiwan.
The term "taikonaut" is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as "a hybrid of the Chinese term "taikong" (space) and the Greek "naut" (sailor)"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the "Shenzhou 5" spacecraft. This is the term used by Xinhua News Agency in the English version of the Chinese "People's Daily" since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups.
Other terms.
With the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term "spaceflight participant" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.
While no nation other than the Russian Federation (and previously the former Soviet Union), the United States, and China have launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term "spationaut" (French spelling: "spationaute") is sometimes used to describe French space travelers, from the Latin word "spatium" for "space", the Malay term "angkasawan" was used to describe participants in the Angkasawan program, and the Indian Space Research Organization hope to launch a spacecraft in 2018 that would carry "vyomanauts", coined from the Sanskrit word for space.
Space travel milestones.
The first human in space was Soviet Yuri Gagarin, who was launched on April 12, 1961 aboard Vostok 1 and orbited around the Earth for 108 minutes. The first woman in space was Soviet Valentina Tereshkova, who launched on June 16, 1963 aboard Vostok 6 and orbited Earth for almost three days.
Alan Shepard became the first American and second person in space on May 5, 1961 on a 15-minute sub-orbital flight. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983. In 1992 Mae Jemison became the first African American woman to travel in space aboard STS-47.
Cosmonaut Alexei Leonov was the first person to conduct an extra-vehicular activity (EVA), (commonly called a "spacewalk"), on March 18, 1965, on the Soviet Union's Voskhod 2 mission. This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.
The first manned mission to orbit the Moon, "Apollo 8", included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.
The Soviet Union, through its Intercosmos program, allowed people from other "socialist" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exception of France participating in Soyuz TM-7. An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket.
On July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space. In April 1985, Taylor Wang became the first ethnic Chinese person in space. The first person born in Africa to fly in space was Patrick Baudry (France), in 1985. In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.
With the larger number of seats available on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1984, Marc Garneau became the first of 8 Canadian astronauts to fly in space (through 2010).
In 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.
In 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.
On 15 October 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.
Age milestones.
The youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2. (Titov was also the first person to suffer space sickness).
The oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.
Duration and distance milestones.
The longest stay in space thus far has been 438 days, by Russian Valeri Polyakov.
As of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.
Civilian and non-government milestones.
Depending on the exact definition of 'civilian', the first civilian in space was either Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission) or Joseph Albert Walker on X-15 Flight 90 a month later. Tereshkova was only honorarily inducted into the USSR's Air Force, which had no female pilots whatsoever at that time. Joe Walker had joined the US Army Air Force but was not a member during his flight. The first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.
The first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was "Research Cosmonaut". Akiyama suffered severe space sickness during his mission, which affected his productivity.
The first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on 28 April 2001.
Self-funded travelers.
The first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist. Seven others have paid to Russian Space Agency to fly into space:
Training.
The first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and the USSR tended to be jet fighter pilots, and were often test pilots.
Once selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extra-vehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training may also experience short periods of weightlessness in aircraft called the "vomit comet", the nickname given to a pair of modified KC-135s (retired in 2000 and 2004 respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are done out of Edwards Air Force Base.
NASA candidacy requirements.
Mission Specialist Educator.
Mission Specialist Educators, or "Educator Astronauts", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger. 
Barbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist. 
The Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.
Health risks of space travel.
Astronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.
On December 31, 2012, a NASA-supported study reported that manned spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.
Food and drink.
An astronaut on the International Space Station requires about 0.83 kilograms (1.83 pounds) weight of food inclusive of food packaging) per meal each day. (The packaging for each meal weighs around 0.12 kilograms - 0.27 pounds) Longer-duration missions require more food.
Shuttle astronauts worked with nutritionists to select menus that appeal to their individual tastes. Five months before flight, menus are selected and analyzed for nutritional content by the shuttle dietician. Foods are tested to see how they will react in a reduced gravity environment. Caloric requirements are determined using a basal energy expenditure (BEE) formula.
On Earth, the average American uses about 35 gallons (132 liters) of water every day. On board the ISS astronauts limit water use to only about three gallons (11 liters) per day.
Insignia.
In Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation. This follows the practice established in the Soviet Union.
At NASA, those who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.
Deaths.
Eighteen astronauts (fourteen men and four women) have lost their lives during four space flights. By nationality, thirteen were American (including one of Indian origin), four were Russian (Soviet Union), and one was Israeli.
Eleven people (all men) have lost their lives training for spaceflight: eight Americans and three Russians. Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.
The Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.

</doc>
<doc id="665" url="http://en.wikipedia.org/wiki?curid=665" title="A Modest Proposal">
A Modest Proposal

A Modest Proposal for Preventing the Children of Poor People From Being a Burthen to Their Parents or Country, and for Making Them Beneficial to the Publick, commonly referred to as A Modest Proposal, is a Juvenalian satirical essay written and published anonymously by Jonathan Swift in 1729. Swift suggests that the impoverished Irish might ease their economic troubles by selling their children as food for rich gentlemen and ladies. This satirical hyperbole mocks heartless attitudes towards the poor, as well as Irish policy in general.
In English writing, the phrase "a modest proposal" is now conventionally an allusion to this style of straight-faced satire.
Details.
Swift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion. He uses methods of argument throughout his essay which lampoon the then-influential William Petty and the social engineering popular among followers of Francis Bacon. These lampoons include appealing to the authority of "a very knowing American of my acquaintance in London" and "the famous Psalmanazar, a native of the island Formosa" (who had already confessed to "not" being from Formosa in 1706). This essay is widely held to be one of the greatest examples of sustained irony in the history of the English language. Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states, "A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout."
Readers unacquainted with its reputation as a satirical work often do not immediately realise that Swift was not seriously proposing cannibalism and infanticide, nor would readers unfamiliar with the satires of Horace and Juvenal recognise that Swift's essay follows the rules and structure of Latin satires.
The satirical element of the pamphlet is often only understood after the reader notes the allusions made by Swift to the attitudes of landlords, such as the following: "I grant this food may be somewhat dear, and therefore very proper for Landlords, who as they have already devoured most of the Parents, seem to have the best Title to the Children." Swift extends the conceit to get in a few jibes at England’s mistreatment of Ireland, noting that "For this kind of commodity will not bear exportation, and flesh being of too tender a consistence, to admit a long continuance in salt, although perhaps I could name a country, which would be glad to eat up our whole nation without it."
In the tradition of Roman satire, Swift introduces the reforms he is actually suggesting by paralipsis:
Population solutions.
George Wittkowsky argued that Swift’s main target in "A Modest Proposal" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills. Swift was especially insulted by projects that tried to fix population and labour issues with a simple cure-all solution. A memorable example of these sorts of schemes "involved the idea of running the poor through a joint-stock company". In response, Swift's "Modest Proposal" was "a burlesque of projects concerning the poor" that were in vogue during the early 18th century.
"A Modest Proposal" also targets the calculating way people perceived the poor in designing their projects. The pamphlet targets reformers who "regard people as commodities". In the piece, Swift adopts the "technique of a political arithmetician" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics.
Critics differ about Swift's intentions in using this faux-mathematical philosophy. Edmund Wilson argues that statistically "the logic of the 'Modest proposal' can be compared with defense of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population". Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that "springs from a spirit of bitter mockery, not from the delight in calculations for their own sake".
Rhetoric.
Charles K. Smith argues that Swift's rhetorical style persuades the reader to detest the speaker and pity the Irish. Swift's specific strategy is twofold, using a "trap" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, "details vividly and with rhetorical emphasis the grinding poverty" but feels emotion solely for members of his own class. Swift's use of gripping details of poverty and his narrator's cool approach towards them create "two opposing points of view" that "alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way."
Swift has his proposer further degrade the Irish by using language ordinarily reserved for animals. Lewis argues that the speaker uses "the vocabulary of animal husbandry" to describe the Irish. Once the children have been commodified, Swift's rhetoric can easily turn "people into animals, then meat, and from meat, logically, into tonnage worth a price per pound".
Swift uses the proposer's serious tone to highlight the absurdity of his proposal. In making his argument, the speaker uses the conventional, text book approved order of argument from Swift's time (which was derived from the Latin rhetorician Quintilian). The contrast between the "careful control against the almost inconceivable perversion of his scheme" and "the ridiculousness of the proposal" create a situation in which the reader has "to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan".
Influences.
Scholars have speculated about which earlier works Swift may have had in mind when he wrote "A Modest Proposal".
Tertullian's "Apology".
James Johnson argued that "A Modest Proposal" was largely influenced and inspired by Tertullian's "Apology": a satirical attack against early Roman persecution of Christianity. James William Johnson believes that Swift saw major similarities between the two situations. Johnson notes Swift's obvious affinity for Tertullian and the bold stylistic and structural similarities between the works "A Modest Proposal" and "Apology". In structure, Johnson points out the same central theme, that of cannibalism and the eating of babies as well as the same final argument, that "human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human." Stylistically, Swift and Tertullian share the same command of sarcasm and language. In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony. Baker notes the uncanny way that both authors imply an ironic "justification by ownership" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the English mistreatment of the Irish poor.
Defoe's "The Generous Projector".
It has also been argued that "A Modest Proposal" was, at least in part, a response to the 1728 essay "The Generous Projector or, A Friendly Proposal to Prevent Murder and Other Enormous Abuses, By Erecting an Hospital for Foundlings and Bastard Children" by Swift's rival Daniel Defoe.
Economic themes.
Robert Phiddian's article "Have you eaten yet? The Reader in A Modest Proposal" focuses on two aspects of "A Modest Proposal": the voice of Swift and the voice of the Proposer. Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satiric voice of Jonathan Swift and the apparent economic projections of the Proposer. He reminds readers that "there is a gap between the narrator's meaning and the text's, and that a moral-political argument is being carried out by means of parody".
While Swift's proposal is obviously not a serious economic proposal, George Wittkowsky, author of "Swift's Modest Proposal: The Biography of an Early Georgian Pamphlet", argues that to understand the piece fully, it is important to understand the economics of Swift’s time. Wittowsky argues that not enough critics have taken the time to focus directly on the mercantilism and theories of labour in 18th century England. "[I]f one regards the "Modest Proposal" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact".
"People are the riches of a nation".
At the start of a new industrial age in the 18th century, it was believed that "people are the riches of the nation", and there was a general faith in an economy that paid its workers low wages because high wages meant workers would work less. Furthermore, "in the mercantilist view no child was too young to go into industry". In those times, the "somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity".
Louis A. Landa presents Swift's "A Modest Proposal" as a critique of the popular and unjustified maxim of mercantilism in the 18th century that "people are the riches of a nation". Swift presents the dire state of Ireland and shows that mere population itself, in Ireland's case, did not always mean greater wealth and economy. The uncontrolled maxim fails to take into account that a person who does not produce in an economic or political way makes a country poorer, not richer. Swift also recognises the implications of such a fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens. Swift however, Landa argues, is not merely criticising economic maxims but also addressing the fact that England was denying Irish citizens their natural rights and dehumanising them by viewing them as a mere commodity.
Modern usage.
"A Modest Proposal" is included in many literature programs as an example of early modern western satire. It also serves as an exceptional introduction to the concept and use of argumentative language, lending itself well to secondary and post-secondary essay courses. Outside of the realm of English studies, "A Modest Proposal" is a relevant piece included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences.
The essay has been emulated many times. In his book "A Modest Proposal" (1984), evangelical author Frank Schaeffer emulated Swift's work in social conservative polemic against abortion and euthanasia in a future dystopia that advocated recycling of aborted embryos and fetuses, as well as some disabled infants with compound intellectual, physical and physiological difficulties. (Such Baby Doe Rules cases were then a major concern of the pro-life movement of the early 1980s, which viewed selective treatment of those infants as disability discrimination.) In his book "A Modest Proposal for America" (2013), statistician Howard Friedman opens with a satirical reflection of the extreme drive to fiscal stability by ultra-conservatives.
Hunter S. Thompson's "Fear and Loathing in America: The Brutal Odyssey of an Outlaw Journalist", which contains hundreds of private letters written by Thompson over the years, contains a letter in which he uses "A Modest Proposal"'s satire technique against the Vietnam War. Thompson writes a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find. This letter protests the burning of Vietnamese people occurring overseas.

</doc>
<doc id="666" url="http://en.wikipedia.org/wiki?curid=666" title="Alkali metal">
Alkali metal

The alkali metals are a group (column) in the periodic table consisting of the chemical elements lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). This group lies in the s-block of the periodic table as all alkali metals have their outermost electron in an s-orbital: this electron configuration results in their characteristic properties. The alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterized homologous behaviour.
The alkali metals have very similar properties: they are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1. They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen. Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free element. In the modern IUPAC nomenclature, the alkali metals comprise the group 1 elements, excluding hydrogen (H), which is nominally a group 1 element but not normally considered to be an alkali metal as it rarely exhibits behaviour comparable to that of the alkali metals. All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.
All the discovered alkali metals occur in nature: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity and thus occurs only in traces due to its presence in natural decay chains. Experiments have been conducted to attempt the synthesis of ununennium (Uue), which is likely to be the next member of the group, but they have all met with failure. However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.
Most alkali metals have many different applications. Two of the most well-known applications of the pure elements are rubidium and caesium atomic clocks, of which caesium atomic clocks are the most accurate and precise representation of time. A common application of the compounds of sodium is the sodium-vapour lamp, which emits very efficient light. Table salt, or sodium chloride, has been used since antiquity. Sodium and potassium are also essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.
Properties.
Physical and chemical.
The physical and chemical properties of the alkali metals can be readily explained by their having an ns1 valence electron configuration, which results in weak metallic bonding. Hence, all the alkali metals are soft and have low densities, melting and boiling points, as well as heats of sublimation, vaporisation, and dissociation They all crystallize in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited. The ns1 configuration also results in the alkali metals having very large atomic and ionic radii, as well as high thermal and electrical conductivity. Their chemistry is dominated by the loss of their lone valence electron to form the +1 oxidation state, due to the ease of ionizing this electron and the very high second ionization energy. Most of the chemistry has been observed only for the first five members of the group. The chemistry of francium is not well established due to its extreme radioactivity; thus, the presentation of its properties here is limited.
The alkali metals are more similar to each other than the elements in any other group are to each other. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium. One of the very few properties of the alkali metals that does not display a very smooth trend is their reduction potentials: lithium's value is anomalous, being more negative than the others. This is because the Li+ ion has a very high hydration energy in the gas phase: though the lithium ion disrupts the structure of water significantly, causing a higher change in entropy, this high hydration energy is enough to make the reduction potentials indicate it as being the most electropositive alkali metal, despite the difficulty of ionizing it in the gas phase.
The stable alkali metals are all silver-coloured metals except for caesium, which has a golden tint: it is one of only three metals that are clearly coloured (the other two being copper and gold). Their lustre tarnishes rapidly in air due to oxidation. They all crystallize in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited.
All the alkali metals are highly reactive and are never found in elemental forms in nature. Because of this, they are usually stored in mineral oil or kerosene (paraffin oil). They react aggressively with the halogens to form the alkali metal halides, which are white ionic crystalline compounds that are all soluble in water except lithium fluoride (LiF). The alkali metals also react with water to form strongly alkaline hydroxides and thus should be handled with great care. The heavier alkali metals react more vigorously than the lighter ones; for example, when dropped into water, caesium produces a larger explosion than potassium. The alkali metals have the lowest first ionisation energies in their respective periods of the periodic table because of their low effective nuclear charge and the ability to attain a noble gas configuration by losing just one electron. The second ionisation energy of all of the alkali metals is very high as it is in a full shell that is also closer to the nucleus; thus, they almost always lose a single electron, forming cations. The alkalides are an exception: they are unstable compounds which contain alkali metals in a −1 oxidation state, which is very unusual as before the discovery of the alkalides, the alkali metals were not expected to be able to form anions and were thought to be able to appear in salts only as cations. The alkalide anions have filled s-subshells, which gives them more stability and allows them to exist. All the stable alkali metals except lithium are known to be able to form alkalides, and the alkalides have much theoretical interest due to their unusual stoichiometry and low ionisation potentials. Alkalides are chemically similar to the electrides, which are salts with trapped electrons acting as anions. A particularly striking example of an alkalide is "inverse sodium hydride", H+Na− (both ions being complexed), as opposed to the usual sodium hydride, Na+H−: it is unstable in isolation, due to its high energy resulting from the displacement of two electrons from hydrogen to sodium, although several derivatives are predicted to be metastable or stable.
In aqueous solution, the alkali metal ions form aqua ions of the formula [M(H2O)"n")+, where "n" is the solvation number. Their coordination numbers and shapes agree well with those expected from their ionic radii. In aqueous solution the water molecules directly attached to the metal ion are said to belong to the first coordination sphere, also known as the first, or primary, solvation shell. The bond between a water molecule and the metal ion is a dative covalent bond, with the oxygen atom donating both electrons to the bond. Each coordinated water molecule may be attached by hydrogen bonds to other water molecules. The latter are said to reside in the second coordination sphere. However, for the alkali metal cations, the second coordination sphere is not well-defined as the +1 charge on the cation is not high enough to polarize the water molecules in the primary solvation shell enough for them to form strong hydrogen bonds with those in the second coordination sphere, producing a more stable entity. The solvation number for Li+ has been experimentally determined to be 4, forming the tetrahedral [Li(H2O)4]+: while solvation numbers of 3 to 6 have been found for lithium aqua ions, solvation numbers less than 4 may be the result of the formation of contact ion-pairs, and the higher solvation numbers may be interpreted in terms of water molecules that approach [Li(H2O)4]+ through a face of the tetrahedron, though molecular dynamic simulations may indicate the existence of an octahedral hexaaqua ion. There are also probably six water molecules in the primary solvation sphere of the sodium ion, forming the octahedral [Na(H2O)6]+ ion. While it was previously thought that the heavier alkali metals also formed octahedral hexaaqua ions, it has since been found that potassium and rubidium probably form the [K(H2O)8]+ and [Rb(H2O)8]+ ions, which have the square antiprismatic structure, and that caesium forms the 12-coordinate [Cs(H2O)12]+ ion.
Lithium.
The chemistry of lithium shows several differences from that of the rest of the group as the small Li+ cation polarises anions and gives its compounds a more covalent character. Lithium and magnesium have a diagonal relationship due to their similar atomic radii, so that they show some similarities. For example, lithium forms a stable nitride, a property common among all the alkaline earth metals (magnesium's group) but unique among the alkali metals. In addition, among their respective groups, only lithium and magnesium form covalent organometallic compounds (e.g. LiMe and MgMe2).
Lithium fluoride is the only alkali metal halide that is not soluble in water, and lithium hydroxide is the only alkali metal hydroxide that is not deliquescent. Conversely, lithium perchlorate and other lithium salts with large anions that cannot be polarized are much more stable than the analogous compounds of the other alkali metals, probably because Li+ has a high solvation energy. This effect also means that most simple lithium salts are commonly encountered in hydrated form, because the anhydrous forms are extremely hygroscopic: this allows salts like lithium chloride and lithium bromide to be used in dehumidifiers and air-conditioners.
Francium.
Francium is also predicted to show some differences due to its high atomic weight, causing its electrons to travel at considerable fractions of the speed of light and thus making relativistic effects more prominent. In contrast to the trend of decreasing electronegativities and ionisation energies of the alkali metals, francium's electronegativity and ionisation energy are predicted to be higher than caesium's due to the relativistic stabilisation of the 7s electrons; also, its atomic radius is expected to be abnormally low. All known physical properties of francium also deviate from the clear trends going from lithium to caesium, such as the first ionisation energy, electron affinity, and anion polarizability. The CsFr molecule is also polarized as Cs+Fr−, showing that the 7s subshell of francium is much more strongly affected by relativistic effects than the 6s subshell of caesium. Additionally, francium superoxide (FrO2) is expected to have covalent character, unlike the other alkali metal superoxides, because of bonding contributions from the 6p electrons of francium.
Nuclear.
All the alkali metals have odd atomic numbers; hence, their isotopes must be either odd-odd (both proton and neutron number are odd) or odd-even (proton number is odd, but neutron number is even). Odd-odd nuclei have even mass numbers, while odd-even nuclei have odd mass numbers. Odd-odd primordial nuclides are rare because most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.
Due to the great rarity of odd-odd nuclei, almost all the primordial isotopes of the alkali metals are odd-even (the exceptions being the light stable isotope lithium-6 and the long-lived radioisotope potassium-40). For a given odd mass number, there can be only a single beta-stable nuclide, since there is not a difference in binding energy between even-odd and odd-even comparable to that between even-even and odd-odd, leaving other nuclides of the same mass number (isobars) free to beta decay toward the lowest-mass nuclide. An effect of the instability of an odd number of either type of nucleons is that odd-numbered elements, such as the alkali metals, tend to have fewer stable isotopes than even-numbered elements. Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number and all but one also have an even number of neutrons. Beryllium is the single exception to both rules, due to its low atomic number.
All of the alkali metals except lithium and caesium have at least one naturally occurring radioisotope: sodium-22 and sodium-24 are trace radioisotopes produced cosmogenically, potassium-40 and rubidium-87 have very long half-lives and thus occur naturally, and all isotopes of francium are radioactive. Caesium was also thought to be radioactive in the early 20th century, although it has no naturally occurring radioisotopes. (Francium had not been discovered yet at that time.) The natural radioisotope of potassium, potassium-40, makes up about 0.012% of natural potassium, and thus natural potassium is weakly radioactive. This natural radioactivity became a basis for a mistaken claim of the discovery for element 87 (the next alkali metal after caesium) in 1925.
Caesium-137, with a half-life of 30.17 years, is one of the two principal medium-lived fission products, along with strontium-90, which are responsible for most of the radioactivity of spent nuclear fuel after several years of cooling, up to several hundred years after use. It constitutes most of the radioactivity still left from the Chernobyl accident. 137Cs undergoes high-energy beta decay and eventually becomes stable barium-137. It is a strong emitter of gamma radiation. 137Cs has a very low rate of neutron capture and cannot be feasibly disposed of in this way, but must be allowed to decay. 137Cs has been used as a tracer in hydrologic studies, analogous to the use of tritium. Small amounts of caesium-134 and caesium-137 were released into the environment during nearly all nuclear weapon tests and some nuclear accidents, most notably the Goiânia accident and the Chernobyl disaster. As of 2005, caesium-137 is the principal source of radiation in the zone of alienation around the Chernobyl nuclear power plant.
Compounds.
Hydroxides.
All the alkali metals react vigorously or explosively with cold water, producing an aqueous solution of the strongly basic alkali metal hydroxide and releasing hydrogen gas. This reaction becomes more vigorous going down the group: lithium reacts steadily with effervescence, but sodium and potassium can ignite and rubidium and caesium sink in water and generate hydrogen gas so rapidly that shock waves form in the water that may shatter glass containers. When an alkali metal is dropped into water, it produces an explosion, of which there are two separate stages. The metal reacts with the water first, breaking the hydrogen bonds in the water and producing hydrogen gas; this takes place faster for the more reactive heavier alkali metals. Second, the heat generated by the first part of the reaction often ignites the hydrogen gas, causing it to burn explosively into the surrounding air. This secondary hydrogen gas explosion produces the visible flame above the bowl of water, lake or other body of water, not the initial reaction of the metal with water (which tends to happen mostly under water). The alkali metal hydroxides are the most basic known hydroxides.
Compounds with the group 14 elements.
Lithium and sodium react with carbon to form acetylides, Li2C2 and Na2C2, which can also be obtained by reaction of the metal with acetylene. Potassium, rubidium, and caesium react with graphite; their atoms are intercalated between the hexagonal graphite layers, forming graphite intercalation compounds of formulae MC60 (dark grey, almost black), MC48 (dark grey, almost black), MC36 (blue), MC24 (steel blue), and MC8 (bronze) (M = K, Rb, or Cs). These compounds are over 200 times more electrically conductive than pure graphite, suggesting that the valence electron of the alkali metal is transferred to the graphite layers (e.g. ). Upon heating of KC8, the elimination of potassium atoms results in the conversion in sequence to KC24, KC36, KC48 and finally KC60. KC8 is a very strong reducing agent and is pyrophoric and explodes on contact with water. While the large alkali metals (K, Rb, and Cs) initially form MC8, the smaller ones initially form MC6.
When the alkali metals react with the heavier elements in the carbon group, ionic substances with cage-like structures are formed, such as the silicide M4Si4 (M = K, Rb, or Cs), which contains M+ and tetrahedral ions. The chemistry of alkali metal germanides, involving the germanide ion Ge4− and other cluster (Zintl) ions such as , , , and [(Ge9)2]6−, is largely analogous to that of the corresponding silicides. Alkali metal stannides are mostly ionic, sometimes with the stannide ion (Sn4−), and sometimes with more complex Zintl ions such as , which appears in tetrapotassium nonastannide (K4Sn9). The monatomic plumbide ion (Pb4−) is unknown, and indeed its formation is predicted to be energetically unfavourable; alkali metal plumbides have complex Zintl ions, such as .
Nitrides and pnictides.
Lithium, the lightest of the alkali metals, is the only alkali metal which reacts with nitrogen at standard conditions, and its nitride is the only stable alkali metal nitride. Nitrogen is an unreactive gas because breaking the strong triple bond in the dinitrogen molecule (N2) requires a lot of energy. The formation of an alkali metal nitride would consume the ionisation energy of the alkali metal (forming M+ ions), the energy required to break the triple bond in N2 and the formation of N3− ions, and all the energy released from the formation of an alkali metal nitride is from the lattice energy of the alkali metal nitride. The lattice energy is maximised with small, highly charged ions; the alkali metals do not form highly charged ions, only forming ions with a charge of +1, so only lithium, the smallest alkali metal, can release enough lattice energy to make the reaction with nitrogen exothermic, forming lithium nitride. The reactions of the other alkali metals with nitrogen would not release enough lattice energy and would thus be endothermic, so they do not form nitrides at standard conditions. (Sodium nitride (Na3N) and potassium nitride (K3N), while existing, are extremely unstable, being prone to decomposing back into their constituent elements, and cannot be produced by reacting the elements with each other at standard conditions.)
All the alkali metals react readily with phosphorus and arsenic to form phosphides and arsenides with the formula M3Pn (where M represents an alkali metal and Pn represents a pnictogen). This is due to the greater size of the P3− and As3− ions, so that less lattice energy needs to be released for the salts to form. These are not the only phosphides and arsenides of the alkali metals: for example, potassium has nine different known phosphides, with formulae K3P, K4P3, K5P4, KP, K4P6, K3P7, K3P11, KP10.3, and KP15. While most metals form arsenides, only the alkali and alkaline earth metals form mostly ionic arsenides. The structure of Na3As is complex with unusually short Na–Na distances of 328–330 pm which are shorter than in sodium metal, and this indicates that even with these electropositive metals the bonding cannot be straightforwardly ionic. Other alkali metal arsenides not conforming to the formula M3As are known, such as LiAs, which has a metallic lustre and electrical conductivity indicating the presence of some metallic bonding. The antimonides are unstable and reactive as the Sb3− ion is a strong reducing agent; reaction of them with acids form the toxic and unstable gas stibine (SbH3). Bismuthides are not even wholly ionic; they are intermetallic compounds containing partially metallic and partially ionic bonds.
Oxides and chalcogenides.
All the alkali metals react vigorously with oxygen at standard conditions. They form various types of oxides, such as simple oxides (containing the O2− ion), peroxides (containing the ion, where there is a single bond between the two oxygen atoms), superoxides (containing the ion), and many others. Lithium burns in air to form lithium oxide, but sodium reacts with oxygen to form a mixture of sodium oxide and sodium peroxide. Potassium forms a mixture of potassium peroxide and potassium superoxide, while rubidium and caesium form the superoxide exclusively. Their reactivity increases going down the group: while lithium, sodium and potassium merely burn in air, rubidium and caesium are pyrophoric (spontaneously catch fire in air).
The smaller alkali metals tend to polarise the more complex anions (the peroxide and superoxide) due to their small size. This attracts the electrons in the more complex anions towards one of its constituent oxygen atoms, forming an oxide ion and an oxygen atom. This causes lithium to form the oxide exclusively on reaction with oxygen at room temperature. This effect becomes drastically weaker for the larger sodium and potassium, allowing them to form the less stable peroxides. Rubidium and caesium, at the bottom of the group, are so large that even the least stable superoxides can form. Because the superoxide releases the most energy when formed, the superoxide is preferentially formed for the larger alkali metals where the more complex anions are not polarised. (The oxides and peroxides for these alkali metals do exist, but do not form upon direct reaction of the metal with oxygen at standard conditions.) In addition, the small size of the Li+ and O2− ions contributes to their forming a stable ionic lattice structure. Under controlled conditions, however, all the alkali metals, with the exception of francium, are known to form their oxides, peroxides, and superoxides. The alkali metal peroxides and superoxides are powerful oxidizing agents. Sodium peroxide and potassium superoxide react with carbon dioxide to form the alkali metal carbonate and oxygen gas, which allows them to be used in submarine air purifiers; the presence of water vapour, naturally present in breath, makes the removal of carbon dioxide by potassium superoxide even more efficient. All the stable alkali metals except lithium can form red ozonides (MO3) through low-temperature reaction of the powdered anhydrous hydroxide with ozone: the ozonides may be then extracted using liquid ammonia.
Rubidium and caesium can form even more complicated oxides than the superoxides. Rubidium can form Rb6O and Rb9O2 upon oxidation in air, while caesium forms an immense variety of oxides, such as the ozonide CsO3 and several brightly coloured suboxides, such as , , , (dark-green), CsO, , as well as . The latter may be heated under vacuum to generate .
The alkali metals can also react analogously with the heavier chalcogens (sulfur, selenium, tellurium, and polonium), and all the alkali metal chalcogenides are known (with the exception of francium's). Reaction with an excess of the chalcogen can similarly result in lower chalcogenides, with chalcogen ions containing chains of the chalcogen atoms in question. For example, sodium can react with sulfur to form the sulfide (Na2S) and various polysulfides with the formula Na2S"x" ("x" from 2 to 6), containing the ions. Due to the basicity of the Se2− and Te2− ions, the alkali metal selenides and tellurides are alkaline in solution; when reacted directly with selenium and tellurium, alkali metal polyselenides and polytellurides are formed along with the selenides and tellurides with the and ions. The alkali metal polonides are all ionic compounds containing the Po2− ion; they are very chemically stable and can be produced by direct reaction of the elements at around 300–400 °C.
Hydrides and halides.
The alkali metals are among the most electropositive elements on the periodic table and thus tend to bond ionically to the most electronegative elements on the periodic table, the halogens, forming salts known as the alkali metal halides. The reaction is very vigorous and can sometimes result in explosions. This includes sodium chloride, otherwise known as common salt. The reactivity becomes higher from lithium to caesium and drops from fluorine to iodine. All of the alkali metal halides have the formula MX where M is an alkali metal and X is a halogen. They are all white ionic crystalline solids. All the alkali metal halides are soluble in water except for lithium fluoride (LiF), which is insoluble in water due to its very high lattice enthalpy. The high lattice enthalpy of lithium fluoride is due to the small sizes of the Li+ and F− ions, causing the electrostatic interactions between them to be strong: a similar effect occurs for magnesium fluoride, which lithium has a diagonal relationship with. The alkali metals also react similarly with hydrogen to form ionic alkali metal hydrides.
Coordination complexes.
Alkali metal cations do not usually form coordination complexes with simple Lewis bases due to their low charge of just +1 and their relatively large size; thus the Li+ ion forms most complexes and the heavier alkali metal ions form less and less. In aqueous solution, the alkali metal ions exist as octahedral hexahydrate complexes ([M(H2O)6)]+), with the exception of the lithium ion, which due to its small size forms tetrahedral tetrahydrate complexes ([Li(H2O)4)]+); the alkali metals form these complexes because their ions are attracted by electrostatic forces of attraction to the polar water molecules. Because of this, anhydrous salts containing alkali metal cations are often used as desiccants. Alkali metals also readily form complexes with crown ethers (e.g. 12-crown-4 for Li+, 15-crown-5 for Na+, and 18-crown-6 for K+) and cryptands due to electrostatic attraction.
Ammonia solutions.
Unlike most metals, the alkali metals dissolve slowly in liquid ammonia, forming hydrogen gas and the alkali metal amide (MNH2, where M represents an alkali metal): this was first noted by Humphry Davy in 1809 and rediscovered by W. Weyl in 1864. The process may be speeded up by a catalyst. The amide salt is quite insoluble and readily precipitates out of solution, leaving intensely coloured ammonia solutions of the alkali metals. In 1907, Charles Krause identified the colour as being due to the presence of solvated electrons, which contribute to the high electrical conductivity of these solutions. At low concentrations (below 3 M), the solution is dark blue and has ten times the conductivity of aqueous sodium chloride; at higher concentrations (above 3 M), the solution is copper-coloured and has approximately the conductivity of liquid metals like mercury. In addition to the alkali metal amide salt and solvated electrons, such ammonia solutions also contain the alkali metal cation (M+), the neutral alkali metal atom (M), diatomic alkali metal molecules (M2) and alkali metal anions (M−). These are unstable and eventually become the more thermodynamically stable alkali metal amide and hydrogen gas. Solvated electrons are powerful reducing agents and are often used in chemical synthesis.
Organometallic.
Being the smallest alkali metal, lithium forms the widest variety of and most stable organometallic compounds, which are bonded covalently. Organolithium compounds are electrically non-conducting volatile solids or liquids that melt at low temperatures, and tend to form oligomers with the structure (RLi)"x" where R is the organic group. As the electropositive nature of lithium puts most of the charge density of the bond on the carbon atom, effectively creating a carbanion, organolithium compounds are extremely powerful bases and nucleophiles. For use as bases, butyllithiums are often used and are commercially available. An example of an organolithium compound is methyllithium ((CH3Li)"x"), which exists in tetrameric ("x" = 4) and hexameric ("x" = 6) forms.
The application of organosodium compounds in chemistry is limited in part due to competition from organolithium compounds, which are commercially available and exhibit more convenient reactivity. The principal organosodium compound of commercial importance is sodium cyclopentadienide. Sodium tetraphenylborate can also be classified as an organosodium compound since in the solid state sodium is bound to the aryl groups. Organometallic compounds of the higher alkali metals are even more reactive than organosodium compounds and of limited utility. A notable reagent is Schlosser's base, a mixture of "n"-butyllithium and potassium "tert"-butoxide. This reagent reacts with propene to form the compound allylpotassium (KCH2CHCH2). "cis"-2-Butene and "trans"-2-butene equilibrate when in contact with alkali metals. Whereas isomerization is fast with lithium and sodium, it is slow with the higher alkali metals. The higher alkali metals also favour the sterically congested conformation. Several crystal structures of organopotassium compounds have been reported, establishing that they, like the sodium compounds, are polymeric. Organosodium, organopotassium, organorubidium and organocaesium compounds are all mostly ionic and are insoluble (or nearly so) in nonpolar solvents.
Extensions.
Although francium is the heaviest alkali metal that has been discovered, there has been some theoretical work predicting the physical and chemical characteristics of the hypothetical heavier alkali metals. Being the first period 8 element, the undiscovered element ununennium (element 119) is predicted to be the next alkali metal after francium and behave much like their lighter congeners; however, it is also predicted to differ from the lighter alkali metals in some properties. Its chemistry is predicted to be closer to that of potassium or rubidium instead of caesium or francium. This is unusual as periodic trends, ignoring relativistic effects would predict ununennium to be even more reactive than caesium and francium. This lowered reactivity is due to the relativistic stabilisation of ununennium's valence electron, increasing ununennium's first ionisation energy and decreasing the metallic and ionic radii; this effect is already seen for francium. This assumes that ununennium will behave chemically as an alkali metal, which, although likely, may not be true due to relativistic effects. The relativistic stabilisation of the 8s orbital also increases ununennium's electron affinity far beyond that of caesium and francium; indeed, ununennium is expected to have an electron affinity higher than all the alkali metals lighter than it. Relativistic effects also cause a very large drop in the polarisability of ununennium. On the other hand, ununennium is predicted to continue the trend of melting points decreasing going down the group, being expected to have a melting point between 0 °C and 30 °C.
The stabilisation of ununennium's valence electron and thus the contraction of the 8s orbital cause its atomic radius to be lowered to 240 pm, very close to that of rubidium (247 pm), so that the chemistry of ununennium in the +1 oxidation state should be more similar to the chemistry of rubidium than to that of francium. On the other hand, the ionic radius of the Uue+ ion is predicted to be larger than that of Rb+, because the 7p orbitals are destabilised and are thus larger than the p-orbitals of the lower shells. Ununennium may also show the +3 oxidation state, which is not seen in any other alkali metal, in addition to the +1 oxidation state that is characteristic of the other alkali metals and is also the main oxidation state of all the known alkali metals: this is because of the destabilisation and expansion of the 7p3/2 spinor, causing its outermost electrons to have a lower ionisation energy than what would otherwise be expected. Indeed, many ununennium compounds are expected to have a large covalent character, due to the involvement of the 7p3/2 electrons in the bonding.
Not as much work has been done predicting the properties of the alkali metals beyond ununennium. Although a simple extrapolation of the periodic table would put element 169, unhexennium, under ununennium, Dirac-Fock calculations predict that the next alkali metal after ununennium may actually be element 165, unhexpentium, which is predicted to have the electron configuration [Uuo] 5g18 6f14 7d10 8s2 8p1/22 9s1. Further calculations show that unhexpentium would follow the trend of increasing ionisation energy beyond caesium, having an ionisation energy comparable to that of sodium, and that it should also continue the trend of decreasing atomic radii beyond caesium, having an atomic radius comparable to that of potassium. However, the 7d electrons of unhexpentium may also be able to participate in chemical reactions along with the 9s electron, possibly allowing oxidation states beyond +1 and perhaps even making unhexpentium behave more like a boron group element or group 11 element than an alkali metal. Due to the alkali and alkaline earth metals both being s-block elements, these predictions for the trends and properties of ununennium and unhexpentium also mostly hold quite similarly for the corresponding alkaline earth metals unbinilium (Ubn) and unhexhexium (Uhh).
The probable properties of further alkali metals beyond unhexpentium have not been explored yet as of 2012. In periods 8 and above of the periodic table, relativistic and shell-structure effects become so strong that extrapolations from lighter congeners become completely inaccurate. In addition, the relativistic and shell-structure effects (which stabilise the s-orbitals and destabilise and expand the d-, f-, and g-orbitals of higher shells) have opposite effects, causing even larger difference between relativistic and non-relativistic calculations of the properties of elements with such high atomic numbers. Interest in the chemical properties of ununennium and unhexpentium stems from the fact that both elements are located close to the expected locations of islands of stabilities, centered at elements 122 (306Ubb) and 164 (482Uhq).
Other similar substances.
Hydrogen.
The element hydrogen, with one electron per neutral atom, is usually placed at the top of Group 1 of the periodic table for convenience, but hydrogen is not normally considered to be an alkali metal; when it is considered to be an alkali metal, it is because of its atomic properties and not its chemical properties. Under typical conditions, pure hydrogen exists as a diatomic gas consisting of two atoms per molecule (H2); however, the alkali metals only form diatomic molecules (such as dilithium, Li2) at high temperatures, when they are in the gaseous state.
Hydrogen, like the alkali metals, has one valence electron and reacts easily with the halogens but the similarities end there. Its placement above lithium is primarily due to its electron configuration and not its chemical properties. It is sometimes placed above carbon due to their similar electronegativities or fluorine due to their similar chemical properties.
The first ionisation energy of hydrogen (1312.0 kJ/mol) is much higher than that of the alkali metals. As only one additional electron is required to fill in the outermost shell of the hydrogen atom, hydrogen often behaves like a halogen, forming the negative hydride ion, and is sometimes considered to be a halogen. (The alkali metals can also form negative ions, known as alkalides, but these are little more than laboratory curiosities, being unstable.) It was expected for some time that liquid hydrogen would show metallic properties; while this has been shown to not be the case, under extremely high pressures, such as those found at the cores of Jupiter and Saturn, hydrogen does become metallic and behaves like an alkali metal; in this phase, it is known as metallic hydrogen. The electrical resistivity of liquid metallic hydrogen at 3000 K is approximately equal to that of liquid rubidium and caesium at 2000 K at the respective pressures when they undergo a nonmetal-to-metal transition.
The 1s1 electron configuration of hydrogen, while superficially similar to that of the alkali metals (ns1), is unique because there is no 1p subshell. Hence it can lose an electron to form the hydron H+, or gain one to form the hydride ion H−. In the former case it resembles superficially the alkali metals; in the latter case, the halogens, but the differences due to the lack of a 1p subshell are important enough that neither group fits the properties of hydrogen well. Group 14 is the best fit in terms of thermodynamic properties such as ionization energy and electron affinity, but none of the three placements are entirely satisfactory. As an example of hydrogen's unorthodox properties stemming from its unusual electron configuration and small size, the hydrogen ion is very small (radius around 150 fm compared to the 50–220 pm size of most other atoms and ions) and so is nonexistent in condensed systems other than in association with other atoms or molecules. Indeed, transferring of protons between chemicals is the basis of acid-base chemistry. Also unique is hydrogen's ability to form hydrogen bonds, which are an effect of charge-transfer, electrostatic, and electron correlative contributing phenomena. While analogous lithium bonds are also known, they are mostly electrostatic. Nevertheless, hydrogen can perform the same structural role as the alkali metals in some molecular crystals, and has a close relationship with the lightest alkali metals (especially lithium).
Ammonium.
The ammonium ion () has very similar properties to the heavier alkali metals, acting as an alkali metal intermediate between potassium and rubidium, and is often considered a close relative. For example, most alkali metal salts are soluble in water, a property which ammonium salts share. Ammonium is expected to behave stably as a metal ( ions in a sea of electrons) at very high pressures (though less than the typical pressure where transitions from insulating to metallic behaviour occur around, 100 GPa), and could possibly occur inside the ice giants Uranus and Neptune, which may have significant impacts on their interior magnetic fields. It has been estimated that the transition from a mixture of ammonia and dihydrogen molecules to metallic ammonium may occur at pressures just below 25 GPa.
Thallium.
Thallium displays the +1 oxidation state that all the known alkali metals display, and thallium compounds with thallium in its +1 oxidation state closely resemble the corresponding potassium or silver compounds stoichiometrically due to the similar ionic radii of the Tl+ (164 pm), K+ (152 pm) and Ag+ (129 pm) ions. It was sometimes considered an alkali metal in continental Europe (but not in England) in the years immediately following its discovery, and was placed just after caesium as the sixth alkali metal in Dmitri Mendeleev's 1869 periodic table and Julius Lothar Meyer's 1868 periodic table. (Mendeleev's 1871 periodic table and Meyer's 1870 periodic table put thallium in its current position in the boron group and leave the space below caesium blank.) However, thallium also displays the oxidation state +3, which no known alkali metal displays (although ununennium, the undiscovered seventh alkali metal, is predicted to possibly display the +3 oxidation state). The sixth alkali metal is now considered to be francium. While Tl+ is stabilized by the inert pair effect, this inert pair of 6s electrons is still able to participate chemically, so that these electrons are stereochemically active in aqueous solution. Additionally, the thallium halides (except TlF) are quite insoluble in water, and TlI has an unusual structure because of the presence of the inert pair in thallium.
Copper, silver, and gold.
The group 11 metals (or coinage metals), copper, silver, and gold, are typically categorised as transition metals given they can form ions with incomplete d-shells. Physically, they have the relatively low melting points and high electronegativity values associated with post-transition metals. "The filled "d" subshell and free "s" electron of Cu, Ag, and Au contribute to their high electrical and thermal conductivity. Transition metals to the left of group 11 experience interactions between "s" electrons and the partially filled "d" subshell that lower electron mobility." Chemically, the group 11 metals behave like main-group metals in their +1 valence states, and are hence somewhat related to the alkali metals: this is one reason for their previously being labelled as "group IB", paralleling the alkali metals' "group IA". They are occasionally classified as post-transition metals. Their spectra are however analogous to those of the alkali metals.
In Mendeleev's 1871 periodic table, copper, silver, and gold are listed twice, once under group VIII (with the iron triad and platinum group metals), and once under group IB. Group IB was nonetheless parenthesized to note that it was tentative. Mendeleev's main criterion for group assignment was the maximum oxidation state of an element: on that basis, the group 11 elements could not be classified in group IB, due to the existence of Cu(II) and Au(III) compounds being known at that time. However, eliminating group IB would make group I the only main group (group VIII was labelled a transition group) to lack an A–B bifurcation. Soon afterwards, a majority of chemists chose to classify these elements in group IB and remove them from group VIII for the resulting symmetry: this was the predominant classification until the rise of the modern medium-long 18-column periodic table, which separated the alkali metals and group 11 metals.
The coinage metals were traditionally regarded as a subdivision of the alkali metal group, due to them sharing the characteristic s1 electron configuration of the alkali metals (group 1: p6s1; group 11: d10s1). However, the similarities are largely confined to the stochiometries of the +1 compounds of both groups, and not their chemical properties.. This stems from the filled d subshell providing a much weaker shielding effect on the outermost s electron than the filled p subshell, so that the coinage metals have much higher first ionization energies and smaller ionic radii than do the corresponding alkali metals. Furthermore, they have higher melting points, hardnesses, and densities, and lower reactivities and solubilities in liquid ammonia, as well as having more covalent character in their compounds. Finally, the alkali metals are at the top of the electrochemical series, whereas the coinage metals are almost at the very bottom. The coinage metals' filled d shell is much more easily disrupted than the alkali metals' filled p shell, so that the second and third ionization energies are lower, enabling higher oxidation states than +1 and a richer coordination chemistry, thus giving the group 11 metals clear transition metal character. Particularly noteworthy is gold forming ionic compounds with rubidium and caesium, in which it forms the auride ion (Au−) which also occurs in solvated form in liquid ammonia solution: here gold behaves as a pseudohalogen because its 5d106s1 configuration has one electron less than the quasi-closed shell 5d106s2 configuration of mercury.
History.
Sodium compounds have been known since ancient times; salt (sodium chloride) has been an important commodity in human activities, as testified by the English word "salary", referring to "salarium", the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe a compound of sodium with the Latin name of "sodanum" was used as a headache remedy. While potash has been used since ancient times, it was not understood for most of its history to be a fundamentally different substance from sodium mineral salts. Georg Ernst Stahl obtained experimental evidence which led him to suggest the fundamental difference of sodium and potassium salts in 1702, and Henri Louis Duhamel du Monceau was able to prove this difference in 1736. The exact chemical composition of potassium and sodium compounds, and the status as chemical element of potassium and sodium, was not known then, and thus Antoine Lavoisier did include the alkali in his list of chemical elements in 1789.
Pure potassium was first isolated in 1807 in England by Sir Humphry Davy, who derived it from caustic potash (KOH, potassium hydroxide) by the use of electrolysis of the molten salt with the newly invented voltaic pile. Previous attempts at electrolysis of the aqueous salt were unsuccessful due to potassium's extreme reactivity. Potassium was the first metal that was isolated by electrolysis. Later that same year, Davy reported extraction of sodium from the similar substance caustic soda (NaOH, lye) by a similar technique, demonstrating the elements, and thus the salts, to be different. Later that year, the first pieces of pure molten sodium metal were similarly prepared by Humphry Davy through the electrolysis of molten caustic soda (now called sodium hydroxide).
Petalite (LiAlSi4O10) was discovered in 1800 by the Brazilian chemist José Bonifácio de Andrada in a mine on the island of Utö, Sweden. However, it was not until 1817 that Johan August Arfwedson, then working in the laboratory of the chemist Jöns Jacob Berzelius, detected the presence of a new element while analysing petalite ore. This new element was noted by him to form compounds similar to those of sodium and potassium, though its carbonate and hydroxide were less soluble in water and more alkaline than the other alkali metals. Berzelius gave the unknown material the name ""lithion"/"lithina"", from the Greek word "λιθoς" (transliterated as "lithos", meaning "stone"), to reflect its discovery in a solid mineral, as opposed to potassium, which had been discovered in plant ashes, and sodium, which was known partly for its high abundance in animal blood. He named the metal inside the material "lithium". Lithium, sodium, and potassium were part of the discovery of periodicity, as they are among a series of triads of elements in the same group that were noted by Johann Wolfgang Döbereiner in 1850 as having similar properties.
Rubidium and caesium were the first elements to be discovered using the spectroscope, invented in 1859 by Robert Bunsen and Gustav Kirchhoff. The next year, they discovered caesium in the mineral water from Bad Dürkheim, Germany. Their discovery of rubidium came the following year in Heidelberg, Germany, finding it in the mineral lepidolite. The names of rubidium and caesium come from the most prominent lines in their emission spectra: a bright red line for rubidium (from the Latin word "rubidus", meaning dark red or bright red), and a sky-blue line for caesium (derived from the Latin word "caesius", meaning sky-blue).
Around 1865 John Newlands produced a series of papers where he listed the elements in order of increasing atomic weight and similar physical and chemical properties that recurred at intervals of eight; he likened such periodicity to the octaves of music. His version put all the alkali metals then known (lithium to caesium), as well as copper, silver, and thallium (which show the +1 oxidation state characteristic of the alkali metals), together into a group. His table placed hydrogen with the halogens.
After 1869, Dmitri Mendeleev proposed his periodic table placing lithium at the top of a group with sodium, potassium, rubidium, caesium, and thallium. Two years later, Mendeleev revised his table, placing hydrogen in group 1 above lithium, and also moving thallium to the boron group. In this 1871 version, copper, silver, and gold were placed twice, once as part of group IB, and once as part of a "group VIII" encompassing today's groups 8 to 11. After the introduction of the 18-column table, the group IB elements were moved to their current position in the d-block, while alkali metals were left in "group IA". Later the group's name was changed to "group 1" in 1988. The trivial name "alkali metals" comes from the fact that the hydroxides of the group 1 elements are all strong alkalis when dissolved in water.
There were at least four erroneous and incomplete discoveries before Marguerite Perey of the Curie Institute in Paris, France discovered francium in 1939 by purifying a sample of actinium-227, which had been reported to have a decay energy of 220 keV. However, Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one that was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, caused by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure that she later revised to 1%. It was the last element discovered in nature, rather than by synthesis.
The next element below francium (eka-francium) is very likely to be ununennium (Uue), element 119, although this is not completely certain due to relativistic effects. The synthesis of ununennium was first attempted in 1985 by bombarding a target of einsteinium-254 with calcium-48 ions at the superHILAC accelerator at Berkeley, California. No atoms were identified, leading to a limiting yield of 300 nb.
It is highly unlikely that this reaction will be able to create any atoms of ununennium in the near future, given the extremely difficult task of making sufficient amounts of 254Es, which is favoured for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms, to make a large enough target to increase the sensitivity of the experiment to the required level; einsteinium has not been found in nature and has only been produced in laboratories. However, given that ununennium is only the first period 8 element on the extended periodic table, it may well be discovered in the near future through other reactions; indeed, another attempt to synthesise ununennium by bombarding a berkelium target with titanium ions is under way at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany. Currently, none of the period 8 elements have been discovered yet, and it is also possible, due to drip instabilities, that only the lower period 8 elements, up to around element 128, are physically possible. No attempts at synthesis have been made for any heavier alkali metals, such as unhexpentium, due to their extremely high atomic number.
Occurrence.
In the Solar System.
The Oddo-Harkins rule holds that elements with even atomic numbers are more common that those with odd atomic numbers, with the exception of hydrogen. This rule argues that elements with odd atomic numbers have one unpaired proton and are more likely to capture another, thus increasing their atomic number. In elements with even atomic numbers, protons are paired, with each member of the pair offsetting the spin of the other, enhancing stability. All the alkali metals have odd atomic numbers and they are not as common as the elements with even atomic numbers adjacent to them (the noble gases and the alkaline earth metals) in the Solar System. The heavier alkali metals are also less abundant than the lighter ones as the alkali metals from rubidium onward can only be synthesized in supernovae and not in stellar nucleosynthesis. Lithium is also much less abundant than sodium and potassium as it is poorly synthesized in both Big Bang nucleosynthesis and in stars: the Big Bang could only produce trace quantities of lithium, beryllium and boron due to the absence of a stable nucleus with 5 or 8 nucleons, and stellar nucleosynthesis could only pass this bottleneck by the triple-alpha process, fusing three helium nuclei to form carbon, and skipping over those three elements.
On Earth.
The Earth formed from the same cloud of matter that formed the Sun, but the planets acquired different compositions during the formation and evolution of the solar system. In turn, the natural history of the Earth caused parts of this planet to have differing concentrations of the elements. The mass of the Earth is approximately 5.98 kg. It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.
The alkali metals, due to their high reactivity, do not occur naturally in pure form in nature. They are lithophiles and therefore remain close to the Earth's surface because they combine readily with oxygen and so associate strongly with silica, forming relatively low-density minerals that do not sink down into the Earth's core. Potassium, rubidium and caesium are also incompatible elements due to their large ionic radii.
Sodium and potassium are very abundant in earth, both being among the ten most common elements in Earth's crust; sodium makes up approximately 2.6% of the Earth's crust measured by weight, making it the sixth most abundant element overall and the most abundant alkali metal. Potassium makes up approximately 1.5% of the Earth's crust and is the seventh most abundant element. Sodium is found in many different minerals, of which the most common is ordinary salt (sodium chloride), which occurs in vast quantities dissolved in seawater. Other solid deposits include halite, amphibole, cryolite, nitratine, and zeolite. Many of these solid deposits occur as a result of ancient seas evaporating, which still occurs now in places such as Utah's Great Salt Lake and the Dead Sea. Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium.
Despite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size. Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that seawater is approximately 0.14 to 0.25 parts per million (ppm) or 25 micromolar. Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium. Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide.
Rubidium is approximately as abundant as zinc and more abundant than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals. Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten, but is much less abundant than rubidium.
Francium-223, the only naturally occurring isotope of francium, is the product of the alpha decay of actinium-227 and can be found in trace amounts in uranium and thorium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 1018 uranium atoms. It has been calculated that there is at most 30 g of francium in the earth's crust at any time, due to its extremely short half-life of 22 minutes.
Production and isolation.
The production of pure alkali metals is difficult due to their extreme reactivity with commonly used substances, such as water. The alkali metals are so reactive that they cannot be displaced by other elements and must be isolated through high-energy methods such as electrolysis.
Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride.
Potassium occurs in many minerals, such as sylvite (potassium chloride). It is occasionally produced through separating the potassium from the chlorine in potassium chloride, but is more often produced through electrolysis of potassium hydroxide, found extensively in places such as Canada, Russia, Belarus, Germany, Israel, United States, and Jordan, in a method similar to how sodium was produced in the late 1800s and early 1900s. It can also be produced from seawater. Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 °C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide.
For several years in the 1950s and 1960s, a by-product of the potassium production called Alkarb was a main source for rubidium. Alkarb contained 21% rubidium while the rest was potassium and a small fraction of caesium. Today the largest producers of caesium, for example the Tanco Mine, Manitoba, Canada, produce rubidium as by-product from pollucite. Today, a common method for separating rubidium from potassium and caesium is the fractional crystallization of a rubidium and caesium alum (Cs, Rb)Al(SO4)2·12H2O, which yields pure rubidium alum after approximately 30 different reactions. The limited applications and the lack of a mineral rich in rubidium limits the production of rubidium compounds to 2 to 4 tonnes per year. Caesium, however, is not produced from the above reaction. Instead, the mining of pollucite ore is the main method of obtaining pure caesium, extracted from the ore mainly by three methods: acid digestion, alkaline decomposition, and direct reduction. Both metals are produced as by-products of lithium production: after 1958, when interest in lithium's thermonuclear properties increased sharply, the production of rubidium and caesium also increased correspondingly.
Francium-223, the only naturally occurring isotope of francium, is produced naturally as the product of the alpha decay of actinium-227. Francium can be found in trace amounts in uranium and thorium minerals; it has been calculated that at most there are 30 g of francium in the earth's crust at any given time. As a result of its extreme rarity in nature, most francium is synthesized in the nuclear reaction 197Au + 18O → 210Fr + 5 n, yielding francium-209, francium-210, and francium-211. The greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, which were synthesized using the nuclear reaction given above.
From their silicate ores, all the alkali metals may be obtained the same way: sulfuric acid is first used to dissolve the desired alkali metal ion and aluminium(III) ions from the ore (leaching), whereupon basic precipitation removes aluminium ions from the mixture by precipitating it as the hydroxide. The remaining insoluble alkali metal carbonate is then precipitated selectively; the salt is then dissolved in hydrochloric acid. The result is then left to evaporate and the alkali metal can then be isolated through electrolysis.
Lithium and sodium are typically isolated through electrolysis from their liquid chlorides, with calcium chloride typically added to lower the melting point of the mixture. The heavier alkali metals, however, is more typically isolated in a different way, where a reducing agent (typically sodium for potassium and magnesium or calcium for the heaviest alkali metals) is used to reduce the alkali metal chloride. The liquid or gaseous product (the alkali metal) then undergoes fractional distillation for purification.
Applications.
All of the discovered alkali metals excluding francium have many applications. Lithium is often used in batteries, and lithium oxide can help process silica. Lithium can also be used to make lubricating greases, air treatment, and aluminium production.
Pure sodium has many applications, including use in sodium-vapour lamps, which produce very efficient light compared to other types of lighting, and can help smooth the surface of other metals. Being a strong reducing agent, it is often used to reduce many other metals, such as titanium and zirconium, from their chlorides. Sodium compounds have many applications as well, the most well-known compound being table salt. Sodium is also used in soap as salts of fatty acids.
Potassium compounds are often used as fertilisers as potassium is an important element for plant nutrition. Other potassium ions are often used to hold anions. Potassium hydroxide is a very strong base, and is used to control the pH of various substances.
Rubidium and caesium are often used in atomic clocks. Caesium atomic clocks are extraordinarily accurate; if a clock had been made at the time of the dinosaurs, it would be off by less than four seconds (after 80 million years). For that reason, caesium atoms are used as the definition of the second. Rubidium ions are often used in purple fireworks, and caesium is often used in drilling fluids in the petroleum industry.
Francium has no commercial applications, but because of francium's relatively simple atomic structure, among other things, it has been used in spectroscopy experiments, leading to more information regarding energy levels and the coupling constants between subatomic particles. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels, similar to those predicted by quantum theory.
Biological role and precautions.
Lithium naturally only occurs in traces in biological systems and has no known biological role, but does have effects on the body when ingested. Lithium carbonate is used as a mood stabiliser in psychiatry to treat bipolar disorder (manic-depression) in daily doses of about 0.5 to 2 grams, although there are side-effects. Excessive ingestion of lithium causes drowsiness, slurred speech and vomiting, among other symptoms, and poisons the central nervous system, which is dangerous as the required dosage of lithium to treat bipolar disorder is only slightly lower than the toxic dosage. Its biochemistry, the way it is handled by the human body and studies using rats and goats suggest that it is an essential trace element, although the natural biological function of lithium in humans has yet to be identified.
Sodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The DRI for sodium is 1.5 grams per day, but most people in the United States consume more than 2.3 grams per day, the minimum amount that promotes hypertension; this in turn causes 7.6 million premature deaths worldwide.
Potassium is the major cation (positive ion) inside animal cells, while sodium is the major cation outside animal cells. The concentration differences of these charged particles causes a difference in electric potential between the inside and outside of cells, known as the membrane potential. The balance between potassium and sodium is maintained by ion pumps in the cell membrane. The cell membrane potential created by potassium and sodium ions allows the cell to generate an action potential—a "spike" of electrical discharge. The ability of cells to produce electrical discharge is critical for body functions such as neurotransmission, muscle contraction, and heart function.
Rubidium has no known biological role, but may help stimulate metabolism, and, similarly to caesium, replace potassium in the body causing potassium deficiency. Caesium compounds are rarely encountered by most people, but most caesium compounds are mildly toxic because of chemical similarity of caesium to potassium, allowing the caesium to replace the potassium in the body, causing potassium deficiency. Exposure to large amounts of caesium compounds can cause hyperirritability and spasms, but as such amounts would not ordinarily be encountered in natural sources, caesium is not a major chemical environmental pollutant. The median lethal dose (LD50) value for caesium chloride in mice is 2.3 g per kilogram, which is comparable to the LD50 values of potassium chloride and sodium chloride. Caesium chloride has been promoted as an alternative cancer therapy, but has been linked to the deaths of over 50 patients, on whom it was used as part of a scientifically unvalidated cancer treatment. Radioisotopes of caesium require special precautions: the improper handling of caesium-137 gamma ray sources can lead to release of this radioisotope and radiation injuries. Perhaps the best-known case is the Goiânia accident of 1987, in which an improperly-disposed-of radiation therapy system from an abandoned clinic in the city of Goiânia, Brazil, was scavenged from a junkyard, and the glowing caesium salt sold to curious, uneducated buyers. This led to four deaths and serious injuries from radiation exposure. Together with caesium-134, iodine-131, and strontium-90, caesium-137 was among the isotopes distributed by the Chernobyl disaster which constitute the greatest risk to health.
Francium has no biological role and is most likely to be toxic due to its extreme radioactivity, causing radiation poisoning, but since the greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, it is unlikely that most people will ever encounter francium.

</doc>
<doc id="670" url="http://en.wikipedia.org/wiki?curid=670" title="Alphabet">
Alphabet

An alphabet is a standard set of letters (basic written symbols or graphemes) which is used to write one or more languages based on the general principle that the letters represent phonemes (basic significant sounds) of the spoken language. This is in contrast to other types of writing systems, such as syllabaries (in which each character represents a syllable) and logographies (in which each character represents a word, morpheme, or semantic unit).
A true alphabet has letters for the vowels of a language as well as the consonants. The first "true alphabet" in this sense is believed to be the Greek alphabet, which is a modified form of the Phoenician alphabet. In other types of alphabet either the vowels are not indicated at all, as was the case in the Phoenician alphabet (such systems are known as abjads), or else the vowels are shown by diacritics or modification of consonants, as in the devanagari used in India and Nepal (these systems are known as abugidas or alphasyllabaries).
There are dozens of alphabets in use today, the most popular being the Latin alphabet (which was derived from the Greek). Many languages use modified forms of the Latin alphabet, with additional letters formed using diacritical marks. While most alphabets have letters composed of lines (linear writing), there are also exceptions such as the alphabets used in Braille, fingerspelling, and Morse code.
Alphabets are usually associated with a standard ordering of their letters. This makes them useful for purposes of collation, specifically by allowing words to be sorted in alphabetical order. It also means that their letters can be used as an alternative method of "numbering" ordered items, in such contexts as numbered lists.
Etymology.
The English word "alphabet" came into Middle English from the Late Latin word "alphabetum", which in turn originated in the Greek ἀλφάβητος ("alphabētos"), from "alpha" and "beta," the first two letters of the Greek alphabet. "Alpha" and "beta" in turn came from the first two letters of the Phoenician alphabet, and originally meant "ox" and "house" respectively.
History.
Middle Eastern scripts.
The history of the alphabet started in ancient Egypt. By the 27th century BC Egyptian writing had a set of some 24 hieroglyphs which are called uniliterals, to represent syllables that begin with a single consonant of their language, plus a vowel (or no vowel) to be supplied by the native speaker. These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names.
In the Middle Bronze Age an apparently "alphabetic" system known as the Proto-Sinaitic script appears in Egyptian turquoise mines in the Sinai peninsula dated to circa the 15th century BC, apparently left by Canaanite workers. In 1999, John and Deborah Darnell discovered an even earlier version of this first alphabet at Wadi el-Hol dated to circa 1800 BC and showing evidence of having been adapted from specific forms of Egyptian hieroglyphs that could be dated to circa 2000 BC, strongly suggesting that the first alphabet had been developed circa that time. Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs. This script had no characters representing vowels. An alphabetic cuneiform script with 30 signs including three which indicate the following vowel was invented in Ugarit before the 15th century BC. This script was not used after the destruction of Ugarit.
The Proto-Sinaitic script eventually developed into the Phoenician alphabet, which is conventionally called "Proto-Canaanite" before ca. 1050 BC. The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram. This script is the parent script of all western alphabets. By the tenth century two other forms can be distinguished namely Canaanite and Aramaic. The Aramaic gave rise to Hebrew. The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez alphabet (an abugida) is descended. Vowelless alphabets, which are not true alphabets, are called abjads, currently exemplified in scripts including Arabic, Hebrew, and Syriac. The omission of vowels was not a satisfactory solution and some "weak" consonants were used to indicate the vowel quality of a syllable (matres lectionis). These had dual function since they were also used as pure consonants.
The Proto-Sinatic or Proto Canaanite script and the Ugaritic script were the first scripts with limited number of signs, in contrast to the other widely used writing systems at the time, Cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenician script was probably the first phonemic script and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn. Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically.
The script was spread by the Phoenicians, across the Mediterranean. In Greece, the script was modified to add the vowels, giving rise to the ancestor of all alphabets in the West. The indication of the vowels is the same way as the indication of the consonants, therefore it was the first true alphabet. The Greeks took letters which did not represent sounds that existed in Greek, and changed them to represent the vowels. The vowels are significant in the Greek language, and the syllabical Linear B script which was used by the Mycenaean Greeks from the 16th century BC had 87 symbols including 5 vowels. In its early years, there were many variants of the Greek alphabet, a situation which caused many different alphabets to evolve from it.
European alphabets.
The Greek alphabet, in its Euboean form, was carried over by Greek colonists to the Italian peninsula, where it gave rise to a variety of alphabets used to write the Italic languages. One of these became the Latin alphabet, which was spread across Europe as the Romans expanded their empire. Even after the fall of the Roman state, the alphabet survived in intellectual and religious works. It eventually became used for the descendant languages of Latin (the Romance languages) and then for most of the other languages of Europe.
Some adaptations of the Latin alphabet are augmented with ligatures, such as æ in Old English and Icelandic and Ȣ in Algonquian; by borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and by modifying existing letters, such as the eth ð of Old English and Icelandic, which is a modified "d". Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and Italian, which uses the letters "j, k, x, y" and "w" only in foreign words.
Another notable script is Elder Futhark, which is believed to have evolved out of one of the Old Italic alphabets. Elder Futhark gave rise to a variety of alphabets known collectively as the Runic alphabets. The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages. Its usage is mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood. These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century.
The Old Hungarian script is a contemporary writing system of the Hungarians. It was in use during the entire history of Hungary, albeit not as an official writing system. From the 19th century it once again became more and more popular.
The Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic script. Cyrillic is one of the most widely used modern alphabetic scripts, and is notable for its use in Slavic languages and also for other languages within the former Soviet Union. Cyrillic alphabets include the Serbian, Macedonian, Bulgarian, and Russian alphabets. The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was invented by the Bulgarian scholar Clement of Ohrid, who was their disciple. They feature many letters that appear to have been borrowed from or influenced by the Greek alphabet and the Hebrew alphabet.
Asian alphabets.
Beyond the logographic Chinese writing, many phonetic scripts are in existence in Asia. The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet, but because these writing systems are largely consonant-based they are often not considered true alphabets.
Most alphabetic scripts of India and Eastern Asia are descended from the Brahmi script, which is often believed to be a descendant of Aramaic.
In Korea, the Hangul alphabet was created by Sejong the Great. Hangul is a unique alphabet: it is a featural alphabet, where many of the letters are designed from a sound's place of articulation (P to look like the widened mouth, L to look like the tongue pulled in, etc.); its design was planned by the government of the day; and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters, to allow for mixed-script writing (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block).
Zhuyin (sometimes called "Bopomofo") is a semi-syllabary used to phonetically transcribe Mandarin Chinese in the Republic of China. After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited, but it's still widely used in Taiwan where the Republic of China still governs. Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet the phonemes of syllable initials are represented by individual symbols, but like a syllabary the phonemes of the syllable finals are not; rather, each possible final (excluding the medial glide) is represented by its own symbol. For example, "luan" is represented as ㄌㄨㄢ ("l-u-an"), where the last symbol ㄢ represents the entire final "-an". While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a romanization system—that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cellphones.
European alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad (as with Urdu and Persian) and sometimes as a complete alphabet (as with Kurdish and Uyghur).
Types.
The term "alphabet" is used by linguists and paleographers in both a wide and a narrow sense. In the wider sense, an alphabet is a script that is "segmental" at the phoneme level—that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish "true" alphabets from two other types of segmental script, abjads and abugidas. These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with diacritics to or a systematic graphic modification of the consonants. In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters. The earliest known alphabet in the wider sense is the Wadi el-Hol script, believed to be an abjad, which through its successor Phoenician is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet) and Hebrew (via Aramaic).
Examples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean hangul; and abugidas are used to write Tigrinya, Amharic, Hindi, and Thai. The Canadian Aboriginal syllabics are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant which is modified by rotation to represent the following vowel. (In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.)
All three types may be augmented with syllabic glyphs. Ugaritic, for example, is basically an abjad, but has syllabic letters for . (These are the only time vowels are indicated.) Cyrillic is basically a true alphabet, but has syllabic letters for (я, е, ю); Coptic has a letter for . Devanagari is typically an abugida augmented with dedicated letters for initial vowels, though some traditions use अ as a zero consonant as the graphic base for such vowels.
The boundaries between the three types of segmental scripts are not always clear-cut. For example, Sorani Kurdish is written in the Arabic script, which is normally an abjad. However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas. On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but all vowel marks were written after the preceding consonant rather than as diacritic marks. Although short "a" was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet. Conversely, the vowel marks of the Tigrinya abugida and the Amharic abugida (ironically, the original source of the term "abugida") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became logographic. (See below.)
Thus the primary classification of alphabets reflects how they treat vowels. For tonal languages, further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas. Such scripts are to tone what abjads are to vowels. Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas. This is the case for Vietnamese (a true alphabet) and Thai (an abugida). In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation. In the Pollard script, an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang. For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in Zhuyin not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the virama of Indic.
The number of letters in an alphabet can be quite small. The Book Pahlavi script, an abjad, had only twelve letters at one point, and may have had even fewer later on. Today the Rotokas alphabet has only twelve letters. (The Hawaiian alphabet is sometimes claimed to be as small, but it actually consists of 18 letters, including the ʻokina and five long vowels. However, Hawaiian Braille has only 13 letters.) While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been "conflated"—that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in Arabic, another script that lost many of its distinct letter shapes. For example, a comma-shaped letter represented "g, d, y, k," or "j". However, such apparent simplifications can perversely make a script more complicated. In later Pahlavi papyri, up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole—that is, they had become logograms as in Egyptian Demotic.
The largest segmental script is probably an abugida, Devanagari. When written in Devanagari, Vedic Sanskrit has an alphabet of 53 letters, including the "visarga" mark for final aspiration and special letters for "kš" and "jñ," though one of the letters is theoretical and not actually used. The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the "khutma" letters (letters with a dot added) to represent sounds from Persian and English. Thai has a total of 59 symbols, consisting of 44 consonants, 13 vowels and 2 syllabics, not including 4 diacritics for tone marks and one for vowel length.
The largest known abjad is Sindhi, with 51 letters. The largest alphabets in the narrow sense include Kabardian and Abkhaz (for Cyrillic), with 58 and 56 letters, respectively, and Slovak (for the Latin script), with 46. However, these scripts either count di- and tri-graphs as separate letters, as Spanish did with "ch" and "ll" until recently, or uses diacritics like Slovak "č". The largest true alphabet where each letter is graphically independent is Georgian, with 33 letters.
Syllabaries typically contain 50 to 400 glyphs, and the glyphs of logographic systems typically number from the many hundreds into the thousands. Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.
The Armenian alphabet (Armenian: Հայոց գրեր Hayots grer or Հայոց այբուբեն Hayots aybuben) is a graphically unique alphabetical writing system that has been used to write the Armenian language. It was introduced by Mesrob Mashdots around 405 AD, an Armenian linguist and ecclesiastical leader, and originally contained 36 letters. Two more letters, օ (o) and ֆ (f), were added in the Middle Ages. During the 1920s orthography reform, a new letter և (capital ԵՎ) was added, which was a ligature before ե+ւ, while the letter Ւ ւ was discarded and reintroduced as part of a new letter ՈՒ ու (which was a digraph before).
The Armenian word for "alphabet" is այբուբեն aybuben (Armenian pronunciation: [ɑjbubɛn]), named after the first two letters of the Armenian alphabet Ա այբ ayb and Բ բեն ben. The Armenian script's directionality is horizontal left-to-right, like the Latin and Greek alphabets.[3]
Alphabetical order.
Alphabets often come to be associated with a standard ordering of their letters, which can then be used for purposes of collation – namely for the listing of words and other items in what is called "alphabetical order".
The basic ordering of the Latin alphabet (A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z), which is derived from the Northwest Semitic "Abgad" order, is well established, although languages using this alphabet have different conventions for their treatment of modified letters (such as the French "é", "à", and "ô") and of certain combinations of letters (multigraphs). In French, these are not considered to be additional letters for the purposes of collation. However, in Icelandic, the accented letters such as "á", "í", and "ö" are considered to be distinct letters of the alphabet. In Spanish, "ñ" is considered a separate letter, but accented vowels such as "á" and "é" are not. The "ll" and "ch" were also considered single letters, but in 1994 the Real Academia Española changed the collating order so that "ll" is between "lk" and "lm" in the dictionary and "ch" is between "cg" and "ci", and in 2010 the tenth congress of the Association of Spanish Language Academies changed it so they were no longer letters at all.
In German, words starting with "sch-" (which spells the German phoneme ) are inserted between words with initial "sca-" and "sci-" (all incidentally loanwords) instead of appearing after initial "sz", as though it were a single letter—in contrast to several languages such as Albanian, in which "dh-", "ë-", "gj-", "ll-", "rr-", "th-", "xh-" and "zh-" (all representing phonemes and considered separate single letters) would follow the letters "d", "e", "g", "l", "n", "r", "t", "x" and "z" respectively, as well as Hungarian and Welsh. Further, German words with umlaut are collated ignoring the umlaut—contrary to Turkish which allegedly adopted the German graphemes ö and ü, and where a word like "tüfek", would come after "tuz", in the dictionary. An exception is the German telephone directory where umlauts are sorted like "ä" = "ae" since names as "Jäger" appear also with the spelling "Jaeger", and are not distinguished in the spoken language.
The Danish and Norwegian alphabets end with "æ"—"ø"—"å", whereas the Icelandic, Swedish and Finnish ones conventionally put "å"—"ä"—"ö" at the end.
It is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required. However, a dozen Ugaritic tablets from the fourteenth century BC preserve the alphabet in two sequences. One, the "ABCDE" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, "HMĦLQ," was used in southern Arabia and is preserved today in Ethiopic. Both orders have therefore been stable for at least 3000 years.
The historical order was abandoned in Runic and Arabic, although Arabic retains the traditional abjadi order for numbering.
The Brahmic family of alphabets used in India use a unique order based on phonology: The letters are arranged according to how and where they are produced in the mouth. This organization is used in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet.
Names of letters.
The Phoenician letter names, in which each letter was associated with a word that begins with that sound, continue to be used to varying degrees in Samaritan, Aramaic, Syriac, Hebrew, Greek and Arabic.
The names were abandoned in Latin, which instead referred to the letters by adding a vowel (usually e) before or after the consonant; the two exceptions were Y and Z, which were borrowed from the Greek alphabet rather than Etruscan, and were known as "Y Graeca" "Greek Y" (pronounced "I Graeca" "Greek I") and "zeta" (from Greek) – this discrepancy was inherited by many European languages, as in the term "zed" for Z in British English. Over time names sometimes shifted or were added, as in "double U" for W ("double V" in French), the English name for Y, and American "zee" for Z. Comparing names in English and French gives a clear reflection of the Great Vowel Shift: A, B, C and D are pronounced /eɪ, biː, siː, diː/ in today's English, but in contemporary French they are /a, be, se, de/. The French names (from which the English names are derived) preserve the qualities of the English vowels from before the Great Vowel Shift. By contrast, the names of F, L, M, N and S (/ɛf, ɛl, ɛm, ɛn, ɛs/) remain the same in both languages, because "short" vowels were largely unaffected by the Shift.
In Cyrillic originally the letters were given names based on Slavic words; this was later abandoned as well in favor of a system similar to that used in Latin.
Orthography and pronunciation.
When an alphabet is adopted or developed to represent a given language, an orthography generally comes into being, providing rules for the spelling of words in that language. In accordance with the principle on which alphabets are based, these rules will generally map letters of the alphabet to the phonemes (significant sounds) of the spoken language. In a perfectly phonemic orthography there would be a consistent one-to-one correspondence between the letters and the phonemes, so that a writer could predict the spelling of a word given its pronunciation, and a speaker would always know the pronunciation of a word given its spelling, and vice versa. However this ideal is not usually achieved in practice; some languages (such as Spanish and Finnish) come close to it, while others (such as English) deviate from it to a much larger degree.
The pronunciation of a language often evolves independently of its writing system, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.
Languages may fail to achieve a one-to-one correspondence between letters and sounds in any of several ways:
National languages sometimes elect to address the problem of dialects by simply associating the alphabet with the national standard. However, with an international language with wide variations in its dialects, such as English, it would be impossible to represent the language in all its variations with a single phonetic alphabet.
Some national languages like Finnish, Turkish, Serbo-Croatian (Serbian, Croatian and Bosnian) and Bulgarian have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes. Strictly speaking, these national languages lack a word corresponding to the verb "to spell" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables. Similarly, the Italian verb corresponding to 'spell (out)', "compitare", is unknown to many Italians because spelling is usually trivial, as Italian spelling is highly phonemic. In standard Spanish, one can tell the pronunciation of a word from its spelling, but not vice versa, as certain phonemes can be represented in more than one way, but a given letter is consistently pronounced. French, with its silent letters and its heavy use of nasal vowels and elision, may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation, though complex, are actually consistent and predictable with a fair degree of accuracy.
At the other extreme are languages such as English, where the pronunciations of many words simply have to be memorized as they do not correspond to the spelling in a consistent way. For English, this is partly because the Great Vowel Shift occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate.
Sometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system itself, as when Turkey switched from the Arabic alphabet to a Latin-based Turkish alphabet.
The sounds of speech of all languages of the world can be written by a rather-small universal phonetic alphabet. A standard for this is the International Phonetic Alphabet.

</doc>
<doc id="673" url="http://en.wikipedia.org/wiki?curid=673" title="Atomic number">
Atomic number

In chemistry and physics, the atomic number of a chemical element (also known as its proton number) is the number of protons found in the nucleus of an atom of that element, and therefore identical to the charge number of the nucleus. It is conventionally represented by the symbol Z. The atomic number uniquely identifies a chemical element. In an uncharged atom, the atomic number is also equal to the number of electrons.
The atomic number, "Z", should not be confused with the mass number, "A", which is the number of nucleons, the total number of protons and neutrons in the nucleus of an atom. The number of neutrons, "N", is known as the neutron number of the atom; thus, "A" = "Z" + "N" (these quantities are always whole numbers). Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the "relative isotopic mass"), is roughly (to within 1%) equal to the whole number "A".
Atoms with the same atomic number "Z" but different neutron numbers "N", and hence different atomic masses, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.
The conventional symbol "Z" comes from the German word meaning number/numeral/figure, which, prior to the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this "Z" number was also the nuclear charge and a physical characteristic of atoms, did the word (and its English equivalent "atomic number") come into common use in this context.
History.
The periodic table and a natural number for each element.
Loosely speaking, the existence or construction of a periodic table of elements creates an ordering of the elements, and so they can be numbered in order.
Dmitri Mendeleev claimed that he arranged his first periodic tables in order of atomic weight ("Atomgewicht"). However, in consideration of the elements' observed chemical properties, he changed the order slightly and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, "Z", but that number was not known or suspected at the time.
A simple numbering based on periodic table position was never entirely satisfactory, however. Besides the case of iodine and tellurium, later several other pairs of elements (such as argon and potassium, cobalt and nickel) were known to have nearly identical or reversed atomic weights, thus requiring their placement in the periodic table to be determined by their chemical properties. However the gradual identification of more and more chemically similar lanthanide elements, whose atomic number was not obvious, led to inconsistency and uncertainty in the periodic numbering of elements at least from lutetium (element 71) onwards (hafnium was not known at this time). 
The Rutherford-Bohr model and van den Broek.
In 1911, Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% different from the atomic number of gold ("Z" = 79, "A" = 197), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element Z = 79 on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was "exactly" equal to its place in the periodic table (also known as element number, atomic number, and symbolized "Z"). This proved eventually to be the case.
Moseley's 1913 experiment.
The experimental position improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek's and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fitted the Bohr theory's postulation that the frequency of the spectral lines be proportional to the square of "Z".
To do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum ("Z" = 13) to gold ("Z" = 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons (x-rays) increased from one target to the next in an arithmetic progression. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the element number "Z". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members—no fewer and no more—which was far from obvious from the chemistry at that time.
The proton and the idea of nuclear electrons.
In 1915 the reason for nuclear charge being quantized in units of Z, which were now recognized to be the same as the element number, was not understood. An old idea called Prout's hypothesis had postulated that the elements were all made of residues (or "protyles") of the lightest element hydrogen, which in the Bohr-Rutherford model had a single electron and a nuclear charge of one. However, as early as 1907 Rutherford and Thomas Royds had shown that alpha particles, which had a charge of +2, were the nuclei of helium atoms, which had a mass four times that of hydrogen, not two times. If Prout's hypothesis were true, something had to be neutralizing some of the charge of the hydrogen nuclei present in the nuclei of heavier atoms.
In 1917 Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law. He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles). It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei. A helium nucleus was presumed to be composed of four protons plus two "nuclear electrons" (electrons bound inside the nucleus) to cancel two of the charges. At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen, was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of + 79, consistent with its atomic number.
The discovery of the neutron makes Z the proton number.
All consideration of nuclear electrons ended with James Chadwick's discovery of the neutron in 1932. An atom of gold now was seen as containing 118 neutrons rather than 118 nuclear electrons, and its positive charge now was realized to come entirely from a content of 79 protons. After 1932, therefore, an element's atomic number Z was also realized to be identical to the proton number of its nuclei.
The symbol of Z.
The conventional symbol "Z" possibly comes from the German word (atomic number). However, prior to 1915, the word "Zahl" (simply "number") was used for an element's assigned number in the periodic table.
Chemical properties.
Each element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is "Z" (the atomic number). The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence, it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of "any" mixture of atoms with a given atomic number.
New elements.
The quest for new elements is usually described using atomic numbers. As of 2010, elements with atomic numbers 1 to 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life becomes shorter as atomic number increases, though an "island of stability" may exist for undiscovered isotopes with certain numbers of protons and neutrons.

</doc>
<doc id="674" url="http://en.wikipedia.org/wiki?curid=674" title="Anatomy">
Anatomy

Anatomy is the branch of biology concerned with the study of the structure of animals and their parts, and is also referred to as zootomy to separate it from human anatomy. In some of its facets, anatomy is related to embryology and comparative anatomy, which itself is closely related to evolutionary biology and phylogeny. Human anatomy is one of the basic essential sciences of medicine.
The discipline of anatomy is divided into macroscopic and microscopic anatomy. Macroscopic anatomy, or gross anatomy, is the examination of an animal’s body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology and also in the study of cells.
The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcases and cadavers (corpses) to 20th century medical imaging techniques including X-ray, ultrasound, and magnetic resonance imaging.
Definition.
Derived from the Greek "anatemnō" "I cut up, cut open" from ἀνά "ana" "up", and τέμνω "temnō" "I cut", anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, their locations and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.
The discipline of anatomy can be subdivided into a number of branches including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, including histology (the study of tissues), and embryology (the study of an organism in its immature condition).
Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.
The term "anatomy" is commonly taken to refer to human anatomy. However, substantially the same structures and tissues are found throughout the rest of the animal kingdom and the term also includes the anatomy of other animals. The term "zootomy" is also sometimes used to specifically refer to animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.
Animal tissues.
The kingdom Animalia, also called Metazoa, contains multicellular organisms that are heterotrophic and motile (although some have secondarily adopted a sessile lifestyle). Most animals have bodies differentiated into separate tissues and these animals are also known as eumetazoans. They have an internal digestive chamber, with one or two openings; the gametes are produced in multicellular sex organs, and the zygotes include a blastula stage in their embryonic development. Metazoans do not include the sponges, which have undifferentiated cells.
Unlike plant cells, animal cells have neither a cell wall nor chloroplasts. Vacuoles, when present, are more in number and much smaller than those in the plant cell. The body tissues are composed of numerous types of cell, including those found in muscles, nerves and skin. Each typically has a cell membrane formed of phospholipids, cytoplasm and a nucleus. All of the different cells of an animal are derived from the embryonic germ layers. Those simpler invertebrates which are formed from two germ layers of ectoderm and endoderm are called diploblastic and the more developed animals whose structures and organs are formed from three germ layers are called triploblastic. All of a triploblastic animal's tissues and organs are derived from the three germ layers of the embryo, the ectoderm, mesoderm and endoderm.
Animal tissues can be grouped into four basic types: connective, epithelial, muscle and nervous tissue.
Connective tissue.
Connective tissues are fibrous and made up of cells scattered among inorganic material called the extracellular matrix. Connective tissue gives shape to organs and holds them in place. The main types are loose connective tissue, adipose tissue, fibrous connective tissue, cartilage and bone. The extracellular matrix contains proteins, the chief and most abundant of which is collagen. Collagen plays a major part in organizing and maintaining tissues. The matrix can be modified to form a skeleton to support or protect the body. An exoskeleton is a thickened, rigid cuticle which is stiffened by mineralisation, as in crustaceans or by the cross-linking of its proteins as in insects. An endoskeleton is internal and present in all developed animals, as well as in many of those less developed.
Epithelium.
Epithelial tissue is composed of closely packed cells, bound to each other by cell adhesion molecules, with little intercellular space. Epithelial cells can be squamous (flat), cuboidal or columnar and rest on a basal lamina, the upper layer of the basement membrane, the lower layer is the reticular lamina lying next to the connective tissue in the extracellular matrix secreted by the epithelial cells. There are many different types of epithelium, modified to suit a particular function. In the respiratory tract there is a type of ciliated epithelial lining; in the small intestine there are microvilli on the epithelial lining and in the large intestine there are intestinal villi. Skin consists of an outer layer of keratinised stratified squamous epithelium that covers the exterior of the vertebrate body. Keratinocytes make up to 95% of the cells in the skin. The epithelial cells on the external surface of the body typically secrete an extracellular matrix in the form of a cuticle. In simple animals this may just be a coat of glycoproteins. In more advanced animals, many glands are formed of epithelial cells.
Muscle tissue.
Muscle cells (myocytes) form the active contractile tissue of the body. Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs. Muscle is formed of contractile filaments and is separated into three types; smooth muscle, skeletal muscle and obliquely striated muscle. Smooth muscle has no striations when examined microscopically. It contracts slowly but maintains contractibility over a wide range of stretch lengths. It is found in such organs as sea anemone tentacles and the body wall of sea cucumbers. Skeletal muscle contracts rapidly but has a limited range of extension. It is found in the movement of appendages and jaws. Obliquely striated muscle is intermediate between the other two. The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions. In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets. Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach, esophagus, respiratory airways, and blood vessels. Cardiac muscle is found only in the heart, allowing it to contract and pump blood round the body.
Nervous tissue.
Nervous tissue is composed of many nerve cells known as neurons which transmit information. In some slow-moving radially symmetrical marine animals such as ctenophores and cnidarians (including sea anemones and jellyfish), the nerves form a nerve net, but in most animals they are organized longitudinally into bundles. In simple animals, receptor neurons in the body wall cause a local reaction to a stimulus. In more complex animals, specialised receptor cells such as chemoreceptors and photoreceptors are found in groups and send messages along neural networks to other parts of the organism. Neurons can be connected together in ganglia. In higher animals, specialized receptors are the basis of sense organs and there is a central nervous system (brain and spinal cord) and a peripheral nervous system. The latter consists of sensory nerves that transmit information from sense organs and motor nerves that influence target organs. The peripheral nervous system is divided into the somatic nervous system which conveys sensation and controls voluntary muscle, and the autonomic nervous system which involuntarily controls smooth muscle, certain glands and internal organs, including the stomach.
Vertebrate anatomy.
All vertebrates have a similar basic body plan and at some point in their lives, (mostly in the embryonic stage), share the major chordate characteristics; a stiffening rod, the notochord; a dorsal hollow tube of nervous material, the neural tube; pharyngeal slits; and a tail posterior to the anus. The spinal cord is protected by the vertebral column and is above the notochord and the gastrointestinal tract is below it. Nervous tissue is derived from the ectoderm, connective tissues are derived from mesoderm, and gut is derived from the endoderm. At the posterior end is a tail which continues the spinal cord and vertebrae but not the gut. The mouth is found at the anterior end of the animal, and the anus at the base of the tail. The defining characteristic of a vertebrate is the vertebral column, formed in the development of the segmented series of vertebrae. In most vertebrates the notochord becomes the nucleus pulposus of the intervertebral discs. However, a few vertebrates, such as the sturgeon and the coelacanth retain the notochord into adulthood. Jawed vertebrates are typified by paired appendages, fins or legs, which may be secondarily lost. The limbs of vertebrates are considered to be homologous because the same underlying skeletal structure was inherited from their last common ancestor. This is one of the arguments put forward by Charles Darwin to support his theory of evolution.
Fish anatomy.
The body of a fish is divided into a head, trunk and tail, although the divisions between the three are not always externally visible. The skeleton, which forms the support structure inside the fish, is either made of cartilage, in cartilaginous fish, or bone in bony fish. The main skeletal element is the vertebral column, composed of articulating vertebrae which are lightweight yet strong. The ribs attach to the spine and there are no limbs or limb girdles. The main external features of the fish, the fins, are composed of either bony or soft spines called rays, which with the exception of the caudal fins, have no direct connection with the spine. They are supported by the muscles which compose the main part of the trunk. The heart has two chambers and pumps the blood through the respiratory surfaces of the gills and on round the body in a single circulatory loop. The eyes are adapted for seeing underwater and have only local vision. There is an inner ear but no external or middle ear. Low frequency vibrations are detected by the lateral line system of sense organs that run along the length of the sides of fish, and these respond to nearby movements and to changes in water pressure.
Sharks and rays are basal fish with numerous primitive anatomical features similar to those of ancient fish, including skeletons composed of cartilage. Their bodies tend to be dorso-ventrally flattened, they usually have five pairs of gill slits and a large mouth set on the underside of the head. The dermis is covered with separate dermal placoid scales. They have a cloaca into which the urinary and genital passages open, but not a swim bladder. Cartilaginous fish produce a small number of large, yolky eggs. Some species are ovoviviparous and the young develop internally but others are oviparous and the larvae develop externally in egg cases.
The bony fish lineage shows more derived anatomical traits, often with major evolutionary changes from the features of ancient fish. They have a bony skeleton, are generally laterally flattened, have five pairs of gills protected by an operculum, and a mouth at or near the tip of the snout. The dermis is covered with overlapping scales. Bony fish have a swim bladder which helps them maintain a constant depth in the water column, but not a cloaca. They mostly spawn a large number of small eggs with little yolk which they broadcast into the water column.
Amphibian anatomy.
Amphibians are a class of animals comprising frogs, salamanders and caecilians. They are tetrapods, but the caecilians and a few species of salamander have either no limbs or their limbs are much reduced in size. Their main bones are hollow and lightweight and are fully ossified and the vertebrae interlock with each other and have articular processes. Their ribs are usually short and may be fused to the vertebrae. Their skulls are mostly broad and short, and are often incompletely ossified. Their skin contains little keratin and lacks scales, but contains many mucous glands and in some species, poison glands. The hearts of amphibians have three chambers, two atria and one ventricle. They have a urinary bladder and nitrogenous waste products are excreted primarily as urea. Amphibians breathe by means of buccal pumping, a pump action in which air is first drawn into the buccopharyngeal region through the nostrils. These are then closed and the air is forced into the lungs by contraction of the throat. They supplement this with gas exchange through the skin which needs to be kept moist.
In frogs the pelvic girdle is robust and the hind legs are much longer and stronger than the forelimbs. The feet have four or five digits and the toes are often webbed for swimming or have suction pads for climbing. Frogs have large eyes and no tail. Salamanders resemble lizards in appearance; their short legs project sideways, the belly is close to or in contact with the ground and they have a long tail. Caecilians superficially resemble earthworms and are limbless. They burrow by means of zones of muscle contractions which move along the body and they swim by undulating their body from side to side.
Reptile anatomy.
Reptiles are a class of animals comprising turtles, tuataras, lizards, snakes and crocodiles. They are tetrapods, but the snakes and a few species of lizard either have no limbs or their limbs are much reduced in size. Their bones are better ossified and their skeletons stronger than those of amphibians. The teeth are conical and mostly uniform in size. The surface cells of the epidermis are modified into horny scales which create a waterproof layer. Reptiles are unable to use their skin for respiration as do amphibians and have a more efficient respiratory system drawing air into their lungs by expanding their chest walls. The heart resembles that of the amphibian but there is a septum which more completely separates the oxygenated and deoxygenated bloodstreams. The reproductive system is designed for internal fertilisation, with a copulatory organ present in most species. The eggs are surrounded by amniotic membranes which prevents them from drying out and are laid on land, or develop internally in some species. The bladder is small as nitrogenous waste is excreted as uric acid.
Turtles are notable for their protective shells. They have an inflexible trunk encased in a horny carapace above and a plastron below. These are formed from bony plates embedded in the dermis which are overlain by horny ones and are partially fused with the ribs and spine. The neck is long and flexible and the head and the legs can be drawn back inside the shell. Turtles are vegetarians and the typical reptile teeth have been replaced by sharp, horny plates. In aquatic species, the front legs are modified into flippers.
Tuataras superficially resemble lizards but the lineages diverged in the Triassic period. There is one living species, "Sphenodon punctatus". The skull has two openings (fenestrae) on either side and the jaw is rigidly attached to the skull. There is one row of teeth in the lower jaw and this fits between the two rows in the upper jaw when the animal chews. The teeth are merely projections of bony material from the jaw and eventually wear down. The brain and heart are more primitive than is the case in other reptiles and the lungs have a single chamber and lack bronchi. The tuatara has a well-developed parietal eye on its forehead.
Lizards have skulls with only one fenestra on each side, the lower bar of bone below the second fenestra having been lost. This results in the jaws being less rigidly attached which allows the mouth to open wider. Lizards are mostly quadrupeds, with the trunk held off the ground by short, sideways-facing legs, but a few species have no limbs and resemble snakes. Lizards have moveable eyelids, eardrums are present and some species have a central parietal eye.
Snakes are closely related to lizards, having branched off from a common ancestral lineage during the Cretaceous period, and they share many of the same features. The skeleton consists of a skull, a hyoid bone, spine and ribs though a few species retain a vestige of the pelvis and rear limbs in the form of pelvic spurs. The bar under the second fenestra has also been lost and the jaws have extreme flexibility allowing the snake to swallow its prey whole. Snakes lack moveable eyelids, the eyes being covered by transparent "spectacle" scales. They do not have eardrums but can detect ground vibrations through the bones of their skull. Their forked tongues are used as organs of taste and smell and some species have sensory pits on their heads enabling them to locate warm-blooded prey.
Crocodilians are large, low-slung aquatic reptiles with long snouts and large numbers of teeth. The head and trunk are dorso-ventrally flattened and the tail is laterally compressed. It undulates from side to side to force the animal through the water when swimming. The tough keratinised scales provide body armour and some are fused to the skull. The nostrils, eyes and ears are elevated above the top of the flat head enabling them to remain above the surface of the water when the animal is floating. Valves seal the nostrils and ears when it is submerged. Unlike other reptiles, crocodilians have hearts with four chambers allowing complete separation of oxygenated and deoxygenated blood.
Bird anatomy.
Birds are tetrapods but though their hind limbs are used for walking or hopping, their front limbs are wings covered with feathers and adapted for flight. Birds are endothermic, have a high metabolic rate, a light skeletal system and powerful muscles. The long bones are thin, hollow and very light. Air sac extensions from the lungs occupy the centre of some bones. The sternum is wide and usually has a keel and the caudal vertebrae are fused. There are no teeth and the narrow jaws are adapted into a horn-covered beak. The eyes are relatively large, particularly in nocturnal species such as owls. They face forwards in predators and sideways in ducks.
The feathers are outgrowths of the epidermis and are found in localized bands from where they fan out over the skin. Large flight feathers are found on the wings and tail, contour feathers cover the bird's surface and fine down occurs on young birds and under the contour feathers of water birds. The only cutaneous gland is the single uropygial gland near the base of the tail. This produces an oily secretion that waterproofs the feathers when the bird preens. There are scales on the legs, feet and claws on the tips of the toes.
Mammal anatomy.
Mammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight. They mostly have four limbs but some aquatic mammals have no limbs or limbs modified into fins and the forelimbs of bats are modified into wings. The legs of most mammals are situated below the trunk, which is held well clear of the ground. The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel. The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans. Mammals have three bones in the middle ear and a cochlea in the inner ear. They are clothed in hair and their skin contains glands which secrete sweat. Some of these glands are specialised as mammary glands, producing milk to feed the young. Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs. The mammalian heart has four chambers and oxygenated and deoxygenated blood are kept entirely separate. Nitrogenous waste is excreted primarily as urea.
Mammals are amniotes, and most are viviparous, giving birth to live young. The exception to this are the egg-laying monotremes, the platypus and the echidnas of Australia. Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a nipple and completes its development.
Human anatomy.
Humans have the overall body plan of a mammal. Humans have a head, neck, trunk (which includes the thorax and abdomen), two arms and hands and two legs and feet.
Generally, students of certain biological sciences, paramedics, prosthetists and orthotists, physiotherapists, occupational therapists, nurses, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials, and in addition, medical students generally also learn gross anatomy through practical experience of dissection and inspection of cadavers. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope.
Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by physicians, especially surgeons and doctors working in some diagnostic specialties, such as histopathology and radiology.
Academic human anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
Invertebrate anatomy.
Invertebrates constitute a vast array of living organisms ranging from the simplest unicellular eukaryotes such as "Paramecium" to such complex multicellular animals as the octopus, lobster and dragonfly. They constitute about 95% of the animal species. By definition, none of these creatures has a backbone. The cells of single-cell protozoans have the same basic structure as those of multicellular animals but some parts are specialised into the equivalent of tissues and organs. Locomotion is often provided by cilia or flagella or may proceed via the advance of pseudopodia, food may be gathered by phagocytosis, energy needs may be supplied by photosynthesis and the cell may be supported by an endoskeleton or an exoskeleton. Some protozoans can form multicellular colonies.
Metazoans are multicellular organism, different groups of cells of which have separate functions. The most basic types of metazoan tissues are epithelium and connective tissue, both of which are present in nearly all invertebrates. The outer surface of the epidermis is normally formed of epithelial cells and secretes an extracellular matrix which provides support to the organism. An endoskeleton derived from the mesoderm is present in echinoderms, sponges and some cephalopods. Exoskeletons are derived from the epidermis and is composed of chitin in arthropods (insects, spiders, ticks, shrimps, crabs, lobsters). Calcium carbonate constitutes the shells of molluscs, brachiopods and some tube-building polychaete worms and silica forms the exoskeleton of the microscopic diatoms and radiolaria. Other invertebrates may have no rigid structures but the epidermis may secrete a variety of surface coatings such as the pinacoderm of sponges, the gelatinous cuticle of cnidarians (polyps, sea anemones, jellyfish) and the collagenous cuticle of annelids. The outer epithelial layer may include cells of several types including sensory cells, gland cells and stinging cells. There may also be protrusions such as microvilli, cilia, bristles, spines and tubercles.
Arthropod anatomy.
Arthropods comprise the largest phylum in the animal kingdom with over a million known invertebrate species.
Insects possess segmented bodies supported by a hard-jointed outer covering, the exoskeleton, made mostly of chitin. The segments of the body are organized into three distinct parts, a head, a thorax and an abdomen. The head typically bears a pair of sensory antennae, a pair of compound eyes, one to three simple eyes (ocelli) and three sets of modified appendages that form the mouthparts. The thorax has three pairs of segmented legs, one pair each for the three segments that compose the thorax and one or two pairs of wings. The abdomen is composed of eleven segments, some of which may be fused and houses the digestive, respiratory, excretory and reproductive systems. There is considerable variation between species and many adaptations to the body parts, especially wings, legs, antennae and mouthparts.
Spiders a class of arachnids have four pairs of legs; a body of two segments—a cephalothorax and an abdomen. Spiders have no wings and no antennae. They have mouthparts called chelicerae which are often connected to venom glands as most spiders are venomous. They have a second pair of appendages called pedipalps attached to the cephalothorax. These have the same segmentation as the legs and function as taste and smell organs. At the end of each pedipalp is a spoon-shaped cymbium that acts to support the pedipalp.
History.
Ancient.
In 1600 BCE, the Edwin Smith Papyrus, an Ancient Egyptian medical text, described the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder, and showed the blood vessels diverging from the heart. The Ebers Papyrus (c. 1550 BCE) features a "treatise on the heart", with vessels carrying all the body's fluids to or from every member of the body.
The anatomy of the muscles and skeleton is described in the "Hippocratic Corpus", an Ancient Greek medical work written by unknown authors. Aristotle described vertebrate anatomy based on animal dissection. Praxagoras identified the difference between arteries and veins. Also in the 4th century BCE, Herophilos and Erasistratus produced more accurate anatomical descriptions based on vivisection of criminals in Alexandria during the Ptolemaic dynasty.
In the 2nd century, Galen of Pergamum, an anatomist, clinician, writer and philosopher, wrote the final and highly influential anatomy treatise of ancient times. He compiled existing knowledge and studied anatomy through dissection of animals. He was one of the first experimental physiologists through his vivisection experiments on animals. Galen's drawings, based mostly on dog anatomy, became effectively the only anatomical textbook for the next thousand years. His work was known to Renaissance doctors only through Islamic Golden Age medicine until it was translated from the Greek some time in the 15th century.
Medieval to early modern.
Anatomy developed little from classical times until the sixteenth century; as the historian Marie Boas writes, "Progress in anatomy before the sixteenth century is as mysteriously slow as its development after 1500 is startlingly rapid". Between 1275 and 1326, the anatomists Mondino de Luzzi, Alessandro Achillini and Antonio Benivieni at Bologna carried out the first systematic human dissections since ancient times. Mondino's "Anatomy" of 1316 was the first textbook in the medieval rediscovery of human anatomy. It describes the body in the order followed in Mondino's dissections, starting with the abdomen, then the thorax, then the head and limbs. It was the standard anatomy textbook for the next century.
Andreas Vesalius (1514–1564) (Latinized from Andries van Wezel), professor of anatomy at the University of Padua, is considered the founder of modern human anatomy. Originally from Brabant, Vesalius published the influential book "De humani corporis fabrica" ("the structure of the human body"), a large format book in seven volumes, in 1543. The accurate and intricately detailed illustrations, often in allegorical poses against Italianate landscapes, are thought to have been made by the artist Jan van Calcar, a pupil of Titian.
The artist Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. He made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected.
In England, anatomy was the subject of the first public lectures given in any science; these were given by the Company of Barbers and Surgeons in the 16th century, joined in 1583 by the Lumleian lectures in surgery at the Royal College of Physicians.
Late modern.
In the United States, medical schools began to be set up towards the end of the 18th century. Classes in anatomy needed a continual stream of cadavers for dissection and these were difficult to obtain. Philadelphia, Baltimore and New York were all renowned for body snatching activity as criminals raided graveyards at night, removing newly buried corpses from their coffins. A similar problem existed in Britain where demand for bodies became so great that grave-raiding and even anatomy murder were practised to obtain cadavers. Some graveyards were in consequence protected with watchtowers. The practice was halted in Britain by the Anatomy Act of 1832, while in the United States, similar legislation was enacted after the physician William S. Forbes of Jefferson Medical College was found guilty in 1882 of "complicity with resurrectionists in the despoliation of graves in Lebanon Cemetery".
The teaching of anatomy in Britain was transformed by Sir John Struthers, Regius Professor of Anatomy at the University of Aberdeen from 1863 to 1889. He was responsible for setting up the system of three years of "pre-clinical" academic teaching in the sciences underlying medicine, including especially anatomy. This system lasted until the reform of medical training in 1993 and 2003. As well as teaching, he collected many vertebrate skeletons for his museum of comparative anatomy, published over 70 research papers, and became famous for his public dissection of the Tay Whale. From 1822 the Royal College of Surgeons regulated the teaching of anatomy in medical schools. Medical museums provided examples in comparative anatomy, and were often used in teaching. Ignaz Semmelweis investigated puerperal fever and he discovered how it was caused. He noticed that the frequently fatal fever occurred more often in mothers examined by medical students than by midwives. The students went from the dissecting room to the hospital ward and examined women in childbirth. Semmelweis showed that when the trainees washed their hands in chlorinated lime before each clinical examination, the incidence of puerperal fever among the mothers could be reduced dramatically.
Before the era of modern medical procedures, the main means for studying the internal structure of the body were palpation and dissection. It was the advent of microscopy that opened up an understanding of the building blocks that constituted living tissues. Technical advances in the development of achromatic lenses increased the resolving power of the microscope and around 1839, Matthias Jakob Schleiden and Theodor Schwann identified that cells were the fundamental unit of organization of all living things. Study of small structures involved passing light through them and the microtome was invented to provide sufficiently thin slices of tissue to examine. Staining techniques using artificial dyes were established to help distinguish between different types of tissue. The fields of cytology and histology developed from here in the late 19th century. The invention of the electron microscope brought a great advance in resolution power and allowed research into the ultrastructure of cells and the organelles and other structures within them. About the same time, in the 1950s, the use of X-ray diffraction for studying the crystal structures of proteins, nucleic acids and other biological molecules gave rise to a new field of molecular anatomy.
Short wavelength electromagnetic radiation such as X-rays can be passed through the body and used in medical radiography to view interior structures that have different degrees of opaqueness. Nowadays, modern techniques such as magnetic resonance imaging, computed tomography, fluoroscopy and ultrasound imaging have enabled researchers and practitioners to examine organs, living or dead, in unprecedented detail. They are used for diagnostic and therapeutic purposes and provide information on the internal structures and organs of the body to a degree far beyond the imagination of earlier generations.
Bibliography.
"Main article:" Bibliography of anatomy

</doc>
<doc id="675" url="http://en.wikipedia.org/wiki?curid=675" title="Affirming the consequent">
Affirming the consequent

Affirming the consequent, sometimes called converse error or fallacy of the converse, is a formal fallacy of inferring the converse from the original statement. The corresponding argument has the general form:
An argument of this form is invalid, i.e., the conclusion can be false even when statements 1 and 2 are true. Since "P" was never asserted as the "only" sufficient condition for "Q", other factors could account for "Q" (while "P" was false).
To put it differently, if "P" implies "Q", the only inference that can be made is "non-Q" implies "non-P". ("Non-P" and "non-Q" designate the opposite propositions to "P" and "Q".) This is known as logical contraposition. Symbolically:
formula_1
The name "affirming the consequent" derives from the premise "Q", which affirms the "then" clause of the conditional premise.
Examples.
One way to demonstrate the invalidity of this argument form is with a counterexample with true premises but an obviously false conclusion. For example:
Owning Fort Knox is not the "only" way to be rich. Any number of other ways exist to be rich.
However, one can affirm with certainty that "if Bill Gates is not rich" ("non-Q") then "Bill Gates does not own Fort Knox" ("non-P"). This is the contrapositive of the first statement, and it must be true if the original statement is true.
Arguments of the same form can sometimes seem superficially convincing, as in the following example:
But having the flu is not the "only" cause of a sore throat since many illnesses cause sore throat, such as the common cold or strep throat.

</doc>
<doc id="676" url="http://en.wikipedia.org/wiki?curid=676" title="Andrei Tarkovsky">
Andrei Tarkovsky

Andrei Arsenyevich Tarkovsky (; 4 April 1932 – 29 December 1986) was a Soviet and Russian film-maker, writer, film editor, film theorist, theatre and opera director.
Tarkovsky's films include "Ivan's Childhood", "Andrei Rublev", "Solaris", "The Mirror", and "Stalker". He directed the first five of his seven feature films in the Soviet Union; his last two films, "Nostalghia" and "The Sacrifice", were produced in Italy and Sweden, respectively. His work is characterized by spirituality and metaphysical themes, long takes, lack of conventional dramatic structure, and distinctively authored use of cinematography. He is widely regarded as one of the greatest film-makers of all time. Ingmar Bergman said of Tarkovsky:
Life.
Childhood and early life.
Tarkovsky was born in the village of Zavrazhye in the Yuryevetsky District of the Ivanovo Industrial Oblast to poet and translator Arseny Alexandrovich Tarkovsky, native of Kirovohrad, Ukraine; and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute. Andrei's grandfather Aleksander Tarkowski was a Polish nobleman who worked as a bank clerk.
Tarkovsky spent his childhood in Yuryevets. He was described by childhood friends as active and popular, having many friends and being typically in the center of action. In 1937, his father left the family, subsequently volunteering for the army in 1941. Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press. In 1939, Tarkovsky enrolled at the Moscow School № 554. During the war, the three evacuated to Yuryevets, living with his maternal grandmother. In 1943, the family returned to Moscow. Tarkovsky continued his studies at his old school, where the poet Andrey Voznesensky was one of his class-mates. He studied piano at a music school and attended classes at an art school. The family lived on Shshipok Street in the Zamoskvorechye District in Moscow. From November 1947 to Spring 1948 he was in the hospital with tuberculosis. Many themes of his childhood – the evacuation, his mother and her two children, the withdrawn father, the time in the hospital – feature prominently in his film "The Mirror".
Following high school graduation, from 1951 to 1952, Tarkovsky studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the USSR. Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold. He participated in a year-long research expedition to the river Kureikye near Turukhansk in the Krasnoyarsk Province. During this time in the Taiga Tarkovsky decided to study film.
Film school student.
Upon returning from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film directing program. He was in the same class as Irma Raush whom he married in April 1957.
The early Khrushchev era offered unique opportunities for young film directors. Before 1953, annual film production was low and most films were directed by veteran directors. After 1953, more films were produced, many of them by young directors. The Khrushchev Thaw relaxed Soviet social restrictions a bit and permitted a limited influx of European and North American literature, films and music. This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson, Andrzej Wajda (whose film "Ashes and Diamonds" influenced Tarkovsky) and Mizoguchi. Tarkovsky absorbed the idea of the auteur as a necessary condition for creativity.
Tarkovsky's teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors. In 1956, Tarkovsky directed his first student short film, "The Killers", from a short story of Ernest Hemingway. The short film "There Will Be No Leave Today" and the screenplay "Concentrate" followed in 1958 and 1959.
An important influence on Tarkovsky was the film director Grigori Chukhrai, who was teaching at the V.G.I.K. Impressed by the talent of his student, Chukhrai offered Tarkovsky a position as assistant director for his film "Clear Skies". Tarkovsky initially showed interest but then decided to concentrate on his studies and his own projects.
During his third year at the V.G.I.K., Tarkovsky met Andrei Konchalovsky. They found much in common as they liked the same film directors and shared ideas on cinema and films. In 1959, they wrote the script "Antarctica – Distant Country", which was later published in the "Moskovskij Komsomolets". Tarkovsky submitted the script to Lenfilm, but it was rejected. They were more successful with the script "The Steamroller and the Violin", which they sold to Mosfilm. This became Tarkovsky's graduation project, earning him his diploma in 1960 and winning First Prize at the New York Student Film Festival in 1961.
Career.
Film career in the Soviet Union.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He had inherited the film from director Eduard Abalov, who had to abort the project. The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in 1962. In the same year, on 30 September, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born.
In 1965, he directed the film "Andrei Rublev" about the life of Andrei Rublev, the fifteenth-century Russian icon painter. "Andrei Rublev" was not immediately released after completion due to problems with Soviet authorities. Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths. A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize. The film was officially released in the Soviet Union in a cut version in 1971.
He divorced his wife, Irma Raush, in June 1970. In the same year, he married Larissa Kizilova (née Egorkina), who had been a production assistant for the film "Andrei Rublev" (they had been living together since 1965). Their son, Andrei Andreyevich Tarkovsky, was born in the same year on 7 August.
In 1972, he completed "Solaris", an adaptation of the novel "Solaris" by Stanisław Lem. He had worked on this together with screen-writer Fridrikh Gorenshtein as early as 1968. The film was presented at the Cannes Film Festival, won the Grand Prix Spécial du Jury and the FIPRESCI prize, and was nominated for the Palme d'Or. From 1973 to 1974, he shot the film "The Mirror", a highly autobiographical and unconventionally structured film drawing on his childhood and incorporating some of his father's poems. Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles "Confession", "White day" and "A white, white day". From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature. Russian authorities placed the film in the "third category," a severely limited distribution, and only allowed it to be shown in third-class cinemas and workers' clubs. Few prints were made and the film-makers received no returns. Third category films also placed the film-makers in danger of being accused of wasting public funds, which could have serious effects on their future productivity. These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry.
During 1975, Tarkovsky also worked on the screenplay "Hoffmanniana", about the German writer and poet E. T. A. Hoffmann. In December 1976, he directed "Hamlet", his only stage play, at the Lenkom Theatre in Moscow. The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films. At the end of 1978, he also wrote the screenplay "Sardor" together with the writer Aleksandr Misharin.
The last film Tarkovsky completed in the Soviet Union was "Stalker", inspired by the novel "Roadside Picnic" by the brothers Arkady and Boris Strugatsky. Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986. Initially he wanted to shoot a film based on their novel "Dead Mountaineer's Hotel" and he developed a raw script. Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on "Roadside Picnic". Work on this film began in 1976. The production was mired in troubles; improper development of the negatives had ruined all the exterior shots. Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where he hired Alexander Knyazhinsky as a new first cinematographer. Furthermore, Tarkovsky suffered a heart attack in April 1978, resulting in further delay. The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival.
In the same year Tarkovsky also began the production of the film "The First Day" (Russian: Первый День "Pervyj Dyen′"), based on a script by his friend and long-term collaborator Andrei Konchalovsky. The film was set in 18th-century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov. To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, omitting several scenes that were critical of the official atheism in the Soviet Union. After shooting roughly half of the film the project was stopped by Goskino after it became apparent that the film differed from the script submitted to the censors. Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film.
Film career outside the Soviet Union.
During the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary "Voyage in Time" together with his long-time friend Tonino Guerra. Tarkovsky returned to Italy in 1980 for an extended trip during which he and Guerra completed the script for the film "Nostalghia". During 1981 he traveled to the United Kingdom and Sweden. During his trip to Sweden he had considered defecting from the Soviet Union, but ultimately decided to return because of his wife and his son.
Tarkovsky returned to Italy in 1982 to start shooting "Nostalghia". He did not return to his home country. As Mosfilm withdrew from the project, he had to complete the film with financial support provided by the Italian RAI. Tarkovsky completed the film in 1983. "Nostalghia" was presented at the Cannes Film Festival and won the FIPRESCI prize and the Prize of the Ecumenical Jury. Tarkovsky also shared a special prize called "Grand Prix du cinéma de creation" with Robert Bresson. Soviet authorities prevented the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again. In the same year, he also staged the opera "Boris Godunov" at the Royal Opera House in London under the musical direction of Claudio Abbado.
He spent most of 1984 preparing the film "The Sacrifice". At a press conference in Milan on 10 July 1984, he announced that he would never return to the Soviet Union and would remain in Europe. At that time, his son Andrei Jr. was still in the Soviet Union and not allowed to leave the country.
During 1985, he shot the film "The Sacrifice" in Sweden. At the end of the year he was diagnosed with terminal lung cancer. In January 1986, he began treatment in Paris and was joined there by his son, who was finally allowed to leave the Soviet Union. "The Sacrifice" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son, Andrei Jr.
In Tarkovsky's last entry (15 December 1986), he wrote: "But now I have no strength left – that is the problem". The diaries are sometimes also known as "" and were published posthumously in 1989 and in English in 1991.
Tarkovsky died in Paris on 29 December 1986. His funeral ceremony was held at the Alexander Nevsky Cathedral. He was buried on 3 January 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France. The inscription on his gravestone, which was created by the Russian sculptor Ernst Neizvestny, reads: "To the man who saw the Angel".
A controversy emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes but was assassinated by the KGB. Evidence for this hypothesis includes testimonies by former KGB agents who claim that Viktor Chebrikov gave the order to eradicate Tarkovsky to curtail what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky. Other evidence includes several memoranda that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause.
As Tarkovsky, his wife Larisa Tarkovskaya and actor Anatoli Solonitsyn all died from the very same type of lung cancer, Vladimir Sharun, sound designer in "Stalker", is convinced that they were all poisoned when shooting the film near a chemical plant.
Filmography.
Tarkovsky is mainly known as a film director. During his career he directed only seven feature films, as well as three shorts from his time at V.G.I.K. He also wrote several screenplays. He furthermore directed the play "Hamlet" for the stage in Moscow, directed the opera "Boris Godunov" in London, and he directed a radio production of the short story "Turnabout" by William Faulkner. He also wrote "Sculpting in Time", a book on film theory.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He then directed "Andrei Rublev" in 1966, "Solaris" in 1972, "The Mirror" in 1975 and "Stalker" in 1979. The documentary "Voyage in Time" was produced in Italy in 1982, as was "Nostalghia" in 1983. His last film "The Sacrifice" was produced in Sweden in 1986. Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a cowriter. Tarkovsky once said that a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.
Awards.
Numerous awards were bestowed on Tarkovsky throughout his lifetime. At the Venice Film Festival he was awarded the Golden Lion for "Ivan's Childhood". At the Cannes Film Festival, he won the FIPRESCI prize four times, the Prize of the Ecumenical Jury three times (more than any other director), and the Grand Prix Spécial du Jury twice. He was also nominated for the Palme d'Or two times. In 1987, the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to "The Sacrifice".
Under the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the Autumn of 1986, shortly before his death, by a retrospective of his films in Moscow. After his death, an entire issue of the film magazine "Iskusstvo Kino" was devoted to Tarkovsky. In their obituaries, the film committee of the Council of Ministers of the USSR and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile.
Posthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union. In 1989 the "Andrei Tarkovsky Memorial Prize" was established, with its first recipient being the Russian animator Yuriy Norshteyn. Since 1993, the Moscow International Film Festival awards the annual "Andrei Tarkovsky Award". In 1996 the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town. A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Georgievna Karachkina in 1982, has also been named after him.
Tarkovsky has been the subject of several documentaries. Most notable is the 1988 documentary "Moscow Elegy", by Russian film director Alexander Sokurov. Sokurov's own work has been heavily influenced by Tarkovsky. The film consists mostly of narration over stock footage from Tarkovsky's films. "Directed by Andrei Tarkovsky" is 1988 documentary film by Michal Leszczylowski, an editor of the film "The Sacrifice". Film director Chris Marker produced the television documentary "One Day in the Life of Andrei Arsenevich" as an homage to Andrei Tarkovsky in 2000.
Ingmar Bergman was quoted as saying: "Tarkovsky for me is the greatest [of us all], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream". Film historian Steven Dillon claims that much of subsequent film was deeply influenced by the films of Tarkovsky.
At the entrance to the Gerasimov Institute of Cinematography in Moscow, Russia there is a monument that includes statues of Tarkovsky, Gennady Shpalikov and Vasily Shukshin.
Influences.
Tarkovsky became a film director during the mid and late 1950s, a period referred to as the Khrushchev Thaw, during which Soviet society opened to foreign films, literature and music, among other things. This allowed Tarkovsky to see films of European, American and Japanese directors, an experience which influenced his own film making. His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director.
Tarkovsky was, according to Shavkat Abdusalmov, a fellow student at the film school, fascinated by Japanese films. He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight. Tarkovsky has also expressed interest in the art of Haiku and its ability to create "images in such a way that they mean nothing beyond themselves."
In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films. The list includes: "Diary of a Country Priest" and "Mouchette", by Robert Bresson; "Winter Light", "Wild Strawberries" and "Persona", by Ingmar Bergman; "Nazarín", by Luis Buñuel; "City Lights", by Charlie Chaplin; "Ugetsu", by Kenji Mizoguchi; "Seven Samurai", by Akira Kurosawa, and "Woman in the Dunes", by Hiroshi Teshigahara. Among his favorite directors were Buñuel, Mizoguchi, Bergman, Bresson, Kurosawa, Michelangelo Antonioni, Jean Vigo, and Carl Theodor Dreyer.
With the exception of "City Lights", the list does not contain any films of the early silent era. The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude. The list has also no films or directors from Tarkovsky's native Russia, although he rated Soviet directors such as Boris Barnet, Sergei Paradjanov and Alexander Dovzhenko highly.
Although strongly opposed to commercial cinema, in a famous exception Tarkovsky praised the blockbuster film "The Terminator", saying its "vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art". He was critical of the "brutality and low acting skills", but nevertheless impressed by this film.
Cinematic style.
Tarkovsky's films are characterized by metaphysical themes, extremely long takes, and memorable images of exceptional beauty. Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera. He once said, “Juxtaposing a person with an environment that is boundless, collating him with a countless number of people passing by close to him and far away, relating a person to the whole world, that is the meaning of cinema.”
Tarkovsky included levitation scenes into several of his films, most notably "Solaris". To him these scenes possess great power and are used for their photogenic value and magical inexplicability.
Water, clouds, and reflections were used by him for their surreal beauty and photogenic value, as well as their symbolism, such as waves or the forms of brooks or running water.
Bells and candles are also frequent symbols. These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self-reflection.
Tarkovsky developed a theory of cinema that he called "sculpting in time". By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it. Unedited movie footage transcribes time in real time. By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another.
Up to, and including, his film "The Mirror", Tarkovsky focused his cinematic works on exploring this theory. After "The Mirror", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day.
Several of Tarkovsky's films have color or black and white sequences. This first occurs in the otherwise monochrome "Andrei Rublev", which features a color epilogue of Rublev's authentic religious icon paintings. All of his films afterwards contain monochrome, and in "Stalker's" case sepia sequences, while otherwise being in color. In 1966, in an interview conducted shortly after finishing "Andrei Rublev", Tarkovsky dismissed color film as a "commercial gimmick" and cast doubt on the idea that contemporary films meaningfully use color. He claimed that in everyday life one does not consciously notice colors most of the time, and that color should therefore be used in film mainly to emphasize certain moments, but not all the time, as this distracts the viewer. To him, films in color were like moving paintings or photographs, which are too beautiful to be a realistic depiction of life.
Vadim Yusov.
Tarkovsky worked in close collaboration with cinematographer Vadim Yusov from 1958 to 1972, and much of the visual style of Tarkovsky's films can be attributed to this collaboration. Tarkovsky would spend two days preparing for Yusov to film a single long take, and due to the preparation, usually only a single take was needed.
Sven Nykvist.
In his last film, "The Sacrifice", Tarkovsky worked with cinematographer Sven Nykvist, who had worked closely with director Ingmar Bergman on many of Ingmar Bergman's films – multiple people who worked with Bergman worked on the production, notably lead actor Erland Josephson, who had acted for Tarkovsky in "Nostalghia". Nykvist complained that Tarkovsky would frequently look through the camera and even direct actors through it.

</doc>
<doc id="677" url="http://en.wikipedia.org/wiki?curid=677" title="Ambiguity">
Ambiguity

Ambiguity is an attribute of any concept, idea, statement or claim whose meaning, intention or interpretation cannot be definitively resolved according to a rule or process consisting of a finite number of steps.
The concept of ambiguity is generally contrasted with vagueness. In ambiguity, specific and distinct interpretations are permitted (although some may not be immediately apparent), whereas with information that is vague, it is difficult to form any interpretation at the desired level of specificity.
Context may play a role in resolving ambiguity. For example, the same piece of information may be ambiguous in one context and unambiguous in another.
Linguistic forms.
The lexical ambiguity of a word or phrase pertains to its having more than one meaning in the language to which the word belongs. "Meaning" here refers to whatever should be captured by a good dictionary. For instance, the word "bank" has several distinct lexical definitions, including "financial institution" and "edge of a river". Another example is as in "apothecary". One could say "I bought herbs from the apothecary". This could mean one actually spoke to the apothecary (pharmacist) or went to the apothecary (pharmacy).
The context in which an ambiguous word is used often makes it evident which of the meanings is intended. If, for instance, someone says "I buried $100 in the bank", most people would not think someone used a shovel to dig in the mud. However, some linguistic contexts do not provide sufficient information to disambiguate a used word. For example,
Lexical ambiguity can be addressed by algorithmic methods that automatically associate the appropriate meaning with a word in context, a task referred to as word sense disambiguation.
The use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used). The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed. An exception to this could include a politician whose "weasel words" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from their candidate of choice. Ambiguity is a powerful tool of political science.
More problematic are words whose senses express closely related concepts. "Good", for example, can mean "useful" or "functional" ("That's a good hammer"), "exemplary" ("She's a good student"), "pleasing" ("This is good soup"), "moral" ("a good person" versus "the lesson to be learned from a story"), "righteous", etc. " I have a good daughter" is not clear about which sense is intended. The various ways to apply prefixes and suffixes can also create ambiguity ("unlockable" can mean "capable of being unlocked" or "impossible to lock").
Syntactic ambiguity arises when a sentence can have two (or more) different meanings because of the structure of the sentence—its syntax. This is often due to a modifying expression, such as a prepositional phrase, the application of which is unclear. "He ate the cookies on the couch", for example, could mean that he ate those cookies that were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies. "To get in, you will need an entrance fee of $10 or your voucher and your drivers' license." This could mean that you need EITHER ten dollars OR BOTH your voucher and your license. Or it could mean that you need EITHER ten dollars OR a voucher AND you also need your license. Only rewriting the sentence, or placing appropriate punctuation can resolve a syntactic ambiguity.
For the notion of, and theoretic results about, syntactic ambiguity in artificial, formal languages (such as computer programming languages), see Ambiguous grammar.
Spoken language can contain many more types of ambiguities, where there is more than one way to compose a set of sounds into words, for example "ice cream" and "I scream". Such ambiguity is generally resolved according to the context. A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen.
Semantic ambiguity happens when a sentence contains an ambiguous word or phrase—a word or phrase that has more than one meaning. In "We saw her duck" (example due to Richard Nordquist), the word "duck" can refer either
For example, "You could do with a new automobile. How about a test drive?" The clause "You could do with" presents a statement with such wide possible interpretation as to be essentially meaningless. Lexical ambiguity is contrasted with semantic ambiguity. The former represents a choice between a finite number of known and meaningful context-dependent interpretations. The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning. This form of ambiguity is closely related to vagueness.
Linguistic ambiguity can be a problem in law, because the interpretation of written documents and oral agreements is often of paramount importance.
Intentional application.
Philosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments, because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments. For example, a politician might say "I oppose taxes which hinder economic growth", an example of a glittering generality. Some will think he opposes taxes in general, because they hinder economic growth. Others may think he opposes only those taxes that he believes will hinder economic growth. In writing, the sentence can be rewritten to reduce possible misinterpretation, either by adding a comma after "taxes" (to convey the first sense) or by changing "which" to "that" (to convey the second sense), or by rewriting it in other ways. The devious politician hopes that each constituent will interpret the statement in the most desirable way, and think the politician supports everyone's opinion. However, the opposite can also be true - An opponent can turn a positive statement into a bad one, if the speaker uses ambiguity (intentionally or not). The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases.
In Continental philosophy (particularly phenomenology and existentialism), there is much greater tolerance of ambiguity, as it is generally seen as an integral part of the human condition. Martin Heidegger argued that the relation between the subject and object is ambiguous, as is the relation of mind and body, and part and whole. In Heidegger's phenomenology, Dasein is always in a meaningful world, but there is always an underlying background for every instance of signification. Thus, although some things may be certain, they have little to do with Dasein's sense of "care" and existential anxiety, e.g., in the face of death. In calling his work Being and Nothingness an "essay in phenomenological ontology" Jean-Paul Sartre follows Heidegger in defining the human essence as ambiguous, or relating fundamentally to such ambiguity. Simone de Beauvoir tries to base an ethics on Heidegger's and Sartre's writings (The Ethics of Ambiguity), where she highlights the need to grapple with ambiguity: "as long as philosophers and they [men] have thought, most of them have tried to mask it...And the ethics which they have proposed to their disciples has always pursued the same goal. It has been a matter of eliminating the ambiguity by making oneself pure inwardness or pure externality, by escaping from the sensible world or being engulfed by it, by yielding to eternity or enclosing oneself in the pure moment.". Ethics cannot be based on the authoritative certainty given by mathematics and logic, or prescribed directly from the empirical findings of science. She states: "Since we do not succeed in fleeing it, let us therefore try to look the truth in the face. Let us try to assume our fundamental ambiguity. It is in the knowledge of the genuine conditions of our life that we must draw our strength to live and our reason for acting". Other continental philosophers suggest that concepts such as life, nature, and sex are ambiguous. Recently, Corey Anton has argued that we cannot be certain what is separate from or unified with something else: language, he asserts, divides what is not in fact separate. Following Ernest Becker, he argues that the desire to 'authoritatively disambiguate' the world and existence has led to numerous ideologies and historical events such as genocide. On this basis, he argues that ethics must focus on 'dialectically integrating opposites' and balancing tension, rather than seeking a priori validation or certainty. Like the existentialists and phenomenologists, he sees the ambiguity of life as the basis of creativity.
In literature and rhetoric, ambiguity can be a useful tool. Groucho Marx's classic joke depends on a grammatical ambiguity for its humor, for example: "Last night I shot an elephant in my pajamas. How he got in my pajamas, I'll never know". Songs and poetry often rely on ambiguous words for artistic effect, as in the song title "Don't It Make My Brown Eyes Blue" (where "blue" can refer to the color, or to sadness).
In narrative, ambiguity can be introduced in several ways: motive, plot, character. F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel "The Great Gatsby".
Christianity and Judaism employ the concept of paradox synonymously with 'ambiguity'. Many Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery which fascinates humans. The orthodox Catholic writer G. K. Chesterton regularly employed paradox to tease out the meanings in common concepts which he found ambiguous, or to reveal meaning often overlooked or forgotten in common phrases. (The title of one of his most famous books, Orthodoxy, itself employing such a paradox.)
Metonymy involves the use of the name of a subcomponent part as an abbreviation, or jargon, for the name of the whole object (for example "wheels" to refer to a car, or "flowers" to refer to beautiful offspring, an entire plant, or a collection of blooming plants). In modern vocabulary critical semiotics, metonymy encompasses any potentially ambiguous word substitution that is based on contextual contiguity (located close together), or a function or process that an object performs, such as "sweet ride" to refer to a nice car. Metonym miscommunication is considered a primary mechanism of linguistic humour.
Psychology and management.
In sociology and social psychology, the term "ambiguity" is used to indicate situations that involve uncertainty. An increasing amount of research is concentrating on how people react and respond to ambiguous situations. Much of this focuses on ambiguity tolerance. A number of correlations have been found between an individual's reaction and tolerance to ambiguity and a range of factors.
Apter and Desselles (2001) for example, found a strong correlation with such attributes and factors like a greater preference for safe as opposed to risk-based sports, a preference for endurance-type activities as opposed to explosive activities, a more organized and less casual lifestyle, greater care and precision in descriptions, a lower sensitivity to emotional and unpleasant words, a less acute sense of humor, engaging a smaller variety of sexual practices than their more risk-comfortable colleagues, a lower likelihood of the use of drugs, pornography and drink, a greater likelihood of displaying obsessional behavior.
In the field of leadership, David Wilkinson (2006) found strong correlations between an individual leader's reaction to ambiguous situations and the Modes of Leadership they use, the type of creativity, Kirton (2003) and how they relate to others.
Music.
In music, pieces or sections which confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p. 79) any aspect of music. The music of Africa is often purposely ambiguous. To quote Sir Donald Francis Tovey (1935, p. 195), "Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value."
Visual art.
In visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways. Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception.
The opposite of such ambiguous images are impossible objects.
Pictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?
Constructed language.
Some languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity. Lojban and Loglan are two related languages which have been created with this in mind, focusing chiefly on syntactic ambiguity as well. The languages can be both spoken and written. These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized. Languages composed from many diverse sources contain much ambiguity and inconsistency. The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.
Computer science.
In computer science, the SI prefixes kilo-, mega- and giga- are used ambiguously to mean either the first three powers of 1000 (1000, 10002 and 10003) or the first three powers of 1024 (1024, 10242 and 10243), respectively.
Mathematical notation.
Mathematical notation, widely used in physics and other sciences, avoids many ambiguities compared to expression in natural language. However, for various reasons, several lexical, syntactic and semantic ambiguities remain.
Names of functions.
The ambiguity in the style of writing a function should not be confused with a multivalued function, which can (and should) be defined in a deterministic and unambiguous way. Several special functions still do not have established notations. Usually, the conversion to another notation requires to scale the argument and/or the resulting value; sometimes, the same name of the function is used, causing confusions. Examples of such underestablished functions:
Expressions.
Ambiguous expressions often appear in physical and mathematical texts.
It is common practice to omit multiplication signs in mathematical expressions. Also, it is common to give the same name to a variable and a function, for example, formula_1. Then, if one sees formula_2, there is no way to distinguish whether it means formula_1 multiplied by formula_4, or function formula_5 evaluated at argument equal to formula_4. In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning.
Creators of algorithmic languages try to avoid ambiguities. Many algorithmic languages (C++ and Fortran) require the character * as symbol of multiplication. The Wolfram language used in Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions. Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression f=f(x) is qualified as an error.
The order of operations may depend on the context. In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right. Until the last century, many editorials assumed that multiplication is performed first, for example, formula_7 is interpreted as formula_8; in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language. In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity.
Sometimes, one uses "italics" letters to denote elementary functions.
In the scientific journal style, the expression
formula_9
means
product of variables
formula_10,
formula_11,
formula_12 and
formula_13, although in a slideshow, it may mean formula_14.
A comma in subscripts and superscripts sometimes is omitted; it is also ambiguous notation.
If it is written formula_15, the reader should guess from the context, does it mean a single-index object, evaluated while the subscript is equal to product of variables
formula_16, formula_12 and formula_18, or it is indication to a trivalent tensor.
The writing of formula_15 instead of formula_20 may mean that the writer either is stretched in space (for example, to reduce the publication fees) or aims to increase number of publications without considering readers. The same may apply to any other use of ambiguous notations.
Subscripts are also used to denote the argument to a function, as in formula_21.
Examples of potentially confusing ambiguous mathematical expressions.
formula_22, which could be understood to mean either formula_23 or formula_24. In addition, formula_25 may mean formula_26, as formula_27 means formula_28 (see tetration).
formula_29, which by convention means formula_30, though it might be thought to mean formula_31, since formula_32 means formula_33.
formula_34, which arguably should mean formula_35 but would commonly be understood to mean formula_36 .
Notations in quantum optics and quantum mechanics.
It is common to define the coherent states in quantum optics with formula_37 and states with fixed number of photons with formula_38. Then, there is an "unwritten rule": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_39photon state if the Latin characters dominate. The ambiguity becomes even worse, if formula_40 is used for the states with certain value of the coordinate, and formula_41 means the state with certain value of the momentum, which may be used in books on quantum mechanics. Such ambiguities easy lead to confusions, especially if some normalized adimensional, dimensionless variables are used. Expression formula_42 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on. The reader is supposed to guess from the context.
Ambiguous terms in physics and mathematics.
Some physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients), depends on the system of notations. Many terms are ambiguous. Each use of an ambiguous term should be preceded by the definition, suitable for a specific case. Just like Ludwig Wittgenstein states in Tractatus Logico-Philosophicus: "... Only in the context of a proposition has a name meaning." 
A highly confusing term is "gain". For example, the sentence "the gain of a system should be doubled", without context, means close to nothing.
It may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled.
It may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled.
It may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state).
The term "intensity" is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.
Also, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail which still can be resolved at the background of statistical noise. See also Accuracy and precision and its talk.
The Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as "definable" or "nameable". Terms of this kind give rise to vicious circle fallacies. Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.
Mathematical interpretation of ambiguity.
In mathematics and logic, ambiguity can be considered to be an "underdetermined system" (of equations or logic) – for example, formula_43 leaves open what the value of "X" is – while its opposite is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, in an overdetermined system – such as formula_44, which has no solution – see also underdetermination.
Logical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher.
Pedagogic use of ambiguous expressions.
Ambiguity can be used as a pedagogical trick, to force students to reproduce the deduction by themselves. Some textbooks
give the same name to the function and to its Fourier transform:
Rigorously speaking, such an expression requires that formula_46;
even if function formula_47 is a self-Fourier function, the expression should be written as
formula_48; however, it is assumed that
the shape of the function (and even its norm
formula_49) depend on the character used to denote its argument.
If the Greek letter is used, it is assumed to be a Fourier transform of another function,
The first function is assumed, if the expression in the argument contains more characters formula_50 or formula_51, than characters formula_52, and the second function is assumed in the opposite case. Expressions like formula_53 or formula_54 contain symbols formula_50 and formula_52 in equal amounts; they are ambiguous and should be avoided in serious deduction.

</doc>
<doc id="679" url="http://en.wikipedia.org/wiki?curid=679" title="Animal (disambiguation)">
Animal (disambiguation)

An animal is a multicellular, eukaryotic organism of the kingdom Animalia or Metazoa.
Animal or Animals may also refer to:

</doc>
<doc id="680" url="http://en.wikipedia.org/wiki?curid=680" title="Aardvark">
Aardvark

The aardvark ( ; "Orycteropus afer") is a medium-sized, burrowing, nocturnal mammal native to Africa. It is the only living species of the order Tubulidentata, although other prehistoric species and genera of Tubulidentata are known. Unlike New World edentates such as the giant anteater, it has a long pig-like snout, which is used to sniff out food. It roams over most of the southern two-thirds of the African continent, avoiding mainly rocky areas. A nocturnal feeder, it subsists on ants and termites, which it will dig out of their hills using its sharp claws and powerful legs. It also will utilize its digging ability to create burrows in which to live and rear its young. It receives a "least concern" rating from the IUCN, although its numbers seem to be decreasing.
Naming and taxonomy.
The aardvark is sometimes colloquially called "African antbear", "anteater", or the "Cape anteater" after the Cape of Good Hope. The name "aardvark" (;) comes from earlier Afrikaans (erdvark) and means "earth pig" or "ground pig" ("aarde" earth/ground, "vark" pig), because of its burrowing habits (similar origin to the name groundhog). The name "Orycteropus" means burrowing foot, and the name "afer" refers to Africa. The name of the aardvarks's order, "Tubulidentata" comes from the tubule style teeth.
The aardvark is not closely related to the pig; rather, it is the sole extant representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form one variable species of the genus "Orycteropus", the sole surviving genus in the family Orycteropodidae. The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance. The similarities are based on convergent evolution. The closest living relatives of the aardvark are the elephant shrews, along with the sirenians, hyraxes, elephants, and tenrecs. With their extinct relatives, these animals form the superorder Afrotheria. Studies of the brain have shown the similarities with Condylarthra. The scientific name of the aardvark comes from Greek ορυκτερόπους ("orykterópous") meaning "digging footed" and afer: from Africa.
Based on fossils, Bryan Patterson has concluded that early relatives of the aardvark appeared in Africa around the end of the Paleocene. The mysterious Pleistocene "Plesiorycteropus", from Madagascar was, when first described, originally thought to be a tubulidentate that was descended from ancestors that entered the island during the Eocene: too many subtle anatomical differences coupled with recent molecular evidence now lead researchers to believe that "Plesiorycteropus" is a relative of the golden mole and tenrecs that achieved an aardvark-like appearance and ecological niche through convergent evolution. The first known tubulidentate was probably "Myorycteropus africanus" from Kenyan Miocene deposits. The earliest example from the "Orycteropus" genus was the "Orycteropus mauritanicus" found in Algeria in deposits from the middle Miocene, with an equally aged version found in Kenya. Fossils from the aardvark have been dated to 5 million years, and have been located throughout Europe and the Near East. A close relative lived in Madagascar during the last ice age.
Subspecies.
The aardvark has seventeen poorly defined subspecies listed:
Description.
The aardvark is vaguely pig-like in appearance. Its body is stout with a prominently arched back and is sparsely covered with coarse hairs. The limbs are of moderate length, with the rear legs being longer than the forelegs. The front feet have lost the pollex (or 'thumb'), resulting in four toes, while the rear feet have all five toes. Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof. Whereas the aardvark is considered digitigrade, it appears at time to be plantigrade. This confusion happens because when it squats it stands on its soles.
An aardvark's weight is typically between . An aardvark's length is usually between , and can reach lengths of when its tail (which can be up to ) is taken into account. It is tall at the shoulder, and has a girth of about . It is the largest member of the proposed clade Afroinsectiphilia. The aardvark is pale yellowish-gray in color and often stained reddish-brown by soil. The aardvark's coat is thin, and the animal's primary protection is its tough skin. Its hair is short on its head and tail; however its legs tend to have longer hair. The hair on the majority of its body is grouped in clusters of 3-4 hairs. The hair surrounding its nostrils is dense to help filter particulate matter out as it digs. Its tail is very thick at the base and gradually tapers.
Head.
The greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. It contains a thin but complete zygomatic arch. The head of the aardvark contains many unique and different features. One of the most distinctive characteristics of the Tubulidentata is their teeth. Instead of having a pulp cavity, each tooth has a cluster of thin, hexagonal, upright, parallel tubes of vasodentin (a modified form of dentine), with individual pulp canals, held together by cementum. The number of columns is dependent on the size of the tooth, with the largest having about 1,500. The teeth have no enamel coating and are worn away and regrow continuously. The aardvark is born with conventional incisors and canines at the front of the jaw, which fall out and are not replaced. Adult aardvarks have only cheek teeth at the back of the jaw, and have a dental formula of: These remaining teeth are peg-like and rootless and are of unique composition. The teeth consist of 14 upper and 12 lower jaw molars. The nasal area of the aardvark is another unique area, as it contains ten nasal conchae, more than any other placental mammal. The sides of the nostrils are thick with hair. The tip of the snout is highly mobile and is moved by modified mimetic muscles. The fleshy dividing tissue between its nostrils probably has sensory functions, but it is uncertain if it is olfactory or vibration in nature. Its nose is made up of more a turbinate bones than any other mammal, with between 9 and 11, compared to dogs with 4 to 5. With a large quantity of turbinate bones, the aardvark has more space for the moist epithelium, which is the location of the olfactory bulb. The nose contains more olfactory bulbs than any other mammal, with 9. Its keen sense of smell is not just from the quantity of bulbs in the nose but also in the development of the brain, as its olfactory lobe is very developed. The snout resembles an elongated pig snout. The mouth is small and tubular, typical of species that feed on ants and termites. The aardvark has a long, thin, snakelike, protruding tongue (as much as long) and elaborate structures supporting a keen sense of smell. The ears, which are very effective, are disproportionately long, about long. The eyes are small for its head, and consist only of rods.
Digestive system.
The aardvark's stomach has a muscular pyloric area that acts as a gizzard, (it grinds the food up) to make chewing not important. Its cecum is large. Both males and females emit a strong smelling secretion from an anal gland. Its salivary glands are highly developed and almost completely ring the neck, and their output is what causes the tongue to maintain its tackiness. The female has two pairs of teats in the inguinal region.
Genetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa.
Habitat and range.
Aardvarks live in sub-Saharan Africa, where there is suitable habitat for them to live, such as savannas, grasslands, woodlands and bushland, and available food (i.e., ants and termites). They hide in dark underground burrows to avoid the warm weather. The only major habitat that they are not present in is swamp forest, as the high water table interferes with digging. They have been documented as high as in Ethiopia. They are known to live throughout sub-Saharan Africa all the way to South Africa with few exceptions. These exceptions are coastal areas of Namibia, Ivory Coast, and Ghana. They are also not present in Madagascar. They avoid rocky terrain as it causes problems with digging.
Ecology and behavior.
Aardvarks live for up to 23 years in captivity.
Its keen hearing warns it of predators: lions, leopards, hyenas, and pythons. The aardvark's main predators are lions, leopards, hunting dogs and pythons. Some humans also hunt aardvarks for meat. Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs lying motionless except to lash out with all four feet. They are capable of causing substantial damage to unprotected areas of an attacker. They will also dig to escape as they can, when pressed, dig extremely quickly. Their thick skin also protects them to some extent.
Feeding.
The aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (formivore); the only fruit eaten by aardvarks is the aardvark cucumber. In fact, the cucumber and the aardvark have a symbiotic relationship as they eat the subterranean fruit, then defecate the seeds near their burrows, which then grow rapidly due to the loose soil and fertile nature of the area. The time spent in the intestine of the aardvark helps the fertility of the seed, and the fruit provides needed moisture for the aardvark. They avoid eating the African driver ant and red ants. Due to their stringent diet requirements, they require a large range to survive. An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing . While foraging for food, the aardvark will keep its nose to the ground and its ears pointed forward, which indicates that both smell and hearing are involved in the search for food. They zig-zag as they forage and will usually not repeat a route for 5–8 days as they appear to allow time for the termite nests to recover before feeding on it again. During a foraging period, they will stop and dig a "V" shaped trench with their forefeet and then sniff it profusely as a means to explore their location. When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded. Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly. It avoids inhaling the dust by sealing the nostrils. When successful, the aardvark's long (up to ) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin. After an aardvark visit at a termite mound, other animals will visit to pick up all the leftovers. Termite mounds alone don't provide enough food for the aardvark, so they look for termites that are on the move. When these insects move, they can form columns long and these tend to provide easy pickings with little effort exerted by the aardvark. These columns are more common in areas of livestock or other hoofed animals. The trampled grass and dung attract termites from Odontotermes, Microtermes, and Pseudacanthotermes genera.
On a nightly basis they tend to be more active during the first portion of the night time (2000-2400); however, they don't seem to prefer bright or dark nights over the other. During adverse weather or if disturbed they will retreat to their burrow systems. They cover between per night; however, some studies have shown that they may traverse as far as in a night.
Vocalization.
The aardvark is a rather quiet animal. However it does make soft grunting sounds as it forages and loud grunts as it makes for its tunnel entrance. It makes a bleating sound if frightened. When it is threatened it will make for one of its burrows. If one is not close it will dig a new one rapidly. This new one will be short and require the aardvark to back out when the coast is clear.
Movement.
The aardvark is known to be a good swimmer and has been witnessed successfully swimming in strong currents. It can dig 1 yard of tunnel in about 5 minutes, but otherwise moves fairly slowly.
When leaving the burrow at night, they pause at the entrance for about ten minutes, sniffing and listening. After this period of watchfulness, it will bound out and within seconds it will be away. It will then pause, prick its ears, twisting its head to listen, then jump and move off to start foraging.
Aside from digging out ants and termites, the aardvark also excavates burrows in which to live; of which they generally fall into three categories: burrows made while foraging, refuge and resting location, and permanent homes. Temporary sites are scattered around the home range and are used as refuges, while the main burrow is also used for breeding. Main burrows can be deep and extensive, have several entrances and can be as long as . These burrows can be large enough for a man to enter. The aardvark changes the layout of its home burrow regularly, and periodically moves on and makes a new one. The old burrows are an important part of the African wildlife scene. As they are vacated, then they are inhabited by smaller animals like the African wild dog, ant-eating chat, "Nycteris thebaica" and warthogs. Other animals that use them are hares, mongooses, hyenas, owls, pythons, and lizards. Without these refuges many animals would die during wildfire season. Only mothers and young share burrows; however, the aardvark is known to live in small family groups or as a solitary creature. If attacked in the tunnel, it will escape by digging out of the tunnel thereby placing the fresh fill between it and its predator, or if it decides to fight it will roll onto its back, and attack with its claws. The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.
Reproduction.
Aardvarks pair only during the breeding season; after a gestation period of seven months, one cub weighing around is born during May–July. When born, the young has flaccid ears and many wrinkles. When nursing, it will nurse off each teat in succession. After two weeks the folds of skin disappear and after three, the ears can be held upright. After 5–6 weeks, body hair starts growing. It is able to leave the burrow to accompany its mother after only two weeks, and is eating termites at 9 weeks and is weaned by 16 weeks. By 3 months of age the young has been weaned. At six months of age it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually mature from approximately two years of age.
Conservation.
Aardvarks were thought to have declining numbers, however this is possibly due to the fact that they are not readily seen. There are no definitive counts because of their nocturnal and secretive habits; however their numbers seem to be stable overall. They are not considered common anywhere in Africa, but due to their large range, they maintain sufficient numbers. There may be a slight decrease in numbers in eastern, northern, and western Africa. Southern African numbers are not decreasing. It receives an official designation from the IUCN as least concern. However, they are a species in a precarious situation as they are so dependent on such specific food; therefore if a problem arises with the abundance of termites, the species as a whole would be affected drastically.
Aardvarks handle captivity well, and the first zoo to have one was the London Zoo in 1869, from South Africa.
Mythology and popular culture.
In African folklore, the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants. Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree. Wrapped in a piece of skin and worn on the chest, the charm is said to give the owner the ability to pass through walls or roofs at night. The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission. Also, some tribes, such as the Margbetu, Ayanda, and Logo, will use the teeth of the aardvark to make bracelets that are regarded as good luck charms. The meat, which has a resemblance to pork, is eaten in certain cultures.
The Egyptian god Set is said (by some) to have the head of an aardvark or to be part aardvark.
The titular character of "Arthur", an animated television series for children based on a book series and produced by WGBH, shown in more than 180 countries, is an aardvark.

</doc>
<doc id="681" url="http://en.wikipedia.org/wiki?curid=681" title="Aardwolf">
Aardwolf

The aardwolf ("Proteles cristata") is a small, insectivorous mammal, native to East Africa and Southern Africa. Its name means "earth wolf" in the Afrikaans / Dutch language. It is also called "maanhaar jackal", or "civet hyena", based on the secretions (civet) from their anal glands. The aardwolf is in the same family as the hyenas. Unlike many of its relatives in the order Carnivora, the aardwolf does not hunt large animals, or even eat meat on a regular basis; instead it eats insects, mainly termites – one aardwolf can eat about 250,000 termites during a single night by using its long, sticky tongue to capture them. The aardwolf lives in the scrublands of eastern and southern Africa – these are open lands covered with stunted trees and shrubs. The aardwolf is nocturnal, resting in burrows during the day and emerging at night to seek food. Their diet consists mainly of termites, and insect larvae.
Taxonomy.
The aardwolf is the only surviving species in the mammalian subfamily "Protelinae". There is disagreement as to whether there are any subspecies or if the species is monotypic. Some sources say that there are two subspecies, "Proteles cristatus cristatus" of Southern Africa and "Proteles cristatus septentrionalis" of East Africa, whereas others say it is monotypic. Recent studies have shown that the aardwolf probably broke away from the rest of the hyena family early on; however how early is still unclear as the fossil record and the genetic studies differ by 10 millions years.
The aardwolf is generally classified with the Hyaenidae, though it was formerly placed into the family "Protelidae". Early on, scientists felt that it was merely mimicking the striped hyena, which subsequently led to the creation of the Hyaenidae family.
Etymology.
The genus name "proteles" comes from two words both of Greek origin, "protos" and "teleos" which combined means "complete in front" based on the fact that they have 5 toes on their front feet and four on the rear. The species name, "cristatus" comes from Latin and means "provided with a comb", relating to their mane.
Physical characteristics.
The aardwolf resembles a very thin striped hyena, but with a more slender muzzle, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the midline of the neck and back. They also have one or two diagonal stripes down the fore and hindquarters, along with several stripes on its legs. The mane is raised during confrontations in order to make the aardwolves appear larger. It is missing the throat spot that others in the family have. Its lower leg (from the knee down) is all black, and its tail is bushy with a black tip. The aardwolf is about long, excluding its bushy tail, which is about long, and stands about tall at the shoulders. An adult aardwolf weighs approximately , sometimes reaching . The aardwolfs in the south of the continent tend to be smaller (about ), whereas the eastern version weighs more (around ). The front feet have five toes each, unlike the four-toed hyena. The teeth and skull are similar to those of the hyena, though smaller, and its cheek teeth are specialised for eating insects. It does still have canines; however unlike the hyena, these teeth are used primarily for fighting and defense. Its ears, which are large, are very similar to the Striped Hyena.
As the aardwolf ages, it will normally lose some of its teeth, though this has little impact on their feeding habits due to the softness of the insects that they eat. The aardwolf has two anal glands that secrete a musky fluid for marking territory and for communicating with other aardwolves.
Distribution and habitat.
Aardwolves live in open, dry plains and bushland, avoiding mountainous areas. Due to their specific food requirements, they are only found in regions where termites of the family Hodotermitidae occur. Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland. For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens, which are occupied for six weeks at a time.
There are two distinct populations: one in Southern Africa, and another in East and Northeast Africa. The species does not occur in the intermediary miombo forests.
An adult pair, along with their most recent offspring, will occupy a territory of .
Behavior.
Aardwolves are shy and nocturnal, sleeping in underground burrows by day. They will, on occasion during the winter, become diurnal feeders. This happens during the coldest periods as they then stay in at night to conserve heat.
They have often been mistaken for solitary animals. In fact, they live as monogamous pairs with their young. If their territory is infringed upon, they will chase the intruder up to or to the border. If the intruder is caught, which rarely happens, a fight will occur, which is accompanied by soft clucking, hoarse barking, and a type of roar. The majority of incursions occur during mating season, when they can occur 1–2 times per week. When food is scarce the stringent territorial system may be abandoned and as many as three pairs may occupy a "single territory."
The territory is marked by both sexes, as they both have developed anal glands from which they extrude a black substance that is smeared on rocks or grass stalks in long streaks. They often mark near termite mounds within their territory every 20 minutes or so. If they are patrolling their territorial boundaries, the marking frequency increases drastically, to once every . At this rate, an individual may mark 60 marks per hour, and upwards of 200 per night.
An aardwolf pair may have up to ten dens, and numerous middens, within their territory. When they deposit feces at their middens, they dig a small hole and then cover it with sand. Their dens are usually abandoned aardvark, springhare, or porcupine dens, or on occasion they are crevices in rocks. They will also dig their own dens, or enlarge dens started by springhares. They typically will only use one or two dens at a time, rotating through all of their dens every 6 months. During the summer, they may rest outside their den during the night, and sleep underground during the heat of the day.
Aardwolfs are not fast runners nor are they particularly adept at fighting off predators. Therefore, when threatened, the aardwolf will attempt to mislead its foe by doubling back on its tracks. If confronted it will raise its mane in an attempt to appear more menacing. It will also emit a foul-smelling liquid from its anal glands.
Feeding.
The aardwolf feeds primarily on termites and more specifically on "Trinervitermes". This genus of termites has different species throughout the aardwolfs range. In East Africa, they eat "Trinervitermes bettonianus", and in central Africa they eat "Trinervitermes rhodesiensis", and finally in southern Africa, they eat "Trinervitermes trinervoides". Their technique consists of licking them off the ground as opposed to the aardvark which will dig into the mound. They locate their food by sound and also from the scent secreted by the soldier termites. An aardwolf may consume up to 250,000 termites per night using its sticky, long tongue. They do not destroy the termite mound or consume the entire colony, thus ensuring that the termites can rebuild and provide a continuous supply of food. They will often memorize the location of such nests and return to them every few months. During certain seasonal events, such as the onset of the rainy season and the cold of mid-winter, the primary termites become scarce and so the need for other forms of sustenance becomes pronounced. During these times the southern aardwolf will seek out "Hodotermes mossambicus", a type of harvester termite, a termite active in the afternoon, which explains some of their diurnal behavior in the winter. The eastern aardwolf will, during the rainy season, get variety by subsisting on termites from the "Odontotermes" and "Macrotermes" genera. They are also known to feed on other insects, larvae, eggs and, some sources say, occasionally small mammals and birds, but these constitute a very small percentage of their total diet. Unlike other hyenas, aardwolves do not scavenge or kill larger animals. Contrary to popular myths, aardwolfs do not eat carrion, and if they are seen eating while hunched over a dead carcass, it is actually eating larvae and beetles. Also, contrary to some sources, they do not like meat, unless it is finely ground or cooked for them. The adult aardwolf was formerly assumed to forage in small groups, however more recent research has shown that they are primarily solitary foragers, necessary because of the scarcity of their insect prey. Their primary source, "Trinervitermes", forages in small but dense patches of . While foraging, the aardwolf will cover about per hour, which translates to per summer night and per winter night.
Breeding.
The breeding season varies depending on their location, but normally takes place during autumn or spring. In South Africa, breeding occurs in early July. During the breeding season, unpaired male aardwolves will search their own territory, as well as others, for a female to mate with. Dominant males will also mate opportunistically with the females of less dominant neighboring aardwolves, which can result in conflict between rival males. Dominant males will even go a step further and as the breeding season approaches, they will make increasingly greater and greater incursions onto weaker males' territories. As the female comes into oestrus, they will add pasting to their tricks inside of the other territories, sometimes doing so more in rivals' territories than their own. Females will also, when given the opportunity, mate with the dominant male, which will increase the chances of the dominat male guarding "his" cubs with her. Gestation lasts between 89 and 92 days, producing two to five cubs (most often two or three) during the rainy season (Nov–Dec), when termites are more active. They are born with their eyes open but initially are helpless, and weigh around . The first six to eight weeks are spent in the den with their parents. The male may spend up to six hours a night watching over the cubs while the mother is out looking for food. After three months, they begin supervised foraging and by four months are normally independent, though they will often share a den with their mother until the next breeding season. By the time the next set of cubs is born, the older cubs have moved on. Aardwolves generally achieve sexual maturity at one and a half to two years of age.
Conservation.
The aardwolf has not seen decreasing numbers and they are relatively widespread throughout eastern Africa. They are not common throughout their range, as they maintain a density of no more than 1 per square kilometer, if the food is good. Because of these factors, the IUCN has rated the aardwolf as least concern. In some areas, they are persecuted by man because of the mistaken belief that they prey on livestock; however, they are actually beneficial to the farmers because they eat termites which are detrimental. Other areas, the farmers have recognized this, but they are still killed, on occasion, for their fur. Dogs and insecticides are also common killers of the aardwolf.
Interaction with humans.
Aardwolfs are common sights at zoos. Frankfurt Zoo in Germany was home to the oldest recorded aardwolf in captivity at 18 years and 11 months.

</doc>
<doc id="682" url="http://en.wikipedia.org/wiki?curid=682" title="Adobe">
Adobe

Adobe (, , ; Arabic: الطوب) is the Spanish word for mud brick, a natural building material made from sand, clay, water, and some kind of fibrous or organic material (sticks, straw, and/or manure), usually shaped into bricks using molds and dried in the sun. Adobe buildings are similar to cob and rammed earth buildings, but cob and rammed earth are directly made into walls rather than bricks. The Romanian name for this material is chirpici.
For a deeper understanding of adobe, one might examine a cob building. Cob, a close cousin to adobe, contains proportioned amounts of soil, clay, water, manure, and straw. This is blended, but not formed like adobe. Cob is spread and piled over the home's frame and allowed to air dry for several months before habitation. Adobe, then, can be described as dried bricks of cob, stacked and mortared together with more adobe mixture to create a thick wall and/or roof.
Adobe structures are extremely durable, and account for some of the oldest existing buildings in the world. Compared to wooden buildings, adobe buildings offer significant advantages due to their greater thermal mass, in hot climates, but they are known to be particularly susceptible to earthquake damage.
Buildings made of sun-dried earth are common in West Asia, North Africa, West Africa, South America, southwestern North America, Spain (usually in the Mudéjar style), Eastern Europe and East Anglia, particularly Norfolk, known as "clay lump". Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andean region of South America for several thousand years, although often substantial amounts of stone are used in the walls of Pueblo buildings. (Also, the Pueblo people built their adobe structures with handfuls or basketfuls of adobe, until the Spanish introduced them to the making of bricks.) Adobe brickmaking was used in Spain starting by the Late Bronze Age and Iron Age, from the eighth century B.C. on. Its wide use can be attributed to its simplicity of design and manufacture, and the economy of creating it.
A distinction is sometimes made between the smaller "adobes", which are about the size of ordinary baked bricks, and the larger "adobines", some of which may be one to two yards (1–2 m) long.
Etymology.
The word "adobe" has existed for around 4,000 years, with relatively little change in either pronunciation or meaning. The word can be traced from the Middle Egyptian (c. 2000 BC) word "dbt" "mud brick." As Middle Egyptian evolved into Late Egyptian, Demotic, and finally Coptic (c. 600 BC), τωωβε "dj-b-t" became "tobe" "[mud] brick." This was borrowed into Arabic as "al tob", "tuba", or "Al-ţŭb." (الطّوب "al" "the" + "ţŭb." "brick") "[mud] brick," which was assimilated into Old Spanish as "adobe" , still with the meaning "mud brick." English borrowed the word from Spanish in the early 18th century.
In more modern English usage, the term "adobe" has come to include a style of architecture popular in the desert climates of North America, especially in New Mexico. (Compare with stucco).
Composition.
An adobe brick is a composite material made of clay mixed with water and an organic material such as straw or dung. The soil composition typically contains clay and sand. Straw is useful in binding the brick together and allowing the brick to dry evenly, thereby preventing cracking due to uneven shrinkage rates through the brick. Dung offers the same advantage and is also added to repel insects. The most desirable soil texture for producing the mud of adobe is 15% clay, 10-30% silt and 55-75% fine sand. Another source quotes 15-25% clay and the remainder sand and coarser particles up to cobbles 2-10 inches with no deleterious effect. Modern adobe is stabilized with either emulsified asphalt or Portland cement up to 10% by weight.
The clay content should be a mixture of no more that half expansive clays with the remainder non-expansive illite or kaolinite. Too much expansive clay results in uneven drying through the brick and cracking while too much kaolinite will make a weak brick. Typically the soils of the Southwest United States where such construction is in use, are an adequate composition.
Adobe bricks.
Bricks are made in an open frame, being a reasonable size, but any convenient size is acceptable. The mixture is molded into a frame, and then the frame is removed after initial setting. After drying a few hours, the bricks are turned on edge to finish drying. Slow drying in shade reduces cracking.
The same mixture used to make bricks, but without straw, is used for mortar and often for plaster on interior and exterior walls. Some ancient cultures used lime-based cement for the plaster to protect against rain damage. 
The brick’s thickness is preferred partially due to its thermal characteristics, and partially due to the stability of a thicker brick versus a more standard-sized brick. Depending on the form into which the mixture is pressed, adobe can encompass nearly any shape or size, provided drying is even and the mixture includes reinforcement for larger bricks. Reinforcement can include manure, straw, cement, rebar or wooden posts. Experience has shown straw, cement, or manure added to a standard adobe mixture can all produce a stronger, more crack-resistant brick. A general testing is done on the soil content first. To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid. After it is sealed, the container is shaken vigorously for at least one minute. It is then allowed to sit on a flat surface for a day or so until the soil has settled into layers or remains in suspension. Heavier particles settle out first, so gravel will be on the bottom, sand above, silt above that and very fine clay and organic matter will stay in suspension for days. After the water has cleared, percentages of the various particles can be determined. Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks. The New Mexico State University Extension Service recommends a mix of not more than 1/3 clay, not less than 1/2 sand, and never more than 1/3 silt.
Material properties.
Adobe walls are load bearing, i.e. they carry their own weight into the foundation rather than by another structure, hence the adobe must have sufficient compressive strength. In the United States, most building codes call for a minimum compressive strength of 300 lbf/in.2 Adobe construction should be designed so as to avoid lateral structural loads that would cause bending loads. The building codes require the building sustain a 1 g lateral acceleration earthquake load. Such an acceleration will cause lateral loads on the walls, resulting in shear and bending and inducing tensile stresses. To withstand such loads, the codes typically call for a tensile modulus of rupture strength of at least 50 lbf/in.2
In addition to being an inexpensive material with a small resource cost, adobe can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction. In climates typified by hot days and cool nights, the high thermal mass of adobe averages out the high and low temperatures of the day, moderating the living space temperature. The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior. After the sun sets and the temperature drops, the warm wall will then continue to transfer heat to the interior for several hours due to the time lag effect. Thus, a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material.
Thermodynamic material properties are sparsely quoted. The thermal conductivity of adobe is quoted as having an R value of R0 = 0.41 hr*ft2*°F/(Btu*in). To determine the total R value, multiply R0, by the thickness of the adobe wall. From that, the conductivity is found to be k = 0.20 Btu/(hr*ft*°F) or 0.35 W/(m*K). The heat capacity is commonly quoted as cp = 0.20 Btu/(lbm*F) or 840 joules/(kg*K). The density is 95 lbm/ft3 or 1520 kg/m3. The thermal diffusivity is calculated to 0.0105 ft2/hr or 2.72x10−7 m2/s.
Adobe wall construction.
When building an adobe structure, the ground should be compressed as the weight of adobe wall is significantly greater than that of a frame house, and foundation settling may cause cracking of the wall. The footing is dug and compressed once again. Footing depth is dug to below the ground frost level and depends on the region. The footing and stem wall are commonly 24 and 14 inches respectively, much greater than for a frame house due of the weight of the walls. Modern construction codes call for the use of reinforcing steel in the footing and stem wall. Adobe bricks are laid by course. Each course is laid the whole length of the wall, overlapping at the corners on a layer of adobe mortar. Adobe walls usually never rise above two stories as they are load bearing and have low structural strength. When creating window and door openings, a lintel is placed on top of the opening to support the bricks above. Atop the last courses of brick, bond beams made of reinforced concrete or heavy wood beams are laid to provide a horizontal bearing plate for the roof beams and to redistribute lateral earthquake loads to shear walls more able to carry the forces. To protect the interior and exterior adobe wall, finishes can be applied, such as mud plaster, whitewash or stucco. These finishes protect the adobe wall from water damage, but need to be reapplied periodically, or the walls can be finished with other nontraditional plasters providing longer protection. Bricks made with stabilized adobe generally do not need protection of plasters.
Adobe roof.
The traditional adobe roof has been constructed using a mixture of soil/clay, water, sand, and organic materials. The mixture was then formed and pressed into wood forms, producing rows of dried earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe.
Roof materials.
Depending on the materials available, a roof may be assembled using wood or metal beams to create a framework to begin layering adobe bricks. Depending on the thickness of the adobe bricks, the framework has been preformed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly. This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent cracking.
Traditional adobe roof.
The more traditional flat adobe roofs are functional only in dry climates that are not exposed to snow loads which would call for a steepled roof. Cement may be introduced into the adobe mixture to stabilize and strengthen the composite of mud and organic matter. The heaviest wooden beams are called Vigas attach to the wall bond beam. Atop the vigas smaller members called latias are laid and upon those brush is then laid. Finally, atop that, the adobe layer is applied.
Raising a traditional adobe roof.
To construct a flat adobe roof, beams of wood or metal should be assembled and span the extent of the building. The ends of the beams are attach to the tops of the walls. Taking into account the material from which the beams and walls are made, choosing the attachments may prove difficult. A combination of the bricks and adobe mortar that are laid across the beams creates an even load-bearing pressure that can last for many years depending on attrition.
Once the vigas, satias and brush are laid, adobe bricks are placed. An adobe roof is often laid with bricks slightly larger in width to ensure a greater expanse is covered when placing the bricks onto the roof. Following each individual brick should be a layer of adobe mortar, recommended to be at least an inch thick to make certain there is ample strength between the brick’s edges and also to provide a relative moisture barrier during rain. 
Attributes.
Depending on the materials, adobe roofs can be inherently fire-proof. The construction of a chimney can greatly influence the construction of the roof supports, creating an extra need for care in choosing the materials. The builders can make an adobe chimney by stacking simple adobe bricks in a similar fashion as the surrounding walls.
Adobe around the world.
The largest structure ever made from adobe (bricks) was the Bam Citadel, which suffered serious damage (up to 80%) by an earthquake on 26 December 2003. Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks, the "ciudellas" of Chan Chan and Tambo Colorado, both in Peru (in South America).

</doc>
<doc id="683" url="http://en.wikipedia.org/wiki?curid=683" title="Adventure">
Adventure

An adventure is an exciting or unusual experience. It may also be a bold, usually risky undertaking, with an uncertain outcome. Adventures may be activities with some potential for physical danger such as exploring, skydiving, mountain climbing, river rafting or participating in extreme sports. The term also broadly refers to any enterprise that is potentially fraught with physical, financial or psychological risk, such as a business venture, or other major life undertakings.
Motivation.
Adventurous experiences create psychological arousal, which can be interpreted as negative (e.g. fear) or positive (e.g. flow), and which can be detrimental as stated by the Yerkes-Dodson law. For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his "La Condition Humaine" (1933), "If a man is not ready to risk his life, where is his dignity?". Similarly, Helen Keller stated that "Life is either a daring adventure or nothing."
Outdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism. Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneers – the British adventurer Jason Lewis, for example, uses adventures to draw global sustainability lessons from living within finite environmental constraints on expeditions to share with schoolchildren. Adventure education intentionally uses challenging experiences for learning.
Adventure in mythology.
Some of the oldest and most widespread stories in the world are stories of adventure such as Homer's "The Odyssey". Mythologist Joseph Campbell discussed his notion of the monomyth in his book, "The Hero with a Thousand Faces". Campbell proposed that the heroic mythological stories from culture to culture followed a similar underlying pattern, starting with the "call to adventure", followed by a hazardous journey, and eventual triumph. 
The knight errant was the form the "adventure seeker" character took in the late Middle Ages.
The adventure novel exhibits these "protagonist on adventurous journey" characteristics as do many popular feature films, such as "Star Wars" and "The Raiders of the Lost Ark".

</doc>
<doc id="689" url="http://en.wikipedia.org/wiki?curid=689" title="Asia">
Asia

Asia ( or ) is the Earth's largest and most populous continent, located primarily in the eastern and northern hemispheres. It covers 8.7% of the Earth's total surface area and comprises 30% of its land area. With approximately 4.3 billion people, it hosts 60% of the world's current human population. Like most of the world, Asia has a high growth rate in the modern era. For instance, during the 20th century, Asia's population nearly quadrupled, as did the world population.
The boundaries of Asia are culturally determined, as there is no clear geographical separation between it and Europe, which together form one continuous landmass called Eurasia. The most commonly accepted boundaries place Asia to the east of the Suez Canal, the Ural River, and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean.
Given its size and diversity, the concept of Asia – a name dating back to classical antiquity - may actually have more to do with human geography than physical geography. Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems.
Definition and boundaries.
Greek three-continent system.
The border between Asia and Europe has historically been determined by Europeans only.
The original distinction between the two was made by the ancient Greeks. They used the Aegean Sea, the Dardanelles, the Sea of Marmara, the Bosporus, the Black Sea, the Kerch Strait, and the Sea of Azov as the border between Asia and Europe. The Nile was often used as the border between Asia and Africa (then called Libya), although some Greek geographers suggested the Red Sea would form a better boundary. Darius' canal between the Nile and the Red Sea introduced considerable variation in opinion. Under the Roman Empire, the Don River emptying into the Black Sea was the western border of Asia. It was the northernmost navigable point of the European shore. In the 15th century the Red Sea became established as the boundary between Africa and Asia, replacing the Nile.
Asia–Europe boundary.
The Don River became unsatisfactory to northern Europeans when Peter the Great, king of the Tsardom of Russia, defeating rival claims of Sweden and the Ottoman Empire to the eastern lands, and armed resistance by the tribes of Siberia, synthesized a new Russian Empire extending to the Ural Mountains and beyond, founded in 1721. The major geographical theorist of the empire was actually a former Swedish prisoner-of-war, taken at the Battle of Poltava in 1709 and assigned to Tobolsk, where he associated with Peter's Siberian official, Vasily Tatishchev, and was allowed freedom to conduct geographical and anthropological studies in preparation for a future book.
In Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Urals as the border of Asia. The Russians were enthusiastic about the concept, which allowed them to keep their European identity in geography. Tatishchev announced that he had proposed the idea to von Strahlenberg. The latter had suggested the Emba River as the lower boundary. Over the next century various proposals were made until the Ural River prevailed in the mid-19th century. The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects. In the maps of the period, Transcaucasia was counted as Asian. The incorporation of most of that region into the Soviet Union tended to push views of the border to the south. Asian cultures had no say in this system of determining the imaginary boundaries separating them from Europe.
Asia–Oceania boundary.
The border between Asia and the loosely defined region of Oceania is usually placed somewhere in the Malay Archipelago. The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception. The chief factor in determining which islands of the Malay Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European). Lewis and Wigen assert, "The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process."
Ongoing definition.
Geographical Asia is a cultural artifact of European conceptions of the world being imposed onto other cultures, an imprecise concept causing endemic contention about what it means. Asia is larger and more culturally diverse than Europe. It does not exactly correspond to the cultural borders of its various types of constituents.
From the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no or is no substantial physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia". Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass – or of Afro-Eurasia; geologically, Asia, Europe and Africa make up a single continuous landmass (except for the Suez Canal) and share a common continental shelf. Almost all of Europe and the better part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.
Etymology.
The English word, "Asia," was originally a concept of Greek civilization. The place name, "Asia", in various forms in a large number of modern languages is of unknown ultimate provenience. Its etymology and language of origin are uncertain. It appears to be one of the most ancient of recorded names. A number of theories have been published. English Asia can be traced through the formation of English literature to Latin literature, where it has the same form, Asia. Whether all uses and all forms of the name derive also from the Latin of the Roman Empire is much less certain.
Classical antiquity.
Latin Asia and Greek Ἀσία appear to be the same word. Roman authors translated Ἀσία as Asia. The Romans named a province Asia (Roman province), which roughly corresponds with modern-day central-western Turkey. There was an Asia Minor and an Asia Major located in modern-day Iraq. As the earliest evidence of the name is Greek, it is likely circumstantially that Asia came from Ἀσία, but ancient transitions, due to the lack of literary contexts, are difficult to catch in the act. The most likely vehicles were the ancient geographers and historians, such as Herodotus, who were all Greek. Roman civilization Hellenized extensively. Ancient Greek certainly evidences early and rich uses of the name.
The first continental use of Asia is attributed to Herodotus (about 440 BC), not because he innovated it, but because his "Histories" are the earliest surviving prose to describe it in any detail. He defines it carefully, mentioning the previous geographers whom he had read, but whose works are now missing. By it he means Anatolia and the Persian Empire, in contrast to Greece and Egypt. Herodotus comments that he is puzzled as to why three women's names were "given to a tract which is in reality one" (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asies, son of Cotys, who passed the name on to a tribe at Sardis. In Greek mythology, "Asia" ("Ἀσία") or "Asie" ("Ἀσίη") was the name of a "Nymph or Titan goddess of Lydia."
Herodotus' geographical puzzlement was perhaps only a form of disagreement, as, having read the earlier Greek poetry along with everyone else literate, he would have known perfectly well why places received female names. Athens, Mycenae, Thebes and many other locations in fact had them. In ancient Greek religion, places were under the care of female divinities, parallel to guardian angels. The poets detailed their doings and generations in allegoric language salted with entertaining stories, which subsequently playwrights transformed into classical Greek drama and became "Greek mythology."
For example, Hesiod mentions the daughters of Tethys and Ocean, among whom are a "holy company", "who with the Lord Apollo and the Rivers have youths in their keeping." Many of these are geographic: Doris, Rhodea, Europa, Asia. Hesiod explains:"For there are three-thousand neat-ankled daughters of Ocean who are dispersed far and wide, and in every place alike serve the earth and the deep waters." The Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians (the tribe that replaced the Luvians in Lydia) in the Trojan War named Asios (an adjective meaning "Asian"); and also a marsh or lowland containing a marsh in Lydia as ασιος.
Bronze Age.
Before Greek poetry, the Aegean Sea area was in a Greek Dark Age, at the beginning of which syllabic writing was lost and alphabetic writing had not begun. Prior to then in the Bronze Age the records of the Assyrian Empire, the Hittite Empire and the various Mycenaean states of Greece mention a region undoubtedly Asia, certainly in Anatolia, including if not identical to Lydia. These records are administrative and do not include poetry.
The Mycenaean states were destroyed about 1200 BC by unknown agents although one school of thought assigns the Dorian invasion to this time. The burning of the palaces baked clay diurnal administrative records written in a Greek syllabic script called Linear B, deciphered by a number of interested parties, most notably by a young World War II cryptographer, Michael Ventris, subsequently assisted by the scholar, John Chadwick. A major cache discovered by Carl Blegen at the site of ancient Pylos included hundreds of male and female names formed by different methods.
Some of these are of women held in servitude (as study of the society implied by the content reveals). They were used in trades, such as cloth-making, and usually came with children. The epithet, lawiaiai, "captives," associated with some of them identifies their origin. Some are ethnic names. One in particular, aswiai, identifies "women of Asia." Perhaps they were captured in Asia, but some others, Milatiai, appear to have been of Miletus, a Greek colony, which would not have been raided for slaves by Greeks. Chadwick suggests that the names record the locations where these foreign women were purchased. The name is also in the singular, Aswia, which refers both to the name of a country and to a female of it. There is a masculine form, aswios. This Aswia appears to have been a remnant of a region known to the Hittites as Assuwa, centered on Lydia, or "Roman Asia." This name, "Assuwa", has been suggested as the origin for the name of the continent "Asia". The Assuwa league was a confederation of states in western Anatolia, defeated by the Hittites under Tudhaliya I around 1400 BC.
Alternatively, the etymology of the term may be from the Akkadian word "", which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word "asa" meaning east. This may be contrasted to a similar etymology proposed for "Europe", as being from Akkadian "erēbu(m)" 'to enter' or 'set' (of the sun).
T.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from "asu", meaning 'east' in Assyrian ("ereb" for "Europe" meaning 'west'). The ideas of "Occidental" (form Latin "Occidens" 'setting') and "Oriental" (from Latin "Oriens" for 'rising') are also European invention, synonymous with "Western" and "Eastern". Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two outspoken Japanese figures on the subject.
History.
The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.
The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Huanghe shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.
The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.
The center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
The Islamic Caliphate took over the Middle East and Central Asia during the Muslim conquests of the 7th century. The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Before the Mongol invasion, Song China reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.
The Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.
The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Middle East, North Africa and the Balkans from the 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing Dynasty. In the 16th century, the Islamic Mughal Empire controlled much of India.
Geography and climate.
Asia is the largest continent on Earth. It covers 8.8% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at . Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, two of them (Russia and Turkey) having part of their land in Europe.
Asia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature ranges on Earth occur in western sections of Asia. The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer. Southwestern sections of the continent are hot. Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America. The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan. The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.
Climate change.
A survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change. Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years. The Asian countries of Bangladesh, India, Vietnam, Thailand, Pakistan and Sri Lanka were among the 16 countries facing extreme risk from climate change. Some shifts are already occurring. For example, in tropical parts of India with a semi-arid climate, the temperature increased by 0.4 °C between 1901 and 2003.
A 2013 study by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) aimed to find science-based, pro-poor approaches and techniques that would enable Asia's agricultural systems to cope with climate change, while benefitting poor and vulnerable farmers. The study's recommendations ranged from improving the use of climate information in local planning and strengthening weather-based agro-advisory services, to stimulating diversification of rural household incomes and providing incentives to farmers to adopt natural resource conservation measures to enhance forest cover, replenish groundwater and use renewable energy.
Economy.
Asia has the second largest nominal GDP of all continents, after Europe, but the largest when measured in purchasing power parity. As of 2011, the largest economies in Asia are China, Japan, India, South Korea and Indonesia. Based on Global Office Locations 2011, Asia dominated the office locations with 4 of top 5 were in Asia, Hong Kong, Singapore, Tokyo, Seoul and Shanghai. Around 68 percent of international firms have office in Hong Kong.
In the late 1990s and early 2000s, the economies of the PRC and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very high growth nations in Asia include Israel, Malaysia, Indonesia, Bangladesh, Thailand, Vietnam, Mongolia, Uzbekistan, Cyprus and the Philippines, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.
According to economic historian Angus Maddison in his book "The World Economy: A Millennial Perspective", India had the world's largest economy during 0 BCE and 1000 BCE. China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid-19th century. For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). This ended in 2010 when China overtook Japan to become the world's second largest economy.
In the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equaled that of the USA as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.
It is forecasted that India will overtake Japan in terms of nominal GDP by 2020. By 2027, according to Goldman Sachs, China will have the largest economy in the world. Several trade blocs exist, with the most developed being the Association of Southeast Asian Nations.
Asia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver. Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in the China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
According to Citigroup 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth. They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, Philippines, Sri Lanka and Vietnam. Asia has four main financial centers: Tokyo, Hong Kong, Singapore and Shanghai. Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centers. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
In 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires. Last year Asia had toppled Europe.
Citigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's "economic center of gravity" continued moving east. At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.
Tourism.
With growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 international visitors.
Demographics.
East Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report’s analysis of health, education and income data. China, the second highest achiever in the world in terms of HDI improvement since
1970, is the only country on the "Top 10 Movers" list due to income rather than health or education achievements. Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty. Yet it was not among the region’s top performers in improving school enrolment and life expectancy.
<br>Nepal, a South Asian country, emerges as one of the world’s fastest movers since 1970 mainly due to health and education achievements. Its present life expectancy is 25 years longer than in the 1970s. More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.
<br> Japan and South Korea ranked highest among the countries grouped on the HDI (number 11 and 12 in the world, which are in the "very high human development" category), followed by Hong Kong (21) and Singapore (27). Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.
Languages.
Asia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India, and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.
Religions.
Many of the world's major religions have their origins in Asia. Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Christians in the Old Testament, is first found in Mesopotamian mythology, in the "Epic of Gilgamesh". Hindu mythology tells about an Avatar of the God Vishnu in the form of a fish who warned Manu of a terrible flood. In ancient Chinese mythology, Shan Hai Jing, the Chinese ruler Da Yu, had to spend 10 years to control a deluge which swept out most of ancient China and was aided by the goddess Nüwa who literally fixed the broken sky through which huge rains were pouring.
Abrahamic.
The Abrahamic religions of Judaism, Christianity, Islam and Bahá'í Faith originated in West Asia. Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the birthplace and historical homeland of the Hebrew nation which today consists equally of those Israelites who remained in Asia/North Africa and those who returned from diaspora in Europe, North America, and other regions, though sizable communities continue to live abroad.
Christianity is also present throughout Asia. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Asian Russia, Eastern Orthodoxy is the predominant religion. Various Christian denominations have adherents in portions of the Middle East, as well as China and India. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century.
Islam, which originated in Saudi Arabia, is the largest and most widely spread religion in Asia. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan, India, Bangladesh, Iran and Turkey. Mecca, Medina and to a lesser extent Jerusalem are the holiest cities for Islam in all the world. These religious sites attract large numbers of devotees from all over the world, particularly during the Hajj and Umrah seasons. Iran is the largest Shi'a country and Pakistan has the largest Ahmadiyya population.
The Bahá'í Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh. Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Bahá'í activities in many Muslim countries has been severely suppressed by authorities. Lotus Temple is a big Baha'i Temple in India.
Indian and East Asian religions.
Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world. The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.
As of 2012, Hinduism has around 1.1 billion adherents. The faith represents around 25% of Asia's population and is the second largest religion in Asia. However, it is mostly concentrated in South Asia. Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia. Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.
Buddhism has a great following in mainland Southeast Asia and East Asia. Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80%-89%), Japan (36%–96%), Bhutan (75%-84%), Sri Lanka (70%), Laos (60%-67%) and Mongolia (53%-93%). Large Buddhist populations also exist in Singapore (33%-51%), Taiwan (35%–93%), South Korea (23%-50%), Malaysia (19%-21%), Nepal (9%-11%), Vietnam (10%–75%), China (20%–50%), North Korea (1.5%–14%), and small communities in India and Bangladesh. In many Chinese communities, Mahayana Buddhism is easily syncretized with Taoism, thus exact religious statistics is difficult to obtain and may be understated or overstated. The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.
Jainism is found mainly in India and in oversea Indian communities such as the United States and Malaysia.
Sikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia.
Confucianism is found predominantly in Mainland China, South Korea, Taiwan and in overseas Chinese populations.
Taoism is found mainly in Mainland China, Taiwan, Malaysia and Singapore. Taoism is easily syncretized with Mahayana Buddhism for many Chinese, thus exact religious statistics is difficult to obtain and may be understated or overstated.
Modern conflicts.
Some of the events pivotal in the Asia territory related to the relationship with the outside world in the post-Second World War were:
Culture.
Nobel prizes.
The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.
Other Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012). Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely "The Good Earth" (1931) and "The Mother" (1933), as well as the biographies of her parents of their time in China, "The Exile" and "Fighting Angel", all of which earned her the Literature prize in 1938.
Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma(Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Most recently, Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for "his long and non-violent struggle for fundamental human rights in China." He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China.
Sir C. V. Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him".
Amartya Sen, (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.
Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Salam (Pakistan), Arafat (Palestinian Territories) Kim (South Korea), Horta and Belo (Timor Leste).
In 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his PhD in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitute people with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low.
The Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.
See also.
References to articles:
Special topics:
Lists:
External links.
<br>

</doc>
<doc id="690" url="http://en.wikipedia.org/wiki?curid=690" title="Aruba">
Aruba

Aruba ( ; ) is an island in the southern Caribbean Sea, located about west of the Lesser Antilles and north of the coast of Venezuela. It measures long from its northwestern to its southeastern end and across at its widest point. Together with Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands. Collectively, Aruba and the other Dutch islands in the Caribbean are often called the Netherlands Antilles or the Dutch Caribbean.
Aruba is one of the four constituent countries that form the Kingdom of the Netherlands, along with the Netherlands, Curaçao and Sint Maarten. The citizens of these countries all share a single nationality: Dutch. Aruba has no administrative subdivisions, but, for census purposes, is divided into eight regions. Its capital is Oranjestad.
Unlike much of the Caribbean region, Aruba has a dry climate and an arid, cactus-strewn landscape. This climate has helped tourism as visitors to the island can reliably expect warm, sunny weather. It has a land area of and is densely populated, with a total of 102,484 inhabitants at the 2010 Census. It lies outside the hurricane belt.
History.
Aruba's first inhabitants are thought to have been Caquetíos Amerinds from the Arawak tribe, who migrated there from Venezuela to escape attacks by the Caribs. Fragments of the earliest known Indian settlements date back to 1000 AD. As sea currents made canoe travel to other Caribbean islands difficult, Caquetio culture remained more closely associated with that of mainland South America.
Europeans first learned of Aruba following the explorations for Spain by Amerigo Vespucci and Alonso de Ojeda in the summer of 1499. Though Vespucci boasted of discovering the island, Ojeda was likely first, learning of it from natives of nearby islands. Both described Aruba as an "island of giants," remarking on the comparatively large stature of the native Caquetíos compared to Europeans. Gold was not discovered on Aruba for another 300 years. Vespucci returned to Spain with stocks of cotton and brazilwood from the island and described houses built into the ocean. Vespucci and Ojeda's tales spurred interest in Aruba, and Spaniards soon colonized the island.
Because it had low rainfall, Aruba was not considered profitable for the plantation system and the economics of the slave trade.
Aruba was colonized by Spain for over a century. "Simas," the "Cacique" or chief in Aruba, welcomed the first Catholic priests in Aruba, who gave him a wooden cross as a gift. In 1508, the Spanish Crown appointed Alonso de Ojeda as its first Governor of Aruba, as part of "Nueva Andalucía."
Arawaks spoke the "broken Spanish" which their ancestors had learned on Hispaniola
Another governor appointed by Spain was Juan Martínez de Ampiés. A "cédula real" decreed in November 1525 gave Ampíes, factor of Española, the right to repopulate Aruba. In 1528, Ampíes was replaced by a representative of the House of Welser.
The Netherlands have covered the island with their regulations since 1629. Since 1636, Aruba has been under Dutch administration, initially governed by Peter Stuyvesant, later appointed to New Amsterdam (New York City). Stuyvesant was on a special mission in Aruba in November and December 1642. The island was included under the Dutch West India Company (W.I.C.) administration, as "New Netherland and Curaçao," from 1648 to 1664. In 1667 the Dutch administration appointed an Irishman as "Commandeur" in Aruba.
The Dutch took control 135 years after the Spanish, leaving the Arawaks to farm and graze livestock, and used the island as a source of meat for other Dutch possessions in the Caribbean.
In August 1806, General Francisco de Miranda and a group of 200 freedom fighters, traveling to liberate Venezuela from Spain, stayed in Aruba for several weeks.
In 1933, Aruba sent its first petition to the Queen seeking independent status and autonomy.
During World War II, Aruba was one of the main suppliers of refined petroleum products to the Allies. During the war, after the German occupation of the Netherlands, Aruba was made a British protectorate from 1940 to 1942, and a US protectorate from 1942 to 1945.
On 16 February 1942, a German submarine ("U-156") under the command of Werner Hartenstein attacked the island's oil processing refinery, but the mission failed.
In March 1944, Eleanor Roosevelt, the First Lady of the United States, briefly visited American troops stationed in Aruba. In attendance were Curaçao Governor P. Kasteel, and U.S. Rear Admiral T. E. Chandler.
Move towards independence.
In August 1947, Aruba presented its first "Staatsreglement" (constitution), for Aruba's "status aparte" as an autonomous state within the Kingdom of the Netherlands. By 1954, the Charter of the Kingdom of the Netherlands was established, providing a framework for relations between Aruba and the rest of the Kingdom.
In 1972, at a conference in Suriname, Betico Croes (MEP), a politician from Aruba, proposed a "sui-generis" Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname and the Netherlands Antilles, each to have its own nationality. C. Yarzagaray, a parliamentary member representing the AVP political party, proposed a referendum so that the people of Aruba could choose whether they wanted total independence or "Status Aparte" as a full autonomous state under the Crown.
Croes worked in Aruba to inform and prepare the people of Aruba for independence. In 1976, he appointed a committee that chose the national flag and anthem, introducing them as symbols of Aruba's sovereignty and independence. He set 1981 as a target date for independence. In March 1977, the first Referendum for Self Determination was held with the support of the United Nations; 82% of the participants voted for independence.
The Island Government of Aruba assigned the Institute of Social Studies in The Hague to prepare a study for independence; it was titled "Aruba en Onafhankelijkheid, achtergronden, modaliteiten en mogelijkheden; een rapport in eerste aanleg" (Aruba and independence, backgrounds, modalities and opportunities; a preliminary report) (1978). At the conference in The Hague in 1981, Aruba's independence was set for the year 1991.
In March 1983, Aruba reached an official agreement within the Kingdom for its independence, to be developed in a series of steps as the Crown granted increasing autonomy. In August 1985 Aruba drafted a constitution that was unanimously approved. On 1 January 1986, after elections were held for its first parliament, Aruba seceded from the Netherlands Antilles; it officially became a country of the Kingdom of the Netherlands. Full independence was projected in 1996.
After his death in 1986, Croes was proclaimed "Libertador di Aruba". At a convention in The Hague in 1990, at the request of Aruba's Prime Minister, the governments of Aruba, the Netherlands, and the Netherlands Antilles postponed indefinitely its transition to full independence. The article scheduling Aruba's complete independence was rescinded in 1995, although the process could be revived after another referendum.
Geography.
Aruba is a generally flat, riverless island in the Leeward Antilles island arc of the Lesser Antilles in the southern part of the Caribbean. It has white sandy beaches on the western and southern coasts of the island, relatively sheltered from fierce ocean currents. This is where most tourist development has occurred. The northern and eastern coasts, lacking this protection, are considerably more battered by the sea and have been left largely untouched by humans.
The hinterland of the island features some rolling hills, the best known of which are called Hooiberg at and Mount Jamanota, the highest on the island at above sea level. Oranjestad, the capital, is located at .
To the east of Aruba are Bonaire and Curaçao, two island territories which once formed the southwest part of the Netherlands Antilles. This group of islands is sometimes called the ABC islands.
The Natural Bridge was a large, naturally formed limestone bridge on the island's north shore. It was a popular tourist destination until its collapse in 2005.
Cities and towns.
The island, with a population of just over 100,000 inhabitants, does not have major cities.
Fauna.
The island provides a habitat for the endemic Aruba Island Rattlesnake.
Climate.
In the Köppen climate classification, Aruba has a tropical semi-arid climate. Mean monthly temperature in Oranjestad varies little from to , moderated by constant trade winds from the Atlantic Ocean, which comes from north-east. Yearly precipitation barely exceeds in Oranjestad.
Demographics.
The population is estimated to be 80% mixed Black/White/Caribbean Amerindian and 20% other ethnicities.
The Arawak heritage is stronger on Aruba than on most Caribbean islands. Although no full-blooded Aboriginals remain, the features of the islanders clearly indicate their genetic Arawak heritage. Most of the population is descended mostly from Arawak, and to a lesser extent Spanish, Italian, Dutch, and a few French, Portuguese, British, and African ancestors.
Recently, there has been substantial immigration to the island from neighboring American and Caribbean nations, possibly attracted by the higher paid jobs. In 2007, new immigration laws were introduced to help control the growth of the population by restricting foreign workers to a maximum of three years residency on the island.
Demographically, Aruba has felt the impact of its proximity to Venezuela. Many of Aruba's families are descended from Venezuelan immigrants. There is a seasonal increase of Venezuelans living in second homes.
Language.
Language can be seen as an important part of island culture in Aruba. The official languages are Dutch and – since 2003 – Papiamento. Papiamento is the predominant language on Aruba. A creole language spoken on Aruba, Bonaire, and Curaçao, it incorporates words from other languages including Portuguese, West African languages, Dutch, and Spanish. English is known by many; its usage has grown due to tourism. Other common languages spoken based on the size of their community are Portuguese, Chinese, German, Spanish, and French.
In recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of its native language. Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento. The orthography differs per island and even per group of people. Some are more oriented towards Portuguese and use the equivalent spelling (e.g. "y" instead of "j"), where others are more oriented towards Dutch.
The book "The Buccaneers of America", first published in 1678, states through eyewitness account that the Indians on Aruba spoke "Spanish". The oldest government official statement written in Papiamento dates from 1803. Around 12.6% of the population today speaks Spanish.
Aruba has four newspapers published in Papiamento: "Diario", "Bon Dia", "Solo di Pueblo" and "Awe Mainta"; and three in English: "Aruba Daily", "Aruba Today" and "The News". "Amigoe" is the newspaper published in Dutch. Aruba also has 18 radio stations (two AM and 16 FM) and three local television stations (Telearuba, Aruba Broadcast Company and Channel 22).
Regions.
For census purposes, Aruba is divided into eight regions, which have no administrative functions:
Government.
As a constituent country of the Kingdom of the Netherlands, Aruba's politics take place within a framework of a 21-member Parliament and an eight-member Cabinet. The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are elected by the Staten (or "Parlamento") for four-year terms. The Staten is made up of 21 members elected by direct, popular vote to serve a four-year term.
Together with the Netherlands, the countries of Aruba, Curaçao and Sint Maarten form the Kingdom of the Netherlands. As they share the same Dutch citizenship, these four countries still also share the Dutch passport as the Kingdom of the Netherlands passport. As Aruba, Curaçao and Sint Maarten have small populations, the three countries had to limit immigration. To protect their population, they have the right to control the admission and expulsion of people from the Netherlands.
Aruba is designated as a member of the Overseas Countries and Territories (OCT) and is thus officially not a part of the European Union, though Aruba can and does receive support from the European Development Fund.
Politics.
The Aruban legal system is based on the Dutch model. Instead of juries or grand juries, in Aruba, legal jurisdiction lies with the "Gerecht in Eerste Aanleg" (Court of First Instance) on Aruba, the "Gemeenschappelijk Hof van Justitie van Aruba, Curaçao, Sint Maarten en van Bonaire, Sint Eustatius en Saba" (Joint Court of Justice of Aruba, Curaçao, Sint Maarten, and of Bonaire, Sint Eustatius and Saba) and the "Hoge Raad der Nederlanden" (Supreme Court of Justice of the Netherlands). The "Korps Politie Aruba" (Aruba Police Force) is the island's law enforcement agency and operates district precincts in Oranjestad, Noord, San Nicolaas, and Santa Cruz, where it is headquartered.
Deficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well. By 2006, the government's debt had grown to 1.883 billion Aruban florins. Aruba received some development aid from the Dutch government each year through 2009, as part of a deal (signed as "Aruba's Financial Independence") in which the Netherlands gradually reduced its financial help to the island each successive year.
In 2006, the Aruban government changed several tax laws to reduce the deficit. Direct taxes have been converted to indirect taxes as proposed by the IMF. A 3% tax has been introduced on sales and services, while income taxes have been lowered and revenue taxes for business reduced by 20%. The government compensated workers with 3.1% for the effect that the B.B.O. would have on the inflation for 2007.
Education.
Aruba's educational system is patterned after the Dutch system of education.
The Government of Aruba finances the national education system. Private schools, such as the International School of Aruba (ISA), finance their own activities. The percentage of money earmarked for education is higher than the average for the Caribbean/Latin American region.
Arubans have a primary school system, followed by a segmented secondary school program which includes vocational training, basic education, college preparation and advanced placement.
The study of Spanish, English, Dutch and French is offered in secondary school and college, since a high percentage of students continue their studies in Europe.
Higher education is available through the Professional Education program (EPI), the teachers college (IPA) as well as through the University of Aruba (UA) which offers bachelors and masters programs in law, finance and economics and hospitality and tourism management. Since the choice for higher education on the island itself is limited, many students choose study in the Netherlands, or abroad in countries in North America, South America as well as the rest of Europe.
Aruba is also home to two medical schools: Aureus University School of Medicine and Xavier University School of Medicine.
Economy.
Aruba has one of the highest standards of living in the Caribbean region. There is a low unemployment rate.
The GDP per capita for Aruba was estimated to be $21,800 in 2004; among the highest in the Caribbean and the Americas. Its main trading partners are Venezuela, the United States and the Netherlands.
The island's economy has been dominated by five main industries: tourism, gold mining, phosphate mining (The Aruba Phosphaat Maatschappij), aloe export, and petroleum refining (The Lago Oil and Transport Company and the Arend Petroleum Maatschappij Shell Co.). Before the "Status Aparte" (a separate completely autonomous country/state within the Kingdom), oil processing was the dominant industry in Aruba despite expansion of the tourism sector. Today, the influence of the oil processing business is minimal. The size of the agriculture and manufacturing sectors also remains minimal.
The official exchange rate of the Aruban florin is pegged to the US dollar at 1.79 florins to 1 USD. Because of this fact, and due to a large number of American tourists, many businesses operate using US dollars instead of florins, especially in the hotel and resort districts.
Tourism.
About three quarters of the Aruban gross national product is earned through tourism or related activities. Most tourists are from the United States (predominantly from the north-east US), the Netherlands and South-America, mainly Venezuela and Colombia.
As part of the Kingdom of the Netherlands, citizens of (mainland) the Netherlands can travel with relative ease to Aruba and other islands of the Dutch Antilles. No visas are needed for Dutch citizens, only a passport, and although the currency used in Aruba is different (the Netherlands has the Euro), Euros are still not widely accepted but can be easily exchanged at a local bank for Aruban Florins.
For the facilitation of the passengers whose destination is the United States, the United States Department of Homeland Security (DHS), U.S. Customs and Border Protection (CBP) full pre-clearance facility in Aruba has been in effect since 1 February 2001 with the expansion in the Queen Beatrix Airport. United States and Aruba have had the agreement since 1986. It began as a USDA and Customs post. Since 2008, Aruba has been the only island to have this service for private flights.
Military.
In 1999, the U.S. Department of Defense established a Forward Operating Location (FOL) at the airport.
There is also a small Dutch marines base by Savaneta containing approximately 200 Dutch Marines and about 100 AruMil forces.
Culture.
On 18 March, Aruba celebrates its National Day. In 1976, Aruba presented its National Anthem (Aruba Dushi Tera) and Flag.
Aruba has a varied culture. According to the "Bureau Burgelijke Stand en Bevolkingsregister" (BBSB), in 2005 there were ninety-two different nationalities living on the island. Dutch influence can still be seen, as in the celebration of "Sinterklaas" on 5 and 6 December and other national holidays like 30 April, when in Aruba and the rest of the Kingdom of the Netherlands the Queen's birthday or "Dia di La Reina" (Koninginnedag) is celebrated.
Christmas and New Year's Eve are celebrated with the typical music and songs for gaitas for Christmas and the Dande for New Year, and "ayaca", "ponche crema", ham, and other typical foods and drinks. Millions of florins worth of fireworks are burnt at midnight on New Year's Eve. On 25 January, Betico Croes' birthday is celebrated. Dia di San Juan is celebrated on June 24.
Besides Christmas, the religious holy days of the Feast of the Ascension and Good Friday are holidays on the island.
The holiday of Carnaval is also an important one in Aruba, as it is in many Caribbean and Latin American countries, and, like Mardi Gras, that goes on for weeks. Its celebration in Aruba started, around the 1950s, influenced by the inhabitants from Venezuela and the nearby islands (Curaçao, St. Vincent, Trinidad, Barbados, St. Maarten and Anguilla) who came to work for the Oil refinery. Over the years the Carnival Celebration has changed and now starts from the beginning of January till the Tuesday before Ash Wednesday with a large parade on the last Sunday of the festivities (Sunday before Ash Wednesday).
Tourism from the United States has recently increased the visibility of American culture on the island, with such celebrations as Halloween and Thanksgiving Day in November.
Infrastructure.
Aruba's Queen Beatrix International Airport is located near Oranjestad. According to the Aruba Airport Authority, almost 1.7 million travelers used the airport in 2005, 61% of whom were Americans.
Aruba has two ports, Barcadera and Playa, which are located in Oranjestad. The Port of Playa services all the cruise-ship lines, including Royal Caribbean, Carnival Cruise Lines, NCL, Holland America Line, Disney Cruise Line and others. Nearly one million tourists enter this port per year. Aruba Ports Authority, owned and operated by the Aruban government, runs these seaports.
Arubus is a government-owned bus company. Its buses operate from 3:30 am until 12:30 am 365 days a year. Small private vans also provide transportation services in certain areas such Hotel Area, San Nicolaas, Santa Cruz and Noord.
 A street car service runs on rails on the Mainstreet.
Utilities.
Water- en Energiebedrijf (W.E.B.) Aruba NV produces potable industrial water at the world's third largest desalination plant. Average daily consumption in Aruba is about .
Communications.
There are three telecommunications providers: Setar, a government-based company, Mio Wireless and Digicel, both of which are privately owned. Setar is the provider of services such as internet, video conferencing, GSM wireless technology and land lines. Digicel is Setar's competitor in wireless technology using the GSM platform, and Mio Wireless provides wireless technology and services using CDMA. 

</doc>
<doc id="691" url="http://en.wikipedia.org/wiki?curid=691" title="Articles of Confederation">
Articles of Confederation

The Articles of Confederation, formally the Articles of Confederation and Perpetual Union, was a document signed amongst the 13 original colonies that established the United States of America as a confederation of sovereign states and served as its first constitution. Its drafting by a committee appointed by the Second Continental Congress began on July 12, 1776, and an approved version was sent to the states for ratification in late 1777. The formal ratification by all 13 states was completed in early 1781. Even when not yet ratified, the Articles provided domestic and international legitimacy for the Continental Congress to direct the American Revolutionary War, conduct diplomacy with Europe and deal with territorial issues and Native American relations. Nevertheless, the weakness of the government created by the Articles became a matter of concern for key nationalists. On March 4, 1789, general government under the Articles was replaced with the federal government under the U.S. Constitution. The new Constitution provided for a much stronger federal government with a chief executive (the president), courts, and taxing powers.
Background and context.
The political push to increase cooperation among the then-loyal colonies began with the Albany Congress in 1754 and Benjamin Franklin's proposed intercolonial collaboration to help solve mutual local problems themselves; the Articles of Confederation would bear some resemblance to it. Over the next two decades, some of the basic concepts it addressed would strengthen and others would weaken, particularly the degree of deserved loyalty to the crown. With civil disobedience resulting in coercive and intolerable acts, and armed conflict resulting in dissidents being proclaimed rebels and outside the King's protection, any loyalty remaining shifted toward independence and how to achieve it. In 1775, with events outpacing communications, the Second Continental Congress began acting as the provisional government to run the American Revolutionary War and gain the colonies their collective independence.
It was an era of constitution writing—most states were busy at the task—and leaders felt the new nation must have a written constitution, even though other nations did not. During the war, Congress exercised an unprecedented level of political, diplomatic, military and economic authority. It adopted trade restrictions, established and maintained an army, issued fiat money, created a military code and negotiated with foreign governments.
To transform themselves from outlaws into a legitimate nation, the colonists needed international recognition for their cause and foreign allies to support it. In early 1776, Thomas Paine argued in the closing pages of the first edition of "Common Sense" that the “custom of nations” demanded a formal declaration of American independence if any European power were to mediate a peace between the Americans and Great Britain. The monarchies of France and Spain in particular could not be expected to aid those they considered rebels against another legitimate monarch. Foreign courts needed to have American grievances laid before them persuasively in a “manifesto” which could also reassure them that the Americans would be reliable trading partners. Without such a declaration, Paine concluded, “[t]he custom of all courts is against us, and will be so, until, by an independence, we take rank with other nations.”
Beyond improving their existing association, the records of the Second Continental Congress show that the need for a declaration of independence was intimately linked with the demands of international relations. On June 7, 1776, Richard Henry Lee introduced a resolution before the Continental Congress declaring the colonies independent; at the same time he also urged Congress to resolve “to take the most effectual measures for forming foreign Alliances” and to prepare a plan of confederation for the newly independent states. Congress then created three overlapping committees to draft the Declaration, a Model Treaty, and the Articles of Confederation. The Declaration announced the states' entry into the international system; the model treaty was designed to establish amity and commerce with other states; and the Articles of Confederation, which established “a firm league” among the thirteen free and independent states, constituted an international agreement to set up central institutions for the conduct of vital domestic and foreign affairs.
Drafting.
On June 12, 1776, a day after appointing a committee to prepare a draft of the Declaration of Independence, the Second Continental Congress resolved to appoint a committee of 13 to prepare a draft of a constitution for a union of the states. The committee met repeatedly, and chairman John Dickinson presented their results to the Congress on July 12, 1776. There were long debates on such issues as sovereignty, the exact powers to be given the confederate government, whether to have a judiciary, and voting procedures. The final draft of the Articles was prepared in the summer of 1777 and the Second Continental Congress approved them for ratification by the individual states on November 15, 1777, after a year of debate.
In practice, the Articles were in use beginning in 1777; the final draft of the Articles served as the de facto system of government used by the Congress ("the United States in Congress assembled") until it became de jure by final ratification on March 1, 1781; at which point Congress became the Congress of the Confederation. Under the Articles, the states retained sovereignty over all governmental functions not specifically relinquished to the national government. The individual articles set the rules for current and future operations of the United States government. It was made capable of making war and peace, negotiating diplomatic and commercial agreements with foreign countries, and deciding disputes between the states, including their additional and contested western territories. Article XIII stipulated that "their provisions shall be inviolably observed by every state" and "the Union shall be perpetual".
John Dickinson's and Benjamin Franklin's handwritten drafts of the Articles of Confederation are housed at the National Archives in Washington, DC.
Operation.
The Articles were created by delegates from the states in the Second Continental Congress out of a need to have "a plan of confederacy for securing the freedom, sovereignty, and independence of the United States." After the war, nationalists, especially those who had been active in the Continental Army, complained that the Articles were too weak for an effective government. There was no president, no executive agencies, no judiciary and no tax base. The absence of a tax base meant that there was no way to pay off state and national debts from the war years except by requesting money from the states, which seldom arrived.
In 1788, with the approval of Congress, the Articles were replaced by the United States Constitution and the new government began operations in 1789.
Ratification.
Congress began to move for ratification of the Articles of Confederation in 1777:
"Permit us, then, earnestly to recommend these articles to the immediate and dispassionate attention of the legislatures of the respective states. Let them be candidly reviewed under a sense of the difficulty of combining in one system the various sentiments and interests of a continent divided into so many sovereign and independent communities, under a conviction of the absolute necessity of uniting all our councils and all our strength, to maintain and defend our common liberties..."
The document could not become officially effective until it was ratified by all 13 states. The first state to ratify was Virginia on December 16, 1777; the thirteenth state to ratify was Maryland on February 2, 1781. A ceremonial confirmation of this thirteenth, final ratification took place in the Congress on March 1, 1781 at high noon.
Dates of ratification are:
The ratification process dragged on for several years, stalled by the refusal of some states to rescind their claims to land in the West. Maryland was the last holdout; it refused to go along until Virginia and New York agreed to cede their claims in the Ohio River Valley. It took a little over three years for all states to ratify.
The Articles provided for a blanket acceptance of Province of Quebec (referred to as "Canada" in the Articles) into the United States if it chose to do so. It did not, and the subsequent Constitution carried no such special provision of admission.
Article summaries.
Even though the Articles of Confederation and the Constitution were established by many of the same people, the two documents are very different. Stylistically, the Articles are more wordy, less straightforward and less quotable than the Constitution. Functionally, they lay out very different forms of government. The original five-page Articles contained a preamble, 13 articles, a conclusion, and a signatory section.
The preamble states that the signatory states "agree to certain articles of Confederation and perpetual Union" between the 13 states.
The following list contains short summaries of each of the 13 articles.
While still at war with Britain, the Founding Fathers were divided between those seeking a powerful, centralized national government (the "federalists"), and those seeking a loosely structured one (the "anti federalists"). Jealously guarding their new independence, members of the Continental Congress arrived at a compromise solution dividing sovereignty between the states and the central government, with a unicameral legislature that protected the liberty of the individual states. While calling on Congress to regulate military and monetary affairs, for example, the Articles of Confederation provided no mechanism with which to compel the States to comply with requests for either troops or revenue. At times, this left the military without adequate funding, supplies or even food.
The end of the Revolutionary War.
The Treaty of Paris (1783), which ended hostilities with Great Britain, languished in Congress for months because several state representatives failed to attend sessions of the national legislature to ratify it. Yet Congress had no power to enforce attendance. In September 1783, George Washington complained that Congress was paralyzed. Many revolutionaries had gone to their respective home countries after the war, and local government and self-rule seemed quite satisfactory.
Function.
The Army.
The Articles supported the Congressional direction of the Continental Army, and allowed the states to present a unified front when dealing with the European powers. As a tool to build a centralized war-making government, they were largely a failure: Historian Bruce Chadwick wrote:
The Continental Congress, before the Articles were approved, had promised soldiers a pension of half pay for life. However Congress had no power to compel the states to fund this obligation, and as the war wound down after the victory at Yorktown the sense of urgency to support the military was no longer a factor. No progress was made in Congress during the winter of 1783–84. General Henry Knox, who would later become the first Secretary of War under the Constitution, blamed the weaknesses of the Articles for the inability of the government to fund the army. The army had long been supportive of a strong union. Knox wrote:
As Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, "As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution."
Once the war had been won, the Continental Army was largely disbanded. A very small national force was maintained to man the frontier forts and to protect against Native American attacks. Meanwhile, each of the states had an army (or militia), and 11 of them had Navies. The wartime promises of bounties and land grants to be paid for service were not being met. In 1783, George Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced Congress to leave Philadelphia temporarily.
The Congress from time to time during the Revolutionary War requisitioned troops from the states. Any contributions were voluntary, and in the debates of 1788 the Federalists (who supported the proposed new Constitution) claimed that state politicians acted unilaterally, and contributed when the Continental army protected their state's interests. The Anti-Federalists claimed that state politicians understood their duty to the Union and contributed to advance its needs. Dougherty (2009) concludes that generally the States' behavior validated the Federalist analysis. This helps explain why the Articles of Confederation needed reforms.
Foreign policy.
Even after peace had been achieved in 1783, the weakness of the Confederation government frustrated the ability of the government to conduct foreign policy. In 1789, Thomas Jefferson, concerned over the failure to fund an American naval force to confront the Barbary pirates, wrote to James Monroe, "It will be said there is no money in the treasury. There never will be money in the treasury till the Confederacy shows its teeth. The states must see the rod.”
Furthermore, the Jay–Gardoqui Treaty with Spain in 1789 also showed weakness in foreign policy. In this treaty — which was never ratified due to its immense unpopularity — the United States was to give up rights to use the Mississippi River for 25 years, which would have economically strangled the settlers west of the Appalachian Mountains. Finally, due to the Confederation's military weakness, it could not compel the British army to leave frontier forts which were on American soil — forts which, in 1783, the British promised to leave, but which they delayed leaving pending U.S. implementation of other provisions such as ending action against Loyalists and allowing them to seek compensation. This incomplete British implementation of the Treaty of Paris (1783) was superseded by the implementation of Jay's Treaty in 1795 under the new U.S. Constitution.
Taxation and Commerce.
Under the Articles of Confederation, the central government's power was kept quite limited. The Confederation Congress could make decisions, but lacked enforcement powers. Implementation of most decisions, including modifications to the Articles, required unanimous approval of all thirteen state legislatures.
Congress was denied any powers of taxation: it could only request money from the states. The states often failed to meet these requests in full, leaving both Congress and the Continental Army chronically short of money. As more money was printed by Congress, the continental dollars depreciated. In 1779, George Washington wrote to John Jay, who was serving as the president of the Continental Congress, "that a wagon load of money will scarcely purchase a wagon load of provisions." Mr. Jay and the Congress responded in May by requesting $45 million from the States. In an appeal to the States to comply, Jay wrote that the taxes were "the price of liberty, the peace, and the safety of yourselves and posterity." He argued that Americans should avoid having it said "that America had no sooner become independent than she became insolvent" or that "her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith." The States did not respond with any of the money requested from them.
Congress had also been denied the power to regulate either foreign trade or interstate commerce and, as a result, all of the States maintained control over their own trade policies. The states and the Confederation Congress both incurred large debts during the Revolutionary War, and how to repay those debts became a major issue of debate following the War. Some States paid off their war debts and others did not. Federal assumption of the states' war debts became a major issue in the deliberations of the Constitutional Convention.
Accomplishments of the Confederation.
Nevertheless, the Confederation Congress did take two actions with long lasting impact. The Land Ordinance of 1785 and Northwest Ordinance created territorial government, set up protocols for the admission of new states, the division of land into useful units, and set aside land in each township for public use. This system represented a sharp break from imperial colonization, as in Europe, and provided the basis for the rest of American continental expansion through the 19th Century.
The Land Ordinance of 1785 established both the general practices of land surveying in the west and northwest and the land ownership provisions used throughout the later westward expansion beyond the Mississippi River. Frontier lands were surveyed into the now-familiar squares of land called the township (36 square miles), the section (one square mile), and the quarter section (160 acres). This system was carried forward to most of the States west of the Mississippi (excluding areas of Texas and California that had already been surveyed and divided up by the Spanish Empire). Then, when the Homestead Act was enacted in 1867, the quarter section became the basic unit of land that was granted to new settler-farmers.
The Northwest Ordinance of 1787 noted the agreement of the original states to give up northwestern land claims, organized the Northwest Territory and thus cleared the way for the entry of five new states, and part of a sixth to the Union. To be specific, Massachusetts, Connecticut, New York, Pennsylvania, and Virginia gave up all of their claims to land north of the Ohio River and west of the (present) western border of Pennsylvania. Over several decades a number of new states were formed from this land: Ohio, Indiana, Illinois, Michigan, and Wisconsin, and the part of Minnesota east of the Mississippi River. The Northwest Ordinance of 1787 also made great advances in the abolition of slavery. New states admitted to the union in said territory would never be slave states.
The United States of America under the Articles.
The peace treaty left the United States independent and at peace but with an unsettled governmental structure. The Articles envisioned a permanent confederation, but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced. There was no president and no national court. Although historians generally agree that the Articles were too weak to hold the fast-growing nation together, they do give credit to the settlement of the western issue, as the states voluntarily turned over their lands to national control.
By 1783, with the end of the British blockade, the new nation was regaining its prosperity. However, trade opportunities were restricted by the mercantilism of the British and French empires. The ports of the British West Indies were closed to all staple products which were not carried in British ships. France and Spain established similar policies. Simultaneously, new manufacturers faced sharp competition from British products which were suddenly available again. Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution. The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation. In 1786–87, Shays' Rebellion, an uprising of farmers in western Massachusetts against the state court system, threatened the stability of state government.
The Continental Congress printed paper money which was so depreciated that it ceased to pass as currency, spawning the expression "not worth a continental". Congress could not levy taxes and could only make requisitions upon the States. Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone.
When Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce. Demands were made for favors and there was no assurance that individual states would agree to a treaty. Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain. Congress had already requested and failed to get power over navigation laws. Meanwhile, each State acted individually against Great Britain to little effect. When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports.
By 1787 Congress was unable to protect manufacturing and shipping. State legislatures were unable or unwilling to resist attacks upon private contracts and public credit. Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population.
The idea of a convention to revise the Articles of Confederation grew in favor. Alexander Hamilton realized while serving as Washington's top aide that a strong central government was necessary to avoid foreign intervention and allay the frustrations due to an ineffectual Congress. Hamilton led a group of like-minded nationalists, won Washington's endorsement, and convened the Annapolis Convention in 1786 to petition Congress to call a constitutional convention to meet in Philadelphia to remedy the long-term crisis.
Signatures.
The Second Continental Congress approved the Articles for distribution to the states on November 15, 1777. A copy was made for each state and one was kept by the Congress. On November 28, the copies sent to the states for ratification were unsigned, and the cover letter, dated November 17, had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress.
The "Articles", however, were unsigned, and the date was blank. Congress began the signing process by examining their copy of the "Articles" on June 27, 1778. They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification.
On July 9, 1778, the prepared copy was ready. They dated it, and began to sign. They also requested each of the remaining states to notify its delegation when ratification was completed. On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified. New Jersey, Delaware and Maryland could not, since their states had not ratified. North Carolina and Georgia also didn't sign that day, since their delegations were absent.
After the first signing, some delegates signed at the next meeting they attended. For example, John Wentworth of New Hampshire added his name on August 8. John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the "Articles" on July 21, 1778.
The other states had to wait until they ratified the "Articles" and notified their Congressional delegation. Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779. Maryland refused to ratify the "Articles" until every state had ceded its western land claims.
On February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis. As the last piece of business during the afternoon Session, "among engrossed Bills" was "signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation" and perpetual union among the states. The Senate then adjourned "to the first Monday in August next." The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12. The confirmation signing of the "Articles" by the two Maryland delegates took place in Philadelphia at noon time on March 1, 1781 and was celebrated in the afternoon. With these events, the Articles were entered into force and the United States of America came into being as a sovereign federal state.
Congress had debated the "Articles" for over a year and a half, and the ratification process had taken nearly three and a half years. Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived. The "Articles of Confederation and Perpetual Union" were signed by a group of men who were never present in the Congress at the same time.
Signers.
The signers and the states they represented were:
Connecticut
Delaware
Georgia
Maryland
Massachusetts Bay
New Hampshire
New Jersey
New York
North Carolina
Pennsylvania
Rhode Island and Providence Plantations
South Carolina
Virginia
Roger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
Robert Morris (Pennsylvania) signed three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
John Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).
Presidents of the Congress.
The following list is of those who led the Congress of the Confederation under the "Articles of Confederation" as the Presidents of the United States in Congress Assembled. Under the Articles, the president was the presiding officer of Congress, chaired the Committee of the States when Congress was in recess, and performed other administrative functions. He was not, however, an executive in the way the successor President of the United States is a chief executive, since all of the functions he executed were under the direct control of Congress.
"For a full list of Presidents of the Congress Assembled and Presidents under the two Continental Congresses before the Articles, see President of the Continental Congress."
Revision and replacement.
On January 21, 1786, the Virginia Legislature, following James Madison's recommendation, invited all the states to send delegates to Annapolis, Maryland to discuss ways to reduce interstate conflict. At what came to be known as the Annapolis Convention, the few state delegates in attendance endorsed a motion that called for all states to meet in Philadelphia in May 1787 to discuss ways to improve the Articles of Confederation in a "Grand Convention." Although the states' representatives to the Constitutional Convention in Philadelphia were only authorized to amend the Articles, the representatives held secret, closed-door sessions and wrote a new constitution. The new Constitution gave much more power to the central government, but characterization of the result is disputed. The general goal of the authors was to get close to a republic as defined by the philosophers of the Age of Enlightenment, while trying to address the many difficulties of the interstate relationships. Historian Forrest McDonald, using the ideas of James Madison from "Federalist 39", describes the change this way:
In May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation. Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries. Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus. The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts.
Historian Ralph Ketcham comments on the opinions of Patrick Henry, George Mason, and other antifederalists who were not so eager to give up the local autonomy won by the revolution:
Historians have given many reasons for the perceived need to replace the articles in 1787. Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines.
Rakove (1988) identifies several factors that explain the collapse of the Confederation. The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power. It could not collect customs after the war because tariffs were vetoed by Rhode Island. Rakove concludes that their failure to implement national measures "stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace." The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy. Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power.
When the war ended in 1783, certain special interests had incentives to create a new "merchant state," much like the British state people had rebelled against. In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims. Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government.
Legitimacy of closing down.
Political scientist David C. Hendrickson writes that two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that "the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges."
According to Article XIII of the Confederation, any alteration had to be approved unanimously: 
[T]he Articles of this Confederation shall be inviolably observed by every State, and the Union shall be perpetual; nor shall any alteration at any time hereafter be made in any of them; unless such alteration be agreed to in a Congress of the United States, and be afterwards confirmed by the legislatures of every State.
On the other hand, Article VII of the proposed Constitution stated that it would become effective after ratification by a mere nine states, without unanimity:
The Ratification of the Conventions of nine States, shall be sufficient for the Establishment of this Constitution between the States so ratifying the Same.
The apparent tension between these two provisions was addressed at the time, and remains a topic of scholarly discussion. In 1788, James Madison remarked (in "Federalist No. 40") that the issue had become moot: "As this objection...has been in a manner waived by those who have criticised the powers of the convention, I dismiss it without further observation." Nevertheless, it is an interesting historical and legal question whether opponents of the Constitution could have plausibly attacked the Constitution on that ground. At the time, there were state legislators who argued that the Constitution was not an alteration of the Articles of Confederation, but rather would be a complete replacement so the unanimity rule did not apply. Moreover, the Confederation had proven woefully inadequate and therefore was supposedly no longer binding.
Modern scholars such as Francisco Forrest Martin agree that the Articles of Confederation had lost its binding force because many states had violated it, and thus "other states-parties did not have to comply with the Articles' unanimous consent rule". In contrast, law professor Akhil Amar suggests that there may not have really been any conflict between the Articles of Confederation and the Constitution on this point; Article VI of the Confederation specifically allowed side deals among states, and the Constitution could be viewed as a side deal until all states ratified it.
Final months.
According to their terms for modification (Article XIII), the Articles would still have been in effect until 1790, the year in which the last of the 13 states, Rhode Island, ratified the new Constitution. The Congress under the Articles continued to convene with a quorum until October 1788, overseeing the adoption of the new Constitution by the states, setting elections and attending to other business.
By July 1788, 11 of the 13 states had ratified the new Constitution.
On Saturday, September 13, 1788, the Confederation Congress voted the resolve to implement the new Constitution, and on Monday, September 15 published an announcement that the new Constitution had been ratified by the necessary nine states, set the first Wednesday in February 1789 for the presidential electors to meet and select a new president, and set the first Wednesday of March 1789 as the day the new government would take over and the government under the Articles of Confederation would come to an end.
On that same September 13, it determined that New York would remain the national capital.

</doc>
<doc id="694" url="http://en.wikipedia.org/wiki?curid=694" title="Asia Minor (disambiguation)">
Asia Minor (disambiguation)

Asia Minor is an alternative name for Anatolia, the westernmost protrusion of Asia, comprising the majority of the Republic of Turkey. It may also refer to:

</doc>
<doc id="696" url="http://en.wikipedia.org/wiki?curid=696" title="Aa River">
Aa River

Aa is the name of a large number of small European rivers. Aa originated from an Indo-European word meaning water, and it can be seen in the German "ach" or "aach" or the North Germanic "a" or "aa". 

</doc>
<doc id="698" url="http://en.wikipedia.org/wiki?curid=698" title="Atlantic Ocean">
Atlantic Ocean

The Atlantic Ocean is the world's second largest ocean, following the Pacific Ocean. With a total area of about , it covers approximately 20 percent of the Earth's surface and about 29 percent of its water surface area. The first part of its name refers to Atlas of Greek mythology, making the Atlantic the "Sea of Atlas".
The oldest known mention of "Atlantic" is in "The Histories" of Herodotus around 450 BC (Hdt. 1.202.4): "Atlantis thalassa" (Greek: Ἀτλαντὶς θάλασσα; English: Sea of Atlas). The term Ethiopic Ocean, derived from Ethiopia, was applied to the southern Atlantic as late as the mid-19th century. Before Europeans discovered other oceans, their term "ocean" was synonymous with the waters beyond the Strait of Gibraltar that are now known as the Atlantic. The early Greeks believed this ocean to be a gigantic river encircling the world.
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Eurasia and Africa to the east, and the Americas to the west. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The equator subdivides it into the North Atlantic Ocean and South Atlantic Ocean.
Geography.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Sea–one of its marginal seas–and, in turn, the Black Sea, both of which also touch upon Asia) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. Some authorities show it extending south to Antarctica, while others show it bounded at the 60° parallel by the Southern Ocean.
In the southwest, the Drake Passage connects it to the Pacific Ocean. The man-made Panama Canal links the Atlantic and Pacific. Besides those mentioned, other large bodies of water adjacent to the Atlantic are the Caribbean Sea, the Gulf of Mexico, Hudson Bay, the Arctic Ocean, the Mediterranean Sea, the North Sea, the Baltic Sea, and the Celtic Sea.
Covering approximately 22% of Earth's surface, the Atlantic is second in size to the Pacific. With its adjacent seas, it occupies an area of about ; without them, it has an area of . The land that drains into the Atlantic covers four times that of either the Pacific or Indian oceans. The volume of the Atlantic with its adjacent seas is 354,700,000 cubic kilometers (85,100,000 cu mi) and without them 323,600,000 cubic kilometres (77,640,000 cu mi).
The average depth of the Atlantic with its adjacent seas, is ; without them it is . The greatest depth, Milwaukee Deep with , is in the Puerto Rico Trench. The Atlantic's width varies from between Brazil and Sierra Leone to over in the south.
Cultural significance.
The Atlantic Ocean was named by the ancient Greeks after either Atlas the Titan or the Atlas Mountains named for him; both involve the concept of holding up the sky. Transatlantic travel played a major role in the expansion of Western civilization into the Americas. It is the Atlantic that separates the "Old World" from the "New World". In modern times, some idioms refer to the ocean in a humorously diminutive way as the Pond, describing both the geographical and cultural divide between North America and Europe, in particular between the English-speaking nations of both continents. Many British people refer to the United States and Canada as "across the pond", and vice versa.
The "Black Atlantic" refers to the role of this ocean in shaping black people's history, especially through the Atlantic slave trade. Irish migration to the US is meant when the term "The Green Atlantic" is used. The term "Red Atlantic" has been used in reference to the Marxian concept of an Atlantic working class, as well as to the Atlantic experience of indigenous Americans.
Ocean floor.
The principal feature of the bathymetry (bottom topography) is a submarine mountain range called the Mid-Atlantic Ridge. It extends from Iceland in the north to approximately 58° South latitude, reaching a maximum width of about . A great rift valley also extends along the ridge over most of its length. The depth of water at the apex of the ridge is less than in most places, while the bottom of the ridge is three times as deep. Several peaks rise above the water and form islands. The South Atlantic Ocean has an additional submarine ridge, the Walvis Ridge.
The Mid-Atlantic Ridge separates the Atlantic Ocean into two large troughs with depths from . Transverse ridges running between the continents and the Mid-Atlantic Ridge divide the ocean floor into numerous basins. Some of the larger basins are the Blake, Guiana, North American, Cape Verde, and Canaries basins in the North Atlantic. The largest South Atlantic basins are the Angola, Cape, Argentina, and Brazil basins.
The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
Ocean floor trenches and seamounts:
Ocean sediments are composed of:
Water characteristics.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 – 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below to over . Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by .
The Atlantic Ocean consists of four major water masses. The North and South Atlantic central waters make up the surface. The sub-Antarctic intermediate water extends to depths of . The North Atlantic Deep Water reaches depths of as much as . The Antarctic Bottom Water occupies ocean basins at depths greater than 4,000 metres.
Within the North Atlantic, ocean currents isolate the Sargasso Sea, a large elongated body of water, with above average salinity. The Sargasso Sea contains large amounts of seaweed and is also the spawning ground for both the European eel and the American eel.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation occurs.
Climate.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift, for example, warms the atmosphere of the British Isles and north-western Europe and influences weather and climate as far south as the northern Mediterranean. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas. Hurricanes develop in the southern part of the North Atlantic Ocean. More local particular weather examples could be found in examples such as the; Azores High, Benguela Current, Nor'easter.
History.
The Atlantic Ocean appears to be the second youngest of the five oceans. It did not exist prior to 130 million years ago, when the continents that formed from the breakup of the ancestral super continent Pangaea were drifting apart. The Atlantic has been extensively explored since the earliest settlements along its shores.
The Vikings, the Portuguese, and the Spaniards were the most famous among early explorers. After Columbus, European exploration rapidly accelerated, and many new trade routes were established.
As a result, the Atlantic became and remains the major artery between Europe and the Americas (known as transatlantic trade). Scientific explorations include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
Ethiopic Ocean.
The Aethiopian Sea, Ethiopic Ocean or Ethiopian Ocean (Okeanos Aithiopos), is an old name for what is now called the South Atlantic Ocean, which is separated from the North Atlantic Ocean by a narrow region between Natal, Brazil and Monrovia, Liberia. The use of this term illustrates a past trend towards referring to the whole continent of Africa by the name "Aethiopia". The modern nation of Ethiopia, in northeast Africa, is nowhere near the Ethiopic Ocean, which would be said to lie off the west coast of Africa. The term Ethiopian Ocean sometimes appeared until the mid-19th century, as e.g. on the map "The Dutch colony of the Cape of Good Hope by L.S. de la Rochette, MDCCXCV", published by W. Faden in London in 1795.
Economy.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves. The Atlantic hosts the world's richest fishing resources, especially in the waters covering the shelves. The major fish are cod, haddock, hake, herring, and mackerel.
The most productive areas include the Grand Banks of Newfoundland, the Nova Scotia shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Dogger Bank of the North Sea, and the Falkland Banks. Eel, lobster, and whales appear in great quantities. Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea. 
Terrain.
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea. A clockwise warm-water gyre occupies the northern Atlantic, and a counter-clockwise warm-water gyre appears in the southern Atlantic. The Mid-Atlantic Ridge, a rugged north-south centerline for the entire Atlantic basin, first discovered by the Challenger Expedition dominates the ocean floor. This was formed by the vulcanism that also formed the ocean floor and the islands rising from it.
The Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Norwegian Sea, Baltic Sea, North Sea, Labrador Sea, Black Sea, Gulf of Saint Lawrence, Bay of Fundy, Gulf of Maine, Mediterranean Sea, Gulf of Mexico, and Caribbean Sea.
Islands include Newfoundland (including hundreds of surrounding islands), Greenland, Iceland, Faroe Islands, Great Britain (including numerous surrounding islands), Ireland, Rockall, Sable Island, Azores, St. Pierre and Miquelon, Madeira, Bermuda, Canary Islands, Caribbean, Cape Verde, São Tomé and Príncipe, Annobón Province, Fernando de Noronha, Rocas Atoll, Ascension Island, Saint Helena, Trindade and Martim Vaz, Tristan da Cunha, Gough Island (Also known as Diego Alvarez), Falkland Islands, Tierra del Fuego, South Georgia Island, South Sandwich Islands, and Bouvet Island.
Natural resources.
The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones.
Natural hazards.
Icebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December).
The United States' southeast coast has a long history of shipwrecks due to its many shoals and reefs. The Virginia and North Carolina coasts were particularly dangerous.
The Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.
Hurricanes are also a natural hazard in the Atlantic, but mainly in the northern part of the ocean, rarely tropical cyclones form in the southern parts. Hurricanes usually form between 1 June and 30 November of every year.
Current environmental issues.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
In 2005, there was some concern that warm northern European currents were slowing down.
On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
Bordering countries and territories.
The states (territories in italics) with a coastline on the Atlantic Ocean (excluding the Black, Baltic and Mediterranean Seas) are:

</doc>
<doc id="700" url="http://en.wikipedia.org/wiki?curid=700" title="Arthur Schopenhauer">
Arthur Schopenhauer

Arthur Schopenhauer (; 22 February 1788 – 21 September 1860) was a German philosopher best known for his book, "The World as Will and Representation" (German: "Die Welt als Wille und Vorstellung"), in which he claimed that our world is driven by a continually dissatisfied will, continually seeking satisfaction. Influenced by Eastern philosophy, he maintained that the "truth was recognized by the sages of India"; consequently, his solutions to suffering were similar to those of Vedantic and Buddhist thinkers (e.g., asceticism). The influence of "transcendental ideality" led him to choose atheism.
At age 25, he published his doctoral dissertation, "On the Fourfold Root of the Principle of Sufficient Reason", which examined the four distinct aspects of experience in the phenomenal world; consequently, he has been influential in the history of phenomenology. He has influenced many thinkers, including Friedrich Nietzsche, Richard Wagner, Otto Weininger, Ludwig Wittgenstein, Erwin Schrödinger, Albert Einstein, Sigmund Freud, Otto Rank, Carl Jung, Joseph Campbell, Leo Tolstoy, Thomas Mann, and Jorge Luis Borges, among others.
Life.
Arthur Schopenhauer was born on 22 February 1788 in the city of Danzig, on Heiligegeistgasse (known in the present day as Św. Ducha 47), the son of Johanna Schopenhauer (née Trosiener) and Heinrich Floris Schopenhauer, both descendants of wealthy German patrician families. At the time Danzig became part of Prussia in 1793, Heinrich removed to Hamburg, although his firm continued trading in Danzig. In 1805, Schopenhauer's father may have committed suicide. Shortly thereafter, Schopenhauer's mother Johanna moved with his sister Adele to Weimar, then the centre of German literature, to pursue her writing career. After one year, Schopenhauer left the family business in Hamburg to join her. As early as 1799, he started playing the flute.
He became a student at the University of Göttingen in 1809. There he studied metaphysics and psychology under Gottlob Ernst Schulze, the author of "Aenesidemus", who advised him to concentrate on Plato and Immanuel Kant. In Berlin, from 1811 to 1812, he had attended lectures by the prominent post-Kantian philosopher Johann Gottlieb Fichte and the theologian Friedrich Schleiermacher.
In 1814, Schopenhauer began his seminal work "The World as Will and Representation" ("Die Welt als Wille und Vorstellung"). He finished it in 1818 and published it the following year. In Dresden in 1819, Schopenhauer fathered, with a servant, an illegitimate daughter who was born and died the same year. In 1820, Schopenhauer became a lecturer at the University of Berlin. He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a "clumsy charlatan." However, only five students turned up to Schopenhauer's lectures, and he dropped out of academia. A late essay, "On University Philosophy," expressed his resentment towards the work conducted in academies.
While in Berlin, Schopenhauer was named as a defendant in a lawsuit initiated by a woman named Caroline Marquet.
She asked for damages, alleging that Schopenhauer had pushed her. According to Schopenhauer's court testimony, she deliberately annoyed him by raising her voice while standing right outside his door. Marquet alleged that the philosopher had assaulted and battered her after she refused to leave his doorway. Her companion testified that she saw Marquet prostrate outside his apartment. Because Marquet won the lawsuit, Schopenhauer made payments to her for the next twenty years. When she died, he wrote on a copy of her death certificate, "Obit anus, abit onus" ("The old woman dies, the burden is lifted"). In 1819 the fortunes of his mother and sister, and himself, were threatened by the failure of the firm in Danzig in which his father had been a director and shareholder. His sister accepted a compromise compensation package of 70 per cent, but Schopenhauer angrily refused this, and eventually recovered 9400 thalers.
In 1821, he fell in love with nineteen-year old opera singer, Caroline Richter (called Medon), and had a relationship with her for several years. He discarded marriage plans, however, writing, "Marrying means to halve one's rights and double one's duties," and "Marrying means to grasp blindfolded into a sack hoping to find an eel amongst an assembly of snakes." When he was forty-three years old, seventeen-year old Flora Weiss recorded rejecting him in her diary.
Schopenhauer had a notably strained relationship with his mother Johanna Schopenhauer. After his father's death, Arthur Schopenhauer endured two long years of drudgery as a merchant, in honor of his dead father. Then his mother retired to Weimar, and Arthur Schopenhauer dedicated himself wholly to studies in the gymnasium of Gotha. He left it in disgust after seeing one of the masters lampooned, and went to live with his mother. But by that time she had already opened her famous salon, and Arthur was not compatible with what he considered to be the vain, ceremonious ways of the salon. He was also disgusted by the ease with which Johanna Schopenhauer had forgotten his father's memory. Consequently, he attempted university life. There, he wrote his first book, "On the Fourfold Root of the Principle of Sufficient Reason". His mother informed him that the book was incomprehensible and it was unlikely that anyone would ever buy a copy. In a fit of temper Arthur Schopenhauer told her that his work would be read long after the "rubbish" she wrote would have been totally forgotten.
In 1831, a cholera epidemic broke out in Berlin and Schopenhauer left the city. Schopenhauer settled permanently in Frankfurt in 1833, where he remained for the next twenty-seven years, living alone except for a succession of pet poodles named Atman and Butz. The numerous notes that he made during these years, amongst others on aging, were published posthumously under the title "Senilia".
Schopenhauer had a robust constitution, but in 1860 his health began to deteriorate. He died of heart failure on 21 September 1860 while sitting at home on his couch with his cat. He was 72.
Thought.
Philosophy of the "Will".
A key focus of Schopenhauer was his investigation of individual motivation. Before Schopenhauer, Hegel had popularized the concept of "Zeitgeist", the idea that society consisted of a collective consciousness which moved in a distinct direction, dictating the actions of its members. Schopenhauer, a reader of both Kant and Hegel, criticized their logical optimism and the belief that individual morality could be determined by society and reason. Schopenhauer believed that humans were motivated by only their own basic desires, or ("Will to Live"), which directed all of mankind.
For Schopenhauer, human desire was futile, illogical, directionless, and, by extension, so was all human action in the world. He wrote "Man can indeed do what he wants, but he cannot will what he wants". In this sense, he adhered to the Fichtean principle of idealism: "the world is "for" a subject". This idealism so presented, immediately commits it to an ethical attitude, unlike the purely epistemological concerns of Descartes and Berkeley. To Schopenhauer, the Will is a malignant, metaphysical existence which controls not only the actions of individual, intelligent agents, but ultimately all observable phenomena; an evil to be terminated via mankind's duties: asceticism and chastity. He is credited with one of the most famous opening lines of philosophy: "The world is my representation". Will, for Schopenhauer, is what Kant called the "thing-in-itself." Nietzsche was greatly influenced by this idea of Will, while developing it in a different direction.
Art and aesthetics.
For Schopenhauer, human desiring, "willing," and craving cause suffering or pain. A temporary way to escape this pain is through aesthetic contemplation (a method comparable to Zapffe's ""Sublimation"). Aesthetic contemplation allows one to escape this pain—albeit temporarily—because it stops one perceiving the world as mere presentation. Instead, one no longer perceives the world as an object of perception (therefore as subject to the Principle of Sufficient Grounds; time, space and causality) from which one is separated; rather one becomes one with that perception: "one can thus no longer separate the perceiver from the perception" ("The World as Will and Representation", section 34). From this immersion with the world one no longer views oneself as an individual who suffers in the world due to one's individual will but, rather, becomes a "subject of cognition" to a perception that is "Pure, will-less, timeless"" (section 34) where the essence, "ideas," of the world are shown. Art is the practical consequence of this brief aesthetic contemplation as it attempts to depict one's immersion with the world, thus tries to depict the essence/pure ideas of the world. Music, for Schopenhauer, was the purest form of art because it was the one that depicted the will itself without it appearing as subject to the Principle of Sufficient Grounds, therefore as an individual object. According to Daniel Albright, "Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself."
He deemed music to be a timeless, universal, language which is comprehended everywhere, and can imbue global enthusiasm, if in possession of a significant melody.
Ethics.
Schopenhauer's moral theory proposed that only compassion can drive moral acts. According to Schopenhauer, compassion alone is the good of the object of the acts, that is, they cannot be inspired by either the prospect of personal utility or the feeling of duty. Mankind can also be guided by egoism and malice. Egotistic acts are those guided by self-interest, desire for pleasure or happiness. Schopenhauer believed most of our deeds belong to this class. Acts of malice are different from egotistic acts. As in the case of acts of compassion, these do not target personal utility. Their aim is to cause damage to others, independently of personal gains.
Punishment.
According to Schopenhauer, whenever we make a choice, "we assume as necessary that decision was preceded by something from which it ensued, and which we call the ground or reason, or more accurately the motive, of the resultant action." Choices are not made freely. Our actions are necessary and determined because "every human being, even every animal, after the motive has appeared, must carry out the action which alone is in accordance with his inborn and immutable character." A definite action inevitably results when a particular motive influences a person's given, unchangeable character.
The State, Schopenhauer claimed, punishes criminals in order to prevent future crimes. It does so by placing "beside every possible motive for committing a wrong a more powerful motive for leaving it undone, in the inescapable punishment. Accordingly, the criminal code is as complete a register as possible of counter-motives to all criminal actions that can possibly be imagined..."
Should capital punishment be legal? "For safeguarding the lives of citizens," he asserted, "capital punishment is therefore absolutely necessary." "The murderer," wrote Schopenhauer, "who is condemned to death according to the law must, it is true, be now used as a mere "means", and with complete right. For public security, which is the principal object of the State, is disturbed by him; indeed it is abolished if the law remains unfulfilled. The murderer, his life, his person, must be the "means" of fulfilling the law, and thus of re-establishing public security." Schopenhauer disagreed with those who would abolish capital punishment. "Those who would like to abolish it should be given the answer: 'First remove murder from the world, and then capital punishment ought to follow.' "
People, according to Schopenhauer, cannot be improved. They can only be influenced by strong motives that overpower criminal motives. Schopenhauer declared that "real moral reform is not at all possible, but only determent from the deed..."
He claimed that this doctrine was not original with him. Previously, it appeared in the writings of Plato, Seneca, Hobbes, Pufendorf, and Anselm Feuerbach. Schopenhauer declared that their teaching was corrupted by subsequent errors and therefore was in need of clarification.
God.
Even though Schopenhauer ended his treatise on the freedom of human will with the postulate of everyone's responsibility for their character and, consequently, acts—the responsibility following from one's being the Will as noumenon (from which also all the characters and creations come)—he considered his views incompatible with theism, on grounds of fatalism and, more generally, responsibility for evil. In Schopenhauer's philosophy the dogmas of Christianity lose their significance, and the "Last Judgment" is no longer preceded by anything—"the world is itself the Last Judgment on it". Whereas God, if he existed, would be evil.
Psychology.
Philosophers have not traditionally been impressed by the tribulations of sex, but Schopenhauer addressed it and related concepts forthrightly:
He gave a name to a force within man which he felt had invariably precedence over reason: the Will to Live or Will to Life ("Wille zum Leben"), defined as an inherent drive within human beings, and indeed all creatures, to stay alive; a force which inveigles us into reproducing.
Schopenhauer refused to conceive of love as either trifling or accidental, but rather understood it to be an immensely powerful force lying unseen within man's psyche and dramatically shaping the world:
These ideas foreshadowed the discovery of evolution, Freud's concepts of the libido and the unconscious mind, and evolutionary psychology in general.
Political and social thought.
Politics.
Schopenhauer's politics were, for the most part, an echo of his system of ethics (the latter being expressed in "Die beiden Grundprobleme der Ethik", available in English as two separate books, "On the Basis of Morality" and "On the Freedom of the Will"). Ethics also occupies about one quarter of his central work, "The World as Will and Representation".
In occasional political comments in his "Parerga and Paralipomena" and "Manuscript Remains", Schopenhauer described himself as a proponent of limited government. What was essential, he thought, was that the state should "leave each man free to work out his own salvation", and so long as government was thus limited, he would "prefer to be ruled by a lion than one of [his] fellow rats" — i.e., by a monarch, rather than a democrat. Schopenhauer shared the view of Thomas Hobbes on the necessity of the state, and of state action, to check the destructive tendencies innate to our species. He also defended the independence of the legislative, judicial and executive branches of power, and a monarch as an impartial element able to practice justice (in a practical and everyday sense, not a cosmological one). He declared monarchy as "that which is natural to man" for "intelligence has always under a monarchical government a much better chance against its irreconcilable and ever-present foe, stupidity" and disparaged republicanism as "unnatural as it is unfavourable to the higher intellectual life and the arts and sciences."
Schopenhauer, by his own admission, did not give much thought to politics, and several times he writes proudly of how little attention he had paid "to political affairs of [his] day". In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he did indeed maintain his aloof position of "minding not the times but the eternities". He wrote many disparaging remarks about Germany and the Germans. A typical example is, "For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect."
Schopenhauer attributed civilizational primacy to the northern "white races" due to their sensitivity and creativity (except for the ancient Egyptians and Hindus whom he saw as equal):
The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate. This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization.
Despite this, he was adamantly against differing treatment of races, was fervently anti-slavery, and supported the abolitionist movement in the United States. He describes the treatment of "[our] innocent black brothers whom force and injustice have delivered into [the slave-master's] devilish clutches" as "belonging to the blackest pages of mankind's criminal record".
Schopenhauer additionally maintained a marked metaphysical and political anti-Judaism. Schopenhauer argued that Christianity constituted a revolt against the materialistic basis of Judaism, exhibiting an Indian-influenced ethics reflecting the Aryan-Vedic theme of spiritual "self-conquest." This he saw as opposed to what he held to be the ignorant drive toward earthly utopianism and superficiality of a worldly Jewish spirit:
While all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations.
Views on women.
In Schopenhauer's 1851 essay "Of Women", he expressed his opposition to what he called "Teutonico-Christian stupidity" on female affairs. Schopenhauer wrote that "Women are directly fitted for acting as the nurses and teachers of our early childhood by the fact that they are themselves childish, frivolous and short-sighted". He opined that women are deficient in artistic faculties and sense of justice, and expressed opposition to monogamy. He claimed that "woman is by nature meant to obey". The essay does give some compliments, however: that "women are decidedly more sober in their judgment than [men] are" and are more sympathetic to the suffering of others.
Schopenhauer's controversial writings have influenced many, from Friedrich Nietzsche to nineteenth-century feminists. Schopenhauer's biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists.
After the elderly Schopenhauer sat for a sculpture portrait by Elisabet Ney, he told Richard Wagner's friend Malwida von Meysenbug, "I have not yet spoken my last word about women. I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man."
Heredity and eugenics.
Schopenhauer believed that personality and intellect were inherited. He quotes Horace's saying, "From the brave and good are the brave descended" ("Odes", iv, 4, 29) and Shakespeare's line from "Cymbeline", "Cowards father cowards, and base things sire base" (IV, 2) to reinforce his hereditarian argument.
Mechanistically, Schopenhauer believed that a person inherits his level of intellect through his mother, and personal character through one's father.
This belief in heritability of traits informed Schopenhauer's view of love –  placing it at the highest level of importance. For Schopenhauer the "final aim of all love intrigues, be they comic or tragic, is really of more importance than all other ends in human life. What it all turns upon is nothing less than the composition of the next generation... It is not the weal or woe of any one individual, but that of the human race to come, which is here at stake." This view of the importance for the species of whom we choose to love was reflected in his views on eugenics or good breeding. Here Schopenhauer wrote:
With our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation. Plato had something of the kind in mind when, in the fifth book of his "Republic", he explained his plan for increasing and improving his warrior caste. If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles.
In another context, Schopenhauer reiterated his antidemocratic-eugenic thesis: "If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women. This proposal constitutes my Utopia and my Platonic Republic". Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's advocacy of anti-egalitarianism and eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor.
Animal welfare.
As a consequence of his monistic philosophy, Schopenhauer was very concerned about the welfare of animals. For him, all individual animals, including humans, are essentially the same, being phenomenal manifestations of the one underlying Will. The word "will" designated, for him, force, power, impulse, energy, and desire; it is the closest word we have that can signify both the real essence of all external things and also our own direct, inner experience. Since everything is basically Will, then humans and animals are fundamentally the same and can recognize themselves in each other. For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers.
Compassion for animals is intimately associated with goodness of character, and it may be confidently asserted that he who is cruel to living creatures cannot be a good man.
Nothing leads more definitely to a recognition of the identity of the essential nature in animal and human phenomena than a study of zoology and anatomy.
The assumption that animals are without rights and the illusion that our treatment of them has no moral significance is a positively outrageous example of Western crudity and barbarity. Universal compassion is the only guarantee of morality.
In 1841, he praised the establishment, in London, of the Society for the Prevention of Cruelty to Animals, and also the Animals' Friends Society in Philadelphia. Schopenhauer even went so far as to protest against the use of the pronoun "it" in reference to animals because it led to the treatment of them as though they were inanimate things. To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter.
He was very attached to his succession of pet poodles. Schopenhauer criticized Spinoza's belief that animals are to be used as a mere means for the satisfaction of humans.
Views on homosexuality and pederasty.
Schopenhauer was also one of the first philosophers since the days of Greek philosophy to address the subject of male homosexuality. In the third, expanded edition of "The World as Will and Representation" (1859), Schopenhauer added an appendix to his chapter on the "Metaphysics of Sexual Love". He also wrote that homosexuality did have the benefit of preventing ill-begotten children. Concerning this, he stated, "... the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her, it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils." Shrewdly anticipating the interpretive distortion, on the part of the popular mind, of his attempted scientific "explanation" of pederasty as personal "advocacy" (when he had otherwise described the act, in terms of spiritual ethics, as an "objectionable aberration"), Schopenhauer sarcastically concludes the appendix with the statement that "by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour, for they are very disconcerted by the ever-increasing publicization of my philosophy which they so carefully concealed. I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty."
Intellectual interests and affinities.
Schopenhauer learned from Christian philosophy.
Indology.
Schopenhauer read the Latin translation of the Upanishads which had been translated by French writer Anquetil du Perron from the Persian translation of Prince Dara Shikoh entitled "Sirre-Akbar" ("The Great Secret"). He was so impressed by their philosophy that he called them "the production of the highest human wisdom," and considered them to contain superhuman conceptions. The Upanishads was a great source of inspiration to Schopenhauer, and writing about them he said:
It is the most satisfying and elevating reading (with the exception of the original text) which is possible in the world; it has been the solace of my life and will be the solace of my death.
It is well known that the book "Oupnekhat" (Upanishad) always lay open on his table, and he invariably studied it before sleeping at night. He called the opening up of Sanskrit literature "the greatest gift of our century", and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West.
Schopenhauer was first introduced to the 1802 Latin Upanishad translation through Friedrich Majer. They met during the winter of 1813–1814 in Weimar at the home of Schopenhauer's mother according to the biographer Sanfranski. Majer was a follower of Herder, and an early Indologist. Schopenhauer did not begin a serious study of the Indic texts, however, until the summer of 1814. Sansfranski maintains that between 1815 and 1817, Schopenhauer had another important cross-pollination with Indian Thought in Dresden. This was through his neighbor of two years, Karl Christian Friedrich Krause. Krause was then a minor and rather unorthodox philosopher who attempted to mix his own ideas with that of ancient Indian wisdom. Krause had also mastered Sanskrit, unlike Schopenhauer, and the two developed a professional relationship. It was from Krause that Schopenhauer learned meditation and received the closest thing to expert advice concerning Indian thought.
Most noticeable, in the case of Schopenhauer’s work, was the significance of the Chandogya Upanishad, whose Mahavakya, Tat Tvam Asi is mentioned throughout "The World as Will and Representation".
Buddhism.
Schopenhauer noted a correspondence between his doctrines and the Four Noble Truths of Buddhism. Similarities centered on the principles that life involves suffering, that suffering is caused by desire (taṇhā), and that the extinction of desire leads to liberation. Thus three of the four "truths of the Buddha" correspond to Schopenhauer's doctrine of the will. In Buddhism, however, while greed and lust are always unskillful, desire is ethically variable – it can be skillful, unskillful, or neutral.
For Schopenhauer, Will had ontological primacy over the intellect; in other words, desire is understood to be prior to thought. Schopenhauer felt this was similar to notions of puruṣārtha or goals of life in Vedānta Hinduism.
In Schopenhauer's philosophy, denial of the will is attained by either:
However, Buddhist nirvāṇa is not equivalent to the condition that Schopenhauer described as denial of the will. Nirvāṇa is not the extinguishing of the "person" as some Western scholars have thought, but only the "extinguishing" (the literal meaning of nirvana) of the flames of greed, hatred, and delusion that assail a person's character. Occult historian Joscelyn Godwin (1945– ) stated, "It was Buddhism that inspired the philosophy of Arthur Schopenhauer, and, through him, attracted Richard Wagner. This Orientalism reflected the struggle of the German Romantics, in the words of Leon Poliakov, to free themselves from Judeo-Christian fetters". In contradistinction to Godwin's claim that Buddhism inspired Schopenhauer, the philosopher himself made the following statement in his discussion of religions:
If I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others. In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other. And this agreement must be yet the more pleasing to me, inasmuch as "in my philosophizing I have certainly not been under its influence" [emphasis added]. For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism.
Buddhist philosopher Nishitani Keiji, however, sought to distance Buddhism from Schopenhauer.
While Schopenhauer's philosophy may sound rather mystical in such a summary, his methodology was resolutely empirical, rather than speculative or transcendental:
Philosophy ... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions.
Also note:
This actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.
The argument that Buddhism affected Schopenhauer’s philosophy more than any other Dharmic faith loses more credence when viewed in light of the fact that Schopenhauer did not begin a serious study of Buddhism until after the publication of "The World as Will and Representation" in 1818. Scholars have started to revise earlier views about Schopenhauer's discovery of Buddhism. Proof of early interest and influence, however, appears in Schopenhauer's 1815/16 notes (transcribed and translated by Urs App) about Buddhism. They are included in a recent case study that traces Schopenhauer's interest in Buddhism and documents its influence.
Influences.
Schopenhauer said he was influenced by the Upanishads, Immanuel Kant and Plato. References to Eastern philosophy and religion appear frequently in Schopenhauer's writing. As noted above, he appreciated the teachings of the Buddha and even called himself a "Buddhist". He said that his philosophy could not have been conceived before these teachings were available.
Concerning the Upanishads and Vedas, he writes in "The World as Will and Representation":
If the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him. It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there.
Among Schopenhauer's other influences were: Shakespeare, Jean-Jacques Rousseau, John Locke, Thomas Reid, Baruch Spinoza, Matthias Claudius, George Berkeley, David Hume, and René Descartes.
Critique of Kant and Hegel.
Critique of the Kantian philosophy.
Schopenhauer accepted Kant's double-aspect of the universe – the phenomenal (world of experience) and the noumenal (the true world, independent of experience). Some commentators suggest that Schopenhauer claimed that the noumenon, or thing-in-itself, was the basis for Schopenhauer's concept of the will. Other commentators suggest that Schopenhauer considered will to be only a subset of the "thing-in-itself" class, namely that which we can most directly experience.
Schopenhauer's identification of the Kantian "noumenon" (i.e., the actually existing entity) with what he termed "will" deserves some explanation. The noumenon was what Kant called the "Ding an sich" (the Thing in Itself), the reality that is the foundation of our sensory and mental representations of an external world. In Kantian terms, those sensory and mental representations are mere phenomena. Schopenhauer departed from Kant in his description of the relationship between the phenomenon and the noumenon. According to Kant, things-in-themselves ground the phenomenal representations in our minds; Schopenhauer, on the other hand, believed phenomena and noumena to be two different sides of the same coin. Noumena do not "cause" phenomena, but rather phenomena are simply the way by which our minds perceive the noumena, according to the principle of sufficient reason. This is explained more fully in Schopenhauer's doctoral thesis, "On the Fourfold Root of the Principle of Sufficient Reason".
Schopenhauer's second major departure from Kant's epistemology concerns the body. Kant's philosophy was formulated as a response to the radical philosophical skepticism of David Hume, who claimed that causality could not be observed empirically. Schopenhauer begins by arguing that Kant's demarcation between external objects, knowable only as phenomena, and the Thing in Itself of noumenon, contains a significant omission. There is, in fact, one physical object we know more intimately than we know any object of sense perception: our own body.
We know our human bodies have boundaries and occupy space, the same way other objects known only through our named senses do. Though we seldom think of our body as a physical object, we know even before reflection that it shares some of an object's properties. We understand that a watermelon cannot successfully occupy the same space as an oncoming truck; we know that if we tried to repeat the experiment with our own body, we would obtain similar results – we know this even if we do not understand the physics involved.
We know that our consciousness inhabits a physical body, similar to other physical objects only known as phenomena. Yet our consciousness is not commensurate with our body. Most of us possess the power of voluntary motion. We usually are not aware of the breathing of our lungs or the beating of our heart unless somehow our attention is called to them. Our ability to control either is limited. Our kidneys command our attention on their schedule rather than one we choose. Few of us have any idea what our liver is doing right now, though this organ is as needful as lungs, heart, or kidneys. The conscious mind is the servant, not the master, of these and other organs; these organs have an agenda which the conscious mind did not choose, and over which it has limited power.
When Schopenhauer identifies the "noumenon" with the desires, needs, and impulses in us that we name "will," what he is saying is that we participate in the reality of an otherwise unachievable world outside the mind through will. We cannot "prove" that our mental picture of an outside world corresponds with a reality by reasoning; through will, we know – without thinking – that the world can stimulate us. We suffer fear, or desire: these states arise involuntarily; they arise prior to reflection; they arise even when the conscious mind would prefer to hold them at bay. The rational mind is, for Schopenhauer, a leaf borne along in a stream of pre-reflective and largely unconscious emotion. That stream is will, and through will, if not through logic, we can participate in the underlying reality beyond mere phenomena. It is for this reason that Schopenhauer identifies the "noumenon" with what we call our will.
In his criticism of Kant, Schopenhauer claimed that sensation and understanding are separate and distinct abilities. Yet, for Kant, an object is known through each of them. Kant wrote: "... [T]here are two stems of human knowledge ... namely, sensibility and understanding, objects being given by the former [sensibility] and thought by the latter [understanding]." Schopenhauer disagreed. He asserted that mere sense impressions, not objects, are given by sensibility. According to Schopenhauer, objects are intuitively perceived by understanding and are discursively thought by reason (Kant had claimed that (1) the understanding thinks objects through concepts and that (2) reason seeks the unconditioned or ultimate answer to "why?"). Schopenhauer said that Kant's mistake regarding perception resulted in all of the obscurity and difficult confusion that is exhibited in the Transcendental Analytic section of his critique.
Lastly, Schopenhauer departed from Kant in how he interpreted the Platonic ideas. In "The World as Will and Representation" Schopenhauer explicitly stated:
...Kant used the word [Idea] wrongly as well as illegitimately, although Plato had already taken possession of it, and used it most appropriately.
Instead Schopenhauer relied upon the Neoplatonist interpretation of the biographer Diogenes Laërtius from "Lives and Opinions of Eminent Philosophers". In reference to Plato’s Ideas, Schopenhauer quotes Laërtius verbatim in an explanatory footnote.
Diogenes Laërtius (III, 12) Plato ideas in natura velut exemplaria dixit subsistere; cetera his esse similia, ad istarum similitudinem consistencia.
Critique of Hegel.
Schopenhauer expressed his dislike for the philosophy of his contemporary Georg Wilhelm Friedrich Hegel many times in his published works. The following quotations are typical:
In his Foreword to the first edition of his work "Die beiden Grundprobleme der Ethik", Schopenhauer suggested that he had shown Hegel to have fallen prey to the "Post hoc ergo propter hoc" fallacy.
Schopenhauer suggested that Hegel's works were filled with "castles of abstraction," and that Hegel used deliberately impressive but ultimately vacuous verbiage. He also thought that his glorification of church and state were designed for personal advantage and had little to do with the search for philosophical truth. For instance, the Right Hegelians interpreted Hegel as viewing the Prussian state of his day as perfect and the goal of all history up until then.
Criticism of Schopenhauer's personal life.
The British philosopher and historian Bertrand Russell deemed Schopenhauer an insincere person, because judging by his life:
Bryan Magee points out that "the answer to such shallow, but not uncommon criticism" is found in a quote from Schopenhauer:
Influence.
Schopenhauer has had a massive influence upon later thinkers, though more so in the arts (especially literature and music) and psychology than in philosophy. His popularity peaked in the early twentieth century, especially during the Modernist era, and waned somewhat thereafter. Nevertheless, a number of recent publications have reinterpreted and modernised the study of Schopenhauer. His theory is also being explored by some modern philosophers as a precursor to evolutionary theory and modern evolutionary psychology.
Russian writer and philosopher Leo Tolstoy was greatly influenced by Schopenhauer. After reading Schopenhauer's "The World as Will and Representation", Tolstoy gradually became converted to the ascetic morality upheld in that work as the proper spiritual path for the upper classes: "Do you know what this summer has meant for me? Constant raptures over Schopenhauer and a whole series of spiritual delights which I've never experienced before. ... no student has ever studied so much on his course, and learned so much, as I have this summer"
Richard Wagner, writing in his autobiography, remembered his first impression that Schopenhauer left on him (when he read "World as Will and Representation"):
Wagner also commented that "serious mood, which was trying to find ecstatic expression" created by Schopenhauer inspired the conception of Tristan und Isolde. See also Influence of Schopenhauer on Tristan und Isolde.
Friedrich Nietzsche owed the awakening of his philosophical interest to reading "The World as Will and Representation" and admitted that he was one of the few philosophers that he respected, dedicating to him his essay "Schopenhauer als Erzieher" one of his "Untimely Meditations".
Jorge Luis Borges remarked that the reason he had never attempted to write a systematic account of his world view, despite his penchant for philosophy and metaphysics in particular, was because Schopenhauer had already written it for him.
As a teenager, Ludwig Wittgenstein adopted Schopenhauer's epistemological idealism. However, after his study of the philosophy of mathematics, he rejected epistemological idealism for Gottlob Frege's conceptual realism. In later years, Wittgenstein was highly dismissive of Schopenhauer, describing him as an ultimately "shallow" thinker: "Schopenhauer has quite a crude mind... where real depth starts, his comes to an end".
The philosopher Gilbert Ryle read Schopenhauer's works as a student, but later largely forgot them, only to unwittingly recycle ideas from Schopenhauer in his "The Concept of Mind".

</doc>
<doc id="701" url="http://en.wikipedia.org/wiki?curid=701" title="Angola">
Angola

Angola , officially the Republic of Angola ( ; Kikongo, Kimbundu, Umbundu: "Repubilika ya Ngola"), is a country in Southern Africa. It is the seventh largest country in Africa, and is bordered by Namibia on the south, the Democratic Republic of the Congo on the north, and Zambia on the east; its west coast is on the Atlantic Ocean and Luanda is its capital city. The exclave province of Cabinda has borders with the Republic of the Congo and the Democratic Republic of the Congo.
The Portuguese were present in some – mostly coastal – points of the territory of what is now Angola, from the 16th to the 19th century, interacting in diverse ways with the peoples who lived there. In the 19th century, they slowly and hesitantly began to establish themselves in the interior. Angola as a Portuguese colony encompassing the present territory was not established before the end of the 19th century, and "effective occupation", as required by the Berlin Conference (1884) was achieved only by the 1920s after the Mbunda resistance and abduction of their King, Mwene Mbandu I Lyondthzi Kapova. Independence was achieved in 1975, after a protracted liberation war. After independence, Angola was the scene of an intense civil war from 1975 to 2002. Despite the civil war, areas such as Baixa de Cassanje continue a lineage of kings which have included the former King Kambamba Kulaxingo and current King Dianhenga Aspirante Mjinji Kulaxingo.
The country has vast mineral and petroleum reserves, and its economy has on average grown at a double-digit pace since the 1990s, especially since the end of the civil war. In spite of this, standards of living remain low for the majority of the population, and life expectancy and infant mortality rates in Angola are among the worst in the world. Angola is considered to be economically disparate, with the majority of the nation's wealth concentrated in a disproportionately small sector of the population.
Angola is a member state of the African Union, the Community of Portuguese Language Countries, the Latin Union and the Southern African Development Community.
Etymology.
The name "Angola" comes from the Portuguese colonial name "Reino de Angola (Kingdom of Angola)", appearing as early as Dias de Novais's 1571 charter. The toponym was derived by the Portuguese from the title "ngola" held by the kings of Ndongo. Ndongo was a kingdom in the highlands, between the Kwanza and Lukala Rivers, nominally tributary to the king of Kongo but which was seeking greater independence during the 16th century.
History.
Early migrations and political units.
Khoisan hunter-gatherers are the earliest known modern human inhabitants of the area. They were largely absorbed and/or replaced by Bantu peoples during the Bantu migrations, though small numbers remain in parts of southern Angola to the present day. The Bantu came from the north, probably from somewhere near the present-day Republic of Cameroon and Sudan. The establishment of the Bantu took many centuries and gave rise to various groups who took on different ethnic characteristics.
During this time, the Bantu established a number of political units ("kingdoms", "empires") in most parts of what today is Angola. The best known of these is the Kingdom of the Kongo that had its centre in the northwest of contemporary Angola, but included important regions in the west of present day Democratic Republic of the Congo and Republic of Congo, and in southern Gabon. It established trade routes with other trading cities and civilizations up and down the coast of southwestern and West Africa and even with the Great Zimbabwe Mutapa Empire, but engaged in little or no transoceanic trade.
Others include the Mbunda, whose Kingdom was established in the fifteenth century at the confluence of Kwilu and Kasai rivers, in the south of present day Democratic Republic of the Congo, after a misunderstanding in Kola, also known as the origin of the Lunda and the Luba Kingdoms. The Mbunda trace their origin from Sudan, trekking southwards through Kola where they came in contact with the Luba and Ruund people. They reached what is now Angola in the sixteenth century, where they encountered the Khoisan, Bushmen and other groups considerably less technologically advanced, whom they easily dominated with their superior knowledge of metal-working, ceramics and agriculture. The Mbunda Kingdom in Mbundaland, southeast of the now Angola endured until late nineteenth century, one of the oldest and biggest ethnic grouping in Southern Africa.
Portuguese colonization.
The geographical areas now designated as Angola entered into contact with the Portuguese in the late 15th century, concretely in 1483, when Portugal established relations with the Kongo State, which stretched from modern Gabon in the north to the Kwanza River in the south. In this context, the Portuguese established a small trade-post at the port of Mpinda, in Soyo. The Portuguese explorer Paulo Dias de Novais founded Luanda in 1575 as "São Paulo de Loanda", with a hundred families of settlers and four hundred soldiers. Benguela, a Portuguese fort from 1587 which became a town in 1617, was another important early settlement they founded and ruled. The Portuguese would establish several settlements, forts and trading posts along the coastal strip of current-day Angola, which relied on the slave trade, commerce in raw materials, and the exchange of goods for survival.
The Atlantic slave trade provided a large number of black slaves to merchants and to slave dealers in Angola. European traders would export manufactured goods to the coast of Africa where they would be exchanged for slaves. Within the Portuguese Empire, most black African slaves were traded to Portuguese merchants who bought them to sell as cheap labour for use on Brazilian agricultural plantations. This trade would last until the first half of the 19th century. According to John Iliffe, "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys".
The Portuguese gradually took control of the coastal strip during the 16th century by a series of treaties and wars, forming the Portuguese colony of Angola. Taking advantage of the Portuguese Restoration War, the Dutch occupied Luanda from 1641 to 1648, where they allied with local peoples, consolidating their colonial rule against the remaining Portuguese resistance. In 1648, a fleet under the command of Salvador de Sá retook Luanda for Portugal and initiated a conquest of the lost territories, which restored Portugal to its former possessions by 1650. Treaties regulated relations with Kongo in 1649 and Njinga's Kingdom of Matamba and Ndongo in 1656. The conquest of Pungo Andongo in 1671 was the last major Portuguese expansion from Luanda outwards, as attempts to invade Kongo in 1670 and Matamba in 1681 failed. Portugal also expanded its territory behind the colony of Benguela to some extent, but until the 19th century the inroads from Luanda and Benguela were very limited, and Portugal had neither the intention nor the means to carry out a large scale territorial occupation and colonization.
The process resulted in few gains until the 1880s. Development of the hinterland began after the Berlin Conference in 1885 fixed the colony's borders, and British and Portuguese investment fostered mining, railways, and agriculture based on various forced-labour systems. Full Portuguese administrative control of the hinterland did not establish itself until the beginning of the 20th century, after the Mbunda resistance and abduction of their King, Mwene Mbandu I Lyondthzi Kapova, eventually dislodged the Mbunda Kingdom extending Angolan territory over Mbundaland. In 1951 the Portuguese government designated the colony as an overseas province of Portugal, called the Overseas Province of Angola.
Portugal had a minimalist presence in Angola for nearly five hundred years, and early calls for independence provoked little reaction amongst the population. More overtly political organisations first appeared in the 1950s and began to make organised demands for self-determination, especially in international forums such as the Non-Aligned Movement.
The Portuguese regime, meanwhile, refused to accede to the demands for independence, provoking an armed conflict that started in 1961 when black guerrillas attacked both white and black civilians in cross-border operations in northeastern Angola. The war came to be known as the Colonial War. In this struggle, the principal protagonists included, the People's Movement for the Liberation of Angola (MPLA), founded in 1956, the National Front for the Liberation of Angola (FNLA), which appeared in 1961 and the National Union for the Total Independence of Angola (UNITA), founded in 1966.
After many years of conflict that led to the weakening of all the insurgent parties, Angola gained its independence on 11 November 1975, after the 1974 coup d'état in Lisbon, Portugal, which overthrew the Portuguese regime headed by Marcelo Caetano.
Portugal's new revolutionary leaders began in 1974 a process of political change at home and accepted independence for its former colonies abroad. In Angola a fight for dominance broke out immediately between the three nationalist movements. The events prompted a mass exodus of Portuguese citizens, creating up to 300 000 destitute Portuguese refugees—the "retornados".
The new Portuguese government tried to mediate an understanding between the three competing movements, and succeeded in getting them to agree, on paper, to form a common government. But in the end none of the African parties respected the commitments made, and military force resolved the issue.
Independence and civil war.
After independence in November 1975, Angola faced a devastating civil war which lasted several decades and claimed millions of lives and produced many refugees. Following negotiations held in Portugal, itself under severe social and political turmoil and uncertainty due to the April 1974 revolution, Angola's three main guerrilla groups agreed to establish a transitional government in January 1975.
Within two months, however, the FNLA, MPLA and UNITA were fighting each other and the country was well on its way to being divided into zones controlled by rival armed political groups. The superpowers were quickly drawn into the conflict, which became a flash point for the Cold War. The United States, Zaire (today's Democratic Republic of the Congo) and South Africa supported the FNLA and UNITA. The Soviet Union and Cuba supported the MPLA.
In the beginning of the Civil War, most of the half million Portuguese that lived in Angola and accounted for the majority of the skilled work in the public administration, agriculture, industries and trade fled the country leaving its once prosperous and growing economy to a state of bankruptcy.
During most of this period, 1975–1990, the MPLA organised and maintained a socialist regime.
Ceasefire with UNITA.
On 22 March 2002, Jonas Savimbi, the leader of UNITA, was killed in combat with government troops. A cease-fire was reached by the two factions shortly afterwards. UNITA gave up its armed wing and assumed the role of major opposition party, although in the knowledge that in the present regime a legitimate democratic election was impossible. Although the political situation of the country began to stabilize, regular democratic processes were not established before the Elections in Angola in 2008 and 2012 and the adoption of a new Constitution of Angola in 2010, all of which strengthened the prevailing Dominant-party system. MPLA head officials continue e.g. to be given senior positions in top level companies or other fields, although a few outstanding UNITA figures are given some shares in the economic as well as in the military share.
Among Angola's major problems are a serious humanitarian crisis (a result of the prolonged war), the abundance of minefields, the continuation of the political, and to a much lesser degree, military activities in favour of the independence of the northern exclave of Cabinda, carried out in the context of the protracted Cabinda Conflict by the Frente para a Libertação do Enclave de Cabinda, but most of all, the dilapidation of the country's rich mineral resources by the regime. While most of the internally displaced have now settled around the capital, in the so-called "Musseques", the general situation for Angolans remains desperate.
Geography.
At , Angola is the world's twenty-third largest country. It is comparable in size to Mali, or twice the size of France or Texas. It lies mostly between latitudes 4° and 18°S, and longitudes 12° and 24°E.
Angola is bordered by Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east, and the South Atlantic Ocean to the west. The coastal exclave of Cabinda in the north, borders the Republic of the Congo to the north, and the Democratic Republic of the Congo to the south. Angola's capital, Luanda, lies on the Atlantic coast in the northwest of the country.
Climate.
Angola has three seasons, a dry season which lasts from May to October, a transitional season with some rain from November to January and a hot rainy season from February to April. April is the wettest month. Angola's average temperature on the coast is in the winter and in the summer, while the interior is generally hotter and dryer.
Politics.
Angola's motto is "Virtus Unita Fortior", a Latin phrase meaning "Virtue is stronger when united". The executive branch of the government is composed of the President, the Vice-Presidents and the Council of Ministers. For decades, political power has been concentrated in the Presidency.
Governors of the 18 provinces are appointed by the president. The Constitutional Law of 1992 establishes the broad outlines of government structure and delineates the rights and duties of citizens. The legal system is based on Portuguese and customary law but is weak and fragmented, and courts operate in only 12 of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court with powers of judicial review has not been constituted until 2010, despite statutory authorization.
After the end of the Civil War the regime came under pressure from within as well as from the international environment, to become more democratic and less authoritarian. Its reaction was to operate a number of changes without substantially changing its character.
Angola is classified as 'not free' by Freedom House in the Freedom in the World 2013 report. The report noted that the August 2012 parliamentary elections, in which the ruling Popular Movement for the Liberation of Angola won more than 70% of the vote, suffered from serious flaws, including outdated and inaccurate voter rolls. Voter turnout dropped from 80% in 2008 to 60%.
Angola scored poorly on the 2013 Ibrahim Index of African Governance. It was ranked 39 out of 52 sub-Saharan African countries, scoring particularly badly in the areas of Participation and Human Rights, Sustainable Economic Opportunity and Human Development. The Ibrahim Index uses a number of different variables to compile its list which reflects the state of governance in Africa.
The new constitution, adopted in 2010, further sharpened the authoritarian character of the regime. In the future, there will be no presidential elections: the president and the vice-president of the political party which comes out strongest in the parliamentary elections become automatically president and vice-president of Angola. Through a variety of mechanisms, the state president controls all the other organs of the state, so that the principle of the division of power is not maintained. As a consequence, Angola has no longer a presidential system, in the sense of the systems existing e.g. in the USA or in France. In terms of the classifications used in constitutional law, its regime is considered one of several authoritarian regimes in Africa.
Military.
The Angolan Armed Forces (AAF) is headed by a Chief of Staff who reports to the Minister of Defense. There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA), and National Air Force (Força Aérea Nacional, FAN). Total manpower is about 110,000. Its equipment includes Russian-manufactured fighters, bombers, and transport planes. There are also Brazilian-made EMB-312 Tucano for training role, Czech-made L-39 for training and bombing role, Czech Zlin for training role and a variety of western made aircraft such as C-212\Aviocar, Sud Aviation Alouette III, etc. A small number of AAF personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
Police.
The National Police departments are Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police. The National Police are in the process of standing up an air wing, which will provide helicopter support for operations. The National Police are developing their criminal investigation and forensic capabilities. The force has an estimated 6,000 patrol officers, 2,500 taxation and frontier supervision officers, 182 criminal investigators and 100 financial crimes detectives and around 90 economic activity inspectors.
The National Police have implemented a modernization and development plan to increase the capabilities and efficiency of the total force. In addition to administrative reorganization, modernization projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programs and the replacement of AKM rifles with 9 mm Uzis for officers in urban areas.
Administrative divisions.
Angola is divided into eighteen provinces ("províncias") and 163 municipalities. The municipalities are further divided into 475 communes (townships). The provinces are:
Exclave of Cabinda.
With an area of approximately , the Northern Angolan province of Cabinda is unusual in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo along the lower Congo river. Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south. The town of Cabinda is the chief population center.
According to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom live in neighboring countries. Population estimates are, however, highly unreliable. Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil. The product for which it is best known, however, is its oil, which has given it the nickname, "the Kuwait of Africa". Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output. Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards.
Ever since Portugal handed over sovereignty of its former overseas province of Angola to the local independence groups (MPLA, UNITA, and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its military forces, the FAA—Forças Armadas Angolanas) and Cabindan separatists. The Front for the Liberation of the Enclave of Cabinda-Armed Forces of Cabinda (FLEC-FAC) announced a virtual Federal Republic of Cabinda under the Presidency of N'Zita Henriques Tiago. One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions.
Economy.
Angola's financial system is maintained by the National Bank of Angola and managed by Governor . 
Angola has a rich subsoil heritage, from diamonds, oil, gold, copper, and a rich wildlife (dramatically impoverished during the civil war), forest, and fossils. Since independence, oil and diamonds have been the most important economic resource. Smallholder and plantation agriculture have dramatically dropped because of the Angolan Civil War, but have begun to recover after 2002. The transformation industry that had come into existence in the late colonial period collapsed at independence, because of the exodus of most of the ethnic Portuguese population, but has begun to reemerge (with updated technologies), partly because of the influx of new Portuguese entrepreneurs. Similar developments can be verified in the service sector.
Overall, Angola's economy has undergone a period of transformation in recent years, moving from the disarray caused by a quarter century of civil war to being the fastest growing economy in Africa and one of the fastest in the world, with an average GDP growth of 20 percent between 2005 and 2007. In the period 2001–2010, Angola had the world's highest annual average GDP growth, at 11.1 percent. In 2004, China's Eximbank approved a $2 billion line of credit to Angola. The loan is being used to rebuild Angola's infrastructure, and has also limited the influence of the International Monetary Fund in the country. China is Angola's biggest trade partner and export destination as well as the fourth-largest importer. Bilateral trade reached $27.67 billion in 2011, up 11.5 percent year-on-year. China's imports, mainly crude oil and diamonds, increased 9.1 percent to $24.89 billion while China's exports, including mechanical and electrical products, machinery parts and construction materials, surged 38.8 percent. The overabundance of oil led to a local unleaded gasoline "pricetag" of £0.37 per gallon.
"The Economist" reported in 2008 that diamonds and oil make up 60 percent of Angola's economy, almost all of the country's revenue and are its dominant exports. Growth is almost entirely driven by rising oil production which surpassed in late 2005 and was expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. However, operations in diamond mines include partnerships between state-run Endiama and mining companies such as ALROSA which continue operations in Angola. The economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007. However, due to the global recession the economy contracted an estimated −0.3% in 2009. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Although the country's economy has developed significantly since achieving political stability in 2002, mainly thanks to the fast-rising earnings of the oil sector, Angola faces huge social and economic problems. These are in part a result of the almost continual state of conflict from 1961 onwards, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. However, high poverty rates and blatant social inequality are chiefly the outcome of a combination of a persistent political authoritarianism, of "neo-patrimonial" practices at all levels of the political, administrative, military, and economic apparatuses, and of a pervasive corruption. The main beneficiary of this situation is a social segment constituted since 1975, but mainly during the last decades, around the political, administrative, economic, and military power holders, which has accumulated (and continues accumulating) enormous wealth. "Secondary beneficiaries" are the middle strata which are about to become social classes. However, overall almost half the population has to be considered as poor, but in this respect there are dramatic differences between the countryside and the cities (where by now slightly more than 50% of the people live).
An inquiry carried out in 2008 by the Angolan Instituto Nacional de Estatística has it that in the rural areas roughly 58% must be classified as "poor", according to UN norms, but in the urban areas only 19%, while the overall rate is 37%. In the cities, a majority of families, well beyond those officially classified as poor, have to adopt a variety of survival strategies. At the same time, in urban areas social inequality is most evident, and assumes extreme forms in the capital, Luanda. In the Human Development Index Angola constantly ranks in the bottom group.
According to The Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil. Growing oil revenues have also created opportunities for corruption: according to a recent Human Rights Watch report, 32 billion US dollars disappeared from government accounts from 2007 to 2010.
Before independence in 1975, Angola was a breadbasket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war (1975–2002) destroyed the fertile countryside, leaving it littered with landmines and driving millions into the cities. The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90 percent of farming is done at family and subsistence level. Thousands of Angolan small-scale farmers are trapped in poverty.
The enormous differences between the regions pose a serious structural problem in the Angolan economy. This is best illustrated by the fact that about one third of the economic activities is concentrated in Luanda and the neighbouring Bengo province, while several areas of the interior are characterized by stagnation and even regression.
One of the economic consequences of the social and regional disparities is a sharp increase in Angolan private investments abroad. The small fringe of Angolan society where most of the accumulation takes place seeks to spread its assets, for reasons of security and profit. For the time being, the biggest share of these investments is concentrated in Portugal where the Angolan presence (including that of the family of the state president) in banks as well as in the domains of energy, telecommunications, and mass media has become notable, as has the acquisition of vineyards and orchards as well as of touristic enterprises.
With a stock of assets corresponding to 70 billion USD (6.8 billion Kz), Angola is now the third largest financial market in sub-Saharan Africa, surpassed only by Nigeria and South Africa. According to the Angolan Minister of Economy, Abraão Gourgel, the financial market of the country grew modestly from 2002 and now lies in third place at the level of sub-Saharan Africa.
Angola’s economy is expected to grow by 3.9 percent in 2014 said the International Monetary Fund (IMF). According to the Fund, robust growth in the nonoil economy, mainly driven by a very good performance in the agricultural sector, is expected to offset a temporary drop in oil production.
According to a study on the banking sector, carried out by Deloitte, the monetary policy led by Banco Nacional de Angola (BNA), the Angolan national bank, allowed a decrease in the inflation rate put at 7.96% in December 2013, which contributed to the sector’s growth trend.
Transport.
Transport in Angola consists of:
Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. While a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road. The Angolan government has contracted the restoration of many of the country's roads. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Progress to complete the road infrastructure is likely to take some decades, but substantial efforts are already being made in the right directions.
Demographics.
Angola's population is estimated to be 18,056,072 (2012). It is composed of Ovimbundu (language Umbundu) 37%, Ambundu (language Kimbundu) 25%, Bakongo 13%, and 32% other ethnic groups (including the Chokwe, the Ovambo, the Mbunda, with the latter having been replaced by Ganguela, a generic term for peoples east of the Central Highlands, which has a slightly derogatory meaning when applied by the western ethnic groups, and the Xindonga) as well as about 2% "mestiços" (mixed European and African), 1.4% Chinese and 1% European. The Ambundu and Ovimbundu nations combined form a majority of the population, at 62%. The population is forecast to grow to over 47 million people to 2060, nearly tripling the estimated 16 to 18 million in 2011. The last official census was taken in 1970, and showed the total population as being 5.6 million. The first post-independence census is to be held in 2014.
It is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007. 11,400 of those refugees were originally from the Democratic Republic of Congo, who arrived in the 1970s. As of 2008 there were an estimated 400,000 Democratic Republic of the Congo migrant workers, at least 30,000 Portuguese, and about 259,000 Chinese living in Angola.
Since 2003, more than 400,000 Congolese migrants have been expelled from Angola. Prior to independence in 1975, Angola had a community of approximately 350,000 Portuguese; currently, there are about 200,000 who are registered with the consulates, and increasing due to the debt crisis in Portugal. The Chinese population stands at 258,920, mostly composed of temporary migrants.
The total fertility rate of Angola is 5.54 children born per woman (2012 estimates), the 11th highest in the world.
Languages.
The languages in Angola are those originally spoken by the different ethnic groups and Portuguese, introduced during the Portuguese colonial era. The indigenous languages with the largest usage are Umbundu, Kimbundu, and Kikongo, in that order. Portuguese is the official language of the country.
Mastery of the official language is probably more extended in Angola than it is elsewhere in Africa, and this certainly applies to its use in everyday life. Moreover, and above all, the proportion of native (or near native) speakers of the language of the former colonizer, turned official after independence, is no doubt considerably higher than in any other African country.
There are three intertwined historical reasons for this situation.
As a consequence of all this, the African “lower middle class” which at that stage formed in Luanda and other cities began to often prevent their children from learning the local African language, in order to guarantee that they learned Portuguese as their native language. At the same time, the white and “mestiço” population, where some knowledge of African languages could previously often been found, neglected this aspect more and more, to the point of frequently ignoring it totally.
After independence, these tendencies continued, and were even strengthened, under the rule of the MPLA which has its main social roots exactly in those social segments where the mastery of Portuguese as well as the proportion of native Portuguese speakers was highest. This became a political side issue, as FNLA and UNITA, given their regional constituencies, came out in favour of a greater attention to the African languages, and as the FNLA favoured French over Portuguese.
The dynamics of the language situation, as described above, were additionally fostered by the massive migrations triggered by the Civil War. Ovimbundu, the most populous ethnic group and the most affected by the war, appeared in great numbers in urban areas outside their areas, especially in Luanda and surroundings. At the same time, a majority of the Bakongo who had fled to the Democratic Republic of Congo in the early 1960s, or of their children and grandchildren, returned to Angola, but mostly did not settle in their original "habitat", but in the cities—and again above all in Luanda. As a consequence, more than half the population is now living in the cities which, from the linguistic point of view, have become highly heterogeneous. This means, of course, that Portuguese as the overall language of communication is by now of paramount importance, and that the role of the African languages is steadily decreasing among the urban population—a trend which is beginning to spread into rural areas as well.
The exact numbers of those fluent in Portuguese or who speak Portuguese as a first language are unknown, although a census is expected to be carried out in July–August 2013. Quite a number of voices demand the recognition of "Angolan Portuguese" as a specific variant, comparable to those spoken in Portugal or in Brazil. However, while there exists a certain number of idiomatic particularities in everyday Portuguese, as spoken by Angolans, it remains to be seen whether or not the Angolan government comes to the conclusion that these particularities constitute a configuration that justifies the claim to be a new language variant.
Religion.
There are about 1000 mostly Christian religious communities in Angola. While reliable statistics are nonexistent, estimates have it that more than half of the population are Catholics, while about a quarter adhere to the Protestant churches introduced during the colonial period: the Congregationalists mainly among the Ovimbundu of the Central Highlands and the coastal region to its West, the Methodists concentrating on the Kimbundu speaking strip from Luanda to Malanje, the Baptists almost exclusively among the Bakongo of the Northwest (now massively present in Luanda as well) and dispersed Adventists, Reformed and Lutherans. In Luanda and region there subsists a nucleus of the "syncretic" Tocoists and in the northwest a sprinkling of Kimbanguism can be found, spreading from the Congo/Zaire. Since independence, hundreds of Pentecostal and similar communities have sprung up in the cities, where by now about 50% of the population is living; several of these communities/churches are of Brazilian origin.
The U.S. Department of State estimates the Muslim population at 80,000–90,000, while the Islamic Community of Angola puts the figure closer to 500,000. Muslims consist largely of migrants from West Africa and the Middle East (especially Lebanon), although some are local converts. The Angolan government does not legally recognize any Muslim organizations and often shuts down mosques or prevents their construction.
In a study assessing nations' levels of religious regulation and persecution with scores ranging from 0 to 10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution.
Foreign missionaries were very active prior to independence in 1975, although since the beginning of the anti-colonial fight in 1961 the Portuguese colonial authorities expelled a series of Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments. Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them until 2002 from restoring many of their former inland mission stations.
The Catholic Church and some major Protestant denominations mostly keep to themselves in contrast to the "New Churches" which actively proselytize. Catholics, as well as some major Protestant denominations, provide help for the poor in the form of crop seeds, farm animals, medical care and education.
Culture.
In Angola, there is a Culture Ministry that is managed by Culture Minister Rosa Maria Martins da Cruz e Silva. Portugal has been present in Angola for 400 years, occupied the territory in the 19th and early 20th century, and ruled over it for about 50 years. As a consequence, both countries share cultural aspects: language (Portuguese) and main religion (Roman Catholic Christianity). The "substrate" of Angolan culture is African, mostly Bantu, while Portuguese culture has been imported. The diverse ethnic communities – the Ovimbundu, Ambundu, Bakongo, Chokwe, Mbunda and other peoples – maintain to varying degrees their own cultural traits, traditions and languages, but in the cities, where slightly more than half of the population now lives, a mixed culture has been emerging since colonial times – in Luanda since its foundation in the 16th century. In this urban culture, the Portuguese heritage has become more and more dominant. An African influence is evident in music and dance, and is moulding the way in which Portuguese is spoken, but is almost disappearing from the vocabulary. This process is well reflected in contemporary Angolan literature, especially in the works of Pepetela and Ana Paula Ribeiro Tavares.
Leila Lopes, Miss Angola 2011, was crowned Miss Universe 2011 in Brazil on 12 September 2011 making her the first Angolan to win the pageant.
Health.
Epidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country. Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates. Dengue, filariasis, leishmaniasis, and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region. Angola has one of the highest infant mortality rates in the world and one of the world's lowest life expectancies. A 2007 survey concluded that low and deficient niacin status was common in Angola. Demographic and Health Surveys is currently conducting several surveys in Angola on malaria, domestic violence and more.
In 2014, Angola launched a national campaign of vaccination against measles, extended to every child under ten years old and aiming to go to all 18 provinces in the country. The measure is part of the Strategic Plan for the Elimination of Measles 2014-2020 created by the Angolan Ministry of Health which includes strengthening routine immunization, a proper dealing with measles cases, national campaigns, introducing a second dose of vaccination in the national routine vaccination calendar and active epidemiological surveillance for measles. This campaign took place together with the vaccination against polio and vitamin A supplementation. 
Education.
Although by law education in Angola is compulsory and free for eight years, the government reports that a percentage of students are not attending due to a lack of school buildings and teachers. Students are often responsible for paying additional school-related expenses, including fees for books and supplies.
In 1999, the gross primary enrollment rate was 74 percent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 percent. Gross and net enrollment ratios are based on the number of students formally registered in primary school and therefore do not necessarily reflect actual school attendance. There continue to be significant disparities in enrollment between rural and urban areas. In 1995, 71.2 percent of children ages 7 to 14 years were attending school. It is reported that higher percentages of boys attend school than girls. During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding.
The Ministry of Education hired 20,000 new teachers in 2005 and continued to implement teacher trainings. Teachers tend to be underpaid, inadequately trained, and overworked (sometimes teaching two or three shifts a day). Some teachers may reportedly demand payment or bribes directly from their students. Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health prevent children from regularly attending school. Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded.
According to estimates by the UNESCO Institute for Statistics, the adult literacy rate in 2011 was 70.4%. 82.9% of males and 54.2% of women are literate as of 2001. Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes, and universities in Portugal, Brazil and Cuba through bilateral agreements; in general, these students belong to the elites.
In September 2014, the Angolan Ministry of Education announced an investment of 16 million Euros in the computerization of over 300 classrooms across the country. The project also includes training teachers at a national level, “as a way to introduce and use new information technologies in primary schools, thus reflecting an improvement in the quality of teaching.”
Sports.
Angola is the top basketball team of FIBA Africa, and a regular competitor at the Summer Olympic Games and the FIBA World Cup. The Angola national football team qualified for the 2006 FIFA World Cup, as this was their first appearance on the World Cup finals stage. They were eliminated after one defeat and two draws in the group stage. They won 3 COSAFA Cups and finished runner up in 2011 African Nations Championship. Angola has participated in the World Women's Handball Championship for several years. The country has also appeared in the Summer Olympics for seven years and both compete and have hosted the FIRS Roller Hockey World Cup. Angola is also often believed to have historic roots in the martial art "Capoeira Angola" and "Batuque" which were practiced by enslaved African Angolans transported as part of the Atlantic slave trade.

</doc>
<doc id="704" url="http://en.wikipedia.org/wiki?curid=704" title="Demographics of Angola">
Demographics of Angola

This article is about the demographic features of the population of Angola, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
Ethnically, there are three main groups, each speaking a Bantu language: the Ovimbundu who represent 37% of the population, the (Ambundu) with 25%, and the Bakongo 13%. Other numerically important groups include the closely interrelated Chokwe and Lunda, the Ganguela and Nhaneca-Humbe (in both cases classification terms that stand for a variety of small groups), the Ovambo, the Herero, the Xindonga and scattered residual groups of Khoisan. In addition, mixed race (European and African) people amount to about 2%, with a small (1%) population of whites, mainly ethnically Portuguese. 
As a former overseas territory of Portugal until 1975, Angola possesses a Portuguese population of over 200,000, a number that has been growing from 2000 onwards, because of Angola's growing demand for qualified human resources. Besides the Portuguese, significant numbers of people from other European and from diverse Latin American countries (especially Brazil) can be found. From the 2000s, many Chinese have settled and started up small businesses, while at least as many have come as workers for large enterprises (construction or other). Observers claim that the Chinese community in Angola might include as many as 300,000 persons at the end of 2010, but reliable statistics are not at this stage available. In 1974/75, over 25,000 Cuban soldiers arrived in Angola to help the MPLA forces during the decolonization conflict. Once this was over, a massive development cooperation in the field of health and education brought in numerous civil personnel from Cuba. However, only a very small percentage of all these people has remained in Angola, either for personal reasons (intermarriage) or as professionals (e.g., medical doctors).
The largest religious denomination is Roman Catholicism, to which adheres about half the population. Roughly 26% are followers of traditional forms of Protestantism (Congregationals, Methodists, Baptista, Lutherans, Reformed), but over the last decades there has in addition been a growth of Pentecostal communities and African Initiated Churches. In 2006, one out of 221 people were Jehovah's Witnesses. Blacks from Mali, Nigeria and Senegal are mostly Sunnite Muslims, but do not make up more than 1 - 2% of the population. By now few Angolans retain African traditional religions following different ethnic faiths.
Population.
According to the 2010 revison of the World Population Prospects the total population was 19 082 000 in 2010, compared to only 4 148 000 in 1950. The proportion of children below the age of 15 in 2010 was 46.6%, 50.9% was between 15 and 65 years of age, while 2.5% was 65 years or older
Vital statistics.
Registration of vital events is in Angola not complete. The Population Department of the United Nations prepared the following estimates.
Fertility and Births.
Total Fertility Rate (TFR) and Crude Birth Rate (CBR):
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population.
Population growth.
The population is growing by 2.184% annually. There are 44.51 births and 24.81 deaths per 1,000 citizens. The net migration rate is 2.14 migrants per 1,000 citizens. The fertility rate of Angola is 5.97 children born per woman as of 2011. The infant mortality rate is 184.44 deaths for every 1,000 live births with 196.55 deaths for males and 171.72 deaths for females for every 1,000 live births. Life expectancy at birth is 37.63 years; 36.73 years for males and 38.57 years for females.
Health.
According to the CIA World Factbook, 2% of adults (aged 15–49) are living with HIV/AIDS (as of 2009). The risk of contracting disease is very high. There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.
Ethnic groups.
37% of Angolans are Ovimbundu, 25% are Ambundu, 13% are Bakongo, 2% are mestiço, 1-2% are white Africans, and people from other ethnicities make up 22% of Angola's population.
Religions.
Angola is a majority Christian country. Reliable statistics don't exist, but well over 80% belong in principle to a Christian church or community, although many of them don't practice their religion and are in fact non believers. More than half of the Christians (whether practising or not) are Roman Catholic, the remaining ones comprising members of traditional Protestant churches as well as of new, often Pentecostal communities. Only 1 - 2% are Muslims - generally immmigrants from other African countries. Traditional indigenous religions are practized by a very small minority, generally in peripheral rural societies; however, some traditional beliefs are held by a substantial number of Christians.
Education.
Literacy is quite low, with 67.4% of the population over the age of 15 able to read and write in Portuguese. 82.9% of males and 54.2% of women are literate as of 2001.
Languages.
Portuguese is the official language of Angola, but Bantu and other African languages are also widely spoken. In fact, Kikongo, Kimbundu, Umbundu, Tuchokwe, Nganguela, and Ukanyama have the official status of "national languages". The mastery of Portuguese is widespread; in the cities the overwhelming majority are either fluent in Portuguese or have at least a reasonable working knowledge of this language; an increasing minority are native Portuguese speakers and have a poor, if any, knowledge of an African language.

</doc>
<doc id="705" url="http://en.wikipedia.org/wiki?curid=705" title="Politics of Angola">
Politics of Angola

Since the adoption of a new constitution in 2010, the politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in the President, the government and parliament.
Angola changed from a one-party Marxist-Leninist system ruled by the Popular Movement for the Liberation of Angola (MPLA), in place since independence in 1975, to a multiparty democracy based on a new constitution adopted in 1992. That same year the first parliamentary and presidential elections were held. The MPLA won an absolute majority in the parliamentary elections. In the presidential elections, President José Eduardo dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%. A runoff election would have been necessary, but never took place. The renewal of civil war immediately after the elections, which were considered as fraudulent by UNITA, and the collapse of the Lusaka Protocol, created a split situation. To a certain degree the new democratic institutions worked, notably the National Assembly, with the active participation of UNITA's and the FNLA's elected MPs - while José Eduardo dos Santos continued to exercise his functions without democratic legitimation. However the armed forces of the MPLA (now the official armed forces of the Angolan state) and of UNITA fought each other until the leader of UNITA, Jonas Savimbi, was killed in action in 2002.
From 2002 to 2010, the system as defined by the constitution of 1992 functioned in a relatively normal way. The executive branch of the government was composed of the President, the Prime Minister and Council of Ministers. The Council of Ministers, composed of all ministers and vice ministers, met regularly to discuss policy issues. Governors of the 18 provinces were appointed by and served at the pleasure of the president. The Constitutional Law of 1992 established the broad outlines of government structure and the rights and duties of citizens. The legal system was based on Portuguese and customary law but was weak and fragmented. Courts operated in only 12 of more than 140 municipalities. A Supreme Court served as the appellate tribunal; a Constitutional Court with powers of judicial review was never constituted despite statutory authorization. In practice, power was more and more concentrated in the hands of the President who, supported by an ever increasing staff, largely controlled parliament, government, and the judiciary.
The 26-year long civil war has ravaged the country's political and social institutions. The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million. Daily conditions of life throughout the country and specifically Luanda (population approximately 4 million) mirror the collapse of administrative infrastructure as well as many social institutions. The ongoing grave economic situation largely prevents any government support for social institutions. Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work.
The 2010 constitution grants the President almost absolute power. Elections for the National assembly are to take place every five years, and the President is automatically the leader of the winning party or coalition. It is for the President to appoint (and dismiss) all of the following:
The President is also provided a variety of powers, like defining the policy of the country. Even though it's not up to him/her to make laws (only to promulgate them and make edicts), the President is the leader of the winning party.
The only "relevant" post that is not directly appointed by the President is the Vice-President, which is the second in the winning party.
Legislative branch.
The National Assembly ("Assembleia Nacional") has 223 members, elected for a four year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad. The next general elections, due for 1997, have been rescheduled for 5 September 2008. The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats). The elections however have been described as only partly free but certainly not fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
Political parties and elections.
Parliamentary elections were held in September 2008. These elections were the first since 1992. Presidential elections are planned for 2009.
Judicial branch.
Supreme Court (or "Tribunal da Relacao") judges of the Supreme Court are appointed by the president.
Administrative divisions.
Angola has eighteen provinces (provincias, singular - provincia); Bengo, Benguela, Bie, Cabinda, Cuando Cubango, Cuanza Norte, Cuanza Sul, Cunene, Huambo, Huila, Luanda, Lunda Norte, Lunda Sul, Malanje, Moxico, Namibe, Uige, Zaire
Political pressure groups and leaders.
Front for the Liberation of the Enclave of Cabinda or FLEC (Henrique N'zita Tiago; António Bento Bembe)
International organization participation.
African, Caribbean and Pacific Group of States, AfDB, CEEAC, United Nations Economic Commission for Africa, FAO, Group of 77, IAEA, IBRD, ICAO, International Criminal Court (signatory), ICFTU, International Red Cross and Red Crescent Movement, International Development Association, IFAD, IFC, IFRCS, International Labour Organization, International Monetary Fund, International Maritime Organization, Interpol, IOC, International Organization for Migration, ISO (correspondent), ITU, Non-Aligned Council (temporary), UNCTAD, UNESCO, UNIDO, UPU, World Customs Organization, World Federation of Trade Unions, WHO, WIPO, WMO, WToO, WTrO

</doc>
<doc id="706" url="http://en.wikipedia.org/wiki?curid=706" title="Economy of Angola">
Economy of Angola

The Economy of Angola is one of the fastest-growing economies in the world, with the Economist asserting that for 2001 to 2010, Angolas' Annual average GDP growth was 11.1 percent. It is still recovering from the Angolan Civil War that plagued Angola from independence in 1975 until 2002. Despite extensive oil and gas resources, diamonds, hydroelectric potential, and rich agricultural land, Angola remains poor, and a third of the population relies on subsistence agriculture. Since 2002, when the 27-year civil war ended, the country has worked to repair and improve ravaged infrastructure and weakened political and social institutions. High international oil prices and rising oil production have contributed to the very strong economic growth since 1998, but corruption and public-sector mismanagement remain, particularly in the oil sector, which accounts for over 50 percent of GDP, over 90 percent of export revenue, and over 80 percent of government revenue.
History.
Portugal's explorers and settlers founded trading posts and forts along the coast of Africa beginning in the 15th century, and reached the Angolan coast in the 16th century. Portuguese explorer Paulo Dias de Novais founded Luanda in 1575 as "São Paulo de Loanda", and the region developed as a slave trade market with the help of local Imbangala and Mbundu peoples who were notable slave hunters. Trade was mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the ports of Luanda and Benguela. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong Brazilian influence was also exercised by the Jesuits in religion and education. War gradually gave way to the philosophy of trade. The great trade routes and the agreements that made them possible were the driving force for activities between the different areas; warlike states become states ready to produce and to sell. In the Planalto (the high plains), the most important states were those of Bié and Bailundo, the latter being noted for its production of foodstuffs and rubber. The colonial power, Portugal, becoming ever richer and more powerful, would not tolerate the growth of these neighbouring states and subjugated them one by one, so that by the beginning of this century the Portuguese had complete control over the entire area. During the period of the Iberian Union (1580–1640), Portugal lost influence and power and made new enemies. The Dutch, a major enemy of Castile, invaded many Portuguese overseas possessions, including Luanda. The Dutch ruled Luanda from 1640 to 1648 as Fort Aardenburgh. They were seeking black slaves for use in sugarcane plantations of Northeastern Brazil (Pernambuco, Olinda, Recife) which they had also seized from Portugal. John Maurice, Prince of Nassau-Siegen, conquered the Portuguese possessions of Saint George del Mina, Saint Thomas, and Luanda, Angola, on the west coast of Africa. After the dissolution of the Iberian Union in 1640, Portugal would reestablish its authority over the lost territories of the Portuguese Empire.
The Portuguese started to develop townships, trading posts, logging camps and small processing factories. From 1764 onwards, there was a gradual change from a slave-based society to one based on production for domestic consumption and export. Meanwhile, with the independence of Brazil in 1822, the slave trade was abolished in 1836, and in 1844 Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Mainland Portugal, full of trading companies, exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products. Maize, tobacco, dried meat and cassava flour also began to be produced locally. The Angolan bourgeoisie was born. From the 1920s to the 1960s, strong economic growth, abundant natural resources and development of infrastructure, led to the arrival of even more Portuguese settlers.
The Portuguese discovered petroleum in Angola in 1955. Production began in the Cuanza basin in the 1950s, in the Congo basin in the 1960s, and in the exclave of Cabinda in 1968. The Portuguese government granted operating rights for Block Zero to the Cabinda Gulf Oil Company, a subsidiary of ChevronTexaco, in 1955. Oil production surpassed the exportation of coffee as Angola's largest export in 1973.
A leftist military-led coup d'état, started on April 25, 1974, in Lisbon, overthrew the Marcelo Caetano government in Portugal, and promised to hand over power to an independent Angolan government. Mobutu Sese Seko, the President of Zaire, met with António de Spínola, the transitional President of Portugal, on September 15, 1974 on Sal island in Cape Verde, crafting a plan to empower Holden Roberto of the National Liberation Front of Angola, Jonas Savimbi of UNITA, and Daniel Chipenda of the MPLA's eastern faction at the expense of MPLA leader Agostinho Neto while retaining the façade of national unity. Mobutu and Spínola wanted to present Chipenda as the MPLA head, Mobutu particularly preferring Chipenda over Neto because Chipenda supported autonomy for Cabinda. The Angolan exclave has immense petroleum reserves estimated at around 300 million tons (~300 kg) which Zaire, and thus the Mobutu government, depended on for economic survival. After independence thousands of white Portuguese left, most of them to Portugal and many travelling overland to South Africa. There was an immediate crisis because the indigenous African population lacked the skills and knowledge needed to run the country and maintain its well-developed infrastructure.
The Angolan government created Sonangol, a state-run oil company, in 1976. Two years later Sonangol received the rights to oil exploration and production in all of Angola. After independence from Portugal in 1975, Angola was ravaged by a horrific civil war between 1975 and 2002.
1990s.
United Nations Angola Verification Mission III and MONUA spent USD1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war. The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile. UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso. In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts. While the U.S. government gave USD250 million to UNITA between 1986 to 1991, UNITA made USD1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe. At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, China, and South Africa. While no arms shipment to the government violated the protocol, no country informed the U.N. Register on Conventional Weapons as required.
Despite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999. The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.
2000s.
An economic reform effort was launched in 1998. Angola ranked 160 out of 174 nations in the United Nations Human Development Index of 2000. In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP). The program formally lapsed in June 2001, but the IMF remains engaged. In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates. The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized. A privatization effort, prepared with World Bank assistance, has begun with the BCI bank. Nevertheless, a legacy of fiscal mismanagement and corruption persists. The civil war internally displaced 3.8 million people, 32% of the population, by 2001. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Angola produced over 3 million carats of diamonds per year in 2003, with its production expected to grow to 10 million carats per year by 2007. In 2004 China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure. The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade.
The construction industry is another sector taking advantage of the growing economy, with various housing projects stimulated by the government that created various initiatives for this. Examples are the program "Angola Investe" and the projects "Casa Feliz" or "Meña". However, not all public construction projects are functional; a case in point is Kilamba Kiaxi where a whole new satellite town of Luanda, consisting in the main of housing facilities for several hundreds of thousands of people, was completely inhibited for over four years because of the skyrocket prices but it completely sold out after the government decreased the original price and created mortgage plans at around the election time thus made it affordable for middle-class people. 
ChevronTexaco started pumping from Block 14 in January 2000, but production has decreased to in 2007 due to the poor quality of the oil. Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007.
Cabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.
Overview.
Despite its abundant natural resources, output per capita is among the world's lowest. Subsistence agriculture provides the main livelihood for 85% of the population. Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. With revenues booming from oil exports, the government has started to implement ambitious development programs in building roads and other basic infrastructure for the nation.
In the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food. Because of severe wartime conditions, including extensive planting of landmines throughout the countryside, agricultural activities have been brought to a near standstill. Some efforts to recover have gone forward, however, notably in fisheries. Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports. In sharp contrast to a bleak picture of devastation and bare subsistence is expanding oil production, now almost half of GDP and 90% of exports, at . Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade. Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits.
This is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.
Foreign trade.
Exports in 2004 reached US$10,530,764,911. The vast majority of Angola's exports, 92% in 2004, are petroleum products. US$ 785 million worth of diamonds, 7.5% of exports, were sold abroad that year. Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to China, in 2006. In the first quarter of 2008, Angola became the main exporter of oil to China. The rest of its petroleum exports go to Europe and Latin America. U.S. companies account for more than half the investment in Angola, with Chevron-Texaco leading the way. The U.S. exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum. Trade between Angola and South Africa exceeded USD 300 million in 2007. From the 2000s many Chinese have settled and started up businesses.
Resources.
Petroleum.
Angola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria in the 2000s. In January 2007 Angola became a member of OPEC. By 2010 production is expected to double the 2006 output level with development of deep-water offshore oil fields. Oil sales generated USD 1.71 billion in tax revenue in 2004 and now makes up 80% of the government's budget, a 5% increase from 2003, and 45% of GDP.
Chevron Corporation produces and receives , 27% of Angolan oil. Total S.A., Chevron Corporation, ExxonMobil, Eni, Petrobras, and BP also operate in the country.
Block Zero provides the majority of Angola's crude oil production with produced annually. The largest fields in Block Zero are Takula (Area A), Numbi (Area A), and Kokongo (Area B). Chevron operates in Block Zero with a 39.2% share. SONANGOL, the state oil company, Total, and Eni own the rest of the block. Chevron also operates Angola's first producing deepwater section, Block 14, with .
The United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output.
Angola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports. The U.S. imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991. The U.S. Government has invested USD $4 billion in Angola's petroleum sector.
Diamonds.
Angola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling. Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually. The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge.
The Angolan government loses $375 million annually from diamond smuggling. In 2003 the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006. Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 "Angola's Deadly Diamonds" report as plagued by "murders, beatings, arbitrary detentions and other human rights violations." Marques called on foreign countries to boycott Angola's "conflict diamonds".
Iron.
Under Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971. In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan. After independence in 1975, the Angolan Civil War (1975–2002) destroyed most of the territory's mining infrastructure. The redevelopment of the Angolan mining industry started in the late 2000s.

</doc>
<doc id="708" url="http://en.wikipedia.org/wiki?curid=708" title="Transport in Angola">
Transport in Angola

Transport in Angola comprises:
Railways.
There are three separate railway lines in Angola:
Reconstruction of these three lines began in 2005 and is expected to be completed by the end of the year 2012. The Benguela Railway already connects to the Democratic Republic of the Congo.
Highways.
Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. Whilst a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road.
The Angolan government has contracted the restoration of many of the country's roads, though. Many companies are coming into the country from China and surrounding nations to help improve road surfaces. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Progress to complete the road infrastructure is likely to take some decades, but substantial efforts are already being made in the right directions.
Pipelines.
In April 2012, the Zambian Development Agency (ZDA) and an Angolan company signed a memorandum of understanding (MoU) to build a multi-product pipeline from Lobito to Lusaka, Zambia, to deliver various refined products to Zambia.
Angola plans to build an oil refinery in Lobito in the coming years.
Ports and harbors.
The government plans to build a deep-water port at Barra do Dande, north of Luanda, in Bengo province near Caxito.
Airports.
History.
Angola had an estimated total of 43 airports as of 2004, of which 31 had paved runways as of 2005. There is an international airport at Luanda. International and domestic services are maintained by TAAG Angola Airlines, Aeroflot, British Airways, Brussels Airlines, Lufthansa, Air France, Air Namibia, Cubana, Ethiopian Airlines, Emirates, Delta Air Lines, Royal Air Maroc, Iberia, Hainan Airlines, Kenya Airways, South African Airways, TAP Air Portugal and several regional carriers. In 2003, domestic and international carriers carried 198,000 passengers. There are airstrips for domestic transport at Benguela, Cabinda, Huambo, Namibe, and Catumbela.
References.
"This article comes from the CIA World Factbook 2003."

</doc>
<doc id="709" url="http://en.wikipedia.org/wiki?curid=709" title="Angolan Armed Forces">
Angolan Armed Forces

The Angolan Armed Forces (Portuguese: "Forças Armadas Angolanas") are the military in Angola that succeeded the Armed Forces for the Liberation of Angola (FAPLA) following the abortive Bicesse Accord with the National Union for the Total Independence of Angola (UNITA) in 1991. As part of the peace agreement, troops from both armies were to be demilitarized and then integrated. Integration was never completed as UNITA went back to war in 1992. Later, consequences for UNITA members in Luanda were harsh with FAPLA veterans persecuting their erstwhile opponents in certain areas and reports of vigilantism.
The FAA is headed by Chief of Staff Geraldo Nunda since 2010, who reports to the Minister of Defense, currently Cândido Pereira Van-Dúnem.
There are three components, the Army ("Forças Armadas"), Navy ("Marinha de Guerra") and Air Force "Força Aérea Nacional Angolana". Reported total manpower in 2013 was about 107,000.
Angolan Army.
On August 1, 1974 a few months after a military coup d'état had overthrown the Lisbon regime and proclaimed its intention of granting independence to Angola, the MPLA announced the formation of FAPLA, which replaced the EPLA. By 1976 FAPLA had been transformed from lightly armed guerrilla units into a national army capable of sustained field operations.
In 1990-91, the Army had ten military regions and an estimated 73+ 'brigades', each with a mean strength of 1,000 and comprising inf, tank, APC, artillery, and AA units as required. The Library of Congress said in 1990 that '[t]he regular army's 91,500 troops were organized into more than seventy brigades ranging from 750 to 1,200 men each and deployed throughout the ten military regions. Most regions were commanded by lieutenant colonels, with majors as deputy commanders, but some regions were commanded by majors. Each region consisted of one to four provinces, with one or more infantry brigades assigned to it. The brigades were generally dispersed in battalion or smaller unit formations to protect strategic terrain, urban centers, settlements, and critical infrastructure such as bridges and factories. Counterintelligence agents were assigned to all field units to thwart UNITA infiltration. The army's diverse combat capabilities were indicated by its many regular and motorised infantry brigades with organic or attached armor, artillery, and air defense units; two militia infantry brigades; four antiaircraft artillery brigades; ten tank battalions; and six artillery battalions. These forces were concentrated most heavily in places of strategic importance and recurring conflict: the oil-producing Cabinda Province, the area around the capital, and the southern provinces where UNITA and South African forces operated.'
It was reported in 2011 that the army was by far the largest of the services with about 120,000 men and women. The Angolan Army has around 29,000 "ghost workers" who remain enrolled in the ranks of the FAA and therefore receive a salary.
In 2013, the International Institute for Strategic Studies reported that the FAA had six divisions, the 1st, 5th, and 6th with two or three infantry brigades, and the 2nd, 3rd, and 4th with five to six infantry brigades. The 4th Division included a tank regiment. A separate tank brigade and special forces brigade were also reported.
As of 2011, the IISS reported the ground forces had 42 armoured/infantry regiments ('detachments/groups - strength varies') and 16 infantry 'brigades'. These probably comprised infantry, tanks, APC, artillery, and AA units as required. Major equipment included over 140 main battle tanks, 600 reconnaissance vehicles, over 920 AFVs, infantry fighting vehicles, 298 howitzers.
It was reported on May 3, 2007, that the Special Forces Brigade of the Angolan Armed Forces (FAA) located at Cabo Ledo region, northern Bengo Province, would host a 29th anniversary celebration for the entire armed forces. The brigade was reportedly formed on 5 May 1978 and under the command at the time of Colonel Paulo Falcao.
Army Equipment.
The Army operates a large amount of Russian, Soviet and ex-Warsaw pact hardware. A large amount of its equipment was acquired in the 1980s and 1990s most likely because of hostilities with neighbouring countries and its civil war which lasted from November 1975 until 2002.
Infantry Weapons.
Many of Angola's weapons are of Portuguese colonial and Warsaw Pact origin. Jane's Information Group lists the following as in service:
Angolan Air Force.
The Angolan Air Force's personnel total about 8,000; its equipment includes six Russian-manufactured Sukhoi Su-27 fighter aircraft and transport planes. In 2002 one was lost during the civil war with UNITA forces.
In 1991, the Air Force/Air Defense Forces had 8,000 personnel and 90 combat capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters.
As of 2014, Angolas air force has a total of 270 aircraft, including 82 fighters and interceptors, 10 fixed-wing attack aircraft, 118 transport aircraft, 48 trainer aircraft and 13 attack helicopters.
Angolan Navy (Marinha de Guerra).
The Navy numbers about 2,500 and operates seven small patrol craft and barges.
The Angolan Navy (MGA) has been neglected and ignored as a military arm mainly due to the guerrilla struggle against the Portuguese and the nature of the civil war. From the early 1990s to the present the Angolan Navy has shrunk from around 4,200 personnel to around 1,000, resulting in the loss of skills and expertise needed to maintain equipment. In order to protect Angola’s 1 600 km long coastline, the Angolan Navy is undergoing modernisation but is still lacking in many ways. Portugal has been providing training through its Technical Military Cooperation (CTM) programme. The Navy is requesting procurement of a frigate, three corvettes, three offshore patrol vessel and additional fast patrol boats.
Most of the craft detailed are from the 1980s or earlier, but the navy acquired new boats from Spain and France in the 1990s. Germany will deliver Fast Attack Craft for border protection from 2011.
The navy also has several aircraft for maritime patrol:
Foreign deployments.
The FAPLA's main counterinsurgency effort was directed against UNITA in the southeast, and its conventional capabilities were demonstrated principally in the undeclared South African Border War. The FAPLA first performed its external assistance mission with the dispatch of 1,000 to 1,500 troops to São Tomé and Príncipe in 1977 to bolster the socialist regime of President Manuel Pinto da Costa. During the next several years, Angolan forces conducted joint exercises with their counterparts and exchanged technical operational visits. The Angolan expeditionary force was reduced to about 500 in early 1985.
The Angolan Armed Forces were controversially involved in training the armed forces of fellow Lusophone states Cape Verde and Guinea-Bissau. In the case of the latter, the 2012 Guinea-Bissau coup d'état was cited by the coup leaders as due to Angola's involvement in trying to "reform" the military in connivance with the civilian leadership.
A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville). A presence during the unrest in Côte d'Ivoire, 2010–2011, were not officially confirmed. However, the "Frankfurter Allgemeine Zeitung", citing "Jeune Afrique", said that among President Gbagbo's guards were 92 personnel of President Dos Santos's Presidential Guard Unit. Angola is basically interested in the participation of the FAA operations of the African Union and has formed special units for this purpose.
Further reading.
mining sector, BICC Focus, June 2013

</doc>
<doc id="710" url="http://en.wikipedia.org/wiki?curid=710" title="Foreign relations of Angola">
Foreign relations of Angola

The foreign relations of Angola are based on Angola's strong support of U.S. foreign policy as the Angolan economy is dependent on U.S. foreign aid.
From 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba. Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention. In 1993, it established formal diplomatic relations with the United States. It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south. Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government. It also has intervened in the Republic of the Congo (Brazzaville) to support the existing government in that country.
Since 1998, Angola has successfully worked with the United Nations Security Council to impose and carry out sanctions on UNITA. More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA. At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular.
Sub-Saharan Africa.
Cape Verde.
Cape Verde signed a friendship accord with Angola in December 1975, shortly after Angola gained its independence. Cape Verde and Guinea-Bissau served as stop-over points for Cuban troops on their way to Angola to fight UNITA rebels and South African troops. Prime Minister Pedro Pires sent FARP soldiers to Angola where they served as the personal bodyguards of Angolan President José Eduardo dos Santos.
Democratic Republic of the Congo.
Many thousands of Angolans fled the country after the civil war. More than 20,000 people were forced to leave the Democratic Republic of the Congo in 2009, an action the DR Congo said was in retaliation for regular expulsion of Congolese diamond miners who were in Angola illegally. Angola sent a delegation to DR Congo's capital Kinshasa and succeeded in stopping government-forced expulsions which had become a "tit-for-tat" immigration dispute. "Congo and Angola have agreed to suspend expulsions from both sides of the border," said Lambert Mende, DR Congo information minister, in October 2009. "We never challenged the expulsions themselves; we challenged the way they were being conducted — all the beating of people and looting their goods, even sometimes their clothes," Mende said.
Namibia.
Namibia borders Angola to the south. In 1999 Namibia signed a mutual defense pact with its northern neighbor Angola.
This affected the Angolan Civil War that had been ongoing since Angola's independence in 1975. Namibia's ruling party SWAPO sought to support the ruling party MPLA in Angola against the rebel movement UNITA, whose stronghold is in southern Angola, bordering to Namibia. The defence pact allowed Angolan troops to use Namibian territory when attacking Jonas Savimbi's UNITA.
Nigeria.
Angolan-Nigerian relations are primarily based on their roles as oil exporting nations. Both are members of the Organization of the Petroleum Exporting Countries, the African Union and other multilateral organizations.
South Africa.
Angola-South Africa relations are quite strong as the ruling parties in both nations, the African National Congress in South Africa and the MPLA in Angola, fought together during the Angolan Civil War and South African Border War. They fought against UNITA rebels, based in Angola, and the apartheid-era government in South Africa who supported them. Nelson Mandela mediated between the MPLA and UNITA factions during the last years of Angola's civil war.
Zimbabwe.
Angola-Zimbabwe relations have remained cordial since the birth of both states, Angola in 1975 and Zimbabwe in 1979, during the Cold War. While Angola's foreign policy shifted to a pro-U.S. stance based on substantial economic ties, under the rule of President Robert Mugabe Zimbabwe's ties with the West soured in the late 1990s.
Guinea-Bissau.
Following a request by the government of Guinea-Bissau, Angola sent there a contingent of about 300 troops meant to help putting an end to the political-military unrest in that country, and to reorganize the local military forces. In fact, these troops were perceived as a kinf of Pretorian Guard for the ruling party, PAIGC. In the beginning of April 2012, when a new military Coup d'état was under preparation, the Angolan regime decided to withdraw its military mission from Guinea-Bissau.
Europe.
France.
Relations between the two countries have not always been cordial due to the former French government's policy of supporting militant separatists in Angola's Cabinda province and the international Angolagate scandal embarrassed both governments by exposing corruption and illicit arms deals. Following French President Nicolas Sarkozy's visit in 2008, relations have improved.
Portugal.
Angola-Portugal relations have significantly improved since the Angolan government abandoned communism and nominally embraced democracy in 1991, embracing a pro-U.S. and to a lesser degree pro-Europe foreign policy. Portugal ruled Angola for 400 years, colonizing the territory from 1483 until independence in 1975. Angola's war for independence did not end in a military victory for either side, but was suspended as a result of a coup in Portugal that replaced the Caetano regime.
Russia.
Russia has an embassy in Luanda. Angola has an embassy in Moscow and an honorary consulate in Saint Petersburg. Angola and the precursor to Russia, the Soviet Union, established relations upon Angola's independence.
Serbia.
The Defence Minister of Serbia, Dragan Šutanovac, stated in a 2011 meeting in Luanda that Serbia would negotiate with the Angolan military authorities for the construction of a new military hospital in Angola.
Latin America.
Brazil.
Commercial and economic ties dominate the relations of each country. Parts of both countries were part of the Portuguese Empire from the early 16th century until Brazil's independence in 1822. As of November 2007, "trade between the two countries is booming as never before"
Cuba.
During Angola's civil war Cuban forces fought to install a Marxist-Leninist MPLA-PT government, against Western-backed UNITA and FLNA guerrillas and the South-African army.
Mexico.
Relations between Angola and Mexico, have become of increasing priority due to the cultural similarities between the two nations. Angola has an embassy in Mexico City, and Mexico has a non-resident embassy based in Pretoria, South Africa which is accredited to Angola.
United States.
From the mid-1980s through at least 1992, the United States was the primary source of military and other support for the UNITA rebel movement, which was led from its creation through 2002 by Jonas Savimbi. The U.S. refused to recognize Angola diplomatically during this period.
Relations between the United States of America and the Republic of Angola (formerly the People's Republic of Angola) have warmed since Angola's ideological renunciation of Marxism before the 1992 elections.
Asia.
Israel.
Angola-Israel relations, primarily based on trade and pro-United States foreign policies, are excellent. In March 2006, the trade volume between the two countries amounted to $400 million. The Israeli ambassador to Angola is Avraham Benjamin.[1] In 2005, President José Eduardo dos Santos visited Israel.
Japan.
As of 2007, economic relations played "a fundamental role in the bilateral relations between the two governments". Japan has donated towards demining following the civil war.
People's Republic of China.
Chinese Prime Minister Wen Jiabao visited Angola in June 2006, offering a US$9 billion loan for infrastructure improvements in return for petroleum. The PRC has invested heavily in Angola since the end of the civil war in 2002. João Manuel Bernardo, the current ambassador of Angola to China, visited the PRC in November 2007.
In February 2006, Angola surpassed Saudi Arabia to become the number one supplier of oil to China. 
Vietnam.
Angola-Vietnam relations were established in August 1971, four years before Angola gained its independence, when future President of Angola Agostinho Neto visited Vietnam. Angola and Vietnam have steadfast partners as both transitioned from Cold War-era foreign policies of international communism to pro-Western pragmatism following the fall of the Soviet Union.

</doc>
<doc id="711" url="http://en.wikipedia.org/wiki?curid=711" title="Albert Sidney Johnston">
Albert Sidney Johnston

Albert Sidney Johnston (February 2, 1803 – April 6, 1862) served as a general in three different armies: the Texian ("i.e.", Republic of Texas) Army, the United States Army, and the Confederate States Army. He saw extensive combat during his military career, fighting actions in the Texas War of Independence, the Mexican-American War, the Utah War, and the American Civil War.
Considered by Confederate President Jefferson Davis to be the finest general officer in the Confederacy before the emergence of Robert E. Lee, he was killed early in the Civil War at the Battle of Shiloh. Johnston was the highest-ranking officer, Union or Confederate, killed during the entire war. Davis believed the loss of Johnston "was the turning point of our fate".
Johnston was unrelated to Confederate general Joseph E. Johnston.
Early life and education.
Johnston was born in Washington, Kentucky, the youngest son of Dr. John and Abigail (Harris) Johnston. His father was a native of Salisbury, Connecticut. Although Albert Johnston was born in Kentucky, he lived much of his life in Texas, which he considered his home. He was first educated at Transylvania University in Lexington, where he met fellow student Jefferson Davis. Both were appointed to the United States Military Academy, Davis two years behind Johnston. In 1826 Johnston graduated eighth of 41 cadets in his class from West Point with a commission as a brevet second lieutenant in the 2nd U.S. Infantry.
Johnston was assigned to posts in New York and Missouri and served in the Black Hawk War in 1832 as chief of staff to Bvt. Brig. Gen. Henry Atkinson.
Marriage and family.
In 1829 he married Henrietta Preston, sister of Kentucky politician and future civil war general William Preston. They had one son, William Preston Johnston, who became a colonel in the Confederate Army. The senior Johnston resigned his commission in 1834 to return to Kentucky to care for his dying wife, who succumbed two years later to tuberculosis.
After serving as Secretary of War for the Republic of Texas from 1838 to 1840, Johnston resigned and returned to Kentucky. In 1843, he married Eliza Griffin, his late wife's first cousin. The couple moved to Texas, where they settled on a large plantation in Brazoria County. Johnston named the property China Grove. Here they raised Johnston's two children from his first marriage, and the first three children born to him and Eliza. (A sixth child was born later when they lived in Los Angeles, California).
Texas Army.
In April 1834, Johnston took up farming in Texas. In 1836, he enlisted as a private in the Texas Army during the Texas War of Independence against the Republic of Mexico. One month later, Johnston was promoted to major and the position of aide-de-camp to General Sam Houston. He was named Adjutant General as a colonel in the Republic of Texas Army on August 5, 1836. On January 31, 1837, he became senior brigadier general in command of the Texas Army.
On February 7, 1837, he fought in a duel with Texas Brig. Gen. Felix Huston, as they challenged each other for the command of the Texas Army; Johnston refused to fire on Huston and lost the position after he was wounded in the pelvis.
On December 22, 1838, Mirabeau B. Lamar, the second president of the Republic of Texas, appointed Johnston as Secretary of War. He provided for the defense of the Texas border against Mexican invasion, and in 1839 conducted a campaign against Indians in northern Texas. In February 1840, he resigned and returned to Kentucky.
U.S. Army.
Johnston returned to the Texas Army during the Mexican-American War under General Zachary Taylor as a colonel of the 1st Texas Rifle Volunteers. The enlistments of his volunteers ran out just before the Battle of Monterrey. Johnston convinced a few volunteers to stay and fight as he served as the inspector general of volunteers and fought at the battles of Monterrey and Buena Vista.
He remained on his plantation after the war until he was appointed by President Zachary Taylor to the U.S. Army as a major and was made a paymaster in December 1849. He served in that role for more than five years, making six tours, and traveling more than annually on the Indian frontier of Texas. He served on the Texas frontier at Fort Mason and elsewhere in the West.
In 1855 President Franklin Pierce appointed him colonel of the new 2nd U.S. Cavalry (the unit that preceded the modern 5th U.S.), a new regiment, which he organized. On August 19, 1856, Gen. Persifor Smith, at the request of Kansas Territorial Governor Wilson Shannon, sent Col. Johnston with 1300 men of the 2d Cavalry Dragoons from Fort Riley, troops from Jefferson Barracks, a battalion of the 6th Infantry and Capt. Howe’s artillery company, to protect the territorial capital at Lecompton from an imminent attack by Jim Lane and his abolitionist "Army of the North." As a key figure in the Utah War, Johnston led U.S. troops who established a non-Mormon government in the formerly Mormon territory. He received a brevet promotion to brigadier general in 1857 for his service in Utah. He spent 1860 in Kentucky until December 21, when he sailed for California to take command of the Department of the Pacific.
Civil War.
At the outbreak of the Civil War, Johnston was the commander of the U.S. Army Department of the Pacific in California. Like many regular army officers from the South, he was opposed to secession. But he resigned his commission soon after he heard of the secession of his adopted state Texas. It was accepted by the War Department on May 6, 1861, effective May 3. On April 28 he moved to Los Angeles, the home of his wife's brother John Griffin. Considering staying in California with his wife and five children, Johnston remained there until May.
Soon, under suspicion by local Union officials, he evaded arrest and joined the Los Angeles Mounted Rifles as a private, leaving Warner's Ranch May 27. He participated in their trek across the southwestern deserts to Texas, crossing the Colorado River into the Confederate Territory of Arizona on July 4, 1861.
Early in the Civil War, Confederate President Jefferson Davis decided that the Confederacy would attempt to hold as much of its territory as possible, and he distributed its military forces around its borders and coasts. In the summer of 1861, Davis appointed several generals to defend Confederate lines from the Mississippi River east to the Allegheny Mountains.
The most sensitive, and in many ways the most crucial areas, along the Mississippi River and in western Tennessee along the Tennessee and the Cumberland rivers were placed under the command of Maj. Gen. Leonidas Polk and Brig. Gen. Gideon J. Pillow. The latter had initially been in command in Tennessee as that State's top general. Their impolitic occupation of Columbus, Kentucky on September 3, 1861, two days before Johnston arrived in the Confederacy's capital of Richmond, Virginia, after his cross–country journey, drove Kentucky from its stated neutrality. The majority of Kentuckians allied with the Union camp. Polk and Pillow's action gave Union Brig. Gen. Ulysses S. Grant an excuse to take control of the strategically located town of Paducah, Kentucky without raising the ire of most Kentuckians and the pro-Union majority in the State legislature.
Confederate command in Western Theater.
On September 10, 1861, Johnston was assigned to command the huge area of the Confederacy west of the Allegheny Mountains, except for coastal areas. He became commander of the Confederacy's western armies in the area often called the Western Department or Western Military Department. After his appointment, Johnston immediately headed for his new territory. He was permitted to call on governors of Arkansas, Tennessee and Mississippi for new troops, although this authority was largely stifled by politics, especially with respect to Mississippi. On September 13, 1861, Johnston ordered Brig. Gen. Felix Zollicoffer with 4,000 men to occupy Cumberland Gap in Kentucky in order to block Union troops from coming into eastern Tennessee. The Kentucky legislature had voted to side with the Union after the occupation of Columbus by Polk. By September 18, Johnston had Brig. Gen. Simon Bolivar Buckner with another 4,000 men blocking the railroad route to Tennessee at Bowling Green, Kentucky.
Johnston had fewer than 40,000 men spread throughout Kentucky, Tennessee, Arkansas and Missouri. Of these, 10,000 were in Missouri under Missouri State Guard Maj. Gen. Sterling Price. Johnston did not quickly gain many recruits when he first requested them from the governors, but his more serious problem was lacking sufficient arms and ammunition for the troops he already had. As the Confederate government concentrated efforts on the units in the East, they gave Johnston small numbers of reinforcements and minimal amounts of arms and material. Johnston maintained his defense by conducting raids and other measures to make it appear he had larger forces than he did, a strategy that worked for several months. Johnston's tactics had so annoyed and confused Union Brig. Gen. William Tecumseh Sherman that he became somewhat unnerved, overestimated Johnston's forces, and had to be relieved by Brig. Gen. Don Carlos Buell on November 9, 1861.
Battle of Mill Springs.
Eastern Tennessee was held for the Confederacy by two unimpressive brigadier generals appointed by Jefferson Davis, Felix Zollicoffer, a brave but untrained and inexperienced officer, and soon to be Maj. Gen. George B. Crittenden, a former U.S. Army officer with apparent alcohol problems. While Crittenden was away in Richmond, Zollicoffer moved his forces to the north bank of the upper Cumberland River near Mill Springs (now Nancy, Kentucky), putting the river to his back and his forces into a trap. Zollicoffer decided it was impossible to obey orders to return to the other side of the river because of scarcity of transport and proximity of Union troops. When Union Brig. Gen. George H. Thomas moved against the Confederates, Crittenden decided to attack one of the two parts of Thomas's command at Logan's Cross Roads near Mill Springs before the Union forces could unite. On January 19, 1862, the ill-prepared Confederates, after a night march in the rain, attacked the Union force with some initial success. As the battle progressed, Zollicoffer was killed, Crittenden was unable to lead the Confederate force (he may have been intoxicated), and the Confederates were turned back and routed by a Union bayonet charge, suffering 533 casualties from their force of 4,000. The Confederate troops who escaped were assigned to other units as Crittenden faced an investigation of his conduct.
After this Confederate defeat at the Battle of Mill Springs, Davis sent Johnston a brigade and a few other scattered reinforcements. He also assigned him Gen. P.G.T. Beauregard, who was supposed to attract recruits because of his victories early in the war, and act as a competent subordinate for Johnston . The brigade was led by Brig. Gen. John B. Floyd, considered incompetent. He took command at Fort Donelson as the senior general present just before Union Brig. Gen. Ulysses S. Grant attacked the fort. Historians believe the assignment of Beauregard to the west stimulated Union commanders to attack the forts before Beauregard could make a difference in the theater. Union officers heard that he was bringing 15 regiments with him, but this was an exaggeration of his forces.
Fort Henry, Fort Donelson, Nashville.
Based on the assumption that Kentucky neutrality would act as a shield against a direct invasion from the north, Tennessee initially had sent men to Virginia and concentrated defenses in the Mississippi Valley, circumstances that no longer applied in September 1861. Even before Johnston arrived in Tennessee, construction of two forts had been started to defend the Tennessee and the Cumberland rivers, which provided avenues into the State from the north. Both forts were located in Tennessee in order to respect Kentucky neutrality, but these were not in ideal locations. Fort Henry on the Tennessee River was in an unfavorable low–lying location, commanded by hills on the Kentucky side of the river. Fort Donelson on the Cumberland River, although in a better location, had a vulnerable land side and did not have enough heavy artillery to defend against gunboats.
Maj. Gen. Polk ignored the problems of the forts when he took command. After Johnston took command, Polk at first refused to comply with Johnston's order to send an engineer, Lt. Joseph K. Dixon, to inspect the forts. After Johnston asserted his authority, Polk had to allow Dixon to proceed. Dixon recommended that the forts be maintained and strengthened, although they were not in ideal locations, because much work had been done on them and the Confederates might not have time to build new ones. Johnston accepted his recommendations. Johnston wanted Major, later Lt. Gen., Alexander P. Stewart to command the forts but President Davis appointed Brig. Gen. Lloyd Tilghman as commander.
To prevent Polk from dissipating his forces by allowing some men to join a partisan group, Johnston ordered him to send Brig. Gen. Gideon Pillow and 5,000 men to Fort Donelson. Pillow took up a position at nearby Clarksville, Tennessee and did not move into the fort until February 7, 1862. Alerted by a Union reconnaissance on January 14, 1862, Johnston ordered Tilghman to fortify the high ground opposite Fort Henry, which Polk had failed to do despite Johnston's orders. Tilghman failed to act decisively on these orders, which in any event were too late to be adequately carried out.
Gen. Beauregard arrived at Johnston's headquarters at Bowling Green on February 4, 1862 and was given overall command of Polk's force at the western end of Johnston's line at Columbus, Kentucky. On February 6, 1862, Union Navy gunboats quickly reduced the defenses of ill-sited Fort Henry, inflicting 21 casualties on the small remaining Confederate force. Brig. Gen. Lloyd Tilghman surrendered the 94 remaining officers and men of his approximately 3,000-man force which had not been sent to Fort Donelson before U.S. Grant's force could even take up their positions. Johnston knew he could be trapped at Bowling Green if Fort Donelson fell, so he moved his force to Nashville, the capital of Tennessee and an increasingly important Confederate industrial center, beginning on February 11, 1862.
Johnston also reinforced Fort Donelson with 12,000 more men, including those under Floyd and Pillow, a curious decision in view of his thought that the Union gunboats alone might be able to take the fort. He did order the commanders of the fort to evacuate the troops if the fort could not be held. The senior generals sent to the fort to command the enlarged garrison, Gideon J. Pillow and John B. Floyd, squandered their chance to avoid having to surrender most of the garrison and on February 16, 1862, Brig. Gen. Simon Buckner, having been abandoned by Floyd and Pillow, surrendered Fort Donelson. Colonel Nathan Bedford Forrest escaped with his cavalry force of about 700 men before the surrender. The Confederates suffered about 1,500 casualties with an estimated 12,000 to 14,000 taken prisoner. Union casualties were 500 killed, 2,108 wounded, 224 missing.
Johnston, who had little choice in allowing Floyd and Pillow to take charge at Fort Donelson on the basis of seniority after he ordered them to add their forces to the garrison, took the blame and suffered calls for his removal because a full explanation to the press and public would have exposed the weakness of the Confederate position. His passive defensive performance while positioning himself in a forward position at Bowling Green, spreading his forces too thinly, not concentrating his forces in the face of Union advances, and appointing or relying upon inadequate or incompetent subordinates subjected him to criticism at the time and by later historians. The fall of the forts exposed Nashville to imminent attack, and it fell without resistance to Union forces under Brig. Gen. Buell on February 25, 1862, two days after Johnston had to pull his forces out in order to avoid having them captured as well.
Concentration at Corinth.
Johnston had various remaining military units scattered throughout his territory and retreating to the south to avoid being cut off. Johnston himself retreated with the force under his personal command, the Army of Central Kentucky, from the vicinity of Nashville. With Beauregard's help, Johnston decided to concentrate forces with those formerly under Polk and now already under Beauregard's command at the strategically located railroad crossroads of Corinth, Mississippi, which he reached by a circuitous route. Johnston kept the Union forces, now under the overall command of the ponderous Maj. Gen. Henry Halleck, confused and hesitant to move, allowing Johnston to reach his objective undetected. This delay allowed Jefferson Davis finally to send reinforcements from the garrisons of coastal cities and another highly rated but prickly general, Braxton Bragg, to help organize the western forces. Bragg at least calmed the nerves of Beauregard and Polk who had become agitated by their apparent dire situation in the face of numerically superior forces before the arrival of Johnston on March 24, 1862.
Johnston's army of 17,000 men gave the Confederates a combined force of about 40,000 to 44,669 men at Corinth. On March 29, 1862, Johnston officially took command of this combined force, which continued to use the Army of the Mississippi name under which it had been organized by Beauregard on March 5.
Johnston now planned to defeat the Union forces piecemeal before the various Union units in Kentucky and Tennessee under Grant with 40,000 men at nearby Pittsburg Landing, Tennessee, and the now Maj. Gen. Don Carlos Buell on his way from Nashville with 35,000 men, could unite against him. Johnston started his army in motion on April 3, 1862, intent on surprising Grant's force as soon as the next day, but they moved slowly due to their inexperience, bad roads and lack of adequate staff planning. Johnston's army was finally in position within a mile or two of Grant's force, and undetected, by the evening of April 5, 1862.
Battle of Shiloh and death.
Johnston launched a massive surprise attack with his concentrated forces against Grant at the Battle of Shiloh on April 6, 1862. As the Confederate forces overran the Union camps, Johnston seemed to be everywhere, personally leading and rallying troops up and down the line on his horse. At about 2:30 p.m., while leading one of those charges against a Union camp near the "Peach Orchard", he was wounded, taking a bullet behind his right knee. He apparently did not think the wound was serious at the time, or even possibly did not feel it, and so he sent his personal physician away to attend to some wounded captured Union soldiers instead. It is possible that Johnston's duel in 1837 had caused nerve damage or numbness to his right leg and that he did not feel the wound to his leg as a result. The bullet had in fact clipped a part of his popliteal artery and his boot was filling up with blood. Within a few minutes, Johnston was observed by his staff to be nearly fainting. Among his staff was Isham G. Harris, the Governor of Tennessee, who had ceased to make any real effort to function as governor after learning that Abraham Lincoln had appointed Andrew Johnson as military governor of Tennessee. Seeing Johnston slumping in his saddle and his face turning deathly pale, Harris asked: "General, are you wounded?" Johnston glanced down at his leg wound, then faced Harris and replied in a weak voice his last words: "Yes... and I fear seriously." Harris and other staff officers removed Johnston from his horse and carried him to a small ravine near the "Hornets Nest" and desperately tried to aid the general by trying to make a tourniquet for his leg wound, but little could be done by this point since he had already lost so much blood. He soon lost consciousness and bled to death a few minutes later. It is believed that Johnston may have lived for as long as one hour after receiving his fatal wound. Harris and the other officers wrapped General Johnston's body in a blanket so as not to damage the troops' morale with the sight of the dead general. Johnston and his wounded horse, named Fire Eater, were taken to his field headquarters on the Corinth road, where his body remained in his tent until the Confederate Army withdrew to Corinth the next day, April 7, 1862. From there, his body was taken to the home of Colonel William Inge, which had been his headquarters in Corinth. It was covered in the Confederate flag and lay in state for several hours.
It is probable that a Confederate soldier fired the fatal round. No Union soldiers were observed to have ever gotten behind Johnston during the fatal charge, while it is known that many Confederates were firing at the Union lines while Johnston charged well in advance of his soldiers.
Johnston was the highest-ranking casualty of the war on either side, and his death was a strong blow to the morale of the Confederacy. Jefferson Davis considered him the best general in the country; this was two months before the emergence of Robert E. Lee as the pre-eminent general of the Confederacy.
Legacy and honors.
Johnston was survived by his wife Eliza and six children. His wife and five younger children, including one born after he went to war, chose to live out their days at home in Los Angeles with Eliza's brother, Dr. John Strother Griffin. Johnston's eldest son, Albert Sidney Jr. (born in Texas), had already followed him into the Confederate States Army. In 1863, after taking home leave in Los Angeles, Albert Jr. was on his way out of San Pedro harbor on a ferry. While a steamer was taking on passengers from the ferry, a wave swamped the smaller boat, causing its boilers to explode. Johnston, Jr. was killed in the accident.
Killed in action, General Johnston received the highest praise ever given by the Confederate government; fulsome accounts were published, on December 20, 1862 and thereafter, in the Los Angeles "Star" of his family's hometown. Johnston Street, Hancock Street, and Griffin Avenue, each in northeast Los Angeles, are named after the general and his family, who lived in the neighborhood.
Johnston was initially buried in New Orleans. In 1866, a joint resolution of the Texas Legislature was passed to have his body moved and reinterred at the Texas State Cemetery in Austin. The re-interment occurred in 1867. Forty years later, the state appointed Elisabet Ney to design a monument and sculpture of him to be erected at the gravesite.
The Texas Historical Commission has erected a historical marker near the entrance of what was once Johnston's plantation. An adjacent marker was erected by the San Jacinto Chapter of the Daughters of The Republic of Texas and the Lee, Roberts, and Davis Chapter of the United Daughters of the Confederate States of America.
The University of Texas at Austin has recognized Johnston with a statue on the South Mall.

</doc>
<doc id="713" url="http://en.wikipedia.org/wiki?curid=713" title="Android (robot)">
Android (robot)

An android is a robot or synthetic organism designed to look and act like a human, especially one with a body having a flesh-like resemblance. Until recently, androids have largely remained within the domain of science fiction, frequently seen in film and television. However, advancements in robot technology have allowed the design of functional and realistic humanoid robots.
Etymology.
The word was coined from the Greek root ἀνδρ- 'man' and the suffix "" 'having the form or likeness of'.
The "Oxford English Dictionary" traces the earliest use (as "Androides") to Ephraim Chambers' "Cyclopaedia," in reference to an automaton that St. Albertus Magnus allegedly created. The term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons. The term "android" was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886). This story features an artificial humanlike robot named Hadaly. As said by the officer in the story, "In this age of Realien advancement, who knows what goes on in the mind of those responsible for these mechanical dolls." The term made an impact into English pulp science fiction starting from Jack Williamson's "The Cometeers" (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future (1940–1944).
Although Karel Čapek's robots in "R.U.R. (Rossum's Universal Robots)" (1921)—the play that introduced the word "robot" to the world—were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings. The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts.
The term "droid", coined by George Lucas for the original ' film and now used widely within science fiction, originated as an abridgment of "android", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word "android" was used in ' episode "What Are Little Girls Made Of?" The abbreviation "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?", has seen some further usage, such as within the TV series "Total Recall 2070".
Authors have used the term "android" in more diverse ways than "robot" or "cyborg". In some fictional works, the difference between a robot and android is only their appearance, with androids being made to look like humans on the outside but with robot-like internal mechanics. In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation. Other fictional depictions of androids fall somewhere in between.
Eric G. Wilson, who defines androids as a "synthetic human being", distinguishes between three types of androids, based on their body's composition:
Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence).
Projects.
Japan.
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and Kokoro Co., Ltd. have demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan. In 2006, Kokoro Co. developed a new "DER 2" android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The "air servosystem" which Kokoro Co. developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. Once programmed, it is able to choreograph its motions and gestures with its voice.
The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is "working" at the Science University of Tokyo as a guide.
The Waseda University (Japan) and NTT Docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2". It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses its face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.
Korea.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is tall and weighs , matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot". EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won, of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
United States.
Walt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history (see video at ). Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works. In 2005, the PKD android won a first place artificial intelligence award from AAAI.
United Kingdom.
In 2001, Steve Grand OBE, creator of the computer game "Creatures", created an android, or anthropoid; he named it Lucy. The intention was that she would have to learn everything, including how to use her mechanical vocal chords to speak. Her systems were made to be similar to a human's.
Use in fiction.
Androids are a staple of science fiction. Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series "I, Robot" and "Foundation and Empire". One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved. Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.
The tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions. Some android heroes seek, like Pinocchio, to become human, as in the films "Bicentennial Man", "Hollywood", "Enthiran" and "A.I. Artificial Intelligence", or Data in "". Others, as in the film "Westworld", rebel against abuse by careless humans. Android hunter Deckard in "Do Androids Dream of Electric Sheep?" and its film adaptation "Blade Runner" discovers that his targets are, in some ways, more human than he is. Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human.
One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Blade Runner". Perhaps the clearest example of this is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human. More recently, the androids Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other".
Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman". Examples include the Greek myth of "Pygmalion" and the female robot Maria in Fritz Lang's "Metropolis". Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires," or as submissive, servile companions, such as in "The Stepford Wives". Fiction about gynoids has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.

</doc>
<doc id="717" url="http://en.wikipedia.org/wiki?curid=717" title="Alberta">
Alberta

Alberta () is a province of Canada. With a population of 3,645,257 in 2011 and an estimated population of 4,121,692 as of July 1, 2014, it is Canada's fourth-most populous province and most populous of Canada's three prairie provinces. Alberta and its neighbour, Saskatchewan, were established as provinces on September 1, 1905. The current Premier of the province is Jim Prentice.
Alberta is in western Canada and is one of Canada's three Prairie Provinces. It is bounded by the provinces of British Columbia to the west and Saskatchewan to the east, the Northwest Territories to the north, and the U.S. state of Montana to the south. Alberta is one of three Canadian provinces and territories to border only a single U.S. state and is also one of only two provinces that are landlocked.
Edmonton, the capital city of Alberta, is located near the geographic centre of the province and is the primary supply and service hub for Canada's crude oil, oil sands (Athabasca oil sands) and other northern resource industries. Approximately south of the capital is Calgary, Alberta's largest city. Calgary and Edmonton centre Alberta's two census metropolitan areas, both of which have populations exceeding one million, while the province has 16 census agglomerations. Notable tourist destinations in the province include Banff, Canmore, Drumheller, Jasper and Sylvan Lake.
Etymology.
Alberta is named after Princess Louise Caroline Alberta (1848–1939), the fourth daughter of Victoria, the Queen of Canada and Albert, Prince Consort. Princess Louise was the wife of the Marquess of Lorne, Governor General of Canada from 1878 to 1883. Lake Louise and Mount Alberta were also named in honour of Princess Louise.
Geography.
Alberta, with an area of , is the fourth largest province after Quebec, Ontario, and British Columbia. To the south, the province borders on the 49th parallel north, separating it from the U.S. state of Montana, while on the north the 60th parallel north divides it from the Northwest Territories. To the east, the 110th meridian west separates it from the province of Saskatchewan, while on the west its boundary with British Columbia follows the 120th meridian west south from the Northwest Territories at 60°N until it reaches the Continental Divide at the Rocky Mountains, and from that point follows the line of peaks marking the Continental Divide in a generally southeasterly direction until it reaches the Montana border at 49°N.
The province extends north to south and east to west at its maximum width. Its highest point is at the summit of Mount Columbia in the Rocky Mountains along the southwest border, while its lowest point is on the Slave River in Wood Buffalo National Park in the northeast.
With the exception of the semi-arid steppe of the southeastern section, the province has adequate water resources. There are numerous rivers and lakes used for swimming, fishing and a range of water sports. There are three large lakes, Lake Claire () in Wood Buffalo National Park, Lesser Slave Lake (), and Lake Athabasca () which lies in both Alberta and Saskatchewan. The longest river in the province is the Athabasca River which travels from the Columbia Icefield in the Rocky Mountains to Lake Athabasca. The largest river is the Peace River with an average flow of 2161 m3/s. The Peace River originates in the Rocky Mountains of northern British Columbia and flows through northern Alberta and into the Slave River, a tributary of the Mackenzie River.
Alberta's capital city, Edmonton, is located approximately in the geographic centre of the province. It is the most northerly major city in Canada, and serves as a gateway and hub for resource development in northern Canada. The region, with its proximity to Canada's largest oil fields, has most of western Canada's oil refinery capacity. Calgary is located approximately south of Edmonton and north of Montana, surrounded by extensive ranching country. Almost 75% of the province's population lives in the Calgary–Edmonton Corridor. The land grant policy to the railroads served as a means to populate the province in its early years.
Most of the northern half of the province is boreal forest, while the Rocky Mountains along the southwestern boundary are largely forested (see Alberta Mountain forests and Alberta-British Columbia foothills forests). The southern quarter of the province is prairie, ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it. The central aspen parkland region extending in a broad arc between the prairies and the forests, from Calgary, north to Edmonton, and then east to Lloydminster, contains the most fertile soil in the province and most of the population. Much of the unforested part of Alberta is given over either to grain or to dairy farming, with mixed farming more common in the north and centre, while ranching and irrigated agriculture predominate in the south.
The Alberta badlands are located in southeastern Alberta, where the Red Deer River crosses the flat prairie and farmland, and features deep canyons and striking landforms. Dinosaur Provincial Park, near Brooks, Alberta, showcases the badlands terrain, desert flora, and remnants from Alberta's past when dinosaurs roamed the then lush landscape.
Climate.
Alberta has a dry continental climate with warm summers and cold winters. The province is open to cold arctic weather systems from the north, which often produce extremely cold conditions in winter. As the fronts between the air masses shift north and south across Alberta, the temperature can change rapidly. Arctic air masses in the winter produce extreme minimum temperatures varying from in northern Alberta to in southern Alberta. In the summer, continental air masses produce maximum temperatures from in the mountains to in southern Alberta.
Alberta extends for over from north to south; its climate, therefore, varies considerably. Average high temperatures in January range from in the southwest to in the far north. The climate is also influenced by the presence of the Rocky Mountains to the southwest, which disrupt the flow of the prevailing westerly winds and causes them to drop most of their moisture on the western slopes of the mountain ranges before reaching the province, casting a rain shadow over much of Alberta. The northerly location and isolation from the weather systems of the Pacific Ocean cause Alberta to have a dry climate with little moderation from the ocean. Annual precipitation ranges from in the southeast to in the north, except in the foothills of the Rocky Mountains where total precipitation including snowfall can reach annually. The province is the namesake of the Alberta clipper, a type of intense, fast-moving winter storm that generally forms over or near the province and pushed with great speed by the continental polar jetstream descends over the rest of Southern Canada and the northern tier of the United States.
In the summer, the average daytime temperatures range from around in the Rocky Mountain valleys and far north, up to around in the dry prairie of the southeast. The northern and western parts of the province experience higher rainfall and lower evaporation rates caused by cooler summer temperatures. The south and east-central portions are prone to drought-like conditions sometimes persisting for several years, although even these areas can receive heavy precipitation and sometimes resulting in flooding.
Alberta is a sunny province. Annual bright sunshine totals range between 1900 up to just under 2600 hours per year. Northern Alberta gets about 18 hours of daylight in the summer.
In southwestern Alberta, the cold winters are frequently interrupted by warm, dry chinook winds blowing from the mountains, which can propel temperatures upward from frigid conditions to well above the freezing point in a very short period. During one chinook recorded at Pincher Creek, temperatures soared from in just one hour. The region around Lethbridge has the most chinooks, averaging 30 to 35 chinook days per year. Calgary has a 56% chance of a white Christmas, while Edmonton has an 86% chance.
Northern Alberta is mostly covered by boreal forest and has a subarctic climate. The agricultural area of southern Alberta have a semi-arid steppe climate because the annual precipitation is less than the water that evaporates or is used by plants. The southeastern corner of Alberta, part of the Palliser Triangle, experiences greater summer heat and lower rainfall than the rest of the province, and as a result suffers frequent crop yield problems and occasional severe droughts. Western Alberta is protected by the mountains and enjoys the mild temperatures brought by winter chinook winds. Central and parts of northwestern Alberta in the Peace River region are largely aspen parkland, a biome transitional between prairie to the south and boreal forest to the north.
After Southern Ontario, The Prairies and Central Alberta is the most likely region in Canada to experience tornadoes. Thunderstorms, some of them severe, are frequent in the summer, especially in central and southern Alberta. The region surrounding the Calgary–Edmonton Corridor is notable for having the highest frequency of hail in Canada, which is caused by orographic lifting from the nearby Rocky Mountains, enhancing the updraft/downdraft cycle necessary for the formation of hail.
Ecology.
Flora.
In central and northern Alberta the arrival of spring is marked by the early flowering of the prairie crocus anemone; this member of the buttercup family has been recorded flowering as early as March though April is the usual month for the general population. Other prairie flora known to flower early are the golden bean and wild rose. Members of the sunflower family blossom on the prairie in the summer months between July and September. The southern and east central parts of Alberta are covered by short prairie grass, which dries up as summer lengthens, to be replaced by hardy perennials such as the prairie coneflower, fleabane, and sage. Both yellow and white sweet clover can be found throughout the southern and central areas of the province.
The trees in the parkland region of the province grow in clumps and belts on the hillsides. These are largely deciduous, typically aspen, poplar, and willow. Many species of willow and other shrubs grow in virtually any terrain. On the north side of the North Saskatchewan River evergreen forests prevail for thousands of square kilometres. Aspen poplar, balsam poplar (or cottonwood), and paper birch are the primary large deciduous species. Conifers include Jack pine, Rocky Mountain pine, Lodgepole pine, both white and black spruce, and the deciduous conifer tamarack.
Fauna.
The four climatic regions (alpine, boreal forest, parkland, and prairie) of Alberta are home to many different species of animals. The south and central prairie was the land of the bison, commonly known as buffalo, its grasses providing pasture and breeding ground for millions of buffalo. The buffalo population was decimated during early settlement, but since then buffalo have made a comeback, living on farms and in parks all over Alberta.
Alberta is home to many large carnivores. Among them are the grizzly and black bears, which are found in the mountains and wooded regions. Smaller carnivores of the canine and feline families include coyotes, wolves, fox, lynx, bobcat and mountain lion (cougar).
Herbivorous animals are found throughout the province. Moose, mule deer, elk, and white-tail deer are found in the wooded regions, and pronghorn can be found in the prairies of southern Alberta. Bighorn sheep and mountain goats live in the Rocky Mountains. Rabbits, porcupines, skunks, squirrels and many species of rodents and reptiles live in every corner of the province. Alberta is home to only one variety of venomous snake, the prairie rattlesnake.
Central and northern Alberta and the region farther north is the nesting ground of many migratory birds. Vast numbers of ducks, geese, swans and pelicans arrive in Alberta every spring and nest on or near one of the hundreds of small lakes that dot northern Alberta. Eagles, hawks, owls and crows are plentiful, and a huge variety of smaller seed and insect-eating birds can be found. Alberta, like other temperate regions, is home to mosquitoes, flies, wasps, and bees. Rivers and lakes are populated with pike, walleye, whitefish, rainbow, speckled, brown trout, and sturgeon. Bull Trout, native to the province, is the Alberta's provincial fish. Turtles are found in some water bodies in the southern part of the province. Frogs and salamanders are a few of the amphibians that make their homes in Alberta.
Alberta is the only province in Canada—as well as one of the few places in the world—that is free of Norwegian rats. Since the early 1950s, the Government of Alberta has operated a rat-control program, which has been so successful that only isolated instances of wild rat sightings are reported, usually of rats arriving in the province aboard trucks or by rail. In 2006, Alberta Agriculture reported zero findings of wild rats; the only rat interceptions have been domesticated rats that have been seized from their owners. It is illegal for individual Albertans to own or keep Norwegian rats of any description; the animals can only be kept in the province by zoos, universities and colleges, and recognized research institutions. In 2009, several rats were
found and captured, in small pockets in southern Alberta, putting Alberta's rat-free status in jeopardy. A colony of rats were subsequently found in a landfill near Medicine Hat in 2012, and again in 2014.
Paleontology.
Alberta has one of the greatest diversities and abundances of Late Cretaceous dinosaur fossils in the world. Taxa are represented by complete fossil skeletons, isolated material, microvertebrate remains, and even mass graves. At least 38 dinosaur type specimens were collected in the province. The Foremost Formation, Oldman Formation and Dinosaur Park Formations collectively comprise the Judith River Group and are the most thoroughly studied dinosaur-bearing strata in Alberta.
Dinosaur-bearing strata are distributed widely throughout Alberta. The Dinosaur Provincial Park area contains outcrops of the Dinosaur Park Formation and Oldman Formation. In the central and southern regions of Alberta are intermittent Scollard Formation outcrops. In the Drumheller Valley and Edmonton regions there are exposed Horseshoe Canyon facies. Other formations have been recorded as well, like the Milk River and Foremost Formations. However, these latter two have a lower diversity of documented dinosaurs, primarily due to their lower total fossil quantity and neglect from collectors who are hindered by the isolation and scarcity of exposed outcrops. Their dinosaur fossils are primarily teeth recovered from microvertebrate fossil sites. Additional geologic formations that have produced only few fossils are the Belly River Group and St. Mary River Formations of the southwest and the northwestern Wapiti Formation. The Wapiti Formations contains two "Pachyrhinosaurus" bone beds that break its general trend of low productivity, however. The Bearpaw Formation represents strata deposited during a marine transgression. Dinosaurs are known from this Formation, but represent specimens washed out to sea or reworked from older sediments.
History.
Paleo-Indians arrived in Alberta at least 10,000 years ago, toward the end of the last ice age. They are thought to have migrated from Siberia to Alaska on a land bridge across the Bering Strait, and then may have moved down the east side of the Rocky Mountains through Alberta to settle the Americas. Others may have migrated down the coast of British Columbia and then moved inland. Over time they differentiated into various First Nations peoples, including the Plains Indian tribes of southern Alberta such as those of the Blackfoot Confederacy and the Plains Cree, who generally lived by hunting buffalo (American bison), and the more northerly tribes such as the Woodland Cree and Chipewyan who hunted, trapped, and fished for a living.
After the British arrival in Canada, approximately half of the province of Alberta, south of the Athabasca River drainage, became part of Rupert's Land which consisted of all land drained by rivers flowing into Hudson Bay. This area was granted by Charles II of England to the Hudson's Bay Company (HBC) in 1670, and rival fur trading companies were not allowed to trade in it. After the arrival of French Canadians in the west around 1731, they settled near fur trading posts, establishing communities such as Lac La Biche and Bonnyville. Fort La Jonquière was established near what is now Calgary in 1752.
The Athabasca River and the rivers north of it were not in HBC territory because they drained into the Arctic Ocean instead of Hudson Bay, and they were prime habitat for fur-bearing animals. The first explorer of the Athabasca region was Peter Pond, who learned of the Methye Portage, which allowed travel from southern rivers into the rivers north of Rupert's Land. Fur traders formed the North West Company (NWC) of Montreal to compete with the HBC in 1779. The NWC occupied the northern part of Alberta territory. Peter Pond built Fort Athabasca on Lac la Biche in 1778. Roderick Mackenzie built Fort Chipewyan on Lake Athabasca ten years later in 1788. His cousin, Sir Alexander Mackenzie, followed the North Saskatchewan River to its northernmost point near Edmonton, then setting northward on foot, trekked to the Athabasca River, which he followed to Lake Athabasca. It was there he discovered the mighty outflow river which bears his name—the Mackenzie River—which he followed to its outlet in the Arctic Ocean. Returning to Lake Athabasca, he followed the Peace River upstream, eventually reaching the Pacific Ocean, and so he became the first European to cross the North American continent north of Mexico.
The extreme southernmost portion of Alberta was part of the French (and Spanish) territory of Louisiana, sold to the United States in 1803; in 1818, the portion of Louisiana north of the Forty-Ninth Parallel was ceded to Great Britain.
Fur trade expanded in the north, but bloody battles occurred between the rival HBC and NWC, and in 1821 the British government forced them to merge to stop the hostilities. The amalgamated Hudson's Bay Company dominated trade in Alberta until 1870, when the newly formed Canadian Government purchased Rupert's Land. Northern Alberta was included in the North-Western Territory until 1870, when it and Rupert's land became Canada's Northwest Territories.
The District of Alberta was created as part of the North-West Territories in 1882. As settlement increased, local representatives to the North-West Legislative Assembly were added. After a long campaign for autonomy, in 1905 the District of Alberta was enlarged and given provincial status, with the election of Alexander Cameron Rutherford as the first premier.
On June 21, 2013, during the 2013 Alberta floods Alberta experienced heavy rainfall that triggered catastrophic flooding throughout much of the southern half of the province along the Bow, Elbow, Highwood and Oldman rivers and tributaries. A dozen municipalities in Southern Alberta declared local states of emergency on June 21 as water levels rose and numerous communities were placed under evacuation orders.
Demographics.
In the 2011 census, Alberta had a population of 3,645,257 living in 1,390,275 of its 1,505,007 total dwellings, a 10.8% change from its 2006 population of 3,290,350. With a land area of , it had a population density of in 2011. In 2013, Statistics Canada estimated the province's population to be 4,025,073.
Alberta has experienced a relatively high rate of growth in recent years, mainly because of its burgeoning economy. Between 2003 and 2004, the province had high birthrates (on par with some larger provinces such as British Columbia), relatively high immigration, and a high rate of interprovincial migration when compared to other provinces.
Approximately 81% of the population live in urban areas and only about 19% live in rural areas. The Calgary-Edmonton Corridor is the most urbanized area in the province and is one of the most densely populated areas of Canada. Many of Alberta's cities and towns have also experienced very high rates of growth in recent history. Over the past century, Alberta's population rose from 73,022 in 1901 to 2,974,807 in 2001 and 3,290,350 according to the 2006 census.
The 2006 census found that English, with 2,576,670 native speakers, was the most common mother tongue of Albertans, representing 79.99% of the province's population. The next most common mother tongues were various Chinese languages with 97,275 native speakers (3.02%), followed by German with 84,505 native speakers (2.62%) and French with 61,225 (1.90%).
Other mother tongues (in decreasing order) include: Punjabi, with 36,320 native speakers (1.13%); Tagalog, with 29,740 (0.92%); Ukrainian, with 29,455 (0.91%); Spanish, with 29,125 (0.90%); Polish, with 21,990 (0.68%); Arabic, with 20,495 (0.64%); Dutch, with 19,980 (0.62%); and Vietnamese, with 19,350 (0.60%). The most common aboriginal language is Cree 17,215 (0.53%). Other common mother tongues include Italian with 13,095 speakers (0.41%); Urdu with 11,275 (0.35%); and Korean with 10,845 (0.33%); then Hindi 8,985 (0.28%); Persian 7,700 (0.24%); Portuguese 7,205 (0.22%); and Hungarian 6,770 (0.21%).
Alberta has considerable ethnic diversity. In line with the rest of Canada, many immigrants originated from England, Scotland, Ireland and Wales, but large numbers also came from other parts of Europe, notably Germany, France, Ukraine and Scandinavia. According to Statistics Canada, Alberta is home to the second highest proportion (two percent) of Francophones in western Canada (after Manitoba). Despite this, relatively few Albertans claim French as their mother tongue. Many of Alberta's French-speaking residents live in the central and northwestern regions of the province.
As reported in the 2001 census, the Chinese represented nearly four percent of Alberta's population, and East Indians represented more than two percent. Both Edmonton and Calgary have historic Chinatowns, and Calgary has Canada's third largest Chinese community. The Chinese presence began with workers employed in the building of the Canadian Pacific Railway in the 1880s. Aboriginal Albertans make up approximately three percent of the population.
In the 2006 Canadian census, the most commonly reported ethnic origins among Albertans were: 885,825 English (27.2%); 679,705 German (20.9%); 667,405 Canadian (20.5%); 661,265 Scottish (20.3%); 539,160 Irish (16.6%); 388,210 French (11.9%); 332,180 Ukrainian (10.2%); 172,910 Dutch (5.3%); 170,935 Polish (5.2%); 169,355 North American Indian (5.2%); 144,585 Norwegian (4.4%); and 137,600 Chinese (4.2%). (Each person could choose as many ethnicities as were applicable.)"
Amongst those of British origins, the Scots have had a particularly strong influence on place-names, with the names of many cities and towns including Calgary, Airdrie, Canmore, and Banff having Scottish origins.
Alberta is the third most diverse province in terms of visible minorities after British Columbia and Ontario with 13.9% of the population consisting of visible minorities. Nearly one-fourth of the populations of Calgary and Edmonton belong to a visible minority group.
Aboriginal Identity Peoples make up 5.8% of the population, about half of whom consist of North American Indians and the other half are Metis. There are also small number of Inuit people in Alberta. The number of Aboriginal Identity Peoples have been increasing at a rate greater than the population of Alberta.
As of the Canada 2001 Census the largest religious group was Roman Catholic, representing 25.7% of the population. Alberta had the second highest percentage of non-religious residents in Canada (after British Columbia) at 23.1% of the population. Of the remainder, 13.5% of the population identified themselves as belonging to the United Church of Canada, while 5.9% were Anglican. Lutherans made up 4.8% of the population while Baptists comprised 2.5%.
The remainder belonged to a wide variety of different religious affiliations, none of which constituted more than 2% of the population. The LDS Church of Alberta reside primarily in the extreme south of the province and made up 1.7% of the population. Alberta has a population of Hutterites, a communal Anabaptist sect similar to the Mennonites (Hutterites represented 0.4% of the population while Mennonites were 0.8%), and has a significant population of Seventh-day Adventists at 0.3%. Alberta is home to several Byzantine Rite Churches as part of the legacy of Eastern European immigration, including the Ukrainian Catholic Eparchy of Edmonton, and the Ukrainian Orthodox Church of Canada's Western Diocese which is based in Edmonton.
Muslims, Sikhs, and Hindus live in Alberta. Muslims constituted 1.7% of the population, Sikhs 0.8% and Hindus 0.5%. Many of these are recent immigrants, but others have roots that go back to the first settlers of the prairies. Canada's oldest mosque, the Al-Rashid Mosque, is located in Edmonton, whereas Calgary is home to Canada's largest mosque, the Baitun Nur mosque. Jews constituted 0.4% of Alberta's population. Most of Alberta's 13,000 Jews live in Calgary (7,500) and Edmonton (5,000).
Economy.
Alberta's economy is one of the strongest in Canada, supported by the burgeoning petroleum industry and to a lesser extent, agriculture and technology. The per capita GDP in 2007 was by far the highest of any province in Canada at C$74,825. This was 61% higher than the national average of C$46,441 and more than twice that of some of the Atlantic provinces. In 2006 the deviation from the national average was the largest for any province in Canadian history. According to the 2006 census, the median annual family income after taxes was $70,986 in Alberta (compared to $60,270 in Canada as a whole).
The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada. The region covers a distance of roughly 400 kilometres north to south. In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population). It is also one of the fastest growing regions in the country. A 2003 study by TD Bank Financial Group found the corridor to be the only Canadian urban centre to amass a U.S. level of wealth while maintaining a Canadian style quality of life, offering universal health care benefits. The study found that GDP per capita in the corridor was 10% above average U.S. metropolitan areas and 40% above other Canadian cities at that time.
The Fraser Institute states that Alberta also has very high levels of economic freedom and rates Alberta as the freest economy in Canada, and the second freest economy amongst U.S. states and Canadian provinces. The government of Alberta has invested its earnings wisely; as of 30 September 2013, official statistics reported nearly 500 holdings.
Industry.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in Canada. Alberta is the world’s second largest exporter of natural gas and the fourth largest producer. Two of the largest producers of petrochemicals in North America are located in central and north-central Alberta. In both Red Deer and Edmonton, polyethylene and vinyl manufacturers produce products that are shipped all over the world. Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton.
The Athabasca oil sands surrounding Fort McMurray have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be 1.6 trillion barrels (254 km3). Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands. As of late 2006 there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta.
Another factor determining the viability of oil extraction from the oil sands is the price of oil. The oil price increases since 2003 have made it profitable to extract this oil, which in the past would give little profit or even a loss. By mid-2014 however rising costs and stabilizing oil prices were threatening the economic viability of some projects. An example of this was the shelving of the Joslyn north project in the Athabasca region in May 2014.
With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid crystal display systems. With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Agriculture and forestry.
Agriculture has a significant position in the province's economy. The province has over three million head of cattle, and Alberta beef has a healthy worldwide market. Nearly one half of all Canadian beef is produced in Alberta. Alberta is one of the top producers of plains buffalo (bison) for the consumer market. Sheep for wool and mutton are also raised.
Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production; other grains are also prominent. Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation. Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion. Across the province, the once common grain elevator is slowly being lost as rail lines are decreasing; farmers typically truck the grain to central points.
Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed. Hybrid canola also requires bee pollination, and some beekeepers service this need.
The vast northern forest reserves of softwood allow Alberta to produce large quantities of lumber, oriented strand board (OSB) and plywood, and several plants in northern Alberta supply North America and the Pacific Rim nations with bleached wood pulp and newsprint.
Tourism.
Alberta has been a tourist destination from the early days of the twentieth century, with attractions including outdoor locales for skiing, hiking and camping, shopping locales such as West Edmonton Mall, Calgary Stampede, outdoor festivals, professional athletic events, international sporting competitions such as the Commonwealth Games and Olympic Games, as well as more eclectic attractions. There are also natural attractions like Elk Island National Park, Wood Buffalo National Park, and the Columbia Icefield.
According to Alberta Economic Development, Calgary and Edmonton both host over four million visitors annually. Banff, Jasper and the Rocky Mountains are visited by about three million people per year. Alberta tourism relies heavily on Southern Ontario tourists, as well as tourists from other parts of Canada, the United States, and many international countries.
Alberta's Rocky Mountains include well-known tourist destinations Banff National Park and Jasper National Park. The two mountain parks are connected by the scenic Icefields Parkway. Banff is located west of Calgary on Highway 1, and Jasper is located west of Edmonton on Yellowhead Highway. Five of Canada's fourteen UNESCO World heritage sites are located within the province: Canadian Rocky Mountain Parks, Waterton-Glacier International Peace Park, Wood Buffalo National Park, Dinosaur Provincial Park and Head-Smashed-In Buffalo Jump.
About 1.2 million people visit the of Calgary Stampede, a celebration of Canada's own Wild West and the cattle ranching industry. About 700,000 people enjoy Edmonton's K-Days (formerly Klondike Days and Capital EX). Edmonton was the gateway to the only all-Canadian route to the Yukon gold fields, and the only route which did not require gold-seekers to travel the exhausting and dangerous Chilkoot Pass.
Another tourist destination that draws more than 650,000 visitors each year is the Drumheller Valley, located northeast of Calgary. Drumheller, "Dinosaur Capital of The World", offers the Royal Tyrrell Museum of Palaeontology. Drumheller also had a rich mining history being one of Western Canada's largest coal producers during the war years.
Located in east-central Alberta is Alberta Prairie Railway Excursions, a popular tourist attraction operated out of Stettler, that offers train excursions into the prairie and caters to tens of thousands of visitors every year.
Alberta has numerous ski resorts most notably Sunshine Village, Lake Louise, Marmot Basin, Norquay and Nakiska.
Government and politics.
The Government of Alberta is organized as a parliamentary democracy with a unicameral legislature. Its unicameral legislature—the Legislative Assembly—consists of eighty-seven members elected first past the post (FPTP) from single-member constituencies.
Locally municipal governments and school boards are elected and operate separately. Their boundaries do not necessarily coincide. Municipalities where the same body act as both local government and school board are formally referred to as "counties" in Alberta.
As Canada's head of state, Queen Elizabeth II is the head of state for the Government of Alberta. Her duties in Alberta are carried out by Lieutenant Governor Donald Ethell. The Queen and lieutenant governor are figureheads whose actions are highly restricted by custom and constitutional convention. The lieutenant governor handles numerous honourific duties in the name of the Queen. The government is headed by the premier. The current premier is Jim Prentice, who became the leader of the governing Progressive Conservatives on September 6, 2014. Prentice was sworn in as the 16th Premier of Alberta on September 15, 2014. The premier is normally a member of the Legislative Assembly, and he draws all the members of his Cabinet from among the members of the Legislative Assembly.
The City of Edmonton is the seat of the provincial government—the capital of Alberta.
Alberta's elections tend to yield results which are much more conservative than those of other Canadian provinces. Alberta has traditionally had three political parties, the Progressive Conservatives ("Conservatives" or "Tories"), the Liberals, and the social democratic New Democrats. A fourth party, the strongly conservative Social Credit Party, was a power in Alberta for many decades, but fell from the political map after the Progressive Conservatives came to power in 1971. Since that time, no other political party has governed Alberta. Only four parties have governed Alberta: the Liberals, from 1905 to 1921; the United Farmers of Alberta, from 1921 to 1935; the Social Credit Party, from 1935 to 1971, and the currently governing Progressive Conservative Party, from 1971 to the present.
Alberta has had occasional surges in separatist sentiment. Even during the 1980s, when these feelings were at their strongest, there has never been enough interest in secession to initiate any major movements or referendums. There are several currently active groups wishing to promote the independence of Alberta in some form.
The April 23, 2012 election returned the Progressive Conservative Party to government, making leader Alison Redford Alberta's first elected female premier. In the 2012 provincial election, held on April 23, 2012, the Progressive Conservative Party was re-elected as a majority government and party leader Alison Redford retained as premier with 43.9% of the vote and 61 of 87 seats (The Legislative Assembly added 4 seats, increasing the total to 87, with the 2012 election), the Wildrose Party led by Danielle Smith was elected as the Official Opposition with 34.3% of the vote and 17 members (replacing the Liberal Party), five Liberals were elected with 9.9% of the vote and four NDP members were elected with 9.8% of the vote.
Taxation.
Government revenue comes mainly from royalties on non-renewable natural resources (30.4%), personal income taxes (22.3%), corporate and other taxes (19.6%), and grants from the federal government primarily for infrastructure projects (9.8%). Albertans are the lowest-taxed people in Canada, and Alberta is the only province in Canada without a provincial sales tax (but residents are still subject to the federal sales tax, the Goods and Services Tax of 5%). It is also the only Canadian province to have a flat tax for personal income taxes, which is 10% of taxable income.
The Alberta personal income tax system maintains a progressive character by granting residents personal tax exemptions of $17,787, in addition to a variety of tax deductions for persons with disabilities, students, and the aged. Alberta's municipalities and school jurisdictions have their own governments who usually work in co-operation with the provincial government.
Alberta also privatized alcohol distribution. The privatization increased outlets from 304 stores to 1,726; 1,300 jobs to 4,000 jobs; and 3,325 products to 16,495 products. Tax revenue also increased from $400 million to $700 million.
Albertan municipalities raise a significant portion of their income through levying property taxes. The value of assessed property in Alberta was approximately $727 billion in 2011. Most real property is assessed according to its market value. The exceptions to market value assessment are farmland, railways, machinery & equipment and linear property, all of which is assessed by regulated rates. Depending on the property type, property owners may appeal a property assessment to their municipal 'Local Assessment Review Board', 'Composite Assessment Review Board,' or the Alberta Municipal Government Board.
Military.
Military bases in Alberta include Canadian Forces Base (CFB) Cold Lake, CFB Edmonton, CFB Suffield and CFB Wainwright. Air force units stationed at CFB Cold Lake have access to the Cold Lake Air Weapons Range. CFB Edmonton is the headquarters for the 3rd Canadian Division. CFB Suffield hosts British troops and is the largest training facility in Canada.
Transportation.
Alberta has over of highways and roads, of which nearly are paved. The main north-south corridor is Highway 2, which begins south of Cardston at the Carway border crossing and is part of the CANAMEX Corridor. Highway 4, which effectively extends Interstate 15 into Alberta and is the busiest U.S. gateway to the province, begins at the Coutts border crossing and ends at Lethbridge. Highway 3 joins Lethbridge to Fort Macleod and links Highway 4 to Highway 2. Highway 2 travels northward through Fort Macleod, Calgary, Red Deer, and Edmonton.
North of Edmonton, the highway continues to Athabasca, then northwesterly along the south shore of Lesser Slave Lake into High Prairie, north to Peace River, west to Fairview and finally south to Grande Prairie, where it ends at an interchange with Highway 43. The section of Highway 2 between Calgary and Edmonton has been named the Queen Elizabeth II Highway to commemorate the visit of the monarch in 2005. Highway 2 is supplemented by two more highways that run parallel to it: Highway 22, west of Highway 2, known as "Cowboy Trail", and Highway 21, east of Highway 2. Highway 43 travels northwest into Grande Prairie and the Peace River Country; Highway 63 travels northeast to Fort McMurray, the location of the Athabasca oil sands.
Alberta has two main east-west corridors. The southern corridor, part of the Trans-Canada Highway system, enters the province near Medicine Hat, runs westward through Calgary, and leaves Alberta through Banff National Park. The northern corridor, also part of the Trans-Canada network and known as the Yellowhead Highway (Highway 16), runs west from Lloydminster in eastern Alberta, through Edmonton and Jasper National Park into British Columbia. One of the most scenic drives is along the Icefields Parkway, which runs for between Jasper and Lake Louise, with mountain ranges and glaciers on either side of its entire length.
Another major corridor through central Alberta is Highway 11 (also known as the David Thompson Highway), which runs east from the Saskatchewan River Crossing in Banff National Park through Rocky Mountain House and Red Deer, connecting with Highway 12 west of Stettler. The highway connects many of the smaller towns in central Alberta with Calgary and Edmonton, as it crosses Highway 2 just west of Red Deer.
Urban stretches of Alberta's major highways and freeways are often called "trails". For example, Highway 2, the main north-south highway in the province, is called Deerfoot Trail as it passes through Calgary but becomes Calgary Trail (for southbound traffic) and Gateway Boulevard (for northbound traffic) as it enters Edmonton and then turns into St. Albert Trail as it leaves Edmonton for the City of St. Albert. Calgary, in particular, has a tradition of calling its largest urban expressways "trails" and naming many of them after prominent First Nations individuals and tribes, such as Crowchild Trail, Deerfoot Trail, and Stoney Trail.
Calgary, Edmonton, Red Deer, Medicine Hat, and Lethbridge have substantial public transit systems. In addition to buses, Calgary and Edmonton operate light rail transit (LRT) systems. Edmonton LRT, which is underground in the downtown core and on the surface outside the CBD, was the first of the modern generation of light rail systems to be built in North America, while the Calgary C-Train has one of the highest number of daily riders of any LRT system in North America.
Alberta is well-connected by air, with international airports in both Calgary and Edmonton. Calgary International Airport and Edmonton International Airport are the third and fifth busiest in Canada respectively. Calgary's airport is a hub for WestJet Airlines and a regional hub for Air Canada. Calgary's airport primarily serves the Canadian prairie provinces (Alberta, Saskatchewan and Manitoba) for connecting flights to British Columbia, eastern Canada, 15 major US centres, nine European airports, one Asian airport and four destinations in Mexico and the Caribbean. Edmonton's airport acts as a hub for the Canadian north and has connections to all major Canadian airports as well as 10 major US airports, 3 European airports and 6 Mexican and Caribbean airports.
There are more than of operating mainline railway; the Canadian Pacific Railway and Canadian National Railway companies operate railway freight across the province. Passenger trains include Via Rail's Canadian (Toronto-Vancouver) or Jasper-Prince Rupert trains, which use the CN mainline and pass through Jasper National Park and parallel the Yellowhead Highway during at least part of their routes. The Rocky Mountaineer operates two sections: one from Vancouver to Banff and Calgary over CP tracks, and a section that travels over CN tracks to Jasper.
Health care.
Alberta provides a publicly funded health care system, Alberta Health Services, for all its citizens and residents as set out by the provisions of the Canada Health Act of 1984. Alberta became Canada's second province (after Saskatchewan) to adopt a Tommy Douglas-style program in 1950, a precursor to the modern medicare system.
Alberta's health care budget is currently $17.1 billion during the 2013–2014 fiscal year (approximately 45% of all government spending), making it the best funded health care system per-capita in Canada. Every hour more than $1.9 million is spent on health care in the province.
Notable health, education, research, and resources facilities in Alberta, all of which are located within Calgary or Edmonton:
The Edmonton Clinic complex, completed in 2012, provides a similar research, education, and care environment as the Mayo Clinic in the United States.
All public health care services funded by the Government of Alberta are delivered operationally by Alberta Health Services. AHS is the province's single health authority established on July 1, 2008, which replaced nine local health authorities. AHS also funds all ground ambulance services in the province, as well as the province-wide STARS (Shock Trauma Air Rescue Society) air ambulance service.
Education.
As with any Canadian province, the Alberta Legislature has (almost) exclusive authority to make laws respecting education. Since 1905 the Legislature has used this capacity to continue the model of locally elected public and separate school boards which originated prior to 1905, as well as to create and/or regulate universities, colleges, technical institutions and other educational forms and institutions (public charter schools, private schools, home schooling).
Elementary schools.
There are forty-two public school jurisdictions in Alberta, and seventeen operating separate school jurisdictions. Sixteen of the operating separate school jurisdictions have a Catholic electorate, and one (St. Albert) has a Protestant electorate. In addition, one Protestant separate school district, Glen Avon, survives as a ward of the St. Paul Education Region. The City of Lloydminster straddles the Alberta/Saskatchewan border, and both the public and separate school systems in that city are counted in the above numbers: both of them operate according to Saskatchewan law.
For many years the provincial government has funded the greater part of the cost of providing K–12 education. Prior to 1994 public and separate school boards in Alberta had the legislative authority to levy a local tax on property as a supplementary support for local education. In 1994 the government of the province eliminated this right for public school boards, but not for separate school boards. Since 1994 there has continued to be a tax on property in support of K–12 education; the difference is that the mill rate is now set by the provincial government, the money is collected by the local municipal authority and remitted to the provincial government. The relevant legislation requires that all the money raised by this property tax must go to the support of K–12 education provided by school boards. The provincial government pools the property tax funds from across the province and distributes them, according to a formula, to public and separate school jurisdictions and Francophone authorities.
Public and separate school boards, charter schools, and private schools all follow the Program of Studies and the curriculum approved by the provincial department of education (Alberta Education). Homeschool tutors may choose to follow the Program of Studies or develop their own Program of Studies. Public and separate schools, charter schools, and approved private schools all employ teachers who are certificated by Alberta Education, they administer Provincial Achievement Tests and Diploma Examinations set by Alberta Education, and they may grant high school graduation certificates endorsed by Alberta Education.
Universities.
The University of Alberta, established in Edmonton in 1908, is Alberta's oldest and largest university. The University of Calgary, once affiliated with the University of Alberta, gained its autonomy in 1966 and is now the second largest university in Alberta. There is also Athabasca University, which focuses on distance learning, and the University of Lethbridge, both of which are located in their title cities.
In early September 2009, Mount Royal University became Calgary's second public university, and in late September 2009, a similar move made MacEwan University Edmonton's second public university. There are 15 colleges that receive direct public funding, along with two technical institutes, Northern Alberta Institute of Technology and Southern Alberta Institute of Technology.
There is also a large and active private sector of post-secondary institutions, mostly Christian Universities, bringing the total number of universities to twelve, plus a DeVry University in Calgary, the only location in Canada. Students may also receive government loans and grants while attending selected private institutions. There has been some controversy in recent years over the rising cost of post-secondary education for students (as opposed to taxpayers). In 2005, Premier Ralph Klein made a promise that he would freeze tuition and look into ways of reducing schooling costs. So far, no plan has been released by the Government of Alberta.
Culture.
Summer brings many festivals to the province of Alberta, especially in Edmonton. The Edmonton Fringe Festival is the world's second largest after the Edinburgh Festival. Both Calgary and Edmonton host a number of annual festivals and events including folk music festivals. The city's "heritage days" festival sees the participation of over 70 ethnic groups. Edmonton's Churchill Square is home to a large number of the festivals, including the large Taste of Edmonton & The Works Art & Design Festival throughout the summer months.
The City of Calgary is also famous for its Stampede, dubbed "The Greatest Outdoor Show on Earth." The Stampede is Canada's biggest rodeo festival and features various races and competitions, such as calf roping and bull riding. In line with the western tradition of rodeo are the cultural artisans that reside and create unique Alberta western heritage crafts.
The Banff Centre hosts a range of festivals and other events including the international Mountain Film Festival. These cultural events in Alberta highlight the province's cultural diversity. Most of the major cities have several performing theatre companies who entertain in venues as diverse as Edmonton's Arts Barns and the Francis Winspear Centre for Music. Both Calgary and Edmonton are home to Canadian Football League and National Hockey League teams. Soccer, rugby union and lacrosse are also played professionally in Alberta.
Friendship partners.
Alberta has relationships with several provinces, states, and other entities worldwide.

</doc>
<doc id="728" url="http://en.wikipedia.org/wiki?curid=728" title="List of anthropologists">
List of anthropologists


</doc>
<doc id="734" url="http://en.wikipedia.org/wiki?curid=734" title="Actinopterygii">
Actinopterygii

The Actinopterygii , or ray-finned fishes, constitute a class or subclass of the bony fishes. 
The ray-finned fishes are so called because they possess lepidotrichia or "fin rays", their fins being webs of skin supported by bony or horny spines ("rays"), as opposed to the fleshy, lobed fins that characterize the class Sarcopterygii which also, however, possess lepidotrichia. These actinopterygian fin rays attach directly to the proximal or basal skeletal elements, the radials, which represent the link or connection between these fins and the internal skeleton (e.g., pelvic and pectoral girdles). 
In terms of numbers, actinopterygians are the dominant class of vertebrates, comprising nearly 99% of the over 30,000 species of fish. They are ubiquitous throughout freshwater and marine environments from the deep sea to the highest mountain streams. Extant species can range in size from "Paedocypris", at , to the massive ocean sunfish, at , and the long-bodied oarfish, at .
Characteristics.
Ray-finned fishes occur in many variant forms. The main features of a typical ray-finned fish are shown in the diagram at the left.
Fossil record.
The earliest known fossil actinopterygiian is "Andreolepis hedei", dating back 420 million years (Late Silurian). Remains have been found in Russia, Sweden, and Estonia.
Classification.
Traditionally actinopterygians have been divided into the subclasses Chondrostei and Neopterygii. Neopterygii, in turn, have been divided into the infraclasses Holostei and Teleostei. Some morphological evidence suggests the Neopterygii is paraphyletic; however, recent work, based on taxon orders, is arranged in what has been suggested to represent the evolutionary sequence down to the level of order, based primarily on the long history of morphological studies. This classification, like any other taxonomy based on phylogenetic research, has been in a state of flux. Recent morphological and molecular data have shown several of these ordinal and higher-level groupings represent evolutionary grades rather than clades. Examples of demonstrably paraphyletic groups include the Paracanthopterygii, Scorpaeniformes, and Perciformes. Studies of the nonteleostean actinopterygians shows that polypterids (bichirs and ropefish) are the sister lineage of all other actinopterygians (Actinopteri), Acipenseriformes (sturgeons and paddlefishes) are the sister lineage of Neopterygii, and Holostei (bowfin and gars) are the sister lineage of teleosts. The Elopomorpha (eels and tarpons) appears to be the most basic teleosts. 
 
The listing below follows FishBase with notes when this differs from Nelson and ITIS.

</doc>
<doc id="736" url="http://en.wikipedia.org/wiki?curid=736" title="Albert Einstein">
Albert Einstein

Albert Einstein (; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist and philosopher of science. He developed the general theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). He is best known in popular culture for his mass–energy equivalence formula (which has been dubbed "the world's most famous equation"). He received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect". The latter was pivotal in establishing quantum theory.
Near the beginning of his career, Einstein thought that Newtonian mechanics was no longer enough to reconcile the laws of classical mechanics with the laws of the electromagnetic field. This led to the development of his special theory of relativity. He realized, however, that the principle of relativity could also be extended to gravitational fields, and with his subsequent theory of gravitation in 1916, he published a paper on the general theory of relativity. He continued to deal with problems of statistical mechanics and quantum theory, which led to his explanations of particle theory and the motion of molecules. He also investigated the thermal properties of light which laid the foundation of the photon theory of light. In 1917, Einstein applied the general theory of relativity to model the large-scale structure of the universe.
He was visiting the United States when Adolf Hitler came to power in 1933 and, being Jewish, did not go back to Germany, where he had been a professor at the Berlin Academy of Sciences. He settled in the U.S., becoming an American citizen in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential development of "extremely powerful bombs of a new type" and recommending that the U.S. begin similar research. This eventually led to what would become the Manhattan Project. Einstein supported defending the Allied forces, but largely denounced the idea of using the newly discovered nuclear fission as a weapon. Later, with the British philosopher Bertrand Russell, Einstein signed the Russell–Einstein Manifesto, which highlighted the danger of nuclear weapons. Einstein was affiliated with the Institute for Advanced Study in Princeton, New Jersey, until his death in 1955.
Einstein published more than 300 scientific papers along with over 150 non-scientific works. His intellectual achievements and originality have made the word "Einstein" synonymous with genius.
Biography.
Early life and education.
Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire on 14 March 1879. His father was Hermann Einstein, a salesman and engineer. His mother was Pauline Einstein (née Koch). In 1880, the family moved to Munich, where his father and his uncle founded "Elektrotechnische Fabrik J. Einstein & Cie", a company that manufactured electrical equipment based on direct current.
The Einsteins were non-observant Ashkenazi Jews. Albert attended a Catholic elementary school from the age of 5 for three years. At the age of 8, he was transferred to the Luitpold Gymnasium (now known as the Albert Einstein Gymnasium), where he received advanced primary and secondary school education until he left Germany seven years later. Contrary to popular suggestions that he had struggled with early speech difficulties, the Albert Einstein Archives indicate he excelled at the first school that he attended. He was right-handed; there appears to be no evidence for the widespread popular belief that he was left-handed.
His father once showed him a pocket compass; Einstein realized that there must be something causing the needle to move, despite the apparent "empty space". As he grew, Einstein built models and mechanical devices for fun and began to show a talent for mathematics. When Einstein was 10 years old, Max Talmud (later changed to Max Talmey), a poor Jewish medical student from Poland, was introduced to the Einstein family by his brother. During weekly visits over the next five years, he gave the boy popular books on science, mathematical texts and philosophical writings. These included Immanuel Kant's "Critique of Pure Reason", and "Euclid's Elements" (which Einstein called the "holy little geometry book").
In 1894, his father's company failed: direct current (DC) lost the War of Currents to alternating current (AC). In search of business, the Einstein family moved to Italy, first to Milan and then, a few months later, to Pavia. When the family moved to Pavia, Einstein stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school's regimen and teaching method. He later wrote that the spirit of learning and creative thought were lost in strict rote learning. At the end of December 1894, he travelled to Italy to join his family in Pavia, convincing the school to let him go by using a doctor's note. It was during his time in Italy that he wrote a short essay with the title "On the Investigation of the State of the Ether in a Magnetic Field."
In 1895, at the age of 16, Einstein sat the entrance examinations for the Swiss Federal Polytechnic in Zürich (later the Eidgenössische Technische Hochschule ETH). He failed to reach the required standard in the general part of the examination, but obtained exceptional grades in physics and mathematics. On the advice of the Principal of the Polytechnic, he attended the Argovian cantonal school (gymnasium) in Aarau, Switzerland, in 1895–96 to complete his secondary schooling. While lodging with the family of Professor Jost Winteler, he fell in love with Winteler's daughter, Marie. (Albert's sister Maja later married Wintelers' son Paul.) In January 1896, with his father's approval, he renounced his citizenship in the German Kingdom of Württemberg to avoid military service. In September 1896, he passed the Swiss Matura with mostly good grades, including a top grade of 6 in physics and mathematical subjects, on a scale of 1–6, and, though only 17, enrolled in the four-year mathematics and physics teaching diploma program at the Zürich Polytechnic. Marie Winteler moved to Olsberg, Switzerland for a teaching post.
Einstein's future wife, Mileva Marić, also enrolled at the Polytechnic that same year, the only woman among the six students in the mathematics and physics section of the teaching diploma course. Over the next few years, Einstein and Marić's friendship developed into romance, and they read books together on extra-curricular physics in which Einstein was taking an increasing interest. In 1900, Einstein was awarded the Zürich Polytechnic teaching diploma, but Marić failed the examination with a poor grade in the mathematics component, theory of functions. There have been claims that Marić collaborated with Einstein on his celebrated 1905 papers, but historians of physics who have studied the issue find no evidence that she made any substantive contributions.
Marriages and children.
With the discovery and publication in 1987 of an early correspondence between Einstein and Marić it became known that they had a daughter they called "Lieserl" in their letters, born in early 1902 in Novi Sad where Marić was staying with her parents. Marić returned to Switzerland without the child, whose real name and fate are unknown. Einstein probably never saw his daughter, and the contents of a letter he wrote to Marić in September 1903 suggest that she was either adopted or died of scarlet fever in infancy.
Einstein and Marić married in January 1903. In May 1904, the couple's first son, Hans Albert Einstein, was born in Bern, Switzerland. Their second son, Eduard, was born in Zurich in July 1910. In 1914, Einstein moved to Berlin, while his wife remained in Zurich with their sons. They divorced on 14 February 1919, having lived apart for five years.
Einstein married Elsa Löwenthal on 2 June 1919, after having had a relationship with her since 1912. She was his first cousin maternally and his second cousin paternally. In 1933, they emigrated to the United States. In 1935, Elsa Einstein was diagnosed with heart and kidney problems and died in December 1936.
Patent office.
After graduating, Einstein spent almost two frustrating years searching for a teaching post. He acquired Swiss citizenship in February 1901, but was not conscripted for medical reasons. With the help of Marcel Grossmann's father Einstein secured a job in Bern at the Federal Office for Intellectual Property, the patent office, as an assistant examiner. He evaluated patent applications for a variety of devices including a gravel sorter and an electromechanical typewriter. In 1903, Einstein's position at the Swiss Patent Office became permanent, although he was passed over for promotion until he "fully mastered machine technology".
Much of his work at the patent office related to questions about transmission of electric signals and electrical-mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time.
With a few friends he had met in Bern, Einstein started a small discussion group, self-mockingly named "The Olympia Academy", which met regularly to discuss science and philosophy. Their readings included the works of Henri Poincaré, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.
Academic career.
In 1900, his paper "Folgerungen aus den Capillaritätserscheinungen" ("Conclusions from the Capillarity Phenomena") was published in the prestigious "Annalen der Physik". On 30 April 1905, Einstein completed his thesis, with Alfred Kleiner, Professor of Experimental Physics, serving as "" advisor. As a result, Einstein was awarded a PhD by the University of Zürich, with his dissertation entitled, "A New Determination of Molecular Dimensions." That same year, which has been called Einstein's "annus mirabilis" (miracle year), he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of mass and energy, which were to bring him to the notice of the academic world.
By 1908, he was recognized as a leading scientist and was appointed lecturer at the University of Bern. The following year, after giving a lecture on electrodynamics and the relativity principle at the University of Zurich, Alfred Kleiner recommended him to the faculty for a newly created professorship in theoretical physics. Einstein was appointed associate professor in 1909.
Einstein became a full professor at Charles-Ferdinand University in Prague in 1911, but returned to his alma mater in Zurich in 1912. From 1912 until 1914 he was professor of theoretical physics at the ETH in Zurich, where he taught analytical mechanics and thermodynamics. He also studied continuum mechanics, the molecular theory of heat, and the problem of gravitation, on which he worked with mathematician Marcel Grossmann.
In 1914, he returned to the German Empire after being appointed director of the Kaiser Wilhelm Institute for Physics (1914–1932) and a professor at the Humboldt University of Berlin, but freed from most teaching obligations. He soon became a member of the Prussian Academy of Sciences, and in 1916 was appointed president of the German Physical Society (1916–1918).
Based on calculations Einstein made in 1911, about his new theory of general relativity, light from another star would be bent by the Sun's gravity. In 1919 that prediction was confirmed by Sir Arthur Eddington during the solar eclipse of 29 May 1919. Those observations were published in the international media, making Einstein world famous. On 7 November 1919, the leading British newspaper "The Times" printed a banner headline that read: "Revolution in Science – New Theory of the Universe – Newtonian Ideas Overthrown".
In 1921, Einstein was awarded the Nobel Prize in Physics for his explanation of the photoelectric effect, as relativity was considered still somewhat controversial. He also received the Copley Medal from the Royal Society in 1925.
Travels abroad, 1921–1922.
Einstein visited New York City for the first time on 2 April 1921, where he received an official welcome by Mayor John Francis Hylan, followed by three weeks of lectures and receptions. He went on to deliver several lectures at Columbia University and Princeton University, and in Washington he accompanied representatives of the National Academy of Science on a visit to the White House. On his return to Europe he was the guest of the British statesman and philosopher Viscount Haldane in London, where he met several renowned scientific, intellectual and political figures, and delivered a lecture at King's College.
He also published an essay, "My First Impression of the U.S.A.," in July 1921, in which he tried briefly to describe some characteristics of Americans, much as Alexis de Tocqueville did, who published his own impressions in "Democracy in America" (1835). For some of his observations, Einstein was clearly surprised: "What strikes a visitor is the joyous, positive attitude to life . . . The American is friendly, self-confident, optimistic, and without envy."
In 1922, his travels took him to Asia and later to Palestine, as part of a six-month excursion and speaking tour, as he visited Singapore, Ceylon and Japan, where he gave a series of lectures to thousands of Japanese. After his first public lecture, he met the emperor and empress at the Imperial Palace, where thousands came to watch. In a letter to his sons, Einstein described his impression of the Japanese as being modest, intelligent, considerate, and having a true feel for art.
On his return voyage, he visited Palestine for 12 days in what would become his only visit to that region. Einstein was greeted as if he were a head of state, rather than a physicist, which included a cannon salute upon arriving at the home of the British high commissioner, Sir Herbert Samuel. During one reception, the building was stormed by people who wanted to see and hear him. In Einstein's talk to the audience, he expressed happiness that the Jewish people were beginning to be recognized as a force in the world.
Travel to U.S., 1930–1931.
In December 1930, Einstein visited America for the second time, originally intended as a two-month working visit as a research fellow at the California Institute of Technology. After the national attention he received during his first trip to the U.S., he and his arrangers aimed to protect his privacy. Although swamped with telegrams and invitations to receive awards or speak publicly, he declined them all.
After arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of the "New York Times", and a performance of "Carmen" at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met the president of Columbia University, who described Einstein as "the ruling monarch of the mind." Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue the church made of Einstein, standing at the entrance. Also during his stay in New York, he joined a crowd of 15,000 people at Madison Square Garden during a Hanukkah celebration.
Einstein next traveled to California where he met Caltech president and Nobel laureate, Robert A. Millikan. His friendship with Millikan was "awkward", as Millikan "had a penchant for patriotic militarism," where Einstein was a pronounced pacifist. During an address to Caltech's students, Einstein noted that science was often inclined to do more harm than good.
This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin, both noted for their pacifism. Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin. They had an instant rapport, with Chaplin inviting Einstein and his wife, Elsa, to his home for dinner. Chaplin said Einstein's outward persona, calm and gentle, seemed to conceal a "highly emotional temperament," from which came his "extraordinary intellectual energy."
Chaplin also remembers Elsa telling him about the time Einstein conceived his theory of relativity. During breakfast one morning, he seemed lost in thought and ignored his food. She asked him if something was bothering him. He sat down at his piano and started playing. He continued playing and writing notes for half an hour, then went upstairs to his study, where he remained for two weeks, with Elsa bringing up his food. At the end of the two weeks he came downstairs with two sheets of paper bearing his theory.
Chaplin's film, "City Lights", was to premier a few days later in Hollywood, and Chaplin invited Einstein and Elsa to join him as his special guests, described by Isaacson as "one of the most memorable scenes in the new era of celebrity." Einstein and Chaplin arrived together, in black tie, with Elsa joining them, "beaming." The audience applauded as they entered the theater. Chaplin visited Einstein at his home in later trip to Berlin, and recalled his "modest little flat" and the piano at which he had begun writing his theory. Chaplin speculated that it was "possibly used as kindling wood by the Nazis."
Emigration to U.S. in 1933.
In February 1933 while on a visit to the United States, Einstein knew he could not to return to Germany with the rise to power of the Nazis under Germany's new chancellor, Adolf Hitler.
While at American universities in early 1933, he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena. He and his wife Elsa returned to Belgium by ship in March, and during the trip they learned that their cottage was raided by the Nazis and his personal sailboat confiscated. Upon landing in Antwerp on 28 March, he immediately went to the German consulate and turned in his passport, formally renouncing his German citizenship. A few years later, the Nazis sold his boat and turned his cottage into an Aryan youth camp.
Refugee status.
In April 1933, he also discovered that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities. Historian Gerald Holton describes how, with "virtually no audible protest being raised by their colleagues," thousands of Jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.
A month later, Einstein's works were among those targeted by Nazi book burnings, with Nazi propaganda minister Joseph Goebbels proclaiming, "Jewish intellectualism is dead." One German magazine included him in a list of enemies of the German regime with the phrase, "not yet hanged", offering a $5,000 bounty on his head. In a subsequent letter to physicist and friend, Max Born, who had already emigrated from Germany to England, Einstein wrote, "... I must confess that the degree of their brutality and cowardice came as something of a surprise." After moving to the U.S., he described the book burnings as a "spontaneous emotional outburst" by those who "shun popular enlightenment," and "more than anything else in the world, fear the influence of men of intellectual independence."
Einstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in Germany. He rented a house in Belgium where he lived for a few months. In late July 1933, he went to England for about six weeks at the personal invitation of British naval officer Commander Oliver Locker-Lampson, who had become friends with Einstein in the preceding years. To protect Einstein, Locker-Lampson secretly had two assistants watch over him at his secluded cottage outside of London, with the press publishing a photo of them guarding Einstein.
Locker-Lampson took Einstein to meet Winston Churchill at his home, and later, Austen Chamberlain and former Prime Minister Lloyd George. Einstein asked them to help bring Jewish scientists out of Germany. British historian Martin Gilbert notes that Churchill responded immediately, and sent his friend, physicist Frederick Lindemann to Germany to seek out Jewish scientists and place them in British universities. Churchill later observed that as a result of Germany having driven the Jews out, they lowered their "technical standards," and had put the Allies' technology ahead of theirs.
Einstein later contacted leaders of other nations, including Turkey's Prime Minister, İsmet İnönü, who he wrote in September 1933 requesting placement of unemployed German-Jewish scientists. As a result of Einstein's letter, Jewish invitees to Turkey eventually totaled over "1,000 saved individuals."
Locker-Lampson also submitted a bill to parliament to extend British citizenship to Einstein, during which period Einstein made a number of public appearances describing the crisis brewing in Europe. The bill failed to become law, however, and Einstein then accepted an earlier offer from the Princeton Institute for Advanced Study, in the U.S., to become a resident scholar.
Resident scholar at the Institute for Advanced Study.
In October 1933 Einstein returned to the U.S. and took up a position at the Institute for Advanced Study (in Princeton, New Jersey), noted for having become a refuge for scientists fleeing Nazi Germany. At the time, most American universities, including Harvard, Princeton and Yale, had minimal or no Jewish faculty or students, as a result of their Jewish quota which lasted until the late 1940s.
He was still undecided on his future (he had offers from European universities, including Oxford), but in 1935 he arrived at the decision to remain permanently in the United States and apply for citizenship.
Einstein's affiliation with the Institute for Advanced Study would last until his death in 1955.
He was one of the four first selected (two of the others being John von Neumann and Kurt Gödel) at the new Institute, where he soon developed a close friendship with Gödel. The two would take long walks together discussing their work. Bruria Kaufman, his assistant, later became a physicist. During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully.
Other scientists also fled to America, including Nobel laureates and professors of theoretical physics. With so many other Jewish scientists now forced by circumstances to live in America, often working side by side, Einstein wrote. "In my whole life I have never felt so Jewish as now."
World War II and the Manhattan Project.
In 1939, a group of Hungarian scientists that included émigré physicist Leó Szilárd attempted to alert Washington of ongoing Nazi atomic bomb research. The group's warnings were discounted. Einstein and Szilárd, along with other refugees such as Edward Teller and Eugene Wigner, "regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon." On July 12, 1939, a few months before the beginning of World War II in Europe, Szilárd and Wigner visited Einstein and they explained the possibility of atomic bombs, to which pacifist Einstein replied: "Daran habe ich gar nicht gedacht" ("I had not thought of that at all"). Einstein was persuaded to lend his prestige by writing a letter with Szilárd to President Franklin D. Roosevelt to alert him of the possibility. The letter also recommended that the U.S. government pay attention to and become directly involved in uranium research and associated chain reaction research.
The letter is believed to be "arguably the key stimulus for the U.S. adoption of serious investigations into nuclear weapons on the eve of the U.S. entry into World War II". In addition to the letter, Einstein used his connections with the Belgian Royal Family and the Belgian queen mother to get access with a personal envoy to the White House's Oval Office. President Roosevelt could not take the risk of allowing Hitler to possess atomic bombs first. As a result of Einstein's letter and his meetings with Roosevelt, the U.S. entered the "race" to develop the bomb, drawing on its "immense material, financial, and scientific resources" to initiate the Manhattan Project. It became the only country to successfully develop an atomic bomb during World War II.
For Einstein, "war was a disease ... [and] he called for resistance to war." By signing the letter to Roosevelt he went against his pacifist principles. In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, "I made one great mistake in my life—when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification—the danger that the Germans would make them ..."
US citizenship.
Einstein became an American citizen in 1940. Not long after settling into his career at the Institute for Advanced Study (in Princeton, New Jersey), he expressed his appreciation of the meritocracy in American culture when compared to Europe. He recognized the "right of individuals to say and think what they pleased", without social barriers, and as a result, individuals were encouraged, he said, to be more creative, a trait he valued from his own early education.
Personal life.
Supporter of civil rights.
Einstein was a passionate, committed antiracist and joined National Association for the Advancement of Colored People (NAACP) in Princeton, where he campaigned for the civil rights of African Americans. He considered racism America's "worst disease," seeing it as "handed down from one generation to the next." As part of his involvement, he corresponded with civil rights activist W. E. B. Du Bois and was prepared to testify on his behalf during his trial in 1951. When Einstein offered to be a character witness for Du Bois, the judge decided to drop the case.
In 1946 Einstein visited Lincoln University in Pennsylvania where he was awarded an honorary degree. Lincoln was the first university to grant college degrees to blacks, including Langston Hughes and Thurgood Marshall. To its students, Einstein gave a speech about racism in America, adding, "I do not intend to be quiet about it." A resident of Princeton recalls that Einstein had once paid the college tuition for a black student, and black physicist Sylvester James Gates states that Einstein had been one of his early science heroes, later finding out about Einstein's support for civil rights.
Assisting Zionist causes.
Einstein was a figurehead leader in helping establish Hebrew University of Jerusalem, which opened in 1925, and was among its first Board of Governors. Earlier, in 1921, he was asked by the president of the World Zionist Organization, Chaim Weizmann, to help raise funds for the planned university. He also submitted various suggestions as to its initial programs.
Among those, he advised first creating an Institute of Agriculture in order to settle the undeveloped land. That should be followed, he suggested, by a Chemical Institute and an Institute of Microbiology, to fight the various ongoing epidemics such as malaria, which he called an "evil" that was undermining a third of the country's development. Establishing an Oriental Studies Institute, to include language courses given in both Hebrew and Arabic, for scientific exploration of the country and its historical monuments, was also important.
After the death of Israel's first president, Chaim Weizmann, in November 1952, Prime Minister David Ben-Gurion offered Einstein the position of President of Israel, a mostly ceremonial post. The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer "embodies the deepest respect which the Jewish people can repose in any of its sons". Einstein declined, and wrote in his response that he was "deeply moved", and "at once saddened and ashamed" that he could not accept it.
Love of music.
Einstein developed an appreciation of music at an early age. His mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into German culture. According to conductor Leon Botstein, Einstein is said to have begun playing when he was 5, although he did not enjoy it at that age.
When he turned 13 he discovered the violin sonatas of Mozart, whereupon "Einstein fell in love" with Mozart's music and studied music more willingly. He taught himself to play without "ever practicing systematically", he said, deciding that "love is a better teacher than a sense of duty." At age 17, he was heard by a school examiner in Aarau as he played Beethoven's violin sonatas, the examiner stating afterward that his playing was "remarkable and revealing of 'great insight'." What struck the examiner, writes Botstein, was that Einstein "displayed a deep love of the music, a quality that was and remains in short supply. Music possessed an unusual meaning for this student."
Music took on a pivotal and permanent role in Einstein's life from that period on. Although the idea of becoming a professional himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, and he performed for private audiences and friends. Chamber music had also become a regular part of his social life while living in Bern, Zürich, and Berlin, where he played with Max Planck and his son, among others.
In 1931, while engaged in research at the California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles, where he played some of Beethoven and Mozart's works with members of the Zoellner Quartet. Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them, and the quartet was "impressed by Einstein's level of coordination and intonation."
Political and religious views.
Einstein's political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as "Why Socialism?". Einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics.
Einstein's views about religious belief have been collected from interviews and original writings.
He called himself an agnostic, while disassociating himself from the label atheist. He said he believed in the "pantheistic" God of Baruch Spinoza, but not in a personal god, a belief he criticized.
Death.
On 17 April 1955, Albert Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel's seventh anniversary with him to the hospital, but he did not live long enough to complete it.
Einstein refused surgery, saying: "I want to go when I want. It is tasteless to prolong life artificially. I have done my share, it is time to go. I will do it elegantly." He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.
During the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey, removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent. Einstein's remains were cremated and his ashes were scattered at an undisclosed location.
In his lecture at Einstein's memorial, nuclear physicist Robert Oppenheimer summarized his impression of him as a person: "He was almost wholly without sophistication and wholly without worldliness ... There was always with him a wonderful purity at once childlike and profoundly stubborn."
Scientific career.
Throughout his life, Einstein published hundreds of books and articles. In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.
1905 – Annus Mirabilis papers.
The "Annus Mirabilis" papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc2 that Albert Einstein published in the "Annalen der Physik" scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are:
Thermodynamic fluctuations and statistical physics.
Albert Einstein's first paper submitted in 1900 to "Annalen der Physik" was on capillary attraction. It was published in 1901 with the title "Folgerungen aus den Capillaritätserscheinungen", which translates as "Conclusions from the capillarity phenomena". Two papers he published in 1902–1903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.
General principles.
He articulated the principle of relativity. This was understood by Hermann Minkowski to be a generalization of rotational invariance from space to space-time. Other principles postulated by Einstein and later vindicated are the principle of equivalence and the principle of adiabatic invariance of the quantum number.
Theory of relativity and "E" = "mc"².
Einstein's "Zur Elektrodynamik bewegter Körper" ("On the Electrodynamics of Moving Bodies") was received on 30 June 1905 and published 26 September of that same year. It reconciles Maxwell's equations for electricity and magnetism with the laws of mechanics, by introducing major changes to mechanics close to the speed of light. This later became known as Einstein's special theory of relativity.
Consequences of this include the time-space frame of a moving body appearing to slow down and contract (in the direction of motion) when measured in the frame of the observer. This paper also argued that the idea of a luminiferous aether—one of the leading theoretical entities in physics at the time—was superfluous.
In his paper on mass–energy equivalence, Einstein produced "E" = "mc"2 from his special relativity equations. Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
Photons and energy quanta.
In a 1905 paper, Einstein postulated that light itself consists of localized particles ("quanta"). Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein concluded that each wave of frequency "f" is associated with a collection of photons with energy "hf" each, where "h" is Planck's constant. He does not say much more, because he is not sure how the particles are related to the wave. But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.
Quantized atomic vibrations.
In 1907, Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. In the Einstein model, each atom oscillates independently—a series of equally spaced quantized states for each oscillator. Einstein was aware that getting the frequency of the actual oscillations would be different, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. Peter Debye refined this model.
Adiabatic principle and action-angle variables.
Throughout the 1910s, quantum mechanics expanded in scope to cover many different systems. After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements.
Einstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics.
Wave–particle duality.
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia. In 1908, he became a "Privatdozent" at the University of Bern.
In "über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung" (""), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the "photon" concept (although the name "photon" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave–particle duality in quantum mechanics. Einstein saw this wave-particle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation.
Theory of critical opalescence.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue. Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.
Zero-point energy.
Einstein's physical intuition led him to note that Planck's oscillator energies had an incorrect zero point. He modified Planck's hypothesis by stating that the lowest energy state of an oscillator is equal to "hf", to half the energy spacing between levels. This argument, which was made in 1913 in collaboration with Otto Stern, was based on the thermodynamics of a diatomic molecule which can split apart into two free atoms.
General relativity and the equivalence principle.
General relativity (GR) is a theory of gravitation that was developed by Albert Einstein between 1907 and 1915. According to general relativity, the observed gravitational attraction between masses results from the warping of space and time by those masses. General relativity has developed into an essential tool in modern astrophysics. It provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.
As Albert Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory. Consequently, in 1908 he published an article on acceleration under special relativity. In that article, he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply. This argument is called the equivalence principle. In the same article, Einstein also predicted the phenomenon of gravitational time dilation. In 1911, Einstein published another article expanding on the 1907 article, in which additional effects such as the deflection of light by massive bodies were predicted.
Hole argument and Entwurf theory.
While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.
In June 1913, the Entwurf ("draft") theory was the result of these investigations. As its name suggests, it was a sketch of a theory, with the equations of motion supplemented by additional gauge fixing conditions. Simultaneously less elegant and more difficult than general relativity, after more than two years of intensive work Einstein abandoned the theory in November 1915 after realizing that the hole argument was mistaken.
Cosmology.
In 1917, Einstein applied the general theory of relativity to model the structure of the universe as a whole. He apprehended that his equations predicted the universe to be either contracting or expanding. He wanted the universe to be eternal and unchanging, but this type of universe is not consistent with relativity. To fix this, Einstein modified the general theory by introducing a new notion, the cosmological constant, which he called "Lambda". The purpose of Lambda was to rectify the effects of gravity and allow the whole system to stay balanced. With a positive cosmological constant, the universe could be an eternal static sphere. However, in 1929, Edwin Hubble confirmed that the universe is expanding, Einstein exclaimed after his Mount Wilson visit with Hubble: "If there is no quasi-static world, then away with the cosmological term!" and Einstein supposedly discarded the cosmological constant.
Einstein believed a spherical static universe is philosophically preferred, because it would obey Mach's principle. He had shown that general relativity incorporates Mach's principle to a certain extent in frame dragging by gravitomagnetic fields, but he knew that Mach's idea would not work if space goes on forever. In a closed universe, he believed that Mach's principle would hold. Mach's principle has generated much controversy over the years.
In many of Einstein biographies, writers claim that he called the creation of Lambda his "biggest blunder". Recently, astrophysicist Mario Livio showed that Einstein possibly never said that. Instead of discarding Lambda, Einstein was continually experimenting with it.
In late 2013, Irish physicist Cormac O'Raifeartaigh, happened to discover a handwritten manuscript by Einstein which was since then overlooked by other scientists. The research paper was titled ""Zum kosmologischen Problem"" ("About the Cosmological Problem"). And Einstein proposed a revision of his model, still with a cosmological constant, but now the constant was responsible for the creation of new matter as the universe expanded. Thus, the average density of the system never changed. He stated in the paper, ""In what follows, I would like to draw attention to a solution to equation (1) that can account for Hubbel's [sic] facts, and in which the density is constant over time." And: "If one considers a physically bounded volume, particles of matter will be continually leaving it. For the density to remain constant, new particles of matter must be continually formed in the volume from space.""
This is consistent with the now-obsolete Steady State model of cosmology, proposed later in 1949, and with today's modern understanding of dark energy.
Modern quantum theory.
Einstein was displeased with quantum theory and mechanics (the very theory he helped create), despite its acceptance by other physicists, stating that God "is not playing at dice." Einstein continued to maintain his disbelief in the theory, and attempted unsuccessfully to disprove it until he died at the age of 76. In 1917, at the height of his work on relativity, Einstein published an article in "Physikalische Zeitschrift" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.
This article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.
Einstein discovered Louis de Broglie's work, and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the Hamilton–Jacobi equation of mechanics. This paper would inspire Schrödinger's work of 1926.
Bose–Einstein statistics.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the "Zeitschrift für Physik". Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.
Energy momentum pseudotensor.
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's presecriptions do not make a real tensor for this reason.
Einstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field. This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard.
The use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrödinger and others.
Unified field theory.
Following his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation to include electromagnetism as another aspect of a single entity. In 1950, he described his "unified field theory" in a "Scientific American" article entitled "On the Generalized Theory of Gravitation". Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful.
In his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death. Mainstream physics, in turn, largely ignored Einstein's approaches to unification. Einstein's dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.
Wormholes.
Einstein collaborated with others to produce a model of a wormhole. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches.
If one end of a wormhole was positively charged, the other end would be negatively charged. These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.
Einstein–Cartan theory.
In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.
Equations of motion.
The theory of general relativity has a fundamental law—the Einstein equations which describe how space curves, the geodesic equation which describes how particles move may be derived from the Einstein equations.
Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law. So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself.
This was established by Einstein, Infeld, and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.
Other investigations.
Einstein conducted other investigations that were unsuccessful and abandoned. These pertain to force, superconductivity, gravitational waves, and other research.
Collaboration with other scientists.
In addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.
Einstein–de Haas experiment.
Einstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin. In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum. They confirmed that this leads the bar to rotate, because the electron's angular momentum changes as the magnetization changes. This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.
Schrödinger gas model.
Einstein suggested to Erwin Schrödinger that he might be able to reproduce the statistics of a Bose–Einstein gas by considering a box. Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator. Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it.
This formulation is a form of second quantization, but it predates modern quantum mechanics. Erwin Schrödinger applied this to derive the thermodynamic properties of a semiclassical ideal gas. Schrödinger urged Einstein to add his name as co-author, although Einstein declined the invitation.
Einstein refrigerator.
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, was awarded to Albert Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, as the most promising of their patents were quickly bought up by the Swedish company Electrolux to protect its refrigeration technology from competition.
Bohr versus Einstein.
 The Bohr–Einstein debates were a series of public disputes about quantum mechanics between Albert Einstein and Niels Bohr who were two of its founders. Their debates are remembered because of their importance to the philosophy of science.
Einstein–Podolsky–Rosen paradox.
In 1935, Einstein returned to the question of quantum mechanics. He considered how a measurement on one of two entangled particles would affect the other. He noted, along with his collaborators, that by performing different measurements on the distant particle, either of position or momentum, different properties of the entangled partner could be discovered without disturbing it in any way.
He then used a hypothesis of local realism to conclude that the other particle had these properties already determined. The principle he proposed is that if it is possible to determine what the answer to a position or momentum measurement would be, without in any way disturbing the particle, then the particle actually has values of position or momentum.
This principle distilled the essence of Einstein's objection to quantum mechanics. As a physical principle, it was shown to be incorrect when the Aspect experiment of 1982 confirmed Bell's theorem, which had been promulgated in 1964.
Non-scientific legacy.
While traveling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to The Hebrew University. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Barbara Wolff, of The Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.
Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
In popular culture.
In the period before World War II, the "New York Times" published a vignette in their "The Talk of the Town" feature saying that Einstein was so well known in America that he would be stopped on the street by people wanting him to explain "that theory". He finally figured out a way to handle the incessant inquiries. He told his inquirers "Pardon me, sorry! Always I am mistaken for Professor Einstein."
Einstein has been the subject of or inspiration for many novels, films, plays, and works of music. He is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. "Time" magazine's Frederic Golden wrote that Einstein was "a cartoonist's dream come true".
Awards and honors.
Einstein received numerous awards and honors, including the Nobel Prize in Physics.

</doc>
