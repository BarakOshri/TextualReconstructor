<doc id="6944" url="http://en.wikipedia.org/wiki?curid=6944" title="Cathode">
Cathode

A cathode is the electrode from which a conventional current leaves a polarized electrical device. This definition is sometimes remembered using the mnemonic CCD for "cathode current departs". A conventional current describes the direction in which positive electronic charges move. Electrons have a negative charge, so the movement of electrons is opposite to the conventional current flow. Consequently, the mnemonic "cathode current departs" also means that electrons flow into the device's cathode.
Cathode polarity with respect to the anode can be positive or negative; it depends on how the device operates. Although positively charged cations always move towards the cathode (hence their name) and negatively charged anions move away from it, cathode polarity depends on the device type, and can even vary according to the operating mode. In a device which consumes power, the cathode is negative, and in a device which provides power, the cathode is positive:
An electrode through which current flows the other way (into the device) is termed an anode.
Etymology.
The word was coined in 1834 from the Greek κάθοδος ("kathodos"), 'descent' or 'way down', by William Whewell, who had been consulted by Michael Faraday over some new names needed to complete a paper on the recently discovered process of electrolysis. In that paper Faraday explained that when an electrolytic cell is oriented so that electric current traverses the "decomposing body" (electrolyte) in a direction "from East to West, or, which will strengthen this help to the memory, that in which the sun appears to move", the cathode is where the current leaves the electrolyte, on the West side: ""kata" downwards, "`odos" a way ; the way which the sun sets".
The use of 'West' to mean the 'out' direction (actually 'out' → 'West' → 'sunset' → 'down', i.e. 'out of view') may appear unnecessarily contrived. Previously, as related in the first reference cited above, Faraday had used the more straightforward term "exode" (the doorway where the current exits). His motivation for changing it to something meaning 'the West electrode' (other candidates had been "westode", "occiode" and "dysiode") was to make it immune to a possible later change in the direction convention for current, whose exact nature was not known at the time. The reference he used to this effect was the Earth's magnetic field direction, which at that time was believed to be invariant. He fundamentally defined his arbitrary orientation for the cell as being that in which the internal current would run parallel to and in the same direction as a hypothetical magnetizing current loop around the local line of latitude which would induce a magnetic dipole field oriented like the Earth's. This made the internal current East to West as previously mentioned, but in the event of a later convention change it would have become West to East, so that the West electrode would not have been the 'way out' any more. Therefore "exode" would have become inappropriate, whereas "cathode" meaning 'West electrode' would have remained correct with respect to the unchanged direction of the actual phenomenon underlying the current, then unknown but, he thought, unambiguously defined by the magnetic reference. In retrospect the name change was unfortunate, not only because the Greek roots alone do not reveal the cathode's function any more, but more importantly because, as we now know, the Earth's magnetic field direction on which the "cathode" term is based is subject to reversals whereas the current direction convention on which the "exode" term was based has no reason to change in the future.
Since the later discovery of the electron, an easier to remember, and more durably technically correct (although historically false), etymology has been suggested: cathode, from the Greek "kathodos", 'way down', 'the way (down) into the cell (or other device) for electrons'.
Flow of electrons.
The flow of electrons is almost always from anode to cathode outside of the cell or device, regardless of the cell or device type and operating mode. An exception is when a diode reverse-conducts, either by accident (breakdown of a normal diode) or by design (breakdown of a Zener diode, photo-current of a photodiode or solar cell).
In chemistry.
In chemistry, a cathode is the electrode of an electrochemical cell at which reduction occurs; a useful mnemonic to remember this is AnOx RedCat (Oxidation at the Anode = Reduction at the Cathode). Another mnemonic is to note the cathode has a 'c', as does 'reduction'. Hence, reduction at the cathode. The cathode can be negative like when the cell is electrolytic (where electrical energy provided to the cell is being used for decomposing chemical compounds); or positive like when the cell is galvanic (where chemical reactions are used for generating electrical energy). The cathode supplies electrons to the positively charged cations which flow to it from the electrolyte (even if the cell is galvanic, i.e., when the cathode is positive and therefore would be expected to repel the positively charged cations; this is due to electrode potential relative to the electrolyte solution being different for the anode and cathode metal/electrolyte systems in a galvanic cell).
The cathodic current, in electrochemistry, is the flow of electrons from the cathode interface to a species in solution. The anodic current is the flow of electrons into the anode from a species in solution.
Electrolytic cell.
In an electrolytic cell, the cathode is where the negative polarity is applied to drive the cell. Common results of reduction at the cathode are hydrogen gas or pure metal from metal ions. When discussing the relative reducing power of two redox agents, the couple for generating the more reducing species is said to be more "cathodic" with respect to the more easily reduced reagent.
Galvanic cell.
In a galvanic cell, the cathode is where the positive pole is connected to allow the circuit to be completed: as the anode of the galvanic cell gives off electrons, they return from the circuit into the cell through the cathode.
Electroplating metal cathode (a.k.a Electrolysis).
When metal ions are reduced from ionic solution, they form a pure metal surface on the cathode. Items to be plated with pure metal are attached to and become part of the cathode in the electrolytic solution.
In electronics.
In physics or electronics, a cathode is an electrode that emits electrons into the device. This contrasts with an anode, which accepts electrons. 
Vacuum tubes.
In a vacuum tube or electronic vacuum system, the cathode is a metal surface which emits free electrons into the evacuated space. Since the electrons are attracted to the positive nuclei of the metal atoms, they normally stay inside the metal and require energy to leave it; this is called the "work function" of the metal. Cathodes are induced to emit electrons by several mechanisms: 
Cathodes can be divided into two types:
Hot cathode.
A hot cathode is a cathode that is heated by a filament to produce electrons by thermionic emission. The filament is a thin wire of a refractory metal like tungsten heated red-hot by an electric current passing through it. Before the advent of transistors in the 1960s, virtually all electronic equipment used hot-cathode vacuum tubes. Today hot cathodes are used in vacuum tubes in radio transmitters and microwave ovens, to produce the electron beams in older cathode ray tube (CRT) type televisions and computer monitors, in x-ray machines, electron microscopes, and fluorescent tubes. 
There are two types of hot cathodes:
In order to improve electron emission, cathodes are treated with chemicals, usually compounds of metals with a low work function. Treated cathodes require less surface area, lower temperatures and less power to supply the same cathode current. The untreated tungsten filaments used in early tubes (called "bright emitters") had to be heated to 2500°F (1400°C), white-hot, to produce sufficient thermionic emission for use, while modern coated cathodes produce far more electrons at a given temperature so they only have to be heated to 800-1100°F (425-600°C) There are two main types of treated cathodes: 
Cold cathode.
This is a cathode that is not heated by a filament. They may emit electrons by field electron emission, and in gas-filled tubes by secondary emission. Some examples are electrodes in neon lights, cold-cathode fluorescent lamps (CCFLs) used as backlights in laptops, thyratron tubes, and Crookes tubes. They do not necessarily operate at room temperature; in some devices the cathode is heated by the electron current flowing through it to a temperature at which thermionic emission occurs. For example, in some fluorescent tubes a momentary high voltage is applied to the electrodes to start the current through the tube; after starting the electrodes are heated enough by the current to keep emitting electrons to sustain the discharge.
Cold cathodes may also emit electrons by photoelectric emission. These are often called "photocathodes" and are used in phototubes used in scientific instruments and image intensifier tubes used in night vision goggles.
Diodes.
In a semiconductor diode, the cathode is the N–doped layer of the PN junction with a high density of free electrons due to doping, and an equal density of fixed positive charges, which are the dopants that have been thermally ionized. In the anode, the converse applies: It features a high density of free "holes" and consequently fixed negative dopants which have captured an electron (hence the origin of the holes).
When P and N-doped layers are created adjacent to each other, diffusion ensures that electrons flow from high to low density areas: That is, from the N to the P side. They leave behind the fixed positively charged dopants near the junction. Similarly, holes diffuse from P to N leaving behind fixed negative ionised dopants near the junction. These layers of fixed positive and negative charges, collectively known as the depletion layer because they are depleted of free electrons and holes. The depletion layer at the junction is at the origin of the diode's rectifying properties. This is due to the resulting internal field and corresponding potential barrier which inhibit current flow in reverse applied bias which increases the internal depletion layer field. Conversely, they allow it in forwards applied bias where the applied bias reduces the built in potential barrier.
Electrons which diffuse from the cathode into the P-doped layer, or anode, become what is termed "minority carriers" and tend to recombine there with the majority carriers, which are holes, on a timescale characteristic of the material which is the p-type minority carrier lifetime. Similarly, holes diffusing into the N-doped layer become minority carriers and tend to recombine with electrons. In equilibrium, with no applied bias, thermally assisted diffusion of electrons and holes in opposite directions across the depletion layer ensure a zero net current with electrons flowing from cathode to anode and recombining, and holes flowing from anode to cathode across the junction or depletion layer and recombining.
Like a typical diode, there is a fixed anode and cathode in a Zener diode, but it will conduct current in the reverse direction (electrons flow from anode to cathode) if its breakdown voltage or "Zener voltage" is exceeded.

</doc>
<doc id="6945" url="http://en.wikipedia.org/wiki?curid=6945" title="Chrominance">
Chrominance

Chrominance ("chroma" or C for short) is the signal used in video systems to convey the color information of the picture, separately from the accompanying luma signal (or Y for short). Chrominance is usually represented as two color-difference components: U = B′ − Y′ (blue − luma) and V = R′ − Y′ (red − luma). Each of these difference components may have scale factors and offsets applied to it, as specified by the applicable video standard. 
In composite video signals, the U and V signals modulate a color subcarrier signal, and the result is referred to as the chrominance signal; the phase and amplitude of this modulated chrominance signal correspond approximately to the hue and saturation of the color. In digital-video and still-image color spaces such as Y′CbCr, the luma and chrominance components are digital sample values.
Separating RGB color signals into luma and chrominance allows the bandwidth of each to be determined separately. Typically, the chrominance bandwidth is reduced in analog composite video by reducing the bandwidth of a modulated color subcarrier, and in digital systems by chroma subsampling.
History.
The idea of transmitting a color television signal with distinct luma and chrominance components originated with Georges Valensi, who patented the idea in 1938. Valensi's patent application described:
The use of two channels, one transmitting the predominating color (signal T), and the other the mean brilliance (signal t) output from a single television transmitter to be received not only by color television receivers provided with the necessary more expensive equipment, but also by the ordinary type of television receiver which is more numerous and less expensive and which reproduces the pictures in black and white only.
Previous schemes for color television systems, which were incompatible with existing monochrome receivers, transmitted RGB signals in various ways.
Television standards.
In analog television, chrominance is encoded into a video signal using a subcarrier frequency. Depending on the video standard, the chrominance subcarrier may be either quadrature-amplitude-modulated (NTSC and PAL) or frequency-modulated (SECAM).
In the PAL system, the color subcarrier is 4.43 MHz above the video carrier, while in the NTSC system it is 3.58 MHz above the video carrier. The NTSC and PAL standards are the most commonly used, although there are other video standards that employ different subcarrier frequencies. For example, PAL-M (Brazil) uses a 3.58 MHz subcarrier, and SECAM uses two different frequencies, 4.250 MHz and 4.40625 MHz above the video carrier.
The presence of chrominance in a video signal is indicated by a color burst signal transmitted on the back porch, just after horizontal synchronization and before each line of video starts. If the color burst signal were visible on a television screen, it would appear as a vertical strip of a very dark olive color. In NTSC and PAL, hue is represented by a phase shift of the chrominance signal relative to the color burst, while saturation is determined by the amplitude of the subcarrier. In SECAM (R′ − Y′) and (B′ − Y′) signals are transmitted alternately and phase does not matter.
Chrominance is represented by the U-V color plane in PAL and SECAM video signals, and by the I-Q color plane in NTSC.
Digital systems.
Digital video and digital still photography systems sometimes use a luma/chroma decomposition for improved compression. For example, when an ordinary RGB digital image is compressed via the JPEG standard, the RGB colorspace is first converted (by a rotation matrix) to a YCbCr colorspace, because the three components in that space have less correlation redundancy and because the chrominance components can then be subsampled by a factor of 2 or 4 to further compress the image. On decompression, the Y′CbCr space is rotated back to RGB.

</doc>
<doc id="6946" url="http://en.wikipedia.org/wiki?curid=6946" title="Chirality (disambiguation)">
Chirality (disambiguation)

Chirality ("handedness") is a property of asymmetry.
Chirality may also refer to:

</doc>
<doc id="6947" url="http://en.wikipedia.org/wiki?curid=6947" title="Campus">
Campus

A campus is traditionally the land on which a college or university and related institutional buildings are situated. Usually a campus includes libraries, lecture halls, residence halls, student centers or dining halls, and park-like settings. The definition currently describes a collection of buildings that belong to a given institution, either academic or non-academic.
Etymology.
The word derives from a Latin word for "field" and was first used to describe the grounds of a college at the College of New Jersey (now Princeton University) during the 18th century. Some other American colleges later adopted the word to describe individual fields at their own institutions, but "campus" did not yet describe the whole university property. A school might have one space called a campus, one called a field, and another called a yard.
History.
The tradition of a campus did not start in America, but with the medieval European universities where the students and teachers lived and worked together in a cloistered environment. It was the notion of the importance of the setting to academic life that migrated to America, and early colonial educational institutions were based on the Scottish and English collegiate system.
Uses.
The meaning expanded to include the whole institutional property during the 20th century, with the old meaning persisting into the 1950s in some places. 
Office buildings.
Sometimes the lands on which company office buildings sit, along with the buildings, are called campuses. The Microsoft Campus in Redmond, Washington is a good example. Hospitals, and even airports sometimes use the term to describe the territory of their facilities.
Universities.
The word "campus" has also been applied to European universities, although most such institutions are characterized by ownership of individual buildings in urban settings rather than park-like lawns in which buildings are placed.

</doc>
<doc id="6948" url="http://en.wikipedia.org/wiki?curid=6948" title="Crossbow">
Crossbow

A crossbow is a type of weapon, based on the bow, consisting of a horizontal bow-like assembly mounted on a stock, that shoots projectiles called bolts or quarrels. The medieval crossbow was called by many names, most of which derived from the word ballista, a torsion siege engine resembling a crossbow.
Historically, crossbows played a significant role in the warfare of East Asia, Europe and the Mediterranean. The invention of the crossbow caused a major shift in the role of ranged weaponry among armies, as the traditional bow and arrow had long been a specialized weapons system which required a considerable degree of lifetime training, physical strength and expertise to operate with any degree of efficiency; in many cultures, despite being usually drawn from the common class, bowmen were considered a separate and superior caste, as their archery skill-set (similar to many horseman cultures) was essentially developed from birth and impossible to reproduce outside a pre-established cultural tradition, which many nations lacked. In contrast, the crossbow was the first projectile weapon to be simple, cheap and physically-undemanding enough to be operated by large numbers of conscript soldiers, thus enabling virtually any nation with sufficient coin to field a potent force of ranged crossbowmen with little expense beyond the cost of the weapons themselves. This led to the ascendancy of large mercenary armies of crossbowmen (best exemplified by the Genoese crossbowmen), and the eventual death of the heavily armored aristocratic knight as armies became progressively dominated by conscripts equipped with increasingly-powerful ranged projectile weapons.
In modern times, although largely supplanted by firearms in most roles, crossbows are still widely used for shooting sports, hunting, and when shooting in relative silence is an important consideration.
Construction.
A crossbow is a bow mounted on a stick (called a tiller or stock) with a mechanism in it which holds the drawn bow string. The earliest designs featured a slot in the stock, down into which the string was placed. To shoot this design, a vertical rod is thrust up through a hole in the bottom of the notch, forcing the string out. This rod is usually attached perpendicular to a rear-facing lever called a trigger or "tickler". A later design implemented a rolling cylindrical pawl called a "nut" to retain the string. This nut has a perpendicular center slot for the bolt, and an intersecting axial slot for the string, along with a lower face or slot against which the internal trigger sits. They often also have some form of strengthening internal "sear" or trigger face, usually of metal. These "roller nuts" were either free-floating in their close-fitting hole across the stock, tied in with a binding of sinew or other strong cording, or mounted on a metal axle or pins. Removable or integral plates of wood, ivory or metal on the sides of the stock kept the nut in place laterally. Nuts were made of antler, bone, or metal. Bows could be kept and ready to shoot for some time with little effort, allowing crossbowmen to aim better.
The bow (called the "prod" or "lath" on a crossbow) of early crossbows was made of a single piece of wood, usually ash or yew. Composite bows are made from layers of different material—often wood, horn and sinew—glued together and bound with animal tendon. These composite bows, made of several layers, are much stronger and more efficient in releasing energy than simple wooden bows. As steel became more widely available in Europe around the 14th century, steel prods came into use.
The crossbow prod is very short compared to ordinary bows, resulting in a short draw length. This leads to a higher draw weight in order to store the same amount of energy. Furthermore the thick prods are a bit less efficient at releasing energy, but more energy can be stored by a crossbow. Traditionally the prod was often lashed to the stock with rope, whipcord, or other strong cording. This cording is called the "bridle".
The strings for a crossbow are typically made of strong fibers that would not tend to fray. Whipcord was very common; however linen, hemp, and sinew were used as well. In wet conditions, twisted mulberry root was occasionally used.
Very light crossbows can be drawn by hand, but heavier types need the help of mechanical devices. The simplest version of mechanical cocking device is a hook attached to a belt, drawing the bow by straightening the legs. Other devices are hinged levers which either pulled or pushed the string into place, cranked rack-and-pinion devices called "cranequins" and multiple cord-and-pulley cranked devices called windlasses.
Variants.
Crossbows exist in different variants. One way to classify them is the acceleration system, while another is the size and energy, degree of automation or projectiles.
A recurve crossbow is a bow that has tips curving away from the archer. The recurve bow's bent limbs have a longer draw length than an equivalent straight-limbed bow, giving more acceleration to the projectile and less hand shock. Recurved limbs also put greater strain on the materials used to make the bow, and they may make more noise with the shot.
Multiple bow systems have a special system of pulling the sinew via several bows (which can be recurve bows). The workings can be compared to a modern compound bow system. The weapon uses several different bows instead of one bow with a tackle system to achieve a higher acceleration of the sinew via the multiplication with each bow's pulling effect.
A compound crossbow is a modern crossbow and is similar to a compound bow. The limbs are usually much stiffer than those of a recurve crossbow. This limb stiffness makes the compound bow more energy efficient than other bows, but the limbs are too stiff to be drawn comfortably with a string attached directly to them. The compound bow has the string attached to the pulleys, one or both of which has one or more cables attached to the opposite limb. When the string is drawn back, the string causes the pulleys to turn. This causes the pulleys to pull the cables, which in turn causes the limbs to bend and thus store energy. Other types of compound bows use either (one or both) cam shaped or eccentrically mounted pulleys in order to provide a "let off", such that the archer is not holding against the maximum draw weight of the bow while trying to aim. But in a crossbow the string is held back mechanically, so there is no advantage in providing a let off. Therefore, compound crossbows generally use only pulleys that are both round and concentrically mounted, in order to capture the maximum available energy from the relatively short draw length.
The smallest crossbows are pistol crossbows. Others are simple long stocks with the crossbow mounted on them. These could be shot from under the arm. The next step in development was stocks of the shape that would later be used for firearms, which allowed better aiming. The arbalest was a heavy crossbow which required special systems for pulling the sinew via windlasses. For siege warfare the size of crossbows was further increased to hurl large projectiles such as rocks at fortifications. The required crossbows needed a massive base frame and powerful windlass devices. Such devices include the oxybeles. The ballista has torsion springs replacing the elastic prod of the oxybeles, but later also developed into smaller versions. "Ballista" is still the root word for crossbow in Romance languages such as Italian ("balestra") and Spanish ("ballesta").
The repeating crossbow automated the separate actions of stringing the bow, placing the projectile and shooting. This way the task can be accomplished with a simple one-handed movement, while keeping the weapon stationary. As a result, it is possible to shoot at a faster rate compared to unmodified version. The Greek Polybolos was an ancient repeating ballista reputedly invented by Dionysius of Alexandria in the 3rd century BC. The Chinese repeating crossbow, Chu Ko Nu, is a handheld crossbow that accomplishes the task with a magazine containing a number of bolts on top. The mechanism is worked by moving a rectangular lever forward and backward. The weapon was mainly used as a weapon against lightly armored soldiers, since it shot small bolts that were often dipped in poison.
A bullet crossbow is a type of handheld crossbow which rather than arrows or bolts shoots spherical projectiles made of stone, clay or lead. There are two variants; one has a double string with a pocket for the projectile, and the other has a barrel with a slot for the string.
Projectiles.
The arrow-like projectiles of a crossbow are called bolts. These are much shorter than arrows, but can be several times heavier. There is an optimum weight for bolts to achieve maximum kinetic energy, which varies depending on the strength and characteristics of the crossbow, but most could pass through common chain mail. In ancient times the bolts of a strong crossbow were usually several times heavier than arrows. Modern bolts are stamped with a proof mark to ensure their consistent weight and do not have fletching, i.e. feathered ends like those commonly seen on arrows. Crossbow bolts can be fitted with a variety of heads, some with sickle-shaped heads to cut rope or rigging; but the most common today is a four-sided point called a quarrel. A highly specialized type of bolt is employed to collect blubber biopsy samples used in biology research.
Most modern crossbows are designed to shoot arrows instead of bolts. Crossbow arrows are of similar construction to ordinary bow arrows, just shorter in length because of reduced power stroke.
Crossbows can also be adapted to shoot lead bullets or rocks, in which case they are called stone-bows. Primarily used for hunting wildfowl, these usually have a double string with a pouch between the strings to hold the projectile.
Accessories.
The ancient crossbow often included a metal grid serving as iron sights. Modern crossbow sights often use similar technology to modern firearm sights, such as red dot sights and telescopic sights. Many crossbow scopes feature multiple crosshairs to compensate for the significant effects of gravity over different ranges. In most cases, a newly-bought crossbow will need to be "sighted" for accurate shooting.
Quivers can be mounted to hold ammunition. These are often made from plastic and usually hold the bolts in fixed positions along the structure. A popular detachable design consists of a main arm that is attached to the weapon, a plate on one end that secures four or more individual bolts at a point on their shafts and at the other end a cover that secures their heads. This kind of quiver is attached under the front of the crossbow, parallel to the string and is designed to be quickly detached and reattached. Other designs hold bolts underneath the crossbow parallel to the stock, sometimes on either side of the crossbow.
A major cause of the sound of shooting a crossbow is vibration of various components. Crossbow silencers are multiple components placed on high vibration parts, such as the string and limbs, to dampen vibration and suppress the sound of loosing the bolt.
History.
East Asia.
 
According to Sir Joseph Needham in his Science and Civilisation in China, though there is no way of answering the question of whether the crossbow first arose among the cultures neighboring ancient China before the rise of Chinese culture in their midst, or whether it spread outwards from China to all the environing peoples; the former seems the more probable hypothesis given linguistic evidence, which posits that the Chinese word for 'crossbow' came from an Austroasiatic language.
Bronze crossbow bolts dating as early as the mid-5th century BC were found at a State of Chu burial site in Yutaishan, Hubei. The earliest handheld crossbow stocks with bronze trigger, dating from the 6th century BC, comes from Tomb 3 and 12 found at Qufu, Shandong, capital of the State of Lu. Other early finds of crossbows were discovered in Tomb 138 at Saobatang, Hunan dated to the mid-4th century BC. Repeating crossbows, first mentioned in the "Records of the Three Kingdoms", were discovered in 1986 in Tomb 47 at Qinjiazui, Hubei dated to around the 4th century BC. The earliest Chinese document mentioning a crossbow is in scripts from the 4th to 3rd centuries BC attributed to the followers of Mozi. This source refers to the use of a giant crossbow in the 6th to 5th centuries BC, corresponding to the late Spring and Autumn Period. Sun Tzu's influential book "The Art of War" (first appearance dated in between 500 BC to 300 BC) refers in chapter V to the traits and in XII to the use of crossbows. One of the earliest reliable records of this weapon in warfare is from an ambush, the Battle of Ma-Ling in 341 BC. By the 200s BC, the crossbow () was well developed and quite widely used in China.
The earliest textual evidence of the "handheld" crossbow used in battle dates to the 4th century BC. Handheld crossbows with complex bronze trigger mechanisms have also been found with the Terracotta Army in the tomb of Qin Shihuang (r. 221–210 BC) that are similar to specimens from the subsequent Han Dynasty (202 BC–220 AD), while crossbowmen described in the Qin and Han Dynasty learned drill formations, some were even mounted as cavalry units, and Han Dynasty writers attributed the success of numerous battles against the Xiongnu to massed crossbow volleys. The bronze triggers were designed in such a way that they were able to store a large amount of energy within the bow when drawn, but was easily shot with little recoil when the trigger were pulled (this allowed it for precision shooting). The metal portions of the crossbow were also mass-produced with precision, with the bronze mechanisms being interchangeable. Finally, the Qin and Han Dynasties also developed crossbow shooting lines, with alternating rows of crossbowmen shooting and reloading in a manner similar to a musket firing line.
In Vietnamese historical legend, the ruler and general Thục Phán who ruled over the ancient kingdom of Âu Lạc from 257 to 207 BC is said to have owed his power to a magic crossbow, capable of shooting thousands of bolts at once.
Different varieties of crossbows were also developed, such as the repeating crossbow, multi-shot crossbow, and repeating multi-shot crossbow.
Ancient Greece.
The earliest reasonably reliable date for the crossbow in the Greek world is from the 5th century BC. The historian Diodorus Siculus (fl. 1st century BC), described the invention of a mechanical arrow shooting catapult ("katapeltikon") by a Greek task force in 399 BC. According to the inventor Hero of Alexandria (fl. 1st century AD), who referred to the now lost works of the 3rd-century BC engineer Ctesibius, this weapon was inspired by an earlier hand crossbow, called the "gastraphetes" ("belly shooter"), which could store more energy than the Greek bows. A detailed description of the "gastraphetes", along with a drawing, is found in Heron's technical treatise "Belopoeica". The "gastraphetes" was powered by a composite bow. It was cocked by resting the stomach in a concavity at the rear of the stock and pressing down with all strength. In this way considerably more energy can be summoned up than by using only one arm of the archer as in the hand-bow. The heavy weight and bulk of the "gastraphetes" may have necessitated a prop to keep it standing, i.e. by mounting it on a defensive wall or using a portable prop.
A third Greek author, Biton (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the "gastraphetes", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421 BC and 401 BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.
From the mid-4th century BC onwards, evidence of the Greek use of crossbows becomes more dense and varied: Arrow shooting machines ("katapeltai") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350 BC. An Athenian inventory from 330–329 BC includes catapults bolts with heads and flights. Arrow shooting machines in action are reported from Philip II's siege of Perinthos in Thrace in 340 BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, presumably to house anti-personnel arrow shooters, as in Aigosthena.
The transition to the torsion catapults, which are not considered crossbows and came to dominate Greek and Roman artillery design, is first evident in inventories of the Athenian arsenal from between 338 and 326 BC. Torsion weapons, which rely on the energy generated from twisted animal sinew, became siege weapons and light artillery – such as the Greek ballista or the Roman scorpion.
In Roman times the crossbow became to be known as arcuballista.
Roman Empire.
Besides the "gastraphetes", the ancient world knew a variety of mechanical hand-held weapons similar to the later medieval crossbow. The exact terminology is a subject of continuing scholarly debate.
Greek and Roman authors like Vegetius (fl. 4th century) note repeatedly the use of arrow shooting weapons such as "arcuballista" and "manuballista" respectively "cheiroballista". While most scholars agree that one or more of these terms refer to handheld mechanical weapons, there exist disagreement whether these were flexion bows or torsion powered like the recent Xanten find.
The Roman commander Arrian (c. 86 – after 146) records in his "Tactica" Roman cavalry training for shooting some mechanical handheld weapon from horseback.
Sculptural reliefs from Roman Gaul depict the use of crossbows in hunting scenes. The specimen are remarkably similar to the later medieval crossbow, including the typical nut lock (see image).
Medieval Europe.
The crossbow is portrayed as a hunting weapon on four Pictish stones from early medieval Scotland (6th to 9th centuries): St. Vigeans no. 1, Glenferness, Shandwick, and Meigle. The use of crossbows in European warfare is again evident from the Battle of Hastings until about the year 1500. They almost completely superseded hand bows in many European armies in the 12th century for a number of reasons. Although a longbow achieves comparable accuracy and faster shooting rate than an average crossbow, crossbows release more kinetic energy and can be used effectively after a week of training, while a comparable single-shot skill with a longbow takes years of strength training to overcome the draw strength of the longbow, as well as years of practice needed to use it with skill.
In the armies of Europe, mounted and unmounted crossbowmen, often mixed with slingers, javeliners and archers, occupied a central position in battle formations. Usually they engaged the enemy in offensive skirmishes before an assault of mounted knights. Crossbowmen were also valuable in counterattacks to protect their infantry. The rank of commanding officer of the crossbowmen corps was one of the highest positions in any army of this time. Along with polearm weapons made from farming equipment, the crossbow was also a weapon of choice for insurgent peasants such as the Taborites.
Mounted knights armed with lances proved ineffective against formations of pikemen combined with crossbowmen whose weapons could penetrate most knights' armor. The invention of pushlever and ratchet drawing mechanisms enabled the use of crossbows on horseback, leading to the development of new cavalry tactics. Knights and mercenaries deployed in triangular formations, with the most heavily armored knights at the front. Some of these riders would carry small, powerful all-metal crossbows of their own. Crossbows were eventually replaced in warfare by more powerful gunpowder weapons, although early guns had slower rates of fire and much worse accuracy than contemporary crossbows. Later, similar competing tactics would feature harquebusiers or musketeers in formation with pikemen (pike and shot), pitted against cavalry firing pistols or carbines.
Elsewhere.
The Saracens called the crossbow "qaws Ferengi", or "Frankish bow," as the Crusaders used the crossbow against the Arab and Turkoman horsemen with remarkable success. The adapted crossbow was used by the Islamic armies in defence of their castles. Later footstrapped version become very popular among the Muslim armies in Iberia. During the Crusades, Europeans were exposed to Saracen composite bows, made from layers of different material—often wood, horn and sinew—glued together and bound with animal tendon. These composite bows could be much more powerful than wooden bows, and were adopted for crossbow prods across Europe.
Crossbow prods could be more easily waterproofed than hand bows, which was essential in the European humid climate.
In Western Africa and Central Africa, crossbows served as a scouting weapon and for hunting, with the Spanish and the Portuguese bringing the technology to America. In the American South, the crossbow was used for hunting and warfare when firearms or gunpowder were unavailable because of economic hardships or isolation.
In Northern America, light hunting crossbows were traditionally used by the Inuit .
The native Montagnards of Vietnam's Central Highlands were also known to have used crossbows, as both a tool for hunting, and later, an effective weapon against the Viet Cong during the Vietnam War. Montagnard fighters armed with crossbows proved a highly valuable asset to the US Special Forces operating in Vietnam, and it was not uncommon for the Green Berets to integrate Montagnard crossbowmen into their strike teams.
Modern use.
Hunting, leisure and science.
Crossbows are used for shooting sports and bowhunting in modern archery and for blubber biopsy samples in scientific research. In some countries such as Canada or the United Kingdom, they may be less heavily regulated than firearms, and thus more popular for hunting; some jurisdictions have bow and/or crossbow only seasons.
Modern military and paramilitary use.
In modern times crossbows are no longer used for assassinations, but there are still some applications. For example, in the Americas, the Peruvian army (Ejército) equips some soldiers with crossbows and rope, to establish a zip-line in difficult terrain. In Brazil the CIGS (Jungle Warfare Training Center) also trains soldiers in the use of crossbows. In the United States, SAA International Ltd manufacture a 150 ft·lb crossbow-launched version of the U.S. Army type classified Launched Grapnel Hook (LGH), among other mine countermeasure solutions designed for the middle-eastern theatre. It has been successfully evaluated in Cambodia and Bosnia. It is used to probe for and detonate tripwire initiated mines and booby traps at up to 50 meters. The concept is similar to the LGH device originally only fired from a rifle, as a plastic retrieval line is attached. Reusable up to 20 times, the line can be reeled back in without exposing oneself. The device is of particular use in tactical situations where noise discipline is important.
In Europe, British-based Barnett International supplied crossbows to Serbian forces which according to "The Guardian" were later used "in ambushes and as a counter-sniper weapon", against the Kosovo Liberation Army during the Kosovo War in the areas of Pec and Djakovica, south west of Kosovo. Whitehall launched an investigation, though the department of trade and industry established that not being "on the military list" crossbows were not covered by such export regulations. Paul Beaver of Jane's defence publications commented that, "They are not only a silent killer, they also have a psychological effect". On 15 February 2008 Serbian Minister of Defence Dragan Sutanovac was pictured testing a Barnett crossbow during a public exercise of the Serbian army's Special Forces in Nis, 200 km south of capital Belgrade. Special forces in both Greece and Turkey also continue to employ the crossbow. Spain's Green Berets still use the crossbow as well.
In Asia, some Chinese armed forces use crossbows, including the special force Snow Leopard Commando Unit of the People's Armed Police and the People's Liberation Army. One justification for this comes in the crossbow's ability to stop persons carrying explosives without risk of causing detonation. During the Xinjiang riots of July 2009, crossbows were used alongside modern military hardware to quell protests. The Indian Navy's Marine Commando Force were equipped until the late 1980s with crossbows supplied with cyanide-tipped bolts, as an alternative to suppressed handguns.
Comparison to conventional bows.
With a crossbow, archers could release a draw force far in excess of what they could have handled with a bow. Furthermore the crossbow could hold the tension for a long time, whereas even the strongest longbowman could only hold a drawn bow for so long. The disadvantage is the greater weight and clumsiness compared to a bow, as well as the slower rate of shooting and the lower efficiency of the acceleration system, but there would be reduced elastic hysteresis, making the crossbow a more accurate weapon.
Crossbows have a much smaller draw length than bows. This means that for the same energy to be imparted to the arrow (or bolt), the crossbow has to have a much higher draw weight.
A direct comparison between a fast hand-drawn replica crossbow and a longbow show a 6:10 rate of shooting or a 4:9 rate within 30 seconds and comparable weapons.
Legal issues.
Can. 29 of the Second Lateran Council under Pope Innocent II in 1139 banned the use of crossbows, as well as slings and bows, against Christians. Although the authenticity, interpretation and translation of this source is contested.
Today, the crossbow often has a complicated legal status due to the possibility of lethal use and its similarities to both firearms and archery weapons. While some jurisdictions regard crossbows the same as firearms, many others do not require any sort of license to own a crossbow — even for people, such as felons, who may not legally possess a firearm. The legality of using a crossbow for hunting varies widely around the world, and even within different jurisdictions of some federal countries.

</doc>
<doc id="6949" url="http://en.wikipedia.org/wiki?curid=6949" title="Carbamazepine">
Carbamazepine

Carbamazepine (CBZ) (Tegretol, Equetro) is an anticonvulsant and mood-stabilizing drug used primarily in the treatment of epilepsy and bipolar disorder, as well as trigeminal neuralgia. Off-label uses include attention-deficit hyperactivity disorder, schizophrenia, phantom limb syndrome, complex regional pain syndrome, borderline personality disorder, and post-traumatic stress disorder.
Studies on the use of carbamazepine in pregnant women have demonstrated exposure of the fetus to the drug and its metabolites to be teratogenic and is associated with the development of spina bifida, neurodevelopmental problems, craniofacial defects, cardiovascular malformations, hypospadias, and developmental delays.
It is on the World Health Organization's List of Essential Medicines, a list of the most important medications needed in a basic health system.
Medical uses.
Carbamazepine is typically used for the treatment of seizure disorders and neuropathic pain. It may be used off-label as a second-line treatment for bipolar disorder and as an adjunct, never alone, with an antipsychotic in some cases of schizophrenia when treatment with a conventional antipsychotic alone has failed.
In the United States, the FDA-approved indications are epilepsy (including partial seizures, generalized tonic-clonic seizures and mixed seizures), trigeminal neuralgia, and manic and mixed episodes of bipolar I disorder. Although data are still lacking, carbamazepine appears to be as effective and safe as lithium for the treatment of bipolar disorder, both in the acute and maintenance phases.
Adverse effects.
Common adverse effects may include drowsiness, dizziness, headaches and migraines, motor coordination impairment, nausea, vomiting and/or constipation. Alcohol use while taking carbamazepine may lead to enhanced depression of the central nervous system.
Less common side effects may include cardiac arrhythmias, blurry or double vision, and/or the temporary loss of blood cells or platelets and in rare cases can cause aplastic anemia or agranulocytosis. With normal use, small reductions in white cell count and serum sodium levels are common; however, in rare cases, the loss of platelets may become life-threatening. In this case, a doctor may recommend frequent blood tests during the first few months of use, followed by three to four tests per year for established patients. Additionally, carbamazepine may possibly exacerbate pre-existing cases of hypothyroidism, so yearly thyroid function tests are advisable for persons taking the drug.
Also, rare reports of an auditory side effect for carbamazepine use have been made, whereby patients perceive sounds about a semitone lower than previously. Thus, middle C would be heard as the note B3 just below it, and so on. The inverse effect (that is, notes sounding higher) has also been recorded. This unusual side effect is usually not noticed by most people, and quickly disappears after the person stops taking carbamazepine.
Oxcarbazepine, a derivative of carbamazepine, reportedly has fewer and less serious side effects.
Carbamazepine may cause syndrome of inappropriate antidiuretic hormone, since it both increases the release and potentiates the action of ADH (vasopressin).
Carbamazepine may aggravate juvenile myoclonic epilepsy, so it is important to uncover any history of jerking, especially in the morning, before starting the drug. It may also aggravate other types of generalized seizure disorders, particularly absence seizures.
In addition, carbamazepine has been linked to serious adverse cognitive anomalies, including EEG slowing and apoptosis of cultured cerebellar neurons.
The FDA informed health-care professionals that dangerous or even fatal skin reactions (Stevens–Johnson syndrome and toxic epidermal necrolysis), that can be caused by carbamazepine therapy, are significantly more common in patients with a particular human leukocyte antigen allele, "HLA-B*1502". This allele occurs almost exclusively in patients with ancestry across broad areas of Asia, including South Asian Indians. In Europeans, a large proportion of sensitivity is associated with "HLA-B58". Researchers have also identified another genetic variant, "HLA-A*3101", which has been shown to be a strong predictor of both mild and severe adverse reactions to carbamazepine among Japanese and Europeans.
Associated birth defects.
If taken during pregnancy, carbamazepine can cause birth defects that include cardiovascular and urinary tract anomalies, craniofacial defects such as cleft palate, fingernail hypoplasia, microcephaly, developmental delays, and intrauterine growth restrictions.
Interactions.
Carbamazepine has a potential for drug interactions; caution should be used in combining other medicines with it, including other antiepileptics and mood stabilizers. Lower levels of carbamazepine are seen when administrated with phenobarbital, phenytoin (Dilantin), or primidone (Mysoline), which can result in breakthrough seizure activity. Carbamazepine, as a CYP450 inducer, may increase clearance of many drugs, decreasing their concentration in the blood to subtherapeutic levels and reducing their desired effects. Drugs that are more rapidly metabolized with carbamazepine include warfarin (Coumadin), lamotrigine (Lamictal), phenytoin (Dilantin), theophylline, and valproic acid (Depakote, Depakote ER, Depakene, Depacon). Drugs that decrease the metabolism of carbamazepine or otherwise increase its levels include erythromycin, cimetidine (Tagamet), propoxyphene (Darvon), and calcium channel blockers. Carbamazepine also increases the metabolism of the hormones in birth control pills and can reduce their effectiveness, potentially leading to unexpected pregnancies. As a drug that induces cytochrome P450 enzymes, it accelerates elimination of many benzodiazepines and decreases their action.
Valproic acid and valnoctamide both inhibit microsomal epoxide hydrolase (MEH), the enzyme responsible for the breakdown of carbamazepine-10,11 epoxide into inactive metabolites. By inhibiting MEH, valproic acid and valnoctamide cause a build-up of the active metabolite, prolonging the effects of carbamazepine and delaying its excretion.
Grapefruit juice raises the bioavailability of carbamazepine by inhibiting CYP3A4 enzymes in the gut wall and in the liver.
Environmental impact.
Carbamazepine can enter the environment through discharge of wastewater, and has also been shown to persist and accumulate in the organic components of soil and sludge. As carbamazepine is an emerging contaminant, the effects of its bioaccumulation on other living creatures and plants are not well understood.
Pharmacokinetics.
Carbamazepine is relatively slowly but well absorbed after oral administration. Its plasma half-life is about 30 hours when it is given as single dose, but it is a strong inducer of hepatic enzymes and the plasma half-life shortens to about 15 hours when it is given repeatedly. Some of its metabolites have antiepileptic properties. A slow-release preparation is used for patients who experience transient side effects coinciding with plasma concentration peaks following oral dosing.
Mechanism of action.
The mechanism of action of carbamazepine and its derivatives is relatively well understood. Carbamazepine stabilizes the inactivated state of voltage-gated sodium channels, making fewer of these channels available to subsequently open. This leaves the affected cells less excitable until the drug dissociates. Carbamazepine has also been shown to potentiate GABA receptors made up of alpha1, beta2, and gamma2 subunits. This mechanism may contribute to its efficacy in neuropathic pain and manic-depressive illness.
History.
Carbamazepine was discovered by chemist Walter Schindler at J.R. Geigy AG (now part of Novartis) in Basel, Switzerland, in 1953. Schindler then synthesized the drug in 1960, before its antiepileptic properties had been discovered. It was first marketed as a drug to treat trigeminal neuralgia (formerly known as tic douloureux) in 1962. It has been used as an anticonvulsant and antiepileptic in the UK since 1965, and has been approved in the US since 1974.
In 1971, Drs. Takezaki and Hanaoka first used carbamazepine to control mania in patients refractory to antipsychotics (lithium was not available in Japan at that time). Dr. Okuma, working independently, did the same thing with success. As they were also epileptologists, they had some familiarity with the antiaggression effects of this drug. Carbamazepine was studied for bipolar disorder throughout the 1970s.
Brand names.
Carbamazepine has been sold under the names Biston, Calepsin, Carbatrol, Epitol, Equetro, Finlepsin, Sirtal, Stazepine, Telesmin, Tegretol, Epitab XR, Teril, Timonil, Trimonil, Epimaz, Carbama/Carbamaze, Amizepin, Carzine, Mazetol, Tegrital, Tegrita, Zeptol, Karbapin, Hermolepsin, Degranol, and Tegretal.
Synthesis.
Carbamazepine, 5H-dibenz[b,f]azepine-5-carboxamide, is synthesized by reacting 5H-dibenz[b,f]azepine and phosgene, which forms 5-chlorcarboxy-5H-dibenz-[b,f]azepine), and its subsequent reaction with ammonia to give the desired carbamazepine. Coll. means collidine.
An alternative method of synthesis is the direct reaction of 5H-dibenz[b,f]azepine with potassium cyanate.
Carbamazepine is a dibenzazepine.

</doc>
<doc id="6951" url="http://en.wikipedia.org/wiki?curid=6951" title="CCIR">
CCIR

CCIR is a four-letter abbreviation that may stand for:

</doc>
<doc id="6955" url="http://en.wikipedia.org/wiki?curid=6955" title="Chalcedonian Definition">
Chalcedonian Definition

The Chalcedonian Definition (also Confession or Creed of Chalcedon) was adopted in 451 at the Council of Chalcedon in Asia Minor. That council was the fourth of the first seven Ecumenical Councils, which are accepted by Chalcedonian churches (Eastern Orthodox, Catholic, and many Protestant churches). It is the first Council "not" recognized by any of the Oriental Orthodox churches, which may be classified as non-Chalcedonian.
The Definition defines that Christ is 'acknowledged in two natures', which 'come together into one person and one hypostasis'. The formal definition of 'two natures' in Christ was understood by the critics of the council at the time, and is understood by many historians and theologians today, to side with western and Antiochene Christology and to diverge from the teaching of Cyril of Alexandria, who always stressed that Christ is 'one'. However, a modern analysis of the sources of the creed (by A. de Halleux, in Revue Theologique de Louvain 7, 1976) and a reading of the acts, or proceedings, of the council (recently translated into English) show that the bishops considered Cyril the great authority and that even the language of 'two natures' derives from him.
Oriental Orthodox dissent.
The Chalcedonian Definition was written amid controversy between the western and eastern churches over the meaning of the Incarnation (see Christology), the ecclesiastical influence of the emperor, and the supremacy of the Bishop of Rome. The western churches readily accepted the creed, but some eastern churches did not.
It became standard orthodox doctrine. However the Coptic Church of Alexandria dissented, holding to Cyril of Alexandria's preferred formula for the oneness of Christ’s nature in the incarnation of God the Word as "out of two natures". Cyril's language is not consistent and he may have countenanced the view that it is possible to contemplate in theory two natures after the incarnation, but the Church of Alexandria felt that the Definition should have stated that Christ be acknowledged "out of two natures" rather than "in two natures".
This miaphysite position, historically characterised by Chalcedonian followers as "monophysitism" though this is denied by the dissenters, formed the basis for the distinction from other churches of the Coptic Church of Egypt and Ethiopia and the "Jacobite" churches of Syria and Armenia (see Oriental Orthodoxy). Over the last 30 years, however, the miaphysite position has been accepted as a mere restatement of orthodox belief by Patriarch Bartholomew I of the Eastern Orthodox Church and by Pope John Paul II of the Roman Catholic Church.
English translation.
The key section runs:<br>
Following, then, the holy Fathers, we all unanimously teach that our Lord Jesus Christ is to us 
One and the same Son, the Self-same Perfect in Godhead, the Self-same Perfect in Manhood; truly God and truly Man; the Self-same of a rational soul and body; co-essential with the Father according to the Godhead, the Self-same co-essential with us according to the Manhood; like us in all things, sin apart; before the ages begotten of the Father as to the Godhead, but in the last days, the Self-same, for us and for our salvation (born) of Mary the Virgin Theotokos as to the Manhood; One and the Same Christ, Son, Lord, Only-begotten; acknowledged in Two Natures unconfusedly, unchangeably, indivisibly, inseparably; the difference of the Natures being in no way removed because of the Union, but rather the properties of each Nature being preserved, and (both) concurring into One Person and One Hypostasis; not as though He were parted or divided into Two Persons, but One and the Self-same Son and Only-begotten God, Word, Lord, Jesus Christ; even as from the beginning the prophets have taught concerning Him, and as the Lord Jesus Christ Himself hath taught us, and as the Symbol of the Fathers hath handed down to us.

</doc>
<doc id="6956" url="http://en.wikipedia.org/wiki?curid=6956" title="Conservation law">
Conservation law

In physics, a conservation law states that a particular measurable property of an isolated physical system does not change as the system evolves. 
One particularly important physical result concerning laws of conservation is Noether's theorem, which states that there is a one-to-one correspondence between laws of conservation and differentiable symmetries of physical systems. For example, the conservation of energy follows from the time-invariance of physical systems, and the fact that physical systems behave the same regardless of how they are oriented in space gives rise to the conservation of angular momentum.
Exact laws.
A partial listing of physical laws of conservation that are said to be exact laws, or more precisely "have never been [proven to be] violated:"
Approximate laws.
There are also approximate conservation laws. These are approximately true in particular situations, such as low speeds, short time scales, or certain interactions. 

</doc>
<doc id="6959" url="http://en.wikipedia.org/wiki?curid=6959" title="Chord">
Chord

Chord may refer to:
Chord may also refer to:
The Chords may refer to:
Chords may refer to:

</doc>
<doc id="6960" url="http://en.wikipedia.org/wiki?curid=6960" title="Car Talk">
Car Talk

Car Talk is a Peabody Award-winning radio talk show broadcast weekly on NPR stations and elsewhere. Its subjects were automobiles and automotive repair, discussed often in a humorous way. It was hosted by brothers Tom and Ray Magliozzi, known also as "Click and Clack, the Tappet Brothers".
The show was produced from 1977 to October 2012, until the Magliozzi brothers retired. Edited reruns continue to be available for airing on NPR affiliates.
Show.
"Car Talk" was presented in the form of a call-in radio show: listeners called in with questions related to motor vehicle maintenance and repair. Most of the advice sought was diagnostic, with callers describing symptoms and demonstrating sounds of an ailing vehicle while the Magliozzis make an attempt at identifying the malfunction. While the hosts peppered their call-in sessions with jokes directed at both the caller and at themselves, the Magliozzis were usually able to arrive at a diagnosis and give helpful advice. However, when they are stumped, they attempt anyway with an answer they claim is "unencumbered by the thought process", the official motto of the show. 
Edited reruns are carried on Sirius XM Radio via both the Public Radio and NPR Now channels.
The "Car Talk" theme song is "Dawggy Mountain Breakdown" by bluegrass artist David Grisman.
Call-in procedure.
Throughout the program, listeners were encouraged to dial the toll-free telephone number, 1-888-CAR-TALK (1-888-227-8255), which connected to a 24-hour answering service. Although the approximately 2,000 queries received each week were screened by the "Car Talk" staff, the questions were unknown to the Magliozzis in advance as "that would entail researching the right answer, which is what? ...Work." Producers selected and contacted the callers several days ahead of the show's Wednesday taping to arrange the segment. The caller spoke briefly to a producer before being connected live with the hosts, and was given little coaching other than being told to be prepared to talk, not to use any written preparation and to "have fun". The show deliberately taped more callers than it has time to air each week in order to be able to choose the best ones for broadcast. Those segments that did make it to air were generally edited for time. For the last four years of the show, new shows included previously broadcast segments as much as 10 years old. The re-used segments, including re-used puzzlers, were not acknowledged as old material and sometimes new caller material was mixed in alongside the recycled calls.
Features.
The show originally consisted of two segments with a break in between. Then the show was changed to three segments. The hosts used to refer to content coming up in the second half of the program. Ever since the shift to the three segment format, it became a running joke to refer to the last segment as "the third half" of the program.
The show opened with a short comedy segment, typically jokes sent in by listeners, followed by eight call-in sessions. The hosts ran a contest called the "Puzzler", in which a riddle, sometimes car-related, was presented. The answer to the previous week's "Puzzler" was given at the beginning of the "second half" of the show, and a new "Puzzler" was given at the start of the "third half". The hosts gave instructions to listeners to write answers addressed to "Puzzler Tower" on some non-existent or expensive object, such as a "$26 bill" or an advanced digital SLR camera. This gag initially started as suggestions that the answers be written "on the back of a $20 bill". A running gag concerned Tom's inability to remember the previous week's "Puzzler" without heavy prompting from Ray. For each puzzler, one correct answer was chosen at random, with the winner receiving a $26 gift certificate to the "Car Talk" store, referred to as the "Shameless Commerce Division". It was originally $25, but was increased for inflation after a few years. Originally, the winner received a specific item from the store, but it soon changed to a gift certificate to allow the winner to choose the item they wanted (though Tom often made an item suggestion).
A recurring feature was "Stump the Chumps," in which the hosts revisited a caller from a previous show to determine the accuracy and the effect, if any, of their advice. A similar feature began in May 2001, "Where Are They Now, Tommy?" It began with a comical musical theme with a sputtering, backfiring car engine and a horn as a backdrop. Tom then announced who the previous caller was, followed by a short replay of the essence of the previous caller was played, preceded and followed by harp music often used in other audiovisual media to indicate recalling and returning from a dream. The hosts then greeted the previous caller, asking them if there have been any influences on the answer they're about to relate, such as arcane bribes by the NPR staff. The repair story was then discussed, followed by a fanfare and applause if the Tappet Brothers' diagnosis was correct, or a wah-wah-wah music piece mixed with a car starter operated by a weak battery (an engine which won't start) if the diagnosis was wrong. The hosts then thanked the caller for their return appearance.
The brothers also had an official Animal-Vehicle Biologist and Wildlife Guru named Kieran Lindsey. She answered questions like "How do I remove a snake from my car?" and offered advice on how those living in cities and suburbs can reconnect with wildlife.
 Celebrities have been callers as well. Examples include Geena Davis, Morley Safer, Ashley Judd, Gordon Elliott, former Major League pitcher Bill Lee, and astronaut John Grunsfeld calling from the Space Shuttle.
There were numerous appearances from NPR personalities, including Bob Edwards, Susan Stamberg, Scott Simon, Ray Suarez, Will Shortz, Sylvia Poggioli, and commentator and author Daniel Pinkwater. On one occasion, the show featured Martha Stewart as an in-studio guest, whom the Magliozzis twice during the segment referred to as "Margaret".
In addition to at least one on-orbit call, the Brothers once received a call asking advice on winterizing a couple of "kit cars". After much beating around the bush and increasing evasiveness by the caller, they asked him just how much these kit cars were worth. The answer: about $800 million. It was a joke call from the Jet Propulsion Laboratory concerning the preparation of the Mars Rovers ("Spirit" and "Opportunity") for the oncoming Martian winter. Click and Clack have also been featured in editorial cartoons, including one where a befuddled NASA engineer calls them to ask how to fix the Space Shuttle.
Humor.
Humor and wisecracking pervaded the program. Tom and Ray are known for their self-deprecating humor, often joking about the supposedly poor quality of their advice and the show in general. They also commented at the end of each show: "Well, it's happened again — you've squandered another perfectly good hour listening to "Car Talk"."
At some point in almost every show, usually when giving the address for the Puzzler answers, Ray mentioned Cambridge, Massachusetts (where the show originates), at which point Tom reverently interjected with a tone of civic pride, "Our fair city." Ray invariably mocked "Cambridge, MA" the US Postal Service's two letter abbreviation for "Massachusetts" by pronouncing it as a word.
Leading into each break in the show, one of the hosts led up to the network identification with a humorous take on a disgusted reaction of some usually famous person to hearing that identification. The full line went along the pattern of, for example, "And even though Roger Clemens stabs his radio with a syringe whenever he hears "us" say it, this is NPR: National Public Radio (later just '...this is NPR')."
The ending credits of the show started with thanks to the colorfully nicknamed actual staffers: producer Doug "the subway fugitive, not a slave to fashion, bongo boy frogman" Berman; "John 'Bugsy' Lawlor, just back from the..." every week a different eating event with rhyming foodstuff names; David "Calves of Belleville" Greene; Catherine "Frau Blücher" Fenollosa, whose name caused a horse to neigh and gallop (an allusion to a running gag in the movie "Young Frankenstein"); and Carly "High Voltage" Nix, among others. Following the real staff was a lengthy list of pun-filled fictional staffers and sponsors such as statistician Marge Innovera ("margin of error"), customer care representative Haywood Jabuzoff ("Hey, would ya buzz off"), meteorologist Claudio Vernight ("cloudy overnight"), optometric firm C.F. Eye Care ("see if I care"), Russian chauffeur Pikov Andropov ("pick up and drop off"), Leo Tolstoy biographer Warren Peace ("War and Peace"), hygiene officer and chief of the Tokyo office Otaka Shawa ("oh take a shower"), Swedish snow-board instructor Soren Derkeister ("sore in the keister"), law firm Dewey, Cheetham & Howe ("Do we cheat 'em? And how!"), and many, many others, usually concluding with Erasmus B. Dragon, whose job title varied, but who was often said to be head of the show's working mothers' support group. They sometimes advised that "our chief counsel from the law firm of Dewey, Cheetham, & Howe is Hugh Louis Dewey, known to [group of people] in Harvard Square as Huey Louie Dewey." Huey, Louie, and Dewey were the juvenile nephews being raised by Donald Duck in Walt Disney's Comics and Stories.
At the end of the show, Ray warned the audience, "Don't drive like my brother," to which Tom replied, "And don't drive like "my" brother." The original tag line was "Don't drive like a knucklehead." There have been variations such as, "Don't drive like my brother..." "And don't drive like his brother," and "Don't drive like my sister..." "And don't drive like "my" sister." The tagline was heard in a cameo for the Pixar film "Cars", in which Tom and Ray voiced anthropomorphized vehicles (Rusty and Dusty Rust-Eze, respectively a 1963 Dodge Dart V1.0 and 1963 Dodge A100 van, as Lightning McQueen's racing sponsors) with personalities similar to their own on-air personae. Tom notoriously once owned a "convertible, green with large areas of rust" Dodge Dart, known jokingly on the program by the faux-elegant name "Dartre".
History.
"Car Talk" was first broadcast on WBUR in Boston in 1977 and was picked up nationally by NPR ten years later.
In 1992, "Car Talk" won a Peabody Award, saying "Each week, master mechanics Tom and Ray Magliozzi provide useful information about preserving and protecting our cars. But the real core of this program is what it tells us about human mechanics...The insight and laughter provided by Messrs. Magliozzi, in conjunction with their producer Doug Berman, provide a weekly mental tune-up for a vast and ever-growing public radio audience."
In May 2007, the program, which previously had been available only digitally as a paid subscription from Audible.com, became a free podcast distributed by NPR, after a two-month test period where only a "call of the week" was available via podcast.
As of 2012, it had 3.3 million listeners each week, on about 660 stations. On June 8, 2012, the brothers announced that they would no longer broadcast new episodes as of October. Executive producer Doug Berman said the best material from 25 years of past shows would be used to put together "repurposed" shows for NPR to broadcast. Berman estimated the archives contain enough for eight years' worth of material before anything would have to be repeated. The brothers will continue to write their syndicated newspaper column.
Hosts.
The Magliozzis are long-time car mechanics. Ray Magliozzi has a bachelor of science degree in humanities and science from MIT, while Tom has a bachelor of science degree in economics from MIT and an MBA and DBA from the Boston University Graduate School of Management.
The duo, usually led by Ray, were known for rants on the evils of the internal combustion engine, people who talk on cell phones while driving, Peugeots, women named Donna who always seem to drive Camaros, lawyers, the clever use of the English language, people who choose to live in Alaska (or similar snowy, icy climates), and practically anything else, including themselves. They had a relaxed and humorous approach to cars, car repair, cup holders, pets, lawyers, car repair mechanics, SUVs, and almost everything else. They often cast a critical, jaundiced insider's eye toward the auto industry. Tom and Ray are committed to the values of defensive driving and environmentalism.
The Magliozzis operate a garage. The show's offices were located nearby at the corner of JFK Street and Brattle Street in Harvard Square, marked as "Dewey, Cheetham & Howe", the imaginary law firm to which they refer on-air. DC&H doubled as the business name of Tappet Brothers Associates, the corporation established to manage the business end of "Car Talk". Initially a joke, the company was incorporated after the show expanded from a single station to national syndication.
The two were commencement speakers at MIT in 1999.
Executive producer Doug Berman said in 2012, "The guys are culturally right up there with Mark Twain and the Marx Brothers. They will stand the test of time. People will still be enjoying them years from now. They're that good."
Adaptations.
The show was the inspiration for the short-lived "The George Wendt Show", which briefly aired on CBS in the 1994-95 season- as a mid-season replacement.
In July 2007, PBS announced that it had greenlit an animated adaptation of "Car Talk", to air on prime-time in 2008. The show, titled "Click and Clack's As the Wrench Turns" is based on the adventures of the fictional "Click and Clack" brothers' garage at "Car Talk Plaza". The ten episodes aired in July and August 2008.
"Car Talk: The Musical!!!" was written and directed by Wesley Savick, and composed by Michael Wartofsky. The adaptation was presented by Suffolk University, and opened on March 31, 2011, at the Modern Theatre in Boston, Massachusetts. The play was not officially endorsed by the Magliozzis, but they participated in the production, lending their voices to a central puppet character named "The Wizard of Cahs".

</doc>
<doc id="6962" url="http://en.wikipedia.org/wiki?curid=6962" title="Council of Chalcedon">
Council of Chalcedon

The Council of Chalcedon ( or ) was a church council held from October 8 to November 1, AD 451, at Chalcedon (a city of Bithynia in Asia Minor), on the Asian side of the Bosporus, known in modern times as Kadıköy in Istanbul, although it was then separate from Constantinople. 
The judgements and definitions of divine nature issued by the council marked a significant turning point in the Christological debates that led to the separate establishment of the church in the Western Roman Empire during the 5th century. Many Anglicans and most Protestants consider it to be the last ecumenical council. These churches, per Martin Luther, hold that both conscience and scripture preempt doctrinal councils and generally agree that the conclusions of later councils are unsupported by or contradictory to scripture. 
The Council of Chalcedon was convened by Emperor Marcian, with the reluctant approval of Pope Leo the Great, to set aside the 449 Second Council of Ephesus which would become known as the "Latrocinium" or "Robber Council". The Council of Chalcedon issued the 'Chalcedonian Definition,' which repudiated the notion of a single nature in Christ, and declared that he has two natures in one person and hypostasis; it also insisted on the completeness of his two natures: Godhead and manhood. The council also issued 27 disciplinary canons governing church administration and authority. In a further decree, later known as the canon 28, the bishops declared the See of Constantinople (New Rome) equal in honor and authority to Rome.
The Council is considered to have been the Fourth Ecumenical Council by the Eastern Orthodox Church, the Roman Catholic Church (including its Eastern Catholic Churches), the Old Catholics, and various other Western Christian groups. As such, it is recognized as infallible in its dogmatic definitions by the Roman Catholic and Eastern Orthodox Churches (then one church). Most Protestants also consider the concepts of the Trinity and Incarnation as defined at Nicaea (in 325) and Chalcedon to be orthodox doctrine to which they adhere. However, the Council is not accepted by several of the ancient Eastern Churches, including the Oriental Orthodox of Egypt, Syria, Armenia, Eritrea, Ethiopia. The Oriental Orthodox teach "The Lord Jesus Christ is God the Incarnate Word. He possesses the perfect Godhead and the perfect manhood. His fully divine nature is united with His fully human nature yet without mixing, blending or alteration" which has been misunderstood as monophysitism, a belief which the Oriental Orthodox Church strongly disagree with.
Historical background.
Relics of Nestorianism.
In 325, the first ecumenical council (First Council of Nicaea) determined that Jesus Christ was God, "consubstantial" with the Father, and rejected the Arian contention that Jesus was a created being. This was reaffirmed at the First Council of Constantinople (381) and the Council of Ephesus (431).
After the Council of Ephesus had condemned Nestorianism, there remained a conflict between Patriarchs John of Antioch and Cyril of Alexandria. Cyril claimed that John remained Nestorian in outlook, while John claimed that Cyril held to the Apollinarian heresy. The two settled their differences under the mediation of the Bishop of Beroea, Acacius, on April 12, 433. In the following year, Theodoret of Cyrrhus assented to this formula as well. He agreed to anathematize Nestorius as a heretic in 451, during the Council of Chalcedon, as the price to be paid for being restored to his see (after deposition at the Council of Ephesus of 449). This put a final end to Nestorianism within the Roman Empire
Eutychian controversy.
About two years after Cyril of Alexandria's death in 444, an aged monk from Constantinople named Eutyches began teaching a subtle variation on the traditional Christology in an attempt (as he described in a letter to Pope Leo I in 448) to stop a new outbreak of Nestorianism. He claimed to be a faithful follower of Cyril's teaching, which was declared orthodox in the Union of 433.
Cyril had taught that "There is only one "physis", since it is the Incarnation, of God the Word." Cyril had apparently understood the Greek word "physis" to mean approximately what the Latin word "persona" (person) means, while most Greek theologians would have interpreted that word to mean "natura" (nature). Thus, many understood Eutyches to be advocating Docetism, a sort of reversal of Arianism—where Arius had denied the consubstantial divinity of Jesus, Eutyches seemed to be denying his human nature. Cyril's orthodoxy was not called into question, since the Union of 433 had explicitly spoken of two "physeis" in this context. 
Leo I wrote that Eutyches' error seemed to be more from a lack of skill on the matters than from malice. Further, his side of the controversy tended not to enter into arguments with their opponents, which prevented the misunderstanding from being uncovered. Nonetheless, due to the high regard in which Eutyches was held (second only to the Patriarch of Constantinople in the East), his teaching spread rapidly throughout the East. 
In November 448, during a local synod in Constantinople, Eutyches was denounced as a heretic by the Bishop Eusebius of Dorylaeum. Eusebius demanded that Eutyches be removed from office. Patriarch Flavian of Constantinople preferred not to press the matter on account of Eutyches' great popularity. He finally relented and Eutyches was condemned as a heretic by the synod. However, the Emperor Theodosius II and the Patriarch of Alexandria, Dioscorus, rejected this decision ostensibly because Eutyches had repented and confessed his orthodoxy. Dioscorus then held his own synod which reinstated Eutyches. The competing claims between the Patriarchs of Constantinople and Alexandria led the Emperor to call a council which was held in Ephesus in 449. The emperor invited Pope Leo I to preside. He declined to attend on account of the invasion of Italy by Attila the Hun. However, he agreed to send four legates to represent him. Leo provided his legates, one of whom died en route, with a letter addressed to Flavian of Constantinople explaining Rome's position in the controversy. Leo's letter, now known as Leo's Tome, confessed that Christ had two natures, and was not of or from two natures. Although it could be reconciled with Cyril's Formula of Reunion, it was not compatible in its wording with Cyril's Twelve Anathemas. In particular, the third anathema reads: "If anyone divides in the one Christ the hypostases after the union, joining them only by a conjunction of dignity or authority or power, and not rather by a coming together in a union by nature, let him be anathema." This appeared to some to be incompatible with Leo's definition of two natures hypostatically joined. However, the Council would determine (with the exception of 13 Egyptian bishops) that this was an issue of wording and not of doctrine; a committee of bishops appointed to study the orthodoxy of the Tome using Cyril's letters (which included the twelve anathemas) as their criteria unanimously determined it to be orthodox, and the Council, with few exceptions, supported this.
"Latrocinium" of Ephesus.
On August 8, 449 the Second Council of Ephesus began its first session with Dioscorus presiding by command of the Emperor. Dioscorus began the council by banning all members of the November 447 synod which had deposed Eutyches. He then introduced Eutyches who publicly professed that while Christ had two natures before the incarnation, the two natures had merged to form a single nature after the incarnation. Of the 130 assembled bishops, 111 voted to rehabilitate Eutyches. Throughout these proceedings, Roman legate Hilary repeatedly called for the reading of Leo's Tome, but was ignored. Dioscorus then moved to depose Flavian and Eusebius of Dorylaeum on the grounds that they taught the Word had been made flesh and not just assumed flesh from the Virgin and that Christ had two natures. When Flavian and Hilary objected, Dioscorus called for a pro-monophysite mob to enter the church and assault Flavian as he clung to the altar. Flavian was mortally wounded. Dioscorus then placed Eusebius of Dorylaeum under arrest and demanded the assembled bishops approve his actions. Fearing the mob, they all did. The papal legates refused to attend the second session at which several more orthodox bishops were deposed, including Ibas of Edessa, Irenaeus of Tyre (a close personal friend of Nestorius), Domnus of Antioch, and Theodoret. Dioscorus then pressed his advantage by having Cyril of Alexandria's Twelve Anathemas posthumously declared orthodox with the intent of condemning any confession other than one nature in Christ. Roman Legate Hilary, who as pope dedicated an oratory in the Lateran Basilica in thanks for his life, managed to escape from Constantinople and brought news of the Council to Leo who immediately dubbed it a "synod of robbers"—Latrocinium—and refused to accept its pronouncements. The decisions of this council now threatened schism between the East and the West.
Convocation and session.
The situation continued to deteriorate, with Leo demanding the convocation of a new council and Emperor Theodosius II refusing to budge, all the while appointing bishops in agreement with Dioscorus. All this changed dramatically with the Emperor's death and the elevation of Marcian, an orthodox Christian, to the imperial throne. To resolve the simmering tensions, Marcian announced his intention to hold a new council. Leo had pressed for it to take place in Italy, but Emperor Marcian instead called for it to convene at Nicaea. Hunnish invasions forced it to move at the last moment to Chalcedon, where the council opened on October 8, 451. Marcian had the bishops deposed by Dioscorus returned to their dioceses and had the body of Flavian brought to the capital to be buried honorably.
The Emperor asked Leo to preside over the council, but Leo again chose to send legates in his place. This time, Bishops Paschasinus of Lilybaeum and Julian of Cos and two priests Boniface and Basil represented the western church at the council. The Council of Chalcedon condemned the work of the Robber Council and professed the doctrine of the Incarnation presented in Leo's Tome. Attendance at this council was very high, with about 370 bishops (or presbyters representing bishops) attending. Paschasinus refused to give Dioscorus (who had excommunicated Leo leading up to the council) a seat at the council. As a result, he was moved to the nave of the church. Paschasinus further ordered the reinstatement of Theodoret and that he be given a seat, but this move caused such an uproar among the council fathers, that Theodoret also sat in the nave, though he was given a vote in the proceedings, which began with a trial of Dioscorus.
Marcian wished to bring proceedings to a speedy end, and asked the council to make a pronouncement on the doctrine of the Incarnation before continuing the trial. The council fathers, however, felt that no new creed was necessary, and that the doctrine had been laid out clearly in Leo's Tome. They were also hesitant to write a new creed as the Council of Ephesus had forbidden the composition or use of any new creed. The second session of the council ended with shouts from the bishops, "It is Peter who says this through Leo. This is what we all of us believe. This is the faith of the Apostles. Leo and Cyril teach the same thing." However, during the reading of Leo's Tome, three passages were challenged as being potentially Nestorian, and their orthodoxy was defended by using the writings of Cyril. Nonetheless due to such concerns, the Council decided to adjourn and appoint a special committee to investigate the orthodoxy of Leo's Tome, judging it by the standard of Cyril's Twelve Chapters, as some of the bishops present raised concerns about their compatibility. This committee was headed by Anatolius, Patriarch of Constantinople, and was given five days to carefully study the matter; Cyril's Twelve Chapters were to be used as the orthodox standard. The committee unanimously decided in favor of the orthodoxy of Leo, determining that what he said was compatible with the teaching of Cyril. A number of other bishops also entered statements to the effect that they believed that Leo's Tome was not in contradiction with the teaching of Cyril as well.
The council continued with Dioscorus' trial, but he refused to appear before the assembly. As a result, he was condemned, but by an underwhelming amount (more than half the bishops present for the previous sessions did not attend his condemnation), and all of his decrees were declared null. Marcian responded by exiling Dioscorus. All of the bishops were then asked to sign their assent to the Tome, but a group of thirteen Egyptians refused, saying that they would assent to "the traditional faith". As a result, the Emperor's commissioners decided that a "credo" would indeed be necessary and presented a text to the fathers. No consensus was reached, and indeed the text has not survived to the present. Paschasinus threatened to return to Rome to reassemble the council in Italy. Marcian agreed, saying that if a clause were not added to the "credo" supporting Leo's doctrine , the bishops would have to relocate. The bishops relented and added a clause, saying that, according to the decision of Leo, in Christ there are two natures united, inconvertible, inseparable.
Confession of Chalcedon.
The Confession of Chalcedon provides a clear statement on the human and divine nature of Christ:
Canons.
The work of the council was completed by a series of 30 disciplinary canons the Ancient Epitomes of which are:
Canon 28 grants equal privileges ("") to Constantinople as of Rome because Constantinople is the New Rome as renewed by canon 36 of the Quinisext Council. The papal legates were not present for the vote on this canon, and protested it afterwards, and it was not ratified by Pope Leo in Rome.
According to some ancient Greek collections, canons 29 and 30 are attributed to the council: canon 29, which states that an unworthy bishop cannot be demoted but can be removed, is an extract from the minutes of the 19th session; canon 30, which grants the Egyptians time to consider their rejection of Leo's "Tome", is an extract from the minutes of the fourth session.
In all likelihood an official record of the proceedings was made either during the council itself or shortly afterwards. The assembled bishops informed the pope that a copy of all the "Acta" would be transmitted to him; in March, 453, Pope Leo commissioned Julian of Cos, then at Constantinople, to make a collection of all the Acts and translate them into Latin. Most of the documents, chiefly the minutes of the sessions, were written in Greek; others, e.g. the imperial letters, were issued in both languages; others, again, e.g. the papal letters, were written in Latin. Eventually nearly all of them were translated into both languages.
The status of the sees of Constantinople and Jerusalem.
The status of Jerusalem.
The metropolitan of Jerusalem was given independence from the metropolitan of Antioch and from any other higher-ranking bishop, given what is now known as autocephaly, in the council's seventh session whose "Decree on the Jurisdiction of Jerusalem and Antioch" contains: "the bishop of Jerusalem, or rather the most holy Church which is under him, shall have under his own power the three Palestines". This led to Jerusalem becoming a patriarchate, one of the five patriarchates known as the pentarchy, when the title of "patriarch" was created in 531 by Justinian.
The status of Constantinople.
In a canon of disputed validity, the Council of Chalcedon also elevated the See of Constantinople to a position "second in eminence and power to the Bishop of Rome".
The Council of Nicaea in 325 had noted the primacy of the See of Rome, followed by the Sees of Alexandria and Antioch. At the time, the See of Constantinople was yet of no ecclesiastical prominence but its proximity to the Imperial court, gave rise to its importance. The Council of Constantinople in 381 modified the situation somewhat by placing Constantinople second in honor, above Alexandria and Antioch, stating in Canon III, that "the bishop of Constantinople... shall have the prerogative of honor after the bishop of Rome; because Constantinople is New Rome". In the early 5th century, this status was challenged by the bishops of Alexandria, but the Council of Chalcedon confirmed in Canon XXVIII:
In making their case, the council fathers argued that tradition had accorded "honor" to the see of older Rome because it was the first imperial city. Accordingly, "moved by the same purposes" the fathers "apportioned equal prerogatives to the most holy see of new Rome" because "the city which is honored by the imperial power and senate and enjoying privileges equaling older imperial Rome should also be elevated to her level in ecclesiastical affairs and take second place after her". The framework for allocating ecclesiastical authority advocated by the council fathers mirrored the allocation of imperial authority in the later period of the Roman Empire. The Eastern position could be characterized as being political in nature, as opposed to a doctrinal view. In practice, all Christians East and West addressed the papacy as the See of Peter and Paul or the Apostolic See rather than the See of the Imperial Capital. Rome understands this to indicate that its precedence has always come from its direct lineage from the apostles Peter and Paul rather than its association with Imperial authority.
After the passage of the Canon 28, Rome filed a protest against the reduction of honor given to Antioch and Alexandria. However, fearing that withholding Rome's approval would be interpreted as a rejection of the entire council, in 453 the pope confirmed the council's canons with a protest against the 28th.
Consequences of the council.
The near-immediate result of the council was a major schism. The bishops that were uneasy with the language of Pope Leo's Tome repudiated the council, saying that the acceptance of two "physes" was tantamount to Nestorianism. Dioscorus, the Patriarch of Alexandria, advocated miaphysitism and had dominated the Council of Ephesus. Churches that rejected Chalcedon in favor of Ephesus broke off from the rest of the Church in a schism, the most significant among these being the Church of Alexandria, today known as the Coptic Orthodox Church of Alexandria.
Justinian I attempted to bring those monks who still rejected the decision of the Council of Chalcedon into communion with the greater church. The exact time of this event is unknown, but it is believed to have been between 535 and 548. St Abraham of Farshut was summoned to Constantinople and he chose to bring with him four monks. Upon arrival, Justinian summoned them and informed them that they would either accept the decision of the Council or lose their positions. Abraham refused to entertain the idea. Theodora tried to persuade Justinian to change his mind, seemingly to no avail. Abraham himself stated in a letter to his monks that he preferred to remain in exile rather than subscribe to a faith contrary to that of Athanasius. They were not alone, and the non-Chalcedon churches compose Oriental Orthodoxy, with the Church of Alexandria as their spiritual leader. Only in recent years has a degree of rapprochement between Chalcedonian Christians and the Oriental Orthodox been seen.
Liturgical Commemorations.
The Eastern Orthodox Church commemorates the "Holy Fathers of the 4th Ecumenical Council, who assembled in Chalcedon" on the Sunday on or after July 13;
 however, in some places ("e.g." Russia) on that date is rather a feast of the Fathers of the First Six Ecumenical Councils.
For the both of the above complete propers have been composed and are found in the Menaion.
For the former "The Office of the 630 Holy and God-bearing Fathers of the 4th ... Summoned against the Monophysites Eftyches and Dioskoros ..." was composed in the middle of the 14th century by Patriarch Philotheus I of Constantinople. This contains numerous hymns exposing the council's teaching, commemorating its leaders whom it praises and whose prayers it implores, and naming its opponents pejoratively. "e.g.", "Come let us clearly reject the errors of ... but praise in divine songs the fourth council of pious fathers."
For the latter the propers are titled "We Commemorate Six Holy Ecumenical Councils". This repeatedly damns those anathematized by the councils with such rhetoric as "Christ-smashing deception enslaved Nestorius" and "mindless Arius and ... is tormented in the fires of Gehenna ..." while the fathers of the councils are praised and the dogmas of the councils are expounded in the hymns therein.
Also, in some places, there is another commemoration of the council on May 22.

</doc>
<doc id="6963" url="http://en.wikipedia.org/wiki?curid=6963" title="Canadian football">
Canadian football

Canadian football is a form of gridiron football played in Canada in which two teams of 12 players each compete for territorial control of a field of play long and wide attempting to advance a pointed prolate spheroid ball into the opposing team's scoring area (end zone). In Canada, the term football may refer to Canadian football and American football collectively, or either sport specifically, depending on context. The two sports have shared origins and are closely related but have significant differences. In particular, Canadian football has 12 players on the field per team rather than 11; the field is roughly 10 yards wider, and 10 yards longer between end zones that are themselves 10 yards deeper; and a team has only three downs to gain 10 yards, which results in less offensive rushing than in the American game. In the Canadian game all players on the defending team, when a down begins, must be at least 1 yard from the line of scrimmage. (The American game has a similar "neutral zone" but it is only the width of the football.)
Rugby football in Canada originated in the early 1860s, and over time, the game known as Canadian football developed. Both the Canadian Football League (CFL), the sport's top professional league, and Football Canada, the governing body for amateur play, trace their roots to 1880 and the founding of the Canadian Rugby Football Union. Currently active teams such as the Toronto Argonauts and Hamilton Tiger-Cats have similar longevity. The CFL is the most popular and only major professional Canadian football league. Its championship game, the Grey Cup, is the country's single largest sporting event, attracting a broad television audience (in 2009, about 40% of Canada's population watched part of the game). Canadian football is also played at the high school, junior, collegiate, and semi-professional levels: the Canadian Junior Football League, formed May 8, 1974, and Quebec Junior Football League are leagues for players aged 18–22, many post-secondary institutions compete in Canadian Interuniversity Sport for the Vanier Cup, and senior leagues such as the Alberta Football League have grown in popularity in recent years. Great achievements in Canadian football are enshrined in the Canadian Football Hall of Fame.
Other organizations across Canada perform senior league Canadian football during the summer.
History.
The first documented football match was a practice game played on November 9, 1861, at University College, University of Toronto (approximately 400 yards west of Queen's Park). One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school. A football club was formed at the university soon afterward, although its rules of play at this stage are unclear.
The first written account of a game played was on October 15, 1862, on the Montreal Cricket Grounds. It was between the First Battalion Grenadier Guards and the Second Battalion Scots Fusilier Guards resulting in a win by the Grenadier Guards 3 goals, 2 rouges to nothing. In 1864, at Trinity College, Toronto, F. Barlow Cumberland, Frederick A. Bethune, and Christopher Gwynn, one of the founders of Milton, Massachusetts, devised rules based on rugby football. The game gradually gained a following, with the Hamilton Football Club formed on November 3, 1869, (the oldest football club in Canada). Montreal formed a team April 8, 1872, Toronto was formed on October 4, 1873, and the Ottawa FBC on September 20, 1876.
This rugby-football soon became popular at Montreal's McGill University. McGill challenged Harvard University to a game, in 1874 using a hybrid game of English rugby devised by the University of McGill.
The first attempt to establish a proper governing body and adopted the current set of Rugby rules was the Foot Ball Association of Canada, organized on March 24, 1873 followed by the Canadian Rugby Football Union (CRFU) founded June 12, 1880, which included teams from Ontario and Quebec. Later both the Ontario and Quebec Rugby Football Union (ORFU and QRFU) were formed (January 1883), and then the Interprovincial (1907) and Western Interprovincial Football Union (1936) (IRFU and WIFU). The CRFU reorganized into an umbrella organization forming the Canadian Rugby Union (CRU) in 1891. The original forerunners to the current Canadian Football League, was established in 1956 when the IRFU and WIFU formed an umbrella organization, The Canadian Football Council (CFC). And then in 1958 the CFC left The CRFU to become The CFL.
The Burnside rules closely resembling American Football that were incorporated in 1903 by The ORFU, was an effort to distinguish it from a more rugby-oriented game. The Burnside Rules had teams reduced to 12 men per side, introduced the Snap-Back system, required the offensive team to gain 10 yards on three downs, eliminated the Throw-In from the sidelines, allowed only six men on the line, stated that all Goals by Kicking were to be worth two points and the opposition was to line up 10 yards from the defenders on all Kicks. The Rules were an attempt to standardize the rules throughout the country. The CIRFU, QRFU and CRU refused to adopt the new Rules at first. Forward passes were not allowed in the Canadian game until 1929, and touchdowns, which had been five points, were increased to six points in 1956, in both cases several decades after the Americans had adopted the same changes. The primary differences between the Canadian and American games stem from rule changes that the American side of the border adopted but the Canadian side did not (originally, both sides had three downs, goal posts on the goal lines and unlimited forward motion, but the American side modified these rules and the Canadians did not). The Canadian field width was one rule that was "not" based on American rules, as the Canadian game played in wider fields and stadiums that were not as narrow as the American stadiums.
The Grey Cup was established in 1909 after being donated by Lord Earl Grey, The Governor General of Canada as the championship of teams under the CRU for the Rugby Football Championship of Canada. Initially an amateur competition, it eventually became dominated by professional teams in the 1940s and early 1950s. The Ontario Rugby Football Union, the last amateur organization to compete for the trophy, withdrew from competition in 1954. The move ushered in the modern era of Canadian professional football.
Canadian football has mostly been contained to Canada, with the United States being the only other country to have hosted a high-level Canadian football game. The CFL's controversial "South Division" as it would come to be officially known attempted to put CFL teams in the United States playing under Canadian rules between 1992 and 1995. The move was aborted after three years; the Baltimore Stallions were the most successful of the numerous Americans teams to play in the CFL, winning the 83rd Grey Cup. Continuing financial losses, a lack of proper Canadian football venues, a pervasive belief that the American teams were simply pawns to provide the struggling Canadian teams with expansion fee revenue, and the return of the NFL to Baltimore prompted the end of Canadian football on the American side of the border.
As of 2013, Newfoundland and Labrador is the only province that has neither organized Canadian football at the college, professional or amateur level, nor has hosted a CFL or college game. Prince Edward Island, the smallest of the provinces, has also never hosted a CFL game.
League play.
Canadian football is played at several levels in Canada; the top league is the professional nine-team Canadian Football League (CFL). The CFL regular season begins in June, and playoffs for the Grey Cup are completed by mid-November. In cities with outdoor stadiums such as Calgary, Edmonton, Winnipeg, Montreal, Hamilton, and Regina, low temperatures and icy field conditions can seriously affect the outcome of a game.
Amateur football is governed by Football Canada. At the university level, 26 teams play in four conferences under the auspices of Canadian Interuniversity Sport; the CIS champion is awarded the Vanier Cup. Junior football is played by many after high school before joining the university ranks. There are 20 junior teams in three divisions in the Canadian Junior Football League competing for the Canadian Bowl. The Quebec Junior Football League includes teams from Ontario and Quebec who battle for the Manson Cup.
Semi-professional leagues have grown in popularity in recent years, with the Alberta Football League becoming especially popular. The Northern Football Conference formed in Ontario in 1954 has also surged in popularity as College players that do not continue to or get drafted to a professional team but still want to continue playing football. The Ontario champion plays against the Alberta champion for the "National Championship". The Canadian Major Football League is the governing body for the semi-professional game.
Women's football is starting to gain attention in Canada. The first Canadian women's league to begin operations was the Maritime Women's Football League in 2004. The largest women's league is the Western Women's Canadian Football League.
The field.
The Canadian football field is long and wide with end zones deep, and goal lines apart. At each goal line is a set of goalposts, which consist of two "uprights" joined by an crossbar which is above the goal line. The goalposts may be H-shaped (both posts fixed in the ground) although in the higher-calibre competitions the tuning-fork design (supported by a single curved post behind the goal line, so that each post starts above the ground) is preferred. The sides of the field are marked by white sidelines, the goal line is marked in white, and white lines are drawn laterally across the field every from the goal line. These lateral lines are called "yard lines" and often marked with the distance in yards from and an arrow pointed toward the nearest goal line. In previous decades, arrows were not used and every yard line was usually marked with the distance to the goal line, including the goal line itself which was marked with a "0"—in most stadiums today, only every second yard line from the nearest goal (i.e. those with distances divisible by 10) are marked with numbers, with the goal line sometimes being marked with a "G" for goal line and the centre line usually being marked with a "C" for "Centre line". "Hash marks" are painted in white, parallel to the yardage lines, at intervals, from the sidelines. On fields that have a surrounding running track, such as Commonwealth Stadium, Molson Stadium, and many universities, the end zones are often cut off in the corners to accommodate the track. This was particularly common among U.S.-based teams during the CFL's American expansion, where few American stadiums were able to accommodate the much longer CFL field.
Until 1986, the end zones were deep, giving the field an overall length of , and a correspondingly larger cutoff could be required at the corners.
Play of the game.
Teams advance across the field through the execution of quick, distinct plays, which involve the possession of a brown, prolate spheroid ball with ends tapered to a point. The ball has two one-inch-wide white stripes.
Start of play.
Play begins at the start of each half with one team place-kicking the ball from its own 35-yard line. Both teams then attempt to catch the ball. The player who recovers the ball may run while holding the ball, or lateral throw the ball to a teammate.
Stoppage of play.
Play stops when the ball carrier's knee, elbow, or any other body part aside from the feet and hands, is forced to the ground (a "tackle"); when a forward pass is not caught on the fly (during a scrimmage); when a touchdown (see below) or a field goal is scored; when the ball leaves the playing area by any means (being carried, thrown, or fumbled out of bounds); or when the ball carrier is in a standing position but can no longer move forwards (called forward progress). If no score has been made, the next play starts from "scrimmage".
Scrimmage.
Before scrimmage, an official places the ball at the spot it was at the stop of clock, but no nearer than 24 yards from the sideline or 1 yard from the goal line. The line parallel to the goal line passing through the ball (line from sideline to sideline for the length of the ball) is referred to as the line of scrimmage. This line is similar to "no-man's land"; players must stay on their respective sides of this line until the play has begun again. For a scrimmage to be valid the team in possession of the football must have seven players, excluding the quarterback, within one yard of the line of scrimmage. The defending team must stay a yard or more back from the line of scrimmage.
On the field at the beginning of a play are two teams of 12 (unlike 11 in American football). The team in possession of the ball is the offence and the team defending is referred to as the defence. Play begins with a backwards pass through the legs (the snap) by a member of the offensive team, to the quarterback or punter. If the quarterback or punter receives the ball, he may then do any of the following:
Each play constitutes a "down". The offence must advance the ball at least ten yards towards the opponents' goal line within three downs or forfeit the ball to their opponents. Once ten yards have been gained the offence gains a new set of three downs (rather than the four downs given in American football). Downs do not accumulate. If the offensive team completes 10 yards on their first play, they lose the other two downs and are granted another set of three. If a team fails to gain ten yards in two downs they usually punt the ball on third down or try to kick a field goal (see below), depending on their position on the field. The team may, however use its third down in an attempt to advance the ball and gain a cumulative 10 yards.
Change in possession.
The ball changes possession in the following instances:
Rules of contact.
There are many rules to contact in this type of football. First, the only player on the field who may be legally tackled is the player currently in possession of the football (the ball carrier). Second, a receiver, that is to say, an offensive player sent down the field to receive a pass, may not be interfered with (have his motion impeded, be blocked, etc.) unless he is within one yard of the line of scrimmage (instead of in American football). Any player may block another player's passage, so long as he does not hold or trip the player he intends to block. The kicker may not be contacted after the kick but before his kicking leg returns to the ground (this rule is not enforced upon a player who has blocked a kick), and the quarterback, having already thrown the ball, may not be hit or tackled.
Infractions and penalties.
Infractions of the rules are punished with "penalties", typically a loss of yardage of 5, 10 or 15 yards against the penalized team. Minor violations such as "offside" (a player from either side encroaching into scrimmage zone before the play starts) are penalized five yards, more serious penalties (such as holding) are penalized 10 yards, and severe violations of the rules (such as face-masking) are typically penalized 15 yards. Depending on the penalty, the penalty yardage may be assessed from the original line of scrimmage, from where the violation occurred (for example, for a pass interference infraction), or from where the ball ended after the play. Penalties on the offence may, or may not, result in a loss of down; penalties on the defence may result in a first down being automatically awarded to the offence. For particularly severe conduct, the game official(s) may eject players (ejected players may be substituted for), or in exceptional cases, declare the game over and award victory to one side or the other. Penalties do not affect the yard line which the offence must reach to gain a first down (unless the penalty results in a first down being awarded); if a penalty against the defence results in the first down yardage being attained, then the offence is awarded a first down.
Penalties may occur before a play starts (such as offside), during the play (such as holding), or in a dead-ball situation (such as unsportsmanlike conduct).
Penalties never result in a score for the offence. For example, a point-of-foul infraction committed by the defence in their end zone is not ruled a touchdown, but instead advances the ball to the one-yard line with an automatic first down. For a distance penalty, if the yardage is greater than half the distance to the goal line, then the ball is advanced half the distance to the goal line, though only up to the one-yard line (unlike American football, in Canadian football no scrimmage may start inside either one-yard line). If the original penalty yardage would have resulted in a first down or moving the ball past the goal line, a first down is awarded.
In most cases, the non-penalized team will have the option of "declining" the penalty; in which case the results of the previous play stand as if the penalty had not been called. One notable exception to this rule is if the kicking team on a 3rd down punt play is penalized before the kick occurs: the receiving team may not decline the penalty and take over on downs. After the kick is made, change of possession occurs and subsequent penalties are assessed against either the spot where the ball is caught, or the runback.
Kicking.
Canadian football distinguishes four ways of kicking the ball:
On any kicking play, all onside players (the kicker, and teammates behind the kicker at the time of the kick) may recover and advance the ball. Players on the kicking team who are not onside may not approach within five yards of the ball until it has been touched by the receiving team, or by an onside teammate.
Scoring.
The methods of scoring are:
Resumption of play.
Resumption of play following a score is conducted under procedures which vary with the type of score.
Game timing.
The game consists of two 30-minute halves, each of which is divided into two 15-minute quarters. The clock counts down from 15:00 in each quarter. Timing rules change when there are three minutes remaining in a half.
A short break interval of 2 minutes occurs after the end of each quarter (a longer break of 15 minutes at halftime), and the two teams then change goals.
In the first 27 minutes of a half, the clock stops when:
The clock starts again when the referee determines the ball is ready for scrimmage, except for team time-outs (where the clock starts at the snap), after a time count foul (at the snap) and kickoffs (where the clock starts not at the kick but when the ball is first touched after the kick).
In the last three minutes of a half, the clock stops whenever the ball becomes dead. On kickoffs, the clock starts when the ball is first touched after the kick. On scrimmages, when it starts depends on what ended the previous play. The clock starts when the ball is ready for scrimmage except that it starts on the snap when on the previous play
During the last three minutes of a half, the penalty for failure to place the ball in play within the 20-second play clock, known as "time count" (this foul is known as "delay of game" in American football), is dramatically different from during the first 27 minutes. Instead of the penalty being 5 yards with the down repeated, the base penalty (except during convert attempts) becomes loss of down on first or second down, and 10 yards on third down with the down repeated. In addition, if the referee deems repeated time count violations on third down to be deliberate, he has the right to give possession to the defensive team.
The clock does not run during convert attempts in the last three minutes of a half. If the 15 minutes of a quarter expire while the ball is live, the quarter is extended until the ball becomes dead. If a quarter's time expires while the ball is dead, the quarter is extended for one more scrimmage. A quarter cannot end while a penalty is pending: after the penalty yardage is applied, the quarter is extended one scrimmage. Note that the non-penalized team has the option to "decline" any penalty it considers disadvantageous, so a losing team cannot indefinitely prolong a game by repeatedly committing infractions.
Overtime.
In the CFL, if the game is tied at the end of regulation play, then each team is given an equal number of chances to break the tie. A coin toss is held to determine which team will take possession first; the first team scrimmages the ball at the opponent's 35-yard line and advances through a series of downs until it scores or loses possession. If the team scores a touchdown, starting with the 2010 season, it is required to attempt a 2-point conversion.
The other team then scrimmages the ball at the same 35-yard line and has the same opportunity to score. After the teams have completed their possessions, if one team is ahead, then it is declared the winner; otherwise, the two teams each get another chance to score, scrimmaging from the other 35-yard line. After this second round, if there is still no winner, during the regular season the game ends as a tie. In a playoff or championship game, the teams continue to attempt to score from alternating 35-yard lines, until one team is leading after both have had an equal number of possessions.
In Canadian Interuniversity Sport football, for the Uteck Bowl, Mitchell Bowl, and Vanier Cup, the same overtime procedure is followed until there is a winner.
Players.
Offense.
The offensive positions found in Canadian football have, for the most part, evolved throughout the years, and are not officially defined in the rules. However, among offensive players, the rules recognize three different types of players:
Specific offensive positions include:
Defence.
The rules do not constrain how the defence may arrange itself (other than the requirement that they must remain one yard behind the line of scrimmage until the play starts).
Special teams.
"Special teams" generally refers to kicking plays, which typically involve a change in possession.

</doc>
<doc id="6966" url="http://en.wikipedia.org/wiki?curid=6966" title="Chinese calendar">
Chinese calendar

Chinese calendar may refer to any of the official and civil calendars used in China and some neighbouring countries in different periods of history; however, the phrase is generally synonymous with "Han calendar".
The official calendar in China today is the Gregorian calendar, which is a solar calendar. It is used for public and business affairs.
The civil calendar in much of China is the Han calendar, which is a lunisolar calendar. It is used for selecting the day of a wedding or funeral, for opening a venture, or a relocation. A similar calendar is used in Japan, Korea, and Vietnam for these purposes. Muslims living in Xinjiang, Ningxia and other parts of northern China use the Islamic calendar, which is a mean moon lunar calendar, as their civil calendar. The civil calendar for Tibet is the Tibetan calendar, which is a lunisolar calendar. The civil calendar for Miao is the Miao calendar, which is a solar calendar.
In China, some public holidays relate to the Gregorian calendar, such as Labor Day and National Day while others relate to the Chinese Calendar, such as Chinese New Year, Duanwu Festival, and the Mid-Autumn Festival. In specified provinces(Autonomous~) of China, some extra public holidays related to Islamic calendar or Tibetan calendar, such as Islamic New Year and the Major Festival in Ningxia and Xinjiang, Tibetan New Year and Summer Assembly in Tibet.
The Han calendar is a lunisolar calendar, which indicates both the moon phases and the solar terms. In Han calendar, a year usually begins on the second dark moon after the winter solstice but occasionally on the third dark moon after the winter solstice.
The year from January 31, 2014 to February 18, 2015 is a "Wǔnián" or "Mǎnián" (Year of the Horse).
Early Chinese calendars.
It is found on the oracle bones of the Shang Dynasty (late 2nd millennium BC), which seem to describe a lunisolar year of 12 months, with a possible intercalary 13th, or even 14th, added empirically to prevent calendar "drift". The Sexagenary cycle for recording days was already in use. Tradition holds that, in that era, the year began on the first new moon after the winter solstice.
Early Eastern Zhou texts, such as the "Spring and Autumn Annals", provide better understanding of the calendars used in the Zhou dynasty. One year usually had 12 months, which were alternately 29 and 30 days long (with an additional day added from time to time, to catch up with "drifts" between the calendar and the actual moon cycle), and intercalary months were added in an arbitrary fashion at the end of the year.
These arbitrary rules on day and month intercalation caused the calendars of each state to be slightly different, at times. Thus, texts like the Annals will often state whether the calendar they use (the calendar of Lu) is in phase with the "Royal calendar" (used by the Zhou kings).
Although tradition holds that in the Zhou, the year began on the new moon which preceded the winter solstice, the "Spring and Autumn Annals" seem to indicate that (in Lu at least) the Yin calendar (the calendar used in Shang dynasty, with years beginning on the first new moon after the winter solstice) was in use until the middle of the 7th century, and that the beginning of the year was shifted back one month around 650 BC.
By the beginning of the Warring States, progress in astronomy and mathematics allowed the creation of calculated calendars (where intercalary months and days are set by a rule, and not arbitrarily). The quarter remainder calendar (), which began about 484 BC, was the first calculated Chinese calendar, so named because it used a solar year of 365¼ days (the same as the 1st-century BC Julian Calendar of Rome), along with a 19-year (235-month) Rule Cycle (), known in the West as the Metonic cycle. The year began on the new moon preceding the winter solstice, and intercalary months were inserted at the end of the year.
In 256 BC, as the last Zhou king ceded his territory to Qin, a new calendar (the Qin calendar) began to be used. It followed the same principles as the Sifen calendar, except that the year began at Shíyuè 1(, the closest new moon of the winter beginning). The Qin calendar was used during the Qin dynasty, and in the beginning of the Western Han dynasty. According to the Han Records()21a, 973, for the moment of unification the Middle kingdoms had 6 different calendars: those of the mythological progenitors Yellow Emperor () and Zhuanxu(); of the dynasties Xia (), Yin (), and Zhou (), and Lu state ()of the Zhou Dynasty . Of those, the second was taken to substitute the rest. The Han imperial library is said to contain 82 volumes of descriptions of all those systems ("Han Shu" 30, 1765-6), now mostly lost.
The two oldest printed Chinese calendars are dated 877 and 882; they were found at the Buddhist pilgrimage site of Dunhuang; Patricia Ebrey writes that it is no surprise that some of the earliest printed items were calendars, since the Chinese found it necessary to calculate and mark which days were auspicious and which were not.
Han and Tibetan Calendar.
Emperor Wu of the Han dynasty introduced reforms in the halfway of his administration. His Grand Inception Calendar () introduced 24 solar terms which decides the month names. The solar year was defined as days, and divided into 24 solar terms. Each couples of solar terms are associated into 12 climate terms. The lunar month was defined as days and named according to the closest climate term. The mid-climate in the month decides the month name, and a month without mid-climate is an intercalary.
Ever since then, there are over 100 official calendars in Chinese which are consecutive and follow the structure of "Tàichū calendar" both. There're several innovation in calendar calculation in the history of over 2100 years, such as:
In the "Dàmíng Calendar" released in "Tiānjiān 9"(, 510) of "Liáng Dynasty", "Zhǔ Chōngzhī" introduced the equation of equinoxes.
Actual syzygy method was adopted to decide the month from the "Wùyín Yuán Calendar", which was released in "Wǔdé 2"(, 619) of "Táng Dynasty".
The real measured data was used in calendar calculation from "Shòushí Calendar", which was released in "Zhìyuán 18"(,1281) of "Yuán Dynasty".
the ecliptic longitude is introduced and adopted to determine the solar term from the "Shíxiàn calendar" which was released in "Shùnzhì 2"(1645) of "Qīng Dynasty".
Japan adopt her own tradition calendar which follows the algorithm of "Táng dynasty" from Edo period.
Korea and Vietnam adopts the Han calendar except that Vietnam substitutes the cat for the Rabbit in the Chinese zodiac.
In the Tubo Dynasty, princess Wencheng and Jincheng brought Han calendar to Tibet. And Tibetan built Tibet calendar with the Tibetan calendar with the characters of phenological, Kalachakra, and Han calendar. The Tibetan calendar was finalized before Yuan Dynasty.
the moon phases, and tidal phenomena is much easier to reckon in the Han calendar, such as spring and neap tides, fall on approximately the same day in each lunar month, and the times of high and low water and the tidal streams experienced in a certain location on a certain day of the lunar month are likely to be similar to those for the same place and lunar day in any month. For many years, therefore, mariners in East and South-East Asia have related their tidal observations to the Chinese calendar, so as to be able to provide quick, rule-of-thumb approximations of tides and tidal conditions from memory, based on the day of the Lunar month, without needing to refer to tide tables. Certain inshore passages on the China coast, for example, where there are strong tidal streams associated with spring tides, were regarded by mariners to be passable on certain days of the lunar month, and impassable on others.
Tibetan calendar is used to forecast the climate and earthquake in Tibetan. 
After 1912, the Gregorian Calendar became official calendar, and no authority gave a name for Han calendar. People called Han calendar as the Former Calendar(), the Traditional Calendar(), the Yin Calendar(), etc. In the public media, the Han calendar is usually referred as the Agricultural Calendar(). In addition, some regions also call it as "Xià Calendar"(), for it originated from Xià Dynasty.
In January 1, 1912, nascent Republic of China adopted the Gregorian calendar for official business as Japan did during Meiji Restoration. But, the Gregorian calendar wasn't completely official until 1929 for incessant fighting between warlords. The People's Republic of China adopt the Gregorian calendar include year number since 1949.
The Han calendar remains culturally essential today. For example, most of the traditional festivals, such as Chinese New Year and the Mid-Autumn Festival, occur on new moons or full moons. The Han calendar, as an element of traditional culture, has much cultural and nationalistic sentiment invested in it. The Han calendar is still used in the more traditional Chinese households around the world to pick 'auspicious dates' for important events such as weddings, funerals, and business deals. A special calendar is used for this purpose, called the Imperial Calendar(), which contains auspicious activities, times, and directions for each day. The calendar follows the Gregorian dates but has the corresponding Chinese dates. Every date would have a comprehensive listing of astrological measurements and fortune elements.
In China, Korea, Vietnam, there're many traditional festival is base on the Han calendar, such as New Year's Day, Mid-Autumn Festival, etc.; and there're some traditional festival basing on the solar terms, such as Winter Solstice Festival.
Structure of Han and Tibetan calendar.
Solar Day.
In Han calendar, a day runs from midnight to midnight, as in the Gregorian calendar. Currently, midnight is based on Chinese Standard Time, the mean solar time at longitude 120° east (equivalent to UTC+08).
In Tibetan calendar, a day runs from dawn to dawn.
Subdivisions of a day.
In modern Chinese, the day is divided according to the Western hour-minute-second system, but the older standards are still used in some instances.
In ancient, the sundial is used to show time. At first, there are 69 scales on the sundial, which are included in 68% circumference. The circumference was divided into 12 parts later, which are named with 12 Earthly Branches. The time which the shadow of the stile sweep over a part is a "shí". So, a "shí" is 2 hours.
After Song Dynasty, each "shí" is divided into two hour. The halfway point is "mid-shí"(), for example, midday is "mid-wǔshí"(). The first hour is "ante mid-shí"(); and the second hour is "post mid-shí"().
For the purposes of calculating the calendar, a day starts at midnight or "mid-zǐshí"(), but people tend to regard a day as starting at from dawn (during "5 gēng"). For example, In a familiar couplet for New Year's Eve, it's written as "a night connects double ages, "5 gēng" divides two years"()
In the other hand, the water clock was introduced to show time. There're 100 scales() on the rule to measure the water level. So, the time between 2 scales is a centiday() or 14.4 minutes. A centiday is 60 mils(). Currently, the word "kè" is used to denote a quarter of an hour.
The sundial and water clock shows "shí" and "kè" together. So, people always described the time with "shí" and "kè" together.
Before Song Dynasty, the format is "shí + kè", for example, in "Section 2, Calendar Records, Yuán History"()
According to the estimation of "Shòushí" calendar, the sun eclipsed at the "8 kè, Yínshí"(04:48). According to the estimation of "Dàmíng" Calendar, the sun eclipsed at "0 kè, Mǎoshí"(05:00). ()
After Song Dynasty, the format is "shí + am/pm + kè", for example, in "Section 2, Calendar Records, Yuán History"()
According to the estimation of "Shòushí" calendar, the sun eclipsed at the "1 kè am, Sìshí"(09:07). According to the estimation of "Dàmíng" Calendar, the sun eclipsed at the "0 kè am, Sìshí"(09:00). ()
The "shí - kè" system is exact timing system in ancient. 
In pre-qin, there're 10 key points within a day, which were name with Heavenly Stems from the sunset. The key points were signaled with drum. The 5 key points in the night were signaled by the night watchman with gong later. And, the key points were named with ordinal number correspondingly, such as, "1 gēng"(), "2 gēng"(), "3 gēng"(), "4 gēng"(), "5 gēng"(). The idiom "Sāngēng bànyè".() means that in the dead night of "3 gēng".
There're 60 points within a day, which were signaled with bell tone. The point is called as "diǎn"(). The time between two "diǎns" is 24 minutes, which is called as "diǎn" too. A "diǎn" is 100 "fēn"(mils). Currently, the word "diǎn" is used to denote o'clock.
The authorities release time signal with bell and drum till the end of "Qīng" dynasty. So, people always described the time in night with "gēng" and "diǎn" together, for example, in "Section 4, Military Records, Yuán History" ()
The curfew rule: after "1 gēng 3 diǎn"(20:24) when the bell tone stopped, walking is forbidden; after "5 gēng 3 diǎn" (06:00) when the bell tone rang, walking is allowed. ()
The "gēng - diǎn" system is rough timing system in ancient.
In Sòng dynasty, Sū Sòng built a water powered armillary sphere and celestial globe tower. There's a composite timing device in the tower. The device releases each "shí and kè" signal at the top floor, and shows the "shí - kè" at the second and third floor. And, the device releases each "gēng and diǎn" signal at the fourth floor, and shows the "gēng - diǎn" at the fifth floor.
Week.
Days are grouped within several kinds of weeks.
The days are grouped within a 7-days week, which is called a Luminary week. The name of the weekdays are Sun-day (), Moon-day (), Mars-day(), Mercury-day(), Jupiter-day (), Venus-day (), and Saturn-day ().
In modern China, the names are identified by ordinal numbers, such as: First-day(), Second-day(), Third-day(), Fourth-day(), Fifth-day(), Sixth-day(). The exception is Sunday, which is known as Sunday().
31 January 2013 is "Xīngqīsì" (Thursday).
Each 4 weeks are grouped within a 28-days week. the week days of a 28-days week are marked with Twenty-eight mansions(). For example, 31 January 2014 is "Níu" ().
The days are grouped within a 10-days week, and is called Heavenly stems. The names of the weekdays are "Jiǎrì", "Yǐrì", "Bǐngrì", "Dīngrì", "Wùrì", "Jǐrì", "Gēngrì", "Xīnrì", "Rénrì", and "Guìrì".
31 January 2013 is "Dīngrì".
Some tradition holiday is established according to Heavenly Stems week. Such as the Vernal/Autumn Sacrifice() is the fifth "Wùrì" after the Vernal/Autumn Commence.
The days are grouped within a 12-days week, which are called Earthly Branches. The names of the weekdays are "Zǐrì", "Chǒurì", "Yínrì", "Mǎorì", "Chénrì", "Sìrì", "Wǔrì", "Wèirì", "Shēnrì", "Yǒurì", "Xūrì", and "Hàirì".
31 January 2013 is "Yǒurì".
Some traditional holidays are established according to Earthly Branches week at first. Such as the "Shàngsì" Festival is the first "Sìrì" in "Sānyuè", and the "Duānwǔ" Festival is the first "Wǔrì" in "Wǔyuè" at first. These festivals are moved to the fixed calendar later. The "Shàngsì" Festival is fixed to "Sānyuè" 3rd, and the "Duānwǔ" Festival is fixed to "Wǔyuè" 5th.
The Heavenly stems and Earthly Branches run together and, when combined, make a 60-day week which is called stem-branches week.
31 January 2013 is "Dīngyǒurì".
The earliest evidence of stem-branches week was found on oracle bones dated c. 1350 BC in the Shang Dynasty. The stem-branches week continues to this day, and can still be found on Chinese calendars today.
Although the stem-branches week cannot decide the actual date alone in historical events, it can locate the accurate date along with context and other statement about time, and the difference between versions of the calendar may be neglected. For this reason, the stem-branches week is always used to mark date in the annals. Such as:
"Chronicle of Pope Rén, Sòng History" ()
In the "Bǐngyínrì" of vernal "Zhēngyuè" in the first year of "Tiānshèng", which is the first day of the month, changed the era name; ...()
In the "Wùxūrì" of "Èryuè", accepted the rule of "Gusiluo" annual tribute; in "Dīngsìrì", established the portrait of "Grand Chris" and "Grand Pope" at "Hóngqìng Palace" of the "Southern Capital" ("Suiyang"); lunched the "Monopolizing-Tea-Discount Law" at the 13 tea plantations of Huáinán ()
In the "Jiǎxūrì" of "Sānyuè", established the portrait of "Pope Zhēn" at "Yìngtiān Temple" of the "Western Capital" ("Loyang"); ...; in the "Xīnmǎorì", the "Imperial Astronomer" present the "Chóngtiān Calendar" for the approval to issue; ...()
Lunar phase and lunar month.
The lunar phase refers to the shape of the illuminated (sunlit) portion of the Moon as seen by an observer on Earth. The lunar phases corresponds to the celestial longitude difference of the moon and sun. The principal lunar phases are new moon (0° celestial longitude difference), first quarter moon (90° celestial longitude difference), full moon (180° celestial longitude difference) and last quarter moon (270° celestial longitude difference).
The lunar month is the time from a day between two identical syzygies or principal phase (new moons or full moons).
In Han calendar, a lunar month corresponds to a variation cycle (0°~360°) of the celestial longitude difference. So, the month always starts on the day of with a new astronomical moon. 
 Synodic month (29.48 days) 2013-01-12 03:43:36 ~ 2013-02-10 15:20:06
 + New moon day 2013-01-12 00:00:00 ~ 2013-01-13 00:00:00
 - Next new moon day 2013-02-10 00:00:00 ~ 2013-02-11 00:00:00
 The month (29 days) 2013-01-12 00:00:00 ~ 2013-02-10 00:00:00
 Synodic month (29.52 days) 2013-02-10 15:20:06 ~ 2013-03-12 03:51:00
 + New moon day 2013-02-10 00:00:00 ~ 2013-02-11 00:00:00
 - Next new moon day 2013-03-12 00:00:00 ~ 2013-03-13 00:00:00
 The month (30 days) 2013-02-10 00:00:00 ~ 2013-03-12 00:00:00
In Tibetan calendar, a lunar month corresponds to a variation cycle (-180°~180°) of the celestial longitude difference. The variation cycle is divided to 30 sections, and each section corresponds to a lunar day.
In Han calendar, the dates in the month are arranged in an orderly row from 1 to 29 or 30, and each date are called with two characters, such as:
In the late "Qīng" Dynasty, selected "yùnmù" (, the representative character of the rhymes) are on behalf of the two date characters for telegram use. It was applied into the date in the Gregorian calendar later. For example: "Wénxī" Fire () is a conflagration on the eve of 1938-11-12 ("Evening of 1938-11-12, Chángshā was going to be occupied by the enemy, municipality burnt and fielded the city"), "Wén" () is on behalf of "12th" and "xī"() is on behalf of "evening".
In Tibetan calendar, the dates in the month are arranged according to the lunar days. If there's a whole lunar day within a solar day, the date is arranged according the lunar day which ends in the solar day first; if there's a whole solar day within a lunar day, the date is arranged according to the lunar day; and if the solar day intersects with the lunar day, the date is arranged according to lunar day which ends in the solar day. For example: 
In Tibetan calendar, the month length is decided by the number of the skipped and lapped dates. If there's no skipped and lapped date, it's a lucky month. If there's a skipped date, it's a month with 29 days. If there's a skipped and lapped date, it's a month with 30 days. If there're 2 skipped dates and a lapped date, it's a month with 29 days. If there're 2 skipped and lapped dates, it's a month with 30 days.
The dating method in Han and Tibetan calendar is different, so the date is not the same all the way. But, the bias is within 1.
Solar year and solar term.
A solar year, for general purposes, is the length of time that the Sun takes to return to the same position in the cycle of seasons, as seen from Earth; for example, the time from vernal equinox to vernal equinox, or from winter solstice to winter solstice.
In Han calendar, a solar year is the time from a winter solstice to the next. And a solar year is divided into 24 solar terms which correspond to 15° along the ecliptic. A couple of solar terms are associated into a climate term. The start point of the first term in the couple is called as pre-climate (), and the start point of the last term in the couple is called as mid-climate (). The climate terms are marked with the 12 earthly branches. The time between two mid-climates correspond quite closely to the zodiac. The table right shows the relationship between solar terms, ecliptic positions and the zodiacs.
In the late Spring and Autumn Period (722–481 BC), the former "Sìfēn" calendar was established, and set the tropical year at 365.25 days, the same length as the Julian calendar which was introduced in 46 BC. The Taichu calendar of 104 BC under Emperor Wu of Han rendered the solar year at roughly the same ( formula_1).
Many other calendars were established between then and the Yuan Dynasty (1271–1368), including those established by Li Chunfeng (602–670) and Yi Xing (683–727). In 1281, the Yuan astronomer Guo Shoujing (1233–1316) fixed the calendar at 365.2425 days, the same as the Gregorian calendar established in 1582; this calendar, the Shoushi calendar, would be used in China for the next 363 years. Guo Shoujing established the new calendar with the aid of his own achievements in spherical trigonometry, which he derived largely from the work of Shen Kuo (1031–1095) who established trigonometry in China.
In Tibetan calendar, a solar year is divided into 12 solar months (climate terms) equally. A solar month is about 30.4 days
An astronomical year is approximately 365¼ days, a period between 12 and 13 lunar months. So, to keep the pace with the astronomical year in a long term, common years (a year with 12 months) and leap years (a year with 13 months) are interleaved. Generally, in each 19 years, there're 7 leap years and 12 common years.
In Han and Tibetan calendar, the month name corresponds to the climate term. The closest climate to the moon decides the month name, and the mid-climate in the month is regarded as the key point to choose the closest climate. Generally, the month with the Vernal Showers is "Zhēngyuè" or Dangpo (the 1st month), the month with the Vernal Equinox is "Èryuè" or Gnyispa ( the 2nd month), the month with the Corn Rain is "Sānyuè" or Gsumpa (the 3rd month), the month with the Corn Forms is "Sìyuè" or Bzhipa ( the 4th month), the month with the Summer Solstice is "Wǔyuè" or Lngapa (the 5th month), the month with the Moderate Heat is "Lìuyuè" or Drugpa ( the 6th month), the month with the End of Heat is "Qīyuè" or Ddunpa (the 7th month), the month with the Autumnal Equinox is "Bāyuè" or Brgyadpa (the 8th month), the month with the First Frost term is "Jǐuyuè" or Dgupa ( the 9th month), the month with the Light Snow is "Shíyuè" Bcupa (the 10th month), the month with the Winter Solstice is "Shíyīyuè" or Bcugcigpa (the 11th month), the month with the Great Cold term is "Làyuè" or Bcugnyispa (the 12th month), and the month without a mid-climate is "Rùnyuè" (an intercalary month). For example, the table below show the information of 2014. 
 Month Date mid-Climate Month name (Ordinal)
 A 2014-01-31 ~ 2014-02-28 VC: 2014-02-19 "Zhēngyuè"(1st)
 B 2014-03-01 ~ 2014-03-30 VE: 2014-03-21 " Èryuè"(2nd)
 C 2014-03-31 ~ 2014-04-28 CR: 2014-04-20 " Sānyuè"(3rd)
 D 2014-04-29 ~ 2014-05-28 CF: 2014-05-21 " Sìyuè"(4th)
 E 2014-05-29 ~ 2014-06-26 SS: 2014-06-21 " Wǔyuè"(5th)
 F 2014-06-27 ~ 2014-07-26 GH: 2014-07-23 " Lìuyuè"(6th)
 G 2014-07-27 ~ 2014-08-24 EH: 2014-08-23 " Qīyuè"(7th)
 H 2014-08-25 ~ 2014-09-23 AE: 2014-09-23 " Bāyuè"(8th)
 I 2014-09-24 ~ 2014-10-23 FD: 2014-10-23 " Jǐuyuè"(9th)
 J 2014-10-24 ~ 2014-11-21 N/A " Rùnyuè"(leap over)
 K 2014-11-22 ~ 2014-12-21 LS: 2014-11-22 " Shíyuè"(10th)
 L 2014-12-22 ~ 2015-01-19 WS: 2014-12-22 "Shíyīyuè"(11th)
 M 2015-01-20 ~ 2015-02-18 GC: 2015-01-20 " Làyuè"(12th)
In Han calendar, the solar term are calculated with the true sun position from "Shùnzhì 2" (, 1645), and the length of the solar terms are 14.7-15.7 days against 15.2 days. So, there's a possibility of 0.3% that two mid-climates enters the same month. To avoid the skipped month, the month name is arranged in orderly row from then. When there're 12 month between two winter solstice months, the first month without a mid-climate is an intercalary. The month names except the intercalary are "Shíyīyuè" or Bcugcigpa (the 11th month), "Làyuè" or Bcugnyispa (the 12th month), "Zhēngyuè" or Dangpo (the 1st month), "Èryuè" or Gnyispa ( the 2nd month), "Sānyuè" or Gsumpa (the 3rd month), "Sìyuè" or Bzhipa ( the 4th month), "Wǔyuè" or Lngapa (the 5th month), "Lìuyuè" or Drugpa ( the 6th month), "Qīyuè" or Ddunpa (the 7th month), "Bāyuè" or Brgyadpa (the 8th month), "Jǐuyuè" or Dgupa ( the 9th month), "Shíyuè" Bcupa (the 10th month). For example, the table below shows the information of 2032-2035.
 Month Date mid-Climate Month name
 A 2032/12/03~12/31 WS: 2032/12/21 0 "Shíyīyuè"(11th)
 B 2033/01/01~01/30 GC: 2033/01/20 1 " Làyuè"(12th)
 C 2033/01/31~02/28 RW: 2033/02/18 2 "Zhēngyuè"( 1st)
 D 2033/03/01~03/30 VE: 2033/03/20 3 " Èryuè"( 2nd)
 E 2033/03/31~04/28 GR: 2033/04/20 4 " Sānyuè"( 3rd)
 F 2033/04/29~05/27 GF: 2033/05/21 5 " Sìyuè"( 4th)
 G 2033/05/28~06/26 SS: 2033/06/21 6 " Wǔyuè"( 5th)
 H 2033/06/27~07/25 GH: 2033/07/22 7 " Lìuyuè"( 6th)
 I 2033/07/26~08/24 LH: 2033/08/23 8 " Qīyuè"( 7th)
 J 2033/08/25~09/22 N/A 9 " Bāyuè"( 8th)
 K 2033/09/23~10/22 AE: 2033/09/23 10 " Jǐuyuè"( 9th)
 L 2033/10/23~11/21 FD: 2033/10/23 11 " Shíyuè"(10th)
 M 2033/11/22~12/21 LS: 11/22 / WS: 12/21 0 "Shíyīyuè"(11th)
 N 2033/12/22~01/19 N/A 1 " Rùnyuè"(11th, Intercalary)
 O 2033/01/20~02/18 GC: 01/20 / RW: 02/18 2 " Làyuè"(12th)
 P 2034/02/19~03/19 N/A 3 "Zhēngyuè"( 1st)
 Q 2034/03/20~04/18 VE: 2034/03/20 4 " Èryuè"( 2nd)
 R 2034/04/19~05/17 GR: 2034/04/20 5 " Sānyuè"( 3rd)
 S 2034/05/18~06/15 GF: 2034/05/21 6 " Sìyuè"( 4th)
 T 2034/06/16~07/15 SS: 2034/06/21 7 " Wǔyuè"( 5th)
 U 2034/07/16~08/13 GH: 2034/07/23 8 " Lìuyuè"( 6th)
 V 2034/08/14~09/12 LH: 2034/08/23 9 " Qīyuè"( 7th)
 W 2034/09/13~10/11 AE: 2034/09/23 10 " Bāyuè"( 8th)
 X 2034/10/12~11/10 FD: 2034/10/23 11 " Jǐuyuè"( 9th)
 Y 2034/11/10~12/10 LS: 2034/11/22 12 " Shíyuè"(10th)
 Z 2034/12/11~01/08 WS: 2034/12/22 0 "Shíyīyuè"(11th)
For Han calendar, Ping-Tse Kao (; 1888-1970, one of the founders of Purple Mountain Observatory) mentioned "the calendric intercalary rule". In his "the calendric intercalary rule", the month name and intercalary are decided before rounding the month start to day. For example: 
 Synodic Month mid-Climate Month name Civil Month
 A 2032-12-03 04:52:26 ~ 01-01 18:16:36 WS:12-21 15:55:29 0 "Shíyīyuè" 2032-12-03 ~ 12-31 
 B 2033-01-01 18:16:36 ~ 01-31 05:59:26 GC:01-20 02:32:20 1 " Làyuè" 2033-01-01 ~ 01-30 
 C 2033-01-31 05:59:26 ~ 03-01 16:23:05 VS:02-18 16:33:22 2 "Zhēngyuè" 2033-01-31 ~ 02-28 
 D 2033-03-01 16:23:05 ~ 03-31 01:51:12 VE:03-20 15:22:17 3 " Èryuè" 2033-03-01 ~ 03-30 
 E 2033-03-31 01:51:12 ~ 04-29 10:45:45 CR:04-20 02:12:40 4 " Sānyuè" 2033-03-31 ~ 04-28 
 F 2033-04-29 10:45:45 ~ 05-28 19:36:06 CF:05-21 01:10:30 5 " Sìyuè" 2033-04-29 ~ 05-27 
 G 2033-05-28 19:36:06 ~ 06-27 05:06:36 SS:06-21 09:00:40 6 " Wǔyuè" 2033-05-28 ~ 06-26 
 H 2033-06-27 05:06:36 ~ 07-26 16:12:07 GH:07-22 19:52:21 7 " Lìuyuè" 2033-06-27 ~ 07-25 
 I 2033-07-26 16:12:07 ~ 08-25 05:39:21 EH:08-23 03:01:22 8 " Qīyuè" 2033-07-26 ~ 08-24 
 J 2033-08-25 05:39:21 ~ 09-23 21:39:19 AE:09-23 00:51:12 9 " Bāyuè" 2033-08-25 ~ 09-22 
 K 2033-09-23 21:39:19 ~ 10-23 15:27:58 FF:10-23 10:27:08 10 " Jǐuyuè" 2033-09-23 ~ 10-22 
 L 2033-10-23 15:27:58 ~ 11-22 09:38:40 LS:11-22 08:15:42 11 " Shíyuè" 2033-10-23 ~ 11-21 
 M 2033-11-22 09:38:40 ~ 12-22 02:46:01 WS:12-21 21:45:32 0 "Shíyīyuè" 2033-11-22 ~ 12-21 
 N 2033-12-22 02:46:01 ~ 01-20 18:01:05 GC:01-20 08:26:49 1 " Làyuè" 2033-12-22 ~ 01-19 
 O 2034-01-20 18:01:05 ~ 02-19 07:09:47 VS:02-18 22:29:43 2 "Zhēngyuè" 2034-01-20 ~ 02-18 
 P 2034-02-19 07:09:47 ~ 03-20 18:14:06 3 " Rùnyuè" 2034-02-19 ~ 03-19 
 Q 2034-03-20 18:14:06 ~ 04-19 03:25:25 VE:03-20 21:17:01 4 " Èryuè" 2034-03-20 ~ 04-18 
 R 2034-04-19 03:25:25 ~ 05-18 11:12:07 CR:04-20 08:03:14 5 " Sānyuè" 2034-04-19 ~ 05-17 
 S 2034-05-18 11:12:07 ~ 06-16 18:25:28 CF:05-21 06:56:24 6 " Sìyuè" 2034-05-18 ~ 06-15 
 T 2034-06-16 18:25:28 ~ 07-16 02:14:47 SS:06-21 14:43:42 7 " Wǔyuè" 2034-06-16 ~ 07-15 
 U 2034-07-16 02:14:47 ~ 08-14 11:52:35 GH:07-23 01:35:51 8 " Lìuyuè" 2034-07-16 ~ 08-13 
 V 2034-08-14 11:52:35 ~ 09-13 00:13:20 EH:08-23 08:47:16 9 " Qīyuè" 2034-08-14 ~ 09-12
 W 2034-09-13 00:13:20 ~ 10-12 15:32:10 AE:09-23 06:39:04 10 " Bāyuè" 2034-09-13 ~ 10-11 
 X 2034-10-12 15:32:10 ~ 11-11 09:15:45 FF:10-23 16:15:57 11 " Jǐuyuè" 2034-10-12 ~ 11-10 
 Y 2034-11-11 09:15:45 ~ 12-11 04:13:56 LS:11-22 14:04:28 12 " Shíyuè" 2034-11-11 ~ 12-10 
 Z 2034-12-11 04:13:56 ~ 01-09 23:02:37 SS:12-22 03:33:30 0 "Shíyīyuè" 2034-12-11 ~ 01-08 
"The calendric intercalary rule" reduced the possibility that two mid-climate enters the same month to 0.06%. And, the intercalary is the same using different standard time.
In Han calendar, the climate terms are unequal sections; but in Tibetan calendar, the climate terms are equal sections. So, the month is not the same all the way. But, the bias is within 1.
To sum up the month and date bias, the date in Han and Tibetan calendar is the same, a bias of a day, a bias of a month, or a bias of a month and a day. Such as:
 "Feb 28, 1987" is "Dangpo 1st, Fire rabbit year" or " Èryuè 1st, Dīngmǎonián" with a bias of a month 
 "Feb 18, 1988" is "Dangpo 1st, Earth dragon year" or "Zhēngyuè 2nd, Wùchénnián" with a bias of a day 
 "Feb 27, 1990" is "Dangpo 1st, Iron horse year" or " Èryuè 2nd, Gēngwǔnián" with a bias of a month and a day 
 "Feb 15, 1991" is "Dangpo 1st, Iron goat year" or "Zhēngyuè 1st, Xīngwèinián" without bias
Year.
A year starts at "Zhēngyuè" or "Dangpo" 1st, and always ends at the last day of "Làyuè" or "Bcugnyispa" (if there is an intercalary month after "Làyuè" or "Bcugnyispa", the year will end on the last day of the intercalary month).
There are 12 or 13 months in a year. If there are 12 months in year, there are 353, 354 or 355 days in this year. If there are 13 months in a year, there are 383, 384 or 385 days in this year. For example, the current year starts at 2012-1-23 and ends at 2013-02-09. There are 13 months or 384 days.
The year with 13 month is a leap year. The intercalary month of the leap year between 1900 and 2108 in Han calendar is list as below:
A representative sequence of common and leap years is lcclcclcclclcclcclc, which is the classic nineteen-year Metonic cycle. Han calendar follows the rule in general.
In China, the age recognition for official use is based on the Gregorian calendar. But, for traditional use, it is based on Han calendar. From birthday to the end of the year, it's one year old. And, add one year old after each New Year Eve. Such as, if one's birthday is "Làyuè" 29th 2013, he is 2 years old at "Zhēngyuè" 1st 2014.
The years are named with the era name (which is a name of several years) and ordinal number generally. But, the first year of each era is called as "Yuánnián" ().
For the eras ante "Emperor Wǔ of Hàn Dynasty", the regnal names are regard as the era names. such as "Yǐngōng 1"(, The first year of "Duke Yǐn of Lǔ State", 722 BC).
113 BC, "Emperor Wǔ of Hàn Dynasty" issued the first era name, "Jiànyuán" (), and 140 BC is marked as "Jiànyuán 1"(,140 BC).
In China, the first official era name is "Jiànyuán", the last official era name is "Xuāntǒng"().
In Japan, the first era name is "Taika" (), the last era name for Han calendar is "Keiō" (). The first era name for the Gregorian calendar is "Meiji" (). Current era name is "Heisei" ().
After "Xuāntǒng Sānnián", the republic authority adopted the Gregorian calendar and establish the next year as the first year. The country name() is regard as the era name. The time system is used in Taiwan and some overseas Chinese societies still.
Since 1949, in mainland China, the authority abolished the era name of the ROC, and specified the Chinese of Christ Era as common era (). So, the "Gōngyuán" is regard as the current era name. In some Chinese society, the Western Era () takes the place of "Gōngyuán"
Continuous year numbering system.
There's a epoch for each version of Han Calendar, which is called as "Lìyuán"(). The epoch is optimal origin of the calendar, and it's a "Jiǎzǐrì", the first day of a lunar month, and the dark moon and solstice is just at the mid-night(). And tracing back to a perfect day, such as that the day with magical star sign, there's a supreme epoch(). The continuous year base on the supreme epoch is "shàngyuán jīnián"(). More and more factors was added into the supreme epoch, and the "shàngyuán jīnián" became a huge number. So, the supreme epoch and "shàngyuán jīnián" was neglected from "Shòushí" calendar. 
All Han calendar follows the frame of "Tàichū" calendar. The calendric epoch, the winter solstice of 105BC, may be regarded as the epoch of Han calendar. And it's clear that it will be a neutral epoch. 
 The years before 105 BC, 105-"Gōngyuán", such as: 2698 BC is 1594th year before the calendar 
 The year of 105 BC, 105-"Gōngyuán", 105 BC is 0th year
 The years form 104 BC to 1 BC, 105-"Gōngyuán", Such as: 87 BC is 18th year
 The years after 1 BC, "Gōngyuán"+104, such as: 2013 is 2117th year
Shao Yong (, 1011–1077, Courtesy name: Yáofū, Posthumous title: Kāngjié, a philosopher, cosmologist, poet and historian who greatly influenced the development of Non-Confucianism in China.) introduced a timing system in his "The Ultimate which Manages the World"( )
In his time system, 1 round (), which contains 12'9600 years, is a lifecycle of the world. Each "yuán" (round) is divided into 12 assembly(), which are named with Earthly Branches.
The "Hài", "Zǐ", and "Chǒu" are the Remote Ages(),which is the opening of the world () just as the winter. The "Yín", "Mǎo", and "Chén" are the Early Ages(), which is the evolution of the world () just as the spring. The "Sì", "Wǔ", and "Wèi" are the Middle Ages(), which is the climax of the world () just as the summer. The "Shēn", "Yǒu", and "Xū" are the Late Ages(, ), which is the closing of the world () just as the autumn.
Each assembly is divided into 30 run(), and each run is divided into 12 generation(). So, each generation is equivalent to 30 years.
The "Yuán-Huì-Yùn-Shì" is corresponded with "Nián-Yuè-Rì-Shí". So the "Yuán-Huì-Yùn-Shì" is called as the "major tend" or the "numbers of the heaven", and the "Nián-Yuè-Rì-Shí" is called as the "minor tend" or the "numbers of the earth".
The "major tend" or the "numbers of the heaven" is far away from people seemingly, but the "minor tend" or the "numbers of the earth" is close to people. So the "minor tend" or the "numbers of the earth" is adapted by people for predicting destiny or fate. The numbers of "Nián-Yuè-Rì-Shí" is marked with stem-branches. So the "minor tend" of the "numbers of the earth" is show a form of "Bāzì", and the four terms are be called as Four Pillars of Destiny
For example, the eight characters of the birth of Emperor Qiánlóng is "Xīnmǎo-Dīngyǒu-Gēngwǔ-Bǐngzǐ" ().
Shào "Huángjíjīngshì" recorded the history with stem-branches cycle from the first year of the 180th run or 2149th generation ("ARGY 6-30-1-1", 2577 BC) and marked the year with reign title from the "Jiǎchénnián" of the 2156th generation ("ARGY 6-30-8-11", 2357 BC, "Tángyáo 1", ).
According to this timing system, 2014-1-31 is "ARG/YMD 7-12-10/1-1-1".
The official year system of China is divided into many division with reign titles. And there's not an official recognition continuous year system. Referring to BC and AD , many reference points were purposed in the earlier 20th century, such as:
 Huángdì Era (), Based on the birth or regal of Huángdì
 Confucius Era (), Based on the birth or dead year of Confucius
 Yáo Era (), Based on the regnal of Emperor Yao
 Xià Era (), Based on the regnal of Yu of Xia
 Qín Era (), Based on the year when Qin Unified China
 Yuan Era (), Based on the year when Song lost China completely
 Gònghé Era (), Based on "Gònghé 1"
Generally, no reference date is widely accepted. Huángdì Era has significant public implications in oversea Chinese. 
In the 17th century, the Jesuits tried to determine what year should be considered the epoch of Han calendar. In his "Sinicae historiae decas prima" (first published in Munich in 1658), Martino Martini (1614–1661) dated the royal ascension of Huangdi to 2697 BC, but started the Chinese calendar with the reign of Fu Xi, which he claimed started in 2952 BC. Philippe Couplet's (1623–1693) "Chronological table of Chinese monarchs" ("Tabula chronologica monarchiae sinicae"; 1686) also gave the same date for the Yellow Emperor. The Jesuits' dates provoked great interest in Europe, where they were used for comparisons with Biblical chronology. Modern Chinese chronology has generally accepted Martini's dates, except that it usually places the reign of Huangdi in 2698 BC and omits Huangdi's predecessors Fu Xi and Shennong, who are considered "too legendary to include".
Starting in 1903, radical publications started using the projected date of birth of the Yellow Emperor as the first year of Han calendar. Different newspapers and magazines proposed different dates. "Jiangsu", for example, counted 1905 as year 4396 (use an epoch of 2491 BC), whereas the "Minbao" (, the organ of the Tongmenghui) reckoned 1905 as 4603 (use an epoch of 2698 BC). "Liu Shipei" (; 1884–1919) created the Yellow Emperor Calendar, now often used to calculate the date, to show the unbroken continuity of the Han race and Han culture from earliest times. Liu's calendar started with the birth of the Yellow Emperor, which he determined to be 2711 BC. There is no evidence that this calendar was used before the 20th century. Liu calculated that the 1900 international expedition sent by eight foreign powers to suppress the Boxer Uprising entered Beijing in the 4611th year of the Yellow Emperor.
At January 2, 1912, Sun Yat-sen declared that "the Republic of China adopt the Gregorian calendar, and establish Shíyīyuè 13th 4609 of Huángdì Era as the new year's day of the first year of the Republic of China" (). Sun Yat-sen's choice, which implied an epoch of 2698 BC, was adopted by many overseas Chinese communities outside Southeast Asia such as San Francisco's Chinatown. 
 "Sun Yat-sen"'s version 
 Years before 2698 BC not mentioned
 Years before 1 AD HE=2699-CE
 Years after 1 BC HE=2698+CE such as 1912+2698=4610, the year after spring festival of 1912 CE is 4610 HE
Cycle of years.
The years are grouped within a 10-year cycle which is called the 10 Celestial Stems (or called as 10 Heavenly Stems). The names of each year are: "Jiǎnián", "Yǐnián", "Bǐngnián", "Dīngnián", "Wùnián", "Jǐnián", "Gēngnián", "Xīnnián", "Rénnián" and "Guìnián".
The current year (2014-1-31~2015-2-18) is "Jiǎnián".
Each year corresponds to an element in "Wǔxíng". "Jiǎnián" and "Yǐnián" is Wood(), "Bǐngnián" and "Dīngnián" is Fire(), "Wùnián" and "Jǐnián" is Earth(), "Gēngnián" and "Xīnnián" is Metal(), "Rénnián" and "Guìnián" is Water().
Therefore the current year is a "Mùnián".
The years are grouped within a 12-year cycle which is called the 12 Earthly Branches. The names of they years are: "Zǐnián", "Chǒunián", "Yínnián", "Mǎonián", "Chénnián", "Sìnián", "Wǔnián", "Wèinián", "Shēnnián", "Yǒunián", "Xūnián", and "Hàinián".
The current year is "Wǔnián".
Each cycle year is corresponds to an animal of the Chinese zodiac. "Zǐnián" to Rat(), "Chǒunián" to Ox(), "Yínnián" to Tiger(), "Mǎonián" to Rabbit(), "Chénnián" to Dragon(), "Sìnián" to Snake(), "Wǔnián" to Horse(), "Wèinián" to Goat(), "Shēnnián" to Monkey(), "Yǒunián" to Rooster(), "Xūnián" to Dog() and "Hàinián" to Pig().
The seal characters of the earthly branches show the figure of the animals. For example, "Wǔ" shows the figure of the horse face.
Therefore the current year is "Mǎnián" (Horse).
The Heavenly stems cycle and Earthly Branches cycle runs together, and become a 60-year cycle which is called the stem-branches cycle.
The current year is "Jiǎwǔnián", also called as "Mùmǎnián" (Wood Horse).
Around the Han Dynasty, the stem-branches cycle was introduced. In "Míng Dynasty" and "Qīng Dynasty", the stem-branches cycle was used along with the year mark in the Oration to "Yellow Emperor". Such as:
"Oration to Yellow Emperor at the first year of Wànlì"
"That, at the first year of Wànlì, which is Guìyǒunián, at Sìyuè started at Gēngxūrì, at 16th day which is Yǐchǒurì, the Emperor sent... " ()
The table right shows the stem/branch year names, correspondences to the Western (Gregorian) calendar, and other related information for the current decade. Alternatively, see this larger table of the full 60-year cycle.
Notes
1 Regard 2697 BC as 01:00.
2 Regard 2698 BC as the first year.
Gregorian, Miao and Chinese-Uighur calendar.
From 1912, Chinese adopt Gregorian calendar as the public calendar. Gregorian calendar is a solar calendar.
The Miao calendar is a solar calendar too. The Miao calendar is used from ten-thousands years ago, and retired at "Guāngxù 33" (, 1907). The New Year's Day in Miao calendar is the winter solstice. And Miao calendar is similar with Gregorian calendar.
A year in Gregorian and Miao calendar contains 12 months, which are January(the unformatted month), February(the 0th month), March(the 1st month), April(the 2nd month), May(the 3rd month), June(the 4th month), July(the 5th month), August(the 6th month), September(the 7th month), October(the 8th month), November(the 9th month), December(the 10th month).
In Gregorian calendar, February is an unformatted month, and the length is 29 days in leap year, and 28 days in common year.
In Miao Calendar, January is an unformatted month, and the length is 31 days in leap year, and 30 days in common year.
In Gregorian calendar, January/March/May/July/August/October/December(odd in first half, and even in the second half) is 31 days, and April/June/September/November(even in the first half and odd in the second half) is 30 days.
In Miao calendar, all odd months(the 1st,3rd,5th,7th,9th month) are 31 days, and all even months(the 0th,2nd,4th,6th,8th,10th month) are 30 days.
In Miao calendar, each "Zǐ,Chén,Shēn" is leap year. So the intercalary rule is the same as Julian calendar.
There're 6 season in Miao calendar, cold-warm-hot-hot-warm-cold. Each season contains 2 months.
The 28 mansions and the 12 earthly branches runs together, and works out 84 mansion-branches. The 84 mansion-branches are used to count the days, just as 60 stem-branches do.
In 1258, when both North China and the Islamic world were part of the Mongol Empire, Hulagu Khan established an observatory in Maragheh for the astronomer Nasir al-Din al-Tusi at which a few Chinese astronomers were present, resulting in the Chinese-Uighur calendar that al-Tusi describes in his "Zij-i Ilkhani". The 12 year cycle, including Turkic/Mongolian translations of the animal names (known as "sanawat-e turki" سنوات ترکی,) remained in use for chronology, historiography, and bureaucratic purposes in the Persian and Turkic speaking world from Asia Minor to India and Mongolia throughout the Medieval and Early Modern periods. In Iran it remained common in agricultural records and tax assessments until a 1925 law deprecated its use.
In Chinese-Uighur calendar, the year was computed from the vernal equinox, and each month was determined by the transit of the sun into the corresponding zodiac region. So, the month in Chinese-Uighur calendar is 3 month later than that in Miao calendar, and there're 29-32 days in each month.
Ancient Yi calendars.
In Yi areas, a 10-months calendar is found in the early 1980s. It's a solar calendar, which contains 10 months of 36 days, and 2/3 transition days before the winter solstice(The minor year) and summer solstice(The major year).The 36 days is named with 12 earthly branches(from tiger to ox) for 3 times.
The 10 months is named with 10 animals(Tiger, Otter, Crocodile, Boa, Pangolin, Muntjac, Bharal, Ape, Panther, and Lizard).The 10 months is divided into 5 seasons, which named with 5 elements(earth, bronze, water, wood, fire).
In Yi areas, an 18-months calendar is found in the late 1950s. It's a solar calendar, which contains 18 months of 20 days, and 2/3 transition days before the winter solstice and summer solstice.
The 18 months are named as: wind month, twitter month, sprout month, bloom month, fruit month, drought month, rainy month, freshet month, sunny month, withered month, fallen month, frost month, and celebrate month.

</doc>
<doc id="6968" url="http://en.wikipedia.org/wiki?curid=6968" title="Customer relationship management">
Customer relationship management

Customer relationship management (CRM) is a system for managing a company’s interactions with current and future customers. It often involves using technology to organize, automate and synchronize sales, marketing, customer service, and technical support.
Types.
CRM Solutions.
CRM Solutions come with many features and tools and it's important for a company to choose a product based on their specific organisational needs. Most vendors will present information on their respective websites which includes:
Characteristics of CRM.
Well-designed CRM includes the following characteristics:
Marketing and Customer Service.
Customer relationship management systems track and measure marketing campaigns over multiple networks. These systems can track customer analysis by customer clicks and sales. Places where CRM is used include call centers, social media, direct mail, data storage files, banks, and customer data queries.
CRM in customer contact centers.
CRM systems are customer relationship management platforms. The goal of the system is to track, record, store in databases, and then data mine the information in a way that increases customer relations (predominantly increased ARPU, and decreased churn). The CRM codifies the interactions between you and your customers so that you can maximize sales and profit using analytics and KPIs to give the users as much information on where to focus their marketing and customer service to maximize revenue and decrease idle and unproductive contact with your customers. The contact channels (now aiming to be omni-channel from multi-channel) use such operational methods as contact centers. The CRM software is installed in the contact centers, and help direct customers to the right agent or self-empowered knowledge. CRM software can also be used to identify and reward loyal customers over a period of time.
CRM in B2B (Business-to-Business) market.
The modern environment requires one business to interact with another via the web. According to a Sweeney Group definition, CRM is “all the tools, technologies and procedures to manage, improve, or facilitate sales, support and related interactions with customers, prospects, and business partners throughout the enterprise”. It assumes that CRM is involved in every B2B transaction.
Despite the general notion that CRM systems were created for the customer-centric businesses, they can also be applied to B2B environments to streamline and improve customer management conditions. B2C and B2B CRM systems are not created equally and different CRM software applies to B2B and Business-to-Customer (B2C) conditions. B2B relationships usually have longer maturity times than B2C relationships. For the best level of CRM operation in a B2B environment, the software must be personalized and delivered at individual levels.
Implementing CRM to the company.
The following are general guidelines on implementing a CRM system.
Differences between CRM for B2B (Business 2 Business) and B2C (Business 2 Customers).
B2B and B2C marketing operates differently, that is why they cannot use the same software. All the differences are focused on the approach of these two types of businesses:
B2B operations require special CRM solutions that are not the same as for the B2C industry:
So, all the B2B applications must be both personalized and be able to establish communication channels for support of customers
SaaS CRM Software.
Often referred to as "on-demand" software, SaaS based software is delivered via the Internet and does not require installation on your computer. Instead, you'll generally access the software via your web browser. Businesses using the software do not purchase the software, and typically pay a subscription fee to the software vendor.
Small business.
For small businesses a CRM system may simply consist of a contact manager system which integrates emails, documents, jobs, faxes, and scheduling for individual accounts.
CRM systems available for specific markets (legal, finance) frequently focus on event management and relationship tracking as opposed to financial return on investment (ROI).
Social media.
CRM often makes use of social media to build up customer relationships. Some CRM systems integrate social media sites like Twitter, LinkedIn and Facebook to track and communicate with customers sharing their opinions and experiences with a company, products and services.
Enterprise Feedback Management software platforms such as Confirmit, Medallia, and Satmetrix combine internal survey data with trends identified through social media to allow businesses to make more accurate decisions on which products to supply.
Non-profit and membership-based.
Systems for non-profit and also membership-based organizations help track constituents, fund-raising, Sponsors demographics, membership levels, membership directories, volunteering and communication with individuals.
Customer-centric relationship management (CCRM).
CCRM is a style of customer relationship management that focuses on customer preferences instead of customer leverage. 
This is a nascent sub-discipline of traditional customer relationship management; to take advantage of changes in communications technology.
Customer centric organizations help customers make better decisions and it also helps drive profitability. CCRM adds value by engaging customers in individual, interactive relationships.
Customer-centricity differs from "client-centricity" in that the latter refers almost exclusively to business-to-business models rather than customer-facing firms.
Features of CCRM.
Customer-centric relationship management is used in marketing, customer service and sales, including:
Accenture and Emerald Insight are now beginning to focus on CCRM as a discipline, with studies appearing on Mendeley.
Adoption issues.
In 2003, a Gartner report estimated that more than $2 billion had been spent on software that was not being used. According to "CSO Insights", less than 40 percent of 1,275 participating companies had end-user adoption rates above 90 percent. Many corporations only use CRM systems on a partial or fragmented basis.
In a 2007 survey from the UK, four-fifths of senior executives reported that their biggest challenge is getting their staff to use the systems they had installed. 43 percent of respondents said they use less than half the functionality of their existing system.. Recently, it is found in a study that market research regarding consumers preference may increase the adoption of CRM among the developing countries' consumers.
CRM Paradox.
The CRM Paradox, also referred to as the "Dark side of CRM", 
entails favoritism and differential treatment of some customers. This may cause perceptions of unfairness among other customers' buyers. They may opt out of relationships, spread negative information, or engage in misbehavior that may damage the firm. 
CRM fundamentally involves treating customers differently based on the assumption that customers are different and have different needs. 
Such perceived inequality may cause dissatisfaction, mistrust and result in unfair practices. A customer shows trust when he bonds in a relationship with a firm when he knows that the firm is acting fairly and adding value. However, customers may not trust that firms will be fair in splitting the in the first place. For example, Amazon’s test use of dynamic pricing (different prices for different customers) was a public relations nightmare for the company.
Market leaders.
The CRM market grew by 12.5 percent in 2012. The following table lists the top vendors in 2006–2008 and 2013 (figures in millions of US dollars) published in Gartner studies.
Trends.
Many CRM vendors offer subscription-based web tools (cloud computing) and software as a service (SaaS). Some CRM systems are equipped with mobile capabilities, making information accessible to remote sales staff. Salesforce.com was the first company to provide enterprise applications through a web browser, and has maintained its leadership position. Traditional providers have recently moved into the cloud-based market via acquisitions of smaller providers: Oracle purchased RightNow in October 2011 and SAP acquired SuccessFactors in December 2011.
The era of the "social customer" refers to the use of social media (Twitter, Facebook, LinkedIn, Google Plus, Pinterest, Instagram, Yelp, customer reviews in Amazon, etc.) by customers. CR philosophy and strategy has shifted to encompass social networks and user communities.
Sales forces also play an important role in CRM, as maximizing sales effectiveness and increasing sales productivity is a driving force behind the adoption of CRM. Empowering sales managers was listed as one of the top 5 CRM trends in 2013.
Another related development is vendor relationship management (VRM), which provide tools and services that allow customers to manage their individual relationship with vendors. VRM development has grown out of efforts by ProjectVRM at Harvard's Berkman Center for Internet & Society and Identity Commons' Internet Identity Workshops, as well as by a growing number of startups and established companies. VRM was the subject of a cover story in the May 2010 issue of "CRM" Magazine.
In 2001, Doug Laney developed the concept and coined the term 'Extended Relationship Management' (XRM). Laney defines XRM as extending CRM disciplines to secondary allies such as the government, press and industry consortia.
CRM futurist Dennison DeGregor describes a shift from 'push CRM' toward a 'customer transparency' (CT) model, due to the increased proliferation of channels, devices, and social media.
Capterra, Inc. Ranked the Top CRM Software Solutions in 2013 based on total number of customers, total number of users and social presence.

</doc>
<doc id="6970" url="http://en.wikipedia.org/wiki?curid=6970" title="Chuck-a-luck">
Chuck-a-luck

Chuck-a-luck, also known as birdcage, is a game of chance played with three dice. It is derived from grand hazard, and both can be considered a variant of sic bo, a popular casino game, although chuck-a-luck is more of a carnival game than a true casino game. The game is sometimes used as a fundraiser for charity.
Rules.
Chuck-a-luck is played with three standard dice that are kept in a device shaped somewhat like an hourglass that resembles a wire-frame bird cage and that pivots about its centre. The dealer rotates the cage end over end, with the dice landing on the bottom.
Wagers are placed based on possible combinations that can appear on the three dice. The possible wagers are usually fewer than the wagers that are possible in sic bo and, in that sense, chuck-a-luck can be considered to be a simpler game.
The wagers, and their associated odds, that are typically available are set out in the table below.
House advantage or edge.
Chuck-a-luck is a game of chance. That is, on average, even if the dice are not loaded, the players are expected to lose more than they win. The casino's advantage (house advantage or house edge) is greater than most other casino games and can be much greater.
For example, there are 216 (6 × 6 × 6) possible outcomes for a single throw of three dice. For a specific number:
At odds of 1 to 1, 2 to 1 and 10 to 1 respectively for each of these types of outcome, the expected loss as a percentage of the stake wagered is:
1 - ((75/216) × 2 + (15/216) × 3 + (1/216) × 11) = 4.6%
At worse odds of 1 to 1, 2 to 1 and 3 to 1, the expected loss as a percentage of the stake wagered is:
1 - ((75/216) × 2 + (15/216) × 3 + (1/216) × 4) = 7.9%
It should be noted that if the odds are adjusted to 1 to 1, 3 to 1 and 5 to 1 respectively, the expected loss as a percentage is:
1 - ((75/216) × 2 + (15/216) × 4 + (1/216) × 6) = 0%
However, commercially-organised gambling games always have a house advantage which acts as a fee for the privilege of being allowed to play the game, so the last scenario does not represent real practice.
Notes and references.
There is a reference to chuck-a-luck in the Abbott and Costello film "Hold That Ghost".
In Fritz Lang's 1952 film, "Rancho Notorious", chuck-a-luck is the name of the ranch run by Altar Keane (played by Marlene Dietrich) where outlaws hide from the law. Chuck-a-luck is featured in the lyrics to the theme song and in some plot points.
The game is played by Lazar in the James Bond movie "The Man with the Golden Gun".
The game is played by Freddie Rumsen in "Mad Men Season 2 Episode 9: Six-Month Leave".

</doc>
<doc id="6972" url="http://en.wikipedia.org/wiki?curid=6972" title="Chipmunk">
Chipmunk

Chipmunks are small, striped rodents of the family Sciuridae. All species of chipmunks are found in North America, with the exception of the Siberian chipmunk, which is found primarily in Asia.
Taxonomy and systematics.
Chipmunks may be classified either as a single genus, "Tamias" (), or as three genera: "Tamias", which includes the eastern chipmunk; "Eutamias", which includes the Siberian chipmunk; and "Neotamias", which includes the 23 remaining, mostly western, species. These classifications are arbitrary, and most taxonomies over the twentieth century have placed the chipmunks in a single genus. However, studies of mitochondrial DNA show that the divergence between each of the three chipmunk groups is comparable to the genetic dissimilarity between "Marmota" and "Spermophilus".
The genus name "Tamias" is Greek for "treasurer", "steward", or "housekeeper", which is a reference to the animals' role in plant dispersal through their habit of collecting and storing food for winter use.
The common name originally may have been spelled "chitmunk," from the native Odawa (Ottawa) word "jidmoonh", meaning "red squirrel" ("cf." Ojibwe, "ajidamoo"). The earliest form cited in the "Oxford English Dictionary" (from 1842) is "chipmonk," however, "chipmunk" appears in several books from the 1820s and 1830s. Other early forms include "chipmuck" and "chipminck," and in the 1830s they were also referred to as "chip squirrels;" probably in reference to the sound they make. In the mid-1800s, John James Audubon and his sons, included a lithograph of the chipmunk in their "Viviparous Quadrupeds of North America", calling it the "Chipping Squirrel [or] Hackee." Chipmunks have also been referred to as "striped squirrels," "chippers," "munks," "timber tigers," or "ground squirrels" (although the name "ground squirrel" usually refers to other squirrels, such as those of the genus "Spermophilus").
Diet.
Chipmunks have an omnivorous diet primarily consisting of seeds, nuts and other fruits, and buds. They also commonly eat grass, shoots, and many other forms of plant matter, as well as fungi, insects and other arthropods, small frogs, worms, and bird eggs. Around humans, chipmunks can eat cultivated grains and vegetables, and other plants from farms and gardens, so they are sometimes considered pests. Chipmunks mostly forage on the ground, but they climb trees to obtain nuts such as hazelnuts and acorns. At the beginning of autumn, many species of chipmunk begin to stockpile nonperishable foods for winter. They mostly cache their foods in a larder in their burrows and remain in their nests until spring, unlike some other species, which make multiple small caches of food. Cheek pouches allow chipmunks to carry multiple food items to their burrows for either storage or consumption.
Ecology and life history.
Eastern chipmunks mate in early spring and again in early summer, producing litters of four or five young twice each year. Western chipmunks breed only once a year. The young emerge from the burrow after about six weeks and strike out on their own within the next two weeks.
These small mammals fulfill several important functions in forest ecosystems. Their activities harvesting and hoarding tree seeds play a crucial role in seedling establishment. They consume many different kinds of fungi, including those involved in symbiotic mycorrhizal associations with trees, and are an important vector for dispersal of the spores of subterranean sporocarps (truffles) which have co-evolved with these and other mycophagous mammals and thus lost the ability to disperse their spores through the air.
Chipmunks construct expansive burrows which can be more than 3.5 m in length with several well-concealed entrances. The sleeping quarters are kept extremely clean as shells and feces are stored in refuse tunnels.
The eastern chipmunk hibernates in the winter, while western chipmunks do not, relying on the stores in their burrows.
Chipmunks play an important role as prey for various predatory mammals and birds, but are also opportunistic predators themselves, particularly with regard to bird eggs and nestlings. In Oregon, mountain bluebirds ("Siala currucoides") have been observed energetically mobbing chipmunks that they see near their nest trees.
Chipmunks typically live about three years, although have been observed living to nine years in captivity.
Chipmunks in captivity are said to sleep for an average of about 15 hours a day. It is thought that mammals which can sleep in hiding, such as rodents and bats, tend to sleep longer than those that must remain on alert.
Species list.
Subgenus "Eutamias"
Subgenus "Tamias"
Subgenus "Neotamias"
Extinct:

</doc>
<doc id="6974" url="http://en.wikipedia.org/wiki?curid=6974" title="Computer music">
Computer music

Computer music is the applications of computing technology in music composition. It includes the theory and application of new and existing technologies and basic aspects of music, such as sound synthesis, digital signal processing, sound design, sonic diffusion, acoustics, and psychoacoustics. The field of computer music can trace its roots back to the origins of electronic music, and the very first experiments and innovations with electronic instruments at the turn of the 20th century.
More recently, with the advent of personal computing, and the growth of home recording, the term computer music is sometimes used to describe music that has been created using computing technology.
History.
Much of the work on computer music has drawn on the relationship between music theory and mathematics.
The world's first computer to play music was CSIRAC which was designed and built by Trevor Pearcey and Maston Beard. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the "Colonel Bogey March" of which no known recordings exist.
However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice which is current computer-music practice.
The oldest known recordings of computer generated music were played by the Ferranti Mark 1 computer, a commercial version of the Baby Machine from the University of Manchester in the autumn of 1951. The music program was written by Christopher Strachey. During a session recorded by the BBC, the machine managed to work its way through "Baa Baa Black Sheep", "God Save the King" and part of "In the Mood".
Two further major 1950s developments were the origins of digital sound synthesis by computer, and of algorithmic composition programs beyond rote playback. Max Mathews at Bell Laboratories developed the influential MUSIC I program and its descendents, further popularising computer music through a 1963 article in "Science". Amongst other pioneers, the musical chemists Lejaren Hiller and Leonard Isaacson worked on a series of algorithmic composition experiments from 1956-9, manifested in the 1957 premiere of the "Illiac Suite" for string quartet.
In Japan, experiments in computer music date back to 1962, when Keio University professor Sekine and Toshiba engineer Hayashi experimented with the computer. This resulted in a piece entitled "TOSBAC Suite", influenced by the "Illiac Suite". Later Japanese computer music compositions include a piece by Kenjiro Ezaki presented during Osaka Expo '70 and "Panoramic Sonore" (1974) by music critic Akimichi Takeda. Ezaki also published an article called "Contemporary Music and Computers" in 1970. Since then, Japanese research in computer music has largely been carried out for commercial purposes in popular music, though some of the more serious Japanese musicians used large computer systems such as the "Fairlight" in the 1970s. 
Early computer-music programs typically did not run in real time. Programs would run for hours or days, on multi-million-dollar computers, to generate a few minutes of music. One way around this was to use a 'hybrid system', most notably the Roland MC-8 Microcomposer, where a microprocessor-based system controls an analog synthesizer, released in 1978. John Chowning's work on FM synthesis from the 1960s to the 1970s allowed much more efficient digital synthesis, eventually leading to the development of the affordable FM synthesis-based Yamaha DX7 digital synthesizer, released in 1983. In addition to the Yamaha DX7, the advent of inexpensive digital chips and microcomputers opened the door to real-time generation of computer music. In the 1980s, Japanese personal computers such as the NEC PC-88 came installed with FM synthesis sound chips and featured audio programming languages such as Music Macro Language (MML) and MIDI interfaces, which were most often used to produce video game music, or chiptunes. By the early 1990s, the performance of microprocessor-based computers reached the point that real-time generation of computer music using more general programs and algorithms became possible.
Interesting sounds must have a fluidity and changeability that allows them to remain fresh to the ear. In computer music this subtle ingredient is bought at a high computational cost, both in terms of the number of items requiring detail in a score and in the amount of interpretive work the instruments must produce to realize this detail in sound. 
Advances.
Advances in computing power and software for manipulation of digital media have dramatically affected the way computer music is generated and performed. Current-generation micro-computers are powerful enough to perform very sophisticated audio synthesis using a wide variety of algorithms and approaches. Computer music systems and approaches are now ubiquitous, and so firmly embedded in the process of creating music that we hardly give them a second thought: computer-based synthesizers, digital mixers, and effects units have become so commonplace that use of digital rather than analog technology to create and record music is the norm, rather than the exception.
Research.
Despite the ubiquity of computer music in contemporary culture, there is considerable activity in the field of computer music, as researchers continue to pursue new and interesting computer-based synthesis, composition, and performance approaches. Throughout the world there are many organizations and institutions dedicated to the area of computer and electronic music study and research, including the ICMA (International Computer Music Association), IRCAM, GRAME, SEAMUS (Society for Electro Acoustic Music in the United States), CEC (Canadian Electroacoustic Community), and a great number of institutions of higher learning around the world.
Computer-generated music.
Computer-generated music is music composed by, or with the extensive aid of, a computer. Although any music which uses computers in its composition or realisation is computer-generated to some extent, the use of computers is now so widespread (in the editing of pop songs, for instance) that the phrase computer-generated music is generally used to mean a kind of music which could not have been created "without" the use of computers.
We can distinguish two groups of computer-generated music: music in which a computer generated the score, which could be performed by humans, and music which is both composed and performed by computers. There is a large genre of music that is organized, synthesized, and created on computers.
Music composed and performed by computers.
Later, composers such as Gottfried Michael Koenig had computers generate the sounds of the composition as well as the score. Koenig produced algorithmic composition programs which were a generalisation of his own serial composition practice. This is not exactly similar to Xenakis' work as he used mathematical abstractions and examined how far he could explore these musically. Koenig's software translated the calculation of mathematical equations into codes which represented musical notation. This could be converted into musical notation by hand and then performed by human players. His programs Project 1 and Project 2 are examples of this kind of software. Later, he extended the same kind of principles into the realm of synthesis, enabling the computer to produce the sound directly. SSP is an example of a program which performs this kind of function. All of these programs were produced by Koenig at the Institute of Sonology in Utrecht in the 1970s.
Procedures such as those used by Koenig and Xenakis are still in use today. Since the invention of the MIDI system in the early 1980s, for example, some people have worked on programs which map MIDI notes to an algorithm and then can either output sounds or music through the computer's sound card or write an audio file for other programs to play.
Some of these simple programs are based on fractal geometry, and can map midi notes to specific fractals, or fractal equations. Although such programs are widely available and are sometimes seen as clever toys for the non-musician, some professional musicians have given them attention also. The resulting 'music' can be more like noise, or can sound quite familiar and pleasant. As with much algorithmic music, and algorithmic art in general, more depends on the way in which the parameters are mapped to aspects of these equations than on the equations themselves. Thus, for example, the same equation can be made to produce both a lyrical and melodic piece of music in the style of the mid-nineteenth century, and a fantastically dissonant cacophony more reminiscent of the avant-garde music of the 1950s and 1960s.
Other programs can map mathematical formulae and constants to produce sequences of notes. In this manner, an irrational number can give an infinite sequence of notes where each note is a digit in the decimal expression of that number. This sequence can in turn be a composition in itself, or simply the basis for further elaboration.
Operations such as these, and even more elaborate operations can also be performed in computer music programming languages such as Max/MSP, SuperCollider, Csound, Pure Data (Pd), Keykit, and ChucK. These programs now easily run on most personal computers, and are often capable of more complex functions than those which would have necessitated the most powerful mainframe computers several decades ago.
There exist programs that generate "human-sounding" melodies by using a vast database of phrases. One example is Band-in-a-Box, which is capable of creating jazz, blues and rock instrumental solos with almost no user interaction. Another is Impro-Visor, which uses a stochastic context-free grammar to generate phrases and complete solos.
Another 'cybernetic' approach to computer composition uses specialized hardware to detect external stimuli which are then mapped by the computer to realize the performance. Examples of this style of computer music can be found in the middle-80's work of David Rokeby (Very Nervous System) where audience/performer motions are 'translated' to MIDI segments. Computer controlled music is also found in the performance pieces by the Canadian composer Udo Kasemets such as the Marce(ntennia)l Circus C(ag)elebrating Duchamp (1987), a realization of the Marcel Duchamp process piece "Erratum Musical" using an electric model train to collect a hopper-car of stones to be deposited on a drum wired to an Analog:Digital converter, mapping the stone impacts to a score display (performed in Toronto by pianist Gordon Monahan during the 1987 Duchamp Centennial), or his installations and performance works (e.g. Spectrascapes) based on his Geo(sono)scope (1986) 15x4-channel computer-controlled audio mixer. In these latter works, the computer generates sound-scapes from tape-loop sound samples, live shortwave or sine-wave generators.
Computer-generated scores for performance by human players.
Many systems for generating musical scores actually existed well before the time of computers. One of these was Musikalisches Würfelspiel "(Musical dice game"; 18th century), a system which used throws of the dice to randomly select measures from a large collection of small phrases. When patched together, these phrases combined to create musical pieces which could be performed by human players. Although these works were not actually composed with a computer in the modern sense, it uses a rudimentary form of the random combinatorial techniques sometimes used in computer-generated composition.
The world's first digital computer music was generated in Australia by programmer Geoff Hill on the CSIRAC computer which was designed and built by Trevor Pearcey and Maston Beard, although it was only used to play standard tunes of the day. Subsequently, one of the first composers to write music with a computer was Iannis Xenakis. He wrote programs in the FORTRAN language that generated numeric data that he transcribed into scores to be played by traditional musical instruments. An example is "ST/48" of 1962. Although Xenakis could well have composed this music by hand, the intensity of the calculations needed to transform probabilistic mathematics into musical notation was best left to the number-crunching power of the computer.
Computers have also been used in an attempt to imitate the music of great composers of the past, such as Mozart. A present exponent of this technique is David Cope. He wrote computer programs that analyse works of other composers to produce new works in a similar style. He has used this program to great effect with composers such as Bach and Mozart (his program "Experiments in Musical Intelligence" is famous for creating "Mozart's 42nd Symphony"), and also within his own pieces, combining his own creations with that of the computer.
Melomics, a research group from the University of Málaga, Spain, developed a computer composition cluster named Iamus, which composes complex, multi-instrument pieces for editing and performance. Since its inception, Iamus has composed a full album in 2012, appropriately named Iamus, which New Scientist described as "The first major work composed by a computer and performed by a full orchestra." The group has also developed an API for developers to utilize the technology, and makes its music available on its website.
Computer-Aided Algorithmic Composition.
Computer-Aided Algorithmic Composition (CAAC, pronounced "sea-ack") is the implementation and use of algorithmic composition techniques in software. This label is derived from the combination of two labels, each too vague for continued use. The label "computer-aided composition" lacks the specificity of using generative algorithms. Music produced with notation or sequencing software could easily be considered computer-aided composition. The label "algorithmic composition" is likewise too broad, particularly in that it does not specify the use of a computer. The term computer-aided, rather than computer-assisted, is used in the same manner as Computer-Aided Design.
Machine improvisation.
Machine improvisation uses computer algorithms to create improvisation on existing music materials. This is usually done by sophisticated recombination of musical phrases extracted from existing music, either live or pre-recorded. In order to achieve credible improvisation in particular style, machine improvisation uses machine learning and pattern matching algorithms to analyze existing musical examples. The resulting patterns are then used to create new variations "in the style" of the original music, developing a notion of stylistic reinjection.
This is different from other improvisation methods with computers that use algorithmic composition to generate new music without performing analysis of existing music examples.
Statistical style modeling.
Style modeling implies building a computational representation of the musical surface that captures important stylistic features from data. Statistical approaches are used to capture the redundancies in terms of pattern dictionaries or repetitions, which are later recombined to generate new musical data. Style mixing can be realized by analysis of a database containing multiple musical examples in different styles. Machine Improvisation builds upon a long musical tradition of statistical modeling that began with Hiller and Isaacson's "Illiac Suite for String Quartet" (1957) and Xenakis' uses of Markov chains and stochastic processes. Modern methods include the use of lossless data compression for incremental parsing, prediction suffix tree and string searching by factor oracle algorithm (basically a "factor oracle" is a ﬁnite state automaton constructed in linear time and space in an incremental fashion).
Uses of machine improvisation.
Machine improvisation encourages musical creativity by providing automatic modeling and transformation structures for existing music. This creates a natural interface with the musician without need for coding musical algorithms. In live performance, the system re-injects the musician's material in several different ways, allowing a semantics-level representation of the session and a smart recombination and transformation of this material in real-time. In offline version, machine improvisation can be used to achieve style mixing, an approach inspired by Vannevar Bush's memex imaginary machine.
Implementations.
The first system implementing interactive machine improvisation by means of Markov models and style modeling techniques is the Continuator, , developed by François Pachet at Sony CSL Paris in 2002 based on work on non-real time style modeling.
Matlab implementation of the Factor Oracle machine improvisation can be found as part of Computer Audition toolbox.
OMax is a software environment developed in IRCAM. OMax uses OpenMusic and Max. It is based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov and on researches on improvisation with the computer by G. Assayag, M. Chemillier and G. Bloch (aka the" OMax Brothers") in the Ircam Music Representations group.
Musicians working with machine improvisation.
Gerard Assayag (IRCAM, France),
Jeremy Baguyos (University of Nebraska at Omaha, USA)
Tim Blackwell (Goldsmiths College, Great Britain),
George Bloch (Composer, France),
Marc Chemiller (IRCAM/CNRS, France),
Nick Collins (University of Sussex, UK),
Shlomo Dubnov (Composer, Israel / USA),
Mari Kimura (Juilliard, New York City),
George Lewis (Columbia University, New York City),
Bernard Lubat (Pianist, France),
François Pachet (Sony CSL, France),
Joel Ryan (Institute of Sonology, Netherlands),
Michel Waisvisz (STEIM, Netherlands),
David Wessel (CNMAT, California),
Michael Young (Goldsmiths College, Great Britain),
Pietro Grossi (CNUCE, Institute of the National Research Council, Pisa, Italy),
Toby Gifford and Andrew Brown (Griffith University, Brisbane, Australia),
Davis Salks (jazz composer, Hamburg, PA, USA),
Doug Van Nort (electroacoustic improviser, Montreal/New York)
Live coding.
Live coding (sometimes known as 'interactive programming', 'on-the-fly programming', 'just in time programming') is the name given to the process of writing software in realtime as part of a performance. Recently it has been explored as a more rigorous alternative to laptop musicians who, live coders often feel, lack the charisma and pizzazz of musicians performing live.
Generally, this practice stages a more general approach: one of interactive programming, of writing (parts of) programs while they are interpreted. Traditionally most computer music programs have tended toward the old write/compile/run model which evolved when computers were much less powerful. This approach has locked out code-level innovation by people whose programming skills are more modest. Some programs have gradually integrated real-time controllers and gesturing (for example, MIDI-driven software synthesis and parameter control). Until recently, however, the musician/composer rarely had the capability of real-time modification of program code itself. This legacy distinction is somewhat erased by languages such as ChucK, SuperCollider, and Impromptu.
TOPLAP, an ad-hoc conglomerate of artists interested in live coding was formed in 2004, and promotes the use, proliferation and exploration of a range of software, languages and techniques to implement live coding. This is a parallel and collaborative effort e.g. with research at the Princeton Sound Lab, the University of Cologne, and the Computational Arts Research Group at Queensland University of Technology.

</doc>
<doc id="6978" url="http://en.wikipedia.org/wiki?curid=6978" title="Concept">
Concept

A concept is an abstraction or generalization from experience or the result of a transformation of existing concepts. The concept reifies all of its actual or potential instances whether these are things in the real world or other ideas. Concepts are treated in many if not most disciplines whether explicitly such as in psychology, philosophy, etc. or implicitly such as in mathematics, physics, etc.
In metaphysics, and especially ontology, a concept is a fundamental category of existence. In contemporary philosophy, there are at least three prevailing ways to understand what a concept is:
Etymology.
The term "concept" is traced back to 1554–60 (Latin "" - "something conceived"), but what is today termed "the classical theory of concepts" is the theory of Aristotle on the definition of terms. The meaning of "concept" is explored in mainstream information science, cognitive science, metaphysics, and philosophy of mind. In computer and information science contexts, especially, the term 'concept' is often used in unclear or inconsistent ways.
Abstract objects.
In a platonist theory of mind, concepts are construed as abstract objects. This debate concerns the ontological status of concepts - what they are really like.
There is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept "dog" is philosophically distinct from the things in the world grouped by this concept - or the reference class or extension. Concepts that can be equated to a single word are called "lexical concepts". 
Study of concepts and conceptual structure falls into the disciplines of philosophy, psychology, and cognitive science.
Issues in concept theory.
A priori concepts.
Kant declared that human minds possess pure or "a priori" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things "in general", not of a particular thing. According to Kant, there are 12 categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an "a priori" concept can relate to individual phenomena, in a manner analogous to an "a posteriori" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction "a posteriori concepts" (meaning concepts that arise out of experience). An empirical or an "a posteriori" concept is a general representation ("Vorstellung") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)
A concept is a common feature or characteristic. Kant investigated the way that empirical "a posteriori" concepts are created.
Embodied content.
In cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or "recollections", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.
Ontology.
Plato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.
Gottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7)
According to Carl Benjamin Boyer, in the introduction to his "The History of the Calculus and its Conceptual Development", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.
Mental representations.
In a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.
Notable theories on the structure of concepts.
Classical theory.
The classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both "necessary" and "sufficient" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example "bachelor" is said to be defined by "unmarried" and "man". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the "law of the excluded middle", which means that there are no partial members of a class, you are either in or out. 
The classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy - concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept.
Arguments against the classical theory.
Given that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Rosch and Wittgenstein argued against the classical theory. There are six primary arguments summarized as follows:
Prototype theory.
Prototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as "family resemblances". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member - the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category.
Theory-theory.
Theory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory-Theory of concepts is responding to some of the issues of prototype theory and classic theory.

</doc>
<doc id="6979" url="http://en.wikipedia.org/wiki?curid=6979" title="Cell Cycle (journal)">
Cell Cycle (journal)

Cell Cycle is a peer-reviewed scientific journal covering cell biology. The editor-in-chief is Mikhail V. Blagosklonny (Roswell Park Cancer Institute). "Cell Cycle" is abstracted and indexed in Medline/PubMed and the Science Citation Index Expanded.

</doc>
<doc id="6982" url="http://en.wikipedia.org/wiki?curid=6982" title="List of classical music competitions">
List of classical music competitions

The European Classical art music idiom has long relied on the institution of music competitions to provide a public forum that identifies the strongest young players and contributes to the establishment of their professional careers. This is a list of classical music competitions.

</doc>
<doc id="6984" url="http://en.wikipedia.org/wiki?curid=6984" title="Colin Powell">
Colin Powell

Colin Luther Powell (; born April 5, 1937) is an American statesman and a retired four-star general in the United States Army. He was the 65th United States Secretary of State, serving under U.S. President George W. Bush from 2001 to 2005, the first African American to serve in that position. During his military career, Powell also served as National Security Advisor (1987–1989), as Commander of the U.S. Army Forces Command (1989) and as Chairman of the Joint Chiefs of Staff (1989–1993), holding the latter position during the Persian Gulf War. He was the first, and so far the only, African American to serve on the Joint Chiefs of Staff, and was the first of two consecutive African American office-holders to hold the key Administration position of U.S. Secretary of State.
Early life and education.
Powell was born on April 5, 1937, in Harlem, a neighborhood in the New York City borough of Manhattan, to Jamaican immigrant parents Maud Arial (née McKoy) and Luther Theophilus Powell. He also has Scottish ancestry. Powell was raised in the South Bronx and attended Morris High School, a former public school in the Bronx, from which he graduated in 1954. While at school, he worked at a local baby furniture store where he picked up Yiddish from the shopkeepers and some of the customers. He received his BS degree in geology from the City College of New York in 1958 and was a self-admitted C average student. He was later able to earn a MBA degree from the George Washington University in 1971, after his second tour in Vietnam.
Despite his parents' pronunciation of his name as , Powell has pronounced his name since childhood, after the heroic World War II flyer Colin P. Kelly Jr. Public officials and radio and television reporters have used Powell's preferred pronunciation.
Military career.
Powell was a professional soldier for 35 years, holding a variety of command and staff positions and rising to the rank of General.
Training.
Powell described joining the Reserve Officers' Training Corps (ROTC) during college as one of the happiest experiences of his life; discovering something he loved and could do well, he felt he had "found himself." According to Powell: “It was only once I was in college, about six months into college when I found something that I liked, and that was ROTC, Reserve Officer Training Corps in the military. And I not only liked it, but I was pretty good at it. That's what you really have to look for in life, something that you like, and something that you think you're pretty good at. And if you can put those two things together, then you're on the right track, and just drive on.” Cadet Powell joined the Pershing Rifles, the ROTC fraternal organization and drill team begun by General John Pershing. Even after he had become a general, Powell kept on his desk a pen set he had won for a drill team competition.
Upon graduation, he received a commission as an Army second lieutenant. After attending basic training at Fort Benning, Powell was assigned to the 48th Infantry, in West Germany, as a platoon leader.
Vietnam War.
In his autobiography, Powell said he is haunted by the nightmare of the Vietnam War and felt that the leadership was very ineffective.
Captain Powell served a tour in Vietnam as a South Vietnamese Army (ARVN) advisor from 1962 to 1963. While on patrol in a Viet Cong-held area, he was wounded by stepping on a punji stake. The large infection made it difficult for him to walk, and caused his foot to swell for a short time, shortening his first tour.
He returned to Vietnam as a major in 1968, serving in the Americal Division (23rd Infantry Division), then as assistant chief of staff of operations for the Americal Division. During the second tour in Vietnam he was decorated for bravery after he survived a helicopter crash, single-handedly rescuing three others, including division commander Major General Charles Martin Gettys, from the burning wreckage. He was charged with investigating a detailed letter by Tom Glen (a soldier from the 11th Light Infantry Brigade), which backed up rumored allegations of the My Lai Massacre. Powell wrote: "In direct refutation of this portrayal is the fact that relations between American soldiers and the Vietnamese people are excellent." Later, Powell's assessment would be described as whitewashing the news of the massacre, and questions would continue to remain undisclosed to the public. In May 2004 Powell said to television and radio host Larry King, "I mean, I was in a unit that was responsible for My Lai. I got there after My Lai happened. So, in war, these sorts of horrible things happen every now and again, but they are still to be deplored."
After the Vietnam War.
Powell served a White House fellowship, a highly selective and prestigious position, under President Richard Nixon from 1972 to 1973.
In his autobiography, "My American Journey", Powell named several officers he served under who inspired and mentored him. As a lieutenant colonel serving in South Korea, Powell was very close to General Henry "Gunfighter" Emerson. Powell said he regarded Emerson as one of the most caring officers he ever met. Emerson insisted his troops train at night to fight a possible North Korean attack, and made them repeatedly watch the television film "Brian's Song" to promote racial harmony. Powell always professed that what set Emerson apart, was his great love of his soldiers and concern for their welfare. After a race riot occurred, where African American soldiers almost killed a Caucasian officer, Powell was charged by Emerson to crackdown on black militants; Powell's efforts led to the discharge of one soldier, and other efforts to reduce racial tensions.
A "political general".
In the early 1980s, Powell served at Fort Carson, Colorado. After he left Fort Carson, Powell became senior military assistant to Secretary of Defense Caspar Weinberger, whom he assisted during the 1983 invasion of Grenada and the 1986 airstrike on Libya.
In 1986, Powell took over the command of V Corps in Frankfurt, Germany, from Robert Lewis "Sam" Wetzel.
Following the Iran Contra scandal, Powell became, at the age of 49, Ronald Reagan's National Security Advisor, serving from 1987 to 1989 while retaining his Army commission as a lieutenant general.
In April 1989, after his tenure with the National Security Council, Powell was promoted to four-star general under President George H. W. Bush and briefly served as the Commander in Chief, Forces Command (FORSCOM), headquartered at Fort McPherson, Georgia, overseeing all Army, Army Reserve, and National Guard units in the Continental U.S., Alaska, Hawaii, and Puerto Rico. He became only the third general since World War II, joining Dwight D. Eisenhower and Alexander Haig, to reach four-star rank without ever serving as a division commander.
Later that year, President George H. W. Bush selected him as Chairman of the Joint Chiefs of Staff.
Chairman of the Joint Chiefs of Staff.
Powell's last military assignment, from October 1, 1989, to September 30, 1993, was as the 12th Chairman of the Joint Chiefs of Staff, the highest military position in the Department of Defense. At age 52, he became the youngest officer, and first Afro-Caribbean American, to serve in this position. Powell was also the first JCS Chair who received his commission through ROTC.
During this time, he oversaw 28 crises, including the invasion of Panama in 1989 to remove General Manuel Noriega from power and Operation Desert Storm in the 1991 Persian Gulf War. During these events, Powell earned his nickname, "the reluctant warrior." He rarely advocated military intervention as the first solution to an international crisis, and instead usually prescribed diplomacy and containment.
As a military strategist, Powell advocated an approach to military conflicts that maximizes the potential for success and minimizes casualties. A component of this approach is the use of overwhelming force, which he applied to Operation Desert Storm in 1991. His approach has been dubbed the "Powell Doctrine". Powell continued as chairman of the JCS into the Clinton presidency but as a dedicated "realist" he considered himself a bad fit for an administration largely made up of liberal internationalists. He clashed with then-U.S. ambassador to the United Nations Madeleine Albright over the Bosnian crisis, as he opposed any military interventions that didn't involve US interests.
During his chairmanship of the JCS, there was discussion of awarding Powell a fifth star, granting him the rank of General of the Army. But even in the wake of public and Congressional pressure to do so, Clinton-Gore presidential transition team staffers decided against it.
13 Rules of Leadership.
First printed in the August 13, 1989 issue of "Parade" magazine, these are Colin Powell's 13 Rules of Leadership.
Potential presidential candidate.
Powell's experience in military matters made him a very popular figure with both American political parties. Many Democrats admired his moderate stance on military matters, while many Republicans saw him as a great asset associated with the successes of past Republican administrations. Put forth as a potential Democratic Vice Presidential nominee in the 1992 U.S. presidential election or even potentially replacing Vice President Dan Quayle as the Republican Vice Presidential nominee, Powell eventually declared himself a Republican and began to campaign for Republican candidates in 1995. He was touted as a possible opponent of Bill Clinton in the 1996 U.S. presidential election, possibly capitalizing on a split conservative vote in Iowa and even leading New Hampshire polls for the GOP nomination, but Powell declined, citing a lack of passion for politics. Powell defeated Clinton 50-38 in a hypothetical match-up proposed to voters in the exit polls conducted on Election Day. Despite not standing in the race, Powell won the Republican New Hampshire Vice-Presidential primary on write-in votes.
In 1997 Powell founded America's Promise with the objective of helping children from all socioeconomic sectors. That same year saw the establishment of The Colin L. Powell Center for Leadership and Service. The mission of the Center is to "prepare new generations of publicly engaged leaders from populations previously underrepresented in public service and policy circles, to build a strong culture of civic engagement at City College, and to mobilize campus resources to meet pressing community needs and serve the public good." 
In the 2000 U.S. presidential election Powell campaigned for Senator John McCain and later Texas Governor George W. Bush after the latter secured the Republican nomination. Bush eventually won, and Powell was appointed Secretary of State.
Secretary of State.
As Secretary of State in the Bush administration, Powell was perceived as moderate. Powell was unanimously confirmed by the United States Senate. Over the course of his tenure he traveled less than any other U.S. Secretary of State in 30 years.
On September 11, 2001, Powell was in Lima, Peru, meeting with President Alejandro Toledo and US Ambassador John Hamilton, and attending the special session of the OAS General Assembly that subsequently adopted the Inter-American Democratic Charter. After the September 11 attacks, Powell's job became of critical importance in managing America's relationships with foreign countries in order to secure a stable coalition in the War on Terrorism.
Powell came under fire for his role in building the case for the 2003 Invasion of Iraq. In a press statement on February 24, 2001, he had said that sanctions against Iraq had prevented the development of any weapons of mass destruction by Saddam Hussein. As was the case in the days leading up to the Persian Gulf War, Powell was initially opposed to a forcible overthrow of Saddam, preferring to continue a policy of containment. However, Powell eventually agreed to go along with the Bush administration's determination to remove Saddam. He had often clashed with others in the administration, who were reportedly planning an Iraq invasion even before the September 11 attacks, an insight supported by testimony by former terrorism czar Richard Clarke in front of the 9/11 Commission. The main concession Powell wanted before he would offer his full support for the Iraq War was the involvement of the international community in the invasion, as opposed to a unilateral approach. He was also successful in persuading Bush to take the case of Iraq to the United Nations, and in moderating other initiatives. Powell was placed at the forefront of this diplomatic campaign.
Powell's chief role was to garner international support for a multi-national coalition to mount the invasion. To this end, Powell addressed a plenary session of the United Nations Security Council on February 5, 2003, to argue in favor of military action. Citing numerous anonymous Iraqi defectors, Powell asserted that "there can be no doubt that Saddam Hussein has biological weapons and the capability to rapidly produce more, many more." Powell also stated that there was "no doubt in my mind" that Saddam was working to obtain key components to produce nuclear weapons.
Most observers praised Powell's oratorical skills. However, Britain's "Channel 4 News" reported soon afterwards that a UK intelligence dossier that Powell had referred to as a "fine paper" during his presentation had been based on old material and plagiarized an essay by American graduate student Ibrahim al-Marashi.
A 2004 report by the Iraq Survey Group concluded that the evidence that Powell offered to support the allegation that the Iraqi government possessed weapons of mass destruction (WMDs) was inaccurate.
In an interview with Charlie Rose, Powell contended that prior to his UN presentation, he had merely four days to review the data concerning WMD in Iraq.
A Senate report on intelligence failures would later detail the intense debate that went on behind the scenes on what to include in Powell's speech. State Department analysts had found dozens of factual problems in drafts of the speech. Some of the claims were taken out, but others were left in, such as claims based on the yellowcake forgery. The administration came under fire for having acted on faulty intelligence, particularly what was single-sourced to the informant known as Curveball. Powell later recounted how Vice President Dick Cheney had joked with him before he gave the speech, telling him, "You've got high poll ratings; you can afford to lose a few points." Powell's longtime aide-de-camp and Chief of Staff from 1989–2003, Colonel Lawrence Wilkerson, later characterized Cheney's view of Powell's mission as to "go up there and sell it, and we'll have moved forward a peg or two. Fall on your damn sword and kill yourself, and I'll be happy, too."
In September 2005, Powell was asked about the speech during an interview with Barbara Walters and responded that it was a "blot" on his record. He went on to say, "It will always be a part of my record. It was painful. It's painful now."
Wilkerson said that he inadvertently participated in a hoax on the American people in preparing Powell's erroneous testimony before the United Nations Security Council.
Because Powell was seen as more moderate than most figures in the administration, he was spared many of the attacks that have been leveled at more controversial advocates of the invasion, such as Donald Rumsfeld and Paul Wolfowitz. At times, infighting among the Powell-led State Department, the Rumsfeld-led Defense Department, and Cheney's office had the effect of polarizing the administration on crucial issues, such as what actions to take regarding Iran and North Korea.
After Saddam Hussein had been deposed, Powell's new role was to once again establish a working international coalition, this time to assist in the rebuilding of post-war Iraq. On September 13, 2004, Powell testified before the Senate Governmental Affairs Committee, acknowledging that the sources who provided much of the information in his February 2003 UN presentation were "wrong" and that it was "unlikely" that any stockpiles of WMDs would be found. Claiming that he was unaware that some intelligence officials questioned the information prior to his presentation, Powell pushed for reform in the intelligence community, including the creation of a national intelligence director who would assure that "what one person knew, everyone else knew."
Additionally, Powell has been critical of other instances of U.S. foreign policy in the past, such as its support for the 1973 Chilean coup d'état. From two separate interviews in 2003, Powell stated in one about the 1973 event "I can't justify or explain the actions and decisions that were made at that time. It was a different time. There was a great deal of concern about communism in this part of the world. Communism was a threat to the democracies in this part of the world. It was a threat to the United States." In another interview, however, he also simply stated "With respect to your earlier comment about Chile in the 1970s and what happened with Mr. Allende, it is not a part of American history that we're proud of."
Powell announced his resignation as Secretary of State on November 15, 2004. According to "The Washington Post", he had been asked to resign by the president's chief of staff, Andrew Card. Powell announced that he would stay on until the end of Bush's first term or until his replacement's confirmation by Congress. The following day, Bush nominated National Security Advisor Condoleezza Rice as Powell's successor. News of Powell's leaving the Administration spurred mixed reactions from politicians around the world — some upset at the loss of a statesman seen as a moderating factor within the Bush administration, but others hoping for Powell's successor to wield more influence within the cabinet.
In mid-November, Powell stated that he had seen new evidence suggesting that Iran was adapting missiles for a nuclear delivery system. The accusation came at the same time as the settlement of an agreement between Iran, the IAEA, and the European Union.
On December 31, 2004, Powell rang in the New Year by pressing a button in Times Square with New York City Mayor Michael Bloomberg to initiate the ball drop and 60 second countdown, ushering in the year 2005. He appeared on the networks that were broadcasting New Year's Eve specials and talked about this honor, as well as being a native of New York City.
Life after diplomatic service.
After retiring from the role of Secretary of State, Powell returned to private life. In April 2005, he was privately telephoned by Republican senators Lincoln Chafee and Chuck Hagel, at which time Powell expressed reservations and mixed reviews about the nomination of John R. Bolton as ambassador to the United Nations, but refrained from advising the senators to oppose Bolton (Powell had clashed with Bolton during Bush's first term). The decision was viewed as potentially dealing significant damage to Bolton's chances of confirmation. Bolton was put into the position via a recess appointment because of the strong opposition in the Senate.
On April 28, 2005, an opinion piece in "The Guardian" by Sidney Blumenthal (a former top aide to President Bill Clinton) claimed that Powell was in fact "conducting a campaign" against Bolton because of the acrimonious battles they had had while working together, which among other things had resulted in Powell cutting Bolton out of talks with Iran and Libya after complaints about Bolton's involvement from the British. Blumenthal added that "The foreign relations committee has discovered that Bolton made a highly unusual request and gained access to 10 intercepts by the National Security Agency. Staff members on the committee believe that Bolton was probably spying on Powell, his senior advisors and other officials reporting to him on diplomatic initiatives that Bolton opposed."
In July 2005, Powell joined Kleiner, Perkins, Caufield & Byers, a well-known Silicon Valley venture capital firm, with the title of "strategic limited partner."
In September 2005, Powell criticized the response to Hurricane Katrina. Powell said that thousands of people were not properly protected, but because they were poor rather than because they were black.
On January 5, 2006, he participated in a meeting at the White House of former Secretaries of Defense and State to discuss United States foreign policy with Bush administration officials. In September 2006, Powell sided with more moderate Senate Republicans in supporting more rights for detainees and opposing President Bush's terrorism bill. He backed Senators John Warner, John McCain and Lindsey Graham in their statement that U.S. military and intelligence personnel in future wars will suffer for abuses committed in 2006 by the U.S. in the name of fighting terrorism. Powell stated that "The world is beginning to doubt the moral basis of [America's] fight against terrorism."
Also in 2006, Powell began appearing as a speaker at a series of motivational events called "Get Motivated", along with former New York Mayor Rudy Giuliani. In his speeches for the tour, he openly criticized the Bush Administration on a number of issues. Powell has been the recipient of mild criticism for his role with "Get Motivated" which has been called a "get-rich-quick-without-much-effort, feel-good schemology."
In 2007 he joined the Board of Directors of Steve Case's new company Revolution Health. Powell also serves on the Council on Foreign Relations Board of directors.
Powell, in honor of Martin Luther King Day, dropped the ceremonial first puck at a New York Islanders hockey game at Nassau Coliseum on January 21, 2008. On November 11, 2008, Powell again dropped the puck in recognition of Military Appreciation Day and Veterans Day.
Recently, Powell has encouraged young people to continue to use new technologies to their advantage in the future. In a speech at the Center for Strategic and International Studies to a room of young professionals, he said, "That's your generation...a generation that is hard-wired digital, a generation that understands the power of the information revolution and how it is transforming the world. A generation that you represent, and you're coming together to share; to debate; to decide; to connect with each other." At this event, he encouraged the next generation to involve themselves politically on the upcoming Next America Project, which uses online debate to provide policy recommendations for the upcoming administration.
In 2008, Powell served as a spokesperson for National Mentoring Month, a campaign held each January to recruit volunteer mentors for at-risk youth.
Soon after Barack Obama's 2008 election, Powell began being mentioned as a possible cabinet member. He was not nominated.
In September 2009, Powell advised President Obama against surging US forces in Afghanistan. The president announced the surge the following December.
On March 14, 2014, Salesforce.com announced that Powell had joined its Board of Directors.
Political views.
A liberal Republican, Powell is well known for his willingness to support liberal or centrist causes. He is pro-choice regarding abortion, and in favor of "reasonable" gun control. He stated in his autobiography that he supports affirmative action that levels the playing field, without giving a leg up to undeserving persons because of racial issues. Powell was also instrumental in the 1993 implementation of the military's don't ask, don't tell policy, though he later supported its repeal as proposed by Robert Gates and Admiral Mike Mullen in January 2010, saying "circumstances had changed".
The Vietnam War had a profound effect on Powell's views of the proper use of military force. These views are described in detail in the autobiography "My American Journey". The Powell Doctrine, as the views became known, was a central component of US policy in the Gulf War (the first U.S. war in Iraq) and U.S. invasion of Afghanistan (the overthrow of the Taliban regime in Afghanistan following the September 11 attacks). The hallmark of both operations was strong international cooperation, and the use of overwhelming military force.
Powell was the subject of controversy in 2004 when, in a conversation with British Foreign Secretary Jack Straw, he reportedly referred to neoconservatives within the Bush administration as "fucking crazies." In addition to being reported in the press (though generally, the expletive was censored in the U.S. press), the quote was used by James Naughtie in his book, "The Accidental American: Tony Blair and the Presidency", and by Chris Patten in his book, "Cousins and Strangers: America, Britain, and Europe in a New Century".
In a September 2006 letter to Sen. John McCain, General Powell expressed opposition to President Bush's push for military tribunals of those formerly and currently classified as enemy combatants. Specifically, he objected to the effort in Congress to "redefine Common Article 3 of the Geneva Convention." He also asserted: "The world is beginning to doubt the moral basis of our fight against terrorism."
Powell endorsed President Obama in 2008 and again in 2012. When asked why he is still a Republican on Meet the Press he said, “I’m still a Republican. And I think the Republican Party needs me more than the Democratic Party needs me. And you can be a Republican and still feel strongly about issues such as immigration, and improving our education system, and doing something about some of the social problems that exist in our society and our country. I don’t think there’s anything inconsistent with this.” <Ref></ref>
Views on the Iraq War.
While Powell was wary of a military solution, he supported the decision to invade Iraq after the Bush administration concluded that diplomatic efforts had failed. After his departure from the State Department, Powell repeatedly emphasized his continued support for American involvement in the Iraq War.
At the 2007 Aspen Ideas Festival in Colorado, Powell revealed that he had spent two and a half hours explaining to President Bush "the consequences of going into an Arab country and becoming the occupiers." During this discussion, he insisted that the U.S. appeal to the United Nations first, but if diplomacy failed, he would support the invasion: "I also had to say to him that you are the President, you will have to make the ultimate judgment, and if the judgment is this isn't working and we don't think it is going to solve the problem, then if military action is undertaken I'm with you, I support you."
In a 2008 interview on CNN, Powell reiterated his support for the 2003 decision to invade Iraq in the context of his endorsement of Barack Obama, stating: "My role has been very, very straightforward. I wanted to avoid a war. The president [Bush] agreed with me. We tried to do that. We couldn't get it through the U.N. and when the president made the decision, I supported that decision. And I've never blinked from that. I've never said I didn't support a decision to go to war."
Powell's position on the Iraq War troop surge of 2007 has been less clear. In December 2006, he expressed skepticism that the strategy would work and whether the U.S. military had enough troops to carry it out successfully. He stated: "I am not persuaded that another surge of troops into Baghdad for the purposes of suppressing this communitarian violence, this civil war, will work." Following his endorsement of Barack Obama in October 2008, however, Powell praised General David Petraeus and U.S. troops, as well as the Iraqi government, concluding that "it's starting to turn around." By mid-2009, he had concluded a surge of U.S. forces in Iraq should have come sooner, perhaps in late 2003. Throughout this period, Powell consistently argued that Iraqi political progress was essential, not just military force.
Role in presidential election of 2008.
Powell donated the maximum allowable amount to John McCain's campaign in the summer of 2007 and in early 2008, his name was listed as a possible running mate for Republican nominee McCain's bid during the 2008 U.S. presidential election. However, on October 19, 2008, Powell announced his endorsement of Barack Obama during a "Meet the Press" interview, citing "his ability to inspire, because of the inclusive nature of his campaign, because he is reaching out all across America, because of who he is and his rhetorical abilities," in addition to his "style and substance." He additionally referred to Obama as a "transformational figure". Powell further questioned McCain's judgment in appointing Sarah Palin as the vice presidential candidate, stating that despite the fact that she is admired, "now that we have had a chance to watch her for some seven weeks, I don't believe she's ready to be president of the United States, which is the job of the vice president." He said that Obama's choice for vice-president, Joe Biden, was ready to be president. He also added that he was "troubled" by the "false intimations that Obama was Muslim." Powell stated that "[Obama] is a Christian — he's always been a Christian... But the really right answer is, what if he is? Is there something wrong with being a Muslim in this country? The answer's no, that's not America." Powell then referenced Kareem Rashad Sultan Khan, a Muslim American soldier in the U.S. Army who served and died in the Iraq War. He later stated, "Over the last seven weeks, the approach of the Republican Party has become narrower and narrower [...] I look at these kind of approaches to the campaign, and they trouble me." Powell concluded his Sunday morning talk show comments, "It isn't easy for me to disappoint Sen. McCain in the way that I have this morning, and I regret that [...] I think we need a transformational figure. I think we need a president who is a generational change and that's why I'm supporting Barack Obama, not out of any lack of respect or admiration for Sen. John McCain." Later in a December 12, 2008, CNN interview with Fareed Zakaria, Powell reiterated his belief that during the last few months of the campaign, Palin pushed the Republican party further to the right and had a polarizing impact on it.
Views on the Obama administration.
In a July 2009 CNN interview with John King, Powell expressed concern over President Obama growing the size of the federal government and the size of the federal budget deficit. In September 2010, he criticized the Obama administration for not focusing "like a razor blade" on the economy and job creation. Powell reiterated that Obama was a "transformational figure." In a video that aired on CNN.com in November 2011, Colin Powell said in reference to Barack Obama, " . . . many of his decisions have been quite sound. The financial system was put back on a stable basis."
On October 25, 2012, 12 days before the presidential election, he gave his endorsement to President Obama for re-election during a broadcast of CBS This Morning. He cited success and forward progress in foreign and domestic policy arenas under the Obama Administration, and made the following statement: “I voted for him in 2008 and I plan to stick with him in 2012 and I'll be voting for he (sic) and for Vice President Joe Biden next month.”
As additional reason for his endorsement, Powell cited the changing positions and perceived lack of thoughtfulness of Mitt Romney on foreign affairs, and a concern for the validity of Romney's economic plans.
In an interview with ABC’s Diane Sawyer and George Stephanopoulos during ABC’s coverage of President Obama's second inauguration, Powell criticized members of the Republican Party who "...demonize the president”. He called on GOP leaders to publicly denounce such talk.
Views on LGBT issues.
In late May 2012 he expressed support for the legalization of same-sex marriage. He had earlier supported the repeal of Don't Ask Don't Tell.
Personal life.
Powell married Alma Johnson on August 25, 1962. Their son, Michael Powell, was the chairman of the Federal Communications Commission (FCC) from 2001 to 2005. His daughters are Linda Powell, an actress, and Annemarie Powell. As a hobby, Powell restores old Volvo and Saab cars. In 2013, he faced questions about a relationship with a Romanian diplomat, after a hacked AOL email account had been made public. He acknowledged a "very personal" email relationship but denied further involvement.
Civilian awards and honors.
Powell's civilian awards include two Presidential Medals of Freedom, the President's Citizens Medal, the Congressional Gold Medal, the Secretary of State Distinguished Service Medal, the Secretary of Energy Distinguished Service Medal, and the Ronald Reagan Freedom Award. Several schools and other institutions have been named in his honor and he holds honorary degrees from universities and colleges across the country.

</doc>
<doc id="6985" url="http://en.wikipedia.org/wiki?curid=6985" title="Chlorophyll">
Chlorophyll

Chlorophyll (also chlorophyl) is a green pigment found in cyanobacteria and the chloroplasts of algae and plants. Its name is derived from the Greek words χλωρός, "chloros" ("green") and φύλλον, "phyllon" ("leaf"). Chlorophyll is an extremely important biomolecule, critical in photosynthesis, which allows plants to absorb energy from light. Chlorophyll absorbs light most strongly in the blue portion of the electromagnetic spectrum, followed by the red portion. Conversely, it is a poor absorber of green and near-green portions of the spectrum, hence the green color of chlorophyll-containing tissues. Chlorophyll was first isolated by Joseph Bienaimé Caventou and Pierre Joseph Pelletier in 1817.
Chlorophyll and photosynthesis.
Chlorophyll is vital for photosynthesis, which allows plants to absorb energy from light.
Chlorophyll molecules are specifically arranged in and around photosystems that are embedded in the thylakoid membranes of chloroplasts. In these complexes, chlorophyll serves two primary functions. The function of the vast majority of chlorophyll (up to several hundred molecules per photosystem) is to absorb light and transfer that light energy by resonance energy transfer to a specific chlorophyll pair in the reaction center of the photosystems.
The two currently accepted photosystem units are Photosystem II and Photosystem I, which have their own distinct reaction center chlorophylls, named P680 and P700, respectively. These pigments are named after the wavelength (in nanometers) of their red-peak absorption maximum. The identity, function and spectral properties of the types of chlorophyll in each photosystem are distinct and determined by each other and the protein structure surrounding them. Once extracted from the protein into a solvent (such as acetone or methanol), these chlorophyll pigments can be separated in a simple paper chromatography experiment and, based on the number of polar groups between chlorophyll a and chlorophyll b, will chemically separate out on the paper.
The function of the reaction center chlorophyll is to use the energy absorbed by and transferred to it from the other chlorophyll pigments in the photosystems to undergo a charge separation, a specific redox reaction in which the chlorophyll donates an electron into a series of molecular intermediates called an electron transport chain. The charged reaction center chlorophyll (P680+) is then reduced back to its ground state by accepting an electron. In Photosystem II, the electron that reduces P680+ ultimately comes from the oxidation of water into O2 and H+ through several intermediates. This reaction is how photosynthetic organisms such as plants produce O2 gas, and is the source for practically all the O2 in Earth's atmosphere. Photosystem I typically works in series with Photosystem II; thus the P700+ of Photosystem I is usually reduced, via many intermediates in the thylakoid membrane, by electrons ultimately from Photosystem II. Electron transfer reactions in the thylakoid membranes are complex, however, and the source of electrons used to reduce P700+ can vary.
The electron flow produced by the reaction center chlorophyll pigments is used to shuttle H+ ions across the thylakoid membrane, setting up a chemiosmotic potential used mainly to produce ATP chemical energy; and those electrons ultimately reduce NADP+ to NADPH, a universal reductant used to reduce CO2 into sugars as well as for other biosynthetic reductions.
Reaction center chlorophyll–protein complexes are capable of directly absorbing light and performing charge separation events without other chlorophyll pigments, but the absorption cross section (the likelihood of absorbing a photon under a given light intensity) is small. Thus, the remaining chlorophylls in the photosystem and antenna pigment protein complexes associated with the photosystems all cooperatively absorb and funnel light energy to the reaction center. Besides chlorophyll "a", there are other pigments, called accessory pigments, which occur in these pigment–protein antenna complexes.
A green sea slug, "Elysia chlorotica", has been found to use the chlorophyll it has eaten to perform photosynthesis for itself. This process is known as kleptoplasty, and no other animal has been found to have this ability.
Chemical structure.
Chlorophyll is a chlorin pigment, which is structurally similar to and produced through the same metabolic pathway as other porphyrin pigments such as heme. At the center of the chlorin ring is a magnesium ion. At the time of its discovery in the early 1900s, this was the first time that this element had been detected in living tissue. For the structures depicted in this article, some of the ligands attached to the Mg2+ center are omitted for clarity. The chlorin ring can have several different side chains, usually including a long phytol chain. There are a few different forms that occur naturally, but the most widely distributed form in terrestrial plants is chlorophyll "a". After initial work done by German chemist Richard Willstätter spanning from 1905 to 1915, the general structure of chlorophyll "a" was elucidated by Hans Fischer in 1940. By 1960, when most of the stereochemistry of chlorophyll "a" was known, Robert Burns Woodward published a total synthesis of the molecule. In 1967, the last remaining stereochemical elucidation was completed by Ian Fleming, and in 1990 Woodward and co-authors published an updated synthesis. Chlorophyll f was announced to be present in cyanobacteria and other oxygenic microorganisms that form stromatolites in 2010; a molecular formula of C55H70O6N4Mg and a structure of (2-formyl)-chlorophyll "a" were deduced based on NMR, optical and mass spectra. The different structures of chlorophyll are summarized below:
When leaves degreen in the process of plant senescence, chlorophyll is converted to a group of colourless tetrapyrroles known as nonfluorescent chlorophyll catabolites (NCC's) with the general structure:
These compounds have also been identified in several ripening fruits.
Spectrophotometry.
Measurement of the absorption of light is complicated by the solvent used to extract it from plant material, which affects the values obtained,
By measuring the absorption of light in the red and far red regions it is possible to estimate the concentration of chlorophyll within a leaf. 
<br>In his scientific paper Gitelson (1999) states, "The ratio between chlorophyll fluorescence, at 735 nm and the wavelength range 700nm to 710 nm, F735/F700 was found to be linearly proportional to the chlorophyll content (with determination coefficient, r2, more than 0.95) and thus this ratio can be used as a precise indicator of chlorophyll content in plant leaves." The fluorescent ratio chlorophyll content meters use this technique.
Biosynthesis.
In plants, chlorophyll may be synthesized from succinyl-CoA and glycine, although the immediate precursor to chlorophyll "a" and "b" is protochlorophyllide. In Angiosperm plants, the last step, conversion of protochlorophyllide to chlorophyll, is light-dependent and such plants are pale (etiolated) if grown in the darkness. Non-vascular plants and green algae have an additional light-independent enzyme and grow green in the darkness instead.
Chlorophyll itself is bound to proteins and can transfer the absorbed energy in the required direction. Protochlorophyllide occurs mostly in the free form and, under light conditions, acts as a photosensitizer, forming highly toxic free radicals. Hence, plants need an efficient mechanism of regulating the amount of chlorophyll precursor. In angiosperms, this is done at the step of aminolevulinic acid (ALA), one of the intermediate compounds in the biosynthesis pathway. Plants that are fed by ALA accumulate high and toxic levels of protochlorophyllide; so do the mutants with the damaged regulatory system.
Chlorosis is a condition in which leaves produce insufficient chlorophyll, turning them yellow. Chlorosis can be caused by a nutrient deficiency of iron—called iron chlorosis—or by a shortage of magnesium or nitrogen. Soil pH sometimes plays a role in nutrient-caused chlorosis; many plants are adapted to grow in soils with specific pH levels and their ability to absorb nutrients from the soil can be dependent on this. Chlorosis can also be caused by pathogens including viruses, bacteria and fungal infections, or sap-sucking insects.
Complementary light absorbance of anthocyanins with chlorophylls.
Anthocyanins are other plant pigments. The absorbance pattern responsible for the red color of anthocyanins may be complementary to that of green chlorophyll in photosynthetically active tissues such as young "Quercus coccifera" leaves. It may protect the leaves from attacks by plant eaters that may be attracted by green color.
Culinary use.
Chlorophyll is registered as a food additive (colorant), and its E number is E140. Chefs use chlorophyll to color a variety of foods and beverages green, such as pasta and absinthe. Chlorophyll is not soluble in water, and it is first mixed with a small quantity of vegetable oil to obtain the desired solution. Extracted liquid chlorophyll was considered to be unstable and always denatured until 1997, when Frank S. & Lisa Sagliano used freeze-drying of liquid chlorophyll at the University of Florida and stabilized it as a powder, preserving it for future use.
Alternative medicine.
Many claims are made about the healing properties of chlorophyll, but most have been disproved or are exaggerated by the companies that are marketing them. Quackwatch, a website dedicated to debunking false medical claims, has a quote from Toledo Blade (1952) which claims "Chlorophyll Held Useless As Body Deodorant", but later has John C. Kephart pointing out "No deodorant effect can possibly occur from the quantities of chlorophyll put in products such as gum, foot powder, cough drops, etc. To be effective, large doses must be given internally".

</doc>
<doc id="6986" url="http://en.wikipedia.org/wiki?curid=6986" title="Carotene">
Carotene

The term carotene (also carotin, from the Latin "carota", "carrot") is used for several related unsaturated hydrocarbon substances having the formula C40Hx, which are synthesized by plants but cannot be made by animals. Carotene is an orange photosynthetic pigment important for photosynthesis. Carotenes are all coloured to the human eye. They are responsible for the orange colour of the carrot, for which this class of chemicals is named, and for the colours of many other fruits and vegetables (for example, sweet potatoes, chanterelle and orange cantaloupe melon). Carotenes are also responsible for the orange (but not all of the yellow) colours in dry foliage. They also (in lower concentrations) impart the yellow coloration to milk-fat and butter. Omnivorous animal species which are relatively poor converters of coloured dietary carotenoids to colourless retinoids have yellowed-coloured body fat, as a result of the carotenoid retention from the vegetable portion of their diet. The typical yellow-coloured fat of humans and chickens is a result of fat storage of carotenes from their diets.
Carotenes contribute to photosynthesis by transmitting the light energy they absorb to chlorophyll. They also protect plant tissues by helping to absorb the energy from singlet oxygen, an excited form of the oxygen molecule O2 which is formed during photosynthesis.
β-Carotene is composed of two retinyl groups, and is broken down in the mucosa of the human small intestine by β-carotene 15,15'-monooxygenase to retinal, a form of vitamin A. β-Carotene can be stored in the liver and body fat and converted to retinal as needed, thus making it a form of vitamin A for humans and some other mammals. The carotenes α-carotene and γ-carotene, due to their single retinyl group (β-ionone ring), also have some vitamin A activity (though less than β-carotene), as does the xanthophyll carotenoid β-cryptoxanthin. All other carotenoids, including lycopene, have no beta-ring and thus no vitamin A activity (although they may have antioxidant activity and thus biological activity in other ways).
Animal species differ greatly in their ability to convert retinyl (beta-ionone) containing carotenoids to retinals. Carnivores in general are poor converters of dietary ionone-containing carotenoids. Pure carnivores such as ferrets lack β-carotene 15,15'-monooxygenase and cannot convert any carotenoids to retinals at all (resulting in carotenes not being a form of vitamin A for this species); while cats can convert a trace of β-carotene to retinol, although the amount is totally insufficient for meeting their daily retinol needs.
Molecular structure.
Chemically, carotenes are polyunsaturated hydrocarbons containing 40 carbon atoms per molecule, variable numbers of hydrogen atoms, and no other elements. Some carotenes are terminated by hydrocarbon rings, on one or both ends of the molecule. All are coloured to the human eye, due to extensive systems of conjugated double bonds. Structurally carotenes are tetraterpenes, meaning that they are synthesized biochemically from four 10-carbon terpene units, which in turn are formed from eight 5-carbon isoprene units.
Carotenes are found in plants in two primary forms designated by characters from the Greek alphabet: alpha-carotene (α-carotene) and beta-carotene (β-carotene). Gamma-, delta-, epsilon-, and zeta-carotene (γ, δ, ε, and ζ-carotene) also exist. Since they are hydrocarbons, and therefore contain no oxygen, carotenes are fat-soluble and insoluble in water (in contrast with other carotenoids, the xanthophylls, which contain oxygen and thus are less chemically hydrophobic).
Dietary sources.
The following foods are particularly rich in carotenes (also see Vitamin A article for amounts):
Absorption from these foods is enhanced if eaten with fats, as carotenes are fat soluble, and if the food is cooked for a few minutes until the plant cell wall splits and the colour is released into any liquid. 6 μg of dietary β-carotene supplies the equivalent of 1 μg of retinol, or 1 RE (Retinol Equivalent). This is equivalent to 3⅓ IU of vitamin A.
The multiple forms.
The two primary isomers of carotene, α-carotene and β-carotene, differ in the position of a double bond (and thus a hydrogen) in the cyclic group at one end (left in the diagram here).
β-Carotene is the more common form and can be found in yellow, orange, and green leafy fruits and vegetables. As a rule of thumb, the greater the intensity of the orange colour of the fruit or vegetable, the more β-carotene it contains.
Carotene protects plant cells against the destructive effects of ultraviolet light. β-Carotene is an antioxidant.
β-Carotene and cancer.
It has been shown in trials that the ingestion of β-carotene supplements at about 30 mg/day (10 times the Reference Daily Intake) increases the rate of lung and prostate cancer development in smokers and people with a history of asbestos exposure.
An article on the American Cancer Society says that The Cancer Research Campaign has called for warning labels on β-carotene supplements to caution smokers that such supplements may increase the risk of lung cancer.
The New England Journal of Medicine published an article in 1994 about a trial which examined the relationship between daily supplementation of β-carotene and vitamin E (α-tocopherol) and the incidence of lung cancer. The study was done using supplements and researchers were aware of the epidemiological correlation between carotenoid-rich fruits and vegetables and lower lung cancer rates. The research concluded that no reduction in lung cancer was found in the participants using these supplements, and furthermore, these supplements may, in fact, have harmful effects.
The Journal of the National Cancer Institute and The New England Journal of Medicine published articles in 1996 about a trial that was conducted to determine if vitamin A (in the form of retinyl palmitate) and β-carotene had any beneficial effects to prevent cancer. The results indicated an increased risk of lung cancer for the participants who consumed the β-carotene supplement and who had lung irritation from smoking or asbestos exposure, causing the trial to be stopped early.
A review of all randomized controlled trials in the scientific literature by the Cochrane Collaboration published in "JAMA" in 2007 found that synthetic β-carotene "increased" mortality by something between 1 and 8% (Relative Risk 1.05, 95% confidence interval 1.01–1.08). However, this meta-analysis included two large studies of smokers, so it is not clear that the results apply to the general population. The review only studied the influence of synthetic antioxidants and the results should not be translated to potential effects of fruits and vegetables.
β-Carotene and cognition.
A recent report demonstrated that 50 mg of β-carotene every other day prevented cognitive decline in a study of over 4000 physicians at a mean treatment duration of 18 years.
β-Carotene and photosensitivity.
Oral β-carotene is prescribed to people suffering from erythropoietic protoporphyria. It provides them some relief from photosensitivity.
β-Carotene and nanotechnology.
β-Carotene and lycopene molecules can be encapsulated into carbon nanotubes enhancing the optical properties of carbon nanotubes. Efficient energy transfer occurs between the encapsulated dye and nanotube — light is absorbed by the dye and without significant loss is transferred to the single wall carbon nanotube (SWCNT). Encapsulation increases chemical and thermal stability of carotene molecules; it also allows their isolation and individual characterization.
Carotenemia.
Carotenemia or hypercarotenemia is excess carotene, but unlike excess vitamin A, carotene is non-toxic. Although hypercarotenemia is not particularly dangerous, it can lead to an oranging of the skin (carotenodermia), but not the conjunctiva of eyes (thus easily distinguishing it visually from jaundice). It is most commonly associated with consumption of an abundance of carrots, but it also can be a medical sign of more dangerous conditions.
Production.
Most of the world's synthetic supply of carotene comes from a manufacturing complex located in Freeport, Texas and owned by DSM. The other major supplier BASF also uses a chemical process to produce β-carotene. Together these suppliers account for about 85% of the β-carotene on the market. In Spain Vitatene produces natural β-carotene from fungus Blakeslea trispora, as does DSM but at much lower amount when compared to its synthetic β-carotene operation. In Australia, organic β-carotene is produced by Aquacarotene Limited from dried marine algae "Dunaliella salina" grown in harvesting ponds situated in Karratha, Western Australia. BASF Australia is also producing β-carotene from microalgae grown in two sites in Australia that are the world’s largest algae farms. In Portugal, the industrial biotechnology company Biotrend is producing natural all-"trans"-β-carotene from a non genetically modified bacteria of the "Sphingomonas" genus isolated from soil.
Carotenes are also found in palm oil, corn, and in the milk of dairy cows, causing cow's milk to be light yellow, depending on the feed of the cattle, and the amount of fat in the milk (high-fat milks, such as those produced by Guernsey cows, tend to be yellower because their fat content causes them to contain more carotene).
Carotenes are also found in some species of termites, where they apparently have been picked up from the diet of the insects.
Total synthesis.
There are currently two commonly used methods of total synthesis of β-carotene. The first was developed by the Badische Anilin- & Soda-Fabrik (BASF) and is based on the Wittig reaction with Wittig himself as patent holder:
The second is a Grignard reaction, elaborated by Hoffman-La Roche from the original synthesis of Inhoffen et al. They are both symmetrical; the BASF synthesis is C20 + C20, and the Hoffman-La Roche synthesis is C19 + C2 + C19.
Nomenclature.
Carotenes are carotenoids containing no oxygen. Carotenoids containing some oxygen are known as xanthophylls.
The two ends of the β-carotene molecule are structurally identical, and are called β-rings. Specifically, the group of nine carbon atoms at each end form a β-ring.
The α-carotene molecule has a β-ring at one end; the other end is called an ε-ring. There is no such thing as an "α-ring".
These and similar names for the ends of the carotenoid molecules form the basis of a systematic naming scheme, according to which:
ζ-Carotene is the biosynthetic precursor of neurosporene, which is the precursor of lycopene, which, in turn, is the precursor of the carotenes α through ε.
Food additive.
Carotene is also used as a substance to colour products such as juice, cakes, desserts, butter and margarine. It is approved for use as a food additive in the EU (listed as additive E160a) Australia and New Zealand (listed as 160a) and the USA.

</doc>
<doc id="6988" url="http://en.wikipedia.org/wiki?curid=6988" title="Cyclic adenosine monophosphate">
Cyclic adenosine monophosphate

Cyclic adenosine monophosphate (cAMP, cyclic AMP, or 3'-5'-cyclic adenosine monophosphate) is a second messenger important in many biological processes. cAMP is derived from adenosine triphosphate (ATP) and used for intracellular signal transduction in many different organisms, conveying the cAMP-dependent pathway.
History.
Earl Sutherland of Case Western Reserve University won a Nobel Prize in Physiology or Medicine in 1971 "for his discoveries concerning the mechanisms of the action of hormones," especially epinephrine, via second messengers (such as cyclic adenosine monophosphate, cyclic AMP).
Synthesis and decomposition.
cAMP is synthesized from ATP by adenylyl cyclase located on the inner side of the plasma membrane and anchored at various locations in the interior of the cell. Adenylyl Cyclase is activated by a range of signaling molecules through the activation of adenylyl cyclase stimulatory G (Gs)-protein-coupled receptors and inhibited by agonists of adenylyl cyclase inhibitory G (Gi)-protein-coupled receptors. Liver adenylyl cyclase responds more strongly to glucagon, and muscle adenylyl cyclase responds more strongly to adrenaline.
cAMP decomposition into AMP is catalyzed by the enzyme phosphodiesterase.
Functions.
cAMP is a second messenger, used for intracellular signal transduction, such as transferring into cells the effects of hormones like glucagon and adrenaline, which cannot pass through the plasma membrane. It is involved in the activation of protein kinases and regulates the effects of adrenaline and glucagon. cAMP also binds to and regulates the function of ion channels such as the HCN channels and a few other cyclic nucleotide-binding proteins such as Epac1 and RAPGEF2.
Role of cAMP in eukaryotic cells.
cAMP and its associated kinases function in several biochemical processes, including the regulation of glycogen, sugar, and lipid metabolism.
In eukaryotes, cyclic AMP works by activating protein kinase A (PKA, or cAMP-dependent protein kinase). PKA is normally inactive as a tetrameric holoenzyme, consisting of two catalytic and two regulatory units (C2R2), with the regulatory units blocking the catalytic centers of the catalytic units. Cyclic AMP binds to specific locations on the regulatory units of the protein kinase, and causes dissociation between the regulatory and catalytic subunits, thus activating the catalytic units and enabling them to phosphorylate substrate proteins.
The active subunits catalyze the transfer of phosphate from ATP to specific serine or threonine residues of protein substrates. The phosphorylated proteins may act directly on the cell's ion channels, or may become activated or inhibited enzymes. Protein kinase A can also phosphorylate specific proteins that bind to promoter regions of DNA, causing increased expression of specific genes. Not all protein kinases respond to cAMP. Several classes of protein kinases, including protein kinase C, are not cAMP-dependent.
Further effects mainly depend on cAMP-dependent protein kinase, which vary based on the type of cell.
Still, there are some minor PKA-independent functions of cAMP, e.g., activation of calcium channels, providing a minor pathway by which growth hormone-releasing hormone causes a release of growth hormone.
However, the view that the majority of the effects of cAMP are controlled by PKA is an outdated one. In 1998 a family of cAMP-sensitive proteins with guanine nucleotide exchange factor (GEF) activity was discovered. These are termed Exchange proteins activated by cAMP (Epac) and the family comprises Epac1 and Epac2 . The mechanism of activation is similar to that of PKA: the GEF domain is usually masked by the N-terminal region containing the cAMP binding domain. When cAMP binds, the domain dissociates and exposes the now-active GEF domain, allowing Epac to activate small Ras-like GTPase proteins, such as Rap1.
Additional role of secreted cAMP in social amoebas.
In the species "Dictyostelium discoideum", cAMP acts outside the cell as a secreted signal. The chemotactic aggregation of cells is organized by periodic waves of cAMP that propagate between cells over distances as large as several centimetres. The waves are the result of a regulated production and secretion of extracellular cAMP and a spontaneous biological oscillator that initiates the waves at centers of territories.
Role of cAMP in bacteria.
In bacteria, the level of cAMP varies depending on the medium used for growth. In particular, cAMP is low when glucose is the carbon source. This occurs through inhibition of the cAMP-producing enzyme, adenylate cyclase, as a side-effect of glucose transport into the cell. The transcription factor cAMP receptor protein (CRP) also called CAP (catabolite gene activator protein) forms a complex with cAMP and thereby is activated to bind to DNA. CRP-cAMP increases expression of a large number of genes, including some encoding enzymes that can supply energy independent of glucose.
cAMP, for example, is involved in the positive regulation of the lac operon. In an environment of a low glucose concentration, cAMP accumulates and binds to the allosteric site on CRP (cAMP receptor protein), a transcription activator protein. The protein assumes its active shape and binds to a specific site upstream of the lac promoter, making it easier for RNA polymerase to bind to the adjacent promoter to start transcription of the lac operon, increasing the rate of lac operon transcription. With a high glucose concentration, the cAMP concentration decreases, and the CRP disengages from the lac operon.
Pathology.
Role of cAMP in human carcinoma.
Some research has suggested that a deregulation of cAMP pathways and an aberrant activation of cAMP-controlled genes is linked to the growth of some cancers.
Role of cAMP in prefrontal cortex disorders.
Recent research suggests that cAMP affects the function of higher-order thinking in the prefrontal cortex through its regulation of ion channels called hyperpolarization-activated cyclic nucleotide-gated channels (HCN). When cAMP stimulates the HCN, the channels open, closing the brain cell to communication and thus interfering with the function of the prefrontal cortex. This research, especially the cognitive deficits in age-related illnesses and ADHD, is of interest to researchers studying the brain.

</doc>
<doc id="6991" url="http://en.wikipedia.org/wiki?curid=6991" title="Cimabue">
Cimabue

Cimabue (; 1240 – 1302), (also known as Cenni Di Pepi or in modern Italian, Benvenuto di Giuseppe) was a Florentine painter and creator of mosaics.
Cimabue is generally regarded as one of the first great Italian painters to break from the Italo-Byzantine style, although he still relied on Byzantine models. The art of this period comprised scenes and forms that appeared relatively flat and highly stylized. Cimabue was a pioneer in the move towards naturalism; his figures were depicted with more lifelike proportions and shading. Even though he was a pioneer in that move, his "Maestà" paintings evidence Medieval techniques and characteristics. According to Giorgio Vasari, he was the teacher of Giotto, the first great artist of the Italian Renaissance.
Life.
Owing to little surviving documentation, not much is known about Cimabue's life. He was born in Florence and died in Pisa. His career was described in Giorgio Vasari's "Lives of the Most Excellent Painters, Sculptors, and Architects". Although it is one of the few early records about him, its accuracy is uncertain.
He perhaps trained in Florence under unknown masters culturally connected to Byzantine art. His first attributed work, the "Crucifixion" in the church of San Domenico in Arezzo (assigned to him by Italian art historian Pietro Toesca and dated to around 1270), departed from the Byzantine style. His style was at the time more reminiscent of works such as the "Christus patiens" (c. 1250) by Giunta Pisano, although Cimabue's Christ is more bent and the clothes have the golden striations introduced by Coppo di Marcovaldo.
Around 1272 he is documented in Rome. A little later, he made another "Crucifixion" for the Florentine church of Santa Croce (incidentally: damaged by the 1966 Arno River flood). This is a larger and more evoluted work than that in Arezzo, with traces of naturalism perhaps inspired by Nicola Pisano's works. In the same period (c. 1280) he painted the "Maestà" now at the Louvre Museum, originally in the church of San Francesco at Pisa. This work established a style which was followed by numerous artists after him, including Duccio di Buoninsegna in his "Rucellai Madonna" (once wrongly attributed to Cimabue), as well as Giotto himself. Other works dating to this period, in which the influence of his pupil Giotto becomes manifest, include a "Flagellation" (Frick Collection), mosaics for the Baptistery of Florence (now largely restored), the "Maestà" at the Santa Maria dei Servi in Bologna and the "Madonna" in the Pinacoteca of Castelfiorentino. A workshop painting, perhaps assignable to a slightly later period, is the "Maestà with Saints Francis and Dominic" now at the Uffizi.
During the pontificate of Pope Nicholas IV, the first Franciscan pope, Cimabue worked at Assisi. His call was perhaps due to the fame he gained in Rome in 1272, although no works from his stay there are known. At Assisi, in the transept of the Lower Basilica of San Francesco, he frescoed a "Madonna with Child Enthroned, Four Angels and St. Francis"; the left part of the work is missing, and perhaps showed St. Antony of Padua. The authorship of the painting has been recently disputed for technical and stylistic reasons, however. Cimabue was subsequently commissioned the decoration of the apse and the transept of the Upper Basilica of Assisi, in the same period in which Roman artists were frescoing the nave. The cycle comprises scenes from the Gospels, the life of Mary and of St. Peter and St. Paul, and is today in poor condition due to the oxidation of the brighter colors.
The "Maestà of Santa Trinita", originally painted for the church of Santa Trinita in Florence dates to c. 1290–1300. It is now at the Uffizi Gallery. The softer expression of the characters suggests that it was influenced by Giotto, who was by then already active as a solo artist. Cimabue spent the period from 1301 to 1302 in Pisa, where, together with collaborators, he executed the apse mosaic for the city's cathedral. He died in 1302.
Character.
"Cimabue of Florence was a painter who lived during the author's own time, a nobler man than anyone knew but he was as a result so haughty and proud that if someone pointed out to him any mistake or defect in his work, or if he had noted any himself...he would immediately destroy the work, no matter how precious it might be." 
Legacy.
History has long regarded Cimabue as the last of an era that was overshadowed by the Italian Renaissance. As early as 1543, the historian Vasari wrote of Cimabue, "Cimabue was, in one sense, the principle cause of the renewal of painting," with the qualification that, "Giotto truly eclipsed Cimabue's fame just as a great light eclipses a much smaller one."
In Canto XI of his Purgatorio, Dante laments Cimabue's quick loss of public interest in the face of Giotto's revolution in art:
O vanity of human powers,
how briefly lasts the crowning green of glory,
unless an age of darkness follows!
In painting Cimabue thought he held the field
but now it's Giotto has the cry,
so that the other's fame is dimmed.

</doc>
<doc id="6997" url="http://en.wikipedia.org/wiki?curid=6997" title="Corporatocracy">
Corporatocracy

Corporatocracy , is a term used as an economic and political system controlled by corporations or corporate interests. It is a generally pejorative term often used by critics of the current economic situation in a particular country, especially the United States. This is different from corporatism, which is the organisation of society into groups with common interests. Corporatocracy as a term tends to be used by liberal and left-leaning critics, but also some economic libertarian critics and other political observers across the political spectrum. Economist Jeffrey Sachs described the United States as a corporatocracy in his book "The Price of Civilization". He suggested that it arose from four trends: weak national parties and strong political representation of individual districts, the large U.S. military establishment after World War II, big corporate money financing election campaigns, and globalization tilting the balance away from workers.
This collective is what author C Wright Mills called the Power Elite, wealthy individuals who hold prominent positions in corporatocracies. They control the process of determining a society's economic and political policies.
The concept has been used in explanations of bank bailouts, excessive pay for CEOs, as well as complaints such as the exploitation of national treasuries, people, and natural resources. It has been used by critics of globalization, sometimes in conjunction with criticism of the World Bank or unfair lending practices, as well as criticism of "free trade agreements".
Historical corporatocracies.
Corporations have held the right to vote in some jurisdictions. For example, Livery Companies currently appoint most of the voters for the City of London Corporation, which is the municipal government for the area centered on the financial district.

</doc>
<doc id="6999" url="http://en.wikipedia.org/wiki?curid=6999" title="Culture of Canada">
Culture of Canada

Canadian culture is a term that embodies the artistic, culinary, literary, humour, musical, political and social elements that are representative of Canada and Canadians. Throughout Canada's history, its culture has been influenced by European culture and traditions, especially British and French, and by its own indigenous cultures. Over time, elements of the cultures of Canada's immigrant populations have become incorporated into mainstream Canadian culture. The population has also been influenced by American culture because of a shared language, proximity and migration between the two countries.
Canada is often characterised as being "very progressive, diverse, and multicultural". Canada's culture draws influences from its broad range of constituent nationalities, and policies that promote a just society are constitutionally protected. Canadian Government policies – such as publicly funded health care; higher and more progressive taxation; outlawing capital punishment; strong efforts to eliminate poverty; an emphasis on cultural diversity; strict gun control; and most recently, legalizing same-sex marriage – are social indicators of Canada's political and cultural values.
Canada's federal government has influenced Canadian culture with programs, laws and institutions. It has created crown corporations to promote Canadian culture through media, such as the Canadian Broadcasting Corporation (CBC) and the National Film Board of Canada (NFB), and promotes many events which it considers to promote Canadian traditions. It has also tried to protect Canadian culture by setting legal minimums on Canadian content in many media using bodies like the Canadian Radio-television and Telecommunications Commission (CRTC).
Development of Canadian culture.
Historical influences.
For tens of thousands of years, Canada was inhabited by Aboriginal peoples from a variety of different cultures and of several major linguistic groupings. Although not without conflict and bloodshed, early European interactions with First Nations and Inuit populations in what is now Canada were arguably peaceful. First Nations and Métis peoples played a critical part in the development of European colonies in Canada, particularly for their role in assisting European coureur des bois and voyageurs in the exploration of the continent during the fur trade. Combined with late economic development in many regions, this comparably nonbelligerent early history allowed Aboriginal Canadians to have a lasting influence on the national culture (see: The Canadian Crown and Aboriginal peoples). Over the course of three centuries, countless North American Indigenous words, inventions, concepts, and games have become an everyday part of Canadian language and use. Many places in Canada, both natural features and human habitations, use indigenous names. The name "Canada" itself derives from the St. Lawrence Iroquoian word meaning "village" or "settlement". The name of Canada's capital city Ottawa comes from the Algonquin language term "adawe" meaning "to trade".
The French originally settled New France along the shores of the Atlantic Ocean and Saint Lawrence River during the early part of the 17th century. Themes and symbols of pioneers, trappers, and traders played an important part in the early development of French Canadian culture. The British conquest of New France and subsequent immigration, during the mid-18th century brought the large Francophone population under British rule, creating a need for compromise and accommodation. The migration of 46,000 United Empire Loyalists from the Thirteen Colonies during the American Revolution (1775–1783) brought American colonial influences. The Canadian Forces and overall civilian participation in the First World War and Second World War helped to foster Canadian nationalism, however in 1917 and 1944 conscription crisis's highlighted the considerable rift along ethnic lines between Anglophones and Francophones. As a result of the First and Second World Wars, the Government of Canada became more assertive and less deferential to British authority.
Canada until the 1940s saw itself in terms of English and French cultural, linguistic and political identities, and to some extent aboriginal. Legislative restrictions on immigration (such as the Continuous journey regulation and Chinese Immigration Act) that had favoured British, American and other European immigrants (such as Dutch, German, Italian, Polish, Swedish and Ukrainian) were amended during the 1960s, resulting in an influx of diverse people from Asia, Africa, and the Caribbean. By the end of the 20th century, immigrants were increasingly Chinese, Indian, Vietnamese, Jamaican, Filipino, Lebanese and Haitian. As of 2006, Canada has grown to have thirty four ethnic groups with at least one hundred thousand members each, of which eleven have over 1,000,000 people and numerous others are represented in smaller numbers. 16.2% of the population self identify as a visible minority.
Canada has also evolved to be religiously and linguistically diverse, encompassing a wide range of dialects, beliefs and customs. The 2011 Canadian census reported a population count of 33,121,175 individuals of whom 67.3% identify as being Christians; of these, Catholics make up the largest group, accounting for 38.7 percent of the population. The largest Protestant denomination is the United Church of Canada (accounting for 6.1% of Canadians), followed by Anglicans (5.0%), and Baptists (1.9%). About 23.9% of Canadians declare no religious affiliation, including agnostics, atheists, humanists, and other groups. The remaining are affiliated with non-Christian religions, the largest of which is Islam (3.2%), followed by Hinduism (1.5%), Sikhism (1.4%) Buddhism (1.1%) and Judaism (1.0%). English and French are the first languages of approximately 60% and 20% of the population; however in 2011, nearly 6.8 million Canadians listed a non-official language as their mother tongue. Some of the most common non-official first languages include Chinese (mainly Cantonese with 1,072,555 first-language speakers); Punjabi (430,705); Spanish (410,670); German (409,200); and Italian (407,490).
Evolution of legislation.
French Canada's early development was relatively cohesive during the 17th and 18th centuries, and this was preserved by the Quebec Act of 1774, which allowed Roman Catholics to hold offices and practice their faith. In 1867, the Constitution Act was thought to meet the growing calls for Canadian autonomy while avoiding the overly strong decentralization that contributed to the Civil War in the United States. The compromises reached during this time between the English- and French-speaking Fathers of Confederation set Canada on a path to bilingualism which in turn contributed to an acceptance of diversity. The English and French languages have had limited constitutional protection since 1867 and full official status since 1969. Section 133 of the Constitution Act of 1867 (BNA Act) guarantees that both languages may be used in the Parliament of Canada. Canada adopted its first Official Languages Act in 1969, giving English and French equal status in the government of Canada. Doing so makes them "official" languages, having preferred status in law over all other languages used in Canada.
Prior to the advent of the Canadian Bill of Rights in 1960 and its successor the Canadian Charter of Rights and Freedoms in 1982, the laws of Canada did not provide much in the way of civil rights and this issue was typically of limited concern to the courts. Canada since the 1960s has placed emphasis on equality and inclusiveness for all people. For example, in 1995, the Supreme Court of Canada ruled in "Egan v. Canada" that sexual orientation should be "read in" to Section Fifteen of the Canadian Charter of Rights and Freedoms, a part of the Constitution of Canada guaranteeing equal rights to all Canadians. Following a series of decisions by provincial courts and the Supreme Court of Canada, on July 20, 2005, the Civil Marriage Act (Bill C-38) received Royal Assent, legalizing same-sex marriage in Canada. Canada thus became the fourth country to officially sanction same-sex marriage worldwide, after The Netherlands, Belgium, and Spain. Furthermore, sexual orientation was included as a protected status in the human-rights laws of the federal government and of all provinces and territories.
Today, Canada has a diverse makeup of ethnicities and nationalities and constitutional protection for policies that promote multiculturalism rather than cultural assimilation or a single national myth. In Quebec, cultural identity is strong, and many French-speaking commentators speak of a Quebec culture as distinguished from English Canadian culture. However as a whole, Canada is in theory, a cultural mosaic – a collection of several regional, aboriginal, and ethnic subcultures. Celtic influences have allowed survival of non-English dialects in Nova Scotia and Newfoundland; Canada's Pacific trade has also brought a large Chinese influence into British Columbia and other areas. Multiculturalism in Canada was adopted as the official policy of the Canadian government during the prime ministership of Pierre Trudeau, and is enshrined in Section 27 of the Canadian Charter of Rights and Freedoms. In parts of Canada, especially the major cities of Montreal, Vancouver, Ottawa and Toronto, multiculturalism itself is the cultural norm in many urban communities.
Identity.
Canada's large geographic size, the presence of a significant number of indigenous peoples, the conquest of one European linguistic population by another and relatively open immigration policy have led to an extremely diverse society. As a result the issue of Canadian identity remains under scrutiny, perhaps more than the identity of the people of any other modern nation. Journalist and professor Andrew Cohen wrote in 2007:
The question of Canadian identity was traditionally dominated by three fundamental themes: first, the often conflicted relations between English Canadians and French Canadians stemming from the French Canadian imperative for cultural and linguistic survival; secondly, the generally close ties between English Canadians and the British Empire, resulting in a gradual political process towards complete independence from the imperial power; and finally, the close proximity of English-speaking Canadians to the United States. In the 20th century, immigrants from African, Caribbean and Asian nationalities have shaped the Canadian identity, a process that continues today with the ongoing arrival of large numbers of immigrants from non-British or non-French backgrounds, adding the theme of multiculturalism to the debate. Much of the debate over contemporary Canadian identity is argued in political terms, and defines Canada as a country defined by its government policies, which are thought to reflect deeper cultural values.
Nationalism and protectionism.
In general, Canadian nationalists are highly concerned about the protection of Canadian sovereignty and loyalty to the Canadian State, placing them in the civic nationalist category. It has likewise often been suggested that anti-Americanism plays a prominent role in Canadian nationalist ideologies. A unified, bi-cultural, tolerant and sovereign Canada remains an ideological inspiration to many Canadian nationalists. Alternatively French Canadian nationalism and support for maintaining French Canadian culture would inspire Quebec nationalists, many of whom were supporters of the Quebec sovereignty movement during the late-20th century.
Cultural protectionism in Canada has, since the mid-20th century, taken the form of conscious, interventionist attempts on the part of various Canadian governments to promote Canadian cultural production. Sharing a large border and (for the majority) a common language with the United States, Canada faces a difficult position in regard to American culture, be it direct attempts at the Canadian market or the general diffusion of American culture in the globalized media arena. While Canada tries to maintain its cultural differences, it also must balance this with responsibility in trade arrangements such as the General Agreement on Tariffs and Trade (GATT) and the North American Free Trade Agreement (NAFTA).
Symbols.
Official symbols of Canada include the maple leaf, beaver, and the Canadian Horse. Many official symbols of the country such as the Flag of Canada have been changed or modified over the past few decades in order to 'Canadianize' them and de-emphasise or remove references to the United Kingdom. Other prominent symbols include the Canada goose, loon and more recently, the totem pole and Inuksuk. Symbols of the monarchy in Canada continue to be featured in, for example, the Arms of Canada and armed forces Her Majesty's Canadian Ship. The designation 'Royal' remains for institutions as varied as the Royal Canadian Mounted Police and the Royal Winnipeg Ballet. During unification of the forces in the 1960s, a renaming of the branches took place, resulting in the abandonment of "royal designations" of the navy and air force. On August 16, 2011, the Government of Canada announced that the name "Air Command" was re-assuming the air force's original historic name, Royal Canadian Air Force; "Land Command" was re-assuming the name Canadian Army; and "Maritime Command" was re-assuming the name Royal Canadian Navy. These name changes were made to better reflect Canada's military heritage and align Canada with other key Commonwealth of Nations whose militaries use the royal designation.
Humour.
Canadian humour is an integral part of the Canadian Identity. There are several traditions in Canadian humour in both English and French. While these traditions are distinct and at times very different, there are common themes that relate to Canadians' shared history and geopolitical situation in the Western Hemisphere and the world. Various trends can be noted in Canadian comedy. One trend is the portrayal of a "typical" Canadian family in an on-going radio or television series. Other trends include outright absurdity, and political and cultural satire. Satire and self-deprecation are arguably the primary characteristics of Canadian humour.
The beginnings of Canadian radio comedy date to the late 1930s with the debut of The Happy Gang, a long-running weekly variety show that was regularly sprinkled with corny jokes in between tunes. Canadian television comedy begins with Wayne and Shuster, a sketch comedy duo who performed as a comedy team during the Second World War, and moved their act to radio in 1946 before moving on to television. Second City Television, otherwise known as SCTV, Royal Canadian Air Farce, This Hour Has 22 Minutes, The Kids in the Hall and more recently Trailer Park Boys are regarded as television shows which were very influential on the development of Canadian humour. Canadian comedians have had great success in the film industry and are amongst the most recognized in the world.
Humber College in Toronto and the École nationale de l'humour in Montreal offer post-secondary programmes in comedy writing and performance. Montreal is also home to the bilingual (English and French) Just for Laughs festival and to the Just for Laughs Museum, a bilingual, international museum of comedy. Canada has a national television channel, The Comedy Network, devoted to comedy. Many Canadian cities feature comedy clubs and showcases, most notable, The Second City branch in Toronto (originally housed at The Old Fire Hall) and the Yuk Yuk's national chain. The Canadian Comedy Awards were founded in 1999 by the Canadian Comedy Foundation for Excellence, a not-for-profit organization.
Arts.
Visual arts.
Aboriginal artists were producing art in the territory that is now called Canada for thousands of years prior to the arrival of European settler colonists and the eventual establishment of Canada as a nation state. Like the peoples that produced them, indigenous art traditions spanned territories that extended across the current national boundaries between Canada and the United States. The majority of indigenous artworks preserved in museum collections date from the period after European contact and show evidence of the creative adoption and adaptation of European trade goods such as metal and glass beads. Canadian sculpture has been enriched by the walrus ivory, muskox horn and caribou antler and soapstone carvings by the Inuit artists. These carvings show objects and activities from the daily life, myths and legends of the Inuit. Inuit art since the 1950s has been the traditional gift given to foreign dignitaries by the Canadian government.
The works of most early Canadian painters followed European trends. During the mid-19th century, Cornelius Krieghoff, a Dutch-born artist in Quebec, painted scenes of the life of the "habitants" (French-Canadian farmers). At about the same time, the Canadian artist Paul Kane painted pictures of aboriginal life in western Canada. A group of landscape painters called the Group of Seven developed the first distinctly Canadian style of painting. All these artists painted large, brilliantly coloured scenes of the Canadian wilderness.
Since the 1930s, Canadian painters have developed a wide range of highly individual styles. Emily Carr became famous for her paintings of totem poles in British Columbia. Other noted painters have included the landscape artist David Milne, the abstract painters Jean-Paul Riopelle and Harold Town and multi-media artist Michael Snow. The abstract art group Painters Eleven, particularly the artists William Ronald and Jack Bush, also had an important impact on modern art in Canada. Government support has played a vital role in their development enabling visual exposure through publications and periodicals featuring Canadian art, as has the establishment of numerous art schools and colleges across the country.
Literature.
Canadian literature is often divided into French- and English-language literatures, which are rooted in the literary traditions of France and Britain, respectively. Canada’s early literature, whether written in English or French, often reflects the Canadian perspective on nature, frontier life, and Canada’s position in the world, for example the poetry of Bliss Carman or the memoirs of Susanna Moodie and Catherine Parr Traill. These themes, and Canada's literary history, inform the writing of successive generations of Canadian authors, from Leonard Cohen to Margaret Atwood.
By the mid-20th century, Canadian writers were exploring national themes for Canadian readers. Authors were trying to find a distinctly Canadian voice, rather than merely emulating British or American writers. Canadian identity is closely tied to its literature. The question of national identity recurs as a theme in much of Canada's literature, from Hugh MacLennan's "Two Solitudes" (1945) to Alistair MacLeod's "No Great Mischief" (1999). Canadian literature is often categorised by region or province; by the socio-cultural origins of the author (for example, Acadians, Aboriginal peoples, LGBT, and Irish Canadians); and by literary period, such as "Canadian postmoderns" or "Canadian Poets Between the Wars."
Canadian authors have accumulated numerous international awards. In 1992, Michael Ondaatje became the first Canadian to win the Man Booker Prize for "The English Patient". Margaret Atwood won the Booker in 2000 for "The Blind Assassin" and Yann Martel won it in 2002 for the "Life of Pi". Carol Shields's "The Stone Diaries" won the Governor General's Awards in Canada in 1993, the 1995 Pulitzer Prize for Fiction, and the 1994 National Book Critics Circle Award. In 2013, Alice Munro was the first Canadian to be awarded the Nobel Prize in Literature for her work as "master of the modern short story". Munro is also a recipient of the Man Booker International Prize for her lifetime body of work, and three-time winner of Canada's Governor General's Award for fiction.
Theatre.
Canada has had a thriving stage theatre scene since the late 1800s. Theatre festivals draw many tourists in the summer months, especially the Stratford Shakespeare Festival in Stratford, Ontario, and the Shaw Festival in Niagara-on-the-Lake, Ontario. The Famous People Players are only one of many touring companies that have also developed an international reputation. Canada also hosts one of the largest fringe festival the Edmonton International Fringe Festival.
Canada's largest cities host a variety of modern and historical venues. The Toronto Theatre District is Canada's largest, as well as being the third largest English-speaking theatre district in the world. In addition to original Canadian works, shows from the West End and Broadway frequently tour in Toronto. Toronto's Theatre District includes the venerable Roy Thomson Hall; the Princess of Wales Theatre; the Tim Sims Playhouse; The Second City; the Canon Theatre; the Panasonic Theatre; the Royal Alexandra Theatre; historic Massey Hall; and the city's new opera house, the Sony Centre for the Performing Arts. Toronto's Theatre District also includes the Theatre Museum Canada.
Montreal's theatre district ("Quartier des Spectacles") is the scene of performances that are mainly French-language, although the city also boasts a lively anglophone theatre scene, such as the Centaur Theatre. Large French theatres in the city include Theatre Saint-Denis, Theatre du Nouveau Monde, and EXcentris.
Vancouver is host to, among others, the Vancouver Fringe Festival, the Arts Club Theatre Company, Carousel Theatre, Bard on the Beach, Theatre Under the Stars and Studio 58. It also home of Vancouver Theatresports League, the improvisational theatre company, wold-known for providing an impetus for the present worldwide interest in theatresports at Expo in 1986.
Calgary is home to Theatre Calgary, a mainstream regional theatre; Alberta Theatre Projects, a major centre for new play development in Canada; the Calgary Animated Objects Society; and One Yellow Rabbit, a touring company.
There are three major theatre venues in Ottawa; the Ottawa Little Theatre, originally called the Ottawa Drama League at its inception in 1913, is the longest-running community theatre company in Ottawa. Since 1969, Ottawa has been the home of the National Arts Centre, a major performing-arts venue that houses four stages and is home to the National Arts Centre Orchestra, the Ottawa Symphony Orchestra and Opera Lyra Ottawa. Established in 1975, the Great Canadian Theatre Company specializes in the production of Canadian plays at a local level.
Television.
Canadian television, especially supported by the Canadian Broadcasting Corporation, is the home of a variety of locally produced shows. French-language television, like French Canadian film, is buffered from excessive American influence by the fact of language, and likewise supports a host of home-grown productions. The success of French-language domestic television and movies in Canada often exceeds that of its English-language counterpart. In recent years nationalism has been used to prompt products on television. The "I Am Canadian" campaign by Molson beer, most notably the commercial featuring Joe Canadian, infused domestically brewed beer and nationalism.
Canada's television industry is in full expansion as a site for Hollywood productions. Since the 1980s, Canada, and Vancouver in particular, has become known as Hollywood North. The American TV series "Queer as Folk" was filmed in Toronto. Canadian producers have been very successful in the field of science fiction since the mid-1990s, with such shows as "The X-Files", "Stargate SG-1", the new "Battlestar Galactica", "My Babysitter's A Vampire", "Smallville", and "The Outer Limits", all filmed in Vancouver.
The CRTC's Canadian content regulations dictate that a certain percentage of a domestic broadcaster's transmission time must include content that is produced by Canadians, or covers Canadian subjects. These regulations also apply to US cable television channels such as MTV and the Discovery Channel, which have local versions of their channels available on Canadian cable networks. Similarly, BBC Canada, while showing primarily BBC shows from the United Kingdom, also carries Canadian output.
Film.
A number of Canadian pioneers in early Hollywood significantly contributed to the creation of the motion picture industry in the early days of the 20th century. Over the years, many Canadians have made enormous contributions to the American entertainment industry, although they are frequently not recognized as Canadians.
Canada has developed a vigorous film industry that has produced a variety of well-known films, actors, and auteurs. In fact, this eclipsing may sometimes be creditable for the bizarre and innovative directions of some works, such as auteurs Atom Egoyan ("The Sweet Hereafter", 1997) and David Cronenberg ("The Fly", "Naked Lunch", "A History of Violence"). Also, the distinct French-Canadian society permits the work of directors such as Denys Arcand and Denis Villeneuve. At the 76th Academy Awards, Arcand's "The Barbarian Invasions" became Canada's first film to win the Academy Award for Best Foreign Language Film. James Cameron is a very successful Canadian filmmaker, having been nominated for and receiving many Academy Awards.
The National Film Board of Canada is 'a public agency that produces and distributes films and other audiovisual works which reflect Canada to Canadians and the rest of the world'. Canada has produced many popular documentaries such as "The Corporation", "Nanook of the North", "Final Offer", and "". The Toronto International Film Festival (TIFF) is considered by many to be one of the most prevalent film festivals for Western cinema. It is the première film festival in North America from which the Oscars race begins.
Music.
The Music of Canada has reflected the multi-cultural influences that have shaped the country. Aboriginals, the French, and the British have all made contributions to the musical heritage of Canada. The country has produced its own composers, musicians and ensembles since the mid-1600s. From the 17th century onward, Canada has developed a music infrastructure that includes church halls; chamber halls; conservatories; academies; performing arts centers; record companys; radio stations, and television music-video channels. The music has subsequently been heavily influenced by American culture because of its proximity and migration between the two countries. Canadian rock has had a considerable impact on the development of modern popular music and the development of the most popular sub-genres.
Patriotic music in Canada date back over 200 years as a distinct category from British patriotism, preceding the first legal steps to independence by over 50 years. The earliest, "The Bold Canadian", was written in 1812. The national anthem of Canada, "O Canada" adopted in 1980, was originally commissioned by the Lieutenant Governor of Quebec, the Honourable Théodore Robitaille, for the 1880 St. Jean-Baptiste Day ceremony. Calixa Lavallée wrote the music, which was a setting of a patriotic poem composed by the poet and judge Sir Adolphe-Basile Routhier. The text was originally only in French, before it was translated to English in 1906.
Music broadcasting in the country is regulated by the Canadian Radio-television and Telecommunications Commission (CRTC). The Canadian Academy of Recording Arts and Sciences presents Canada's music industry awards, the Juno Awards, which were first awarded in a ceremony during the summer of 1970.
Video games.
Canada has one of the largest video-game industries in terms of employment numbers, right behind the USA and Japan, with 16,000 employees, 348 companies, and a direct annual economic impact of nearly $2 billion. Canada has grown from a minor player in the video-games industry to a major industry player. In part, this prominence is made possible by a large pool of university-educated talent and a high quality of life, but favourable government policies towards digital media companies also play a role in making Canada an attractive location for game development studios.
Media.
Canada has a well-developed media sector, but its cultural output — particularly in English films, television shows, and magazines — is often overshadowed by imports from the United States. Television, magazines, and newspapers are primarily for-profit corporations based on advertising, subscription, and other sales-related revenues. Nevertheless, both the television broadcasting and publications sectors require a number of government interventions to remain profitable, ranging from regulation that bars foreign companies in the broadcasting industry to tax laws that limit foreign competition in magazine advertising.
The promotion of multicultural media in Canada began in the late 1980s as the multicultural policy was legislated in 1988. In the Multiculturalism Act, the federal government proclaimed the recognition of the diversity of Canadian culture. Thus, multicultural media became an integral part of Canadian media overall. Upon numerous government reports showing lack of minority representation or minority misrepresentation, the Canadian government stressed separate provision be made to allow minorities and ethnicities of Canada to have their own voice in the media.
Sport.
Sports in Canada consists of a variety of games. Although there are many contests that Canadians value, the most common are Ice hockey, Lacrosse, Canadian football, basketball, soccer, curling, baseball and ringette. All but curling and soccer are considered domestic sports as they were either invented by Canadians or trace their roots to Canada.
Ice hockey, referred to as simply "hockey", is Canada's most prevalent winter sport, its most popular spectator sport, and its most successful sport in international competition. It is Canada's official national winter sport. Lacrosse, a sport with indigenous origins, is Canada's oldest and official summer sport. Canadian football is Canada's second most popular spectator sport, and the Canadian Football League's annual championship, the Grey Cup, is the country's largest annual sports event.
While other sports have a larger spectator base, association football, known in Canada as "soccer" in both English and French, has the most registered players of any team sport in Canada, and is the most played sport with all demographics, including ethnic origin, ages and genders. Professional teams exist in many cities in Canada and international soccer competitions such as the FIFA World Cup, UEFA Euro and the UEFA Champions League attract some of the biggest audiences in Canada. Other popular team sports include curling, street hockey, cricket, rugby and softball. Popular individual sports include auto racing, boxing, karate, kickboxing, hunting, fishing, cycling, golf, Ultimate frisbee, hiking, horse racing, ice skating, skiing, snowboarding, swimming, triathlon, water sports, and several forms of wrestling.
As a country with a generally cool climate, Canada has enjoyed greater success at the Winter Olympics than at the Summer Olympics, although significant regional variations in climate allow for a wide variety of both team and individual sports. Great achievements in Canadian sports are recognized by Canada's Sports Hall of Fame, while the Lou Marsh Trophy is awarded annually to Canada's top athlete by a panel of journalists. There are numerous other Sports Halls of Fame in Canada.
Cuisine.
Canadian cuisine varies widely depending on the regions of the nation. The former Canadian prime minister Joe Clark has been paraphrased to have noted: "Canada has a cuisine of cuisines. Not a stew pot, but a smorgasbord." There are considerable overlaps between Canadian food and the rest of the cuisine in North America, many unique dishes (or versions of certain dishes) are found and available only in the country. Common contenders for the Canadian national food include Poutine and Butter tarts. A noteworthy fact is that Canada is the world's largest producer of Maple syrup.
The three earliest cuisines of Canada have First Nations, English, and French roots, with the traditional cuisine of English Canada closely related to British and American cuisine, while the traditional cuisine of French Canada has evolved from French cuisine and the winter provisions of fur traders. With subsequent waves of immigration in the 18th and 19th century from Central, Southern, and Eastern Europe, and then from Asia, Africa and Caribbean, the regional cuisines were subsequently augmented. The Jewish immigrants to Canada during the late 1800s also play a significant role to foods in Canada. The Montreal-style bagel and Montreal-style smoked meat are both food items originally developed by Jewish communities living in Montreal.
Outside views.
In 1984, Baron Moran, the British High Commissioner to Canada, stated that, in his opinion, Canadians have limited talents and are "deeply unimpressive." Said Moran, "Anyone who is even moderately good at what they do — in literature, the theater, skiing or whatever — tends to become a national figure. And anyone who stands out at all from the crowd tends to be praised to the skies and given the Order of Canada at once." 
In a 2002 interview with the Globe and Mail, Aga Khan, the 49th Imam of the Ismaili Muslims, described Canada as "the most successful pluralist society on the face of our globe", citing it as "a model for the world". A 2007 poll ranked Canada as the country with the most positive influence in the world. 28,000 people in 27 countries were asked to rate 12 countries as either having a positive or negative worldwide influence. Canada’s overall influence rating topped the list with 54 per cent of respondents rating it mostly positive and only 14 per cent mostly negative.
The average Anglo-Canadian may be perceived as more reserved than his or her American counterpart. Canada and the United States are often inevitably compared as sibling countries, and the perceptions that arise from this oft-held contrast have gone to shape the advertised worldwide identities of both nations: the United States is seen as the rebellious child of the British Crown, forged in the fires of violent revolution; Canada is the calmer offspring of the United Kingdom, known for a more relaxed national demeanour.

</doc>
<doc id="7000" url="http://en.wikipedia.org/wiki?curid=7000" title="List of companies of Canada">
List of companies of Canada

This is a list of notable companies based in Canada. For further information on the types of business entities in this country and their abbreviations, see "Business entities in Canada".

</doc>
<doc id="7003" url="http://en.wikipedia.org/wiki?curid=7003" title="Cauchy distribution">
Cauchy distribution

The Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution.
The simplest Cauchy distribution is called the standard Cauchy distribution. It is the distribution of a random variable that is the ratio of two independent standard normal variables and has the probability density function
Its cumulative distribution function has the shape of an arctangent function arctan("x"):
The Cauchy distribution is often used in statistics as the canonical example of a "pathological" distribution since both its mean and its variance are undefined. (But see the section "Explanation of undefined moments" below.) The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. The Cauchy distribution has no moment generating function.
Its importance in physics is the result of it being the solution to the differential equation describing forced resonance. In mathematics, it is closely related to the Poisson kernel, which is the fundamental solution for the Laplace equation in the upper half-plane. In spectroscopy, it is the description of the shape of spectral lines which are subject to homogeneous broadening in which all atoms interact in the same way with the frequency range contained in the line shape. Many mechanisms cause homogeneous broadening, most notably collision broadening, and Chantler–Alda radiation. In its standard form, it is the maximum entropy probability distribution for a random variate "X" for which 
Characterisation.
Probability density function.
The Cauchy distribution has the probability density function
where "x"0 is the location parameter, specifying the location of the peak of the distribution, and γ is the scale parameter which specifies the half-width at half-maximum (HWHM), alternatively 2γ is full width at half maximum (FWHM). γ is also equal to half the interquartile range and is sometimes called the probable error. Augustin-Louis Cauchy exploited such a density function in 1827 with an infinitesimal scale parameter, defining what would now be called a Dirac delta function.
The amplitude of the above Lorentzian function is given by
The special case when "x"0 = 0 and γ = 1 is called the standard Cauchy distribution with the probability density function
In physics, a three-parameter Lorentzian function is often used:
where "I" is the height of the peak.
Cumulative distribution function.
The cumulative distribution function is:
and the quantile function (inverse cdf) of the Cauchy distribution is
It follows that the first and third quartiles are ("x"0−γ, "x"0+γ), and hence the interquartile range is 2γ.
The derivative of the quantile function, the quantile density function, for the Cauchy distribution is:
The differential entropy of a distribution can be defined in terms of its quantile density, specifically
Properties.
The Cauchy distribution is an example of a distribution which has no mean, variance or higher moments defined. Its mode and median are well defined and are both equal to "x"0.
When "U" and "V" are two independent normally distributed random variables with expected value 0 and variance 1, then the ratio "U"/"V" has the standard Cauchy distribution.
If "X"1, ..., "Xn" are independent and identically distributed random variables, each with a standard Cauchy distribution, then the sample mean ("X"1+ ... +"Xn")/"n" has the same standard Cauchy distribution. To see that this is true, compute the characteristic function of the sample mean:
where formula_13 is the sample mean. This example serves to show that the hypothesis of finite variance in the central limit theorem cannot be dropped. It is also an example of a more generalized version of the central limit theorem that is characteristic of all stable distributions, of which the Cauchy distribution is a special case.
The Cauchy distribution is an infinitely divisible probability distribution. It is also a strictly stable distribution.
The standard Cauchy distribution coincides with the Student's t-distribution with one degree of freedom.
Like all stable distributions, the location-scale family to which the Cauchy distribution belongs is closed under linear transformations with real coefficients. In addition, the Cauchy distribution is the only univariate distribution which is closed under linear fractional transformations with real coefficients. In this connection, see also McCullagh's parametrization of the Cauchy distributions.
Characteristic function.
Let "X" denote a Cauchy distributed random variable. The characteristic function of the Cauchy distribution is given by
which is just the Fourier transform of the probability density. The original probability density may be expressed in terms of the characteristic function, essentially by using the inverse Fourier transform:
Observe that the characteristic function is not differentiable at the origin: this corresponds to the fact that the Cauchy distribution does not have an expected value.
Explanation of undefined moments.
Mean.
If a probability distribution has a density function "f"("x"), then the mean is
The question is now whether this is the same thing as
for an arbitrary real number a.
If at most one of the two terms in (2) is infinite, then (1) is the same as (2). But in the case of the Cauchy distribution, both the positive and negative terms of (2) are infinite. Hence (1) is undefined.
Although we may take (1) to mean
and this is its Cauchy principal value, which is zero, we could also take (1) to mean, for example,
which is "not" zero, as can be seen easily by computing the integral.
Various results in probability theory about expected values, such as the strong law of large numbers, will not work in such cases.
Higher moments.
The Cauchy distribution does not have finite moments of any order. Some of the higher raw moments do exist and have a value of infinity, for example the raw second moment:
By re-arranging the formula, one can see that the second moment is essentially the infinite integral of a constant (here 1). Higher even-powered raw moments will also evaluate to infinity. Odd-powered raw moments, however, "do not exist at all" (i.e. are undefined), which is distinctly different from existing with the value of infinity. The odd-powered raw moments are undefined because their values are essentially equivalent to formula_21 since the two halves of the integral both diverge and have opposite signs. The first raw moment is the mean, which, being odd, does not exist. (See also the discussion above about this.) This in turn means that all of the central moments and standardized moments do not exist (are undefined), since they are all based on the mean. The variance — which is the second central moment — is likewise non-existent (despite the fact that the raw second moment exists with the value infinity).
The results for higher moments follow from Hölder's inequality, which implies that higher moments (or halves of moments) diverge if lower ones do.
Estimation of parameters.
Because the parameters of the Cauchy distribution don't correspond to a mean and variance, attempting to estimate the parameters of the Cauchy distribution by using a sample mean and a sample variance will not succeed. For example, if "n" samples are taken from a Cauchy distribution, one may calculate the sample mean as:
Although the sample values "xi" will be concentrated about the central value "x"0, the sample mean will become increasingly variable as more samples are taken, because of the increased likelihood of encountering sample points with a large absolute value. In fact, the distribution of the sample mean will be equal to the distribution of the samples themselves; i.e., the sample mean of a large sample is no better (or worse) an estimator of "x"0 than any single observation from the sample. Similarly, calculating the sample variance will result in values that grow larger as more samples are taken.
Therefore, more robust means of estimating the central value "x"0 and the scaling parameter γ are needed. One simple method is to take the median value of the sample as an estimator of "x"0 and half the sample interquartile range as an estimator of γ. Other, more precise and robust methods have been developed For example, the truncated mean of the middle 24% of the sample order statistics produces an estimate for "x"0 that is more efficient than using either the sample median or the full sample mean. However, because of the fat tails of the Cauchy distribution, the efficiency of the estimator decreases if more than 24% of the sample is used.
Maximum likelihood can also be used to estimate the parameters "x"0 and γ. However, this tends to be complicated by the fact that this requires finding the roots of a high degree polynomial, and there can be multiple roots that represent local maxima. Also, while the maximum likelihood estimator is asymptotically efficient, it is relatively inefficient for small samples. The log-likelihood function for the Cauchy distribution for sample size n is:
Maximizing the log likelihood function with respect to "x"0 and γ produces the following system of equations:
Note that
is a monotone function in γ and that the solution γ must satisfy
Solving just for "x"0 requires solving a polynomial of degree 2"n"−1, and solving just for γ requires solving a polynomial of degree "n" (first for γ2, then "x"0). Therefore, whether solving for one parameter or for both parameters simultaneously, a numerical solution on a computer is typically required. The benefit of maximum likelihood estimation is asymptotic efficiency; estimating "x"0 using the sample median is only about 81% as asymptotically efficient as estimating "x"0 by maximum likelihood. The truncated sample mean using the middle 24% order statistics is about 88% as asymptotically efficient an estimator of "x"0 as the maximum likelihood estimate. When Newton's method is used to find the solution for the maximum likelihood estimate, the middle 24% order statistics can be used as an initial solution for "x"0.
Circular Cauchy distribution.
If "X" is Cauchy distributed with median μ and scale parameter γ, then the complex variable
has unit modulus and is distributed on the unit circle with density:
with respect to the angular variable θ = arg("z"), where
and ψ expresses the two parameters of the associated linear Cauchy distribution for "x" as a complex number:
The distribution formula_32 is called the circular Cauchy distribution(also the complex Cauchy distribution) with parameter ζ. The circular Cauchy distribution is related to the wrapped Cauchy distribution. If formula_33 is a wrapped Cauchy distribution with the parameter ψ = μ + "i" γ representing the parameters of the corresponding "unwrapped" Cauchy distribution in the variable "y" where θ = "y" mod 2π, then
See also McCullagh's parametrization of the Cauchy distributions and Poisson kernel for related concepts.
The circular Cauchy distribution expressed in complex form has finite moments of all orders
for integer "r" ≥ 1. For |φ| < 1, the transformation
is holomorphic on the unit disk, and the transformed variable "U"("Z", φ) is distributed as complex Cauchy with parameter "U"(ζ, φ).
Given a sample "z"1, ..., "zn" of size "n" > 2, the maximum-likelihood equation
can be solved by a simple fixed-point iteration:
starting with ζ(0) = 0. The sequence of likelihood values is non-decreasing, and the solution is unique for samples containing at least three distinct values.
The maximum-likelihood estimate for the median (formula_39) and scale parameter (formula_40) of a real Cauchy sample is obtained by the inverse transformation:
For "n" ≤ 4, closed-form expressions are known for formula_42. The density of the maximum-likelihood estimator at "t" in the unit disk is necessarily of the form:
where
Formulae for "p"3 and "p"4 are available.
Multivariate Cauchy distribution.
A random vector is said to have the multivariate Cauchy distribution if every linear combination of its components "Y" = "a"1"X"1 + ... + "akXk" has a Cauchy distribution. That is, for any constant vector , the random variable should have a univariate Cauchy distribution. The characteristic function of a multivariate Cauchy distribution is given by:
where "x"0("t") and γ("t") are real functions with "x"0("t") a homogeneous function of degree one and γ("t") a positive homogeneous function of degree one. More formally:
for all "t".
An example of a bivariate Cauchy distribution can be given by:
Note that in this example, even though there is no analogue to a covariance matrix, x and y are not statistically independent.
Analogously to the univariate density, the multidimensional Cauchy density also relates to the multivariate Student distribution. They are equivalent when the degrees of freedom parameter is equal to one. The density of a k dimension Student distribution with one degree of freedom becomes:
Properties and details for this density can be obtained by taking it as a particular case of the multivariate Student density.
Transformation properties.
where "a","b","c" and "d" are real numbers.
Relativistic Breit–Wigner distribution.
In nuclear and particle physics, the energy profile of a resonance is described by the relativistic Breit–Wigner distribution, while the Cauchy distribution is the (non-relativistic) Breit–Wigner distribution.

</doc>
<doc id="7011" url="http://en.wikipedia.org/wiki?curid=7011" title="Control engineering">
Control engineering

Control engineering or control systems engineering is the engineering discipline that applies control theory to design systems with desired behaviors. The practice uses sensors to measure the output performance of the device being controlled and those measurements can be used to give feedback to the input actuators that can make corrections toward desired performance. When a device is designed to perform without the need of human inputs for correction it is called automatic control (such as cruise control for regulating a car's speed). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of systems of a diverse range.
Overview.
Modern day control engineering (also called control systems engineering) is a relatively new field of study that gained a significant attention during 20th century with the advancement in technology. It can be broadly defined or classified as practical application of control theory. Control engineering has an essential role in a wide range of control systems, from simple household washing machines to high-performance F-16 fighter aircraft. It seeks to understand physical systems, using mathematical modeling, in terms of inputs, outputs and various components with different behaviors; use control systems design tools to develop controllers for those systems; and implement controllers in physical systems employing available technology. A system can be mechanical, electrical, fluid, chemical, financial and even biological, and the mathematical modeling, analysis and controller design uses control theory in one or many of the time, frequency and complex-s domains, depending on the nature of the design problem.
History.
Automatic control systems were first developed over two thousand years ago. The first feedback control device on record is thought to be the ancient Ktesibios's water clock in Alexandria, Egypt around the third century B.C. It kept time by regulating the water level in a vessel and, therefore, the water flow from that vessel. This certainly was a successful device as water clocks of similar design were still being made in Baghdad when the Mongols captured the city in 1258 A.D. A variety of automatic devices have been used over the centuries to accomplish useful tasks or simply to just entertain. The latter includes the automata, popular in Europe in the 17th and 18th centuries, featuring dancing figures that would repeat the same task over and over again; these automata are examples of open-loop control. Milestones among feedback, or "closed-loop" automatic control devices, include the temperature regulator of a furnace attributed to Drebbel, circa 1620, and the centrifugal flyball governor used for regulating the speed of steam engines by James Watt in 1788.
In his 1868 paper "On Governors", J. C. Maxwell (who discovered the Maxwell electromagnetic field equations) was able to explain instabilities exhibited by the flyball governor using differential equations to describe the control system. This demonstrated the importance and usefulness of mathematical models and methods in understanding complex phenomena, and signaled the beginning of mathematical control and systems theory. Elements of control theory had appeared earlier but not as dramatically and convincingly as in Maxwell's analysis.
Control theory made significant strides in the next 100 years. New mathematical techniques made it possible to control, more accurately, significantly more complex dynamical systems than the original flyball governor. These techniques include developments in optimal control in the 1950s and 1960s, followed by progress in stochastic, robust, adaptive and optimal control methods in the 1970s and 1980s. Applications of control methodology have helped make possible space travel and communication satellites, safer and more efficient aircraft, cleaner auto engines, cleaner and more efficient chemical processes, to mention but a few.
Before it emerged as a unique discipline, control engineering was practiced as a part of mechanical engineering and control theory was studied as a part of electrical engineering, since electrical circuits can often be easily described using control theory techniques. In the very first control relationships, a current output was represented with a voltage control input. However, not having proper technology to implement electrical control systems, designers left with the option of less efficient and slow responding mechanical systems. A very effective mechanical controller that is still widely used in some hydro plants is the governor. Later on, previous to modern power electronics, process control systems for industrial applications were devised by mechanical engineers using pneumatic and hydraulic control devices, many of which are still in use today.
Control theory.
There are two major divisions in control theory, namely, classical and modern, which have direct implications over the control engineering applications. The scope of classical control theory is limited to single-input and single-output (SISO) system design, except when analyzing for disturbance rejection using a second input. The system analysis is carried out in the time domain using differential equations, in the complex-s domain with the Laplace transform, or in the frequency domain by transforming from the complex-s domain. Many systems may be assumed to have a second order and single variable system response in the time domain. A controller designed using classical theory often requires on-site tuning due to incorrect design approximations. Yet, due to the easier physical implementation of classical controller designs as compared to systems designed using modern control theory, these controllers are preferred in most industrial applications. The most common controllers designed using classical control theory are PID controllers. A less common implementation may include either a Lead or Lag filter and at times both. The ultimate end goal is to meet a requirements set typically provided in the time-domain called the Step response, or at times in the frequency domain called the Open-Loop response. The Step response characteristics applied in a specification are typically percent overshoot, settling time, etc. The Open-Loop response characteristics applied in a specification are typically Gain and Phase margin and bandwidth. These characteristics may be evaluated through simulation including a dynamic model of the system under control coupled with the compensation model.
In contrast, modern control theory is carried out in the state space, and can deal with multi-input and multi-output (MIMO) systems. This overcomes the limitations of classical control theory in more sophisticated design problems, such as fighter aircraft control, with the limitation that no frequency domain analysis is possible. In modern design, a system is represented to the greatest advantage as a set of decoupled first order differential equations defined using state variables. Nonlinear, multivariable, adaptive and robust control theories come under this division. Matrix methods are significantly limited for MIMO systems where linear independence cannot be assured in the relationship between inputs and outputs. Being fairly new, modern control theory has many areas yet to be explored. Scholars like Rudolf E. Kalman and Aleksandr Lyapunov are well-known among the people who have shaped modern control theory.
Control systems.
Control engineering is the engineering discipline that focuses on the modeling of a diverse range of dynamic systems (e.g. mechanical systems) and the design of controllers that will cause these systems to behave in the desired manner. Although such controllers need not be electrical many are and hence control engineering is often viewed as a subfield of electrical engineering. However, the falling price of microprocessors is making the actual implementation of a control system essentially trivial. As a result, focus is shifting back to the mechanical and process engineering discipline, as intimate knowledge of the physical system being controlled is often desired.
Electrical circuits, digital signal processors and microcontrollers can all be used to implement Control systems. Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles.
In most of the cases, control engineers utilize feedback when designing control systems. This is often accomplished using a PID controller system. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system, which adjusts the motor's torque accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback. In practically all such systems stability is important and control theory can help ensure stability is achieved.
Although feedback is an important aspect of control engineering, control engineers may also work on the control of systems without feedback. This is known as open loop control. A classic example of open loop control is a washing machine that runs through a pre-determined cycle without the use of sensors.
Control engineering education.
At many universities, control engineering courses are taught in Electrical and Electronic Engineering, Mechatronics Engineering, Mechanical engineering, and Aerospace engineering; in others it is connected to computer science, as most control techniques today are implemented through computers, often as embedded systems (as in the automotive field). The field of control within chemical engineering is often known as process control. It deals primarily with the control of variables in a chemical process in a plant. It is taught as part of the undergraduate curriculum of any chemical engineering program, and employs many of the same principles in control engineering. Other engineering disciplines also overlap with control engineering, as it can be applied to any system for which a suitable model can be derived. However, specialised control engineering departments do exist, for example the Department of Automatic Control and Systems Engineering at the University of Sheffield.
Control engineering has diversified applications that include science, finance management, and even human behavior. Students of control engineering may start with a linear control system course dealing with the time and complex-s domain, which requires a thorough background in elementary mathematics and Laplace transform (called classical control theory). In linear control, the student does frequency and time domain analysis. Digital control and nonlinear control courses require z transformation and algebra respectively, and could be said to complete a basic control education. From here onwards there are several sub branches.
Recent advancement.
Originally, control engineering was all about continuous systems. Development of computer control tools posed a requirement of discrete control system engineering because the communications between the computer-based digital controller and the physical system are governed by a computer clock. The equivalent to Laplace transform in the discrete domain is the z-transform. Today many of the control systems are computer controlled and they consist of both digital and analog components.
Therefore, at the design stage either digital components are mapped into the continuous domain and the design is carried out in the continuous domain, or analog components are mapped into discrete domain and design is carried out there. The first of these two methods is more commonly encountered in practice because many industrial systems have many continuous systems components, including mechanical, fluid, biological and analog electrical components, with a few digital controllers.
Similarly, the design technique has progressed from paper-and-ruler based manual design to computer-aided design, and now to computer-automated design (CAutoD), which has been made possible by evolutionary computation. CAutoD can be applied not just to tuning a predefined control scheme, but also to controller structure optimisation, system identification and invention of novel control systems, based purely upon a performance requirement, independent of any specific control scheme.

</doc>
<doc id="7012" url="http://en.wikipedia.org/wiki?curid=7012" title="Chagas disease">
Chagas disease

Chagas disease , or American trypanosomiasis, is a tropical parasitic disease caused by the protozoan "Trypanosoma cruzi" and spread mostly by insects known as Triatominae or kissing bugs. The symptoms change over the course of the infection. In the early stage, symptoms are typically either not present or mild and may include: fever, swollen lymph nodes, headaches, or local swelling at the site of the bite. After 8–12 weeks, individuals enter the chronic phase of disease and in 60–70% it never produces further symptoms. The other 30 to 40% of people develop further symptoms 10 to 30 years after the initial infection. This includes enlargement of the ventricles of the heart in 20 to 30% leading to heart failure. An enlarged esophagus or an enlarged colon may also occur in 10% of people.
"T. cruzi" is commonly spread to humans and other mammals by the blood-sucking "kissing bugs" of the subfamily Triatominae. These insects are known by a number of local names, including: "vinchuca" in Argentina, Bolivia, Chile and Paraguay, "barbeiro" (the barber) in Brazil, "pito" in Colombia, "chinche" in Central America, and "chipo" in Venezuela. The disease may also be spread through blood transfusion, organ transplantation, eating food contaminated with the parasites, and from a mother to her fetus. Diagnosis of early disease is by finding the parasite in the blood using a microscope. Chronic disease is diagnosed by finding antibodies for "T. cruzi" in the blood.
Prevention mostly involves eliminating kissing bugs and avoiding their bites. Other preventative efforts include screening blood used for transfusions. A vaccine has not been developed as of 2013. Early infections are treatable with the medication benznidazole or nifurtimox. They nearly always result in a cure if given early however become less effective the longer a person has had Chagas disease. When used in chronic disease they may delay or prevent the development of end stage symptoms. Benznidazole and nifurtimox cause temporary side effects in up to 40% of people including skin disorders, brain toxicity, and digestive system irritation.
It is estimated that 7 to 8 million people, mostly in Mexico, Central America and South America have Chagas disease. It results in about 12,500 deaths a year as of 2006. Most people with the disease are poor and most people with the disease do not realize they are infected. Large-scale population movements have increased the areas where cases of Chagas disease are found and these now include many European countries and the United States. These areas have also seen an increase in the years up to 2014. The disease was first described in 1909 by Carlos Chagas after whom it is named. It affects more than 150 other animals.
Signs and symptoms.
The human disease occurs in two stages: an acute stage, which occurs shortly after an initial infection, and a chronic stage that develops over many years.
The acute phase lasts for the first few weeks or months of infection. It usually occurs unnoticed because it is symptom-free or exhibits only mild symptoms that are not unique to Chagas disease. These can include fever, fatigue, body aches, headache, rash, loss of appetite, diarrhea, and vomiting. The signs on physical examination can include mild enlargement of the liver or spleen, swollen glands, and local swelling (a chagoma) where the parasite entered the body.
The most recognized marker of acute Chagas disease is called Romaña's sign, which includes swelling of the eyelids on the side of the face near the bite wound or where the bug feces were deposited or accidentally rubbed into the eye. Rarely, young children, or adults may die from the acute disease due to severe inflammation/infection of the heart muscle (myocarditis) or brain (meningoencephalitis). The acute phase also can be severe in people with weakened immune systems.
If symptoms develop during the acute phase, they usually resolve spontaneously within three to eight weeks in approximately 90% of individuals. Although the symptoms resolve, even with treatment the infection persists and enters a chronic phase. Of individuals with chronic Chagas disease, 60–80% will never develop symptoms (called "indeterminate" chronic Chagas disease), while the remaining 20–40% will develop life-threatening heart and/or digestive disorders during their lifetime (called "determinate" chronic Chagas disease). In 10% of individuals, the disease progresses directly from the acute form to a symptomatic clinical form of chronic Chagas disease.
The symptomatic (determinate) chronic stage affects the nervous system, digestive system and heart. About two-thirds of people with chronic symptoms have cardiac damage, including dilated cardiomyopathy, which causes heart rhythm abnormalities and may result in sudden death. About one-third of patients go on to develop digestive system damage, resulting in dilation of the digestive tract (megacolon and megaesophagus), accompanied by severe weight loss. Swallowing difficulties (secondary achalasia) may be the first symptom of digestive disturbances and may lead to malnutrition.
20% to 50% of individuals with intestinal involvement also exhibit cardiac involvement. Up to 10% of chronically infected individuals develop neuritis that results in altered tendon reflexes and sensory impairment. Isolated cases exhibit central nervous system involvement, including dementia, confusion, chronic encephalopathy and sensory and motor deficits.
The clinical manifestations of Chagas disease are due to cell death in the target tissues that occurs during the infective cycle, by sequentially inducing an inflammatory response, cellular lesions, and fibrosis. For example, intracellular amastigotes destroy the intramural neurons of the autonomic nervous system in the intestine and heart, leading to megaintestine and heart aneurysms, respectively. If left untreated, Chagas disease can be fatal, in most cases due to heart muscle damage.
Transmission.
In Chagas-endemic areas, the main mode of transmission is through an insect vector called a triatomine bug. A triatomine becomes infected with "T. cruzi" by feeding on the blood of an infected person or animal. During the day, triatomines hide in crevices in the walls and roofs. The bugs emerge at night, when the inhabitants are sleeping. Because they tend to feed on people's faces, triatomine bugs are also known as "kissing bugs". After they bite and ingest blood, they defecate on the person. Triatomines pass "T. cruzi" parasites (called trypomastigotes) in feces left near the site of the bite wound.
Scratching the site of the bite causes the trypomastigotes to enter the host through the wound, or through intact mucous membranes, such as the conjunctiva. Once inside the host, the trypomastigotes invade cells, where they differentiate into intracellular amastigotes. The amastigotes multiply by binary fission and differentiate into trypomastigotes, which are then released into the bloodstream. This cycle is repeated in each newly infected cell. Replication resumes only when the parasites enter another cell or are ingested by another vector. (See also: )
Dense vegetation (such as that of tropical rainforests) and urban habitats are not ideal for the establishment of the human transmission cycle. However, in regions where the sylvatic habitat and its fauna are thinned by economic exploitation and human habitation, such as in newly deforested areas, piassava palm culture areas, and some parts of the Amazon region, a human transmission cycle may develop as the insects search for new food sources.
"T. cruzi" can also be transmitted through blood transfusions. With the exception of blood derivatives (such as fractionated antibodies), all blood components are infective. The parasite remains viable at 4 °C for at least 18 days or up to 250 days when kept at room temperature. It is unclear whether "T. cruzi" can be transmitted through frozen-thawed blood components.
Other modes of transmission include organ transplantation, through breast milk, and by accidental laboratory exposure. Chagas disease can also be spread congenitally (from a pregnant woman to her baby) through the placenta, and accounts for approximately 13% of stillborn deaths in parts of Brazil.
Oral transmission is an unusual route of infection, but has been described. In 1991, farm workers in the state of Paraíba, Brazil, were infected by eating contaminated food; transmission has also occurred via contaminated açaí palm fruit juice and garapa. A 2007 outbreak in 103 Venezuelan school children was attributed to contaminated guava juice.
Chagas disease is a growing problem in Europe, because the majority of cases with chronic infection are asymptomatic and because of migration from Latin America.
Diagnosis.
The presence of "T. cruzi" is diagnostic of Chagas disease. It can be detected by microscopic examination of fresh anticoagulated blood, or its buffy coat, for motile parasites; or by preparation of thin and thick blood smears stained with Giemsa, for direct visualization of parasites. Microscopically, "T. cruzi" can be confused with "Trypanosoma rangeli", which is not known to be pathogenic in humans. Isolation of "T. cruzi" can occur by inoculation into mice, by culture in specialized media (for example, NNN, LIT); and by xenodiagnosis, where uninfected Reduviidae bugs are fed on the patient's blood, and their gut contents examined for parasites.
Various immunoassays for "T. cruzi" are available and can be used to distinguish among strains (zymodemes of "T.cruzi" with divergent pathogenicities). These tests include: detecting complement fixation, indirect hemagglutination, indirect fluorescence assays, radioimmunoassays, and ELISA. Alternatively, diagnosis and strain identification can be made using polymerase chain reaction (PCR).
Prevention.
There is currently no vaccine against Chagas disease and prevention is generally focused on fighting the vector "Triatoma" by using sprays and paints containing insecticides (synthetic pyrethroids), and improving housing and sanitary conditions in rural areas. For urban dwellers, spending vacations and camping out in the wilderness or sleeping at hostels or mud houses in endemic areas can be dangerous; a mosquito net is recommended. Some measures of vector control include:
A number of potential vaccines are currently being tested. Vaccination with "Trypanosoma rangeli" has produced positive results in animal models. More recently, the potential of DNA vaccines for immunotherapy of acute and chronic Chagas disease is being tested by several research groups.
Blood transfusion was formerly the second-most common mode of transmission for Chagas disease, but the development and implementation of blood bank screening tests has dramatically reduced this risk in the last decade. Blood donations in all endemic Latin American countries undergo Chagas screening, and testing is expanding in countries, such as France, Spain and the United States, that have significant or growing populations of immigrants from endemic areas. In Spain, donors are evaluated with a questionnaire to identify individuals at risk of Chagas exposure for screening tests.
The US FDA has approved two Chagas tests, including one approved in April 2010, and has published guidelines that recommend testing of all donated blood and tissue products. While these tests are not required in US, an estimated 75–90% of the blood supply is currently tested for Chagas, including all units collected by the American Red Cross, which accounts for 40% of the U.S. blood supply. The Chagas Biovigilance Network reports current incidents of Chagas-positive blood products in the United States, as reported by labs using the screening test approved by the FDA in 2007.
Management.
There are two approaches to treating Chagas disease, antiparasitic treatment, to kill the parasite; and symptomatic treatment, to manage the symptoms and signs of the infection. Management uniquely involves addressing selective incremental failure of the parasympathetic nervous system. Autonomic disease imparted by Chagas may eventually result in megaesophagus, megacolon and accelerated dilated cardiomyopathy. The mechanisms that explain why Chagas targets the parasympathetic autonomic nervous system and spares the sympathetic autonomic nervous system remain poorly understood.
Medication.
Antiparasitic treatment is most effective early in the course of infection, but is not limited to cases in the acute phase. Drugs of choice include azole or nitro derivatives, such as benznidazole or nifurtimox. Both agents are limited in their capacity to effect parasitologic cure (a complete elimination of "T. cruzi" from the body), especially in chronically infected patients, and resistance to these drugs has been reported.
Studies suggest antiparasitic treatment leads to parasitological cure in about 60–85% of adults and more than 90% of infants treated in the first year of acute phase Chagas disease. Children (aged six to 12 years) with chronic disease have a cure rate of about 60% with benznidazole. While the rate of cure declines the longer an adult has been infected with Chagas, treatment with benznidazole has been shown to slow the onset of heart disease in adults with chronic Chagas infections.
Treatment of chronic infection in women prior to or during pregnancy does not appear to reduce the probability the disease will be passed on to the infant. Likewise, it is unclear whether prophylactic treatment of chronic infection is beneficial in persons who will undergo immunosuppression (for example, organ transplant recipients) or in persons who are already immunosuppressed (for example, those with HIV infection).
Complications.
In the chronic stage, treatment involves managing the clinical manifestations of the disease. For example, pacemakers and medications for irregular heartbeats, such as the anti-arrhythmia drug amiodarone, may be life saving for some patients with chronic cardiac disease, while surgery may be required for megaintestine. The disease cannot be cured in this phase, however. Chronic heart disease caused by Chagas disease is now a common reason for heart transplantation surgery. Until recently, however, Chagas disease was considered a contraindication for the procedure, since the heart damage could recur as the parasite was expected to seize the opportunity provided by the immunosuppression that follows surgery.
It was noted that survival rates in Chagas patients could be significantly improved by using lower dosages of the immunosuppressant drug cyclosporin. Recently, direct stem cell therapy of the heart muscle using bone marrow cell transplantation has been shown to dramatically reduce risks of heart failure in Chagas patients.
Epidemiology.
Chagas disease affects 8 to 10 million people living in endemic Latin American countries, with an additional 300,000–400,000 living in nonendemic countries, including Spain and the United States. An estimated 41,200 new cases occur annually in endemic countries, and 14,400 infants are born with congenital Chagas disease annually. in 2010 it resulted in approximately 10,300 deaths up from 9,300 in 1990.
The disease is present in 18 countries on the American continents, ranging from the southern United States to northern Argentina. Chagas exists in two different ecological zones. In the Southern Cone region, the main vector lives in and around human homes. In Central America and Mexico, the main vector species lives both inside dwellings and in uninhabited areas. In both zones, Chagas occurs almost exclusively in rural areas, where triatomines breed and feed on the over 150 species from 24 families of domestic and wild mammals, as well as humans, that are the natural reservoirs of "T. cruzi".
Although Triatominae bugs feed on them, birds appear to be immune to infection and therefore are not considered to be a "T. cruzi" reservoir. Even when colonies of insects are eradicated from a house and surrounding domestic animal shelters, they can re-emerge from plants or animals that are part of the ancient, sylvatic (referring to wild animals) infection cycle. This is especially likely in zones with mixed open savannah, with clumps of trees interspersed by human habitation.
The primary wildlife reservoirs for "Trypanosoma cruzi" in the United States include opossums, raccoons, armadillos, squirrels, woodrats, and mice. Opossums are particularly important as reservoirs, because the parasite can complete its life cycle in the anal glands of the animal without having to re-enter the insect vector. Recorded prevalence of the disease in opossums in the U.S. ranges from 8.3% to 37.5%.
Studies on raccoons in the Southeast have yielded infection rates ranging from 47% to as low as 15.5%. Armadillo prevalence studies have been described in Louisiana, and range from a low of 1.1% to 28.8%. Additionally, small rodents, including squirrels, mice, and rats, are important in the sylvatic transmission cycle because of their importance as bloodmeal sources for the insect vectors. A Texas study revealed 17.3% percent "T. cruzi" prevalence in 75 specimens representing four separate small rodent species.
Chronic Chagas disease remains a major health problem in many Latin American countries, despite the effectiveness of hygienic and preventive measures, such as eliminating the transmitting insects. However, several landmarks have been achieved in the fight against it in Latin America, including a reduction by 72% of the incidence of human infection in children and young adults in the countries of the Southern Cone Initiative, and at least three countries (Uruguay, in 1997, and Chile, in 1999, and Brazil in 2006) have been certified free of vectorial and transfusional transmission. In Argentina, vectorial transmission has been interrupted in 13 of the 19 endemic provinces, and major progress toward this goal has also been made in both Paraguay and Bolivia.
Screening of donated blood, blood components, and solid organ donors, as well as donors of cells, tissues, and cell and tissue products for "T. cruzi" is mandated in all Chagas-endemic countries and has been implemented. Approximately 300,000 infected people live in the United States, which is likely the result of immigration from Latin American countries. With increased population movements, the possibility of transmission by blood transfusion became more substantial in the United States. Transfusion blood and tissue products are now actively screened in the U.S., thus addressing and minimizing this risk.
History.
The disease was named after the Brazilian physician and epidemiologist Carlos Chagas, who first described it in 1909, but the disease was not seen as a major public health problem in humans until the 1960s (the outbreak of Chagas disease in Brazil in the 1920s went widely ignored). He discovered that the intestines of Triatomidae (now Reduviidae: Triatominae) harbored a flagellate protozoan, a new species of the "Trypanosoma" genus, and was able to prove experimentally that it could be transmitted to marmoset monkeys that were bitten by the infected bug. Later studies showed squirrel monkeys were also vulnerable to infection.
Chagas named the pathogenic parasite as "Trypanosoma cruzi" and later that year as "Schizotrypanum cruzi", both honoring Oswaldo Cruz, the noted Brazilian physician and epidemiologist who successfully fought epidemics of yellow fever, smallpox, and bubonic plague in Rio de Janeiro and other cities in the beginning of the 20th century. Chagas was also the first to unknowingly discover and illustrate the parasitic fungal genus "Pneumocystis", later infamously linked to PCP ("Pneumocystis" pneumonia in AIDS victims). Confusion between the two pathogens' life-cycles led him to briefly recognize his genus "Schizotrypanum", but following the description of "Pneumocystis" by others as an independent genus, Chagas returned to the use of the name "Trypanosoma cruzi".
In Argentina, the disease is known as "mal de Chagas-Mazza", in honor of Salvador Mazza, the Argentine physician who in 1926 began investigating the disease and over the years became the principal researcher of this disease in the country. Mazza produced the first scientific confirmation of the existence of "Trypanosoma cruzi" in Argentina in 1927, eventually leading to support from local and European medical schools and Argentine government policy makers.
It has been hypothesized that Charles Darwin might have suffered from Chagas disease as a result of a bite of the so-called great black bug of the Pampas ("vinchuca") (see Charles Darwin's illness). The episode was reported by Darwin in his diaries of the Voyage of the Beagle as occurring in March 1835 to the east of the Andes near Mendoza. Darwin was young and generally in good health, though six months previously he had been ill for a month near Valparaiso, but in 1837, almost a year after he returned to England, he began to suffer intermittently from a strange group of symptoms, becoming incapacitated for much of the rest of his life. Attempts to test Darwin's remains at Westminster Abbey by using modern PCR techniques were met with a refusal by the Abbey's curator.
Research.
Several experimental treatments have shown promise in animal models. These include inhibitors of oxidosqualene cyclase and squalene synthase, cysteine protease inhibitors, dermaseptins collected from frogs in the genus "Phyllomedusa" ("P. oreades" and "P. distincta"), the sesquiterpene lactone dehydroleucodine (DhL), which affects the growth of cultured epimastigote-phase "Trypanosoma cruzi", inhibitors of purine uptake, and inhibitors of enzymes involved in trypanothione metabolism. Hopefully, new drug targets may be revealed following the sequencing of the "T. cruzi" genome.
A 2004 "in vitro" study suggested that components of green tea (catechins) might be effective against "T. cruzi".
In 2013 researchers from Vanderbilt University released a study entitled "VNI Cures Acute and Chronic Experimental Chagas Disease." In their study mice were given this bio-molecule for 30 days with cure rates of 100% and 100% survival. The drug was described as non-toxic and highly selective. Its mechanism of action is an inhibitor of "T. cruzi" sterol 14α-demethylase. Further research will help determine this drug's clinical application.
Chagas disease has a serious economic impact on the United States and the world. The cost of treatment in the United States is estimated to be $900 million annually, which includes hospitalization and medical devices such as pacemakers. The global cost is estimated at $7 billion.

</doc>
<doc id="7015" url="http://en.wikipedia.org/wiki?curid=7015" title="Christiaan Barnard">
Christiaan Barnard

Christiaan Neethling Barnard (8 November 1922 – 2 September 2001) was a South African cardiac surgeon who performed the world's first successful human-to-human heart transplant.
Early life.
Barnard grew up in Beaufort West, Cape Province, Union of South Africa. His father, Adam Barnard, was a minister in the Dutch Reformed Church. One of his four brothers, Abraham, died of a heart problem at the age of five. Barnard matriculated from the Beaufort West High School in 1940, and went to study medicine at the University of Cape Town Medical School, where he obtained his MB ChB in 1945.
Career.
Barnard did his internship and residency at the Groote Schuur Hospital in Cape Town, after which he worked as a general practitioner in Ceres, a rural town in the Cape Province. In 1951, he returned to Cape Town where he worked at the City Hospital as a Senior Resident Medical Officer, and in the Department of Medicine at the Groote Schuur Hospital as a registrar. He completed his Masters degree, receiving Master of Medicine in 1953 from the University of Cape Town. In the same year he obtained a doctorate in medicine (MD) from the same university for a dissertation entitled "The treatment of tuberculous meningitis".
In 1956, he received a two-year scholarship for postgraduate training in cardiothoracic surgery at the University of Minnesota, Minneapolis, United States under open-heart surgery pioneer Walt Lillehei. It was during this time that Barnard first became acquainted with fellow future heart transplantation surgeon Norman Shumway, who along with Richard Lower did much of the trailblazing research leading to the first successful human heart transplant. In 1958 he received a Master of Science in Surgery for a thesis entitled, "The aortic valve – problems in the fabrication and testing of a prosthetic valve". The same year he was awarded Doctor of Philosophy degree for his dissertation entitled "The aetiology of congenital intestinal atresia". Barnard described the two years he spent in the US as "the most fascinating time in my life."
Upon returning to South Africa in 1958, Barnard was appointed cardiothoracic surgeon at the Groote Schuur Hospital, establishing the hospital's first heart unit. He was promoted to full-time lecturer and Director of Surgical Research at the University of Cape Town. Three years later he was appointed Head of the Division of Cardiothoracic Surgery at the teaching hospitals of the University of Cape Town. He rose to the position of Associate Professor in the Department of Surgery at the University of Cape Town in 1962. Barnard's younger brother Marius, who also studied medicine, eventually became Barnard's right-hand man at the department of Cardiac Surgery. Over time, Barnard became known as a brilliant surgeon with many contributions to the treatment of cardiac diseases, such as the Tetralogy of Fallot and Ebstein's anomaly. He was promoted to Professor of Surgical Science in the Department of Surgery at the University of Cape Town in 1972. Among the many awards he received over the years, he was named Professor Emeritus in 1984.
First successful heart transplant.
Following the first successful kidney transplant in 1953, in the United States, Barnard performed the second kidney transplant in South Africa in October 1967, the first being done in Johannesburg the previous year. Barnard experimented for several years with animal heart transplants. More than 50 dogs received transplanted hearts. With the availability of new breakthroughs introduced by several pioneers, amongst them Norman Shumway, several surgical teams were in a position to prepare for a human heart transplant. Barnard had a patient willing to undergo the procedure, but as with other surgeons, he needed a suitable donor.
He performed the world's first human heart transplant operation on 3 December 1967, in an operation assisted by his brother, Marius Barnard; the operation lasted nine hours and used a team of thirty people. The patient, Louis Washkansky, was a 54-year-old grocer, suffering from diabetes and incurable heart disease. Barnard later wrote, "For a dying man it is not a difficult decision because he knows he is at the end. If a lion chases you to the bank of a river filled with crocodiles, you will leap into the water, convinced you have a chance to swim to the other side." The donor heart came from a young woman, Denise Darvall, who had been rendered brain damaged in an accident on 2 December 1967, while crossing a street in Cape Town. After securing permission from Darvall's father to use her heart, Barnard performed the transplant. Rather than wait for Darvall's heart to stop beating, at his brother, Marius Barnard's urging, Christiaan had injected potassium into her heart to paralyse it and render her technically dead by the whole-body standard. Twenty years later, Marius Barnard recounted, "Chris stood there for a few moments, watching, then stood back and said, 'It works.'" Washkansky survived the operation and lived for 18 days. However, he succumbed to pneumonia as he was taking immunosuppressive drugs. Though the first patient with the heart of another human being survived for only a little more than two weeks, Barnard had passed a milestone in a new field of life-extending surgery.
Barnard was celebrated around the world for his accomplishment. He was photogenic, and enjoyed the media attention following the operation. Barnard continued to perform heart transplants. A transplant operation was conducted on 2 January 1968, and the patient, Philip Blaiberg, survived for 19 months. Dirk van Zyl, who received a new heart in 1971, was the longest-lived recipient, surviving over 23 years.
Barnard performed ten orthotopic transplants (1967–1973). He was also the first to perform a heterotopic heart transplant, an operation that he devised. Forty-nine consecutive heterotopic heart transplants were performed in Cape Town between 1975 and 1984.
Many surgeons gave up cardiac transplantation due to poor results, often due to rejection of the transplanted heart by the patient's immune system. Barnard persisted until the advent of ciclosporin, an effective immunosuppressive drug, which helped revive the operation throughout the world. He was also the first surgeon to attempt xenograft transplantation in a human patient, while attempting to save the life of a young girl unable to leave artificial life support after a second aortic valve replacement.
Public life.
Barnard was an outspoken opponent of South Africa's laws of apartheid, and was not afraid to criticise his nation's government, although he had to temper his remarks to some extent to travel abroad. Rather than leaving his homeland, he used his fame to campaign for a change in the law. Christiaan's brother, Marius Barnard, went into politics, and was elected to the legislature on an anti-apartheid platform. Barnard later stated that the reason he never won the Nobel Prize in Physiology or Medicine was probably because he was a "white South African".
Personal life.
Barnard's first marriage was to Aletta Gertruida Louw, a nurse, whom he married in 1948 while practising medicine in Ceres. The couple had two children: Deirdre (born 1950) and Andre (1951–1984). International fame took a toll on his personal life, and in 1969, Barnard and his wife divorced. In 1970, he married heiress Barbara Zoellner when she was 19, and they had two children: Frederick (born 1972) and Christiaan Jr. (born 1974). He divorced Zoellner in 1982. Barnard married for a third time in 1988 to Karin Setzkorn, a young model. They also had two children: Armin (born 1990) and Lara (born 1997), but this last marriage also ended in divorce in 2000.
Barnard had extramarital affairs with film star Gina Lollobrigida.
Retirement.
Barnard retired as Head of the Department of Cardiothoracic Surgery in Cape Town in 1983 after developing rheumatoid arthritis in his hands which ended his surgical career. He struggled with arthritis since 1956, when it was diagnosed during his postgraduate work in the US. After retirement, he spent two years as the Scientist-In-Residence at the Oklahoma Transplantation Institute in the US, and an acting consultant for various institutions.
He had by this time become very interested in anti-aging research, and his reputation suffered in 1986 when he promoted "Glycel", an expensive "anti-aging" skin cream, whose approval was withdrawn by the United States Food and Drug Administration soon thereafter. He also spent time as a research advisor to the Clinique la Prairie, in Switzerland, where the controversial "rejuvenation therapy" was practised.
Barnard divided the remainder of his years between Austria, where he established the Christiaan Barnard Foundation, dedicated to helping underprivileged children throughout the world, and his game farm in Beaufort West, South Africa.
Death.
Christiaan Barnard died on 2 September 2001, while on holiday in Paphos, Cyprus. Early reports stated that he had died of a heart attack, but an autopsy showed his death was caused by a severe asthma attack.
Books.
Christiaan Barnard wrote two autobiographies. His first book, "One Life", was published in 1969 and sold copies worldwide. Some of the proceeds were used to set up the Chris Barnard Fund for research into heart disease and heart transplants in Cape Town. His second autobiography, "The Second Life", was published in 1993, eight years before his death.
Apart from his autobiographies, Dr Barnard also wrote several other books including:

</doc>
<doc id="7016" url="http://en.wikipedia.org/wiki?curid=7016" title="Concubinage">
Concubinage

Concubinage is an interpersonal relationship in which a person (usually a woman) engages in an ongoing sexual relationship with another person (usually a man) to whom they are not or cannot be married. The inability to marry may be due a multiplicity of factors, such as differences in social rank (including sex slave status), an extant marriage, religious prohibitions, professional ones (for example Roman soldiers or Oxford Dons) or a lack of recognition by appropriate authorities. The woman in such a relationship is referred to as a concubine. Historically, concubinage was frequently voluntary by the woman or her family, as it provided a measure of economic security for the woman involved.
While long term sexual relationships instead of marriage have become increasing common in the western world over the last few decades, these are usually not termed as concubinage.
Ancient Greece.
In Ancient Greece, the practice of keeping a slave concubine (Greek "pallakis") was little recorded but appears throughout Athenian history. Law prescribed that a man could kill another man caught attempting a relationship with his concubine for the production of free children, which suggests that a concubine's children were not granted citizenship. While references to the sexual exploitation of maidservants appear in literature, it was considered disgraceful for a man to keep such women under the same roof as his wife. Some interpretations of "hetaera" have held they were concubines when one had a permanent relationship with a single man.
Ancient Roman "concubinae" and "concubini".
Concubinage was an institution practiced in ancient Rome that allowed a man to enter into an informal but recognized relationship with a woman ("concubina", plural "concubinae") not his wife, most often a woman whose lower social status was an obstacle to marriage. Concubinage was "tolerated to the degree that it did not threaten the religious and legal integrity of the family". It was not considered derogatory to be called a "concubina", as the title was often inscribed on tombstones.
A "concubinus" was a young male slave sexually exploited by his master as a sexual partner. Romans did not mark same-sex relations as "homosexual" if an adult male used a slave boy or male prostitute, characteristically a youth, as his passive partner (see Homosexuality in ancient Rome). The sexual abuse of a slave boy by an adult abuser was permitted. These relations, however, were expected to play a secondary role in marriage, within which institution an adult male demonstrated his masculine authority as head of the household "(paterfamilias)". In one of his wedding poems, Catullus ("fl." mid-1st century BC) assumes that the young bridegroom has a "concubinus" who considers himself elevated above the other slaves, but who will be set aside as his master turns his attention to marriage and family life.
In the Bible.
Among the Israelites, men commonly acknowledged their concubines, and such women enjoyed the same rights in the house as legitimate wives.
The concubine may not have commanded the same respect and inviolability as the wife. In the Levitical rules on sexual relations, the Hebrew word that is commonly translated as "wife" is distinct from the Hebrew word that means "concubine". However, on at least one other occasion the term is used to refer to a woman who is not a wife - specifically, the handmaiden of Jacob's wife. In the Levitical code, sexual intercourse between a man and a wife of a different man was forbidden and punishable by death for both persons involved. Since it was regarded as the highest blessing to have many children, wives often gave their maids to their husbands if they were barren, as in the cases of Sarah and Hagar, and Rachel and Bilhah. The children of the concubine often had equal rights with those of the wife; for example, King Abimelech was the son of Gideon and his concubine. Later biblical figures such as Gideon, and Solomon had concubines in addition to many childbearing wives. For example, the Books of Kings say that Solomon had 700 wives and 300 concubines.
The account of the unnamed Levite shows that the taking of concubines was not the exclusive preserve of Kings or patriarchs in Israel during the time of the Judges and that the rape of a concubine was completely unacceptable to the Israelite nation and led to a civil war. In the story, the Levite appears to be an ordinary member of the tribe dedicated to the worship of God, who was undoubtedly dishonoured both by the unfaithfulness of his concubine and her abandonment of him. However, after four months, he decides to follow her back to her family home to persuade her to return to him. Her father seeks to delay his return and he does not leave early enough to make the return journey in a single day. The hospitality he is offered at Gibeah, the way in which his host's daughter is offered to the townsmen and the circumstances of his concubine's death at their hands describe a lawless time where visitors are both welcomed and threatened in equal measure. The most disturbing aspect of this account is that both the Levite and his (male) host seek to protect themselves by offering their womenfolk to their aggressors for sex, in exchange for their own safety. The Levite acts in a way which indicates that he believes that the multiple rape of his unfaithful concubine is preferable to the violation of the virginity of his host's daughter or a sexual assault on his own person. In the morning, the Levite appears to be quite indifferent to the condition of his concubine and expects her to resume the journey but she is dead. He dismembers her body and distributes her (body parts) throughout the nation of Israel as a terrible message. This outrages and revolts the Israelite tribesmen who then wreak total retribution on the men of Gibeah and the surrounding tribe of Benjamin when they support them, killing them without mercy and burning all their towns. The inhabitants of (the town of) Jabesh Gilead are then slaughtered as a punishment for not joining the eleven tribes in their war against the Benjamites and their four hundred unmarried daughters given in forced marriage to the six hundred Benjamite survivors. Finally, the two hundred Benjamite survivors who still have no wives are granted a mass marriage by abduction by the other tribes.
There are no concubines in the New Testament. Paul, the apostle, emphasises that church leaders should be in monogamous marriages, that believers should not have sexual relationships outside marriage. and that unmarried believers should be celibate. Marriage is to reflect the exclusive relationship between the husband (Christ) and wife (his church), described as a "mystery".
In Judaism.
In Judaism, concubines are referred to by the Hebrew term "pillegesh". The term is a non-Hebrew, non-Semitic loanword derived from the Greek word, "pallakis", Greek "παλλακίς", meaning "a mistress staying in house".
According to the Babylonian Talmud, the difference between a concubine and a full wife was that the latter received a "marriage contract" (Hebrew: "ketubbah") and her marriage ("nissu'in") was preceded by a formal betrothal ("erusin"). Neither was the case for a concubine. One opinion in the Jerusalem Talmud argues that the concubine should also receive a "marriage contract", but without a clause specifying a divorce settlement.
Certain Jewish thinkers, such as Maimonides, believed that concubines were strictly reserved for kings, and thus that a commoner may not have a concubine. Indeed, such thinkers argued that commoners may not engage in any type of sexual relations outside of a marriage.
Maimonides was not the first Jewish thinker to criticise concubinage. For example, Leviticus Rabbah severely condemns the custom. Other Jewish thinkers, such as Nahmanides, Samuel ben Uri Shraga Phoebus, and Jacob Emden, strongly objected to the idea that concubines should be forbidden.
In the Hebrew of the contemporary State of Israel, the word "pillegesh" is often used as the equivalent of the English word "mistress"—i.e., the female partner in extramarital relations—regardless of legal recognition. Attempts have been initiated to popularise "pillegesh" as a form of premarital, non-marital or extramarital relationship (which, according to the perspective of the enacting person(s), is permitted by Jewish law).
In China.
In China, successful men often had concubines until the practice was outlawed after the Communist Party of China came to power in 1949. For example, it has been documented that Chinese Emperors accommodated thousands of concubines. A concubine's treatment and situation were highly variable and were influenced by the social status of the male to whom she was engaged, as well as the attitude of the wife. The position of the concubine was generally inferior to that of the wife. Although a concubine could produce heirs, her children would be inferior in social status to "legitimate" children. Allegedly, concubines were occasionally buried alive with their masters to "keep them company in the afterlife."
Despite the limitations imposed on Chinese concubines, history and literature offer examples of concubines who achieved great power and influence. For example, in one of the Four Great Classical Novels of China, "The Dream of the Red Chamber" (believed to be a semi-autobiographical account of author Cao Xueqin's own family life), three generations of the Jia family are supported by one favorite concubine of the Emperor.
Imperial concubines, kept by Emperors in the Forbidden City, were traditionally guarded by eunuchs to ensure that they could not be impregnated by anyone but the Emperor. Lady Yehenara, otherwise known as Dowager Empress Cixi, was arguably one of the most successful concubines in China’s history. Cixi first entered the court as a concubine to the Xianfeng Emperor and gave birth to his only surviving son, who would become the Tongzhi Emperor. She would eventually become the "de facto" ruler of the Manchu Qing Dynasty in China for 47 years after her son's death.
In Islam.
Chapter four (Surat-un-Nisa), verse three of the Quran states that a man may be married to a maximum of four women if he can treat them with justice, and if he is unable to be just among plural wives, he may marry only one woman.
Islam considers every human free from birth (Deen-al-Fitrah). Concubines were common in pre-Islamic era and when Islam arrived, it had a society with concubines. Islam never encouraged concubines but accepted pre-existing concubines as social members and encouraged gradual elimination of this practice through manumission and marriages. Children of concubines were declared legitimate as children born in wedlock, and the mother of a free child was considered free upon the death of the male partner. A concubine was expected to maintain her chastity in the same manner as a free woman; however, the penalty to be exacted by the Shari'a for a convicted transgression was half that of a free woman. The expectation of a wife to adorn herself and be attentive to the man's desires, which is a formal part of Islamic adab (religiously prescribed comportment) did not extend to concubines.
Ancient times.
In ancient times, Islam imposed very strict checks on how a woman became a concubine in order to prevent misuse. In ancient times, two sources for concubines were permitted under an Islamic regime. Primarily, non-Muslim women taken as prisoners of war were made concubines as happened after the Battle of Bani Qariza. Alternately, in ancient (Pagan/Pre-Islamic) times, sale and purchase of human slaves was a socially legal exercise. However, Islam encouraged their manumission in general. On embracing Islam, it was encouraged to manumit slave women or bring them into formal marriage (Nikah).
Modern times.
According to the rules of Islamic Fiqh, what is "halal" (permitted) by Allah in the Quran cannot be altered by any authority or individual. Therefore, although the "concept" of concubinage is "halal", concubines are mostly no longer available in this modern era nor allowed to be sold or purchased in accordance with the latest human rights standards. However, as change of existing Islamic law is impossible, a concubine in this modern era must be given all the due rights which Islam had preserved in the past.
It is further clarified that all domestic and organizational female employees are not concubines in this era and hence sex is forbidden with them unless Nikah (formal marriage) or mut'ah (temporary marriage - which only Shi'ah Islam permits) is committed through the proper channels. The Sunni scholars also do not allow "mut'ah", or temporary marriage. Although sometimes the two are confused, Mut'ah is not concubinage.
In the United States.
 When slavery became institutionalized in the North American colonies, white men, whether or not they were married, sometimes took enslaved women as concubines. Marriage between the races was prohibited by law in the colonies and the later United States. Many colonies and states also had laws against miscegenation, or any interracial relations. From 1662 the Colony of Virginia, followed by others, incorporated into law the principle that children took their mother's status, i.e., the principle of "partus sequitur ventrem". All children born to enslaved mothers were born into slavery, regardless of their father's status or ancestry. This led to generations of mixed-race slaves, some of whom were otherwise considered legally white (one-eighth or less African, equivalent to a great-grandparent) before the American Civil War.
In some cases, men had long-term relationships with enslaved women, giving them and their mixed-race children freedom and providing their children with apprenticeships, education and transfer of capital. In other cases, the men did nothing for the women except in a minor way. Such arrangements were more prevalent in the South during the antebellum years.
Historians widely believe that the widower Thomas Jefferson, both before and during his presidency of the United States in the early 19th century, had an intimate relationship of 38 years with his mixed-race slave Sally Hemings and fathered all of her six children of record. He freed all four of her surviving children as they came of age. The Hemings' were the only slave family to go free from Monticello. The children were seven-eighths European in ancestry and legally white. Three entered the white community as adults. A 1998 DNA study showed a match between the Jefferson male line and a male descendant of Sally Hemings.
In Louisiana and former French territories, a formalized system of concubinage called "plaçage" developed. European men took enslaved or free women of color as mistresses after making arrangements to give them a dowry, house or other transfer of property, and sometimes, if they were enslaved, offering freedom and education for their children. A third class of free people of color developed, especially in New Orleans. Many became educated, artisans and property owners. French-speaking and practicing Catholics, who combined French and African-American culture, created an elite between the whites of European descent and the masses of slaves. Today descendants of the free people of color are generally called Louisiana Creole people.

</doc>
<doc id="7017" url="http://en.wikipedia.org/wiki?curid=7017" title="Central Plaza (Hong Kong)">
Central Plaza (Hong Kong)

Central Plaza is a 78-storey, skyscraper completed in August 1992 at 18 Harbour Road, in Wan Chai on Hong Kong Island in Hong Kong. It is the third tallest tower in the city after 2 IFC in Central and the ICC in West Kowloon. It was the tallest building in Asia from 1992 to 1996, until the Shun Hing Square in neighbouring Shenzhen was built. Central Plaza surpassed the Bank of China Tower as the tallest building in Hong Kong until the completion of 2IFC.
Central Plaza was also the tallest reinforced concrete building in the world, until it was surpassed by CITIC Plaza, Guangzhou. The building uses a triangular floor plan. On the top of the tower is a four-bar neon clock that indicates the time by displaying different colours in 15 minute intervals, blinking at the change of the quarter.
An anemometer is installed on the tip of the building's mast; the anemometer sits at above sea level. The mast has a height of . It also houses the world's highest church inside a skyscraper, .
Design.
Central Plaza is made up of two principal components: a free standing high office tower and a high podium block attached to it. The tower is made up of three sections: a high tower base forming the main entrance and public circulation spaces; a tall tower body containing 57 office floors, a sky lobby and five mechanical plant floors; and the tower top consist of six mechanical plant floors and a tall tower mast.
The ground level public area along with the public sitting out area form an landscaped garden with fountain, trees and artificial stone paving. No commercial element is included in the podium. The first level is a public thoroughfare for three pedestrian bridges linking the Mass Transit Railway, the Convention and Exhibition Centre and the China Resource Building. By turning these space to public use, the building got 20% plot ratio more as bonus. The shape of the tower is not truly triangular but with its three corners cut off to provide better internal office spaces.
Design constraints.
Triangular shaped floor plan.
The building was designed to be in triangular shape because it could provide 20% more of the office area to enjoy the harbour view as compared with the square or rectangular shaped buildings. From an architectural point of view, this arrangement could provide better floor area utilisation, offering an internal column-free office area with a clear depth of and an overall usable floor area efficiency of 81%.
Nonetheless, the triangular building plan causes the air handling unit (AHU) room in the internal core also to assume a triangular configuration with only limited space. This makes the adoption of a standard AHU not feasible. Furthermore, all air-conditioning ducting, electrical trunking and piping gathered inside the core area had to be squeezed into a very narrow and congested corridor ceiling void.
Super high-rise building.
As the building is situated opposite to the HKCEC, the only way to get more sea view for the building and not be obstructed by the neighbouring high-rise buildings is to build it tall enough. However, a tall building brings a lot of difficulties to structural and building services design, for example, excessive system static pressure for water systems, high line voltage drop and long distance of vertical transportation. All these problems can increase the capital cost of the building systems and impair the safety operation of the building.
Maximum clear ceiling height.
As a general practice, for achieving a clear height of , a floor-to-floor height of would be required. However, because of high windload in Hong Kong for such a super high-rise building, every increase in building height by a metre would increase the structural cost by more than HK$1 million (HK$304,800 per ft). Therefore a comprehensive study was conducted and finally a floor height of was adopted. With this issue alone, an estimated construction cost saving for a total of 58 office floors, would be around HK$30 million. Yet at the same time, a maximum ceiling height of in office area could still be achieved with careful coordination and dedicated integration.
Steel structure vs reinforced concrete.
Steel structure is more commonly adopted in high-rise building. In the original scheme, an externally cross-braced framed tube was applied with primary/secondary beams carrying metal decking with reinforced concrete slab. The core was also of steelwork, designed to carry vertical load only. Later after a financial review by the developer, they decided to reduce the height of the superstructure by increasing the size of the floor plate so as to reduce the complex architectural requirements of the tower base which means a highstrength concrete solution became possible.
In the final scheme, columns at centres and deep floor edge beams were used to replace the large steel corner columns. As climbing form and table form construction method and efficient construction management are used in this project which make this reinforced concrete structure take no longer construction time than the steel structure. And the most attractive point is that the reinforced concrete scheme can save HK$230 million compared to that of steel structure. Hence the reinforced concrete structure was adopted and Central Plaza is now one of the tallest reinforced concrete buildings in the world.
In the reinforced concrete structure scheme, the core has a similar arrangement to the steel scheme and the wind shear is taken out from the core at the lowest basement level and transferred to the perimeter diaphragm walls. In order to reduce large shear reversals in the core walls in the basement, and at the top of the tower base level, the ground floor, basement levels 1 and 2 and the 5th and 6th floors, the floor slabs and beams are separated horizontally from the core walls.
Another advantage of using reinforced concrete structure is that it is more flexible to cope with changes in structural layout, sizes and height according to the site conditions by using table form system.

</doc>
<doc id="7018" url="http://en.wikipedia.org/wiki?curid=7018" title="Caravaggio">
Caravaggio

Michelangelo Merisi (or Amerighi) da Caravaggio (; 29 September 1572? – 18 July? 1610) was an Italian painter active in Rome, Naples, Malta, and Sicily between 1592 (1595?) and 1610. His paintings, which combine a realistic observation of the human state, both physical and emotional, with a dramatic use of lighting, had a formative influence on Baroque painting.
Caravaggio trained as a painter in Milan under Simone Peterzano who had himself trained under Titian. In his twenties Caravaggio moved to Rome where there was a demand for paintings to fill the many huge new churches and palazzos being built at the time. It was also a period when the Church was searching for a stylistic alternative to Mannerism in religious art that was tasked to counter the threat of Protestantism. Caravaggio's innovation was a radical naturalism that combined close physical observation with a dramatic, even theatrical, use of chiaroscuro which came to be known as tenebrism (the shift from light to dark with little intermediate value).
He burst upon the Rome art scene in 1600 with the success of his first public commissions, the "Martyrdom of Saint Matthew" and "Calling of Saint Matthew". Thereafter he never lacked commissions or patrons, yet he handled his success poorly. He was jailed on several occasions, vandalized his own apartment, and ultimately had a death warrant issued for him by the Pope.
An early published notice on him, dating from 1604 and describing his lifestyle three years previously, recounts that "after a fortnight's work he will swagger about for a month or two with a sword at his side and a servant following him, from one ball-court to the next, ever ready to engage in a fight or an argument, so that it is most awkward to get along with him." In 1606 he killed a young man in a brawl and fled from Rome with a price on his head. He was involved in a brawl in Malta in 1608, and another in Naples in 1609, possibly a deliberate attempt on his life by unidentified enemies. This encounter left him severely injured. A year later, at the age of 38, he died under mysterious circumstances in Porto Ercole in Tuscany, reportedly from a fever while on his way to Rome to receive a pardon.
Famous while he lived, Caravaggio was forgotten almost immediately after his death, and it was only in the 20th century that his importance to the development of Western art was rediscovered. Despite this, his influence on the new Baroque style that eventually emerged from the ruins of Mannerism was profound. It can be seen directly or indirectly in the work of Rubens, Jusepe de Ribera, Bernini, and Rembrandt, and artists in the following generation heavily under his influence were called the "Caravaggisti" or "Caravagesques", as well as tenebrists or "tenebrosi" ("shadowists"). The 20th-century art historian André Berne-Joffroy claimed: "What begins in the work of Caravaggio is, quite simply, modern painting."
Biography.
Early life (1571–1592).
Caravaggio (Michelangelo Merisi or Amerighi) was born in Milan where his father, Fermo (Fermo Merixio), was a household administrator and architect-decorator to the Marchese of Caravaggio, a town not far from the city of Bergamo. His mother, Lucia Aratori (Lutia de Oratoribus), came from a propertied family of the same district. In 1576 the family moved to Caravaggio (Caravaggius) to escape a plague which ravaged Milan, and Caravaggio's father died there in 1577. It is assumed that the artist grew up in Caravaggio, but his family kept up connections with the Sforzas and with the powerful Colonna family, who were allied by marriage with the Sforzas and destined to play a major role later in Caravaggio's life.
Caravaggio's mother died in 1584, the same year he began his four-year apprenticeship to the Milanese painter Simone Peterzano, described in the contract of apprenticeship as a pupil of Titian. Caravaggio appears to have stayed in the Milan-Caravaggio area after his apprenticeship ended, but it is possible that he visited Venice and saw the works of Giorgione, whom Federico Zuccari later accused him of imitating, and Titian. He would also have become familiar with the art treasures of Milan, including Leonardo da Vinci's "Last Supper", and with the regional Lombard art, a style which valued simplicity and attention to naturalistic detail and was closer to the naturalism of Germany than to the stylised formality and grandeur of Roman Mannerism.
Rome (1592/95–1600).
Caravaggio left Milan for Rome in 1592, in flight after "certain quarrels" and the wounding of a police officer. He arrived in Rome "naked and extremely needy ... without fixed address and without provision ... short of money." A few months later he was performing hack-work for the highly successful Giuseppe Cesari, Pope Clement VIII's favourite artist, "painting flowers and fruit" in his factory-like workshop. Known works from this period include a small "Boy Peeling a Fruit" (his earliest known painting), a "Boy with a Basket of Fruit", and the "Young Sick Bacchus", supposedly a self-portrait done during convalescence from a serious illness that ended his employment with Cesari. All three demonstrate the physical particularity for which Caravaggio was to become renowned: the fruit-basket-boy's produce has been analysed by a professor of horticulture, who was able to identify individual cultivars right down to "... a large fig leaf with a prominent fungal scorch lesion resembling anthracnose ("Glomerella cingulata")."
Caravaggio left Cesari, determined to make his own way. At this point he forged some extremely important friendships, with the painter Prospero Orsi, the architect Onorio Longhi, and the sixteen-year-old Sicilian artist Mario Minniti. Orsi, established in the profession, introduced him to influential collectors; Longhi, more balefully, introduced him to the world of Roman street-brawls; and Minniti served as a model and, years later, would be instrumental in helping Caravaggio to important commissions in Sicily.
"The Fortune Teller", his first composition with more than one figure, shows Mario being cheated by a gypsy girl. The theme was quite new for Rome, and proved immensely influential over the next century and beyond. This, however, was in the future: at the time, Caravaggio sold it for practically nothing. "The Cardsharps" — showing another naïve youth of privilege falling the victim of card cheats — is even more psychologically complex, and perhaps Caravaggio's first true masterpiece. Like the "Fortune Teller", it was immensely popular, and over 50 copies survive. More importantly, it attracted the patronage of Cardinal Francesco Maria del Monte, one of the leading connoisseurs in Rome. For Del Monte and his wealthy art-loving circle, Caravaggio executed a number of intimate chamber-pieces — "The Musicians", "The Lute Player", a tipsy "Bacchus", an allegorical but realistic "Boy Bitten by a Lizard" — featuring Minniti and other adolescent models.
The realism returned with Caravaggio's first paintings on religious themes, and the emergence of remarkable spirituality. The first of these was the "Penitent Magdalene", showing Mary Magdalene at the moment when she has turned from her life as a courtesan and sits weeping on the floor, her jewels scattered around her. "It seemed not a religious painting at all ... a girl sitting on a low wooden stool drying her hair ... Where was the repentance ... suffering ... promise of salvation?"
It was understated, in the Lombard manner, not histrionic in the Roman manner of the time. It was followed by others in the same style: "Saint Catherine"; "Martha and Mary Magdalene"; "Judith Beheading Holofernes"; a "Sacrifice of Isaac"; a "Saint Francis of Assisi in Ecstasy"; and a "Rest on the Flight into Egypt". The works, while viewed by a comparatively limited circle, increased Caravaggio's fame with both connoisseurs and his fellow artists. But a true reputation would depend on public commissions, and for these it was necessary to look to the Church.
Already evident was the intense realism or naturalism for which Caravaggio is now famous. He preferred to paint his subjects as the eye sees them, with all their natural flaws and defects instead of as idealised creations. This allowed a full display of Caravaggio's virtuosic talents. This shift from accepted standard practice and the classical idealism of Michelangelo was very controversial at the time. Not only was his realism a noteworthy feature of his paintings during this period, he turned away from the lengthy preparations traditional in central Italy at the time. Instead, he preferred the Venetian practice of working in oils directly from the subject – half-length figures and still life. One of the characteristic paintings by Caravaggio at this time which gives a good demonstration of his virtuoso talent was his work "Supper at Emmaus" from c.1600–1601.
"Most famous painter in Rome" (1600–1606).
In 1599, presumably through the influence of Del Monte, Caravaggio was contracted to decorate the Contarelli Chapel in the church of San Luigi dei Francesi. The two works making up the commission, the "Martyrdom of Saint Matthew" and "Calling of Saint Matthew", delivered in 1600, were an immediate sensation. Caravaggio's tenebrism (a heightened chiaroscuro) brought high drama to his subjects, while his acutely observed realism brought a new level of emotional intensity. Opinion among Caravaggio's artist peers was polarized. Some denounced him for various perceived failings, notably his insistence on painting from life, without drawings, but for the most part he was hailed as a great artistic visionary: "The painters then in Rome were greatly taken by this novelty, and the young ones particularly gathered around him, praised him as the unique imitator of nature, and looked on his work as miracles."
Caravaggio went on to secure a string of prestigious commissions for religious works featuring violent struggles, grotesque decapitations, torture and death, most notable and most technically masterful among them "The Taking of Christ" of circa 1602 for the Mattei Family, recently rediscovered in Ireland after two centuries. For the most part each new painting increased his fame, but a few were rejected by the various bodies for whom they were intended, at least in their original forms, and had to be re-painted or find new buyers. The essence of the problem was that while Caravaggio's dramatic intensity was appreciated, his realism was seen by some as unacceptably vulgar.
His first version of "Saint Matthew and the Angel", featured the saint as a bald peasant with dirty legs attended by a lightly clad over-familiar boy-angel, was rejected and a second version had to be painted as "The Inspiration of Saint Matthew". Similarly, "The Conversion of Saint Paul" was rejected, and while another version of the same subject, the "Conversion on the Way to Damascus", was accepted, it featured the saint's horse's haunches far more prominently than the saint himself, prompting this exchange between the artist and an exasperated official of Santa Maria del Popolo: "Why have you put a horse in the middle, and Saint Paul on the ground?" "Because!" "Is the horse God?" "No, but he stands in God's light!"
Other works included "Entombment", the "Madonna di Loreto" ("Madonna of the Pilgrims"), the "Grooms' Madonna", and the "Death of the Virgin". The history of these last two paintings illustrates the reception given to some of Caravaggio's art, and the times in which he lived. The "Grooms' Madonna", also known as "Madonna dei palafrenieri", painted for a small altar in Saint Peter's Basilica in Rome, remained there for just two days, and was then taken off. A cardinal's secretary wrote: "In this painting there are but vulgarity, sacrilege, impiousness and disgust...One would say it is a work made by a painter that can paint well, but of a dark spirit, and who has been for a lot of time far from God, from His adoration, and from any good thought..."
The "Death of the Virgin", then, commissioned in 1601 by a wealthy jurist for his private chapel in the new Carmelite church of Santa Maria della Scala, was rejected by the Carmelites in 1606. Caravaggio's contemporary Giulio Mancini records that it was rejected because Caravaggio had used a well-known prostitute as his model for the Virgin.
Giovanni Baglione, another contemporary, tells us it was due to Mary's bare legs —a matter of decorum in either case. Caravaggio scholar John Gash suggests that the problem for the Carmelites may have been theological rather than aesthetic, in that Caravaggio's version fails to assert the doctrine of the Assumption of Mary, the idea that the Mother of God did not die in any ordinary sense but was assumed into Heaven. The replacement altarpiece commissioned (from one of Caravaggio's most able followers, Carlo Saraceni), showed the Virgin not dead, as Caravaggio had painted her, but seated and dying; and even this was rejected, and replaced with a work which showed the Virgin not dying, but ascending into Heaven with choirs of angels. In any case, the rejection did not mean that Caravaggio or his paintings were out of favour. The "Death of the Virgin" was no sooner taken out of the church than it was purchased by the Duke of Mantua, on the advice of Rubens, and later acquired by Charles I of England before entering the French royal collection in 1671.
One secular piece from these years is "Amor Victorious", painted in 1602 for Vincenzo Giustiniani, a member of Del Monte's circle. The model was named in a memoir of the early 17th century as "Cecco", the diminutive for Francesco. He is possibly Francesco Boneri, identified with an artist active in the period 1610–1625 and known as Cecco del Caravaggio ('Caravaggio's Cecco'), carrying a bow and arrows and trampling symbols of the warlike and peaceful arts and sciences underfoot. He is unclothed, and it is difficult to accept this grinning urchin as the Roman god Cupid – as difficult as it was to accept Caravaggio's other semi-clad adolescents as the various angels he painted in his canvases, wearing much the same stage-prop wings. The point, however, is the intense yet ambiguous reality of the work: it is simultaneously Cupid and Cecco, as Caravaggio's Virgins were simultaneously the Mother of Christ and the Roman courtesans who modeled for them.
Exile and death (1606–1610).
Caravaggio led a tumultuous life. He was notorious for brawling, even in a time and place when such behavior was commonplace, and the transcripts of his police records and trial proceedings fill several pages. On 29 May 1606, he killed, possibly unintentionally, a young man named Ranuccio Tomassoni from Terni (Umbria). The circumstances of the brawl and the death of Ranuccio Tomassoni remain mysterious. Several contemporary "avvisi" referred to a quarrel over a gambling debt and a tennis game, and this explanation has become established in the popular imagination. But recent scholarship has made it clear that more was involved. Good modern accounts are to be found in Peter Robb's "M" and Helen Langdon's "Caravaggio: A Life". An interesting theory relating the death to Renaissance notions of honour and symbolic wounding has been advanced by art historian Andrew Graham-Dixon. Previously his high-placed patrons had protected him from the consequences of his escapades, but this time they could do nothing. Caravaggio, outlawed, fled to Naples. There, outside the jurisdiction of the Roman authorities and protected by the Colonna family, the most famous painter in Rome became the most famous in Naples. His connections with the Colonnas led to a stream of important church commissions, including the "Madonna of the Rosary", and "The Seven Works of Mercy".
Despite his success in Naples, after only a few months in the city Caravaggio left for Malta, the headquarters of the Knights of Malta, presumably hoping that the patronage of Alof de Wignacourt, Grand Master of the Knights, could help him secure a pardon for Tomassoni's death. De Wignacourt proved so impressed at having the famous artist as official painter to the Order that he inducted him as a knight, and the early biographer Bellori records that the artist was well pleased with his success. Major works from his Malta period include a huge "Beheading of Saint John the Baptist" (the only painting to which he put his signature) and a "Portrait of Alof de Wignacourt and his Page", as well as portraits of other leading knights. Yet by late August 1608 he was arrested and imprisoned. The circumstances surrounding this abrupt change of fortune have long been a matter of speculation, but recent investigation has revealed it to have been the result of yet another brawl, during which the door of a house was battered down and a knight seriously wounded. He was imprisoned by the knights and managed to escape. By December he had been expelled from the Order "as a foul and rotten member."
Caravaggio made his way to Sicily where he met his old friend Mario Minniti, who was now married and living in Syracuse. Together they set off on what amounted to a triumphal tour from Syracuse to Messina and, maybe, on to the island capital, Palermo. In Syracuse and Messina Caravaggio continued to win prestigious and well-paid commissions. Among other works from this period are "Burial of St. Lucy", "The Raising of Lazarus", and "Adoration of the Shepherds". His style continued to evolve, showing now friezes of figures isolated against vast empty backgrounds. "His great Sicilian altarpieces isolate their shadowy, pitifully poor figures in vast areas of darkness; they suggest the desperate fears and frailty of man, and at the same time convey, with a new yet desolate tenderness, the beauty of humility and of the meek, who shall inherit the earth." Contemporary reports depict a man whose behaviour was becoming increasingly bizarre, sleeping fully armed and in his clothes, ripping up a painting at a slight word of criticism, mocking the local painters.
After only nine months in Sicily, Caravaggio returned to Naples. According to his earliest biographer he was being pursued by enemies while in Sicily and felt it safest to place himself under the protection of the Colonnas until he could secure his pardon from the pope (now Paul V) and return to Rome. In Naples he painted "The Denial of Saint Peter", a final "John the Baptist (Borghese)", and his last picture, "The Martyrdom of Saint Ursula". His style continued to evolve — Saint Ursula is caught in a moment of highest action and drama, as the arrow fired by the king of the Huns strikes her in the breast, unlike earlier paintings which had all the immobility of the posed models. The brushwork was much freer and more impressionistic.
In Naples an attempt was made on his life, by persons unknown. At first it was reported in Rome that the "famous artist" Caravaggio was dead, but then it was learned that he was alive, but seriously disfigured in the face. He painted a "Salome with the Head of John the Baptist (Madrid)", showing his own head on a platter, and sent it to de Wignacourt as a plea for forgiveness. Perhaps at this time he painted also a "David with the Head of Goliath", showing the young David with a strangely sorrowful expression gazing on the severed head of the giant, which is again Caravaggio's. This painting he may have sent to his patron the unscrupulous art-loving Cardinal Scipione Borghese, nephew of the pope, who had the power to grant or withhold pardons.
In the summer of 1610 he took a boat northwards to receive the pardon, which seemed imminent thanks to his powerful Roman friends. With him were three last paintings, gifts for Cardinal Scipione. What happened next is the subject of much confusion and conjecture. The bare facts are that on 28 July an anonymous "avviso" (private newsletter) from Rome to the ducal court of Urbino reported that Caravaggio was dead. Three days later another "avviso" said that he had died of fever on his way from Naples to Rome. A poet friend of the artist later gave 18 July as the date of death, and a recent researcher claims to have discovered a death notice showing that the artist died on that day of a fever in Porto Ercole, near Grosseto in Tuscany. Human remains found in a church in Porto Ercole in 2010 are believed to almost certainly belong to Caravaggio. The findings come after a year-long investigation using DNA, carbon dating and other analyses.
Some scholars argue that Caravaggio was murdered by the same "enemies" that had been pursuing him since he fled Malta, possibly Wignacourt and/or factions in the Order of St. John. Caravaggio might have died of lead poisoning. Bones with high lead levels were recently found in a grave likely to be Caravaggio's. Paints used at the time contained high amounts of lead salts. Caravaggio is known to have indulged in violent behavior, as caused by lead poisoning.
Sexuality.
Caravaggio never married and had no known children, and Howard Hibbard notes the absence of erotic female figures from the artist's oeuvre: "In his entire career he did not paint a single female nude." On the other hand, the cabinet-pieces from the Del Monte period are replete with "full-lipped, languorous boys ... who seem to solicit the onlooker with their offers of fruit, wine, flowers - and themselves." Nevertheless, a connection with a certain Lena is mentioned in a 1605 court deposition by Pasqualone, where she is described as "Michelangelo's girl". According to G.B.Passeri this 'Lena' was Caravaggio's model for the "Madonna di Loreto". According to Catherine Puglisi 'Lena' may have been the same as the courtesan Maddalena di Paolo Antognetti, who named Caravaggio as an "intimate friend" by her own testimony in 1604. Caravaggio also probably enjoyed close relationships with other "whores and courtesans" such as Fillide Melandroni, of whom he painted a portrait.
Since the 1970s art scholars and historians have debated the inferences of homoeroticism in Caravaggio's works. The model of "Omnia vincit amor" is known as Cecco di Caravaggio. Cecco stayed with him even after he was obliged to leave Rome in 1606, and the two may have been lovers."
Aside from the paintings, evidence also comes from the libel trial brought against Caravaggio by Giovanni Baglione in 1603. Baglione accused Caravaggio and his friends of writing and distributing scurrilous doggerel attacking him; the pamphlets, according to Baglione's friend and witness Mao Salini, had been distributed by a certain Giovanni Battista, a "bardassa," or boy prostitute, shared by Caravaggio and his friend Onorio Longhi. Caravaggio denied knowing any young boy of that name, and the allegation was not followed up. Baglione's painting of "Divine Love" has also been seen as a visual accusation of sodomy against Caravaggio. Such accusations were damaging and dangerous as sodomy was a capital crime at the time. Even though the authorities were unlikely to investigate such a well-conneced person as Caravaggio: "Once an artist had been smeared as a pederast, his work was smeared too." Francesco Susinoo in his later biography relates the story of how the artist was chased by a school-master in Sicily for spending too long gazing at the boys in his care. Susino presents it as a misunderstanding, but Caravaggio may indeed have been seeking sexual solace; and the incident could explain one of his most homoerotic paintings: his last depiction of St John the Baptist.
The art historian, Andrew Graham-Dixon has summarised the debate:
A lot has been made of Caravaggio's presumed homosexuality, which has in more than one previous account of his life been presented as the single key that explains everything, both the power of his art and the misfortunes of his life. There is no absolute proof of it, only strong circumstantial evidence and much rumour. The balance of probability suggests that Caravaggio did indeed have sexual relations with men. But certainly had female lovers. Throughout the years that he spent in Rome he kept close company with a number of prostitutes. The truth is that Caravaggio was as uneasy in his relationships as he was in most other aspects of life. He likely slept with men. He did sleep with women. He settled with no one... [but] the idea that he was an early martyr to the drives of an unconventional sexuality is an anachronistic fiction.
As an artist.
The birth of Baroque.
Caravaggio "put the oscuro (shadows) into chiaroscuro." Chiaroscuro was practiced long before he came on the scene, but it was Caravaggio who made the technique a dominant stylistic element, darkening the shadows and transfixing the subject in a blinding shaft of light. With this came the acute observation of physical and psychological reality which formed the ground both for his immense popularity and for his frequent problems with his religious commissions. He worked at great speed, from live models, scoring basic guides directly onto the canvas with the end of the brush handle; very few of Caravaggio's drawings appear to have survived, and it is likely that he preferred to work directly on the canvas. The approach was anathema to the skilled artists of his day, who decried his refusal to work from drawings and to idealise his figures. Yet the models were basic to his realism. Some have been identified, including Mario Minniti and Francesco Boneri, both fellow artists, Mario appearing as various figures in the early secular works, the young Francesco as a succession of angels, Baptists and Davids in the later canvasses. His female models include Fillide Melandroni, Anna Bianchini, and Maddalena Antognetti (the "Lena" mentioned in court documents of the "artichoke" case as Caravaggio's concubine), all well-known prostitutes, who appear as female religious figures including the Virgin and various saints. Caravaggio himself appears in several paintings, his final self-portrait being as the witness on the far right to the "Martyrdom of Saint Ursula".
Caravaggio had a noteworthy ability to express in one scene of unsurpassed vividness the passing of a crucial moment. "The Supper at Emmaus" depicts the recognition of Christ by his disciples: a moment before he is a fellow traveler, mourning the passing of the Messiah, as he never ceases to be to the inn-keeper's eyes, the second after, he is the Saviour. In "The Calling of St Matthew", the hand of the Saint points to himself as if he were saying "who, me?", while his eyes, fixed upon the figure of Christ, have already said, "Yes, I will follow you". With "The Resurrection of Lazarus", he goes a step further, giving us a glimpse of the actual physical process of resurrection. The body of Lazarus is still in the throes of rigor mortis, but his hand, facing and recognizing that of Christ, is alive. Other major Baroque artists would travel the same path, for example Bernini, fascinated with themes from Ovid's "Metamorphoses".
The "Caravaggisti".
The installation of the St. Matthew paintings in the Contarelli Chapel had an immediate impact among the younger artists in Rome, and Caravaggism became the cutting edge for every ambitious young painter. The first Caravaggisti included Orazio Gentileschi and Giovanni Baglione. Baglione's Caravaggio phase was short-lived; Caravaggio later accused him of plagiarism and the two were involved in a long feud. Baglione went on to write the first biography of Caravaggio. In the next generation of Caravaggisti there were Carlo Saraceni, Bartolomeo Manfredi and Orazio Borgianni. Gentileschi, despite being considerably older, was the only one of these artists to live much beyond 1620, and ended up as court painter to Charles I of England. His daughter Artemisia Gentileschi was also close to Caravaggio, and one of the most gifted of the movement. Yet in Rome and in Italy it was not Caravaggio, but the influence of Annibale Carracci, blending elements from the High Renaissance and Lombard realism, which ultimately triumphed.
Caravaggio's brief stay in Naples produced a notable school of Neapolitan Caravaggisti, including Battistello Caracciolo and Carlo Sellitto. The Caravaggisti movement there ended with a terrible outbreak of plague in 1656, but the Spanish connection – Naples was a possession of Spain – was instrumental in forming the important Spanish branch of his influence.
A group of Catholic artists from Utrecht, the "Utrecht Caravaggisti", travelled to Rome as students in the first years of the 17th century and were profoundly influenced by the work of Caravaggio, as Bellori describes. On their return to the north this trend had a short-lived but influential flowering in the 1620s among painters like Hendrick ter Brugghen, Gerrit van Honthorst, Andries Both and Dirck van Baburen. In the following generation the effects of Caravaggio, although attenuated, are to be seen in the work of Rubens (who purchased one of his paintings for the Gonzaga of Mantua and painted a copy of the "Entombment of Christ"), Vermeer, Rembrandt, and Velázquez, the last of whom presumably saw his work during his various sojourns in Italy.
Death and rebirth of a reputation.
Caravaggio's innovations inspired the Baroque, but the Baroque took the drama of his chiaroscuro without the psychological realism. While he directly influenced the style of the artists mentioned above, and, at a distance, the Frenchmen Georges de La Tour and Simon Vouet, and the Spaniard Giuseppe Ribera, within a few decades his works were being ascribed to less scandalous artists, or simply overlooked. The Baroque, to which he contributed so much, had evolved, and fashions had changed, but perhaps more pertinently Caravaggio never established a workshop as the Carracci did, and thus had no school to spread his techniques. Nor did he ever set out his underlying philosophical approach to art, the psychological realism which can only be deduced from his surviving work.
Thus his reputation was doubly vulnerable to the critical demolition-jobs done by two of his earliest biographers, Giovanni Baglione, a rival painter with a personal vendetta, and the influential 17th century critic Gian Pietro Bellori, who had not known him but was under the influence of the earlier Giovanni Battista Agucchi and Bellori's friend Poussin, in preferring the "classical-idealistic" tradition of the Bolognese school led by the Carracci. Baglione, his first biographer, played a considerable part in creating the legend of Caravaggio's unstable and violent character, as well as his inability to draw.
In the 1920s, art critic Roberto Longhi brought Caravaggio's name once more to the foreground, and placed him in the European tradition: "Ribera, Vermeer, La Tour and Rembrandt could never have existed without him. And the art of Delacroix, Courbet and Manet would have been utterly different". The influential Bernard Berenson agreed: "With the exception of Michelangelo, no other Italian painter exercised so great an influence."
Oeuvre.
Only about 80 paintings by Caravaggio have survived, but some lost works have been found from time to time. One, "The Calling of Saints Peter and Andrew", was recently authenticated and restored; it had been in storage in Hampton Court, mislabeled as a copy. Richard Francis Burton writes of a "picture of St. Rosario (in the museum of the Grand Duke of Tuscany), showing a circle of thirty men "turpiter ligati"" which is not known to have survived. The rejected version of "The Inspiration of Saint Matthew" intended for the Contarelli Chapel in San Luigi dei Francesi in Rome was destroyed during the bombing of Dresden, though black and white photographs of the work exist. In June 2011 it was announced that a previously unknown Caravaggio painting of Saint Augustine dating to about 1600 had been discovered in a private collection in Britain. Called a "significant discovery", the painting had never been published and is thought to have been commissioned by Vincenzo Giustiniani, a patron of the painter in Rome.
Epitaph.
Caravaggio's epitaph was composed by his friend Marzio Milesi. It reads:
"Michelangelo Merisi, son of Fermo di Caravaggio – in painting not equal to a painter, but to Nature itself – died in Port' Ercole – betaking himself hither from Naples – returning to Rome – 15th calend of August – In the year of our Lord 1610 – He lived thirty-six years nine months and twenty days – Marzio Milesi, Jurisconsult – Dedicated this to a friend of extraordinary genius."
Theft of his Nativity.
In October 1969, two thieves entered the Oratory of San Lorenzo in Palermo, Italy and removed the Caravaggio "Nativity with San Lorenzo and San Francesco" from its frame. Experts estimate its value at $20 million.
References.
The main primary sources for Caravaggio's life are:
All have been reprinted in Howard Hibbard's "Caravaggio" and in the appendices to Catherine Puglisi's "Caravaggio".
External links.
Biography
Articles and essays
Art works
Music
Video

</doc>
<doc id="7019" url="http://en.wikipedia.org/wiki?curid=7019" title="Jean-Baptiste-Siméon Chardin">
Jean-Baptiste-Siméon Chardin

Jean-Baptiste-Siméon Chardin (; 2 November 1699 – 6 December 1779) was an 18th-century French painter. He is considered a master of still life, and is also noted for his genre paintings which depict kitchen maids, children, and domestic activities. Carefully balanced composition, soft diffusion of light, and granular impasto characterize his work.
Life.
Chardin was born in Paris, the son of a cabinetmaker, and rarely left the city. He lived on the Left Bank near Saint-Sulpice until 1757, when Louis XV granted him a studio and living quarters in the Louvre.
Chardin entered into a marriage contract with Marguerite Saintard in 1723, whom he did not marry until 1731. He served apprenticeships with the history painters Pierre-Jacques Cazes and Noël-Nicolas Coypel, and in 1724 became a master in the Académie de Saint-Luc.
According to one nineteenth-century writer, at a time when it was hard for unknown painters to come to the attention of the Royal Academy, he first found notice by displaying a painting at the "small Corpus Christi" (held eight days after the regular one) on the Place Dauphine (by the Pont Neuf). Van Loo, passing by in 1720, bought it and later assisted the young painter.
Upon presentation of "The Ray" in 1728, he was admitted to the Académie Royale de Peinture et de Sculpture. The following year he ceded his position in the Académie de Saint-Luc. He made a modest living by "produc[ing] paintings in the various genres at whatever price his customers chose to pay him", and by such work as the restoration of the frescoes at the Galerie François I at Fontainebleau in 1731.
In November 1731 his son Jean-Pierre was baptized, and a daughter, Marguerite-Agnès, was baptized in 1733. In 1735 his wife Marguerite died, and within two years Marguerite-Agnès had died as well.
Beginning in 1737 Chardin exhibited regularly at the Salon. He would prove to be a "dedicated academician", regularly attending meetings for fifty years, and functioning successively as counsellor, treasurer, and secretary, overseeing in 1761 the installation of Salon exhibitions.
His work gained popularity through reproductive engravings of his genre paintings (made by artists such as F.-B. Lépicié and P.-L. Sugurue), which brought Chardin income in the form of "what would now be called royalties".
In 1744 he entered his second marriage, this time to Françoise-Marguerite Pouget. The union brought a substantial improvement in Chardin's financial circumstances. In 1745 a daughter, Angélique-Françoise, was born, but she died in 1746.
In 1752 Chardin was granted a pension of 500 livres by Louis XV. At the Salon of 1759 he exhibited nine paintings; it was the first Salon to be commented upon by Denis Diderot, who would prove to be a great admirer and public champion of Chardin's work. Beginning in 1761, his responsibilities on behalf of the Salon, simultaneously arranging the exhibitions and acting as treasurer, resulted in a diminution of productivity in painting, and the showing of 'replicas' of previous works. In 1763 his services to the Académie were acknowledged with an extra 200 livres in pension. In 1765 he was unanimously elected associate member of the Académie des Sciences, Belles-Lettres et Arts of Rouen, but there is no evidence that he left Paris to accept the honor. By 1770 Chardin was the 'Premier peintre du roi', and his pension of 1,400 livres was the highest in the Academy.
In 1772 Chardin's son, also a painter, drowned in Venice, a probable suicide. The artist's last known oil painting was dated 1776; his final Salon participation was in 1779, and featured several pastel studies. Gravely ill by November of that year, he died in Paris on December 6, at the age of 80.
Work.
Chardin worked very slowly and painted only slightly more than 200 pictures (about four a year) total.
Chardin's work had little in common with the Rococo painting that dominated French art in the 18th century. At a time when history painting was considered the supreme classification for public art, Chardin's subjects of choice were viewed as minor categories. He favored simple yet beautifully textured still lifes, and sensitively handled domestic interiors and genre paintings. Simple, even stark, paintings of common household items ("Still Life with a Smoker's Box") and an uncanny ability to portray children's innocence in an unsentimental manner ("Boy with a Top" [right]) nevertheless found an appreciative audience in his time, and account for his timeless appeal.
Largely self-taught, Chardin was greatly influenced by the realism and subject matter of the 17th-century Low Country masters. Despite his unconventional portrayal of the ascendant bourgeoisie, early support came from patrons in the French aristocracy, including Louis XV. Though his popularity rested initially on paintings of animals and fruit, by the 1730s he introduced kitchen utensils into his work ("The Copper Cistern", ca.1735, Louvre). Soon figures populated his scenes as well, supposedly in response to a portrait painter who challenged him to take up the genre. "Woman Sealing a Letter" (ca. 1733), which may have been his first attempt, was followed by half-length compositions of children saying grace, as in "Le Bénédicité", and kitchen maids in moments of reflection. These humble scenes deal with simple, everyday activities, yet they also have functioned as a source of documentary information about a level of French society not hitherto considered a worthy subject for painting. The pictures are noteworthy for their formal structure and pictorial harmony. Chardin has said about painting, "Who said one paints with colors? One "employs" colors, but one paints with "feeling"." 
Chardin frequently painted replicas of his compositions—especially his genre paintings, nearly all of which exist in multiple versions which in many cases are virtually indistinguishable. Beginning with "The Governess" (1739, in the National Gallery of Canada, Ottawa), Chardin shifted his attention from working-class subjects to slightly more spacious scenes of bourgeois life.
In 1756 he returned to the subject of the still life. In the 1770s his eyesight weakened and he took to painting in pastels, a medium in which he executed portraits of his wife and himself (see "Self-portrait" at top right). His works in pastels are now highly valued. Chardin's extant paintings, which number about 200, are in many major museums, including the Louvre.
Influence.
Chardin's influence on the art of the modern era was wide-ranging, and has been well-documented. Édouard Manet's half-length "Boy Blowing Bubbles" and the still lifes of Paul Cézanne are equally indebted to their predecessor. He was one of Henri Matisse's most admired painters; as an art student Matisse made copies of four Chardin paintings in the Louvre. Chaim Soutine's still lifes looked to Chardin for inspiration, as did the paintings of Georges Braque, and later, Giorgio Morandi. In 1999 Lucian Freud painted and etched several copies after "The Young Schoolmistress" (National Gallery, London).
Marcel Proust, in the chapter "How to open your eyes?" from "In Search of Lost Time" ("À la recherche du temps perdu"), describes a melancholic young man sitting at his simple breakfast table. The only comfort he finds is in the imaginary ideas of beauty depicted in the great masterpieces of the Louvre, materializing fancy palaces, rich princes, and the like. The author tell him to follow him to another section of the Louvre where the pictures of Jean-Baptiste Chardin are. There he would see the beauty in still life at home and in everyday activities like peeling turnips.

</doc>
<doc id="7021" url="http://en.wikipedia.org/wiki?curid=7021" title="Crookes radiometer">
Crookes radiometer

The Crookes radiometer, also known as a light mill, consists of an airtight glass bulb, containing a partial vacuum. Inside are a set of vanes which are mounted on a spindle. The vanes rotate when exposed to light, with faster rotation for more intense light, providing a quantitative measurement of electromagnetic radiation intensity. The reason for the rotation was a cause of much scientific debate in the ten years following the invention of the device, but in 1879 the currently accepted explanation for the rotation was published. Today the device is mainly used in physics education as a demonstration of a heat engine run by light energy.
It was invented in 1873 by the chemist Sir William Crookes as the by-product of some chemical research. In the course of very accurate quantitative chemical work, he was weighing samples in a partially evacuated chamber to reduce the effect of air currents, and noticed the weighings were disturbed when sunlight shone on the balance. Investigating this effect, he created the device named after him.
It is still manufactured and sold as an educational aid or curiosity.
General description.
The radiometer is made from a glass bulb from which much of the air has been removed to form a partial vacuum. Inside the bulb, on a low friction spindle, is a rotor with several (usually four) vertical lightweight metal vanes spaced equally around the axis. The vanes are polished or white on one side and black on the other.
When exposed to sunlight, artificial light, or infrared radiation (even the heat of a hand nearby can be enough), the vanes turn with no apparent motive power, the dark sides retreating from the radiation source and the light sides advancing.<br>
Cooling the radiometer causes rotation in the opposite direction.
Effect observations.
The effect begins to be observed at partial vacuum pressures of a few torr (several hundred pascals), reaches a peak at around 10−2 torr (1 pascal) and has disappeared by the time the vacuum reaches 10−6 torr (10−4 pascal) (see explanations note 1). At these very high vacuums the effect of photon radiation pressure on the vanes can be observed in very sensitive apparatus (see Nichols radiometer) but this is insufficient to cause rotation.
Origin of the name.
This can be done, for example, by visual means (e.g., a spinning slotted disk, which functions as a simple stroboscope) without interfering with the measurement itself.
Radiometers are now commonly sold worldwide as a novelty ornament; needing no batteries, but only light to get the vanes to turn. They come in various forms, such as the one pictured, and are often used in science museums to illustrate "radiation pressure" – a scientific principle that they do not in fact demonstrate.
Thermodynamic explanation.
Movement with black-body absorption.
When a radiant energy source is directed at a Crookes radiometer, the radiometer becomes a heat engine. The operation of a heat engine is based on a difference in temperature that is converted to a mechanical output. In this case, the black side of the vane becomes hotter than the other side, as radiant energy from a light source warms the black side by black-body absorption faster than the silver or white side. The internal air molecules are "heated up" (i.e. experience an increase in their speed) when they touch the black side of the vane. The details of exactly how this moves the hotter side of the vane forward are given in the section below.
The internal temperature rises as the black vanes impart heat to the air molecules, but the molecules are cooled again when they touch the bulb's glass surface, which is at ambient temperature. This heat loss through the glass keeps the internal bulb temperature steady so that the two sides of the vanes can develop a temperature difference. The white or silver side of the vanes are slightly warmer than the internal air temperature but cooler than the black side, as some heat conducts through the vane from the black side. The two sides of each vane must be thermally insulated to some degree so that the silver or white side does not immediately reach the temperature of the black side. If the vanes are made of metal, then the black or white paint can be the insulation. The glass stays much closer to ambient temperature than the temperature reached by the black side of the vanes. The higher external air pressure helps conduct heat away from the glass.
The air pressure inside the bulb needs to strike a balance between too low and too high. A strong vacuum inside the bulb does not permit motion, because there are not enough air molecules to cause the air currents that propel the vanes and transfer heat to the outside before both sides of each vane reach thermal equilibrium by heat conduction through the vane material. High inside pressure inhibits motion because the temperature differences are not enough to push the vanes through the higher concentration of air: there is too much air resistance for "eddy currents" to occur, and any slight air movement caused by the temperature difference is damped by the higher pressure before the currents can "wrap around" to the other side.
Movement with black-body radiation.
When the radiometer is heated in the absence of a light source, it turns in the forward direction (i.e. black sides trailing). If a person's hands are placed around the glass without touching it, the vanes will turn slowly or not at all, but if the glass is touched to warm it quickly, they will turn more noticeably. Directly heated glass gives off enough infrared radiation to turn the vanes, but glass blocks much of the far-infrared radiation from a source of warmth not in contact with it. However, near-infrared and visible light more easily penetrate the glass.
If the glass is cooled quickly in the absence of a strong light source by putting ice on the glass or placing it in the freezer with the door almost closed, it turns backwards (i.e. the silver sides trail). This demonstrates black-body radiation from the black sides of the vanes rather than black-body absorption. The wheel turns backwards because the black sides cool more quickly than the silver sides.
Explanations for the force on the vanes.
Over the years, there have been many attempts to explain how a Crookes radiometer works:
All-black light mill.
To rotate, a light mill does not have to be coated with different colors across each vane. In 2009, researchers at the University of Texas, Austin created a monocolored light mill which has four curved vanes; each vane forms a convex and a concave surface. The light mill is uniformly coated by gold nanocrystals, which are a strong light absorber. Upon exposure, due to geometric effect, the convex side of the vane receives more photon energy than the concave side does, and subsequently the gas molecules receive more heat from the convex side than from the concave side. At rough vacuum, this asymmetric heating effect generates a net gas movement across each vane, from the concave side to the convex side, as shown by the researchers' Direct Simulation Monte Carlo (DSMC) modeling. The gas movement causes the light mill to rotate with the concave side moving forward, due to Newton's Third Law.
This monocolored design promotes the fabrication of micrometer- or nanometer- scaled light mills, as it is difficult to pattern materials of distinct optical properties within a very narrow, three-dimensional space.
Nanoscale light mill.
In 2010 researchers at the University of California, Berkeley succeeded in building a nanoscale light mill that works on an entirely different principle to the Crookes radiometer. A swastika shaped gold light mill, only 100 nanometers in diameter, was built and illuminated by laser light that had been tuned to have an angular momentum. The possibility of doing this had been suggested by the Princeton physicist Richard Beth in 1936. The torque was greatly enhanced by the resonant coupling of the incident light to plasmonic waves in the gold structure.

</doc>
<doc id="7022" url="http://en.wikipedia.org/wiki?curid=7022" title="Cold Chisel">
Cold Chisel

Cold Chisel is a rock band that originated in Adelaide, Australia. It had chart success from the late 70s up until their most recent album releases since 2011, with nine albums making the Australian top ten. Cold Chisel are regarded as having a distinctly Australian popularity and musicianship, exemplifying "pub rock" and highlighting the working class life in Australia.
Beginnings (1973–78).
Originally named Orange, the band formed in Adelaide in 1973 as a heavy-metal cover-band comprising bassist Les Kaczmarek (died December 5, 2008), keyboard player Don Walker, guitarist Ian Moss and drummer Steve Prestwich (died 16 January, 2011). Seventeen-year-old singer Jimmy Barnes—known throughout his time with the band as Jim Barnes—joined in December 1973, taking leave from the band in 1975 for a brief stint as Bon Scott's replacement in Fraternity. The group changed its name several times before settling on Cold Chisel in 1974 after writing a song with that title. Barnes' relationship with other band members was volatile; as a Scot he often came to blows with Liverpool-born Prestwich and he left the band several times. During these periods Moss would handle vocals until Barnes returned.
Walker soon emerged as Cold Chisel's primary songwriter. Walker spent 1974 in Armidale, completing his studies and in 1975 Kaczmarek left the band and was replaced by Phil Small. Barnes' older brother John Swan was a member of Cold Chisel around this time, providing backing vocals and percussion but after several violent incidents he was fired.
In May 1976 Cold Chisel relocated to Melbourne but found little success, moving on to Sydney in November. Six months later, in May 1977, Barnes announced he would quit Cold Chisel in order to join Swan in Feather, a hard-rocking blues band that had evolved from an earlier group called Blackfeather. A farewell performance in Sydney went so well that the singer changed his mind. The following month the Warner Music Group picked up Cold Chisel.
Main career (1978–82).
In the early months of 1978, Cold Chisel recorded its self-titled debut album with producer Peter Walker. All tracks were written by Don Walker except "Juliet", for which Barnes wrote the melody and Walker the lyrics. "Cold Chisel" was released in April and featured appearances from harmonica player Dave Blight, who would become a regular on-stage guest, and saxophonists Joe Camilleri and Wilbur Wilde from Jo Jo Zep & The Falcons. The following month the song "Khe Sanh" was released as a single but was deemed too offensive for airplay on commercial radio because of the lyric "Their legs were often open/But their minds were always closed", although it was played regularly on Sydney rock station, Double J, which was not subject to such restrictions because it was part of the ABC. Despite that setback, it still reached #48 on the Australian singles chart and number four on the Adelaide charts thanks mainly to the band's rising popularity as a touring act and some local radio support in Adelaide where the single was aired in spite of the ban. "Khe Sanh" has since become Cold Chisel's signature tune and arguably its most popular among fans. The song was later remixed, with re-recorded vocals, for inclusion on the international version of 1980's "East".
The band's next release was a live EP titled "You're Thirteen, You're Beautiful, and You're Mine", in November. This had been recorded at a show at Sydney's Regent Theatre in 1977 that had featured Midnight Oil as one of the support acts. One of the EP's tracks, "Merry Go Round" was later recorded on the follow-up, "Breakfast at Sweethearts". This album was recorded between July 1978 and January 1979 with experienced producer Richard Batchens, who had previously worked with Richard Clapton, Sherbet and Blackfeather. Batchens smoothed out many of the band's rough edges and attempted to give their songs a sophisticated sound. This approach has made the album sit a little uncomfortably with the band ever since. Once again, the majority of the songs were penned by Walker, with Barnes collaborating with Walker on the first single "Goodbye (Astrid, Goodbye)" and Moss contributing to "Dresden". "Goodbye (Astrid, Goodbye)" became a live favourite for the band, and even went on to be performed by U2 during Australian tours in the 1980s.
By now the band stood at the verge of major national success, even without significant radio airplay or support from "Countdown", Australia's most important youth music program at the time. The band had become notorious for its wild behaviour, particularly from Barnes who was rumoured to have had sex with over 1000 women and who was known to consume more than a bottle of vodka every night during performances.
In late-1979, following their problematic relationship with Batchens, Cold Chisel chose Mark Opitz to produce the next single, "Choirgirl", a Don Walker composition dealing with a young woman's experience with abortion. In spite of the controversial subject matter, the track became a hit and paved the way for Cold Chisel's next album. Recorded over two months in early-1980, "East" reached #2 on the Australian album charts and was the second-highest selling album by an Australian artist for the year. Despite the continued dominance of Walker, during Cold Chisel's later career all four of the other members began to contribute songs to the band, and this was the first of their albums to feature songwriting contributions from each member of the band. Cold Chisel is the only Australian rock band to score hits with songs written by every member of the group.
Of the album's 12 tracks, two were written by Barnes, with Moss, Prestwich and Small contributing one song each. The songs ranged from straight ahead rock tracks such as "Standing on the Outside" and "My Turn to Cry" to rockabilly-flavoured work-outs ("Rising Sun", written about Barnes' relationship with his girlfriend Jane Mahoney) and pop-laced love songs ("My Baby", featuring Joe Camilleri on saxophone) to a poignant piano ballad about prison life, "Four Walls". The cover featured Barnes asleep in a bathtub wearing a kamikaze bandanna in a room littered with junk and was inspired by Jacques-Louis David's 1793 painting "The Death of Marat". The Ian Moss-penned "Never Before" was chosen as the first song to air by the ABC's radio station Triple J when it switched to the FM band that year.
Following the release of "East", Cold Chisel embarked on the Youth in Asia Tour, which took its name from a lyric in "Star Hotel". This tour saw the group play more than 60 shows in 90 days and would form the basis of 1981's double live album "Swingshift".
In April 1981 the band was nominated for all seven of the major awards at the joint "Countdown"/"TV Week" music awards held at the Sydney Entertainment Centre, and won them all. As a protest against the concept of a TV magazine being involved in a music awards ceremony, the band refused to accept its awards and finished the night by performing "My Turn to Cry". After only one verse and chorus, the band smashed up the set and left the stage.
"Swingshift" debuted at No. 1 on the Australian album charts, crystallizing the band's status as the biggest-selling act in the country. Overseas, however, Cold Chisel was unable to make an impact. With a slightly different track-listing, "East" had been issued in the United States and the band undertook its first (and only) US tour. But while it was popular as a live act, the American arm of their label did little to support the album. According to Barnes biographer Toby Creswell, at one point the band was ushered into an office to listen to the US master only to find it drenched in tape hiss and other ambient noise, making it almost unreleasable. The band was even booed off stage after a lacklustre performance in Dayton, Ohio in May 1981 opening for Ted Nugent, who at the time was touring with his guitar army aka the 'D.C. Hawks'. European audiences were more accepting of the band and the group developed a small but significant fan base in Germany.
In August 1981, the band began work on the album "Circus Animals", again with Opitz producing. The album opened with "You Got Nothing I Want", an aggressive Barnes-penned hard rock track that attacked the American industry for its handling of the band. The song would later cause problems for Barnes when he later attempted to break into the US market as a solo performer as senior music executives there continued to hold it against him. Like its predecessor, "Circus Animals" contained songs of contrasting styles, with harder-edged tracks like "Bow River" and "Hound Dog" in place beside more expansive ballads such as "Forever Now" and "When the War Is Over", both written by Prestwich. The latter track has proved to be the most popular Cold Chisel song for other artists to record -- Uriah Heep included a version on the 1989 album "Raging Silence" and John Farnham has recorded it twice, once while he and Prestwich were members of Little River Band in the mid-80s and again for his 1990 solo album "Age of Reason". The song was also a No. 1 hit for former "Australian Idol" contestant Cosima De Vito in 2004 and was also performed by Bobby Flynn during that show's 2006 season. "Forever Now" was also covered (as a country waltz) by Australian band The Reels.
To launch the album, the band performed under a circus tent at Wentworth Park in Sydney and toured heavily once more, including a show in Darwin that attracted more than 10 per cent of the city's population.
Break-up and aftermath (1983–84).
"Circus Animals" and its three singles, "You Got Nothing I Want", "Forever Now" and "When the War is Over" were all major hits in Australia during 1982 but further success overseas continued to elude the band and cracks began to appear. In early 1983 the band toured Germany but the shows went so badly that in the middle of the tour Walker up-ended his keyboard and stormed off stage during one show and Prestwich was fired. Returning to Australia, Prestwich was replaced by Ray Arnott, formerly of the 1970s progressive rock band Spectrum. After this, Barnes requested a large advance from management. Now married with a young child, exorbitant spending had left him almost broke. His request was refused however because there was a standing arrangement that any advance to one band member had to be paid to all the others. After a meeting on 17 August during which Barnes quit the band it was decided that Cold Chisel would split up. A final concert series known as The Last Stand was planned and a final studio album was also recorded. Prestwich returned for the tour, which began in October. Before the Sydney shows however, Barnes lost his voice and those dates were re-scheduled for December. The band's final performance was at the Sydney Entertainment Centre on 12 December 1983, apparently precisely 10 years since its first live appearance. The Sydney shows formed the basis of the film "The Last Stand", the biggest-selling concert film of any Australian band. Several other recordings from the tour were used on the 1984 live album "Barking Spiders Live: 1983", the title of which was inspired by the name the group occasionally used to play warm-up shows before tours, and as b-sides for a three-CD singles package known as "Three Big XXX Hits", issued ahead of the release of the 1994 compilation album, "Teenage Love".
During breaks in the tour, "Twentieth Century" was recorded. It was a fragmentary process, spread across various studios and sessions as the individual members often refused to work together, but nonetheless successful. Released in February 1984, it reached No. 1 upon release and included the songs "Saturday Night" and "Flame Trees", both of which remain radio staples. "Flame Trees", co-written by Prestwich and Walker, took its title from the BBC series "The Flame Trees of Thika" although it was lyrically inspired by the organist's hometown of Grafton, New South Wales. Barnes later recorded an acoustic version of the song on his 1993 album "Flesh and Wood" and the track was also covered by Sarah Blasko in 2006.
Barnes launched a solo career in January 1984 that has produced nine Australian No. 1 albums and an array of hit singles. One of those, "Too Much Ain't Enough Love" also peaked at No. 1. He has recorded with INXS, Tina Turner, Joe Cocker, John Farnham and a long list of other Australian and international artists and is arguably the country's most popular male rock singer.
Prestwich joined Little River Band in 1984 and appeared on the albums "Playing to Win" and "No Reins" before departing in 1986 to join John Farnham's touring band. Walker, Moss and Small all took extended breaks from music. Small, the least prominent member of the band virtually disappeared from the scene for many years, playing in a variety of minor acts. Walker formed Catfish in 1988, ostensibly a solo band with floating membership that included Moss, Charlie Owen and Dave Blight at various times. The music had a distinctly modern jazz aspect and his recordings during this phase attracted little commercial success. During 1989 he wrote several songs for Moss including "Tucker's Daughter" and "Telephone Booth" that the guitarist recorded on his debut solo album "Matchbook". Both the album and "Tucker's Daughter" peaked at No. 1 on the chart in 1989 and won Moss five ARIA Awards. His other albums met with little success.
Reunion.
Throughout the '80s and most of the '90s, Cold Chisel was courted to re-form but refused, at one point reportedly turning down an offer of $5 million to play a single show in each of the major Australian state capitals. While Moss and Walker often collaborated on projects, neither would work with Barnes again until Walker wrote "Stone Cold" for the singer's "Heat" in 1993. The pair then recorded an acoustic version for "Flesh and Wood" later the same year. Thanks primarily to continued radio airplay and Barnes' solo success, Cold Chisel's legacy remained solidly intact and by the early 90s the group had surpassed 3 million album sales, most of which had been sold since 1983. The 1991 compilation album "Chisel" was re-issued and re-packaged several times, once with the long-deleted 1978 EP as a bonus disc and a second time in 2001 as a double album. The "Last Stand" soundtrack album was also finally released in 1992 and in 1994 a complete album of previously unreleased demo and rare live recordings also surfaced. "Teenage Love" spawned a string of hit singles.
Cold Chisel reunited in 1998 to record the album "The Last Wave of Summer" and supported it with a sold-out national concert tour. The album debuted at number one on the Australian album chart. In 2003, the band re-grouped once more for the "Ringside" tour and in 2005 again reunited to perform at a benefit for the victims of the Boxing Day tsunami at the Myer Music Bowl in Melbourne.
On 10 September 2009, two days after Barnes' 15th studio solo album "The Rhythm and the Blues" hit No. 1 on the Australian album charts, Cold Chisel announced it would reform for a one-off performance at the Sydney 500 V8 Supercars event on 5 December 2009. The reunion saw the band perform at ANZ Stadium to the largest crowd of its career, with more than 45,000 fans in attendance. Cold Chisel played a single live performance in 2010, at the Deniliquin ute muster in October. In December Ian Moss confirmed that Cold Chisel was working on new material for an album.
Death of Steve Prestwich.
In January 2011, Steve Prestwich was diagnosed with a brain tumour. He underwent surgery on 14 January but never regained consciousness and died two days later, aged 56.
Catalogue Re-release and Light the Nitro Tour.
All six of Cold Chisel's studio albums were re-released in digitally re-mastered CD versions in mid-2011 and were also made available in digital format. The thirty-date Light the Nitro Tour was announced in July along with the news that former Divinyls and Catfish drummer Charley Drayton had been recruited to replace Steve Prestwich. Most shows on the tour sold out within days and new dates were later announced for early 2012. Midway through 2012, , as well as .
Musical style and lyrical themes.
Influences from blues and early rock n' roll was broadly apparent, fostered by the love of those styles by Moss, Barnes and Walker and Small and Prestwich contributed strong pop sensibilities. This allowed volatile rock songs like "You Got Nothing I Want" and "Merry-Go-Round" to stand beside thoughtful ballads like "Choirgirl", pop-flavoured love songs like "My Baby" and caustic political statements like "Star Hotel", an attack on the late-70s government of Malcolm Fraser, inspired by the Star Hotel riot in Newcastle.
The songs were not overtly political but rather observations of everyday life within Australian society and culture, in which the members with their various backgrounds (Moss was from Alice Springs, Walker grew up in rural New South Wales, Barnes and Prestwich were working-class immigrants from the UK) were quite well able to provide.
Cold Chisel's songs were about distinctly Australian experiences, a factor often cited as a major reason for the band's lack of international appeal. "Saturday Night" and "Breakfast at Sweethearts" were observations of the urban experience of Sydney's Kings Cross district where Walker lived for many years. "Misfits", which featured on the b-side to "My Baby", was about homeless kids in the suburbs surrounding Sydney. Songs like "Shipping Steel" and "Standing on The Outside" were working class anthems and many others featured characters trapped in mundane, everyday existences, yearning for the good times of the past ("Flame Trees") or for something better from life ("Bow River").
Reputation.
Alongside contemporaries like The Angels and Midnight Oil, whose rise to popularity came in their wake, Cold Chisel was renowned as one of the most dynamic live acts of their day and from early in their career concerts routinely became sell-out events. But the band was also famous for its wild lifestyle, particularly the hard-drinking Barnes, who played his role as one of the wild men of Australian rock to the hilt, never seen on stage without at least one bottle of vodka and often so drunk he could barely stand upright. Despite this, by 1982 he was a devoted family man who refused to tour without his wife and daughter. All the other band members were also settled or married; Ian Moss had a long-term relationship with the actress Megan Williams (she even sang on "Twentieth Century") whose own public persona could have hardly been more different. Yet it was the band's public image that often saw them compared less favourably with other important acts like Midnight Oil, whose music and politics (while rather more overt) were often similar but whose image and reputation was far more clean-cut. Cold Chisel remained hugely popular however and by the mid-90s had continued to sell records at such a consistent rate they became the first Australian band to achieve higher sales after their split than during their active years. While repackages and compilations accounted for much of these sales, 1994's "Teenage Love" album of rarities and two of its singles were Top Ten hits and when the group finally reformed in 1998 the resultant album was also a major hit and the follow-up tour sold out almost immediately.
Cold Chisel is one of the few Australian acts (along with AC/DC, The Easybeats,and Slim Dusty) to have become the subject of a major tribute album. In 2007, "Standing on the Outside: The Songs of Cold Chisel" was released, featuring a collection of the band's songs as performed by artists including The Living End, Evermore, Something for Kate, Pete Murray, Katie Noonan, You Am I, Paul Kelly, Alex Lloyd, Thirsty Merc and Ben Lee, many of whom were still only children when Cold Chisel first disbanded and some of whom, like the members of Evermore, had not even been born.
Discography.
Singles.
1 "Khe Sanh" re-entered the ARIA Singles Chart in August 2011.

</doc>
<doc id="7023" url="http://en.wikipedia.org/wiki?curid=7023" title="Confederate States of America">
Confederate States of America

The Confederate States of America (CSA or C.S.A.), commonly referred to as the Confederacy, was a secessionist government established in 1861 by seven slave states in the Lower South whose economy and political leadership was based on slavery. Each had declared their secession from the United States following the November 1860 election of Republican Abraham Lincoln on an anti-slavery expansion platform. Those seven states proclaimed their creation of a new nation in February 1861 before Lincoln took office in March. After war began in April, four states of the Upper South also declared their secession and joined the Confederacy. The Confederacy later accepted Missouri and Kentucky as members, although neither officially declared secession nor were ever controlled by Confederate forces.
The United States (the Union) government rejected secession and considered the Confederacy illegal. The American Civil War began with the April 12, 1861 Confederate attack upon Fort Sumter, a fort in the harbor of Charleston, South Carolina. By 1865, after very heavy fighting, largely on Confederate territory, CSA forces all surrendered and the Confederacy vanished. No foreign state officially recognized the Confederacy as an independent country, but Britain and France granted belligerent status. Although lacking a formal end, Jefferson Davis lamented that the Confederacy “disappeared” in 1865.
Span of Control.
On March 11, 1861, the Confederate Constitution of seven state signatories—South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas— replaced the February 7 CSA provisional constitution with one stating in its preamble a desire for a "permanent federal government". Four additional slave-holding states—Virginia, Arkansas, Tennessee, and North Carolina—declared their secession and joined the Confederacy following a call by U.S. President Abraham Lincoln for troops from each state to recapture Sumter and other lost federal properties in the South. Missouri and Kentucky were represented by partisan factions from those states. Also aligned with the Confederacy were two of the "Five Civilized Tribes" and a new Confederate Territory of Arizona. Efforts to secede in Maryland were halted by federal imposition of martial law, while Delaware, though of divided loyalty, did not attempt it. A Unionist government in western parts of Virginia organized the new state of West Virginia which was admitted to the Union on June 20, 1863. The Confederate government in Richmond, Virginia, had an uneasy relationship with its member states due to issues related to control of manpower, although the CSA mobilized nearly its entire white male population for war.
Confederate control over its claimed territory and population in Congressional Districts steadily shrank from 73% to 34% during the course of the Civil War due to the Union's successful overland campaigns, its control of the inland waterways into the South, and its blockade of the southern seacoast. With the Emancipation Proclamation on January 1, 1863, the Union made abolition of slavery a war goal (in addition to reunion). As Union forces moved south, large numbers of plantation slaves were freed, and many were enrolled as soldiers, teamsters and laborers. The most notable advance was Sherman's "March to the Sea" in late 1864. Much of the CSA's infrastructure was destroyed, including telegraph, railroads and bridges. Plantations in their path were severely damaged. Internal movement became increasingly difficult for Southerners, weakening the economy and limiting army mobility.
These losses created an insurmountable disadvantage in men, materiel, and finance. Public support for Confederate President Jefferson Davis's administration eroded over time due to repeated military reverses, economic hardships, and allegations of autocratic government. After four years of campaigning, Richmond fell to federal forces in April 1865, and shortly afterward, Confederate General Robert E. Lee surrendered to Union General Ulysses S. Grant—with that the Confederacy effectively collapsed. President Davis was captured on May 10, 1865, and jailed in preparation for a treason trial that was never held.
The U.S. Government began a decade-long process known as reconstruction which attempted to resolve the political and constitutional issues of the Civil War. The priorities were: to guarantee that Confederate nationalism and slavery were indeed dead, to enforce the Thirteenth Amendment which outlawed slavery, the Fourteenth which guaranteed dual U.S. and state citizenship to all, and the Fifteenth which guaranteed the right of Freedmen to vote. By 1877, Reconstruction had ended in the former Confederate states and political control in each of them had been gradually re-established by white Southern Democrats, many of whom had formerly been disenfranchised by Reconstruction policy. The war left the South economically devastated by military action, ruined infrastructure, and exhausted resources. The region remained well below national levels of prosperity until after World War II.
History.
The Confederacy was established in the Montgomery Convention in February 1861 (before Lincoln's inauguration in March) and disintegrated in April and May 1865. It was formed by delegations from seven Southern states that had proclaimed their secession from the Union. After the fighting began in April, four additional slave states seceded and were admitted. Later, two states (Missouri and Kentucky) and two territories were given seats in the Confederate Congress.
Many southern whites had considered themselves more Southern than American and would fight for their state and their region to be independent of the larger nation. That regionalism became a Southern nationalism, or the "Cause". For the duration of its existence, the Confederacy underwent trial by war. The "Southern Cause" transcended the ideology of "states' rights", tariff policy, or internal improvements. This "Cause" supported, or descended from, cultural and financial dependence on the south's slavery-based economy. The convergence of race and slavery, politics, and economics raised almost all South-related policy questions to the status of a moral question over "way of life," commingling love of things Southern and hatred of things Yankee (the North). Not only did national political parties split, but national churches and interstate families as well divided along sectional lines as the war approached.
During the elections of 1860, in no Southern state — other than South Carolina (which did not allow for voters to directly choose their electors) — was support for John Breckenridge (the choice of the Southern Democratic faction) unanimous. All of the other states recorded at least some popular votes for one or more of the other three candidates (Lincoln, Douglas and Bell). Support for these candidates, collectively, ranged from significant to an outright majority, with extremes running from 25% in Texas to 81% in Missouri. There were minority views everywhere, especially in the upland and plateau areas of the South, with western Virginia and eastern Tennessee of particular concentration.
During the actual vote for secession — with South Carolina (voting in 1860) once again the exception — there were no other Southern states which considered the question in 1861, where support for leaving the Union was unanimous. All had a population which cast significant numbers of Unionist votes in either the legislature, conventions, popular referendums, or in all three. Voting to remain in the Union, however, did not necessarily translate into being a northern sympathizer and, once hostilities actually commenced, many of these who voted to remain, particularly in the Lower South, accepted the majority decision, and supported the Confederacy.
The American Civil War became an American tragedy, the Brothers' War according to some scholars, "brother against brother, father against son, kith against kin of every degree".
A revolution in disunion.
The Confederate States of America was created by secessionists in Southern slave states who refused to remain in a nation that they believed was turning them into second–class citizens. They judged the agent of change to be abolitionists and anti-slavery elements in the Republican Party, whom they believed used repeated insult and injury to subject them to intolerable "humiliation and degradation". The "Black Republicans" (as the Southerners called them) and their allies would soon become a majority in the United States House, Senate, and Presidency. On the Supreme Court, Chief Justice Roger B. Taney (a presumed supporter of slavery) was 83 and ailing.
During the campaign for president in 1860, some secessionists threatened disunion should Lincoln (who opposed the expansion of slavery into the territories) be elected, most notably William L. Yancey. Yancey toured the North calling for secession as Stephen A. Douglas toured the South calling for union in the event of Lincoln's election. To Secessionists the Republican intent was clear: the elimination or, more realistically possible, the restriction of slavery. A Lincoln victory presented them with a momentous choice (as they saw it), even before his inauguration, "The Union without slavery, or slavery without the Union."
Causes of secession.
The immediate spark for secession came from the victory of the Republican Party and the election of Abraham Lincoln in the 1860 elections. Civil War historian James M. McPherson suggested that the most ominous feature of the Republican victory for Congress and the presidency was the magnitude of victory, over 60 percent in the Northern vote, three-fourths of their Congressional delegations. They were said by the Southern press to represent the antislavery portion of the North, “a party founded on the single sentiment…of hatred of African slavery”, now to be the controlling power in national affairs. The “Black Republican party” could overwhelm conservative Yankee persuasion. Said the New Orleans Delta, “It is in fact, essentially, a revolutionary party” to overthrow slavery.
By 1860, sectional disagreements between North and South revolved primarily around the maintenance or expansion of slavery. Historian Drew Gilpin Faust observed that "leaders of the secession movement across the South cited slavery as the most compelling reason for southern independence." Even though most white Southerners did not own slaves, the majority of white Southerners supported slavery. Besides supporting a right to hold slaves, one explanation given for why the majority might support this minority position was that they did not want to be at the bottom of the social ladder. Related and intertwined secondary issues also fueled the dispute; these secondary differences included issues of free speech, runaway slaves, expansion into Cuba and states' rights.
Historian Emory Thomas reconstructed the Confederacy's self–image by studying the correspondence sent by the Confederate government in 1861–62 to foreign governments. He found that Confederate diplomacy projected multiple contradictory self images:
In what later became known as the Cornerstone Speech, C.S. Vice President Alexander Stephens declared that the "cornerstone" of the new government "rest[ed] upon the great truth that the negro is not equal to the white man; that slavery—subordination to the superior race—is his natural and normal condition. This, our new government, is the first, in the history of the world, based upon this great physical, philosophical, and moral truth". After the war, however, Stephens made efforts to qualify his remarks, claiming they were extemporaneous, metaphorical, and intended to refer to public sentiment rather than "the principles of the new Government on this subject."
Four of the seceding states, the Deep South states of South Carolina,
Mississippi, Georgia, and Texas, issued formal declarations of causes, each of which identified the threat to slaveholders' rights as the cause of, or a major cause of, secession. Georgia also claimed a general Federal policy of favoring Northern over Southern economic interests. Texas mentioned slavery 21 times, but also listed the failure of the federal government to live up to its obligations, in the original annexation agreement, to protect settlers along the exposed western frontier. Texas resolutions further stated that governments of states and nation were established “exclusively by the white race, for themselves and their posterity”. Equal civil and political rights adhered to all white men, to the exclusion of those of the “African race”, and the end of existing racial enslavement would “bring inevitable calamities upon both [races] and desolation upon the fifteen slave-holding states".
Alabama did not provide a separate declaration of causes. Instead the Alabama ordinance stated "the election of Abraham Lincoln and Hannibal Hamlin to the offices of President and Vice-President of the United States of America, by a sectional party, avowedly hostile to the domestic institutions and to the peace and security of the people of the State of Alabama, preceded by many and dangerous infractions of the Constitution of the United States by many of the States and people of the northern section, is a political wrong of so insulting and menacing a character as to justify the people of the State of Alabama in the adoption of prompt and decided measures for their future peace and security." The ordinance invited "the slaveholding States of the South, who may approve such purpose, in order to frame a provisional as well as a permanent Government upon the principles of the Constitution of the United States" to participate in a February 4, 1861 convention in Montgomery, Alabama.
The secession ordinances of the remaining two states, Florida and Louisiana, simply declared their severing of ties with the federal Union, without stating any causes. Afterward, the Florida secession convention formed a committee to draft a declaration of causes, but the committee was discharged before completion of the task. Only an undated, untitled draft remains.
Of the four Upper South states which initially rejected secession until after the clash at Ft. Sumter (Virginia, Arkansas, North Carolina and Tennessee) Virginia's ordinance stated a kinship with the slave-holding states of the Lower South, but did not name the institution itself as a primary reason for its course
Arkansas's secession ordinance primarily revolved around strong objection to the use of military force to maintain the Union as its motivating factor. However, prior to the outbreak of war, the Arkansas Convention had on March 20 given as their first resolution: "The people of the Northern States have organized a poli­tical party, purely sectional in its character, the central and controlling idea of which is hostility to the institution of African slavery, as it exists in the Southern States; and that party has elected a President and Vice President of the United States, pledged to administer the Government upon principles inconsist­ent with the rights and subversive of the interests of the Southern States."
North Carolina and Tennessee limited their ordinances to simply withdrawing, although Tennessee went so far as to make clear they wished to make no comment at all on the "abstract doctrine of secession."
Secessionists and conventions.
The Fire-Eaters, calling for immediate secession, were opposed by two factions. "Cooperationists" in the Deep South would delay secession until several states went together, maybe in a Southern Convention. Under the influence of men such as Texas Governor Sam Houston, delay would have had the effect of sustaining the Union. "Unionists", especially in the Border South, often former Whigs, appealed to sentimental attachment to the United States. Southern Unionists' favorite presidential candidate was John Bell of Tennessee, sometimes running under an "Opposition Party" banner .
Many secessionists were active politically. Governor William Henry Gist of South Carolina corresponded secretly with other Deep South governors, and most southern governors exchanged clandestine commissioners. Charleston's secessionist "1860 Association" published over 200,000 pamphlets to persuade the youth of the South. The most influential were: "The Doom of Slavery" and "The South Alone Should Govern the South", both by John Townsend of South Carolina; and James D.B. De Bow's "The Interest of Slavery of the Southern Non-slaveholder".
Developments in South Carolina started a chain of events. The foreman of a jury refused the legitimacy of federal courts, so Federal Judge Andrew Magrath ruled that U.S. judicial authority in South Carolina was vacated. A mass meeting in Charleston celebrating the Charleston and Savannah railroad and state cooperation led to the South Carolina legislature to call for a Secession Convention. U.S. Senator James Chesnut, Jr. resigned, as did Senator James Henry Hammond.
Elections for Secessionist conventions were heated to "an almost raving pitch, no one dared dissent," says Freehling. Even once–respected voices, including the Chief Justice of South Carolina, John Belton O'Neall, lost election to the Secession Convention on a Cooperationist ticket. Across the South mobs expelled Yankees and (in Texas) killed Germans suspected of loyalty to the United States. Generally, seceding conventions which followed did not call for a referendum to ratify, although Texas, Arkansas, and Tennessee did, as well as Virginia's second convention. Missouri and Kentucky declared neutrality.
Inauguration and response.
The first secession state conventions from the Deep South sent representatives to meet at the Montgomery Convention in Montgomery, Alabama, on February 4, 1861. There the fundamental documents of government were promulgated, a provisional government was established, and a representative Congress met for the Confederate States of America.
The new 'provisional' Confederate President Jefferson Davis, a former "Cooperationist" who had insisted on delaying secession until a united South could move together, issued a call for 100,000 men from the various states' militias to defend the newly formed Confederacy. Previously John B. Floyd, U.S. Secretary of War under President James Buchanan (and soon to become a Confederate general), had moved arms south out of northern U.S. armories. Using the rationale "to economize War Department expenditures", Floyd and Congressional elements persuaded Buchanan not to install the cannons in the then-ungarrisoned southern forts.
The uninstalled cannon were appropriated by the Confederacy just before war began, along with gold bullion and coining dies at the U.S. mints in Charlotte, North Carolina; Dahlonega, Georgia; and New Orleans.
The Confederate capital was moved from Montgomery to Richmond, Virginia, in May 1861. Five days later, Davis extended an earlier martial law decree encompassing Norfolk and Portsmouth, Virginia, to include an area ten miles beyond Richmond. On February 22, 1862 (George Washington's birthday), Davis was inaugurated as permanent president with a term of six years, having been elected in November 1861.
In his first Inaugural Address, Lincoln tried to contain the expansion of the Confederacy. To quiet the rising calls for secession in additional slave-holding states, he assured the Border States that slavery would be preserved in the states where it existed, and he entertained a proposed thirteenth amendment to the Constitution, the "Corwin Amendment", then under consideration, which would explicitly grant irrevocable Constitutional protection for slavery in those states which might choose to practice its use.
The newly inaugurated Confederate administration pursued a policy of national territorial integrity, continuing earlier state efforts in 1860 and early 1861 to remove U.S. government presence from within their boundaries. These efforts included taking possession of U.S. courts, custom houses, post offices, and most notably, arsenals and forts. But after the Confederate attack and capture of Fort Sumter in April 1861, Lincoln called up 75,000 of the states' militia to muster under his command. The stated purpose was to re-occupy U.S. properties throughout the South, as the U.S. Congress had not authorized their abandonment. The resistance at Fort Sumter signaled his change of policy from that of the Buchanan Administration. Lincoln's response ignited a firestorm of emotion. The people both North and South demanded war, and young men rushed to their colors in the hundreds of thousands. Four more states (Virginia, North Carolina, Tennessee, and Arkansas) refused Lincoln's call for troops and declared secession, while Kentucky maintained an uneasy "neutrality".
Secession.
Secessionists argued that the United States Constitution was a compact among states that could be abandoned at any time without consultation and that each state had a right to secede. After intense debates and statewide votes, seven Deep South cotton states passed secession ordinances by February 1861 (before Abraham Lincoln took office as president), while secession efforts failed in the other eight slave states. Delegates from those seven formed the C.S.A. in February 1861, selecting Jefferson Davis as the provisional president. Unionist talk of reunion failed and Davis began raising a 100,000 man army.
States.
Initially, some secessionists may have hoped for a peaceful departure. Moderates in the Confederate Constitutional Convention included a provision against importation of slaves from Africa to appeal to the Upper South. Non-slave states might join, but the radicals secured a two-thirds hurdle for them.
Seven states declared their secession from the United States before Lincoln took office on March 4, 1861. After the Confederate attack on Fort Sumter April 12, 1861 and Lincoln's subsequent call for troops on April 15, four more states declared their secession:
Kentucky declared neutrality but after Confederate troops moved in, the state government asked for Union troops to drive them out. The splinter Confederate state government relocated to accompany western Confederate armies and never controlled the state population.
In Missouri, a constitutional convention was approved and delegates elected by voters. The convention rejected secession 89-1 on March 19, 1861. However, the governor maneuvered to take control of the St. Louis Arsenal and restrict Federal movements. This led to confrontation and in June Federal forces drove him and the General Assembly from Jefferson City. The executive committee of the constitutional convention called the members together in July. The convention declared the state offices vacant, and appointed a Unionist interim state government. The exiled governor called a rump session of the former General Assembly together in Neosho and, on October 31, 1861, passed an ordinance of secession. It is still a matter of debate as to whether a quorum existed for this vote. The Confederate state government was unable to control very much Missouri territory. It had its capital first at Neosho, then at Cassville, before being driven out of the state. For the remainder of the war, it operated as a government in exile at Marshall, Texas.
Neither Kentucky nor Missouri were declared in rebellion in Lincoln's Emancipation Proclamation. The Confederacy recognized the pro-Confederate claimants in both Kentucky and Missouri and laid claim to those states, granting them Congressional representation and adding two stars to the Confederate flag. Voting for the representatives was mostly done by Confederate soldiers from Kentucky and Missouri.
The order of secession resolutions and dates are:
1. South Carolina (December 20, 1860) 
2. Mississippi (January 9, 1861) 
3. Florida (January 10) 
4. Alabama (January 11) 
5. Georgia (January 19) 
6. Louisiana (January 26) 
7. Texas (February 1; referendum February 23) 
– Ft. Sumter (April 12) and Lincoln's call up (April 15) –
8. Virginia (April 17; referendum May 23, 1861) 
9. Arkansas (May 6) 
10. Tennessee (May 7; referendum June 8) 
11. North Carolina (May 20) 
In Virginia the populous counties along the Ohio and Pennsylvania borders rejected the Confederacy. Unionists held a Convention in Wheeling in June 1861, establishing a "restored government" with a rump legislature, but sentiment in the region remained deeply divided. In the 50 counties that would make up the state of West Virginia, voters from 24 counties had voted for disunion in Virginia's May 23 referendum on the ordinance of secession. In the 1860 Presidential election "Constitutional Democrat" Breckenridge had outpolled "Constitutional Unionist" Bell in the 50 counties by 1,900 votes, 44% to 42%. Regardless of scholarly disputes over election procedures and results county by county, altogether they simultaneously supplied over 20,000 soldiers to each side of the conflict. Representatives for most of the counties were seated in both state legislatures at Wheeling and at Richmond for the duration of the war.
Attempts to secede from the Confederacy by some counties in East Tennessee were checked by martial law.
Although slave-holding Delaware and Maryland did not secede, citizens from those states exhibited divided loyalties. Maryland regiments fought in Lee's Army of Northern Virginia. Delaware never produced a full regiment for the Confederacy, but neither did it emancipate slaves as did Missouri and West Virginia. District of Columbia citizens made no attempts to secede and through the war years, Lincoln-sponsored referendums approved systems of compensated emancipation and slave confiscation from "disloyal citizens".
Territories.
Citizens at Mesilla and Tucson in the southern part of New Mexico Territory formed a secession convention, which voted to join the Confederacy on March 16, 1861, and appointed Lewis Owings as the new territorial governor. They won the Battle of Mesilla and established a territorial government with Mesilla serving as its capital. The Confederacy proclaimed the Confederate Arizona Territory on February 14, 1862 north to the 34th parallel. Marcus H. MacWillie served in both Confederate Congresses as Arizona's delegate. In 1862 the Confederate New Mexico Campaign to take the northern half of the U.S. territory failed and the Confederate territorial government in exile relocated to San Antonio, Texas.
Confederate supporters in the trans-Mississippi west also claimed portions of United States Indian Territory after the United States evacuated the federal forts and installations. Over half of the American Indian troops participating in the Civil War from the Indian Territory supported the Confederacy; troops and one general were enlisted from each tribe. On July 12, 1861, the Confederate government signed a treaty with both the Choctaw and Chickasaw Indian nations. After several battles Northern armies moved back into the territory.
Indian Territory was never formally ceded into the Confederacy by American Indian councils, but like Missouri and Kentucky, the Five Civilized Nations received representation in the Confederate Congress and their citizens were integrated into regular Confederate Army units. After 1863 the tribal governments sent representatives to the Confederate Congress: Elias Cornelius Boudinot representing the Cherokee and Samuel Benton Callahan representing the Seminole and Creek people. The Cherokee Nation, aligning with the Confederacy, alleged northern violations of the Constitution, waging war against slavery commercial and political interests, abolishing slavery in the Indian Territory, and that the North intended to seize additional Indian lands.
Capitals.
Montgomery, Alabama served as the capital of the Confederate States of America from February 4 until May 29, 1861 in the Alabama State Capitol. Six states created the Confederate States of America there on February 8, 1861. The Texas delegation was seated at the time, so it is counted in the "original seven" states of the Confederacy. But it had no roll call vote until after its referendum made secession "operative". Two sessions of the Provisional Congress were held in Montgomery, adjourning May 21. The Permanent Constitution was adopted there on March 12, 1861.
The permanent capital provided for in the Confederate Constitution called for a state cession of a ten-miles square (100 square mile) district to the central government. Atlanta, which had not yet supplanted Milledgeville, Georgia as its state capital, put in a bid noting its central location and rail connections, as did Opelika, Alabama, noting its strategically interior situation, rail connections and nearby deposits of coal and iron.
Richmond, Virginia was chosen for the interim capital at the Virginia State Capitol. The move was used by Vice President Stephens and others to encourage other border states to follow Virginia into the Confederacy. In the political moment it was a show of "defiance and strength". The war for southern independence was surely to be fought in Virginia, but it also had the largest Southern military-aged white population, with infrastructure, resources and supplies required to sustain a war. The Davis Administration's policy was that, "It must be held at all hazards."
The naming of Richmond as the new capital took place on May 30, 1861, and the last two sessions of the Provisional Congress were held in the new capital. The Permanent Confederate Congress and President were elected in the states and army camps on November 6, 1861. The First Congress met in four sessions in Richmond from February 18, 1862 to February 17, 1864. The Second Congress met there in two sessions, from May 2, 1864 to March 18, 1865.
As war dragged on, Richmond became crowded with training and transfers, logistics and hospitals. Prices rose dramatically despite government efforts at price regulation. A movement in Congress led by Henry S. Foote of Tennessee argued for moving the capital from Richmond. At the approach of Federal armies in early summer 1862, the government's archives were readied for removal. As the Wilderness Campaign progressed, Congress authorized Davis to remove the executive department and call Congress to session elsewhere in 1864 and again in 1865. Shortly before the end of the war, the Confederate government evacuated Richmond, planning to relocate farther south. Little came of these plans before Lee's surrender at Appomattox Court House, Virginia on April 9, 1865. Davis and most of his cabinet fled to Danville, Virginia, which served as the last Confederate capital for about one week.
Unionism.
Unionism was widespread in the Confederacy, especially in the mountain regions of Appalachia and the Ozarks. Unionists, led by Parson Brownlow and Senator Andrew Johnson, took control of eastern Tennessee in 1863. Unionists also attempted control over western Virginia but never effectively held more than half the counties that formed the new state of West Virginia. Union forces captured parts of coastal North Carolina, and at first were welcomed by local unionists. That changed quickly as the occupiers appeared oppressive, callous, radical and favorable to the Freedmen. Occupiers engaged in pillaging, confiscation of slaves, and eviction of those refusing to take or reneging on the loyalty oaths, as ex-Unionists began to support the Confederate cause.
Support for the Confederacy was perhaps weakest in Texas; Elliott estimates that only a third of the population in early 1861 supported the Confederacy. Many unionists supported the Confederacy after the war began, but many others clung to their unionism throughout the war, especially in the northern counties, the German districts, and the Mexican areas. Local officials harassed unionists and engaged in large-scale massacres against unionists and Germans. In Cooke County 150 suspected unionists were arrested; 25 were lynched without trial and 40 more were hanged after a summary trial. Draft resistance was widespread especially among Texans of Germans or Mexican descent; many of the latter went to Mexico. Potential draftees went into hiding, Confederate officials hunted them down, and many were shot.
However, Elliot's conclusions have been widely disputed by many Texas Civil War historians on the grounds its premises deliberately attempt to blur the lines between actual active support for the Union as in being a northern sympathizer—which was a tiny minority—and the failure to back every measure undertaken by the central Confederate government by otherwise loyal Confederates, was tantamount to giving passive support to the Union. When, in fact, the two cannot be combined; that the overwhelming majority of white Texans supported the Confederacy throughout the war.
Civil liberties were of small concern in North and South. Lincoln and Davis both took a hard line against dissent. Neely explores how the Confederacy became a virtual police state with guards and patrols all about, and a domestic passport system whereby everyone needed official permission each time they wanted to travel. Over 4000 suspected unionists were imprisoned without trial.
Diplomacy.
United States, a foreign power.
During the four years of its existence under trial by war, the Confederate States of America asserted its independence and appointed dozens of diplomatic agents abroad. The United States government regarded the southern states in rebellion and so refused any formal recognition of their status.
Even before Fort Sumter, U.S. Secretary of State William H. Seward issued formal instructions to the American minister to Britain, Charles Francis Adams:
If the British seemed inclined to recognize the Confederacy, or even waver in that regard, Seward instructed Adams they were to receive a sharp warning, with a strong hint of war: "[if Britain is] tolerating the application of the so-called seceding States, or wavering about it, [they cannot] remain friends with the United States ... if they determine to recognize [the Confederacy], [Britain] may at the same time prepare to enter into alliance with the enemies of this republic."
The United States government never declared war on those "kindred and countrymen" in the Confederacy, but conducted its military efforts beginning with a presidential proclamation issued April 15, 1861. It called for troops to recapture forts and suppress what Lincoln later called an "insurrection and rebellion."
Mid-war parlays between the two sides occurred without formal political recognition, though the laws of war predominantly governed military relationships on both sides of uniformed conflict.
On the part of the Confederacy, immediately following Fort Sumter the Confederate Congress proclaimed "... war exists between the Confederate States and the Government of the United States, and the States and Territories thereof ..." A state of war was not to formally exist between the Confederacy and those states and territories in the United States allowing slavery, although Confederate Rangers were compensated for destruction they could effect there throughout the war.
Concerning the international status and nationhood of the Confederate States of America, in 1869 the United States Supreme Court in "Texas v. White" ruled Texas' declaration of secession was legally null and void. Jefferson Davis, former President of the Confederacy, and Alexander Stephens, its former Vice-President, both wrote postwar arguments in favor of secession's legality and the international legitimacy of the Government of the Confederate States of America, most notably Davis' "The Rise and Fall of the Confederate Government".
International diplomacy.
Once the war with the United States began, the Confederacy pinned its hopes for survival on military intervention by Great Britain and France. The Confederates who had believed that "cotton is king"—that is, Britain had to support the Confederacy to obtain cotton—proved mistaken. The British had stocks to last over a year and had been developing alternative sources of cotton, most notably India and Egypt. They were not about to go to war with the U.S. to acquire more cotton at the risk of losing the large quantities of food imported from the North. The Confederate government sent repeated delegations to Europe but historians give them low marks for their poor diplomacy. James M. Mason went to London and John Slidell traveled to Paris. They were unofficially interviewed, but neither secured official recognition for the Confederacy.
In late 1861 illegal actions of the U.S. Navy in seizing a British ship outraged Britain and led to a war scare in the Trent Affair. Recognition of the Confederacy seemed at hand, but Lincoln released the two detained Confederate diplomats, tensions cooled, and the Confederacy gained no advantage.
Throughout the early years of the war, British foreign secretary Lord John Russell, Emperor Napoleon III of France, and, to a lesser extent, British Prime Minister Lord Palmerston, showed interest in recognition of the Confederacy or at least mediation of the war. The Union victory at the Battle of Antietam (Sharpsburg) and abolitionist opposition in Britain put an end to these plans. The cost to Britain of a war with the U.S. would have been high: the immediate loss of American grain shipments, the end of exports to the U.S., the seizure of billions of pounds invested in American securities. War would have meant higher taxes, another invasion of Canada, and full-scale worldwide attacks on the British merchant fleet. While outright recognition would have meant certain war with the United States, in the summer of 1862 fears of race war as had transpired in Haiti led to the British considering intervention for humanitarian reasons. Lincoln's Emancipation Proclamation did not lead to interracial violence let alone a bloodbath, but it did give the friends of the Union strong talking points in the arguments that raged across Britain.
John Slidell, emissary to France, did succeed in negotiating a loan of $15,000,000 from Erlanger and other French capitalists. The money was used to buy ironclad warships, as well as military supplies that came in by blockade runners.
The British government did allow blockade runners to be built in Britain and operated by British seamen. Several European nations maintained diplomats in place who had been appointed to the U.S., but no country appointed any diplomat to the Confederacy. However, those nations did recognize the Union and Confederate sides as belligerents. In 1863, the Confederacy expelled the European diplomatic missions for advising their resident subjects to refuse to serve in the Confederate army. Both Confederate and Union agents were allowed to work openly in British territories. Some state governments in northern Mexico negotiated local agreements to cover trade on the Texas border. Pope Pius IX wrote a letter to Jefferson Davis in which he addressed Davis as the "Honorable President of the Confederate States of America." The Confederacy appointed Ambrose Dudley Mann as special agent to the Holy See on September 24, 1863. But the Holy See never released a formal statement supporting or recognizing the Confederacy.
The Confederacy was seen internationally as a serious attempt at nationhood, and European governments sent military observers, both official and unofficial, to assess the "de facto" establishment of independence. These included Arthur Freemantle of the British Coldstream Guards, Fitzgerald Ross of the Austrian Hussars and Justus Scheibert of the Prussian Army. European travelers visited and wrote accounts for publication. Importantly in 1862, the Frenchman Charles Girard's "Seven months in the rebel states during the North American War" testified "this government ... is no longer a trial government ... but really a normal government, the expression of popular will".
French Emperor Napoleon III assured Confederate diplomat John Slidell that he would make "direct proposition" to Britain for joint recognition. The Emperor made the same assurance to Members of Parliament John A. Roebuck and John A. Lindsay. Roebuck in turn publicly prepared a bill to submit to Parliament June 30 supporting joint Anglo-French recognition of the Confederacy. Preparations for Lee's incursion into Pennsylvania were underway to influence the midterm U.S. elections. Confederate independence and nationhood was at a turning point. "Southerners had a right to be optimistic, or at least hopeful, that their revolution would prevail, or at least endure". The result was a defeat at Gettysburg and Lee barely escaped to Virginia, withdrawing into an interior defensive position. Following the dual reverses at Vicksburg and Gettysburg, the Confederates "suffered a severe loss of confidence in themselves." There would be no help from the Europeans.
By December 1864, Davis considered sacrificing slavery in order to enlist recognition and aid from Paris and London; he secretly sent Duncan F. Kenner to Europe with a message that the war was fought solely for "the vindication of our rights to self-government and independence" and that "no sacrifice is too great, save that of honor." The message stated that if the French or British governments made their recognition conditional on anything at all, the Confederacy would consent to such terms. Davis's message could not explicitly acknowledge that slavery was on the bargaining table due to still-strong domestic support for slavery among the wealthy and politically influential. Europe, however, could see that the Confederacy was on the verge of total defeat.
The Confederacy at war.
Motivations of soldiers.
The great majority of young white men voluntarily joined Confederate national or state military units. Perman (2010) says historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:
Military strategy.
Southern Civil War historian E. Merton Coulter noted that for those who would secure its independence, "The Confederacy was unfortunate in its failure to work out a general strategy for the whole war". Aggressive strategy called for offensive force concentration. Defensive strategy sought dispersal to meet demands of locally minded governors. The controlling philosophy evolved into a combination "dispersal with a defensive concentration around Richmond". The Davis administration considered the war purely defensive, a "simple demand that the people of the United States would cease to war upon us." Northern historian James M. McPherson is a critic of Lee's Offensive Strategy: "Lee pursued a faulty military strategy that ensured Confederate defeat".
As the Confederate government lost control of territory in campaign after campaign, it was said that "the vast size of the Confederacy would make its conquest impossible". The enemy would be struck down by the same elements which so often debilitated or destroyed visitors and transplants in the South. Heat exhaustion, sunstroke, endemic diseases such as malaria and typhoid would match the destructive effectiveness of the Moscow winter on the invading armies of Napoleon.
But despite the Confederacy's essentially defensive stance, in the early stages of the war there were offensive visions of seizing the Rocky Mountains or cutting the North in two by marching to Lake Erie. Then, at a time when both sides believed that one great battle would decide the conflict, the Confederate won a great victory at the First Battle of Bull Run, also known as First Manassas (the name used by Confederate forces). It drove the Confederate people "insane with joy", the public demanded a forward movement to capture Washington DC, relocate the Capital there, and admit Maryland to the Confederacy. A council of war by the victorious Confederate generals decided not to advance against larger numbers of fresh Federal troops in defensive positions. Davis did not countermand it. Following the Confederate incursion halted at the Battle of Antietam, (Sharpsburg), in October 1862 generals proposed concentrating forces from state commands to re-invade the north. Nothing came of it. Again in early 1863 at his incursion into Pennsylvania, Lee requested of Davis that Beauregard simultaneously attack Washington with troops taken from the Carolinas. But the troops there remained in place during the Gettysburg Campaign.
In terms of white men the eleven states of the Confederacy were outnumbered by the North about four to one in terms of men of military age. It was overmatched far more in military equipment, industrial facilities, railroads for transport, and wagons supplying the front.
Confederate military policy innovated to slow the invaders, but at heavy cost to the Southern infrastructure. The Confederates burned bridges, laid land mines in the roads, and made harbors inlets and inland waterways unusable with sunken mines (called "torpedos at the time). Coulter reports:
The Confederacy relied on external sources for war materials. The first came from trade with the enemy. "Vast amounts of war supplies" came through Kentucky, and thereafter, western armies were "to a very considerable extent" provisioned with illicit trade via Federal agents and northern private traders. But that trade was interrupted in the first year of war by Admiral Porter's river gunboats as they gained dominance along navigable rivers north–south and east–west. Overseas blockade running then came to be of "outstanding importance". On April 17, President Davis called on privateer raiders, the "militia of the sea", to make war on U.S. seaborne commerce. Despite noteworthy effort, over the course of the war the Confederacy was found unable to match the Union in ships and seamanship, materials and marine construction.
Perhaps the most implacable obstacle to success in the 19th century warfare of mass armies was the Confederacy's lack of manpower, sufficient numbers of disciplined, equipped troops in the field at the point of contact with the enemy. During the wintering of 1862–1863, Lee observed that none of his famous victories had resulted in the destruction of the opposing army. He lacked reserve troops to exploit an advantage on the battlefield as Napoleon had done. Lee explained, "More than once have most promising opportunities been lost for want of men to take advantage of them, and victory itself had been made to put on the appearance of defeat, because our diminished and exhausted troops have been unable to renew a successful struggle against fresh numbers of the enemy."
Armed forces.
The military armed forces of the Confederacy comprised three branches: Army, Navy and Marine Corps.
The Confederate military leadership included many veterans from the United States Army and United States Navy who had resigned their Federal commissions and had won appointment to senior positions in the Confederate armed forces. Many had served in the Mexican-American War (including Robert E. Lee and Jefferson Davis), but some such as Leonidas Polk (who graduated from West Point but did not serve in the Army) had little or no experience.
The Confederate officer corps consisted of men from both slave-owning and non-slave-owning families. The Confederacy appointed junior and field grade officers by election from the enlisted ranks. Although no Army service academy was established for the Confederacy, some colleges (such as The Citadel and Virginia Military Institute) maintained cadet corps that trained Confederate military leadership. A naval academy was established at Drewry's Bluff, Virginia in 1863, but no midshipmen graduated before the Confederacy's end.
The soldiers of the Confederate armed forces consisted mainly of white males aged between 16 and 28. The median year of birth was 1838, so half the soldiers were 23 or older by 1861. In early 1862, the Confederate Army was allowed to disintegrate for two months following expiration of short term enlistments. A majority of those in uniform would not re-enlist following their one-year commitment, so on April 16, 1862 the Confederate Congress enacted the first mass conscription on the North American continent. (The U.S. Congress would follow a year later on March 3, 1863 with the Enrollment Act.) Rather than a universal draft, the initial program was a selective service with physical, religious, professional and industrial exemptions. These were narrowed as the war progressed. Initially substitutes were permitted, but by December 1863 these were disallowed. In September 1862 the age limit was increased from 35 to 45 and by February 1864, all men under 18 and over 45 were conscripted to form a reserve for state defense inside state borders. By March 1864, the Superintendent of Conscription reported that all across the Confederacy, every officer in constituted authority, man and woman, "engaged in opposing the enrolling officer in the execution of his duties." Although challenged in the state courts, the Confederate State Supreme Courts routinely rejected legal challenges to conscription.
Many thousands of slaves served as laborers, cooks, and pioneers. Some freed blacks and men of color served in local state militia units of the Confederacy, primarily in Louisiana and South Carolina, but their officers deployed them for "local defense, not combat." Depleted by casualties and desertions, the military suffered chronic manpower shortages. In the spring of 1865, the Confederate Congress, influenced by the public support by General Lee, approved the recruitment of black infantry units. Contrary to Lee's and Davis's recommendations, the Congress refused "to guarantee the freedom of black volunteers." No more than two hundred black combat troops were ever raised.
Raising troops.
The immediate onset of war meant that it was fought by the "Provisional" or "Volunteer Army". State governors resisted concentrating a national effort. Several wanted a strong state army for self-defense. Others feared large "Provisional" armies answering only to Davis. When filling the Confederate government's call for 100,000 men, another 200,000 were turned away by accepting only those enlisted "for the duration" or twelve-month volunteers who brought their own arms or horses.
It was important to raise troops; it was just as important to provide capable officers to command them. With few exceptions the Confederacy secured excellent general officers. Efficiency in the lower officers was "greater than could have been reasonably expected". As with the Federals, political appointees could be indifferent. Otherwise, the officer corps was governor-appointed or elected by unit enlisted. Promotion to fill vacancies was made internally regardless of merit, even if better officers were immediately available.
Anticipating the need for more "duration" men, in January 1862 Congress provided for company level recruiters to return home for two months, but their efforts met little success on the heels of Confederate battlefield defeats in February. Congress allowed for Davis to require numbers of recruits from each governor to supply the volunteer shortfall. States responded by passing their own draft laws.
The veteran Confederate army of early 1862 was mostly twelve-month volunteers with terms about to expire. Enlisted reorganization elections disintegrated the army for two months. Officers pleaded with the ranks to re-enlist, but a majority did not. Those remaining elected majors and colonels whose performance led to officer review boards in October. The boards caused a "rapid and widespread" thinning out of 1700 incompetent officers. Troops thereafter would elect only second lieutenants.
In early 1862, the popular press suggested the Confederacy required a million men under arms. But veteran soldiers were not re-enlisting, and earlier secessionist volunteers did not reappear to serve in war. One Macon, Georgia, newspaper asked how two million brave fighting men of the South were about to be overcome by four million northerners who were said to be cowards.
Conscription.
The Confederacy passed the first American law of national conscription on April 16, 1862. The white males of the Confederate States from 18 to 35 were declared members of the Confederate army for three years, and all men then enlisted were extended to a three-year term. They would serve only in units and under officers of their state. Those under 18 and over 35 could substitute for conscripts, in September those from 35 to 45 became conscripts. The cry of "rich man's war and a poor man's fight" led Congress to abolish the substitute system altogether in December 1863. All principals benefiting earlier were made eligible for service. By February 1864, the age bracket was made 17 to 50, those under eighteen and over forty-five to be limited to in-state duty.
Confederate conscription was not universal; it was actually a selective service. The First Conscription Act of April 1862 exempted occupations related to transportation, communication, industry, ministers, teaching and physical fitness. The Second Conscription Act of October 1862 expanded exemptions in industry, agriculture and conscientious objection. Exemption fraud proliferated in medical examinations, army furloughs, churches, schools, apothecaries and newspapers.
Rich men's sons were appointed to the socially outcast "overseer" occupation, but the measure was received in the country with "universal odium". The legislative vehicle was the controversial Twenty Negro Law that specifically exempted one white overseer or owner for every plantation with at least 20 slaves. Backpedalling six months later, Congress provided overseers under 45 could be exempted only if they held the occupation before the first Conscription Act. The number of officials under state exemptions appointed by state Governor patronage expanded significantly. By law, substitutes could not be subject to conscription, but instead of adding to Confederate manpower, unit officers in the field reported that over-50 and under-17-year-old substitutes made up to 90% of the desertions.
The Conscription Act of February 1864 "radically changed the whole system" of selection. It abolished industrial exemptions, placing detail authority in President Davis. As the shame of conscription was greater than a felony conviction, the system brought in "about as many volunteers as it did conscripts." Many men in otherwise "bombproof" positions were enlisted in one way or another, nearly 160,000 additional volunteers and conscripts in uniform. Still there was shirking. To administer the draft, a Bureau of Conscription was set up to use state officers, as state Governors would allow. It had a checkered career of "contention, opposition and futility". Armies appointed alternative military "recruiters" to bring in the out-of-uniform 17–50-year-old conscripts and deserters. Nearly 3000 officers would be tasked with the job. By fall 1864, Lee was calling for more troops. "Our ranks are constantly diminishing by battle and disease, and few recruits are received; the consequences are inevitable." By March 1865 conscription was to be administered by generals of the state reserves calling out men over 45 and under 18 years old. All exemptions were abolished. These regiments were assigned to recruit conscripts ages 17–50, recover deserters, and repel enemy cavalry raids. The service retained men who had lost but one arm or a leg in home guards. April 1865 Lee surrendered an army of 50,000. Conscription had been a failure.
The survival of the Confederacy depended on a strong base of civilians and soldiers devoted to victory. The soldiers performed well, though increasing numbers deserted in the last year of fighting, and the Confederacy never succeeded in replacing casualties as the Union could. The civilians, although enthusiastic in 1861–62, seem to have lost faith in the future of the Confederacy by 1864, and instead looked to protect their homes and communities. As Rable explains, "This contraction of civic vision was more than a crabbed libertarianism; it represented an increasingly widespread disillusionment with the Confederate experiment."
Victories: 1861.
The American Civil War broke out in April 1861 with the Battle of Fort Sumter in Charleston. In December 1860, Federal troops had withdrawn to the island fort from others in Charleston Harbor soon after South Carolina's declaration of secession to avoid soldier-civilian street confrontations.
In January, President James Buchanan had attempted to resupply the garrison with the "Star of the West", but Confederate artillery drove it away. In March, President Lincoln notified Governor Pickens that without Confederate resistance to resupply there would be no military reinforcement without further notice, but Lincoln prepared to force resupply if it were not allowed. Confederate President Davis in cabinet decided to capture Fort Sumter before the relief fleet arrived and on April 12, 1861, General Beauregard forced their surrender.
Following Fort Sumter, Lincoln directed states to provide 75,000 troops for three months to recapture the Charleston Harbor forts and all other federal property that had been seized without Congressional authorization. In May, Federal troops crossed into Confederate territory along the entire border from the Chesapeake Bay to New Mexico. The Confederate victory at Fort Sumter was followed by Confederate victories at the battles of Big Bethel, (Bethel Church) VA in June, First Bull Run, (First Manassas) in July and in August, Wilson's Creek, (Oak Hills) in southwest Missouri. At all three, Confederate forces could not follow up their victory due to inadequate supply and shortages of fresh troops to exploit their successes. Following each battle, Federals maintained a military presence and their occupation of Washington DC, Fort Monroe VA and Springfield MO. Both North and South began training up armies for major fighting the next year.
Confederate commerce-raiding just south of the Chesapeake Bay was ended in August at the loss of Hatteras NC. Early November a Union expedition at sea secured Port Royal and Beaufort SC south of Charleston, seizing Confederate-burned cotton fields along with escaped and owner-abandoned "contraband" field hands. December saw the loss of Georgetown SC north of Charleston. Federals there began a war-long policy of burning grain supplies up rivers into the interior wherever they could not occupy.
Incursions: 1862.
The victories of 1861 were followed by a series of defeats east and west in early 1862. To restore the Union by military force the Federal intent was to (1) secure the Mississippi River, (2) seize or close Confederate ports and (3) march on Richmond. To secure independence, the Confederate intent was to (1) repel the invader on all fronts, costing him blood and treasure and (2) carry the war into the north by two offensives in time to impact the mid-term elections.
Much of northwestern Virginia was under Federal control.
In February and March, most of Missouri and Kentucky were Union "occupied, consolidated, and used as staging areas for advances further South". Following the repulse of Confederate counter-attack at the Battle of Shiloh, (Pittsburg Landing) Tennessee, permanent Federal occupation expanded west, south and east. Confederate forces then repositioned south along the Mississippi River to Memphis, where at the naval Battle of Memphis its River Defense Fleet was sunk and Confederates then withdrew from northern Mississippi and northern Alabama. New Orleans was captured April 29 by a combined Army-Navy force under U.S. Admiral Farragut, and the Confederacy lost control of the mouth of the Mississippi River, conceding large agricultural resources that supported the Union's sea-supplied logistics base.
Although Confederates had suffered major reverses everywhere but Virginia, as of the end of April the Confederacy still controlled 72% of its population. Federal forces disrupted Missouri and Arkansas; they had broken through in western Virginia, Kentucky, Tennessee and Louisiana. Along the Confederacy's shores it had closed ports and made garrisoned lodgments on every coastal Confederate state but Alabama and Texas. Although scholars sometimes assess the Union blockade as ineffectual under international law until the last few months of the war, from the first months it disrupted Confederate privateers making it "almost impossible to bring their prizes into Confederate ports". Nevertheless, British firms developed small fleets of blockade running companies, such as John Fraser and Company and the Ordnance Department secured its own blockade runners for dedicated munitions cargos.
The Civil War saw the advent of fleets of armored warships deployed in sustained blockades at sea. After some success against the Union blockade, in March the ironclad "CSS Virginia" was forced into port and burned by Confederates at their retreat. Despite several attempts mounted from their port cities, C.S. naval forces were unable to break the Union blockade including Commodore Josiah Tattnall's ironclads from Savannah, in 1862 with the "CSS Atlanta". Secretary of the Navy Stephen Mallory placed his hopes in a European-built ironclad fleet, but they were never realized. On the other hand, four new English-built commerce raiders saw Confederate service, and several fast blockade runners were sold in Confederate ports, then converted into commerce-raiding cruisers, manned by their British crews.
In the east, Union forces could not close on Richmond. General McClellan landed his army on the Lower Peninsula of Virginia. Lee subsequently ended that threat from the east, then Union General John Pope attacked overland from the north only to be repulsed at Second Bull Run, (Second Manassas). Lee's strike north was turned back at Antietam MD, then Union Major General Ambrose Burnside's offensive was disastrously ended at Fredericksburg VA in December. Both armies then turned to winter quarters to recruit and train for the coming spring.
In an attempt to seize the initiative, reprovision, protect farms in mid-growing season and influence U.S. Congressional elections, two major Confederate incursions into Union territory had been launched in August and September 1862. Both Braxton Bragg's invasion of Kentucky and Lee's invasion of Maryland were decisively repulsed, leaving Confederates in control of but 63% of its population. Civil War scholar Allan Nevins argues that 1862 was the strategic high-water mark of the Confederacy. The failures of the two invasions were attributed to the same irrecoverable shortcomings: lack of manpower at the front, lack of supplies including serviceable shoes, and exhaustion after long marches without adequate food.
Anaconda: 1863–1864.
The failed Middle Tennessee campaign was ended January 2, 1863 at the inconclusive Battle of Stones River, (Murfreesboro), both sides losing the largest percentage of casualties suffered during the war. It was followed by another strategic withdrawal by Confederate forces. The Confederacy won a significant victory April 1863, repulsing the Federal advance on Richmond at Chancellorsville, but the Union consolidated positions along the Virginia coast and the Chesapeake Bay.
Without an effective answer to Federal gunboats, river transport and supply, the Confederacy lost the Mississippi River following the capture of Vicksburg, Mississippi, and Port Hudson in July, ending Southern access to the trans-Mississippi West. July brought short-lived counters, Morgan's Raid into Ohio and the New York City draft riots. Robert E. Lee's strike into Pennsylvania was repulsed at Gettysburg, Pennsylvania despite Pickett's famous charge and other acts of valor. Southern newspapers assessed the campaign as "The Confederates did not gain a victory, neither did the enemy."
September and November left Confederates yielding Chattanooga, Tennessee, the gateway to the lower south. For the remainder of the war fighting was restricted inside the South, resulting in a slow but continuous loss of territory. In early 1864, the Confederacy still controlled 53% of its population, but it withdrew further to reestablish defensive positions. Union offensives continued with Sherman's March to the Sea to take Savannah and Grant's Wilderness Campaign to encircle Richmond and besiege Lee's army at Petersburg.
In April 1863, the C.S. Congress authorized a uniformed Volunteer Navy, many of whom were British. Wilmington and Charleston had more shipping while "blockaded" than before the beginning of hostilities. The Confederacy had altogether eighteen commerce destroying cruisers, which seriously disrupted Federal commerce at sea and increased shipping insurance rates 900 percent. Commodore Tattnall unsuccessfully attempted to break the Union blockade on the Savannah River GA with an ironclad again in 1863. However beginning April 1864 the ironclad CSS Albemarle engaged Union gunboats and sank or cleared them for six months on the Roanoke River NC. The Federals closed Mobile Bay by sea-based amphibious assault in August, ending Gulf coast trade east of the Mississippi River. In December, the Battle of Nashville ended Confederate operations in the western theater.
The National Library of Medicine holds a collection of papers from Confederate hospitals and medical service during wartime.
Collapse: 1865.
The first three months of 1865 saw the Federal Carolinas Campaign, devastating a wide swath of the remaining Confederate heartland. The "breadbasket of the Confederacy" in the Great Valley of Virginia was occupied by Philip Sheridan. The Union Blockade captured Fort Fisher NC, and Sherman finally took Charleston SC by land attack.
The Confederacy controlled no ports, harbors or navigable rivers. Railroads were captured or had ceased operating. Its major food producing regions had been war-ravaged or occupied. Its administration survived in only three pockets of territory holding one-third its population. Its armies were defeated or disbanding. At the February 1865 Hampton Roads Conference with Lincoln, senior Confederate officials rejected his invitation to restore the Union with compensation for emancipated slaves. The three pockets of unoccupied Confederacy were southern Virginia-North Carolina, central Alabama-Florida, and Texas. The Davis policy was independence or nothing, while Lee's army was wracked by disease and desertion, barely holding the trenches defending Jefferson Davis' capital.
The Confederacy's last remaining blockade-running port, Wilmington, North Carolina, was lost. When the Union broke through Lee's lines at Petersburg, Richmond fell immediately. Lee surrendered the Army of Northern Virginia at Appomattox Court House, Virginia, on April 9, 1865. "The Surrender" marked the end of the Confederacy.
The CSS Stonewall sailed from Europe to break the Union blockade in March; on making Havana, Cuba it surrendered. Some high officials escaped to Europe, but President Davis was captured May 10; all remaining Confederate forces surrendered by June 1865. The U.S. Army took control of the Confederate areas without post-surrender insurgency or guerrilla warfare against them, but peace was subsequently marred by a great deal of local violence, feuding and revenge killings.
Historian Gary Gallagher concluded that the Confederacy capitulated in the spring of 1865 because northern armies crushed "organized southern military resistance." The Confederacy's population, soldier and civilian, had suffered material hardship and social disruption. They had expended and extracted a profusion of blood and treasure until collapse; "the end had come". Jefferson Davis' assessment in 1890 determined, "With the capture of the capital, the dispersion of the civil authorities, the surrender of the armies in the field, and the arrest of the President, the Confederate States of America disappeared ... their history henceforth became a part of the history of the United States."
Texas v. White.
In "Texas v. White", the United States Supreme Court ruled—by a 5-3 majority that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States of America. In this case, the court held that the Constitution did not permit a state to unilaterally secede from the United States. Further, that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution. This case decided one of the "central constitutional questions" of the Civil War. The Union is perpetual and indestructible, as a matter of constitutional law until amended. In declaring that no state could leave the Union, it was "explicitly repudiating the position of the Confederate states that the United States was a voluntary compact between sovereign states”.
Theories regarding the Confederacy's demise.
"Died of states' rights".
Historian Frank Lawrence Owsley argued that the Confederacy "died of states' rights." The central government was denied requisitioned soldiers and money by governors and state legislatures because they feared that Richmond would encroach on the rights of the states. Georgia's governor Joseph Brown warned of a secret conspiracy by Jefferson Davis to destroy states' rights and individual liberty. The first conscription act in North America authorizing Davis to draft soldiers was said to be the "essence of military despotism."
Vice President Alexander Stephens feared losing the very form of republican government. Allowing President Davis to threaten "arbitrary arrests" to draft hundreds of governor-appointed "bomb-proof" bureaucrats conferred "more power than the English Parliament had ever bestowed on the king. History proved the dangers of such unchecked authority." The abolishment of draft exemptions for newspaper editors was interpreted as an attempt by the Confederate government to muzzle presses, such as the Raleigh NC "Standard", to control elections and to suppress the peace meetings there. As Rable concludes, "For Stephens, the essence of patriotism, the heart of the Confederate cause, rested on an unyielding commitment to traditional rights" without considerations of military necessity, pragmatism or compromise.
In 1863 governor Pendleton Murrah of Texas determined that state troops were required for defense against Plains Indians and Union forces that might attack from Kansas. He refused to send his soldiers to the East. Governor Zebulon Vance of North Carolina showed intense opposition to conscription, limiting recruitment success. Vance's faith in states' rights drove him into repeated, stubborn opposition to the Davis administration.
Despite political differences within the Confederacy, no national political parties were formed because they were seen as illegitimate. "Anti-partyism became an article of political faith." Without a two-party system building alternative sets of national leaders, electoral protests tended to be narrowly state-based, "negative, carping and petty". The 1863 mid-term elections became mere expressions of futile and frustrated dissatisfaction. According to historian David M. Potter, this lack of a functioning two-party system caused "real and direct damage" to the Confederate war effort since it prevented the formulation of any effective alternatives to the conduct of the war by the Davis administration.
"Died of Davis".
The enemies of President Davis proposed that the Confederacy "died of Davis." He was unfavorably compared to George Washington by critics such as Edward Alfred Pollard, editor of the most influential newspaper the "Richmond Examiner." Coulter summarizes, "The American Revolution had its Washington; the Southern Revolution had its Davis ... one succeeded and the other failed." Besides the early honeymoon period, Davis was never popular. He unwittingly caused much internal dissention from early on. His ill health and temporary bouts of blindness disabled him for days at a time.
Coulter says Davis was heroic and his will was indomitable. But his "tenacity, determination, and will power" stirred up lasting opposition of enemies Davis could not shake. He failed to overcome "petty leaders of the states" who made the term "Confederacy" into a label for tyranny and oppression, denying the "Stars and Bars" from becoming a symbol of larger patriotic service and sacrifice. Instead of campaigning to develop nationalism and gain support for his administration, he rarely courted public opinion, assuming an aloofness, "almost like an Adams".
Escott argues that Davis was unable to effectively mobilize Confederate nationalism in support of his government, and especially failed to appeal to the small farmers who comprise the bulk of the population. In addition to the problems caused by states rights, Escott also emphasizes that the widespread opposition to any strong central government combined with the vast difference in wealth between the slaveowning class and the small farmers created insolvable dilemmas when the Confederate survival presupposed a strong central government backed by a united populace. The prewar claim that white solidarity was necessary to provide a unified Southern voice in Washington no longer held. Davis failed to build a network of supporters who would speak up when he came under criticism, and he repeatedly alienated governors and other state-based leaders by demanding centralize control of the war effort.
Davis was not an effective administrator. He attended to too many details. He protected his friends after their failures were obvious. He spent too much time on military affairs versus his civil responsibilities. Coulter concludes he was not the ideal leader for the Southern Revolution, but he showed "fewer weaknesses than any other" contemporary character available for the role. Robert E. Lee's assessment of Davis as President was, "I knew of none that could have done as well."
Government and politics.
Constitution.
The Southern leaders met in Montgomery, Alabama, to write their constitution. Much of the Confederate States Constitution replicated the United States Constitution verbatim, but it contained several explicit protections of the institution of slavery including provisions for the recognition and protection of negro slavery in any new state admitted to the Confederacy. It maintained the existing ban on international slave-trading while protecting the existing internal trade of slaves among slaveholding states.
In certain areas, the Confederate Constitution gave greater powers to the states (or curtailed the powers of the central government more) than the U.S. Constitution of the time did, but in other areas, the states actually lost rights they had under the U.S. Constitution. Although the Confederate Constitution, like the U.S. Constitution, contained a commerce clause, the Confederate version prohibited the central government from using revenues collected in one state for funding internal improvements in another state. The Confederate Constitution's equivalent to the U.S. Constitution's general welfare clause prohibited protective tariffs (but allowed tariffs for providing domestic revenue), and spoke of "carry[ing] on the Government of the Confederate States" rather than providing for the "general welfare". State legislatures had the power to impeach officials of the Confederate government in some cases. On the other hand, the Confederate Constitution contained a Necessary and Proper Clause and a Supremacy Clause that essentially duplicated the respective clauses of the U.S. Constitution. The Confederate Constitution also incorporated each of the 12 amendments to the U.S. Constitution that had been ratified up to that point.
The Confederate Constitution did not specifically include a provision allowing states to secede; the Preamble spoke of each state "acting in its sovereign and independent character" but also of the formation of a "permanent federal government". During the debates on drafting the Confederate Constitution, one proposal would have allowed states to secede from the Confederacy. The proposal was tabled with only the South Carolina delegates voting in favor of considering the motion. The Confederate Constitution also explicitly denied States the power to bar slaveholders from other parts of the Confederacy from bringing their slaves into any state of the Confederacy or to interfere with the property rights of slave owners traveling between different parts of the Confederacy. In contrast with the language of the United States Constitution, the Confederate Constitution overtly asked God's blessing ("... invoking the favor and guidance of Almighty God ...").
Executive.
The Montgomery Convention to establish the Confederacy and its executive met February 4, 1861. Each state as a sovereignty had one vote, with the same delegation size as it held in the U.S. Congress, and generally 41 to 50 members attended. Offices were "provisional", limited to a term not to exceed one year. One name was placed in nomination for president, one for vice president. Both were elected unanimously, 6–0.
Jefferson Davis was elected provisional president. His U.S. Senate resignation speech greatly impressed with its clear rationale for secession and his pleading for a peaceful departure from the Union to independence. Although he had made it known that he wanted to be commander-in-chief of the Confederate armies, when elected, he assumed the office of Provisional President. Three candidates for provisional Vice President were under consideration the night before the February 9 election. All were from Georgia, and the various delegations meeting in different places determined two would not do, so Alexander Stephens was elected unanimously provisional Vice President, though with some privately held reservations. Stephens was inaugurated February 11, Davis February 18.
Davis and Stephens were elected President and Vice President, unopposed on November 6, 1861. They were inaugurated on February 22, 1862.
Historian E. M. Coulter observed, "No president of the U.S. ever had a more difficult task." Washington was inaugurated in peacetime. Lincoln inherited an established government of long standing. The creation of the Confederacy was accomplished by men who saw themselves as fundamentally conservative. Although they referred to their "Revolution", it was in their eyes more a counter-revolution against changes away from their understanding of U.S. founding documents. In Davis' inauguration speech, he explained the Confederacy was not a French-like revolution, but a transfer of rule. The Montgomery Convention had assumed all the laws of the United States until superseded by the Confederate Congress.
The Permanent Constitution provided for a President of the Confederate States of America, elected to serve a six-year term but without the possibility of re-election. Unlike the United States Constitution, the Confederate Constitution gave the president the ability to subject a bill to a line item veto, a power also held by some state governors.
The Confederate Congress could overturn either the general or the line item vetoes with the same two-thirds majorities that are required in the U.S. Congress. In addition, appropriations not specifically requested by the executive branch required passage by a two-thirds vote in both houses of Congress. The only person to serve as president was Jefferson Davis, due to the Confederacy being defeated before the completion of his term.
Legislative.
The only two "formal, national, functioning, civilian administrative bodies" in the Civil War South were the Jefferson Davis administration and the Confederate Congresses. The Confederacy was begun by the Provisional Congress in Convention at Montgomery, Alabama on February 28, 1861. It had one vote per state in a unicameral assembly.
The Permanent Confederate Congress was elected and began its first session February 18, 1862. The Permanent Congress for the Confederacy followed the United States forms with a bicameral legislature. The Senate had two per state, twenty-six Senators. The House numbered 106 representatives apportioned by free and slave populations within each state. Two Congresses sat in six sessions until March 18, 1865.
The political influences of the civilian, soldier vote and appointed representatives reflected divisions of political geography of a diverse South. These in turn changed over time relative to Union occupation and disruption, the war impact on local economy, and the course of the war. Without political parties, key candidate identification related to adopting secession before or after Lincoln's call for volunteers to retake Federal property. Previous party affiliation played a part in voter selection, predominantly secessionist Democrat or unionist Whig.
The absence of political parties made individual roll call voting all the more important, as the Confederate "freedom of roll-call voting [was] unprecedented in American legislative history. Key issues throughout the life of the Confederacy related to (1) suspension of habeas corpus, (2) military concerns such as control of state militia, conscription and exemption, (3) economic and fiscal policy including impressment of slaves, goods and scorched earth, and (4) support of the Jefferson Davis administration in its foreign affairs and negotiating peace.
Provisional Congress 
For the first year, the unicameral Provisional Confederate Congress functioned as the Confederacy's legislative branch.
President of the Provisional Congress
Presidents pro tempore of the Provisional Congress
Sessions of the Confederate Congress
Tribal Representatives to Confederate Congress
Judicial.
The Confederate Constitution outlined a judicial branch of the government, but the ongoing war and resistance from states-rights advocates, particularly on the question of whether it would have appellate jurisdiction over the state courts, prevented the creation or seating of the "Supreme Court of the Confederate States;" the state courts generally continued to operate as they had done, simply recognizing the Confederate States as the national government.
Confederate district courts were authorized by Article III, Section 1, of the Confederate Constitution, and President Davis appointed judges within the individual states of the Confederate States of America. In many cases, the same US Federal District Judges were appointed as Confederate States District Judges. Confederate district courts began reopening in the spring of 1861 handling many of the same type cases as had been done before. Prize cases, in which Union ships were captured by the Confederate Navy or raiders and sold through court proceedings, were heard until the blockade of southern ports made this impossible. After a Sequestration Act was passed by the Confederate Congress, the Confederate district courts heard many cases in which enemy aliens (typically Northern absentee landlords owning property in the South) had their property sequestered (seized) by Confederate Receivers.
When the matter came before the Confederate court, the property owner could not appear because he was unable to travel across the front lines between Union and Confederate forces. Thus, the District Attorney won the case by default, the property was typically sold, and the money used to further the Southern war effort. Eventually, because there was no Confederate Supreme Court, sharp attorneys like South Carolina's Edward McCrady began filing appeals. This prevented their clients' property from being sold until a supreme court could be constituted to hear the appeal, which never occurred. Where Federal troops gained control over parts of the Confederacy and re-established civilian government, US district courts sometimes resumed jurisdiction.
Supreme Court – not established.
District Courts – judges
Post Office.
When the Confederacy was formed and its seceding states broke from the Union, it was at once confronted with the arduous task of providing its citizens with a mail delivery system, and, in the midst of the American Civil War, the newly formed Confederacy created and established the Confederate Post Office. One of the first undertakings in establishing the Post Office was the appointment of John H. Reagan to the position of Postmaster General, by Jefferson Davis in 1861, making him the first Postmaster General of the Confederate Post Office as well as a member of Davis' presidential cabinet. Through Reagan's resourcefulness and remarkable industry, he had his department assembled, organized and in operation before the other Presidential cabinet members had their departments fully operational.
When the war began, the US Post Office still delivered mail from the secessionist states for a brief period of time. Mail that was postmarked after the date of a state's admission into the Confederacy through May 31, 1861, and bearing US postage was still delivered. After this time, private express companies still managed to carry some of the mail across enemy lines. Later, mail that crossed lines had to be sent by 'Flag of Truce' and was allowed to pass at only two specific points. Mail sent from the South to the North states was received, opened and inspected at Fortress Monroe on the Virginia coast before being passed on into the U.S. mail stream. Mail sent from the North to the South passed at City Point, also in Virginia, where it was also inspected before being sent on.
With the chaos of the war, a working postal system was more important than ever for the Confederacy. The Civil War had divided family members and friends and consequently letter writing naturally increased dramatically across the entire divided nation, especially to and from the men who were away serving in an army. Mail delivery was also important for the Confederacy for a myriad of business and military reasons. Because of the Union blockade, basic supplies were always in demand and so getting mailed correspondence out of the country to suppliers was imperative to the successful operation of the Confederacy. Volumes of material have been written about the Blockade runners who evaded Union ships on blockade patrol, usually at night, and who moved cargo and mail in and out of the Confederate States throughout the course of the war. Of particular interest to students and historians of the American Civil War is "Prisoner of War mail" and "Blockade mail" as these items were often involved with a variety of military and other war time activities. The postal history of the Confederacy along with has helped historians document the various people, places and events that were involved in the American Civil War as it unfolded.
Civil liberties.
The Confederacy actively used the army to arrest people suspected of loyalty to the United States. Historian Mark Neely found 4,108 names of men arrested and estimated a much larger total. The Confederacy arrested pro-Union civilians in the South at about the same rate as the Union arrested pro-Confederate civilians in the North. Neely concludes:
Economy.
Political economy.
Most whites were subsistence farmers who traded their surpluses locally. The plantations of the South, with white ownership and an enslaved labor force, produced substantial wealth from cash crops. It supplied two-thirds of the world's cotton, which was in high demand for textiles, along with tobacco, sugar, and naval stores (such as turpentine). These raw materials were exported to factories in Europe and the Northeast. Planters reinvested their profits in more slaves and fresh land, for cotton and tobacco depleted the soil. There was little manufacturing or mining; shipping was controlled by outsiders.
The plantations that employed over three million black slaves were the principal source of wealth. Most were concentrated in "black belt" plantation areas (because few white families in the poor regions owned slaves.) For decades there had been widespread fear of slave revolts. During the war extra men were assigned to "home guard" patrol duty and governors sought to keep militia units at home for protection. No slave revolts took place anywhere during the war.
Slave labor was applied in industry in a limited way in the Upper South and in a few port cities. One reason for the regional lag in industrial development was "top-heavy income distribution". Mass production requires mass markets, and slave-labor living in packed-earth cabins, using self-made tools and outfitted with one suit of work clothes each year of inferior fabric, did not generate consumer demand to sustain local manufactures of any description in the same way a mechanized family farm of free labor did in the North. The Southern economy was "pre-capitalist" in that slaves were employed in the largest revenue producing enterprises, not free labor. That labor system as practiced in the American South encompassed paternalism, whether abusive or indulgent, and that meant labor management considerations apart from productivity.
Approximately 85% of both North and South white populations lived on family farms, both regions were predominantly agricultural, and mid-century industry in both was mostly domestic. But the Southern economy was uniquely pre-capitalist in its overwhelming reliance on the agriculture of cash crops to produce wealth. Southern cities and industries grew faster than ever before, but the thrust of the rest of the country's exponential growth elsewhere was toward urban industrial development along transportation systems of canals and railroads. The South was following the dominant currents of the American economic mainstream, but at a "great distance" as it lagged in the all-weather modes of transportation that brought cheaper, speedier freight shipment and forged new, expanding inter-regional markets.
A third count of southern pre-capitalist economy relates to the cultural setting. The South and southerners did not adopt a frenzied work ethic, nor the habits of thrift that marked the rest of the country. It had access to the tools of capitalism, but it did not adopt its culture. The Southern Cause as a national economy in the Confederacy was grounded in "slavery and race, planters and patricians, plain folk and folk culture, cotton and plantations".
National production.
The Confederacy started its existence as an agrarian economy with exports, to a world market, of cotton, and, to a lesser extent, tobacco and sugarcane. Local food production included grains, hogs, cattle, and gardens. The cash came from exports but the Southern people spontaneously stopped exports in spring 1861 to hasten the impact of "King Cotton." When the blockade was announced, commercial shipping practically ended (the ships could not get insurance), and only a trickle of supplies came via blockade runners. The cutoff of exports was an economic disaster for the South, rendering useless its most value properties, its plantations and their enslaved workers. Many planters kept growing cotton, which piled up everywhere, but most turned to food production. All across the region, the lack of repair and maintenance wasted away the physical assets.
The 11 states had produced $155 million in manufactured goods in 1860, chiefly from local grist-mills, and lumber, processed tobacco, cotton goods and naval stores such as turpentine. The main industrial areas were border cities such as Baltimore, Wheeling, Louisville and St. Louis, that were never under Confederate control. The government did set up munitions factories in the Deep South. Combined with captured munitions and those coming via blockade runners, the armies were kept minimally supplied with weapons. The soldiers suffered from reduced rations, lack of medicines, and the growing shortages of uniforms, shoes and boots. Shortages were much worse for civilians, and the prices of necessities steadily rose.
The Confederacy adopted a tariff or tax on imports of 15 per cent, and imposed it on all imports from other countries, including the United States. The tariff mattered little; the Union blockade minimized commercial traffic through the Confederacy's ports, and very few people paid taxes on goods smuggled from the North. The Confederate government in its entire history collected only $3.5 million in tariff revenue. The lack of adequate financial resources led the Confederacy to finance the war through printing money, which led to high inflation. The Confederacy underwent an economic revolution by centralization and standardization, but it was too little too late as its economy was systematically strangled by blockade and raids.
Transportation systems.
In peacetime, the extensive and connected systems of navigable rivers and coastal access allowed for cheap and easy transportation of agricultural products. The railroad system in the South had been built as a supplement to the navigable rivers to enhance the all-weather shipment of cash crops to market. They tied plantation areas to the nearest river or seaport and so made supply more dependable, lowered costs and increased profits. In the event of invasion, the vast geography of the Confederacy made logistics difficult for the Union. Wherever Union armies invaded, they assigned many of their soldiers to garrison captured areas and to protect rail lines.
At onset of the Civil War, the Southern rail network was disjointed and plagued by change in track gauge as well as lack of interchange. Locomotives and freight cars had fixed axles and could not roll on tracks of different gauges (widths). Railroads of different gauges leading to the same city required all freight to be off-loaded onto wagons to be transported to the connecting railroad station where it would await freight cars and a locomotive to proceed. These included Vicksburg, New Orleans, Montgomery, Wilmington and Richmond. In addition, most rail lines led from coastal or river ports to inland cities, with few lateral railroads. Due to this design limitation, the relatively primitive railroads of the Confederacy were unable to overcome the Union Naval Blockade of the South's crucial intra-coastal and river routes.
The Confederacy had no plan to expand, protect or encourage its railroads. Refusal to export the cotton crop in 1861 left railroads bereft of their main source of income. Many lines had to lay off employees; many critical skilled technicians and engineers were permanently lost to military service. For the early years of the war, the Confederate government had a hands-off approach to the railroads. Only in mid-1863 did the Confederate government initiate an national policy, and it was confined solely to aiding the war effort. Railroads came under the de facto control of the military. In contrast, U.S. Congress had authorized military administration of railroad and telegraph January 1862, imposed a standard gauge, and built railroads into the South using that gauge. Confederate reoccupation of territory by successful armies could not be resupplied directly by rail as they advanced. The C.S. Congress formally authorized military administration of railroads in February 1865.
In the last year before the end of the war, the Confederate railroad system stood permanently on the verge of collapse. There was no new equipment and raids on both sides systematically destroyed key bridges, as well as locomotives and freight cars. Spare parts were cannibalized; feeder lines were torn up to get replacement rails for trunk lines, and the heavy use of rolling stock wore them out.
Horses and mules.
The army was always short of horses and mules, and requisitioned them with dubious promissory notes given to local farmers and breeders. Union forces paid in real money and found ready sellers in the South. Horses were needed for cavalry and artillery. Mules pulled the wagons. The supply was undermined by an unprecedented epidemic of glanders, a fatal disease that baffled veterinarians. After 1863 the policy of the invading Union forces was to shoot all the local horses and mules they did not need to keep them out of Confederate hands. The Confederate armies and farmers experienced a growing shortage of horses and mules, which hurt the economy and the war effort. The South lost half its 2.5 million horses and mules; many farmers ended the war with none left. Army horses were used up by hard work, malnourishment, disease and battle wounds; their life expectancy was about seven months.
Financial instruments.
Both the individual Confederate states and later the Confederate government printed Confederate States of America dollars as paper currency in various denominations, with a total face value of $1.5 billion. Much of it was signed by the Treasurer Edward C. Elmore. Inflation became rampant as the paper money depreciated and eventually became worthless. The state governments and some localities printed their own paper money, adding to the runaway inflation. Many bills still exist, although in recent years counterfeit copies have proliferated.
The Confederate government initially wanted to finance its war mostly through tariffs on imports, export taxes, and voluntary donations of gold. However, after the spontaneous imposition of an embargo on cotton sales to Europe in 1861, these sources of revenue dried up and the Confederacy increasingly turned to issuing debt and printing money to pay for war expenses. The Confederate States politicians were worried about angering the general population with hard taxes. A tax increase might disillusion many Southerners, so the Confederacy resorted to printing more money. As a result, inflation increased and remained a problem for the southern states throughout the rest of the war.
At the time of their secession, the states (and later the Confederate government) took over the national mints in their territories: the Charlotte Mint in North Carolina, the Dahlonega Mint in Georgia, and the New Orleans Mint in Louisiana. During 1861, the first two produced small amounts of gold coinage, the latter half dollars. Since the mints used the current dies on hand, these issues remain indistinguishable from those minted by the Union. However, in New Orleans the Confederacy did use its own reverse design to strike four half dollars. US coinage was hoarded and did not have any general circulation. U.S. coinage was admitted as legal tender up to $10, as were English sovereigns, French Napoleons and Spanish and Mexican doubloons at a fixed rate of exchange. Confederate money was paper and postage stamps.
Food shortages and riots.
By summer 1861, the Union naval blockade virtually shut down the export of cotton and the import of manufactured goods. Food that formerly came overland was cut off.
Women had charge of making do. They cut back on purchases, brought out old spinning wheels and enlarged their gardens with peas and peanuts to provide clothing and food. They used ersatz substitutes when possible, but there was no real coffee and it was hard to develop a taste for the okra or chicory substitutes used. The households were severely hurt by inflation in the cost of everyday items and the shortages of food, fodder for the animals, and medical supplies for the wounded.
State governments pleaded with planters to grow less cotton and more food. Most refused, some believing that the Yankees would not or could not fight. When cotton prices soared in Europe, expectations were that Europe would soon intervene to break the blockade and make them rich. Neither proved true and the myth of omnipotent "King Cotton" died hard. The Georgia legislature imposed cotton quotas, making it a crime to grow an excess. But food shortages only worsened, especially in the towns.
The overall decline in food supplies, made worse by the inadequate transportation system, led to serious shortages and high prices in urban areas. When bacon reached a dollar a pound in 1864, the poor women of Richmond, Atlanta and many other cities began to riot; they broke into shops and warehouses to seize food. The women expressed their anger at ineffective state relief efforts, speculators, merchants and planters. As wives and widows of soldiers they were hurt by the inadequate welfare system.
Devastation by 1865.
By the end of the war deterioration of the Southern infrastructure was widespread. The number of civilian deaths is unknown. Most of the war was fought in Virginia and Tennessee, but every Confederate state was affected as well as Maryland, West Virginia, Kentucky, Missouri, and Indian Territory. Texas and Florida saw the least military action. Much of the damage was caused by military action, but most was caused by lack of repairs and upkeep, and by deliberately using up resources. Historians have recently estimated how much of the devastation was caused by military action. Military operations were conducted in 56% of 645 counties in nine Confederate states (excluding Texas and Florida). These counties contained 63% of the 1860 white population and 64% of the slaves. By the time the fighting took place, undoubtedly some people had fled to safer areas, so the exact population exposed to war is unknown.
The eleven Confederate states in the 1860 census had 297 towns and cities with 835,000 people; of these 162 with 681,000 people were at one point occupied by Union forces. Eleven were destroyed or severely damaged by war action, including Atlanta (with an 1860 population of 9,600), Charleston, Columbia, and Richmond (with prewar populations of 40,500, 8,100, and 37,900, respectively); the eleven contained 115,900 people in the 1860 census, or 14% of the urban South. Historians have not estimated what their actual population was when Union forces arrived. The number of people (as of 1860) who lived in the destroyed towns represented just over 1% of the Confederacy's 1860 population. In addition, 45 court houses were burned (out of 830). The South's agriculture was not highly mechanized. The value of farm implements and machinery in the 1860 Census was $81 million; by 1870, there was 40% less, worth just $48 million. Many old tools had broken through heavy use; new tools were rarely available; even repairs were difficult.
The economic losses affected everyone. Banks and insurance companies were mostly bankrupt. Confederate currency and bonds were worthless. The billions of dollars invested in slaves vanished. However, most debts were left behind. Most farms were intact but most had lost their horses, mules and cattle; fences and barns were in disrepair. Paskoff shows the loss of farm infrastructure was about the same whether or not fighting took place nearby. The loss of infrastructure and productive capacity meant that rural widows throughout the region faced not only the absence of able-bodied men, but a depleted stock of material resources that they could manage and operate themselves. During four years of warfare, disruption, and blockades, the South used up about half its capital stock. The North, by contrast, absorbed its material losses so effortlessly that it appeared richer at the end of the war than at the beginning.
The rebuilding would take years and was hindered by the low price of cotton after the war. Outside investment was essential, especially in railroads. One historian has summarized the collapse of the transportation infrastructure needed for economic recovery:
Effect on women and families.
About 250,000 men never came home, or 30% of all white men aged 18 to 40, in 1860. Widows who were overwhelmed often abandoned the farm and merged into the households of relatives, or even became refugees living in camps with high rates of disease and death. In the Old South, being an "old maid" was something of an embarrassment to the woman and her family. Now it became almost a norm. Some women welcomed the freedom of not having to marry. Divorce, while never fully accepted, became more common. The concept of the "New Woman" emerged—she was self-sufficient, independent, and stood in sharp contrast to the "Southern Belle" of antebellum lore.
Flags.
National flags.
The first official flag of the Confederate States of America—called the "Stars and Bars" – originally had seven stars, representing the first seven states that initially formed the Confederacy. As more states joined, more stars were added, until the total was 13 (two stars were added for the divided states of Kentucky and Missouri). However, during the First Battle of Bull Run, (First Manassas) it sometimes proved difficult to distinguish the Stars and Bars from the Union flag. To rectify the situation, a separate "Battle Flag" was designed for use by troops in the field. Also known as the "Southern Cross", many variations sprang from the original square configuration. Although it was never officially adopted by the Confederate government, the popularity of the Southern Cross among both soldiers and the civilian population was a primary reason why it was made the main color feature when a new national flag was adopted in 1863. This new standard—known as the "Stainless Banner" – consisted of a lengthened white field area with a Battle Flag canton. This flag too had its problems when used in military operations as, on a windless day, it could easily be mistaken for a flag of truce or surrender. Thus, in 1865, a modified version of the Stainless Banner was adopted. This final national flag of the Confederacy kept the Battle Flag canton, but shortened the white field and added a vertical red bar to the fly end.
Because of its depiction in the 20th-century and popular media, many people consider the rectangular battle flag with the dark blue bars as being synonymous with "the Confederate Flag". This flag, however, was never adopted as a Confederate national flag. The "Confederate Flag" has a color scheme similar to the official Battle Flag, but is rectangular, not square. (Its design and shape matches the Naval Jack, but the blue bars are darker.) The "Confederate Flag" is the most recognized symbol of the South in the United States today, and continues to be a controversial icon.
Geography.
Region and climate.
The Confederate States of America claimed a total of of coastline, thus a large part of its territory lay on the seacoast with level and often sandy or marshy ground. Most of the interior portion consisted of arable farmland, though much was also hilly and mountainous, and the far western territories were deserts. The lower reaches of the Mississippi River bisected the country, with the western half often referred to as the Trans-Mississippi. The highest point (excluding Arizona and New Mexico) was Guadalupe Peak in Texas at .
Climate
Much of the area claimed by the Confederate States of America had a humid subtropical climate with mild winters and long, hot, humid summers. The climate and terrain varied from vast swamps (such as those in Florida and Louisiana) to semi-arid steppes and arid deserts west of longitude 100 degrees west. The subtropical climate made winters mild but allowed infectious diseases to flourish. Consequently, on both sides more soldiers died from disease than were killed in combat, a fact hardly atypical of pre–World War I conflicts.
Demographics.
Population
The United States Census of 1860 gives a picture of the overall 1860 population of the areas that joined the Confederacy. Note that population-numbers exclude non-assimilated Indian tribes.
In 1860 the areas that later formed the 11 Confederate States (and including the future West Virginia) had 132,760 (1.46%) free blacks. Males made up 49.2% of the total population and females 50.8% (whites: 48.60% male, 51.40% female; slaves: 50.15% male, 49.85% female; free blacks: 47.43% male, 52.57% female).
Rural/urban configuration
The area claimed by the Confederate States of America consisted overwhelmingly of rural land. Few urban areas had populations of more than 1,000 – the typical county seat had a population of fewer than 500 people. Cities were rare. Of the twenty largest U.S. cities in the 1860 census, only New Orleans lay in Confederate territory – and the Union captured New Orleans in 1862. Only 13 Confederate-controlled cities ranked among the top 100 U.S. cities in 1860, most of them ports whose economic activities vanished or suffered severely in the Union blockade. The population of Richmond swelled after it became the Confederate capital, reaching an estimated 128,000 in 1864. Other Southern cities in the Border slave-holding states such as Baltimore MD, Washington DC, Wheeling VA/WV and Alexandria VA, Louisville KY, and St. Louis MO, never came under the control of the Confederate government.
The cities of the Confederacy included most prominently in order of size of population:
"(See also Atlanta in the Civil War, Charleston, South Carolina, in the Civil War, Nashville in the Civil War, New Orleans in the Civil War, Wilmington, North Carolina, in the American Civil War, and Richmond in the Civil War)."
Amnesty and treason issue.
When the war ended over 14,000 Confederates petitioned President Johnson for a pardon; he was generous in giving them out. He issued a general amnesty to all Confederate participants in the "late Civil War" in 1868. Congress passed additional Amnesty Acts in May 1866 with restrictions on office holding, and again in May 1872 lifting those restrictions. See Amnesty Act. There was a great deal of discussion in 1865 about bringing treason trials, especially against Jefferson Davis. There was no consensus in President Johnson's cabinet and there were no treason trials against anyone. In the case of Davis there was a strong possibility of acquittal which would be humiliating for the government.
Davis was indicted for treason but never tried; he was released from prison on bail in May 1867. The amnesty of December 25, 1868 by President Johnson removed any possibility of a trial for Davis.
Military leaders.
Military leaders of the Confederacy (with their state or country of birth and highest rank) included:
See also.
States below the Mason–Dixon line
Alabama
Arkansas
Delaware
Florida
Georgia
Kentucky
Louisiana
Maryland
Mississippi
Missouri
North Carolina
South Carolina
Tennessee
Texas
Virginia
West Virginia
Confederate government
Confederacy in Latin America & abroad
Confederacy in popular culture
Confederacy at war
Further reading.
Overviews and historiography.
Overviews
Historiography
Topical history.
Social history
Intellectual history
Political history
Foreign affairs
Economic history

</doc>
<doc id="7025" url="http://en.wikipedia.org/wiki?curid=7025" title="Cranberry">
Cranberry

Cranberries are a group of evergreen dwarf shrubs or trailing vines in the subgenus Oxycoccus of the genus "Vaccinium". Cranberry may also be the common name of the species Vaccinium oxycoccos, particularly in Britain. In some methods of classification, "Oxycoccus" is regarded as a genus in its own right. They can be found in acidic bogs throughout the cooler regions of the northern hemisphere..
Cranberries are low, creeping shrubs or vines up to long and in height; they have slender, wiry stems that are not thickly woody and have small evergreen leaves. The flowers are dark pink, with very distinct "reflexed" petals, leaving the style and stamens fully exposed and pointing forward. They are pollinated by bees. The fruit is a berry that is larger than the leaves of the plant; it is initially white, but turns a deep red when fully ripe. It is edible, with an acidic taste that can overwhelm its sweetness.
Cranberries are a major commercial crop in certain American states and Canadian provinces (see cultivation and uses below). Most cranberries are processed into products such as juice, sauce, jam, and sweetened dried cranberries, with the remainder sold fresh to consumers. Cranberry sauce is a traditional accompaniment to turkey at Thanksgiving dinners in the United States and Canada. 
Since the early 21st century within the global functional food industry, raw cranberries have been marketed as a "superfruit" due to their nutrient content and antioxidant qualities.
Species and description.
There are three to four species of cranberry, classified in two sections:
Cranberries are related to bilberries, blueberries, and huckleberries, all in "Vaccinium" subgenus "Vaccinium". These differ in having stouter, woodier stems forming taller shrubs, and in the bell-shaped flowers, the petals not being reflexed.
Some plants of the completely unrelated genus "Viburnum" are sometimes inaccurately called "highbush cranberries" ("Viburnum trilobum").
Cranberries are susceptible to false blossom, a harmful but controllable phytoplasma disease common in the eastern production areas of Massachusetts and New Jersey.
Etymology and history.
The name cranberry derives from "craneberry", first named by early European settlers in America who felt the expanding flower, stem, calyx, and petals resembled the neck, head, and bill of a crane. Another name used in northeastern Canada is mossberry. The traditional English name for "Vaccinium oxycoccos", , originated from plants found growing in fen (marsh) lands. In 17th-century New England cranberries were sometimes called "bearberries" as bears were often seen feeding on them.
In North America, Native Americans were the first to use cranberries as food. Native Americans used cranberries in a variety of foods, especially for pemmican, wound medicine, and dye. Calling the red berries Sassamanash, Algonquian peoples may have introduced cranberries to starving English settlers in Massachusetts who incorporated the berries into traditional Thanksgiving feasts. American Revolutionary War veteran Henry Hall is credited as first to farm cranberries in the Cape Cod town of Dennis around 1816. In the 1820s cranberries were shipped to Europe. Cranberries became popular for wild harvesting in the Nordic countries and Russia. In Scotland the berries were originally wild-harvested but, with the loss of suitable habitat, the plants have become so scarce that this is no longer done.
Cultivation.
Geography and bog method.
Cranberries are a major commercial crop in the U.S. states of Massachusetts, New Jersey, Oregon, Washington, and Wisconsin, as well as in the Canadian provinces of British Columbia, New Brunswick, Ontario, Nova Scotia, Prince Edward Island, Newfoundland and Quebec. British Columbia's Fraser River Valley region produces an annual volume of 17 million kg of cranberries from 1150 hectares, about 95% of total Canadian production. In the United States, Wisconsin is the leading producer of cranberries, with over half of U.S. production. Massachusetts is the second largest U.S. producer. Small volume production occurs in southern Argentina, Chile and the Netherlands.
Historically, cranberry beds were constructed in wetlands. Today's cranberry beds are constructed in upland areas with a shallow water table. The topsoil is scraped off to form dykes around the bed perimeter. Clean sand is hauled into a depth of four to eight inches. The surface is laser leveled flat to provide even drainage. Beds are frequently drained with socked tile in addition to the perimeter ditch. In addition to making it possible to hold water, the dykes allow equipment to service the beds without driving on the vines. Irrigation equipment is installed in the bed to provide irrigation for vine growth and for spring and autumn frost protection.
Cultivation.
Cranberry vines are propagated by moving vines from an established bed. The vines are spread on the surface of the sand of the new bed and pushed into the sand with a blunt disk. The vines are watered frequently during the first few weeks until roots form and new shoots grow. Beds are given frequent light application of nitrogen fertilizer during the first year. The cost of establishment for new cranberry beds is estimated to be about US$70,000 per hectare (approx. $28,300 per acre).
A common misconception about cranberry production is that the beds remain flooded throughout the year. During the growing season cranberry beds are not flooded, but are irrigated regularly to maintain soil moisture. Beds are flooded in the autumn to facilitate harvest and again during the winter to protect against low temperatures. In cold climates like Wisconsin, Maine, and eastern Canada, the winter flood typically freezes into ice, while in warmer climates the water remains liquid. When ice forms on the beds, trucks can be driven onto the ice to spread a thin layer of sand that helps to control pests and rejuvenate the vines. Sanding is done every three to five years.
Harvesting.
Cranberries are harvested in the fall when the fruit takes on its distinctive deep red color. This is usually in September through the first part of November. To harvest cranberries, the beds are flooded with six to eight inches of water above the vines. A harvester is driven through the beds to remove the fruit from the vines. For the past 50 years, water reel type harvesters have been used. Harvested cranberries float in the water and can be corralled into a corner of the bed and conveyed or pumped from the bed. From the farm, cranberries are taken to receiving stations where they are cleaned, sorted, and stored prior to packaging or processing.
Although most cranberries are wet-picked as described above, 5–10% of the US crop is still dry-picked. This entails higher labor costs and lower yield, but dry-picked berries are less bruised and can be sold as fresh fruit instead of having to be immediately frozen or processed. Originally performed with two-handed comb scoops, dry picking is today accomplished by motorized, walk-behind harvesters which must be small enough to traverse beds without damaging the vines.
White cranberry juice is made from regular cranberries that have been harvested after the fruits are mature, but before they have attained their characteristic dark red color. Yields are lower on beds harvested early and the early flooding tends to damage vines, but not severely.
Cranberries for fresh market are stored in shallow bins or boxes with perforated or slatted bottoms, which deter decay by allowing air to circulate. Because harvest occurs in late autumn, cranberries for fresh market are frequently stored in thick walled barns without mechanical refrigeration. Temperatures are regulated by opening and closing vents in the barn as needed. Cranberries destined for processing are usually frozen in bulk containers shortly after arriving at a receiving station.
Food uses.
As fresh cranberries are hard and bitter, about 95% of cranberries are processed and used to make cranberry juice and sauce. They are also sold dried and sweetened.
Cranberry juice is usually sweetened or blended with other fruit juices to reduce its natural tartness. Many cocktails, including the Cosmopolitan, are made with cranberry juice. At one teaspoon of sugar per ounce, cranberry juice cocktail is more highly sweetened than even soda drinks that have been linked to obesity.
Usually cranberries as fruit are cooked into a compote or jelly, known as cranberry sauce. Such preparations are traditionally served with roast turkey, as a staple of English Christmas dinners, and the Canadian and US holiday Thanksgiving. The berry is also used in baking (muffins, scones, cakes and breads). In baking it is often combined with orange or orange zest. Less commonly, innovative cooks use cranberries to add tartness to savory dishes such as soups and stews.
Fresh cranberries can be frozen at home, and will keep up to nine months; they can be used directly in recipes without thawing.
Cranberry wine is made in some of the cranberry-growing regions of the United States and Canada from either whole cranberries, cranberry juice or cranberry juice concentrate.
Potential health effects.
Urinary Tract Infections.
Two reviews of available research concluded that there is no evidence that cranberry compounds are effective in treating urinary tract infections. Long-term tolerance is also an issue.
Nutrients.
Raw cranberries have moderate levels of vitamin C, dietary fiber and the essential dietary mineral, manganese, (each nutrient having more than 10% of the Daily Value per 100 g serving; see right table) as well as other essential micronutrients in minor amounts.
Phytochemicals.
Raw cranberries are a source of polyphenols which are under active research for possible benefits to the cardiovascular system and immune system, and as anti-cancer agents, such as in isolated prostate cancer cells. However, it is uncertain whether polyphenols account for the benefits of diets rich in plant foods like cranberries.
Cranberry juice contains a high molecular weight non-dializable material that is under research for its potential to affect formation of plaque by "Streptococcus mutans" pathogens that cause tooth decay. Cranberry juice components are also being studied for possible effects on kidney stone formation.
Raw cranberries and cranberry juice are abundant food sources of polyphenols such as proanthocyanidins, flavonols and quercetin. These compounds have shown possible activity as anti-cancer agents in vitro. However, their effectiveness in humans remains unknown, and is limited by poor absorption into cells and rapid excretion.
Cranberry tannins may interact with proteins and possibly digestive enzymes, α-amylase and glucoamylase, which in turn may affect starch hydrolysis, as shown in one in vitro study.
Possible contraindications.
An autumn 2004 caution from the Committee on Safety of Medicines, the UK agency dealing with drug safety, advised patients taking warfarin not to drink cranberry juice after adverse effects (such as increased incidence of bruising) were reported, possibly resulting from the presence of salicylic acid native to polyphenol-rich plants such as the cranberry. However, during 2006–08, several reviews of case reports and pilot studies failed to confirm this effect, collectively indicating no statistically significant interaction between daily consumption of 250 mL cranberry juice and warfarin in the general population. A gene (VKORC1, CYP2C9) has been shown to change warfarin sensitivity. This gene may also contribute to bruising susceptibility as a result of cranberries for carriers of the gene.
A couple of possible cases of warfarin interaction with cranberry have been reported.
Marketing and economics.
History.
In 1550, James White Norwood made reference to Indians using cranberries. In James Rosier's book "The Land of Virginia" there is an account of Europeans coming ashore and being met with Indians bearing bark cups full of cranberries. In Plymouth, Massachusetts, there is a 1633 account of the husband of Mary Ring auctioning her cranberry-dyed petticoat for 16 shillings. In 1640's "Key Into the Language" Roger Williams described cranberries, referring to them as "bearberries" because bears ate them. In 1648, preacher John Elliott was quoted in Thomas Shepard's book "Clear Sunshine of the Gospel" with an account of the difficulties the Pilgrims were having in using the Indians to harvest cranberries as they preferred to hunt and fish. In 1663, the Pilgrim cookbook appears with a recipe for cranberry sauce. In 1667, New Englanders sent to King Charles 10 barrels of cranberries, 3 barrels of codfish and some Indian corn as a means of appeasement for his anger over their local coining of the Pine Tree shilling. In 1669, Captain Richard Cobb had a banquet in his house (to celebrate both his marriage to Mary Gorham and his election to the Convention of Assistance), serving wild turkey with sauce made from wild cranberries. In the 1672 book "New England Rarities Discovered" author John Josselyn described cranberries, writing:
"Sauce for the Pilgrims, cranberry or bearberry, is a small trayling plant that grows in salt marshes that are overgrown with moss. The berries are of a pale yellow color, afterwards red, as big as a cherry, some perfectly round, others oval, all of them hollow with sower [sic] astringent taste; they are ripe in August and September. They are excellent against the Scurvy. They are also good to allay the fervor of hoof diseases. The Indians and English use them mush, boyling [sic] them with sugar for sauce to eat with their meat; and it is a delicate sauce, especially with roasted mutton. Some make tarts with them as with gooseberries."
"The Compleat Cook's Guide", published in 1683, made reference to cranberry juice. In 1703, cranberries were served at the Harvard University commencement dinner. In 1787, James Madison wrote Thomas Jefferson in France for background information on constitutional government to use at the Constitutional Convention. Jefferson sent back a number of books on the subject and in return asked for a gift of apples, pecans and cranberries.
William Aiton, a Scottish botanist, included an entry for the cranberry in volume II of his 1789 work "Hortus Kewensis". He notes that "Vaccinium macrocarpon" (American cranberry) was cultivated by James Gordon in 1760.
In 1796, cranberries were served at the first celebration of the landing of the Pilgrims, and Amelia Simmons (an American orphan) wrote a book entitled "American Cookery" which contained a recipe for cranberry tarts. In 1816, Henry Hall first commercially grew cranberries in East Dennis, Massachusetts on Cape Cod. In 1843, Eli Howes planted his own crop of cranberries on Cape Cod, using the "Howes" variety. In 1847, Cyrus Cahoon planted a crop of "Early Black" variety near Pleasant Lake, Harwich, Massachusetts. In 1860, Edward Watson, a friend of Henry David Thoreau, wrote a poem called "The Cranberry Tart."
Cranberry sales in the United States have traditionally been associated with holidays of Thanksgiving and Christmas. Until the 1930s most of the crop was sold fresh.
In the U.S., large scale cranberry cultivation has been developed as opposed to other countries. American cranberry growers have a long history of cooperative marketing. As early as 1904, John Gaynor, a Wisconsin grower, and A.U. Chaney, a fruit broker from Des Moines, Iowa, organized Wisconsin growers into a cooperative called the Wisconsin Cranberry Sales Company to receive a uniform price from buyers. Growers in New Jersey and Massachusetts were also organized into cooperatives, creating the National Fruit Exchange that marketed fruit under the Eatmor brand. The success of cooperative marketing almost led to its failure. With consistent and high prices, area and production doubled between 1903 and 1917 and prices fell. In 1918, US$54,000 was spent on advertising, leading to US$1 million in increased sales.
With surplus cranberries and changing American households some enterprising growers began canning cranberries that were below-grade for fresh market. Competition between canners was fierce because profits were thin. The Ocean Spray cooperative was established in 1930 through a merger of three primary processing companies: Ocean Spray Preserving company, Makepeace Preserving Co, and Cranberry Products Co. The new company was called Cranberry Canners, Inc. and used the Ocean Spray label on their products. Since the new company represented over 90% of the market, it would have been illegal (cf. antitrust) had attorney John Quarles not found an exemption for agricultural cooperatives. Morris April Brothers were the producers of Eatmor brand cranberry sauce, in Tuckahoe, New Jersey; Morris April Brothers brought an action against Ocean Spray for violation of the Sherman Antitrust Act and won $200,000 in real damages plus triple damages, in 1958, just in time for the Great Cranberry Scare of 1959. As of 2006, about 65% of the North American industry belongs to the Ocean Spray cooperative. (The percentage may be slightly higher in Canada than in the U.S.)
A turning point for the industry occurred on November 9, 1959, when the secretary of the United States Department of Health, Education, and Welfare Arthur S. Flemming announced that some of the 1959 crop was tainted with traces of the herbicide aminotriazole. The market for cranberries collapsed and growers lost millions of dollars. However, the scare taught the industry that they could not be completely dependent on the holiday market for their products: they had to find year-round markets for their fruit. They also had to be exceedingly careful about their use of pesticides.
After the aminotriazole scare, Ocean Spray reorganized and spent substantial sums on product development. New products such as cranberry/apple juice blends were introduced, followed by other juice blends.
A Federal Marketing Order that is authorized to synchronize supply and demand was approved in 1962. The order has been renewed and modified slightly in subsequent years, but it has allowed for more stable marketing. The market order has been invoked during six crop years: 1962 (12%), 1963 (5%), 1970 (10%), 1971 (12%), 2000 (15%), and 2001 (35%). Even though supply still exceeds demand, there is little will to invoke the Federal Marketing Order out of the realization that any pullback in supply by U.S. growers would easily be filled by Canadian production.
Prices and production increased steadily during the 1980s and 1990s. Prices peaked at about $65.00 per barrel (29 ¢/kg—a cranberry barrel equals 100 pounds or 45.4 kg.) in 1996 then fell to $18.00 per barrel (8.2 ¢/kg) in 2001. The cause for the precipitous drop was classic oversupply. Production had outpaced consumption leading to substantial inventory in freezers or as concentrate.
Cranberry handlers (processors) include Ocean Spray, Cliffstar Corporation, Northland Cranberries Inc.[Sun Northland LLC], Clement Pappas & Co., Decas Cranberry Products as well as a number of small handlers and processors.
Cranberry Marketing Committee.
The Cranberry Marketing Committee of the United States of America is an organization that represents 100% of the United States cranberry handlers in four marketing order districts. The committee was established in 1963 as a Federal Marketing Order to safeguard the orderly supply of a quality product. The Cranberry Marketing Committee, based in Wareham, Massachusetts, represents 18 cranberry handlers which represents about 1,200 United States cranberry growers located in Oregon, Connecticut, Massachusetts, Michigan, Minnesota, New Jersey, New York, Rhode Island, Washington, and Wisconsin. 
The authority for the actions taken by the Cranberry Marketing Committee are provided in Chapter IX, Title 7, Code of Federal Regulations which is called the Federal Cranberry Marketing Order. The Order is part of the Agricultural Marketing Agreement Act of 1937, identifying cranberries as a commodity good that can be regulated by Congress. The Federal Cranberry Marketing Order has been altered over the years to expand the Cranberry Marketing Committees ability to develop projects in the United States and around the world. 
The Cranberry Marketing Committee currently runs promotional programs in the United States, South Korea, the Netherlands, Austria, Australia, Switzerland, France, Poland, Czech Republic, Germany, and Mexico.
Regulation.
Problems may arise with the lack of validation of a quantifying method for the quantification of A-type proanthocyanidins. For instance, in the case of cranberry extracts, it can be performed using several existing methods including for example the European Pharmacopoeia method, LC-MS or a modified 4-dimethylaminocinnamaldehyde (DMAC) colorimetric method. This can lead to difficulties in evaluating the real quality of extracts from different origins: assessments show that quality varies greatly from one commercial product to another.

</doc>
<doc id="7030" url="http://en.wikipedia.org/wiki?curid=7030" title="Code coverage">
Code coverage

In computer science, code coverage is a measure used to describe the degree to which the source code of a program is tested by a particular test suite. A program with high code coverage has been more thoroughly tested and has a lower chance of containing software bugs than a program with low code coverage. Many different metrics can be used to calculate code coverage; some of the most basic are the percent of program subroutines and the percent of program statements called during execution of the test suite.
Code coverage was among the first methods invented for systematic software testing. The first published reference was by Miller and Maloney in "Communications of the ACM" in 1963.
Coverage criteria.
To measure what percentage of code has been exercised by a test suite, one or more "coverage criteria" are used. Coverage criteria is usually defined as a rule or requirement, which test suite needs to satisfy. 
Basic coverage criteria.
There are a number of coverage criteria, the main ones being:
For example, consider the following C++ function:
Assume this function is a part of some bigger program and this program was run with some test suite. 
Condition coverage does not necessarily imply branch coverage. For example, consider the following fragment of code:
Condition coverage can be satisfied by two tests:
However, this set of tests does not satisfy branch coverage since neither case will meet the codice_5 condition.
Fault injection may be necessary to ensure that all conditions and branches of exception handling code have adequate coverage during testing.
Modified condition/decision coverage.
Combination of function coverage and branch coverage is sometimes also called
decision coverage. This criterion requires that every point of entry and exit in the program have been invoked at least once, and every decision in the program have taken on all possible outcomes at least once. In this context the decision is a boolean expression composed of conditions and zero or more boolean operators. This definition is not the same as branch coverage, however, some do use the term "decision coverage" as a synonym for "branch coverage".
Condition/decision coverage requires that both decision and condition coverage been satisfied. However, for safety-critical applications (e.g., for avionics software) it is often required that modified condition/decision coverage (MC/DC) be satisfied. This criterion extends condition/decision criteria with requirements that each condition should affect the decision outcome independently. For example, consider the following code:
The condition/decision criteria will be satisfied by the following set of tests:
However, the above tests set will not satisfy modified condition/decision coverage, since in the first test, the value of 'b' and in the second test the value of 'c' would not influence the output. So, the following test set is needed to satisfy MC/DC:
Multiple condition coverage.
This criterion requires that all combinations of conditions inside each decision are tested. For example, the code fragment from the previous section will require eight tests:
Parameter value coverage.
Parameter value coverage (PVC) requires that in a method taking parameters, all the common values for such parameters been considered. 
The idea is that all common possible values for a parameter are tested. For example, common values for a string are: 1) null, 2) empty, 3) whitespace (space, tabs, newline), 4) valid string, 5) invalid string, 6) single-byte string, 7) double-byte string. It may also be appropriate to use very long strings. Failure to test each possible parameter value may leave a bug. Testing only one of these could result in 100% code coverage as each line is covered, but as only one of seven options are tested, there is only 14.2% PVC.
Other coverage criteria.
There are further coverage criteria, which are used less often:
Safety-critical applications are often required to demonstrate that testing achieves 100% of some form of code coverage.
Some of the coverage criteria above are connected. For instance, path coverage implies decision, statement and entry/exit coverage. Decision coverage implies statement coverage, because every statement is part of a branch.
Full path coverage, of the type described above, is usually impractical or impossible. Any module with a succession of formula_1 decisions in it can have up to formula_2 paths within it; loop constructs can result in an infinite number of paths. Many paths may also be infeasible, in that there is no input to the program under test that can cause that particular path to be executed. However, a general-purpose algorithm for identifying infeasible paths has been proven to be impossible (such an algorithm could be used to solve the halting problem). Basis path testing is for instance a method of achieving complete branch coverage without achieving complete path coverage. 
Methods for practical path coverage testing instead attempt to identify classes of code paths that differ only in the number of loop executions, and to achieve "basis path" coverage the tester must cover all the path classes.
In practice.
The target software is built with special options or libraries and/or run under a special environment such that every function that is exercised (executed) in the program(s) is mapped back to the function points in the source code. This process allows developers and quality assurance personnel to look for parts of a system that are rarely or never accessed under normal conditions (error handling and the like) and helps reassure test engineers that the most important conditions (function points) have been tested. The resulting output is then analyzed to see what areas of code have not been exercised and the tests are updated to include these areas as necessary. Combined with other code coverage methods, the aim is to develop a rigorous, yet manageable, set of regression tests.
In implementing code coverage policies within a software development environment, one must consider the following:
Test engineers can look at code coverage test results to help them devise test cases and input or configuration sets that will increase the code coverage over vital functions. Two common forms of code coverage used by testers are statement (or line) coverage and branch (or edge) coverage. Line coverage reports on the execution footprint of testing in terms of which lines of code were executed to complete the test. Edge coverage reports which branches or code decision points were executed to complete the test. They both report a coverage metric, measured as a percentage. The meaning of this depends on what form(s) of code coverage have been used, as 67% branch coverage is more comprehensive than 67% statement coverage.
Generally, code coverage tools incur computation and logging in addition to the actual program thereby slowing down the application, so typically this analysis is not done in production. As one might expect, there are classes of software that cannot be feasibly subjected to these coverage tests, though a degree of coverage mapping can be approximated through analysis rather than direct testing.
There are also some sorts of defects which are affected by such tools. In particular, some race conditions or similar real time sensitive operations can be masked when run under code coverage environments; and conversely, and reliably, some of these defects may become easier to find as a result of the additional overhead of the testing code.
Usage in industry.
Code coverage is one consideration in the safety certification of avionics equipment. The guidelines by which avionics gear is certified by the Federal Aviation Administration (FAA) is documented in DO-178B and the recently released DO-178C.

</doc>
<doc id="7033" url="http://en.wikipedia.org/wiki?curid=7033" title="Caitlin Clarke">
Caitlin Clarke

Caitlin Clarke (May 3, 1952 – September 9, 2004) was an American theater and film actress best known for her role as Valerian in the 1981 fantasy film "Dragonslayer" and for her role as Charlotte Cardoza in the 1998–1999 Broadway musical "Titanic".
Biography.
Clarke was born Katherine Anne Clarke in Pittsburgh, the oldest of five sisters, the youngest of whom is Victoria Clarke. Her family moved to Sewickley when she was ten.
Clarke received her B.A. in theater arts from Mount Holyoke College in 1974 and her M.F.A. from the Yale School of Drama in 1978. During her final year at Yale Clarke performed with the Yale Repertory Theater in such plays as Tales from the Vienna Woods.
The first few years of Clarke's professional career were largely theatrical, apart from her role in Dragonslayer. After appearing in three Broadway plays in 1985, Clarke moved to Los Angeles for several years as a film and television actress. She returned to theater in the early 1990s, and to Broadway as Charlotte Cardoza in "Titanic".
Clarke was diagnosed with ovarian cancer in 2000. She returned to Pittsburgh to teach theater at the University of Pittsburgh and at the Pittsburgh Musical Theater's Rauh Conservatory as well as to perform in Pittsburgh theatre until her death on September 9, 2004.
Television.
Series: "Northern Exposure", "The Equalizer", "Once A Hero", "Moonlighting", "Sex And The City", "Law & Order" ("Menace", "Juvenile", "Stiff").
Movies: "Mayflower Madam" (1986), "Love, Lies and Murder" (1991), "The Stepford Husbands" (1996).
The episode, " The Witness", Matlock 1990

</doc>
<doc id="7034" url="http://en.wikipedia.org/wiki?curid=7034" title="Cruiser">
Cruiser

A cruiser is a type of warship. The term has been in use for several hundred years, and has had different meanings throughout this period. During the Age of Sail, the term "cruising" referred to certain kinds of missions – independent scouting, raiding or commerce protection – fulfilled by a frigate or sloop, which were the "cruising warships" of a fleet.
In the middle of the 19th century, "cruiser" came to be a classification for the ships intended for this kind of role, though cruisers came in a wide variety of sizes, from the small protected cruiser to armored cruisers which were as large (though not as powerful) as a battleship.
By the early 20th century, cruisers could be placed on a consistent scale of warship size, smaller than a battleship but larger than a destroyer. In 1922, the Washington Naval Treaty placed a formal limit on cruisers, which were defined as warships of up to 10,000 tons displacement carrying guns no larger than 8 inches in calibre. These limits shaped cruisers up until the end of World War II. The very large battlecruisers of the World War I era were now classified, along with battleships, as capital ships.
In the later 20th century, the obsolescence of the battleship left the cruiser as the largest and most powerful surface combatant. The role of the cruiser varied according to ship and navy, often including air defense, commerce raiding and shore bombardment. The U.S. Navy in the Cold War period built guided-missile cruisers primarily designed to provide air defense, while the navy of the USSR built battlecruisers with heavy anti-ship missiles designed to sink NATO carrier task forces.
Currently only three nations, the United States, Russia, and Peru ( is still in service with the Peruvian Navy), operate cruisers, though the line between cruisers and destroyers is once again blurred. New models of destroyers (for instance the ) are often larger and more powerful than cruiser classes they replace.
Early history.
The term "cruiser" or "cruizer" was first commonly used in the 17th century to refer to an independent warship. "Cruiser" meant the purpose or mission of a ship, rather than a category of vessel. However, the term was nonetheless used to mean a smaller, faster warship suitable for such a role. In the 17th century, the ship of the line was generally too large, inflexible, and expensive to be dispatched on long-range missions (for instance, to the Americas), and too strategically important to be put at risk of fouling and foundering by continual patrol duties.
The Dutch navy was noted for its cruisers in the 17th century, while the Royal Navy—and later French and Spanish navies—subsequently caught up in terms of their numbers and deployment. The British Cruiser and Convoy Acts were an attempt by mercantile interests in Parliament to focus the Navy on commerce defence and raiding with cruisers, rather than the more scarce and expensive ships of the line. During the 18th century the frigate became the preeminent type of cruiser. A frigate was a small, fast, long range, lightly armed (single gun-deck) ship used for scouting, carrying dispatches, and disrupting enemy trade. The other principal type of cruiser was the sloop, but many other miscellaneous types of ship were used as well.
Steam cruisers.
During the 19th century, navies began to use steam power for their fleets. The 1840s saw the construction of experimental steam-powered frigates and sloops. By the middle of the 1850s, the British and U.S. Navies were both building steam frigates with very long hulls and a heavy gun armament, for instance or .
The 1860s saw the introduction of the ironclad. The first ironclads were frigates, in the sense of having one gun deck; however, they were also clearly the most powerful ships in the navy, and were principally to serve in the line of battle. In spite of their great speed, they would have been wasted in a cruising role.
The French constructed a number of smaller ironclads for overseas cruising duties, starting with the , commissioned 1865. These "station ironclads" were the beginning of the development of the armored cruisers, a type of ironclad specifically for the traditional cruiser missions of fast, independent, raiding and patrol.
The first true armored cruiser was the Russian , completed in 1874, and followed by the British a few years later.
Until the 1890s armored cruisers were still built with masts for a full sailing rig, to enable them to operate far from friendly coaling stations.
Unarmored cruising warships, built out of wood, iron, steel or a combination of those materials, remained popular until towards the end of the 19th century. The ironclad's armor often meant that they were limited to short range under steam, and many ironclads were unsuited to long-range missions or for work in distant colonies. The unarmored cruiser - often a screw sloop or screw frigate - could continue in this role. Even though mid- or late-19th century cruisers typically carried up-to-date guns firing explosive shells, they were unable to face ironclads in combat. This was evidenced by the clash between , a modern British cruiser, and the Peruvian monitor "Huáscar". Even though the Peruvian vessel was obsolescent by the time of the encounter, it stood up well to roughly 50 hits from British shells.
Steel cruisers.
In the 1880s naval architects began to use steel as a material for construction and armament. A steel cruiser could be lighter and faster than one built of iron or wood. The "Jeune Ecole" school of naval doctrine suggested that a fleet of fast unprotected steel cruisers were ideal for commerce raiding, while the torpedo boat would be able to destroy an enemy battleship fleet.
Steel also offered the cruiser a way of acquiring the protection needed to survive in combat. Steel armor was considerably stronger, for the same weight, than iron. By putting a relatively thin layer of steel armor above the vital parts of the ship, and by placing the coal bunkers where they might stop shellfire, a useful degree of protection could be achieved without slowing the ship too much.
The first protected cruiser was the Chilean ship "Esmeralda". Produced by a shipyard at Elswick, in Britain, owned by Armstrong, she inspired a group of protected cruisers produced in the same yard and known as the "Elswick cruisers". Her forecastle, poop deck and the wooden board deck had been removed, replaced with an armored deck.
"Esmeralda"s armament consisted of fore and aft 10-inch (25.4 cm) guns and 6-inch (15.2 cm) guns in the midships positions. It could reach a speed of , and was propelled by steam alone. It also had a displacement of less than 3,000 tons. During the two following decades, this cruiser type came to be the inspiration for combining heavy artillery, high speed and low displacement.
Pre-dreadnought armored cruisers.
Steel also had an impact on the construction and role of armored cruisers. Steel meant that new designs of battleship, later known as pre-dreadnought battleships, would be able to combine firepower and armor with better endurance and speed than ever before. The armored cruisers of the 1890s greatly resembled the battleships of the day; they tended to carry slightly smaller main armament ( rather than 12-inch) and have somewhat thinner armor in exchange for a faster speed (perhaps rather than 18). Because of their similarity, the lines between battleships and armored cruisers became blurred.
Cruisers from 1900 to 1914.
Shortly after the turn of the 20th century there were difficult questions about the design of future cruisers. Modern armored cruisers, almost as powerful as battleships, were also fast enough to outrun older protected and unarmored cruisers. In the Royal Navy, Jackie Fisher cut back hugely on older vessels, including many cruisers of different sorts, calling them 'a miser's hoard of useless junk' that any modern cruiser would sweep from the seas.
Battlecruisers.
The growing size and power of the armored cruiser resulted in the battlecruiser, with an armament and size similar to the revolutionary new dreadnought battleship; the brainchild of British admiral Jackie Fisher. He believed that to ensure British naval dominance in its overseas colonial possessions, a fleet of large, fast, powerfully-armed vessels which would be able to hunt down and mop up enemy cruisers and armored cruisers with overwhelming fire superiority was needed. This type of vessel came to be known as the battlecruiser, and the first were commissioned into the Royal Navy in 1907. Germany and eventually Japan followed suit to build these vessels, replacing armored cruisers in most frontline roles. Battlecruisers were in many cases larger and more expensive than contemporary battleships.
Light cruisers.
At around the same time as the battlecruiser was developed, the distinction between the armored and the unarmored cruiser finally disappeared. By the British , it was possible for a small, fast cruiser to carry both belt and deck armor, particularly when turbine engines were adopted. These 'light armored cruisers' began to occupy the traditional cruiser role once it became clear that the battlecruiser squadrons were required to operate with the battle fleet.
Flotilla leaders.
Some light cruisers were built specifically to act as the leaders of flotillas of destroyers.
Auxiliary cruisers.
The auxiliary cruiser was a merchant ship hastily armed with small guns on the outbreak of war. Auxiliary cruisers were used to fill gaps in their long-range lines or provide escort for other cargo ships, although they generally proved to be useless in this role because of their low speed, feeble firepower and lack of armor. In both world wars the Germans also used small merchant ships armed with cruiser guns to surprise Allied merchant ships.
Some large liners were armed in the same way. In British service these were known as Armed Merchant Cruisers (AMC). The Germans and French used them in World War I as raiders because of their high speed (around 30 knots (56 km/h)), and they were used again as raiders in World War II by the Germans and Japanese. In both the First World War and in the early part of the Second, they were used as convoy escorts by the British.
World War I.
Cruisers were one of the workhorse types of warship during World War I.
Cruisers from 1919–1945.
Naval construction in the 1920s and 1930s was limited by international treaties designed to prevent the repetition of the Dreadnought arms race of the early 20th century. The Washington Naval Treaty of 1922 placed limits on the construction of ships with a displacement of 10,000 tons or more and an armament of greater than 8-inch (203 mm). A number of navies commissioned classes of cruisers at the top end of this limit.
The London Naval Treaty in 1930 then formalised the distinction between these 'heavy' cruisers and light cruisers: a 'heavy' cruiser was one with guns of 6.1-inch (155 mm) calibre or more. The Second London Naval Treaty attempted to reduce the tonnage of new cruisers to 8,000 or less, but this had little impact; Japan and Germany were not signatories, and navies had already begun to evade treaty limitations on warships.
Heavy cruisers.
The "heavy cruiser" was a type of cruiser, a naval warship designed for long range, high speed and an armament of naval guns roughly 8-inch (203 mm) in calibre. The first heavy cruisers were built in 1915, although it only became a widespread classification following the London Naval Treaty in 1930. The heavy cruiser's immediate precursors were the light cruiser designs of the 1910s and 1920s; the US 8-inch 'treaty cruisers' of the 1920s were originally classed as light cruisers until the London Treaty forced their redesignation. Heavy cruisers continued in use until after World War II.
German pocket battleships.
The German was a series of three "Panzerschiffe" ("armored ships"), a form of heavily armed cruiser, built by the German Reichsmarine in nominally accordance with restrictions imposed by the Treaty of Versailles. The class is named after the first ship of this class to be completed (the ). All three ships were launched between 1931 and 1934, and served with Germany's Kriegsmarine during World War II. During the war, they were reclassified as heavy cruisers.
The British press began referring to the vessels as pocket battleships, in reference to the heavy firepower contained in the relatively small vessels; they were considerably smaller than contemporary battleships, though at 28 knots, were slower than battlecruisers. And although their displacement and scale of armor protection was that of a heavy cruiser, they were armed with guns larger than the heavy cruisers of other nations. "Deutschland"-class ships continue to be called "pocket battleships" in some circles.
Anti-aircraft cruisers.
The development of the anti-aircraft cruiser began in 1935 when the Royal Navy re-armed and . Torpedo tubes and 6-inch (15 cm) low-angle guns were removed from these World War I light cruisers and replaced by ten 4-inch (10 cm) high-angle guns with appropriate fire-control equipment to provide larger warships with protection against high-altitude bombers.
A tactical shortcoming was recognized after completing six additional conversions of s. Having sacrificed anti-ship weapons for anti-aircraft armament, the converted anti-aircraft cruisers might need protection themselves against surface units. New construction was undertaken to create cruisers of similar speed and displacement with dual-purpose guns.
Dual-purpose guns offered good anti-aircraft protection with anti-surface capability for the traditional light cruiser role of defending capital ships from destroyers. The first purpose built anti-aircraft cruiser was the British , completed shortly before the beginning of World War II. The US Navy anti-aircraft cruisers (CLAA) were designed to match capabilities of the Royal Navy. Both "Dido" and "Atlanta" carried torpedo tubes.
The quick-firing dual-purpose gun anti-aircraft cruiser concept was embraced in several designs completed too late to see combat including and completed in 1948 and 1949, two s completed in 1953, and completed in 1955 and 1959, and , and completed between 1959 and 1961.
Most post–World War II cruisers were tasked with air defense roles. In the early 1950s, advances in aviation technology forced the move from anti-aircraft artillery to anti-aircraft missiles. Therefore most cruisers of today are equipped with surface-to-air missiles as their main armament. The modern equivalent of the anti-aircraft cruiser is the guided missile cruiser (CAG/CLG/CG/CGN).
Later 20th century.
The rise of air power during World War II dramatically changed the nature of naval combat. Even the fastest cruisers could not maneuver quickly enough to evade aerial attack, and aircraft now had torpedoes, allowing moderate-range standoff capabilities. This change led to the end of independent operations by single ships or very small task groups, and for the second half of the 20th century naval operations were based on very large fleets able to fend off all but the largest air attacks.
This has led most navies to change to fleets designed around ships dedicated to a single role, anti-submarine or anti-aircraft typically, and the large "generalist" ship has disappeared from most forces. The United States Navy, the Russian Navy, and the Peruvian Navy are the only remaining navies which operate cruisers. Italy used the until 2003; France operated a single helicopter cruiser until May 2010, , for training purposes only.
In the Soviet Navy, cruisers formed the basis of combat groups. In the immediate post-war era it built a fleet of large-gun ships, but replaced these fairly quickly with very large ships carrying huge numbers of guided missiles and anti-aircraft missiles. The most recent ships of this type, the four s, were built in the 1970s and 1980s. Two of the "Kirov" class are in refit until 2020, and one was scheduled to leave refit until 2012, with the in active service. Russia also operates three s and one "Admiral Kuznetsov"-class carrier which is officially designated as a cruiser.
Currently, the "Kirov"-class heavy missile cruisers are used for command purposes, as the "Petr Velikiy" is the flagship of the Northern Fleet. However, their air defense capabilities are still powerful, as shown by the array of Point defense missiles they carry, from 44 OSA-MA missiles to 196 9K311 Tor missiles. For longer range targets, the S-300 is used. For closer range targets, AK-630 or Kashtan CIWSs are used. Aside from that, "Kirov"s have 20 P-700 Granit missiles for anti-ship warfare. For target acquisition beyond the radar horizon, three helicopters can be used. Besides a vast array of armament, "Kirov"-class cruisers are also outfitted with many sensors and communications equipment, allowing them to lead the fleet.
The United States Navy has centered on the aircraft carrier since World War II. The s, built in the 1980s, were originally designed and designated as a class of destroyer, intended to provide a very powerful air-defense in these carrier-centered fleets. The ships were later redesignated largely as a public relations move, in order to highlight the capability of the Aegis combat system the ships were designed around.
In the years since the launch of in 1981 the class has received a number of upgrades that have dramatically improved its members' capabilities for anti-submarine and land attack (using the Tomahawk missile). Like their Soviet counterparts, the modern "Ticonderoga"s can also be used as the basis for an entire battle group. Their cruiser designation was almost certainly deserved when first built, as their sensors and combat management systems enable them to act as 'flagships' for a surface warship flotilla if no carrier is present, but newer ships rated as destroyers and also equipped with AEGIS approach them very closely in capability, and once more blur the line between the two classes. The Japanese and the South Korean are an example of this.
Aircraft cruisers.
From time to time, some navies have experimented with aircraft-carrying cruisers. One example is the Swedish . Another variant is the helicopter cruiser. The last example in service was the Soviet Navy's , whose last unit was converted to a pure aircraft carrier and sold to India as . The Russian Navy's is nominally designated as an aviation cruiser but otherwise resembles a standard medium aircraft carrier, albeit with a surface-to-surface missile battery. The Royal Navy's aircraft-carrying and the Italian Navy's aircraft-carrying vessels were originally designated 'through-deck cruisers', but have since been designated as small aircraft carriers. Similarly, the Japan Maritime Self-Defense Force's and "helicopter destroyers" are really more along the lines of helicopter cruisers in function and aircraft complement, but due to the Treaty of San Francisco, must be designated as destroyers.
Cruisers in service today.
Few cruisers remain operational in the world navies. Those that do are:
US Navy's "cruiser gap".
Prior to the introduction of the "Ticonderoga"s, the US Navy used odd naming conventions that left its fleet seemingly without many cruisers, although a number of their ships were cruisers in all but name. From the 1950s to the 1970s, US Navy "cruisers" were large vessels equipped with heavy offensive missiles (including the Regulus nuclear cruise missile) for wide-ranging combat against land-based and sea-based targets. All save one——were converted from World War II -, - and s.
"Frigates" under this scheme were almost as large as the cruisers and optimized for anti-aircraft warfare, although they were capable anti-surface warfare combatants as well. In the late 1960s, the US government perceived a "cruiser gap"—at the time, the US Navy possessed six ships designated as "cruisers", compared to 19 for the Soviet Union, even though the USN possessed at the time 21 "frigates" with equal or superior capabilities to the Soviet cruisers. Because of this, in 1975 the Navy performed a massive redesignation of its forces:
Also, a series of Patrol Frigates of the , originally designated PFG, were redesignated into the FFG line. The cruiser-destroyer-frigate realignment and the deletion of the Ocean Escort type brought the US Navy's ship designations into line with the rest of the world's, eliminating confusion with foreign navies. In 1980, the Navy's then-building DDG-47-class destroyers were redesignated as cruisers (CG-47 guided missile cruisers) to emphasize the additional capability provided by the ships' Aegis combat systems, and their flag facilities suitable for an admiral and his staff.
Museum Cruisers.
As of 2014, seven decommissioned cruisers have been saved from scrapping and exist world wide as museum ships. They are (London), (Belfast), (St. Petersburg), (Quincy, MA), (Buffalo, NY), (Philladelphia, PA) and the (Athens). For approximately 13 years (Bordeaux) was on display. However the organisation maintaining her closed and the ship was towed away to be scrapped at a future date. There is interest in preserving a in the future as ships of the class retire from active service.

</doc>
<doc id="7037" url="http://en.wikipedia.org/wiki?curid=7037" title="Chlamydia infection">
Chlamydia infection

Chlamydia infection (from the Greek, χλαμύδα meaning "cloak") is a common sexually transmitted infection in humans caused by the bacterium "Chlamydia trachomatis". The term "Chlamydia infection" can also refer to infection caused by any species belonging to the bacterial family "Chlamydiaceae". "C. trachomatis" is found only in humans. Chlamydia is a major infectious cause of human genital and eye disease.
Chlamydia infection is one of the most common sexually transmitted infections worldwide; it is estimated that about 1 million individuals in the United States are infected with chlamydia.
"C. trachomatis" is naturally found living only inside human cells. Chlamydia can be transmitted during vaginal, anal, or oral sex, and can be passed from an infected mother to her baby during childbirth. Between half and three-quarters of all women who have a chlamydia infection of the cervix (cervicitis) have no symptoms and do not know that they are infected. In men, infection of the urethra (urethritis) is usually symptomatic, causing a white discharge from the penis with or without pain on urinating (dysuria). Occasionally, the condition spreads to the upper genital tract in women (causing pelvic inflammatory disease) or to the epididymis in men (causing epididymitis). Chlamydia infection can be effectively cured with antibiotics. If left untreated, chlamydial infections can cause serious reproductive and other health problems with both short-term and long-term consequences.
Chlamydia conjunctivitis or trachoma is a common cause of blindness worldwide. The World Health Organization (WHO) estimates that it accounted for 15% of blindness cases in 1995, but only 3.6% in 2002.
Signs and symptoms.
Genital disease.
Women.
Chlamydial infection of the neck of the womb (cervicitis) is a sexually transmitted infection which is asymptomatic for about 50-70% of women infected with the disease. The infection can be passed through vaginal, anal, or oral sex. Of those who have an asymptomatic infection that is not detected by their doctor, approximately half will develop pelvic inflammatory disease (PID), a generic term for infection of the uterus, fallopian tubes, and/or ovaries. PID can cause scarring inside the reproductive organs, which can later cause serious complications, including chronic pelvic pain, difficulty becoming pregnant, ectopic (tubal) pregnancy, and other dangerous complications of pregnancy.
Chlamydia is known as the "Silent Epidemic" because in women, it may not cause any symptoms in 70–80% of cases, and can linger for months or years before being discovered. Symptoms that may occur include unusual vaginal bleeding or discharge, pain in the abdomen, painful sexual intercourse (dyspareunia), fever, painful urination or the urge to urinate more frequently than usual (urinary urgency).
Men.
In men, chlamydia shows symptoms of infectious urethritis (inflammation of the urethra) in about 50% of cases. Symptoms that may occur include: a painful or burning sensation when urinating, an unusual discharge from the penis, swollen or tender testicles, or fever. Discharge, or the purulent exudate, is generally less viscous and lighter in color than for gonorrhea. If left untreated, it is possible for chlamydia in men to spread to the testicles causing epididymitis, which in rare cases can cause sterility if not treated within 6 to 8 weeks. Chlamydia is also a potential cause of prostatitis in men, although the exact relevance in prostatitis is difficult to ascertain due to possible contamination from urethritis.
Eye disease.
Chlamydia conjunctivitis or trachoma was once the most important cause of blindness worldwide, but its role diminished from 15% of blindness cases by trachoma in 1995 to 3.6% in 2002. The infection can be spread from eye to eye by fingers, shared towels or cloths, coughing and sneezing and eye-seeking flies. Newborns can also develop chlamydia eye infection through childbirth (see below). Using the SAFE strategy (acronym for surgery for in-growing or in-turned lashes, antibiotics, facial cleanliness, and environmental improvements), the World Health Organisation aims for the global elimination of trachoma by 2020 (GET 2020 initiative).
Rheumatological conditions.
Chlamydia may also cause reactive arthritis (reiter's syndrome) - the triad of arthritis, conjunctivitis and urethritis (inflammation of the urethra) - especially in young men. About 15,000 men develop reactive arthritis due to chlamydia infection each year in the U.S., and about 5,000 are permanently affected by it. It can occur in both sexes, though is more common in men.
Perinatal infections.
As many as half of all infants born to mothers with chlamydia will be born with the disease. Chlamydia can affect infants by causing spontaneous abortion; premature birth; conjunctivitis, which may lead to blindness; and pneumonia. Conjunctivitis due to chlamydia typically occurs one week after birth (compared with chemical causes (within hours) or gonorrhea (2–5 days)).
Other conditions.
Chlamydia trachomatis is also the cause of lymphogranuloma venereum, an infection of the lymph nodes and lymphatics. It usually presents with genital ulceration and swollen lymph nodes in the groin, but it may also manifest as proctitis (inflammation of the rectum), fever or swollen lymph nodes in other regions of the body.
Transmission.
Chlamydia can be transmitted during vaginal, anal, or oral sex. Chlamydia can also be passed from an infected mother to her baby during vaginal childbirth.
Pathophysiology.
"Chlamydiae" have the ability to establish long-term associations with host cells. When an infected host cell is starved for various nutrients such as amino acids (for example, tryptophan), iron, or vitamins, this has a negative consequence for "Chlamydiae" since the organism is dependent on the host cell for these nutrients. Long-term cohort studies indicate that approximately 50% of those infected clear within a year, 80% within two years, and 90% within three years.
The starved chlamydiae enter a persistent growth state wherein they stop cell division and become morphologically aberrant by increasing in size. Persistent organisms remain viable as they are capable of returning to a normal growth state once conditions in the host cell improve.
There is much debate as to whether persistence has "in vivo" relevance. Many believe that persistent chlamydiae are the cause of chronic chlamydial diseases. Some antibiotics such as β-lactams can also induce a persistent-like growth state, which can contribute to the chronicity of chlamydial diseases.
Screening.
For sexually active women who are not pregnant, screening is recommended in those under 25 and others at risk of infection. Risk factors include a history of chlamydial or other sexually transmitted infection, new or multiple sexual partners, and inconsistent condom use. For pregnant women, guidelines vary: screening women with age or other risk factors is recommended by the U.S. Preventive Services Task Force (USPSTF) (which recommends screening women under 25) and the American Academy of Family Physicians (which recommends screening women aged 25 or younger). The American College of Obstetricians and Gynecologists recommends screening all at risk, while the Centers for Disease Control and Prevention recommend universal screening of pregnant women. The USPSTF acknowledges that in some communities there may be other risk factors for infection, such as ethnicity. Evidence-based recommendations for screening initiation, intervals and termination are currently not possible. There is no universal agreement on screening men for chlamydia.
In England and Wales the NHS aims to
Diagnosis.
The diagnosis of genital chlamydial infections evolved rapidly from the 1990s through 2006. Nucleic acid amplification tests (NAAT), such as polymerase chain reaction (PCR), transcription mediated amplification (TMA), and the DNA strand displacement amplification (SDA) now are the mainstays. NAAT for chlamydia may be performed on swab specimens sampled from the cervix (women) or urethra (men), on self-collected vaginal swabs, or on voided urine. NAAT has been estimated to have a sensitivity of approximately 90% and a specificity of approximately 99%, regardless of sampling from a cervical swab or by urine specimen. In women seeking an STI clinic and a urine test is negative, a subsequent cervical swab has been estimated to be positive in approximately 2% of the time.
At present, the NAATs have regulatory approval only for testing urogenital specimens, although rapidly evolving research indicates that they may give reliable results on rectal specimens.
Because of improved test accuracy, ease of specimen management, convenience in specimen management, and ease of screening sexually active men and women, the NAATs have largely replaced culture, the historic gold standard for chlamydia diagnosis, and the non-amplified probe tests. The latter test is relatively insensitive, successfully detecting only 60–80% of infections in asymptomatic women, and often giving falsely positive results. Culture remains useful in selected circumstances and is currently the only assay approved for testing non-genital specimens.
Treatment.
"C. trachomatis" infection can be effectively cured with antibiotics once it is detected. Current guidelines recommend azithromycin, doxycycline, erythromycin, or ofloxacin. Agents recommended for pregnant women include erythromycin or amoxicillin.
An option for treating partners of patients (index cases) diagnosed with chlamydia or gonorrhea is "patient-delivered partner therapy" (PDT or PDPT), which is the clinical practice of treating the sex partners of index cases by providing prescriptions or medications to the patient to take to his/her partner without the health care provider first examining the partner.
Epidemiology.
Globally, as of 2010, sexually transmitted chlamydia affects approximately 215 million people (3.1% of the population). It is more common in women (3.8%) than men (2.5%). In that year it resulted in about 1,200 deaths down from 1,500 in 1990.
CDC estimates that there are approximately 2.8 million new cases of chlamydia in the United States each year and that it affects around 2% of young people in that country. Chlamydial infection is the most common bacterial sexually transmitted infection in the UK.
Chlamydia causes more than 250,000 cases of epididymitis in the U.S. each year. Chlamydia causes 250,000 to 500,000 cases of PID every year in the United States. Women infected with chlamydia are up to five times more likely to become infected with HIV, if exposed.
Evolution.
Recent phylogenetic studies have revealed that "Chlamydia" likely shares a common ancestor with cyanobacteria, the group containing the endosymbiont ancestor to the chloroplasts of modern plants, hence, "Chlamydia" retains unusual plant-like traits, both genetically and physiologically. In particular, the enzyme L,L-diaminopimelate aminotransferase, which is related to lysine production in plants, is also linked with the construction of chlamydia's cell wall. The genetic encoding for the enzymes is remarkably similar in plants, cyanobacteria, and "Chlamydia", demonstrating a close common ancestry. This unexpected discovery may help scientists develop new treatment avenues: if scientists could find a safe and effective inhibitor of L,L-diaminopimelate aminotransferase, they might have a highly effective and extremely specific new antibiotic against chlamydia.

</doc>
<doc id="7038" url="http://en.wikipedia.org/wiki?curid=7038" title="Candidiasis">
Candidiasis

Candidiasis, thrush, or yeast infection is a fungal infection (mycosis) of any species from the genus "Candida" (one genus of yeasts). "Candida albicans" is the most common agent of candidiasis in humans. It is also technically known as candidosis, moniliasis, and oidiomycosis.
Candidiasis encompasses infections that range from superficial, such as oral thrush and vaginitis, to systemic and potentially life-threatening diseases. "Candida" infections of the latter category are also referred to as candidemia or invasive candidiasis, and are usually confined to severely immunocompromised persons, such as cancer, transplant, and AIDS patients, as well as nontrauma emergency surgery patients.
Superficial infections of skin and mucosal membranes by "Candida" causing local inflammation and discomfort are common. While clearly attributable to the presence of the opportunistic pathogens of the genus "Candida", candidiasis describes a number of different disease syndromes that often differ in their causes and outcomes.
Classification.
Candidiasis may be divided into these types:
Signs and symptoms.
Symptoms of candidiasis vary depending on the area affected. Most candidial infections result in minimal complications such as redness, itching, and discomfort, though complications may be severe or even fatal if left untreated in certain populations. In immunocompetent persons, candidiasis is usually a very localized infection of the skin or mucosal membranes, including the oral cavity (thrush), the pharynx or esophagus, the gastrointestinal tract, the urinary bladder, the fingernails or toenails (onychomycosis), and the genitalia (vagina, penis).
Candidiasis is a very common cause of vaginal irritation, or vaginitis, and can also occur on the male genitals. In immunocompromised patients, "Candida" infections can affect the esophagus with the potential of becoming systemic, causing a much more serious condition, a fungemia called candidemia.
Thrush is commonly seen in infants. It is not considered abnormal in infants unless it lasts longer than a few weeks.
Infection of the vagina or vulva may cause severe itching, burning, soreness, irritation, and a whitish or whitish-gray cottage cheese-like discharge. These symptoms are also present in the more common bacterial vaginosis. In a 2002 study, only 33% of women who were self-treating for a yeast infection actually had a such an infection, while most had either bacterial vaginosis or a mixed-type infection. Symptoms of infection of the male genitalia (balanitis thrush) include red skin around the head of the penis, swelling, irritation, itchiness and soreness of the head of the penis, thick, lumpy discharge under the foreskin, unpleasant odour, difficulty retracting the foreskin (phimosis), and pain when passing urine
or during sex.
Perianal candidiasis can cause pruritis ani. The lesion can be erythematous, papular, or ulcerative in appearance, and it is not considered to be a sexually transmissible disease.
Esophageal candidiasis can cause dysphagia (difficulty swallowing), or less commonly odynophagia (painful swallowing).
Causes.
"Candida" yeasts are generally present in healthy humans, particularly on the skin, but their growth is normally limited by the human immune system, by competition of other microorganisms, such as bacteria occupying the same locations in the human body, and in the case of skin, by the relative dryness of the skin, as "Candida" requires moisture for growth.
"C. albicans" was isolated from the vaginas of 19% of apparently healthy women, i.e., those who experienced few or no symptoms of infection. External use of detergents or douches or internal disturbances (hormonal or physiological) can perturb the normal vaginal flora, consisting of lactic acid bacteria, such as lactobacilli, and result in an overgrowth of "Candida" cells, causing symptoms of infection, such as local inflammation. Pregnancy and the use of oral contraceptives have been reported as risk factors. Diabetes mellitus and the use of antibacterial antibiotics are also linked to increased rates of yeast infections. Diets high in simple carbohydrates have been found to affect rates of oral candidiases, and hormone replacement therapy and infertility treatments may also be predisposing factors. Wearing wet swimwear for long periods
of time is also believed to be a risk factor.
A weakened or undeveloped immune system or metabolic illnesses such as diabetes are significant predisposing factors of candidiasis. Diseases or conditions linked to candidiasis include HIV/AIDS, mononucleosis, cancer treatments, steroids, stress, and nutrient deficiency. Almost 15% of people with weakened immune systems develop a systemic illness caused by "Candida" species. In extreme cases, these superficial infections of the skin or mucous membranes may enter into the bloodstream and cause systemic "Candida" infections.
In penile candidiasis, the causes include sexual intercourse with an infected individual, low immunity, antibiotics, and diabetes. Male genital yeast infections are less common, and incidences of infection are only a fraction of those in women; however, yeast infection on the penis from direct contact via sexual intercourse with an infected partner is not uncommon.
"Candida" species are frequently part of the human body's normal oral and intestinal flora. Treatment with antibiotics can lead to eliminating the yeast's natural competitors for resources, and increase the severity of the condition. In the Western Hemisphere, about 75% of females are affected at some time in their lives.
Diagnosis.
Diagnosis of a yeast infection is done either via microscopic examination or culturing. For identification by light microscopy, a scraping or swab of the affected area is placed on a microscope slide. A single drop of 10% potassium hydroxide (KOH) solution is then added to the specimen. The KOH dissolves the skin cells, but leaves the "Candida" cells intact, permitting visualization of pseudohyphae and budding yeast cells typical of many "Candida" species.
For the culturing method, a sterile swab is rubbed on the infected skin surface. The swab is then streaked on a culture medium. The culture is incubated at 37°C for several days, to allow development of yeast or bacterial colonies. The characteristics (such as morphology and colour) of the colonies may allow initial diagnosis of the organism causing disease symptoms.
Treatment.
Candidiasis is commonly treated with antimycotics; these antifungal drugs include topical clotrimazole, topical nystatin, fluconazole, and topical ketoconazole.
Localized infection.
A one-time dose of fluconazole is 90% effective in treating a vaginal yeast infection. Local treatment may include vaginal suppositories or medicated douches. Other types of yeast infections require different dosing. Gentian violet can be used for thrush in breastfeeding babies, but when used in large quantities, it can cause mouth and throat ulcerations, and has been linked to mouth cancer in humans and to cancer in the digestive tract of other animals. "C. albicans" can develop resistance to fluconazole, this being more of an issue in those with HIV/AIDS who are often treated with multiple courses of fluconazole for recurrent oral infections.
For vaginal yeast infection in pregnancy, topical imidazole or triazole antifungal is considered the therapy of choice owing to available safety data. Systemic absorption of these topical formulations is minimal, posing little risk of transplacental transfer. In vaginal yeast infection in pregnancy, treatment with topical azole antifungals is recommended for 7 days instead of a shorter duration.
Not enough evidence is available to determine if probiotics, either as pills or as yogurt, have an effect on the rate of occurrence of vaginal yeast infections. No benefit has been found for active infections.
Blood infection.
In candidial infections of the blood, intravenous fluconazole or an echinocandin such as caspofungin may be used. Amphotericin B is another option.
Prognosis.
Among individuals being treated in intensive care units, the mortality rate is about 30-50% when systemic candidiasis develops.
Epidemiology.
Oral candidiasis is the most common form, by far the most common fungal infection of the mouth, and it also represents the most common opportunistic oral infection in humans. Candida septicemia is rare.
Esophageal candidiasis is the most common esophageal infection in persons with AIDS, and accounts for about 50% of all esophageal infections, often coexisting with other esophageal diseases. About two-thirds of people with AIDS and esophageal candidiasis also have oral candidiasis.
History.
Descriptions of what sounds like oral thrush go back to the time of Hippocrates "circa" 460 - 370 BCE.
Vulvovaginal candidiasis was first described in 1849 by Wilkinson. In 1875, Haussmann demonstrated the causative organism in both vulvovaginal and oral candidiasis is the same.
With the advent of antibiotics following World War II, the rates of candidiasis increased. The incidence fell once more in the 1950s following the development of nystatin.
The colloquial term "thrush" refers to the resemblance of the white flecks present in some forms of candidiasis ("e.g." pseudomembranous candidiasis) with the breast of the bird of the same name.<ref name=emedicine/medscape></ref> The term candidosis is largely used in British English, and candidiasis in American English. "Candida" is also pronounced differently, in American English, the stress is on the "i", whereas in British English the stress is on the first syllable.
The genus "Candida" and species "C. albicans" were described by botanist Christine Marie Berkhout in her doctoral thesis at the University of Utrecht in 1923. Over the years, the classification of the genera and species has evolved. Obsolete names for this genus include "Mycotorula" and "Torulopsis". The species has also been known in the past as "Monilia albicans" and "Oidium albicans". The current classification is "nomen conservandum", which means the name is authorized for use by the International Botanical Congress (IBC).
The genus "Candida" includes about 150 different species; however, only a few are known to cause human infections. "C. albicans" is the most significant pathogenic species. Other species pathogenic in humans include "C. tropicalis", "C. glabrata", "C. krusei", "C. parapsilosis", "C. dubliniensis", and "C. lusitaniae".
The name "Candida" was proposed by Berkhout. It is from the Latin word "toga candida", referring to the white toga (robe) worn by candidates for the Senate of the ancient Roman republic. The specific epithet "albicans" also comes from Latin, "albicare" meaning "to whiten". These names refer to the generally white appearance of "Candida" species when cultured.
Alternative medicine.
What has been described as "a large pseudoscientific cult" has developed around the topic of "Candida", with claims up to one in three people are affected by conditions with terms such as systemic candidiasis, "candidiasis hypersensitivity", fungal type dysbiosis, "Candida"-related complex, the yeast syndrome, yeast allergy, yeast overgrowth, or simply "Candida" or "yeast problem".
Some practitioners of alternative medicine have promoted these purported conditions and sold dietary supplements as supposed cures; a number of them have been prosecuted.
In 1990, alternative health vendor Nature's Way signed an FTC consent agreement not to misrepresent in advertising any self-diagnostic test concerning yeast conditions or to make any unsubstantiated representation concerning any food or supplement's ability to control yeast conditions, with a fine of $30,000 payable to the National Institutes of Health for research in genuine candidiasis.

</doc>
<doc id="7039" url="http://en.wikipedia.org/wiki?curid=7039" title="Control theory">
Control theory

Control theory is an interdisciplinary branch of engineering and mathematics that deals with the behavior of dynamical systems with inputs, and how their behavior is modified by feedback. The usual objective of control theory is to control a system, often called the "plant", so its output follows a desired control signal, called the "reference", which may be a fixed or changing value. To do this a "controller" is designed, which monitors the output and compares it with the reference. The difference between actual and desired output, called the "error" signal, is applied as feedback to the input of the system, to bring the actual output closer to the reference. Some topics studied in control theory are stability; whether the output will converge to the reference value or oscillate about it; controllability and observability. 
Extensive use is usually made of a diagrammatic style known as the block diagram. The transfer function, also known as the system function or network function, is a mathematical representation of the relation between the input and output based on the differential equations describing the system.
Although a major application of control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs; a few examples are in physiology, electronics, climate modeling, machine design, ecosystems, navigation, neural networks, predator-prey interaction, and gene expression.
Overview.
Control theory is
Control systems may be thought of as having four functions: Measure, Compare, Compute, and Correct. These four functions are completed by five elements: Detector, Transducer, Transmitter, Controller, and Final Control Element. The measuring function is completed by the detector, transducer and transmitter. In practical applications these three elements are typically contained in one unit. A standard example of a measuring unit is a resistance thermometer. The compare and compute functions are completed within the controller, which may be implemented electronically by proportional control, a PI controller, PID controller, bistable, hysteretic control or programmable logic controller. Older controller units have been mechanical, as in a centrifugal governor or a carburetor. The correct function is completed with a final control element. The final control element changes an input or output in the control system that affects the manipulated or controlled variable.
An example.
Consider a car's cruise control, which is a device designed to maintain vehicle speed at a constant "desired" or "reference" speed provided by the driver. The "controller" is the cruise control, the "plant" is the car, and the "system" is the car and the cruise control. The system output is the car's speed, and the control itself is the engine's throttle position which determines how much power the engine generates.
A primitive way to implement cruise control is simply to lock the throttle position when the driver engages cruise control. However, if the cruise control is engaged on a stretch of flat road, then the car will travel slower going uphill and faster when going downhill. This type of controller is called an open-loop controller because no measurement of the system output (the car's speed) is used to alter the control (the throttle position.) As a result, the controller cannot compensate for changes acting on the car, like a change in the slope of the road.
In a closed-loop control system, a sensor monitors the system output (the car's speed) and feeds the data to a controller which adjusts the control (the throttle position) as necessary to maintain the desired system output (match the car's speed to the reference speed.) Now, when the car goes uphill, the decrease in speed is measured, and the throttle position changed to increase engine power, speeding up the vehicle. Feedback from measuring the car's speed has allowed the controller to dynamically compensate for changes to the car's speed. It is from this feedback that the paradigm of the control "loop" arises: the control affects the system output, which in turn is measured and looped back to alter the control.
Classification.
The field of control theory can be divided into two branches:
History.
Although control systems of various types date back to antiquity, a more formal analysis of the field began with a dynamics analysis of the centrifugal governor, conducted by the physicist James Clerk Maxwell in 1868, entitled "On Governors". This described and analyzed the phenomenon of self-oscillation, in which lags in the system may lead to overcompensation and unstable behavior. This generated a flurry of interest in the topic, during which Maxwell's classmate, Edward John Routh, abstracted Maxwell's results for the general class of linear systems. Independently, Adolf Hurwitz analyzed system stability using differential equations in 1877, resulting in what is now known as the Routh–Hurwitz theorem.
A notable application of dynamic control was in the area of manned flight. The Wright brothers made their first successful test flights on December 17, 1903 and were distinguished by their ability to control their flights for substantial periods (more so than the ability to produce lift from an airfoil, which was known). Continuous, reliable control of the airplane was necessary for flights lasting longer than a few seconds.
By World War II, control theory was an important part of fire-control systems, guidance systems and electronics.
Sometimes mechanical methods are used to improve the stability of systems. For example, ship stabilizers are fins mounted beneath the waterline and emerging laterally. In contemporary vessels, they may be gyroscopically controlled active fins, which have the capacity to change their angle of attack to counteract roll caused by wind or waves acting on the ship.
The Sidewinder missile uses small control surfaces placed at the rear of the missile with spinning disks on their outer surfaces; these are known as rollerons. Airflow over the disks spins them to a high speed. If the missile starts to roll, the gyroscopic force of the disks drives the control surface into the airflow, cancelling the motion. Thus, the Sidewinder team replaced a potentially complex control system with a simple mechanical solution.
The Space Race also depended on accurate spacecraft control, and control theory has also seen an increasing use in fields such as economics.
People in systems and control.
Many active and historical figures made significant contribution to control theory, including, for example:
Classical control theory.
To overcome the limitations of the open-loop controller, control theory introduces feedback.
A closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is "fed back" as input to the process, closing the loop.
Closed-loop controllers have the following advantages over open-loop controllers:
In some systems, closed-loop and open-loop control are used simultaneously. In such systems, the open-loop control is termed feedforward and serves to further improve reference tracking performance.
A common closed-loop controller architecture is the PID controller.
Closed-loop transfer function.
The output of the system "y(t)" is fed back through a sensor measurement "F" to the reference value "r(t)". The controller "C" then takes the error "e" (difference) between the reference and the output to change the inputs "u" to the system under control "P". This is shown in the figure. This kind of controller is a closed-loop controller or feedback controller.
This is called a single-input-single-output ("SISO") control system; "MIMO" (i.e., Multi-Input-Multi-Output) systems, with more than one input/output, are common. In such cases variables are represented through vectors instead of simple scalar values. For some distributed parameter systems the vectors may be infinite-dimensional (typically functions).
If we assume the controller "C", the plant "P", and the sensor "F" are linear and time-invariant (i.e., elements of their transfer function "C(s)", "P(s)", and "F(s)" do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations:
Solving for "Y"("s") in terms of "R"("s") gives:
The expression formula_5 is referred to as the "closed-loop transfer function" of the system. The numerator is the forward (open-loop) gain from "r" to "y", and the denominator is one plus the gain in going around the feedback loop, the so-called loop gain. If formula_6, i.e., it has a large norm with each value of "s", and if formula_7, then "Y(s)" is approximately equal to "R(s)" and the output closely tracks the reference input.
PID controller.
The PID controller is probably the most-used feedback control design. "PID" is an acronym for "Proportional-Integral-Derivative", referring to the three terms operating on the error signal to produce a control signal. If "u(t)" is the control signal sent to the system, "y(t)" is the measured output and "r(t)" is the desired output, and tracking error formula_8, a PID controller has the general form
The desired closed loop dynamics is obtained by adjusting the three parameters formula_10, formula_11 and formula_12, often iteratively by "tuning" and without specific knowledge of a plant model. Stability can often be ensured using only the proportional term. The integral term permits the rejection of a step disturbance (often a striking specification in process control). The derivative term is used to provide damping or shaping of the response. PID controllers are the most well established class of control systems: however, they cannot be used in several more complicated cases, especially if MIMO systems are considered.
Applying Laplace transformation results in the transformed PID controller equation
with the PID controller transfer function
For practical PID controllers, a pure differentiator is neither physically realisable nor desirable due to amplification of noise and resonant modes in the system. Therefore a phase-lead compensator type approach is used instead, or a differentiator with low-pass roll-off.
Modern control theory.
In contrast to the frequency domain analysis of the classical control theory, modern control theory utilizes the time-domain state space representation, a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations. To abstract from the number of inputs, outputs and states, the variables are expressed as vectors and the differential and algebraic equations are written in matrix form (the latter only being possible when the dynamical system is linear). The state space representation (also known as the "time-domain approach") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With inputs and outputs, we would otherwise have to write down Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state space representation is not limited to systems with linear components and zero initial conditions. "State space" refers to the space whose axes are the state variables. The state of the system can be represented as a vector within that space.
Topics in control theory.
Stability.
The "stability" of a general dynamical system with no input can be described with Lyapunov stability criteria. 
For simplicity, the following descriptions focus on continuous-time and discrete-time linear systems.
Mathematically, this means that for a causal linear system to be stable all of the poles of its transfer function must have negative-real values, i.e. the real part of all the poles are less than zero. Practically speaking, stability requires that the transfer function complex poles reside
The difference between the two cases is simply due to the traditional method of plotting continuous time versus discrete time transfer functions. The continuous Laplace transform is in Cartesian coordinates where the formula_16 axis is the real axis and the discrete Z-transform is in circular coordinates where the formula_17 axis is the real axis.
When the appropriate conditions above are satisfied a system is said to be asymptotically stable: the variables of an asymptotically stable control system always decrease from their initial value and do not show permanent oscillations. Permanent oscillations occur when a pole has a real part exactly equal to zero (in the continuous time case) or a modulus equal to one (in the discrete time case). If a simply stable system response neither decays nor grows over time, and has no oscillations, it is marginally stable: in this case the system transfer function has non-repeated poles at complex plane origin (i.e. their real and complex component is zero in the continuous time case). Oscillations are present when poles with real part equal to zero have an imaginary part not equal to zero.
If a system in question has an impulse response of
then the Z-transform (see this example), is given by
which has a pole in formula_20 (zero imaginary part). This system is BIBO (asymptotically) stable since the pole is "inside" the unit circle.
However, if the impulse response was
then the Z-transform is
which has a pole at formula_23 and is not BIBO stable since the pole has a modulus strictly greater than one.
Numerous tools exist for the analysis of the poles of a system. These include graphical systems like the root locus, Bode plots or the Nyquist plots.
Mechanical changes can make equipment (and control systems) more stable. Sailors add ballast to improve the stability of ships. Cruise ships use antiroll fins that extend transversely from the side of the ship for perhaps 30 feet (10 m) and are continuously rotated about their axes to develop forces that oppose the roll.
Controllability and observability.
Controllability and observability are main issues in the analysis of a system before deciding the best control strategy to be applied, or whether it is even possible to control or stabilize the system. Controllability is related to the possibility of forcing the system into a particular state by using an appropriate control signal. If a state is not controllable, then no signal will ever be able to control the state. If a state is not controllable, but its dynamics are stable, then the state is termed Stabilizable. Observability instead is related to the possibility of "observing", through output measurements, the state of a system. If a state is not observable, the controller will never be able to determine the behaviour of an unobservable state and hence cannot use it to stabilize the system. However, similar to the stabilizability condition above, if a state cannot be observed it might still be detectable.
From a geometrical point of view, looking at the states of each variable of the system to be controlled, every "bad" state of these variables must be controllable and observable to ensure a good behaviour in the closed-loop system. That is, if one of the eigenvalues of the system is not both controllable and observable, this part of the dynamics will remain untouched in the closed-loop system. If such an eigenvalue is not stable, the dynamics of this eigenvalue will be present in the closed-loop system which therefore will be unstable. Unobservable poles are not present in the transfer function realization of a state-space representation, which is why sometimes the latter is preferred in dynamical systems analysis.
Solutions to problems of uncontrollable or unobservable system include adding actuators and sensors.
Control specification.
Several different control strategies have been devised in the past years. These vary from extremely general ones (PID controller), to others devoted to very particular classes of systems (especially robotics or aircraft cruise control).
A control problem can have several specifications. Stability, of course, is always present: the controller must ensure that the closed-loop system is stable, regardless of the open-loop stability. A poor choice of controller can even worsen the stability of the open-loop system, which must normally be avoided. Sometimes it would be desired to obtain particular dynamics in the closed loop: i.e. that the poles have formula_24, where formula_25 is a fixed value strictly greater than zero, instead of simply asking that formula_26.
Another typical specification is the rejection of a step disturbance; including an integrator in the open-loop chain (i.e. directly before the system under control) easily achieves this. Other classes of disturbances need different types of sub-systems to be included.
Other "classical" control theory specifications regard the time-response of the closed-loop system: these include the rise time (the time needed by the control system to reach the desired value after a perturbation), peak overshoot (the highest value reached by the response before reaching the desired value) and others (settling time, quarter-decay). Frequency domain specifications are usually related to robustness (see after).
Modern performance assessments use some variation of integrated tracking error (IAE,ISA,CQI).
Model identification and robustness.
A control system must always have some robustness property. A robust controller is such that its properties do not change much if applied to a system slightly different from the mathematical one used for its synthesis. This specification is important: no real physical system truly behaves like the series of differential equations used to represent it mathematically. Typically a simpler mathematical model is chosen in order to simplify calculations, otherwise the true system dynamics can be so complicated that a complete model is impossible.
The process of determining the equations that govern the model's dynamics is called system identification. This can be done off-line: for example, executing a series of measures from which to calculate an approximated mathematical model, typically its transfer function or matrix. Such identification from the output, however, cannot take account of unobservable dynamics. Sometimes the model is built directly starting from known physical equations: for example, in the case of a system we know that formula_27. Even assuming that a "complete" model is used in designing the controller, all the parameters included in these equations (called "nominal parameters") are never known with absolute precision; the control system will have to behave correctly even when connected to physical system with true parameter values away from nominal.
Some advanced control techniques include an "on-line" identification process (see later). The parameters of the model are calculated ("identified") while the controller itself is running: in this way, if a drastic variation of the parameters ensues (for example, if the robot's arm releases a weight), the controller will adjust itself consequently in order to ensure the correct performance.
Analysis of the robustness of a SISO (single input single output) control system can be performed in the frequency domain, considering the system's transfer function and using Nyquist and Bode diagrams. Topics include gain and phase margin and amplitude margin. For MIMO (multi input multi output) and, in general, more complicated control systems one must consider the theoretical results devised for each control technique (see next section): i.e., if particular robustness qualities are needed, the engineer must shift his attention to a control technique by including them in its properties.
A particular robustness issue is the requirement for a control system to perform properly in the presence of input and state constraints. In the physical world every signal is limited. It could happen that a controller will send control signals that cannot be followed by the physical system: for example, trying to rotate a valve at excessive speed. This can produce undesired behavior of the closed-loop system, or even damage or break actuators or other subsystems. Specific control techniques are available to solve the problem: model predictive control (see later), and anti-wind up systems. The latter consists of an additional control block that ensures that the control signal never exceeds a given threshold.
System classifications.
Linear systems control.
For MIMO systems, pole placement can be performed mathematically using a state space representation of the open-loop system and calculating a feedback matrix assigning poles in the desired positions. In complicated systems this can require computer-assisted calculation capabilities, and cannot always ensure robustness. Furthermore, all system states are not in general measured and so observers must be included and incorporated in pole placement design.
Nonlinear systems control.
Processes in industries like robotics and the aerospace industry typically have strong nonlinear dynamics. In control theory it is sometimes possible to linearize such classes of systems and apply linear techniques, but in many cases it can be necessary to devise from scratch theories permitting control of nonlinear systems. These, e.g., feedback linearization, backstepping, sliding mode control, trajectory linearization control normally take advantage of results based on Lyapunov's theory. Differential geometry has been widely used as a tool for generalizing well-known linear control concepts to the non-linear case, as well as showing the subtleties that make it a more challenging problem.
Decentralized systems.
When the system is controlled by multiple controllers, the problem is one of decentralized control. Decentralization is helpful in many ways, for instance, it helps control systems operate over a larger geographical area. The agents in decentralized control systems can interact using communication channels and coordinate their actions.
Main control strategies.
Every control system must guarantee first the stability of the closed-loop behavior. For linear systems, this can be obtained by directly placing the poles. Non-linear control systems use specific theories (normally based on Aleksandr Lyapunov's Theory) to ensure stability without regard to the inner dynamics of the system. The possibility to fulfill different specifications varies from the model considered and the control strategy chosen. 
Further reading.
For Chemical Engineering

</doc>
<doc id="7042" url="http://en.wikipedia.org/wiki?curid=7042" title="Cracking joints">
Cracking joints

Cracking or popping of joints is the action of joint manipulation to produce a sharp cracking or popping sound. This commonly occurs during deliberate knuckle-cracking. It is possible to crack many joints, such as those in the back and neck vertebrae, hips, wrists, elbows, shoulders, toes, ankles, knees, jaws, feet, sternum, and the Achilles tendon area.
Causes.
The physical mechanism causing a cracking sound produced by bending, twisting, or compressing joints is uncertain. Suggested causes include:
Synovial fluid cavitation has some evidence to support it. When a spinal manipulation is performed, the applied force separates the articular surfaces of a fully encapsulated synovial joint, which in turn creates a reduction in pressure within the joint cavity. In this low-pressure environment, some of the gases that are dissolved in the synovial fluid (which are naturally found in all bodily fluids) leave the solution, making a bubble, or cavity, which rapidly collapses upon itself, resulting in a "clicking" sound. The contents of the resultant gas bubble are thought to be mainly nitrogen. The effects of this process will remain for a period of time known as the "refractory period," during which the joint cannot be "re-cracked," which can range from a few seconds to some hours while the gases are slowly reabsorbed back into the synovial fluid. There is some evidence that ligament laxity may be associated with an increased tendency to cavitate.
The snapping of tendons or scar tissue over a prominence (as in snapping hip syndrome) can also generate a loud snapping or popping sound.
Effects.
The common claim that cracking one's knuckles causes arthritis appears unsupported. A study published in 2011 examined the hand radiographs of 215 people (aged 50 to 89) and compared the joints of those who regularly cracked their knuckles to those who did not. The study concluded that knuckle-cracking did not cause hand osteoarthritis, no matter how many years or how often a person cracked their knuckles. An earlier study also concluded that there was no increased preponderance of arthritis of the hand of chronic knuckle-crackers; however, habitual knuckle-crackers were more likely to have hand swelling and lowered grip strength. Habitual knuckle-cracking was associated with manual labour, biting of the nails, smoking, and drinking alcohol and was suggested to result in functional hand impairment. This early study has been criticized for not taking into consideration the possibility of confounding factors, such as whether the ability to crack one's knuckles is associated with impaired hand functioning rather than being a cause of it. 
There are many ways people have learned to crack the joints of their fingers, one of the most common ways is to put pressure on the joint between the metacarpals and the proximal phalanges. 
Medical doctor Donald Unger cracked the knuckles of his left hand every day for more than sixty years, but he did not crack the knuckles of his right hand. No arthritis or other ailments formed in either hand, earning him the 2009 Ig Nobel Prize in Medicine, a parody of the Nobel Prize.

</doc>
<doc id="7043" url="http://en.wikipedia.org/wiki?curid=7043" title="Chemical formula">
Chemical formula

A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and "plus" (+) and "minus" (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts. A chemical formula is not a chemical name, and it contains no words. Although a chemical formula may imply certain simple chemical structures, it is not the same as a full chemical structural formula. Chemical formulas are more limiting than chemical names and structural formulas.
The simplest types of chemical formulas are called "empirical formulas", which use only letters and numbers indicating atomic proportional ratios (the numerical proportions of atoms of one type to those of other types). "Molecular formulas" indicate the simple numbers of each type of atom in a molecule of a molecular substance, and are thus sometimes the same as empirical formulas (for molecules that only have one atom of a particular type), and at other times require larger numbers than do empirical formulas. An example of the difference is the empirical formula for glucose, which is CH2O, while its molecular formula requires all numbers to be increased by a factor of six, giving C6H12O6.
Sometimes a chemical formula is complicated by being written as a "condensed formula" (or condensed molecular formula, occasionally called a "semi-structural formula"), which conveys additional information about the particular ways in which the atoms are chemically bonded together, either in covalent bonds, ionic bonds, or various combinations of these types. This is possible if the relevant bonding is easy to show in one dimension. An example is the condensed molecular/chemical formula for ethanol, which is CH3-CH2-OH or CH3CH2OH. However, even a condensed chemical formula is necessarily limited in its ability to show complex bonding relationships between atoms, especially atoms that have bonds to four or more different substituents. 
Since a chemical formula must be expressed as a single line of chemical element symbols, it often cannot be as informative as a true structural formula, which is a graphical representation of the spacial relationship between atoms in chemical compounds (see for example the figure for butane structural and chemical formulas, at right). For reasons of structural complexity, there is no condensed chemical formula (or semi-structural formula) that specifies glucose (and there exist many different molecules, for example fructose and mannose, have the same molecular formula C6H12O6 as glucose). Linear equivalent chemical "names" exist that can and do specify any complex structural formula, but these names must use many terms (words), rather than the simple element symbols, numbers, and simple typographical symbols that define a chemical formula.
Chemical formulas may be used in chemical equations to describe chemical reactions and other chemical transformations, such as the dissolving of ionic compounds into solution. While, as noted, chemical formulas do not have the full power of structural formulas to show chemical relationships between atoms, they are sufficient to keep track of numbers of atoms and numbers of electical charges in chemical reactions, thus balancing chemical equations so that these equations can be used in chemical problems involving conservation of atoms, and conservation of electric charge.
Overview.
A chemical formula identifies each constituent element by its chemical symbol and indicates the proportionate number of atoms of each element. In empirical formulas, these proportions begin with a key element and then assign numbers of atoms of the other elements in the compound, as ratios to the key element. For molecular compounds, these ratio numbers can all be expressed as whole numbers. For example, the empirical formula of ethanol may be written C2H6O because the molecules of ethanol all contain two carbon atoms, six hydrogen atoms, and one oxygen atom. Some types of ionic compounds, however, cannot be written with entirely whole-number empirical formulas. An example is boron carbide, whose formula of CBn is a variable non-whole number ratio with n ranging from over 4 to more than 6.5.
When the chemical compound of the formula consists of simple molecules, chemical formulas often employ ways to suggest the structure of the molecule. These types of formulas are variously known as molecular formulas and condensed formulas. A molecular formula enumerates the number of atoms to reflect those in the molecule, so that the molecular formula for glucose is C6H12O6 rather than the glucose empirical formula, which is CH2O. However, except for very simple substances, molecular chemical formulas lack needed structural information, and are ambiguous. 
For simple molecules, a condensed (or semi-structural) formula is a type of chemical formula that may fully imply a correct structural formula. For example, ethanol may be represented by the condensed chemical formula CH3CH2OH, and dimethyl ether by the condensed formula CH3OCH3. These two molecules have the same empirical and molecular formulas (C2H6O), but may be differentiated by the condensed formulas shown, which are sufficient to represent the full structure of these simple organic compounds. 
Condensed chemical formulas may also be used to represent ionic compounds that do not exist as discrete molecules, but nonetheless do contain covalently bound clusters within them. These polyatomic ions are groups of atoms that are covalently bound together and have an overall ionic charge, such as the sulfate ion. Each polyatomic ion in a compound is written individually in order to illustrate the separate groupings. For example, the compound dichlorine hexoxide has an empirical formula , and molecular formula , but in liquid or solid forms, this compound is more correctly shown by an ionic condensed formula , which illustrates that this compound consists of ions and ions. In such cases, the condensed formula only need be complex enough to show at least one of each ionic species.
Chemical formulas must be differentiated from the far more complex chemical systematic names that are used in various systems of chemical nomenclature. For example, one systematic name for glucose is (2R,3S,4R,5R)-2,3,4,5,6-pentahydroxyhexanal. This name, and the rules behind it, fully specify glucose's structural formula, but the name is not a chemical formula, as it uses many extra terms and words that chemical formulas do not permit. Such chemical names may be able to represent full structural formulas without graphs, but in order to do so, they require word terms that are not part of chemical formulas.
Simple empirical formulas.
In chemistry, the empirical formula of a chemical is a simple expression of the relative number of each type of atom or ratio of the elements in the compound. Empirical formulas are the standard for ionic compounds, such as , and for macromolecules, such as . An empirical formula makes no reference to isomerism, structure, or absolute number of atoms. The term empirical refers to the process of elemental analysis, a technique of analytical chemistry used to determine the relative percent composition of a pure chemical substance by element.
For example hexane has a molecular formula of , or structurally , implying that it has a chain structure of 6 carbon atoms, and 14 hydrogen atoms. However, the empirical formula for hexane is . Likewise the empirical formula for hydrogen peroxide, , is simply HO expressing the 1:1 ratio of component elements. Formaldehyde and acetic acid have the same empirical formula, . This is the actual chemical formula for formaldehyde, but acetic acid has double the number of atoms.
Condensed formulas in organic chemistry implying molecular geometry and structural formulas.
The connectivity of a molecule often has a strong influence on its physical and chemical properties and behavior. Two molecules composed of the same numbers of the same types of atoms (i.e. a pair of isomers) might have completely different chemical and/or physical properties if the atoms are connected differently or in different positions. In such cases, a structural formula is useful, as it illustrates which atoms are bonded to which other ones. From the connectivity, it is often possible to deduce the approximate shape of the molecule.
A condensed chemical formula may represent the types and spatial arrangement of bonds in a simple chemical substance, though it does not necessarily specify isomers or complex structures. For example ethane consists of two carbon atoms single-bonded to each other, with each carbon atom having three hydrogen atoms bonded to it. Its chemical formula can be rendered as CH3CH3. In ethylene there is a double bond between the carbon atoms (and thus each carbon only has two hydrogens), therefore the chemical formula may be written: CH2CH2, and the fact that there is a double bond between the carbons is implicit because carbon has a valence of four. However, a more explicit method is to write H2C=CH2 or less commonly H2C::CH2. The two lines (or two pairs of dots) indicate that a double bond connects the atoms on either side of them.
A triple bond may be expressed with three lines or pairs of dots, and if there may be ambiguity, a single line or pair of dots may be used to indicate a single bond.
Molecules with multiple functional groups that are the same may be expressed by enclosing the repeated group in round brackets. For example isobutane may be written (CH3)3CH. This condensed structural formula implies a different connectivity from other molecules that can be formed using the same atoms in the same proportions (isomers). The formula (CH3)3CH implies a central carbon atom attached to one hydrogen atom and three CH3 groups. The same number of atoms of each element (10 hydrogens and 4 carbons, or C4H10) may be used to make a straight chain molecule, butane: CH3CH2CH2CH3.
Chemical names in answer to limitations of chemical formulas.
The alkene called but-2-ene has two isomers, which the chemical formula CH3CH=CHCH3 does not identify. The relative position of the two methyl groups must be indicated by additional notation denoting whether the methyl groups are on the same side of the double bond ("cis" or "Z") or on the opposite sides from each other ("trans" or "E"). Such extra symbols violate the rules for chemical formulas, and begin to enter the territory of more complex naming systems.
As noted above, in order to represent the full structural formulas of many complex organic and inorganic compounds, chemical nomenclature may be needed which goes well beyond the available resources used above in simple condensed formulas. See IUPAC nomenclature of organic chemistry and IUPAC nomenclature of inorganic chemistry 2005 for examples. In addition, linear naming systems such as International Chemical Identifier (InChI) allow a computer to construct a structural formula, and simplified molecular-input line-entry system (SMILES) allows a more human-readable ASCII input. However, all these nomenclature systems go beyond the standards of chemical formulas, and technically are chemical naming systems, not formula systems.
Polymers in condensed formulas.
For polymers in condensed chemical formulas, parentheses are placed around the repeating unit. For example, a hydrocarbon molecule that is described as CH3(CH2)50CH3, is a molecule with fifty repeating units. If the number of repeating units is unknown or variable, the letter "n" may be used to indicate this formula: CH3(CH2)"n"CH3.
Ions in condensed formulas.
For ions, the charge on a particular atom may be denoted with a right-hand superscript. For example Na+, or Cu2+. The total charge on a charged molecule or a polyatomic ion may also be shown in this way. For example: H3O+ or SO42−.
For more complex ions, brackets [ ] are often used to enclose the ionic formula, as in [B12H12]2−, which is found in compounds such as Cs2[B12H12]. Parentheses ( ) can be nested inside brackets to indicate a repeating unit, as in [Co(NH3)6]3+. Here (NH3)6 indicates that the ion contains six NH3 groups, and [ ] encloses the entire formula of the ion with charge +3. 
Isotopes.
Although isotopes are more relevant to nuclear chemistry or stable isotope chemistry than to conventional chemistry, different isotopes may be indicated with a prefixed superscript in a chemical formula. For example, the phosphate ion containing radioactive phosphorus-32 is 32PO43-. Also a study involving stable isotope ratios might include the molecule 18O16O.
A left-hand subscript is sometimes used redundantly to indicate the atomic number. For example, 8O2 for dioxygen, and 168O2 for the most abundant isotopic species of dioxygen. This is convenient when writing equations for nuclear reactions, in order to show the balance of charge more clearly.
Trapped atoms.
The @ symbol (at sign) indicates an atom or molecule trapped inside a cage but not chemically bound to it. For example, a buckminsterfullerene (C60) with an atom (M) would simply be represented as MC60 regardless of whether M was inside the fullerene without chemical bonding or outside, bound to one of the carbon atoms. Using the @ symbol, this would be denoted M@C60 if M was inside the carbon network. A non-fullerene example is [As@Ni12As20]3-, an ion in which one As atom is trapped in a cage formed by the other 32 atoms.
This notation was proposed in 1991 with the discovery of fullerene cages (endohedral fullerenes), which can trap atoms such as La to form, for example, La@C60 or La@C82. The choice of the symbol has been explained by the authors as being concise, readily printed and transmitted electronically (the at sign is included in ASCII, which most modern character encoding schemes are based on), and the visual aspects suggesting the structure of an endohedral fullerene.
Non-stoichiometric chemical formulas.
Chemical formulas most often use integers for each element. However, there is a class of compounds, called non-stoichiometric compounds, that cannot be represented by small integers. Such a formula might be written using decimal fractions, as in Fe0.95O, or it might include a variable part represented by a letter, as in Fe1–xO, where x is normally much less than 1.
General forms for organic compounds.
A chemical formula used for a series of compounds that differ from each other by a constant unit is called general formula. Such a series is called the homologous series, while its members are called homologs.
For example alcohols may be represented by: C"n"H(2n + 1)OH ("n" ≥ 1)
Hill System.
The Hill system is a system of writing chemical formulas such that the number of carbon atoms in a molecule is indicated first, the number of hydrogen atoms next, and then the number of all other chemical elements subsequently, in alphabetical order. When the formula contains no carbon, all the elements, including hydrogen, are listed alphabetically. This deterministic system enables straightforward sorting and searching of compounds.

</doc>
<doc id="7044" url="http://en.wikipedia.org/wiki?curid=7044" title="Beetle">
Beetle

The Coleoptera order of insects is commonly called beetles. The word "coleoptera" is from the Greek , "koleos", meaning "sheath"; and , "pteron", meaning "wing", thus "sheathed wing", because most beetles have two pairs of wings, the front pair, the "elytra", being hardened and thickened into a shell-like protection for the rear pair and the beetle's abdomen. The superficial consistency of most beetles' morphology, in particular their possession of elytra, has long suggested that the Coleoptera are monophyletic, but growing evidence indicates this is unjustified, there being arguments for example, in favor of allocating the current suborder Adephaga their own order, or very likely even more than one.
Overview.
The Coleoptera include more species than any other order, constituting almost 25% of all known types of animal life-forms. About 40% of all described insect species are beetles (about 400,000 species), and new species are discovered frequently. Some estimates put the total number of species, described and undescribed, at as high as 100 million, but a figure of one million is more widely accepted. The largest taxonomic family is commonly thought to be the Curculionidae (the weevils or snout beetles), but recently the Staphylinidae (the rove beetles) have claimed this title.
The diversity of beetles is very wide. They are found in all major habitats, except marine and the polar regions. They have many classes of ecological effects; particular species are adapted to practically every kind of diet. Some are nonspecialist detritus feeders, breaking down animal and plant debris; some feed on particular kinds of carrion such as flesh or hide; some feed on wastes such as dung; some feed on fungi, some on particular species of plants, others on a wide range of plants. Some are generalist pollen, flower and fruit eaters. Some are predatory, usually on other invertebrates; some are parasites or parasitoids. Many of the predatory species are important controls of agricultural pests. For example, beetles in the family Coccinellidae ("ladybirds" or "ladybugs") consume aphids, scale insects, thrips, and other insects that damage crops.
Conversely, beetles are prey of various invertebrates and vertebrates, including other insects, fish, reptiles, birds, and mammals. The Coleoptera are not generally serious pests, but they include agricultural and industrial pests, such as the Colorado potato beetle "Leptinotarsa decemlineata", the boll weevil "Anthonomus grandis", the red flour beetle "Tribolium castaneum", and the mungbean or cowpea beetle "Callosobruchus maculatus". Also included is the death-watch beetle, the larvae of which can cause serious structural damage to buildings by boring into the timbers.
Species in the Coleoptera have a hard exoskeleton, particularly on their forewings (elytra, singular elytron). These elytra distinguish beetles from most other insect species, except for the Dermaptera. The hemelytra of Heteroptera have a slight resemblance, but are not the same and their function is largely different.
Like all armored insects, beetles' exoskeletons comprise numerous plates called sclerites, some fused, and some separated by thin sutures. This combines armored defenses with maintained flexibility. The general anatomy of a beetle is superficially uniform, but specific organs and appendages may vary greatly in appearance and function between the many families in the order, and even more so between the suborders (such as Adephaga) that currently seem increasingly to be separate orders in their own right. All insects' bodies are divided into three sections: the head, the thorax, and the abdomen, and the Coleoptera are no exception. Their internal morphology and physiology also resemble those of other insects.
Beetles are endopterygotes; they undergo complete metamorphosis, a biological process by which an animal physically develops after a birth or hatching, undergoing a series of conspicuous and relatively abrupt changes in its body structure. Males may fight for females in various ways, and such species tend to display marked sexual dimorphism.
Distribution and diversity.
Beetles are by far the largest order of insects, with 350,000–400,000 species in four suborders (Adephaga, Archostemata, Myxophaga, and Polyphaga), making up about 40% of all insect species described, and about 30% of all animals. Though classification at the family level is a bit unstable, about 500 families and subfamilies are recognized. One of the first proposed estimates of the total number of beetle species on the planet is based on field data rather than on catalog numbers. The technique used for this original estimate, possibly as many as 12 million species, was criticized, and was later revised, with estimates of 850,000–4,000,000 species proposed. Some 70–95% of all beetle species, depending on the estimate, remain undescribed. The beetle fauna is not equally well known in all parts of the world. For example, the known beetle diversity of Australia is estimated at 23,000 species in 3265 genera and 121 families. This is slightly lower than reported for North America, a land mass of similar size with 25,160 species in 3526 genera and 129 families. While other predictions show there could be as many as 28,000 species in North America, including those currently undescribed, a realistic estimate of the little-studied Australian beetle fauna's true diversity could vary from 80,000 to 100,000.
Coleoptera are found in nearly all natural habitats, including freshwater and marine habitats, everywhere vegetative foliage is found, from trees and their bark to flowers, leaves, and underground near roots- even inside plants in galls, in every plant tissue, including dead or decaying ones.
External morphology.
Beetles are generally characterized by a particularly hard exoskeleton and hard forewings (elytra). The beetle's exoskeleton is made up of numerous plates, called sclerites, separated by thin sutures. This design provides armored defenses while maintaining flexibility. The general anatomy of a beetle is quite uniform, although specific organs and appendages may vary greatly in appearance and function between the many families in the order. Like all insects, beetles' bodies are divided into three sections: the head, the thorax, and the abdomen.
Head.
The head, having mouthparts projecting forward or sometimes downturned, is usually heavily sclerotized and varies in size. The eyes are compound and may display remarkable adaptability, as in the case of whirligig beetles (family Gyrinidae), where they are split to allow a view both above and below the waterline. Other species also have divided eyes – some longhorn beetles (family Cerambycidae) and weevils – while many have eyes that are notched to some degree. A few beetle genera also possess ocelli, which are small, simple eyes usually situated farther back on the head (on the vertex).
Beetles' antennae are primarily organs of smell, but may also be used to feel a beetle's environment physically. They may also be used in some families during mating, or among a few beetles for defence. Antennae vary greatly in form within the Coleoptera, but are often similar within any given family. In some cases, males and females of the same species will have different antennal forms. Antennae may be ( and are subforms of clavate, or clubbed antennae), , , , , or .
Beetles have mouthparts similar to those of grasshoppers. Of these parts, the most commonly known are probably the mandibles, which appear as large pincers on the front of some beetles. The mandibles are a pair of hard, often tooth-like structures that move horizontally to grasp, crush, or cut food or enemies (see defence, below). Two pairs of finger-like appendages, the maxillary and labial palpi, are found around the mouth in most beetles, serving to move food into the mouth. In many species, the mandibles are sexually dimorphic, with the males' enlarged enormously compared with those of females of the same species.
Thorax.
The thorax is segmented into the two discernible parts, the pro- and pterathorax. The pterathorax is the fused meso- and metathorax, which are commonly separated in other insect species, although flexibly articulate from the prothorax. When viewed from below, the thorax is that part from which all three pairs of legs and both pairs of wings arise. The abdomen is everything posterior to the thorax. When viewed from above, most beetles appear to have three clear sections, but this is deceptive: on the beetle's upper surface, the middle "section" is a hard plate called the pronotum, which is only the front part of the thorax; the back part of the thorax is concealed by the beetle's wings. This further segmentation is usually best seen on the abdomen.
Extremities.
The multisegmented legs end in two to five small segments called tarsi. Like many other insect orders, beetles bear claws, usually one pair, on the end of the last tarsal segment of each leg. While most beetles use their legs for walking, legs may be variously modified and adapted for other uses. Among aquatic families – Dytiscidae, Haliplidae, many species of Hydrophilidae and others – the legs, most notably the last pair, are modified for swimming and often bear rows of long hairs to aid this purpose. Other beetles have fossorial legs that are widened and often spined for digging. Species with such adaptations are found among the scarabs, ground beetles, and clown beetles (family Histeridae). The hind legs of some beetles, such as flea beetles (within Chrysomelidae) and flea weevils (within Curculionidae), are enlarged and designed for jumping.
Wings.
The elytra are connected to the pterathorax, so named because it is where the wings are connected ("pteron" meaning "wing" in Greek). The elytra are not used for flight, but tend to cover the hind part of the body and protect the second pair of wings ("alae"). They must be raised to move the hind flight wings. A beetle's flight wings are crossed with veins and are folded after landing, often along these veins, and stored below the elytra. A fold ("jugum") of the membrane at the base of each wing is a characteristic feature. In some beetles, the ability to fly has been lost. These include some ground beetles (family Carabidae) and some "true weevils" (family Curculionidae), but also desert- and cave-dwelling species of other families. Many have the two elytra fused together, forming a solid shield over the abdomen. In a few families, both the ability to fly and the elytra have been lost, with the best known example being the glow-worms of the family Phengodidae, in which the females are larviform throughout their lives.
Abdomen.
The abdomen is the section behind the metathorax, made up of a series of rings, each with a hole for breathing and respiration, called a spiracle, composing three different segmented sclerites: the tergum, pleura, and the sternum. The tergum in almost all species is membranous, or usually soft and concealed by the wings and elytra when not in flight. The pleura are usually small or hidden in some species, with each pleuron having a single spiracle. The sternum is the most widely visible part of the abdomen, being a more or less scelortized segment. The abdomen itself does not have any appendages, but some (for example, Mordellidae) have articulating sternal lobes.
Internal morphology.
Digestive system.
The digestive system of beetles is primarily based on plants, upon which they, for the most part, feed, with mostly the anterior midgut performing digestion, although in predatory species (for example Carabidae), most digestion occurs in the crop by means of midgut enzymes. In Elateridae species, the predatory larvae defecate enzymes on their prey, with digestion being extraorally. The alimentary canal basically consists of a short, narrow pharynx, a widened expansion, the crop, and a poorly developed gizzard. After is the midgut, that varies in dimensions between species, with a large amount of cecum, with a hindgut, with varying lengths. Typically, four to six Malpighian tubules occur.
Nervous system.
The nervous system in beetles contains all the types found in insects, varying between different species, from three thoracic and seven or eight abdominal ganglia which can be distinguished to that in which all the thoracic and abdominal ganglia are fused to form a composite structure.
Respiratory system.
Like most insects, beetles inhale oxygen and exhale carbon dioxide via a tracheal system. Air enters the body through spiracles, and circulates within the haemocoel in a system of tracheae and tracheoles, through the walls of which the relevant gases can diffuse appropriately.
Diving beetles, such as the Dytiscidae, carry a bubble of air with them when they dive. Such a bubble may be contained under the elytra or against the body by specialized hydrophobic hairs. The bubble covers at least some of the spiracles, thereby permitting the oxygen to enter the tracheae.
The function of the bubble is not so much as to contain a store of air, to act as a physical gill. The air that it traps is in contact with oxygenated water, so as the animal's consumption depletes the oxygen in the bubble, more oxygen can diffuse in to replenish it. Carbon dioxide is more soluble in water than either oxygen or nitrogen, so it readily diffuses out faster than in. Nitrogen is the most plentiful gas in the bubble, and the least soluble, so it constitutes a relatively static component of the bubble and acts as a stable medium for respiratory gases to accumulate in and pass through. Occasional visits to the surface are sufficient for the beetle to re-establish the constitution of the bubble.
Circulatory system.
Like other insects, beetles have open circulatory systems, based on hemolymph rather than blood. Also as in other insects, a segmented tube-like heart is attached to the dorsal wall of the hemocoel. It has paired inlets or "ostia" at intervals down its length, and circulates the hemolymph from the main cavity of the haemocoel and out through the anterior cavity in the head.
Specialized organs.
Different glands specialize for different pheromones produced for finding mates. Pheromones from species of Rutelinea are produced from epithelial cells lining the inner surface of the apical abdominal segments; amino acid-based pheromones of Melolonthinae are produced from eversible glands on the abdominal apex. Other species produce different types of pheromones. Dermestids produce esters, and species of Elateridae produce fatty acid-derived aldehydes and acetates. For means of finding a mate also, fireflies (Lampyridae) use modified fat body cells with transparent surfaces backed with reflective uric acid crystals to biosynthetically produce light, or bioluminescence. The light produce is highly efficient, as it is produced by oxidation of luciferin by enzymes (luciferases) in the presence of adenosine triphosphate (ATP) and oxygen, producing oxyluciferin, carbon dioxide, and light.
A notable number of species have developed special glands to produce chemicals for deterring predators (see Defense and predation). The ground beetle's (of Carabidae) defensive glands, located at the posterior, produce a variety of hydrocarbons, aldehydes, phenols, quinones, esters, and acids released from an opening at the end of the abdomen. African carabid beetles (for example, "Anthia" and "Thermophilum" – Thermophilum generally included within "Anthia") employ the same chemicals as ants: formic acid. Bombardier beetles have well-developed, like other carabid beetles, pygidial glands that empty from the lateral edges of the intersegment membranes between the seventh and eighth abdominal segments. The gland is made of two containing chambers. The first holds hydroquinones and hydrogen peroxide, with the second holding just hydrogen peroxide plus catalases. These chemicals mix and result in an explosive ejection, forming temperatures of around , with the breakdown of hydroquinone to H2 + O2 + quinone, with the O2 propelling the excretion.
Tympanal organs or hearing organs, which is a membrane (tympanum) stretched across a frame backed by an air sac and associated sensory neurons, are described in two families. Several species of the genus "Cicindela" (Cicindelidae) have ears on the dorsal surfaces of their first abdominal segments beneath the wings; two tribes in the subfamily Dynastinae (Scarabaeidae) have ears just beneath their pronotal shields or neck membranes. The ears of both families are sensitive to ultrasonic frequencies, with strong evidence indicating they function to detect the presence of bats by their ultrasonic echolocation. Though beetles constitute a large order and live in a variety of niches, examples of hearing are surprisingly lacking amongst species, though likely most simply remain undiscovered.
Reproduction and development.
Beetles are members of the superorder Endopterygota, and accordingly most of them undergo complete metamorphosis. The typical form of metamorphosis in beetles passes through four main stages: the egg, the larva, the pupa, and the imago or adult. The larvae are commonly called grubs and the pupa sometimes is called the chrysalis. In some species, the pupa may be enclosed in a cocoon constructed by the larva towards the end of its final instar. Going beyond "complete metamorphosis", however, some beetles, such as typical members of the families Meloidae and Rhipiphoridae, undergo hypermetamorphosis in which the first instar takes the form of a triungulin.
Mating.
Beetles may display extremely intricate behavior when mating. Pheromone communication is likely to be important in the location of a mate.
Different species use different chemicals for their pheromones. Some scarab beetles (for example, Rutelinae) utilize pheromones derived from fatty acid synthesis, while other scarab beetles use amino acids and terpenoid compounds (for example, Melolonthinae). Another way species of Coleoptera find mates is the use of biosynthesized light, or bioluminescence. This special form of a mating call is confined to fireflies (Lampyridae) by the use of abdominal light-producing organs. The males and females engage in complex dialogue before mating, identifying different species by differences in duration, flight patterns, composition, and intensity.
Before mating, males and females may engage in various forms of behavior. They may stridulate, or vibrate the objects they are on. In some species (for example, Meloidae), the male climbs onto the dorsum of the female and strokes his antennae on her head, palps, and antennae. In the genus "Eupompha" of said family, the male draws the antennae along his longitudinal vertex. They may not mate at all if they do not perform the precopulatory ritual.
Conflict can play a part in the mating rituals of species such as burying beetles (genus "Nicrophorus"), where conflicts between males and females rage until only one of each is left, thus ensuring reproduction by the strongest and fittest. Many male beetles are territorial and will fiercely defend their small patches of territory from intruding males. In such species, the males may often have horns on their heads and/or thoraces, making their overall body lengths greater than those of the females, unlike most insects. Pairing is generally quick, but in some cases will last for several hours. During pairing, sperm cells are transferred to the female to fertilize the egg.
Lifecycle.
Egg.
A single female may lay from several dozen to several thousand eggs during her lifetime. Eggs are usually laid according to the substrate on which the larvae will feed upon hatching. Among others, they can be laid loose in the substrate (for example, flour beetle), laid in clumps on leaves (for example, Colorado potato beetle), individually attached (for example, mungbean beetle and other seed borers), or buried in the medium (for example, carrot weevil).
Parental care varies between species, ranging from the simple laying of eggs under a leaf to certain scarab beetles, which construct underground structures complete with a supply of dung to house and feed their young. Other beetles are leaf rollers, biting sections of leaves to cause them to curl inwards, then laying their eggs, thus protected, inside.
Larva.
The larva is usually the principal feeding stage of the beetle lifecycle. Larvae tend to feed voraciously once they emerge from their eggs. Some feed externally on plants, such as those of certain leaf beetles, while others feed within their food sources. Examples of internal feeders are most Buprestidae and longhorn beetles. The larvae of many beetle families are predatory like the adults (ground beetles, ladybirds, rove beetles). The larval period varies between species, but can be as long as several years. The larvae are highly varied amongst species, with well-developed and sclerotized heads, and have distinguishable thoracic and abdominal segments (usually the tenth, though sometimes the eighth or ninth).
Beetle larvae can be differentiated from other insect larvae by their hardened, often darkened heads, the presence of chewing mouthparts, and spiracles along the sides of their bodies. Like adult beetles, the larvae are varied in appearance, particularly between beetle families. Beetles whose larvae are somewhat flattened and are highly mobile are the ground beetles, some rove beetles, and others; their larvae are described as campodeiform. Some beetle larvae resemble hardened worms with dark head capsules and minute legs. These are elateriform larvae, and are found in the click beetle (Elateridae) and darkling beetle (Tenebrionidae) families. Some elateriform larvae of click beetles are known as wireworms. Beetles in the families of the Scarabaeoidea have short, thick larvae described as scarabaeiform, but more commonly known as grubs.
All beetle larvae go through several instars, which are the developmental stages between each moult. In many species, the larvae simply increase in size with each successive instar as more food is consumed. In some cases, however, more dramatic changes occur. Among certain beetle families or genera, particularly those that exhibit parasitic lifestyles, the first instar (the planidium) is highly mobile to search out a host, while the following instars are more sedentary and remain on or within their host. This is known as hypermetamorphosis; examples include the blister beetles (family Meloidae) and some rove beetles, particularly those of the genus "Aleochara".
Pupa.
As with all endopterygotes, beetle larvae pupate, and from these pupae emerge fully formed, sexually mature adult beetles, or imagos. Adults have extremely variable lifespans, from weeks to years, depending on the species. In some species, the pupa may go through all four forms during its development, called hypermetamorphosis (for example, Meloidae). Pupae always have no mandibles (are adecticous). In most, the appendages are not attached to the pupae, or they are exarate; with most being obtect in form.
Behavior.
Locomotion.
Aquatic beetles use several techniques for retaining air beneath the water's surface. Beetles of the family Dytiscidae hold air between the abdomen and the elytra when diving. Hydrophilidae have hairs on their under surface that retain a layer of air against their bodies. Adult crawling water beetles use both their elytra and their hind coxae (the basal segment of the back legs) in air retention, while whirligig beetles simply carry an air bubble down with them whenever they dive.
The elytra allow beetles and weevils to both fly and move through confined spaces, doing so by folding the delicate wings under the elytra while not flying, and folding their wings out just before take off. The unfolding and folding of the wings is operated by muscles attached to the wing base; as long as the tension on the radial and cubital veins remains, the wings remain straight. In day-flying species (for example, Buprestidae, Scarabaeidae), flight does not include large amounts of lifting of the elytra, having the metathorac wings extended under the lateral elytra margins.
Communication.
Beetles have a variety of ways to communicate, some of which include a sophisticated chemical language through the use of pheromones. From the host tree, mountain pine beetles have many forms of communication. They can emit both an aggregative pheromone and an anti-aggregative pheromone. The aggregative pheromone attracts other beetles to the tree, and the anti-aggregative pheromone neutralizes the aggregative pheromone. This helps to avoid the harmful effects of having too many beetles on one tree competing for resources. The mountain pine beetle can also stridulate to communicate, or rub body parts together to create sound, having a "scraper" on their abdomens that they rub against a grooved surface on the underside of their left wing cover to create a sound that is not audible to humans. Once the female beetles have arrived on a suitable pine tree host, they begin to stridulate and produce aggregative pheromones to attract other unmated males and females. New females arrive and do the same as they land and bore into the tree. As the males arrive, they enter the galleries that the females have tunneled, and begin to stridulate to let the females know they have arrived, and to also warn others that the female in that gallery is taken. At this point, the female stops producing aggregative pheromones and starts producing anti-aggregative pheromone to deter more beetles from coming.
Since species of Coleoptera use environmental stimuli to communicate, they are affected by the climate. Microclimates, such as wind or temperature, can disturb the use of pheromones; wind would blow the pheromones while they travel through the air. Stridulating can be interrupted when the stimulus is vibrated by something else.
Parental care.
Among insects, parental care is very uncommon, only found in a few species. Some beetles also display this unique social behavior. One theory states parental care is necessary for the survival of the larvae, protecting them from adverse environmental conditions and predators. One species, a rover beetle ("Bledius spectabilis") displays both causes for parental care: physical and biotic environmental factors. Said species lives in salt marshes, so the eggs and/or larvae are endangered by the rising tide. The maternal beetle will patrol the eggs and larvae and apply the appropriate burrowing behavior to keep them from flooding and from asphyxiating. Another advantage is that the mother protects the eggs and larvae from the predatory carabid beetle "Dicheirotrichus gustavi" and from the parasitoid wasp "Barycnemis blediator". Up to 15% of larvae are killed by this parasitoid wasp, being only protected by maternal beetles in their dens.
Some species of dung beetle also display a form of parental care. Dung beetles collect animal feces, or "dung", from which their name is derived, and roll it into a ball, sometimes being up to 50 times their own weight; albeit sometimes it is also used to store food. Usually it is the male that rolls the ball, with the female hitch-hiking or simply following behind. In some cases the male and the female roll together. When a spot with soft soil is found, they stop and bury the dung ball. They will then mate underground. After the mating, one or both of them will prepare the brooding ball. When the ball is finished, the female lays eggs inside it, a form of mass provisioning. Some species do not leave after this stage, but remain to safeguard their offspring.
Feeding.
Besides being abundant and varied, beetles are able to exploit the wide diversity of food sources available in their many habitats. Some are omnivores, eating both plants and animals. Other beetles are highly specialized in their diet. Many species of leaf beetles, longhorn beetles, and weevils are very host-specific, feeding on only a single species of plant. Ground beetles and rove beetles (family Staphylinidae), among others, are primarily carnivorous and will catch and consume many other arthropods and small prey, such as earthworms and snails. While most predatory beetles are generalists, a few species have more specific prey requirements or preferences.
Decaying organic matter is a primary diet for many species. This can range from dung, which is consumed by coprophagous species (such as certain scarab beetles of the family Scarabaeidae), to dead animals, which are eaten by necrophagous species (such as the carrion beetles of the family Silphidae). Some of the beetles found within dung and carrion are in fact predatory. These include the clown beetles, preying on the larvae of coprophagous and necrophagous insects.
Ecology.
Defense and predation.
Beetles and their larvae have a variety of strategies to avoid being attacked by predators or parasitoids. These include camouflage, mimicry, toxicity, and active defense. Camouflage involves the use of coloration or shape to blend into the surrounding environment. This sort of protective coloration is common and widespread among beetle families, especially those that feed on wood or vegetation, such as many of the leaf beetles (family Chrysomelidae) or weevils. In some of these species, sculpturing or various colored scales or hairs cause the beetle to resemble bird dung or other inedible objects. Many of those that live in sandy environments blend in with the coloration of the substrate. The giant African longhorn beetle ("Petrognatha gigas") resembles the moss and bark of the tree it feeds on. Another defense that often uses color or shape to deceive potential enemies is mimicry. A number of longhorn beetles (family Cerambycidae) bear a striking resemblance to wasps, which helps them avoid predation even though the beetles are in fact harmless. This defense is an example of Batesian mimicry and, together with other forms of mimicry and camouflage occurs widely in other beetle families, such as the Scarabaeidae. Beetles may combine their color mimicry with behavioral mimicry, acting like the wasps they already closely resemble. Many beetle species, including ladybirds, blister beetles, and lycid beetles can secrete distasteful or toxic substances to make them unpalatable or even poisonous. These same species often exhibit aposematism, where bright or contrasting color patterns warn away potential predators, and there are, not surprisingly, a great many beetles and other insects that mimic these chemically protected species.
Chemical defense is another important defense found amongst species of Coleoptera, usually being advertised by bright colors. Others may utilize behaviors that would be done when releasing noxious chemicals (for example, Tenebrionidae). Chemical defense may serve purposes other than just protection from vertebrates, such as protection from a wide range of microbes, and repellents. Some species release chemicals in the form of a spray with surprising accuracy, such as ground beetles (Carabidae), may spray chemicals from their abdomen to repel predators. Some species take advantage of the plants from which they feed, and sequester the chemicals from the plant that would protect it and incorporate into their own defense. African carabid beetles (for example, "Anthia" and "Thermophilum") employ the same chemicals used by ants, while Bombardier beetles have a their own unique separate gland, spraying potential predators from far distances.
Large ground beetles and longhorn beetles may defend themselves using strong mandibles and/or spines or horns to forcibly persuade a predator to seek out easier prey. Many species have large protrusions from their thorax and head such as the Rhinoceros beetle, which can be used to defended themselves from predators. Many species of weevil that feed out in the open on leaves of plants react to attack by employing a "drop-off reflex". Even further, some will combine it with thanatosis, which they will close up their legs, antennae, mandibles, etc. and use their cryptic coloration to blend in with the background. Species with varied coloration do not do this as they can not camaflouge.
Parasitism.
Over 1000 species of beetles are known to be either parasitic, predatory, or commensals in the nests of ants. Technically, many species of beetles and their larvae could be considered to be ectoparasites, because they feed on plants and some live inside the bark or wood of trees, but such relationships are generally regarded as herbivory (plant eating) rather than parasitism.
A few species of beetles are actually ectoparasitic on mammals. One such species, "Platypsyllus castoris", parasitises beavers ("Castor" spp.). This beetle lives as a parasite both as a larva and as an adult, feeding on epidermal tissue and possibly on skin secretions and wound exudates. They are strikingly flattened dorsoventrally, no doubt as an adaptation for slipping between the beavers' hairs. They also are wingless and eyeless, as are many other ectoparasites.
Other parasitic beetles include those that are kleptoparasites of other invertebrates, such as the small hive beetle ("Aethina tumida") that infests honey bee hives. The larvae tunnel through comb towards stored honey or pollen, damaging or destroying cappings and comb in the process. Larvae defecate in honey and the honey becomes discolored from the feces, which causes fermentation and a frothiness in the honey; the honey develops a characteristic odor of decaying oranges. Damage and fermentation cause honey to run out of combs, destroying large amounts of it both in hives and sometimes also in honey extracting rooms. Heavy infestations cause bees to abscond; though the beetle is only a minor pest in Africa, beekeepers in other regions have reported the rapid collapse of even strong colonies.
Pollination.
Beetle-pollinated flowers are usually large, greenish or off-white in color, and heavily scented. Scents may be spicy, fruity, or similar to decaying organic material. Most beetle-pollinated flowers are flattened or dish-shaped, with pollen easily accessible, although they may include traps to keep the beetle longer. The plants' ovaries are usually well protected from the biting mouthparts of their pollinators. Beetles may be particularly important in some parts of the world such as semiarid areas of southern Africa and southern California and the montane grasslands of KwaZulu-Natal in South Africa.
Mutualism.
Amongst most orders of insects, mutualism is not common, but some examples occur in species of Coleoptera, such as the ambrosia beetle, the ambrosia fungus, and probably bacteria. The beetles excavate tunnels in dead trees in which they cultivate fungal gardens, their sole source of nutrition. After landing on a suitable tree, an ambrosia beetle excavates a tunnel in which it releases spores of its fungal symbiont. The fungus penetrates the plant's xylem tissue, digests it, and concentrates the nutrients on and near the surface of the beetle gallery, so the weevils and the fungus both benefit. The beetles cannot eat the wood due to toxins, and uses its relationship with fungi to help overcome its host tree defenses and to provide nutrition for their larvae. Chemically mediated by a bacterially produced polyunsaturated peroxide, this mutualistic relationship between the beetle and the fungus is coevolved.
Commensalism.
Pseudoscorpions are small arachnids with flat, pear-shaped bodies and pincers that resemble those of scorpions (only distant relatives), usually ranging from in length. Their small size allows them to hitch rides under the elytra of giant harlequin beetles to be dispersed over wide areas while simultaneously being protected from predators. They may also find mating partners as other individuals join them on the beetle. This would be a form of parasitism if the beetle were harmed in the process, but the beetle is, presumably, unaffected by the presence of the hitchhikers.
Eusociality.
"Austroplatypus incompertus" exhibits eusociality, one of the few organisms outside Hymenoptera to do so, and the only species of Coleoptera.
Phylogeny and systematics.
Fossil record.
A 2007 study based on DNA of living beetles and maps of likely beetle evolution indicated beetles may have originated during the Lower Permian, up to 299 million years ago. In 2009, a fossil beetle was described from the Pennsylvanian of Mazon Creek, Illinois, pushing the origin of the beetles to an earlier date, . Fossils from this time have been found in Asia and Europe, for instance in the red slate fossil beds of Niedermoschel near Mainz, Germany. Further fossils have been found in Obora, Czechia and Tshekarda in the Ural mountains, Russia. However, there are only a few fossils from North America before the middle Permian, although both Asia and North America had been united to Euramerica. The first discoveries from North America made in the Wellington formation of Oklahoma were published in 2005 and 2008.
As a consequence of the Permian–Triassic extinction event, the fossil record of insects is scant, including beetles from the Lower Triassic. However, a few exceptions are noted, as in Eastern Europe; at the Babiy Kamen site in the Kuznetsk Basin, numerous beetle fossils were discovered, even entire specimen of the infraorders Archostemata (e.g. Ademosynidae, Schizocoleidae), Adephaga (e.., Triaplidae, Trachypachidae) and Polyphaga (e.g. Hydrophilidae, Byrrhidae, Elateroidea) and in nearly a perfectly preserved condition. However, species from the families Cupedidae and Schizophoroidae are not present at this site, whereas they dominate at other fossil sites from the Lower Triassic. Further records are known from Khey-Yaga, Russia, in the Korotaikha Basin. There are many important sites from the Jurassic, with more than 150 important sites with beetle fossils, the majority being situated in Eastern Europe and North Asia. In North America and especially in South America and Africa, the number of sites from that time period is smaller, and the sites have not been exhaustively investigated yet. Outstanding fossil sites include Solnhofen in Upper Bavaria, Germany, Karatau in South Kazakhstan, the Yixian formation in Liaoning, North China, as well as the Jiulongshan formation and further fossil sites in Mongolia. In North America there are only a few sites with fossil records of insects from the Jurassic, namely the shell limestone deposits in the Hartford basin, the Deerfield basin and the Newark basin.
A large number of important fossil sites worldwide contain beetles from the Cretaceous. Most are located in Europe and Asia and belong to the temperate climate zone during the Cretaceous. A few of the fossil sites mentioned in the chapter Jurassic also shed some light on the early Cretaceous beetle fauna (for example, the Yixian formation in Liaoning, North China). Further important sites from the Lower Cretaceous include the Crato fossil beds in the Araripe basin in the Ceará, North Brazil, as well as overlying Santana formation, with the latter was situated near the paleoequator, or the position of the earth's equator in the geologic past as defined for a specific geologic period. In Spain, important sites are located near Montsec and Las Hoyas. In Australia, the Koonwarra fossil beds of the Korumburra group, South Gippsland, Victoria, are noteworthy. Important fossil sites from the Upper Cretaceous include Kzyl-Dzhar in South Kazakhstan and Arkagala in Russia.
Evolution.
The oldest known insect that resembles species of Coleoptera date back to the Lower Permian (270 mya), though they instead have 13-segmented antennae, elytra with more fully developed venation and more irregular longitudinal ribbing, and an abdomen and ovipositor extending beyond the apex of the elytra. At the end of the Permian, the biggest mass extinction in history took place, collectively called the Permian–Triassic extinction event (P-Tr): 30% of all insect species became extinct; however, it is the only mass extinction of insects in Earth's history until today.
Due to the P-Tr extinction, the fossil record of insects only includes beetles from the Lower Triassic (). Around this time, during the Late Triassic, mycetophagous, or fungus-feeding species (e.g. Cupedidae) appear in the fossil record. In the stages of the Upper Triassic, representatives of the algophagous, or algae-feeding species (e.g. Triaplidae and Hydrophilidae) begin to appear, as well as predatory water beetles. The first primitive weevils appear (e.g. Obrienidae), as well as the first representatives of the rove beetles (e.g. Staphylinidae), which show no marked difference in morphology compared to recent species.
During the Jurassic (), a dramatic increase in the known diversity of family-level Coleoptera occurred, including the development and growth of carnivorous and herbivorous species. Species of the superfamily Chrysomeloidea are believed to have developed around the same time, which include a wide array of plant hosts ranging from cycads and conifers, to angiosperms. Close to the Upper Jurassic, the portion of the Cupedidae decreased, but at the same time the diversity of the early plant-eating, or phytophagous species increased. Most of the recent phytophagous species of Coleoptera feed on flowering plants or angiosperms. The increase in diversity of the angiosperms is also believed to have influenced the diversity of the phytophagous species, which doubled during the Middle Jurassic. However, doubts have been raised recently, since the increase of the number of beetle families during the Cretaceous does not correlate with the increase of the number of angiosperm species. Also around the same time, numerous primitive weevils (e.g. Curculionoidea) and click beetles (e.g. Elateroidea) appeared. Also, the first jewel beetles (e.g. Buprestidae) are present, but they were rather rare until the Cretaceous. The first scarab beetles appeared around this time, but they were not coprophagous (feeding upon fecal matter), instead presumably feeding upon the rotting wood with the help of fungus; they are an early example of a mutualistic relationship.
The Cretaceous included the initiation of the most recent round of southern landmass fragmentation, via the opening of the southern Atlantic ocean and the isolation of New Zealand, while South America, Antarctica, and Australia grew more distant. During the Cretaceous, the diversity of Cupedidae and Archostemata decreased considerably. Predatory ground beetles (Carabidae) and rove beetles (Staphylinidae) began to distribute into different patterns; whereas the Carabidae predominantly occurred in the warm regions, the Staphylinidae and click beetles (Elateridae) preferred many areas with temperate climates. Likewise, predatory species of Cleroidea and Cucujoidea hunted their prey under the bark of trees together with the jewel beetles (Buprestidae). The jewel beetles' diversity increased rapidly during the Cretaceous, as they were the primary consumers of wood, while longhorn beetles (Cerambycidae) were rather rare, and their diversity increased only towards the end of the Upper Cretaceous. The first coprophagous beetles have been recorded from the Upper Cretaceous, and are believed to have lived on the excrement of herbivorous dinosaurs, but discussion is still ongoing as to whether the beetles were always tied to mammals during their development. Also, the first species with an adaption of both larvae and adults to the aquatic lifestyle are found. Whirligig beetles (Gyrinidae) were moderately diverse, although other early beetles (e.g. Dytiscidae) were less, with the most widespread being the species of Coptoclavidae, which preyed on aquatic fly larvae.
Between the Paleogene and the Neogene is when today's beetles developed. During this time, the continents began to be located closer to where they are today. Around , the land bridge between South America and North America was formed, and the fauna exchange between Asia and North America started. Though many recent genera and species already existed during the Miocene, their distribution differed considerably from today's.
Phylogeny.
The suborders diverged in the Permian and Triassic. Their phylogenetic relationship is uncertain, with the most popular hypothesis being that Polyphaga and Myxophaga are most closely related, with Adephaga as the sister group to those two, and Archostemata as sister to the other three collectively. Although six other competing hypotheses are noted, the other most widely discussed one has Myxophaga as the sister group of all remaining beetles rather than just of Polyphaga. Evidence for a close relationship of the two suborders, Polyphaga and Myxophaga, includes the shared reduction in the number of larval leg articles. The Adephaga are further considered as sister to Myxophaga and Polyphaga, based on their completely sclerotized elytra, reduced number of crossveins in the hind wings, and the folded (as opposed to rolled) hind wings of those three suborders.
Recent cladistic analysis of some of the structural characteristics supports the Polyphaga and Myxophaga hypothesis. The membership of the clade Coleoptera is not in dispute, with the exception of the twisted-wing parasites, Strepsiptera. These odd insects have been regarded as related to the beetle families Rhipiphoridae and Meloidae, with which they share first-instar larvae that are active, host-seeking triungulins and later-instar larvae that are endoparasites of other insects, or the sister group of beetles, or more distantly related to insects. Recent molecular genetic analysis strongly supports the hypothesis that Strepsiptera is the sister group to beetles.
Taxonomy.
About 450,000 species of beetles occur – representing about 40% of all known insects. Such a large number of species poses special problems for classification, with some families consisting of thousands of species and needing further division into subfamilies and tribes. This immense number of species allegedly led evolutionary biologist J. B. S. Haldane to quip, when some theologians asked him what could be inferred about the mind of the Creator from the works of His Creation, that God displayed "an inordinate fondness for beetles".
Relationship to humans.
Beetles as pests.
About 75% of beetle species are phytophagous in both the larval and adult stages, living in or on plants, wood, fungi, and a variety of stored products, including cereals, tobacco, and dried fruits. Because many of these plants are important for agriculture, forestry, and the household, beetles can be considered pests. Some of these species cause significant damage, such as the boll weevil, which feeds on cotton buds and flowers. The boll weevil crossed the Rio Grande near Brownsville, Texas, to enter the United States from Mexico around 1892, and had reached southeastern Alabama by 1915. By the mid-1920s, it had entered all cotton-growing regions in the US, traveling per year. It remains the most destructive cotton pest in North America. Mississippi State University has estimated, since the boll weevil entered the United States, it has cost cotton producers about $13 billion, and in recent times about $300 million per year. Many other species also have done extensive damage to plant populations, such as the bark beetle and elm leaf beetle. The bark beetle and elm leaf beetle, among other species, have been known to nest in elm trees. Bark beetles in particular carry Dutch elm disease as they move from infected breeding sites to feed on healthy elm trees. The spread of Dutch elm disease by the beetle has led to the devastation of elm trees in many parts of the Northern Hemisphere, notably in Europe and North America.
Situations in which a species has developed immunity to pesticides are worse, as in the case of the Colorado potato beetle, "Leptinotarsa decemlineata", which is a notorious pest of potato plants. Crops are destroyed and the beetle can only be treated by employing expensive pesticides, to many of which it has begun to develop resistance. Suitable hosts can include a number of plants from the potato family (Solanaceae), such as nightshade, tomato, eggplant and capsicum, as well as potatoes. The Colorado potato beetle has developed resistance to all major insecticide classes, although not every population is resistant to every chemical.
Pests do not only affect agriculture, but can also even affect houses, such as the death watch beetle. The death watch beetle, "Xestobium rufovillosum" (family Anobiidae), is of considerable importance as a pest of older wooden buildings in Great Britain. It attacks hardwoods such as oak and chestnut, always where some fungal decay has taken or is taking place. The actual introduction of the pest into buildings is thought to take place at the time of construction.
Other pest include the coconut hispine beetle, "Brontispa longissima", which feeds on young leaves and damages seedlings and mature coconut palms. On September 27, 2007, Philippines' Metro Manila and 26 provinces were quarantined due to having been infested with this pest (to save the $800-million Philippine coconut industry). The mountain pine beetle normally attacks mature or weakened lodgepole pine. It can be the most destructive insect pest of mature pine forests. The current infestation in British Columbia is the largest Canada has ever seen.
Beetles as beneficial resources.
Beetles are not only pests, but can also be beneficial, usually by controlling the populations of pests. One of the best, and widely known, examples are the ladybugs or ladybirds (family Coccinellidae). Both the larvae and adults are found feeding on aphid colonies. Other ladybugs feed on scale insects and mealybugs. If normal food sources are scarce, they may feed on small caterpillars, young plant bugs, or honeydew and nectar. Ground beetles (family Carabidae) are common predators of many different insects and other arthropods, including fly eggs, caterpillars, wireworms, and others.
Dung beetles (Scarabidae) have been successfully used to reduce the populations of pestilent flies and parasitic worms that breed in cattle dung. The beetles make the dung unavailable to breeding pests by quickly rolling and burying it in the soil, with the added effect of improving soil fertility, tilth, and nutrient cycling. The Australian Dung Beetle Project (1965–1985), led by Dr. George Bornemissza of the Commonwealth Scientific and Industrial Research Organization, introduced species of dung beetle to Australia from South Africa and Europe, and effectively reduced the bush fly ("Musca vetustissima") population by 90%.
Dung beetles play a remarkable role in agriculture. By burying and consuming dung, they improve nutrient recycling and soil structure. They also protect livestock, such as cattle, by removing dung, which, if left, could provide habitat for pests such as flies. Therefore, many countries have introduced the creatures for the benefit of animal husbandry. In developing countries, the beetle is especially important as an adjunct for improving standards of hygiene. The American Institute of Biological Sciences reports that dung beetles save the United States cattle industry an estimated US$380 million annually through burying above-ground livestock feces.
Some beetles help in a professional setting, doing things that people cannot; those of the family Dermestidae are often used in taxidermy and preparation of scientific specimens to clean bones of remaining soft tissue. The beetle larvae are used to clean skulls because they do a thorough job of cleaning, and do not leave the tool marks that taxidermists' tools do. Another benefit is, with no traces of meat remaining and no emulsified fats in the bones, the trophy will not develop the unpleasant dead odor. Using the beetle larvae means that all cartilage is removed along with the flesh, leaving the bones spotless.
Beetle consumption.
Insects are used as human food in 80% of the world's nations. Beetles are the most widely eaten insects. About 344 species are known to be used as food, usually eaten in the larval stage. The mealworm is the most commonly eaten beetle species. The larvae of the darkling beetle and the rhinoceros beetle are also commonly eaten.
Beetles in art.
Many beetles have beautiful and durable elytra that have been used as material in arts, with beetlewing the best example. Sometimes, they are also incorporated into ritual objects for their religious significance. Whole beetles, either as-is or encased in clear plastic, are also made into objects varying from cheap souvenirs such as key chains to expensive fine-art jewelry. In parts of Mexico, beetles of the genus "Zopherus" are made into living brooches by attaching costume jewelry and golden chains, which is made possible by the incredibly hard elytra and sedentary habits of the genus.
Beetles in ancient cultures.
Many beetles were prominent in ancient cultures. Of these, the most prominent might be the dung beetle in Ancient Egypt. Several species of dung beetle, most notably the species "Scarabaeus sacer" (often referred to as the "sacred scarab"), enjoyed a sacred status among the ancient Egyptians. Popular interpretation in modern academia theorizes the hieroglyphic image of the beetle represents a triliteral phonetic that Egyptologists transliterate as "xpr" or ḫpr and translate as "to come into being", "to become", or "to transform". The derivative term "xprw" or ḫpr(w) is variously translated as "form", "transformation", "happening", "mode of being", or "what has come into being", depending on the context. It may have existential, fictional, or ontologic significance.
The scarab was linked to Khepri ("he who has come into being"), the god of the rising sun. The ancients believed the dung beetle was only male in gender, and reproduced by depositing semen into a dung ball. The supposed self-creation of the beetle resembles that of Khepri, who created himself out of nothing. Moreover, the dung ball rolled by a dung beetle resembles the sun. Plutarch wrote: 
The ancient Egyptians believed Khepri renewed the sun every day before rolling it above the horizon, then carried it through the other world after sunset, only to renew it, again, the next day. Some New Kingdom royal tombs exhibit a threefold image of the sun god, with the beetle as symbol of the morning sun. The astronomical ceiling in the tomb of Ramses VI portrays the nightly "death" and "rebirth" of the sun as being swallowed by Nut, goddess of the sky, and re-emerging from her womb as Khepri.
Excavations of ancient Egyptian sites have yielded images of the scarab in bone, ivory, stone, Egyptian faience, and precious metals, dating from the Sixth Dynasty and up to the period of Roman rule. They are generally small, bored to allow stringing on a necklace, and the base bears a brief inscription or cartouche. Some have been used as seals. Pharaohs sometimes commissioned the manufacture of larger images with lengthy inscriptions, such as the commemorative scarab of Queen Tiye. Massive sculptures of scarabs can be seen at Luxor Temple, at the Serapeum in Alexandria (see Serapis) and elsewhere in Egypt.
The scarab was of prime significance in the funerary cult of ancient Egypt. Scarabs, generally, though not always, were cut from green stone, and placed on the chest of the deceased. Perhaps the most famous example of such "heart scarabs" is the yellow-green pectoral scarab found among the entombed provisions of Tutankhamen. It was carved from a large piece of Libyan desert glass. The purpose of the "heart scarab" was to ensure the heart would not bear witness against the deceased at judgement in the Afterlife. Other possibilities are suggested by the "transformation spells" of the "Coffin Texts", which affirm that the soul of the deceased may transform ("xpr") into a human being, a god, or a bird and reappear in the world of the living.
In contrast to funerary contexts, some of ancient Egypt's neighbors adopted the scarab motif for seals of varying types. The best-known of these are the Judean LMLK seals (eight of 21 designs contained scarab beetles), which were used exclusively to stamp impressions on storage jars during the reign of Hezekiah. The scarab remains an item of popular interest due to modern fascination with the art and beliefs of ancient Egypt. Scarab beads in semiprecious stones or glazed ceramics can be purchased at most bead shops, while at Luxor Temple, a massive ancient scarab has been roped off to discourage visitors from rubbing the base of the statue "for luck".
Beetles in modern cultures.
Beetles still play roles in modern culture. One example is in insect fighting for entertainment and gambling. This sport exploits the territorial behavior and mating competition of certain species of large beetles. Enthusiasts collect and raise various species of insects for fighting. Among beetles, the most popular are large species of stag beetles, rhinoceros beetles, "kabutomushi", and goliath beetles.
The study of beetles is called coleopterology (from Coleoptera, see above, and Greek , "-logia"), and its practitioners are coleopterists. Coleopterists have formed organizations to facilitate the study of beetles. Among these is The Coleopterists Society, an international organization based in the United States. Such organizations may have both professionals and amateurs as members, interested in beetles. Research in this field is often published in peer-reviewed journals specific to the field of coleopterology, though journals dealing with general entomology also publish many papers on various aspects of beetle biology. Some of the journals specific to beetle research are:

</doc>
<doc id="7045" url="http://en.wikipedia.org/wiki?curid=7045" title="Concorde">
Concorde

Aérospatiale-BAC Concorde is a retired turbojet-powered supersonic passenger airliner or supersonic transport (SST). It is one of only two SSTs to have entered commercial service; the other was the Tupolev Tu-144. Concorde was jointly developed and produced by Aérospatiale and the British Aircraft Corporation (BAC) under an Anglo-French treaty. First flown in 1969, Concorde entered service in 1976 and continued commercial flights for 27 years.
Among other destinations, Concorde flew regular transatlantic flights from London Heathrow and Paris-Charles de Gaulle Airport to New York JFK, Washington Dulles and Barbados; it flew these routes in less than half the time of other airliners. With only 20 aircraft built, the development of Concorde was a substantial economic loss; Air France and British Airways also received considerable government subsidies to purchase them. Concorde was retired in 2003 due to a general downturn in the aviation industry after the type's only crash in 2000, the 9/11 terrorist attacks in 2001, and a decision by Airbus, the successor firm of Aérospatiale and BAC, to discontinue maintenance support.
A total of 20 aircraft were built in France and the United Kingdom; six of these were prototypes and development aircraft. Seven each were delivered to Air France and British Airways. Concorde's name reflects the development agreement between the United Kingdom and France. In the UK, any or all of the type—unusually for an aircraft—are known simply as "Concorde", without an article. The aircraft is regarded by many people as an aviation icon and an engineering marvel.
Development.
Early studies.
The origins of the Concorde project date to the early 1950s, when Arnold Hall, director of the Royal Aircraft Establishment (RAE) asked Morien Morgan to form a committee to study the SST concept. The group met for the first time in February 1954 and delivered their first report in April 1955.
At the time it was known that the drag at supersonic speeds was strongly related to the span of the wing. This led to the use of very short-span, very thin rectangular wings like those seen on the control surfaces of many missiles, or in aircraft like the Lockheed F-104 Starfighter or the Avro 730 that the team studied. The team outlined a baseline configuration that looked like an enlarged Avro 730, or more interestingly, almost exactly like the Lockheed CL-400 "Suntan" proposal.
This same short span produced very little lift at low speed, which resulted in extremely long takeoff runs and frighteningly high landing speeds. In an SST design, this would have required enormous engine power to lift off from existing runways, and to provide the fuel needed, "some horribly large aeroplanes" resulted. Based on this, the group considered the concept of an SST unfeasible, and instead suggested continued low-level studies into supersonic aerodynamics.
Slender deltas.
Soon after, Dietrich Küchemann at the RAE published a series of reports on a new wing planform, known in the UK as the "slender delta" concept. Küchemann's team, including Eric Maskell and Küchemann's wife Johanna Weber, worked with the fact that delta wings can produce strong vortexes on their upper surfaces at high angles of attack. The vortex will lower the air pressure and cause lift to be greatly increased. This effect had been noticed earlier, notably by Chuck Yeager in the Convair XF-92, but its qualities had not been fully appreciated. Küchemann suggested that this was no mere curiosity, and the effect could be deliberately used to improve low speed performance.
Küchemann's papers changed the entire nature of supersonic design almost overnight. Although the delta had already been used on aircraft prior to this point, these designs used planforms that were not much different from a swept wing of the same span. Küchemann noted that the lift from the vortex was increased by the length of the wing it had to operate over, which suggested that the effect would be maximized by extending the wing along the fuselage as far as possible. Such a layout would still have good supersonic performance inherent to the short span, while also offering reasonable takeoff and landing speeds using vortex generation. The only downside to such a design is that the aircraft would have to take off and land very "nose high" in order to generate the required vortex lift, which led to questions about the low speed handling qualities of such a design. It would also need to have long landing gear to produce the required angles while still on the runway.
Küchemann presented the idea at a meeting where Morgan was also present. Test pilot Eric Brown recalls Morgan's reaction to the presentation, saying that he immediately seized on it as the solution to the SST problem. Brown considers this moment as being the true birth of the Concorde project.
STAC.
On 1 October 1956 the Ministry of Supply asked Morgan to form a new study group, the Supersonic Transport Advisory Committee or STAC, with the explicit goal of developing a practical SST design and finding industry partners to build it. At the very first meeting, on 5 November 1956, the decision was made to fund the development of a testbed aircraft to examine the low speed performance of the slender delta, a contract that eventually produced the Handley Page HP.115. This aircraft would ultimately demonstrate safe control at speeds as low as 69 mph, about ⅓rd that of the Starfighter.
STAC claimed that an SST would have economic performance similar to existing subsonic types. Although they would burn more fuel in cruise, they would be able to fly more sorties in a given period of time, so fewer aircraft would be needed to service a particular route. This would remain economically advantageous as long as fuel represented a small percentage of operational costs, as it did at the time. STAC suggested that two designs naturally fell out of their work, a transatlantic model flying at about Mach 2, and a shorter-range version flying at perhaps Mach 1.2. Morgan suggested that a 150 passenger transatlantic SST would cost about £75 to £90 million to develop to production, and be in service in 1970. The smaller 100 passenger short-range version would cost perhaps £50 to £80 million, and be ready for service in 1968. To meet this schedule, development would need to begin in 1960, with production contracts let in 1962. Morgan strongly suggested that the US was already involved in a similar project, and that if the UK failed to respond it would be locked out of an airliner market that he believed would be dominated by SST aircraft.
In 1959 a study contract was let to Hawker Siddeley and Bristol for preliminary designs based on the slender delta concept, which developed as the HSA.1000 and Bristol 198. Armstrong Whitworth also responded with an internal design, the "M-Wing", for the lower-speed shorter-range category. Even at this early time, both the STAC group and the government were looking for partners to develop the designs. In September 1959 Hawker approached Lockheed, and after the creation of British Aircraft Corporation in 1960, the former Bristol team immediately started talks with Boeing, General Dynamics, Douglas Aircraft and Sud Aviation.
Ogee planform selected.
Küchemann and others at the RAE continued their work on the slender delta throughout, considering three basic shapes; the classic straight-edge delta, the "gothic delta" that was rounded outwards to appear like a gothic arch, and the "ogival wing" that was compound-rounded into the shape of an ogee. Each of these planforms had their own advantages and disadvantages in terms of aerodynamics. As they worked with these shapes, a practical concern grew to become so important that it forced selection of one of these designs.
Generally one wants to have the wing's center of pressure (CoP, or "lift point") close to the aircraft's center of gravity (CoG, or "balance point") in order to lower the amount of control force that needs to be applied in order to pitch the aircraft. As the aircraft layout changes during the design phase, it is common for the CoG to move fore or aft. With a normal wing design this can be addressed by moving the wing slightly fore or aft to account for this. With a delta wing running most of the length of the fuselage, this was no longer easy; moving the wing would leave it in front of the nose or behind the tail. Studying the various layouts in terms of CoG changes, both during design and changes due to fuel use during flight, the ogee planform immediately came to the fore.
While the wing planform was evolving, so was the basic SST concept. Bristol's original Model 198 was a small design with an almost pure slender delta wing, but evolved into the larger Model 223 with an ogival wing and canards as well.
Partnership with Sud.
By this time similar political and economic concerns in France had led to their own SST plans. In the late 1950s the government requested designs from both the government-owned Sud and Nord, as well as Dassault. All three returned designs based on Küchemann's slender delta; Nord suggested a ramjet powered design flying at Mach 3, the other two were jet powered Mach 2 designs that were similar to each other. Of the three, the Sud Aviation Super Caravelle won the design contest with a medium-range design deliberately sized to avoid competition with transatlantic US designs they assumed were already on the drawing board.
As soon as the design was complete, in April 1960, Pierre Satre, the company's technical director, was sent to Bristol to discuss a partnership. Bristol was surprised to find that the Sud team had designed a very similar aircraft after considering the SST problem and coming to the very same conclusions as the Bristol and STAC teams in terms of economics. It was later revealed that the original STAC report, marked "For UK Eyes Only", had secretly been passed to the French in order to win political favour. Sud made minor changes, and presented it as their own work.
The two teams found much to agree on. The French had no modern large jet engines, and had already concluded they would be buying a British design anyway (as they had on the earlier subsonic Caravelle). As neither company had experience in the use of high-heat metals for airframes, a maximum speed of around Mach 2 was selected so aluminium could be used – above this speed the friction with the air warms the metal so much that aluminium begins to soften. This lower speed would also speed development and allow their design to fly before the Americans. Finally, everyone involved agreed that Küchemann's ogive design (ogee shaped wings) was the right one.
The only disagreements were over the size and range. The UK team was still focussed on a 150 passenger design serving transatlantic routes, while the French were deliberately avoiding these. However, this proved not to be the barrier it might seem; common components could be used in both designs, with the shorter range version using a clipped fuselage and four engines, the longer one with a stretched fuselage and six engines, leaving only the wing to be extensively re-designed. The teams continued to meet through 1961, and by this time it was clear that the two aircraft would be considerably more similar in spite of different range and seating arrangements. A single design emerged that differed primarily in fuel load. More powerful engines, being developed for the TSR-2, allowed either design to be powered by only four engines.
Cabinet response, treaty.
While the development teams met, French Minister of Public Works and Transport Robert Burton was meeting with the UK Minister of Aviation Peter Thorneycroft, and Thorneycroft soon revealed to the cabinet that the French were much more serious about a partnership than any of the US companies. The various US companies had proved uninterested in such a venture, likely due to the belief that the government would be funding development and would frown on any partnership with a European company, and the risk of "giving away" US technological leadership to a European partner.
When the STAC plans were presented to the UK cabinet, a very negative reaction resulted. The economic considerations were considered highly questionable, especially as these were based on development costs, now estimated to be £150 million, which were constantly being overrun in the industry. The Treasury Ministry in particular presented a very negative view, suggesting that there was no way the project would have any positive financial returns for the government, especially in light that "the industry's past record of over-optimistic estimating (including the recent history of the TSR.2) suggests that it would be prudent to consider the £150 million [cost] to turn out much too low."
This concern led to an independent review of the project by the Committee on Civil Scientific Research and Development, which met on topic between July and September 1962. The Committee ultimately rejected the economic arguments, including considerations of supporting the industry made by Thorneycroft. Their report in October stated that it was unlikely there would be any direct positive economic outcome, but that the project should still be considered for the simple reason that everyone else was going supersonic, and they were concerned they would be locked out of future markets. Conversely, it appeared the project would not be likely to significantly impact other, more important, research efforts.
After considerable argument, the decision to proceed ultimately fell to an unlikely political expediency. At the time, the UK was pressing for admission to the European Common Market, which was being controlled by Charles de Gaulle who felt the UK's Special Relationship with the US made them unacceptable in a pan-European group. Cabinet felt that signing a deal with Sud would pave the way for Common Market entry, and this became the main deciding reason for moving ahead with the deal. It was this belief that had led the original STAC documents being leaked to the French. However, De Gaulle spoke of the European origin of the design, and continued to block the UK's entry into the Common Market.
The development project was negotiated as an international treaty between the two countries rather than a commercial agreement between companies and included a clause, originally asked for by the UK, imposing heavy penalties for cancellation. A draft treaty was signed on 29 November 1962.
Naming.
Reflecting the treaty between the British and French governments which led to Concorde's construction, the name "Concorde" is from the French word "concorde" (), which has an English equivalent, "concord". Both words mean "agreement", "harmony" or "union". The name was officially changed to "Concord" by Harold Macmillan in response to a perceived slight by Charles de Gaulle. In 1967, at the French roll-out in Toulouse the British Government Minister for Technology, Tony Benn, announced that he would change the spelling back to "Concorde". This created a nationalist uproar that died down when Benn stated that the suffixed 'e' represented "Excellence, England, Europe and Entente (Cordiale)." In his memoirs, he recounts a tale of a letter from an irate Scotsman claiming: "[Y]ou talk about 'E' for England, but part of it is made in Scotland." Given Scotland’s contribution of providing the nose cone for the aircraft, Benn replied, "[I]t was also 'E' for 'Écosse' (the French name for Scotland)  — and I might have added 'e' for extravagance and 'e' for escalation as well!"
Concorde also acquired an unusual nomenclature for an aircraft. In common usage in the United Kingdom, the type is known as "Concorde" without an article, rather than the Concorde" or a Concorde".
Sales efforts.
At first the new consortium intended to produce one long-range and one short-range version. However, prospective customers showed no interest in the short-range version and it was dropped.
The consortium secured orders (i.e., non-binding options) for over 100 of the long-range version from the major airlines of the day: Pan Am, BOAC, and Air France were the launch customers, with six Concordes each. Other airlines in the order book included Panair do Brasil, Continental Airlines, Japan Airlines, Lufthansa, American Airlines, United Airlines, Air India, Air Canada, Braniff, Singapore Airlines, Iran Air, Olympic Airways, Qantas, CAAC, Middle East Airlines, and TWA.
At the time of the first flight the options list contained 74 options from 16 airlines:
Testing.
The design work was supported by a preceding research programme studying the flight characteristics of low ratio delta wings. The supersonic BAC 221 was modified for flight tests of the high speed flight envelope, the Handley Page HP.115 also provided valuable information on low speed performance.
Construction of two prototypes began in February 1965: 001, built by Aerospatiale at Toulouse, and 002, by BAC at Filton, Bristol. Concorde 001 made its first test flight from Toulouse on 2 March 1969, piloted by André Turcat, and first went supersonic on 1 October. The first UK-built Concorde flew from Filton to RAF Fairford on 9 April 1969, piloted by Brian Trubshaw. Both prototypes were presented to the public for the first time on 7–8 June 1969 at the Paris Air Show. As the flight programme progressed, 001 embarked on a sales and demonstration tour on 4 September 1971, which was also the first transatlantic crossing of Concorde. Concorde 002 followed suit on 2 June 1972 with a tour of the Middle and Far East. Concorde 002 made the first visit to the United States in 1973, landing at the new Dallas/Fort Worth Regional Airport to mark that airport’s opening.
While Concorde had initially held a great deal of customer interest, the project was hit by a large number of order cancellations. The Paris Le Bourget air show crash of the competing Soviet Tupolev Tu-144 had shocked potential buyers, and public concern over the environmental issues presented by a supersonic aircraft – the sonic boom, takeoff-noise and pollution – had produced a shift in public opinion of SSTs. By 1976 four nations remained as prospective buyers: Britain, France, China, and Iran. Only Air France and British Airways (the successor to BOAC) took up their orders, with the two governments taking a cut of any profits made.
The United States cancelled the Boeing 2707, its rival supersonic transport programme, in 1971. Observers have suggested that opposition to Concorde on grounds of noise pollution had been encouraged by the United States Government, as it lacked its own competitor. The US, India, and Malaysia all ruled out Concorde supersonic flights over the noise concern, although some of these restrictions were later relaxed. Professor Douglas Ross characterised restrictions placed upon Concorde operations by President Jimmy Carter's administration as having been an act of protectionism of American aircraft manufacturers.
Concorde had other considerable difficulties that led to its dismal sales performance. Costs had spiralled during development to more than six times the original projections, arriving at a unit cost of £23 million in 1977. World events had also dampened Concorde sales prospects, the 1973 oil crisis made many airlines think twice about aircraft with high rates of fuel consumption; and new wide-body aircraft, such as the Boeing 747, had recently made subsonic aircraft significantly more efficient and presented a low-risk option for airlines. While carrying a full load, Concorde achieved 15.8 passenger miles per gallon of fuel, while the Boeing 707 reached 33.3 pm/g, the Boeing 747 46.4 pm/g, and the McDonnell Douglas DC-10 53.6 pm/g. An emerging trend in the industry in favour of cheaper airline tickets had also caused airlines such as Qantas to question Concorde's market suitability.
Design.
General features.
Concorde is an ogival (also "ogee") delta-winged aircraft with four Olympus engines based on those employed in the RAF's Avro Vulcan strategic bomber. Concorde was the first airliner to have a (in this case, analogue) fly-by-wire flight-control system; the avionics of Concorde were unique because it was the first commercial aircraft to employ hybrid circuits. The principal designer for the project was Pierre Satre, with Sir Archibald Russell as his deputy.
Concorde pioneered the following technologies:
For high speed and optimisation of flight:
For weight-saving and enhanced performance:
Powerplant.
Concorde needed to fly long distances to be economically viable; this required high efficiency. Turbofan engines were rejected due to their larger cross-section producing excessive drag. Turbojets were found to be the best choice of engines. The engine used was the twin spool Rolls-Royce/Snecma Olympus 593, a development of the Bristol engine first used for the Avro Vulcan bomber, and developed into an afterburning supersonic variant for the BAC TSR-2 strike bomber. Rolls-Royce's own engine proposed for the aircraft at the time of Concorde's initial design was the RB.169.
The aircraft used reheat (afterburners) at takeoff and to pass through the upper transonic regime and to supersonic speeds, between Mach 0.95 and Mach 1.7. The afterburners were switched off at all other times. Due to jet engines being highly inefficient at low speeds, Concorde burned two tonnes of fuel (almost 2% of the maximum fuel load) taxiing to the runway. Fuel used is Jet A-1. Due to the high thrust produced even with the engines at idle, only the two outer engines were run after landing for easier taxiing.
The intake design for Concorde’s engines was especially critical. The intakes had to provide low distortion levels (to prevent engine surge) and high efficiency for all likely ambient temperatures to be met in cruise. They had to provide adequate subsonic performance for diversion cruise and low engine-face distortion at take-off. They also had to provide an alternate path for excess intake air during engine throttling or shutdown. The variable intake features required to meet all these requirements consisted of front and rear ramps, a dump door, an auxiliary inlet and a ramp bleed to the exhaust nozzle.
As well as supplying air to the engine the intake also supplied air through the ramp bleed to the propelling nozzle. The nozzle ejector (or aerodynamic) design, with variable exit area and secondary flow from the intake, contributed to good expansion efficiency from take-off to cruise.
Engine failure causes problems on conventional subsonic aircraft; not only does the aircraft lose thrust on that side but the engine creates drag, causing the aircraft to yaw and bank in the direction of the failed engine. If this had happened to Concorde at supersonic speeds, it theoretically could have caused a catastrophic failure of the airframe. Although computer simulations predicted considerable problems, in practice Concorde could shut down both engines on the same side of the aircraft at Mach 2 without the predicted difficulties. During an engine failure the required air intake is virtually zero so, on Concorde, engine failure was countered by the opening of the auxiliary spill door and the full extension of the ramps, which deflected the air downwards past the engine, gaining lift and minimising drag. Concorde pilots were routinely trained to handle double engine failure.
Concorde's thrust-by-wire engine control system was developed by Ultra Electronics.
Heating issues.
Air compression on the outer surfaces caused the cabin to heat up during flight. Every surface, such as windows and panels, was warm to the touch by end of the flight. Besides engines, the hottest part of the structure of any supersonic aircraft, due to aerodynamic heating, is the nose. The engineers used Hiduminium R.R. 58, an aluminium alloy, throughout the aircraft due to its familiarity, cost and ease of construction. The highest temperature that aluminium could sustain over the life of the aircraft was , which limited the top speed to Mach 2.02. Concorde went through two cycles of heating and cooling during a flight, first cooling down as it gained altitude, then heating up after going supersonic. The reverse happened when descending and slowing down. This had to be factored into the metallurgical and fatigue modelling. A test rig was built that repeatedly heated up a full-size section of the wing, and then cooled it, and periodically samples of metal were taken for testing. The Concorde airframe was designed for a life of 45,000 flying hours.
Owing to air friction as the plane travelled at supersonic speed, the fuselage would heat up and expand by as much as 300 mm (almost 1 ft). The most obvious manifestation of this was a gap that opened up on the flight deck between the flight engineer's console and the bulkhead. On some aircraft that conducted a retiring supersonic flight, the flight engineers placed their caps in this expanded gap, wedging the cap when it shrank again. To keep the cabin cool, Concorde used the fuel as a heat sink for the heat from the air conditioning. The same method also cooled the hydraulics. During supersonic flight the surfaces forward from the cockpit became heated, and a visor was used to deflect much of this heat from directly reaching the cockpit.
Concorde had livery restrictions; the majority of the surface had to be covered with a highly reflective white paint to avoid overheating the aluminium structure due to heating effects from supersonic flight at Mach 2. The white finish reduced the skin temperature by 6 to 11 degrees Celsius. In 1996, Air France briefly painted F-BTSD in a predominantly blue livery, with the exception of the wings, in a promotional deal with Pepsi. In this paint scheme, Air France were advised to remain at Mach 2 for no more than 20 minutes at a time, but there was no restriction at speeds under Mach 1.7. F-BTSD was used because it was not scheduled for any long flights that required extended Mach 2 operations.
Structural issues.
Due to the high speeds at which Concorde travelled, large forces were applied to the aircraft's structure during banks and turns. This caused twisting and the distortion of the aircraft’s structure. In addition there were concerns over maintaining precise control at supersonic speeds; both of these issues were resolved by active ratio changes between the inboard and outboard elevons, varying at differing speeds including supersonic. Only the innermost elevons, which are attached to the stiffest area of the wings, were active at high speed. Additionally, the narrow fuselage meant that the aircraft flexed. This was visible from the rear passengers’ viewpoints.
When any aircraft passes the critical mach of that particular airframe, the centre of pressure shifts rearwards. This causes a pitch down force on the aircraft if the centre of mass remains where it was. The engineers designed the wings in a specific manner to reduce this shift, but there was still a shift of about 2 metres. This could have been countered by the use of trim controls, but at such high speeds this would have caused a dramatic increase in the drag on the aircraft. Instead, the distribution of fuel along the aircraft was shifted during acceleration and deceleration to move the centre of mass, effectively acting as an auxiliary trim control.
Range.
In order to fly non-stop across the Atlantic Ocean, Concorde was developed to have the greatest supersonic range of any aircraft. This was achieved by a combination of engines which were highly efficient at supersonic speeds, a slender fuselage with high fineness ratio, and a complex wing shape for a high lift to drag ratio. This also required carrying only a modest payload and a high fuel capacity, and the aircraft was trimmed with precision to avoid unnecessary drag.
Nevertheless, soon after Concorde began flying, a Concorde "B" model was designed with slightly larger fuel capacity and slightly larger wings with leading edge slats to improve aerodynamic performance at all speeds, with the objective of expanding the range to reach markets in new regions. It featured more powerful engines with sound deadening and without the fuel-hungry and noisy reheat. It was speculated that it was reasonably possible to create an engine with up to 25% gain in efficiency over the Rolls-Royce/Snecma Olympus 593. This would have given additional range and a greater payload, making new commercial routes possible. This was cancelled due in part to poor sales of Concorde, but also to the rising cost of aviation fuel in the 1970s.
Radiation concerns.
The high altitude at which Concorde cruised meant passengers received almost twice the flux of extraterrestrial ionising radiation as those travelling on a conventional long-haul flight. Upon Concorde's introduction, it was speculated that this exposure during supersonic travels would increase the likelihood of skin cancer. Due to the proportionally reduced flight time, the overall equivalent dose would normally be less than a conventional flight over the same distance. Unusual solar activity might lead to an increase in incident radiation. To prevent incidents of excessive radiation exposure, the flight deck had a radiometer and an instrument to measure the rate of decrease of radiation. If the radiation level became too high, Concorde would descend below .
Cabin pressurisation.
Airliner cabins were usually maintained at a pressure equivalent to 6,000–8,000 feet (1,800–2,400 m) elevation. Concorde’s pressurisation was set to an altitude at the lower end of this range, . Concorde’s maximum cruising altitude was ; subsonic airliners typically cruise below .
A sudden reduction in cabin pressure is hazardous to all passengers and crew. Above , in the event of a sudden cabin depressurisation, the lack of air pressure would give a "time of useful consciousness" in even a conditioned athlete of no more than 10–15 seconds. At Concorde’s altitude, the air density is very low; a breach of cabin integrity would result in a loss of pressure severe enough so that the plastic emergency oxygen masks installed on other passenger jets would not be effective and passengers would quickly suffer from hypoxia despite quickly donning them. Concorde was equipped with smaller windows to reduce the rate of loss in the event of a breach, a reserve air supply system to augment cabin air pressure, and a rapid descent procedure to bring the aircraft to a safe altitude. The FAA enforces minimum emergency descent rates for aircraft and made note of Concorde’s higher operating altitude, concluding that the best response to a loss of pressure would be a rapid descent. Continuous positive airway pressure would have delivered pressurised oxygen directly to the pilots through masks.
Flight characteristics.
While commercial jets took eight hours to fly from New York to Paris, the average supersonic flight time on the transatlantic routes was just under 3.5 hours. Concorde had a maximum cruise altitude of and an average cruise speed of Mach 2.02, about 1155 knots (2140 km/h or 1334 mph), more than twice the speed of conventional aircraft.
With no other civil traffic operating at its cruising altitude of about , dedicated oceanic airways or "tracks" were used by Concorde to cross the Atlantic. Due to the nature of high altitude winds, these SST tracks were fixed in terms of their co-ordinates, unlike the North Atlantic Tracks at lower altitudes whose co-ordinates alter daily according to forecast weather patterns (jetstreams). Concorde would also be cleared in a block, allowing for a slow climb from 45,000 to during the oceanic crossing as the fuel load gradually decreased. In regular service, Concorde employed an efficient "cruise-climb" flight profile following take-off.
The delta-shaped wings required Concorde to adopt a higher angle of attack at low speeds than conventional aircraft, but it allowed the formation of large low pressure vortices over the entire upper wing surface, maintaining lift. The normal landing speed was . Because of this high angle, during a landing approach Concorde was on the "back side" of the drag force curve, where raising the nose would increase the rate of descent; the aircraft was thus largely flown on the throttle and was fitted with an autothrottle to reduce the pilot's workload.
Brakes and undercarriage.
Because of the way Concorde's delta-wing generated lift, the undercarriage had to be unusually strong. At rotation, Concorde would rise to a high angle of attack, about 18 degrees. Prior to rotation the wing generated almost no lift, unlike typical aircraft wings. Combined with the high airspeed at rotation (199 knots indicated airspeed), this increased the stresses on the rear undercarriage in a way that was initially unexpected during the development and required a major redesign. Due to the high angle needed at rotation, a small set of wheels were added aft to prevent tailstrikes. The rear main undercarriage units swing towards each other to be stowed but due to their great height also need to retract telescopically before swinging in order to clear each other when stowed. The four main wheel tyres on each bogie unit are inflated to 232 lb/sq in. The twin-wheel nose undercarriage retracts forwards and its tyres are inflated to a pressure of 191 lb/sq in, and the wheel assembly carries a spray deflector to prevent standing water being thrown up into the engine intakes. The tyres are rated to 250 mph. The starboard nose wheel carries a single disc brake to halt wheel rotation while the undercarriage is being retracted. The port nose wheel carries speed generators for the anti-skid braking system which prevents brake activation until nose and main wheels are rotating at the same rate.
Additionally, due to the high average takeoff speed of , Concorde needed upgraded brakes. Like most airliners, Concorde has anti-skid braking – a system which prevents the tyres from losing traction when the brakes are applied for greater control during roll-out. The brakes, developed by Dunlop, were the first carbon-based brakes used on an airliner. The use of carbon over equivalent steel brakes provided a weight-saving of . Each wheel has multiple discs which are cooled by electric fans. Wheel sensors include brake overload, brake temperature, and tyre deflation. After a typical landing at Heathrow, brake temperatures were around 300-400 °C (572-752 °F).
Droop nose.
Concorde’s drooping nose, developed by Marshall Aerospace, enabled the aircraft to switch between being streamlined to reduce drag and achieve optimum aerodynamic efficiency, and not obstructing the pilot's view during taxi, takeoff, and landing operations. Due to the high angle of attack, the long pointed nose obstructed the view and necessitated the capability to droop. The droop nose was accompanied by a moving visor that retracted into the nose prior to being lowered. When the nose was raised to horizontal, the visor would rise in front of the cockpit windscreen for aerodynamic streamlining.
A controller in the cockpit allowed the visor to be retracted and the nose to be lowered to 5° below the standard horizontal position for taxiing and takeoff. Following takeoff and after clearing the airport, the nose and visor were raised. Prior to landing, the visor was again retracted and the nose lowered to 12.5° below horizontal for maximum visibility. Upon landing the nose was raised to the five-degree position to avoid the possibility of damage.
The Federal Aviation Administration had objected to the restrictive visibility of the visor used on the first two prototype Concordes and thus requiring alteration before the FAA would permit Concorde to serve US airports; this led to the redesigned visor used on the production and the four pre-production aircraft (101, 102, 201, and 202). The nose window and visor glass needed to endure temperatures in excess of 100 °C (212 °F) at supersonic flight were developed by Triplex.
Operational history.
Scheduled flights.
Scheduled flights began on 21 January 1976 on the London–Bahrain and Paris–Rio (via Dakar) routes, with BA flights using the "Speedbird Concorde" call sign to notify air traffic control of the aircraft’s unique abilities and restrictions, but the French using their normal call signs. The Paris-Caracas route (via Azores) began on 10 April. The US Congress had just banned Concorde landings in the US, mainly due to citizen protest over sonic booms, preventing launch on the coveted North Atlantic routes. The US Secretary of Transportation, William Coleman, gave permission for Concorde service to Washington Dulles International Airport, and Air France and British Airways simultaneously began service to Dulles on 24 May 1976.
When the US ban on JFK Concorde operations was lifted in February 1977, New York banned Concorde locally. The ban came to an end on 17 October 1977 when the Supreme Court of the United States declined to overturn a lower court’s ruling rejecting efforts by the Port Authority and a grass-roots campaign led by Carol Berman to continue the ban. In spite of complaints about noise, the noise report noted that Air Force One, at the time a Boeing VC-137, was louder than Concorde at subsonic speeds and during takeoff and landing. Scheduled service from Paris and London to New York’s John F. Kennedy Airport began on 22 November 1977.
In 1977, British Airways and Singapore Airlines shared a Concorde for flights between London and Singapore International Airport at Paya Lebar via Bahrain. The aircraft, BA’s Concorde G-BOAD, was painted in Singapore Airlines livery on the port side and British Airways livery on the starboard side. The service was discontinued after three return flights because of noise complaints from the Malaysian government; it could only be reinstated on a new route bypassing Malaysian airspace in 1979. A dispute with India prevented Concorde from reaching supersonic speeds in Indian airspace, so the route was eventually declared not viable and discontinued in 1980.
During the Mexican oil boom, Air France flew Concorde twice weekly to Mexico City’s Benito Juárez International Airport via Washington, DC, or New York City, from September 1978 to November 1982. The worldwide economic crisis during that period resulted in this route’s cancellation; the last flights were almost empty. The routing between Washington or New York and Mexico City included a deceleration, from Mach 2.02 to Mach 0.95, to cross Florida subsonically and avoid creating a sonic boom over the state; Concorde then re-accelerated back to high speed while crossing the Gulf of Mexico. On 1 April 1989, on an around-the-world luxury tour charter, British Airways implemented changes to this routing that allowed G-BOAF to maintain Mach 2.02 by passing around Florida to the east and south. Periodically Concorde visited the region on similar chartered flights to Mexico City and Acapulco.
From 1978 to 1980, Braniff International Airways leased 10 Concordes, five each from Air France and British Airways. These were used on subsonic flights between Dallas-Fort Worth and Washington Dulles International Airport, flown by Braniff flight crews. Air France and British Airways crews then took over for the continuing supersonic flights to London and Paris. The aircraft were registered in both the United States and their home countries; the European registration was covered while being operated by Braniff, retaining full AF/BA liveries. The flights were not profitable and typically less than 50% booked, forcing Braniff to end its tenure as the only US Concorde operator in May 1980.
In its early years, the British Airways Concorde service had a greater number of "no shows" (passengers who booked a flight and then failed to appear at the gate for boarding) than any other aircraft in the fleet.
British Caledonian interest.
Following the launch of British Airways' Concorde services, Britain's other major airline, British Caledonian (BCal), set up a task force headed by Gordon Davidson, BA's former Concorde director, to investigate the possibility of their own Concorde operations. This was seen as particularly viable for the airline's long-haul network as there were two unsold aircraft then available for purchase.
One important reason for BCal's interest in Concorde was that the British Government's 1976 aviation policy review had opened the possibility of BA setting up supersonic services in competition with BCal's established sphere of influence. To counteract this potential threat, BCal considered their own independent Concorde plans, as well as a partnership with BA. BCal were considered most likely to have set up a Concorde service on the Gatwick–Lagos route, a major source of revenue and profits within BCal's scheduled route network; BCal's Concorde task force did assess the viability of a daily supersonic service complementing the existing subsonic widebody service on this route.
BCal entered into a bid to acquire at least one Concorde. However, BCal eventually arranged for two aircraft to be leased from BA and Aérospatiale respectively, to be maintained by either BA or Air France. BCal's envisaged two-Concorde fleet would have required a high level of aircraft utilisation to be cost-effective; therefore, BCal had decided to operate the second aircraft on a supersonic service between Gatwick and Atlanta, with a stopover at either Gander or Halifax. Consideration was given to services to Houston and various points on its South American network at a later stage. Both supersonic services were to be launched at some point during 1980; however, steeply rising oil prices caused by the 1979 energy crisis led to BCal shelving their supersonic ambitions.
British Airways buys its Concordes outright.
By around 1981 in the UK, the future for Concorde looked bleak. The British government had lost money operating Concorde every year, and moves were afoot to cancel the service entirely. A cost projection came back with greatly reduced metallurgical testing costs because the test rig for the wings had built up enough data to last for 30 years and could be shut down. Despite this, the government was not keen to continue. In 1983, BA's managing director, Sir John King, convinced the government to sell the aircraft outright to British Airways for £16.5 million plus the first year’s profits.
King recognised that, in Concorde, BA had a premier product that was underpriced. Market research had revealed that many customers thought Concorde was more expensive than it actually was; thus ticket prices were progressively raised to match these perceptions. It is reported that British Airways then ran Concorde at a profit, unlike their French counterpart.
Between 1984 and 1991, British Airways flew a thrice-weekly Concorde service between London and Miami, stopping at Washington Dulles International Airport. Until 2003, Air France and British Airways continued to operate the New York services daily. Concorde routinely flew to Grantley Adams International Airport, Barbados, during the winter holiday season.
Prior to the Air France Paris crash, several UK and French tour operators operated charter flights to European destinations on a regular basis; the charter business was viewed as lucrative by British Airways and Air France.
In 1997, British Airways held a promotional contest to mark the 10th anniversary of the airline's move into the private sector. The promotion was a lottery to fly to New York held for 190 tickets valued at £5,400 each, to be offered at £10. Contestants had to call a special hotline to compete with up to 20 million people.
Concorde Flight 4590 crash.
On 25 July 2000, Air France Flight 4590, registration F-BTSC, crashed in Gonesse, France after departing from Paris Charles de Gaulle en route to John F. Kennedy International Airport in New York City, killing all 100 passengers and nine crew members on board the flight, and four people on the ground. It was the only fatal accident involving Concorde.
According to the official investigation conducted by the "Bureau d'Enquêtes et d'Analyses pour la Sécurité de l'Aviation Civile" (BEA), the crash was caused by a metallic strip that fell from a Continental Airlines DC-10 that had taken off minutes earlier. This fragment punctured a tyre on Concorde's left main wheel bogie during takeoff. The tyre exploded, and a piece of rubber hit the fuel tank, which caused a fuel leak and led to a fire. The crew shut down engine number 2 in response to a fire warning, and with engine number 1 surging and producing little power, the aircraft was unable to gain altitude or speed. The aircraft entered a rapid pitch-up then a violent descent, rolling left and crashing tail-low into the Hôtelissimo Les Relais Bleus Hotel in Gonesse.
Prior to the accident, Concorde had been arguably the safest operational passenger airliner in the world in passenger deaths-per-kilometres travelled with zero, but had a rate of tyre damage some 30 times higher than subsonic airliners from 1995 to 2000. Safety improvements were made in the wake of the crash, including more secure electrical controls, Kevlar lining on the fuel tanks and specially developed burst-resistant tyres. On 6 December 2010, Continental Airlines and John Taylor, one of their mechanics, were found guilty of involuntary manslaughter, but on 30 November 2012 a French court overturned the conviction, saying mistakes by Continental and Taylor did not make them criminally responsible. 
The first flight after the modifications departed from London Heathrow on 17 July 2001, piloted by BA Chief Concorde Pilot Mike Bannister. During the 3-hour 20-minute flight over the mid-Atlantic towards Iceland, Bannister attained Mach 2.02 and before returning to RAF Brize Norton. The test flight, intended to resemble the London–New York route, was declared a success and was watched on live TV, and by crowds on the ground at both locations. The first flight with passengers after the accident took place on 11 September 2001, landing shortly before the World Trade Center attacks in the United States. This was not a revenue flight, as all the passengers were BA employees.
Normal commercial operations resumed on 7 November 2001 by BA and AF (aircraft G-BOAE and F-BTSD), with service to New York JFK, where passengers were welcomed by then mayor Rudy Giuliani.
Retirement.
On 10 April 2003, Air France and British Airways simultaneously announced that they would retire Concorde later that year. They cited low passenger numbers following the 25 July 2000 crash, the slump in air travel following the September 11, 2001 attacks, and rising maintenance costs. Although Concorde was technologically advanced when introduced in the 1970s, 30 years later, its analogue cockpit was dated. There had been little commercial pressure to upgrade Concorde due to a lack of competing aircraft, unlike other airliners of the same era such as the Boeing 747. By its retirement, it was the last aircraft in British Airways' fleet that had a flight engineer; other aircraft, such as the modernised 747-400, had eliminated the role.
On 11 April 2003, Virgin Atlantic founder Sir Richard Branson announced that the company was interested in purchasing British Airways’ Concorde fleet for their nominal original price of £1 (US$1.57 in April 2003) each. British Airways dismissed the idea, prompting Virgin to increase their offer to £1 million each. Branson claimed that when BA was privatised, a clause in the agreement required them to allow another British airline to operate Concorde if BA ceased to do so, but the Government denied the existence of such a clause. In October 2003, Branson wrote in "The Economist" that his final offer was "over £5 million" and that he had intended to operate the fleet "for many years to come". The chances for keeping Concorde in service were stifled by Airbus's lack of support for continued maintenance.
It has been suggested that Concorde was not withdrawn for the reasons usually given but that it became apparent during the grounding of Concorde that the airlines could make more profit carrying first class passengers subsonically. A lack of commitment to Concorde from Director of Engineering Alan MacDonald was cited as having undermined BA’s resolve to continue operating Concorde.
Air France.
Air France made its final commercial Concorde landing in the United States in New York City from Paris on 30 May 2003. Air France's final Concorde flight took place on 27 June 2003 when F-BVFC retired to Toulouse.
An auction of Concorde parts and memorabilia for Air France was held at Christie's in Paris on 15 November 2003; 1,300 people attended, and several lots exceeded their predicted values. French Concorde F-BVFC was retired to Toulouse and kept functional for a short time after the end of service, in case taxi runs were required in support of the French judicial enquiry into the 2000 crash. The aircraft is now fully retired and no longer functional.
French Concorde F-BTSD has been retired to the "Musée de l'Air et de l'Espace" at Le Bourget (near Paris) and, unlike the other museum Concordes, a few of the systems are being kept functional. For instance, the famous "droop nose" can still be lowered and raised. This led to rumours that they could be prepared for future flights for special occasions.
French Concorde F-BVFB currently rests at the Auto & Technik Museum Sinsheim at Sinsheim, Germany, after its last flight from Paris to Baden-Baden, followed by a spectacular transport to Sinsheim via barge and road. The museum also has a Tu-144 on display – this is the only place where both supersonic airliners can be seen together.
British Airways.
British Airways conducted a North American farewell tour in October 2003. G-BOAG visited Toronto Pearson International Airport on 1 October, after which it flew to New York’s John F. Kennedy International Airport. G-BOAD visited Boston’s Logan International Airport on 8 October, and G-BOAG visited Washington Dulles International Airport on 14 October. It has been claimed that G-BOAD’s flight from London Heathrow to Boston set a transatlantic flight record of 3 hours, 5 minutes, 34 seconds. However the fastest transatlantic flight was from New York JFK airport to Heathrow on 7 February 1996, taking 2 hours, 52 minutes, 59 seconds; 90 seconds less than a record set in April 1990.
In a week of farewell flights around the United Kingdom, Concorde visited Birmingham on 20 October, Belfast on 21 October, Manchester on 22 October, Cardiff on 23 October, and Edinburgh on 24 October. Each day the aircraft made a return flight out and back into Heathrow to the cities, often overflying them at low altitude. On 22 October, both Concorde flight BA9021C, a special from Manchester, and BA002 from New York landed simultaneously on both of Heathrow's runways. On 23 October 2003, the Queen consented to the illumination of Windsor Castle, an honour reserved for state events and visiting dignitaries, as Concorde's last west-bound commercial flight departed London.
British Airways retired its Concorde fleet on 24 October 2003. G-BOAG left New York to a fanfare similar to that given for Air France’s F-BTSD, while two more made round trips, G-BOAF over the Bay of Biscay, carrying VIP guests including former Concorde pilots, and G-BOAE to Edinburgh. The three aircraft then circled over London, having received special permission to fly at low altitude, before landing in sequence at Heathrow. The captain of the New York to London flight was Mike Bannister. The final flight of a Concorde in the US occurred on 5 November 2003 when G-BOAG flew from New York's Kennedy Airport to Seattle's Boeing Field to join the Museum of Flight's permanent collection. The plane was piloted by Mike Bannister and Les Broadie who claimed a flight time of three hours, 55 minutes and 12 seconds, a record between the two cities. The museum had been pursuing a Concorde for their collection since 1984. The final flight of a Concorde world-wide took place on 26 November 2003 with a landing at Filton, Bristol, UK.
All of BA's Concorde fleet have been grounded, drained of hydraulic fluid and their airworthiness certificates withdrawn. Jock Lowe, ex-chief Concorde pilot and manager of the fleet estimated in 2004 that it would cost £10–15 million to make G-BOAF airworthy again. BA maintain ownership and have stated that they will not fly again due to a lack of support from Airbus. On 1 December 2003, Bonhams held an auction of British Airways’ Concorde artifacts, including a nose cone, at Kensington Olympia in London. Proceeds of around £750,000 were raised, with the majority going to charity. G-BOAD is currently on display at the Intrepid Sea, Air & Space Museum in New York. In 2007, BA announced that the advertising spot at Heathrow where a 40% scale model of Concorde was located would not be retained; the model is now on display at the Brooklands Museum.
Restoration.
Although only used for spares after being retired from test flying and trials work in 1981, Concorde G-BBDG was dismantled and transported by road from Filton then restored from essentially a shell at the Brooklands Museum in Surrey.
One of the youngest Concordes (F-BTSD) is on display at Le Bourget Air and Space Museum in Paris. In February 2010, it was announced that the museum and a group of volunteer Air France technicians intend to restore F-BTSD so it can taxi under its own power. In May 2010, it was reported that the British Save Concorde Group and French Olympus 593 groups had begun inspecting the engines of a Concorde at the French museum; their intent is to restore the airliner to a condition where it can fly in demonstrations.
Comparable aircraft.
The only supersonic airliner in direct competition with Concorde was the Soviet Tupolev Tu-144, nicknamed "Concordski" by Western European journalists for its outward similarity to Concorde. It had been alleged that Soviet espionage efforts had resulted in the theft of Concorde blueprints, ostensibly to assist in the design of the Tu-144. As a result of a rushed development programme, the first Tu-144 prototype was substantially different from the preproduction machines, but both were cruder than Concorde. The Tu-144"S" had a significantly shorter range than Concorde, due to its low-bypass turbofan engines. The aircraft had poor control at low speeds because of a simpler supersonic wing design; in addition the Tu-144 required braking parachutes to land while Concorde used anti-lock brakes. The Tu-144 had two crashes, one at the 1973 Paris Air Show, and another during a pre-delivery test flight in May 1978.
Later production Tu-144 versions were more refined and competitive. They had retractable canards for better low-speed control, turbojet engines providing nearly the fuel efficiency and range of Concorde and a top speed of Mach 2.35. Passenger service commenced in November 1977, but after the 1978 crash the aircraft was taken out of service. The aircraft had an inherently unsafe structural design as a consequence of an automated production method chosen in order to simplify and speed up manufacturing.
The American designs, the Boeing 2707 and the Lockheed L-2000, were to have been larger, with seating for up to 300 people. Running a few years behind Concorde, the Boeing 2707 was redesigned to a cropped delta layout; the extra cost of these changes helped to kill the project. The operation of US military aircraft such as the XB-70 Valkyrie and B-58 Hustler had shown that sonic booms were quite capable of reaching the ground, and the experience from the Oklahoma City sonic boom tests led to the same environmental concerns that hindered the commercial success of Concorde. The American government cancelled the project in 1971, after having spent more than $1 billion.
The only other large supersonic aircraft comparable to Concorde are strategic bombers, principally the Russian Tu-22, Tu-22M, M-50 (experimental), T-4 (experimental), Tu-160 and the American XB-70 (experimental), B-1.
Impact.
Environmental.
Before Concorde’s flight trials, developments in the civil aviation industry were largely accepted by governments and their respective electorates. Opposition to Concorde’s noise, particularly on the east coast of the United States, forged a new political agenda on both sides of the Atlantic, with scientists and technology experts across a multitude of industries beginning to take the environmental and social impact more seriously. Although Concorde led directly to the introduction of a general noise abatement programme for aircraft flying out of John F. Kennedy Airport, many found that Concorde was quieter than expected, partly due to the pilots temporarily throttling back their engines to reduce noise during overflight of residential areas. Even before revenue flights started it had been claimed that Concorde was quieter than several aircraft then in service. In 1971 BAC's technical director was quoted "It is certain on present evidence and calculations that in the airport context, production Concordes will be no worse than aircraft now in service and will in fact be better than many of them."
Concorde produced nitrogen oxides in its exhaust, which, despite complicated interactions with other ozone-depleting chemicals, are understood to result in degradation to the ozone layer at the stratospheric altitudes it cruised. It has been pointed out that other, lower-flying, airliners produce ozone during their flights in the troposphere, but vertical transit of gases between the layers is restricted. The small fleet meant overall ozone-layer degradation caused by Concorde was negligible.
David W. Fahey, of the National Oceanic and Atmospheric Administration, found that to produce a drop in stratospheric ozone of 1 to 2% would require a fleet of 500 supersonic aircraft to be operated. Dr. Fahey stated that this would not be a limiting factor for further supersonic transport development.
Concorde’s technical leap forward boosted the public’s understanding of conflicts between technology and the environment as well as awareness of the complex decision analysis processes that surround such conflicts. In France, the use of acoustic fencing alongside TGV tracks might not have been achieved without the 1970s controversy over aircraft noise. In the UK, the CPRE has issued tranquillity maps since 1990.
Some sources say Concorde typically flew per passenger (100 passengers were maximum capacity).
Public perception.
Concorde was normally perceived as a privilege of the rich, but special circular or one-way (with return by other flight or ship) charter flights were arranged to bring a trip within the means of moderately well-off enthusiasts.
The aircraft was usually referred to by the British as simply "Concorde". In France it was known as "le Concorde" due to "le", the definite article, used in French grammar to introduce the name of a ship or aircraft, and the capital being used to distinguish a proper name from a common noun of the same spelling. In French, the common noun "concorde" means "agreement, harmony, or peace". Concorde’s pilots and British Airways in official publications often refer to Concorde both in the singular and plural as "she" or "her".
As a symbol of national pride, an example from the BA fleet made occasional flypasts at selected Royal events, major air shows and other special occasions, sometimes in formation with the Red Arrows. On the final day of commercial service, public interest was so great that grandstands were erected at Heathrow Airport. Significant numbers of people attended the final landings; the event received widespread media coverage.
In 2006, 37 years after its first test flight, Concorde was announced the winner of the Great British Design Quest organised by the BBC and the Design Museum. A total of 212,000 votes were cast with Concorde beating design icons such as the Mini, mini skirt, Jaguar E-type, Tube map and the Supermarine Spitfire.
Special missions.
Heads of France and the United Kingdom flew Concorde many times. Presidents Georges Pompidou, Valéry Giscard d'Estaing and François Mitterrand regularly used the Concorde as French flagman aircraft in foreign visits. British Queen Elizabeth II and Prime Ministers Edward Heath, Jim Callaghan, Margaret Thatcher, John Major, Tony Blair took the Concorde in some charter flights such as Queen's trips to Barbados on her Silver Jubilee in 1977, in 1987 and in 2003, to Middle East in 1984, to the United States in 1991, etc. 
Pope John Paul II flew Concorde in May 1989. Both the French President, as well as the British Prime Minister flew Concordes to San Juan for the second G-6 Economic Summit, held in the United States and hosted by President Gerald Ford at the Dorado Beach Hotel in Dorado, Puerto Rico on June 27-28, 1976. 
The Concorde sometimes made special flights for its demonstration, for exhibit on airshows (Farnborough, Paris-LeBourget, MAKS, etc.) and other expositions, for taking part in parades and celebrations (as ex., of Zürich airport anniversary in 1998), for private charters (as ex., many times by President of Zaire Mobutu Sese Seko), for promo-advertising of companies (OKI, etc.), for Olympic torch relays (1992 Winter Olympics in Albertville), for observing of solar eclipse, etc.
Records.
The fastest transatlantic airliner flight was from New York JFK to London Heathrow on 7 February 1996 by British Airways' G-BOAD in 2 hours, 52 minutes, 59 seconds from takeoff to touchdown aided by a 175 mph tailwind. Concorde also set other records, including the official FAI "Westbound Around the World" and "Eastbound Around the World" world air speed records. On 12–13 October 1992, in commemoration of the 500th anniversary of Columbus’ first New World landing, Concorde Spirit Tours (USA) chartered Air France Concorde F-BTSD and circumnavigated the world in 32 hours 49 minutes and 3 seconds, from Lisbon, Portugal, including six refuelling stops at Santo Domingo, Acapulco, Honolulu, Guam, Bangkok, and Bahrain.
The eastbound record was set by the same Air France Concorde (F-BTSD) under charter to Concorde Spirit Tours in the USA on 15–16 August 1995. This promotional flight circumnavigated the world from New York/JFK International Airport in 31 hours 27 minutes 49 seconds, including six refuelling stops at Toulouse, Dubai, Bangkok, Andersen AFB in Guam, Honolulu, and Acapulco. By its 30th flight anniversary on 2 March 1999 Concorde had clocked up 920,000 flight hours, with more than 600,000 supersonic, much more than all of the other supersonic aircraft in the Western world combined.
On its way to the Museum of Flight in November 2003, G-BOAG set a New York City-to-Seattle speed record of 3 hours, 55 minutes, and 12 seconds.

</doc>
<doc id="7053" url="http://en.wikipedia.org/wiki?curid=7053" title="Cannon">
Cannon

A cannon is any piece of artillery that uses gunpowder or other usually explosive-based propellants to launch a projectile. Cannon vary in caliber, range, mobility, rate of fire, angle of fire, and firepower; different forms of cannon combine and balance these attributes in varying degrees, depending on their intended use on the battlefield. The word "cannon" is derived from several languages, in which the original definition can usually be translated as "tube", "cane", or "reed". In the modern era, the term "cannon" has fallen into decline, replaced by "guns" or "artillery" if not a more specific term such as "mortar" or "howitzer", except for in the field of aerial warfare, where it is usually shorthand for autocannon.
First invented in China, cannon were among the earliest forms of gunpowder artillery, and over time replaced siege engines—among other forms of ageing weaponry—on the battlefield. In the Middle East, the first use of the hand cannon is argued to be during the 1260 Battle of Ain Jalut between the Mamluks and Mongols. The first cannon in Europe were probably used in Iberia in the 11th and 12th centuries. On the African continent, the cannon was first used by the Somali Imam Ahmad ibn Ibrihim al-Ghazi of the Adal Sultanate in his conquest of the steppes of Ugaden in 1529. It was during this period, the Middle Ages, that cannon became standardized, and more effective in both the anti-infantry and siege roles. After the Middle Ages most large cannon were abandoned in favour of greater numbers of lighter, more manoeuvrable pieces. In addition, new technologies and tactics were developed, making most defences obsolete; this led to the construction of star forts, specifically designed to withstand artillery bombardment though these too (along with the Martello Tower) would find themselves rendered obsolete when explosive and armour piercing rounds made even these types of fortifications vulnerable.
Cannon also transformed naval warfare in the early modern period, as European navies took advantage of their firepower. As rifling became commonplace, the accuracy and destructive power of cannon was significantly increased, and they became deadlier than ever, both to infantry who belatedly had to adopt different tactics, and to ships, which had to be armoured. In World War I, the majority of combat fatalities were caused by artillery; they were also used widely in World War II. Most modern cannon are similar to those used in the Second World War, although the importance of the larger calibre weapons has declined with the development of missiles.
Etymology and terminology.
"Cannon" is derived from the Old Italian word "cannone", meaning "large tube", which came from Latin "canna", in turn originating from the Greek κάννα ("kanna"), "reed", and then generalized to mean any hollow tube-like object; cognate with Akkadian term "qanu" and Hebrew "qāneh", meaning "tube" or "reed". The word has been used to refer to a gun since 1326 in Italy, and 1418 in England. "Cannon" serves both as the singular and plural of the noun, although in American English the plural "cannons" is more common.
Cannon materials, parts, and terms.
Cannon in general have the form of a truncated cone with an internal cylindrical bore for holding an explosive charge and a projectile. The thickest, strongest, and closed part of the cone is located near the explosive charge. As any explosive charge will dissipate in all directions equally, the thickest portion of the cannon is useful for containing and directing this force. The backward motion of the cannon as its projectile leaves the bore is termed its recoil and the effectiveness of the cannon can be measured in terms of how much this response can be diminished, though obviously diminishing recoil through increasing the overall mass of the cannon means decreased mobility.
Field artillery cannon in Europe and the Americas were initially made most often of bronze, though later forms were constructed of cast iron and eventually steel. Bronze has several characteristics that made it preferable as a construction material: although it is relatively expensive, does not always alloy well, and can result in a final product that is "spongy about the bore", bronze is more flexible than iron and therefore less prone to bursting when exposed to high pressure; cast iron cannon are less expensive and more durable generally than bronze and withstand being fired more times without deteriorating. However, cast iron cannon have a tendency to burst without having shown any previous weakness or wear, and this makes them more dangerous to operate.
The older and more-stable forms of cannon were muzzle-loading as opposed to breech-loading— in order to be used they had to have their ordnance packed down the bore through the muzzle rather than inserted through the breech.
The following terms refer to the components or aspects of a classical western cannon (c. 1850) as illustrated here. In what follows, the words "near", "close", and "behind" will refer to those parts towards the thick, closed end of the piece, and "far", "front", "in front of", and "before" to the thinner, open end.
Solid spaces.
The main body of a cannon consists of three basic extensions— the foremost and the longest is called the "chase", the middle portion is the "reinforce", and the closest and briefest portion is the "cascabel" or "cascable".
To pack a muzzle-loading cannon, first gunpowder is poured down the bore. This is followed by a layer of wadding (often nothing more than paper), and then the cannonball itself. A certain amount of windage allows the ball to fit down the bore, though the greater the windage the less efficient the propulsion of the ball when the gunpowder is ignited. To fire the cannon, the fuse located in the vent is lit, quickly burning down to the gunpowder, which then explodes violently, propelling wadding and ball down the bore and out of the muzzle. A small portion of exploding gas also escapes through the vent, but this does not dramatically affect the total force exerted on the ball.
Any large, smoothbore, muzzle-loading gun—used before the advent of breech-loading, rifled guns—may be referred to as a cannon, though once standardized names were assigned to different sized cannon, the term specifically referred to a gun designed to fire a shot, as opposed to a demi-cannon - , culverin - , or demi-culverin - . "Gun" specifically refers to a type of cannon that fires projectiles at high speeds, and usually at relatively low angles; they have been used in warships, and as field artillery. The term "cannon" is also used for autocannon, a modern repeating weapon firing explosive projectiles. Cannon have been used extensively in fighter aircraft since World War II, and in place of machine guns on land vehicles.
History.
Development in China.
The invention of the cannon, driven by gunpowder, was first developed in China and later spread to Islamic world and Europe. Like small arms, cannon are a descendant of the fire lance, a gunpowder-filled tube attached to the end of a spear and used as a flamethrower in China. Shrapnel was sometimes placed in the barrel, so that it would fly out along with the flames. The first documented battlefield use of fire lances took place in 1132 when Chen Gui used them to defend De'an from attack by the Jurchen Jin. Eventually, the paper and bamboo of which fire lance barrels were originally constructed came to be replaced by metal. It has been disputed at which point flame-projecting cannon were abandoned in favour of missile-projecting ones, as words meaning either "incendiary" or "explosive" are commonly translated as "gunpowder". The earliest known depiction of a gun is a sculpture from a cave in Sichuan, dating to the 12th century that portrays a figure carrying a vase-shaped bombard, firing flames and a ball. The oldest surviving gun, found in Acheng, Heilongjiang, and dated to no later than 1290, is 34 cm long with a muzzle bore diameter of and weighs 3.5 kg. The second oldest, dated to 1332 is 35.3 cm long, a muzzle bore diameter of and weighs 6.94 kg; both are made of bronze.
The earliest known illustration of a cannon is dated to 1326. In his 1341 poem, "The Iron Cannon Affair", one of the first accounts of the use of gunpowder artillery in China, Xian Zhang wrote that a cannonball fired from an eruptor could "pierce the heart or belly when it strikes a man or horse, and can even transfix several persons at once."
Joseph Needham suggests that the proto-shells described in the "Huolongjing" may be among the first of their kind. The weapon was later taken up by both the Mongol conquerors and the Koreans. Chinese soldiers fighting under the Mongols appear to have used hand cannon in Manchurian battles during 1288, a date deduced from archaeological findings at battle sites. The Ming Chinese also mounted over 3,000 cast bronze and iron cannon on the Great Wall of China, to defend against the Mongols.
Cannon were used by Ming dynasty forces at the Battle of Lake Poyang. Ming dynasty era ships had bronze cannon. One shipwreck in Shandong had a cannon dated to 1377 and an anchor dated to 1372. From the 13th to 15th centuries cannon armed Chinese ships also travelled throughout Southeast Asia.
In the 1593 Siege of Pyongyang, 40,000 Ming troops deployed a variety of cannon to bombard an equally large Japanese army. Despite both forces having similar numbers, the Japanese were easily defeated due to the Ming cannon. Throughout the Seven Year War in Korea, the Chinese-Korean coalition used artillery widely, in both land and naval battles, including on the Turtle Ships of Yi Sun-sin.
Islamic world.
Arabic manuscripts dated from the 14th century document the use of the hand cannon, a forerunner of the handgun, in the Arabic world. Ahmad Y. al-Hassan argues that these manuscripts are copies of earlier manuscripts and reported on hand-held cannon being used by the Mamluks at the Battle of Ain Jalut in 1260. Al-Hassan also interprets Ibn Khaldun as reporting cannon being used as siege machines by the Marinid sultan Abu Yaqub Yusuf at the siege of Sijilmasa in 1274. Other historians urge caution regarding claims of Islamic firearms use in the 1204–1324 period as late medieval Arabic texts used the same word for gunpowder, "naft", that they used for an earlier incendiary naphtha. The Mamluks had certainly acquired siege cannon by the 1360s, and possibly as early as 1320.
Sixty-eight super-sized bombards referred to as Great Turkish Bombards were used by Mehmed II to capture Constantinople in 1453. Orban, a Hungarian cannon engineer, is credited with introducing the cannon from Central Europe to the Ottomans. These cannon could fire heavy stone balls a mile, and the sound of their blast could reportedly be heard from a distance of . Shkodran historian Marin Barleti discusses Turkish bombards at length in his book "De obsidione Scodrensi" (1504), describing the 1478–79 siege of Shkodra in which eleven bombards and two mortars were employed.
The similar Dardanelles Guns (for the location) were created by Munir Ali in 1464 and were still in use during the Anglo-Turkish War (1807–1809). These were cast in bronze into two parts. The chase (the barrel) and the breech, which combined weighed 18.4 tonnes. The two parts were screwed together using levers to facilitate moving it.
In medieval Somalia, general Ahmad ibn Ibrihim al-Ghazi of the Sultanate of Adal was the first African commander presently known to have used cannon in warfare on the continent during Adal's conquest of the Ethiopian Empire, which demoralized the Ethiopians.
Fathullah Shirazi, a Persian-Indian who worked for Akbar the Great in the Mughal Empire, developed a volley gun in the 16th century.
Medieval Europe.
In Europe, one of the first mentions of gunpowder use appears in a passage found in Roger Bacon's "Opus Maius" and "Opus Tertium" in what has been interpreted as being firecrackers. In the early 20th century, a British artillery officer proposed that another work tentatively attributed to Bacon, "Epistola de Secretis Operibus Artis et Naturae, et de Nullitate Magiae" contained an encrypted formula for gunpowder. These claims have been disputed by historians of science. In any case, the formula claimed to have been decrypted is not useful for firearms use or even firecrackers, burning slowly and producing mostly smoke.
The first confirmed use of cannon in Europe was in southern Iberia, by the Moors, in the Siege of Cordoba in 1280. By this time, hand guns were probably in use, as "scopettieri"—"gun bearers"—were mentioned in conjunction with crossbowmen, in 1281. In Iberia, the "first artillery-masters on the Peninsula" were enlisted, at around the same time.
The first metal cannon was the "pot-de-fer". Loaded with an arrow-like bolt that was probably wrapped in leather to allow greater thrusting power, it was set off through a touch hole with a heated wire. This weapon, and others similar, were used by both the French and English during the Hundred Years' War, when cannon saw their first real use on the European battlefield. While still a relatively rarely used weapon, cannon were employed in increasing numbers during the war. The battle of Arnemuiden, fought on 23 September 1338, was the first naval battle using artillery, as the English ship "Christofer" had three cannon and one hand gun. "Ribaldis", which shot large arrows and simplistic grapeshot, were first mentioned in the English Privy Wardrobe accounts during preparations for the Battle of Crécy, between 1345 and 1346. The Florentine Giovanni Villani recounts their destructiveness, indicating that by the end of the battle, "the whole plain was covered by men struck down by arrows and cannon balls." Similar cannon were also used at the Siege of Calais, in the same year, although it was not until the 1380s that the "ribaudekin" clearly became mounted on wheels.
The first cannon appeared in Russia around 1380, though they were used only in sieges, often by the defenders. Large cannon known as bombards ranged from three to five feet in length and were used by Dubrovnik and Kotor in defence in the later 14th century. The first bombards were made of iron, but bronze was quickly recognized as being stronger and capable of propelling stones weighing as much as a hundred pounds (45 kg). Byzantine strategists did not have the money to invest in this technology. Around the same period, the Byzantine Empire began to accumulate its own cannon to face the Ottoman threat, starting with medium-sized cannon long and of 10 in calibre. The first definite use of artillery in the region was against the Ottoman siege of Constantinople, in 1396, forcing the Ottomans to withdraw. They acquired their own cannon, and laid siege to the Byzantine capital again, in 1422, using "falcons", which were short but wide cannon. By 1453, the Ottomans used 68 Hungarian-made cannon for the 55-day bombardment of the walls of Constantinople, "hurling the pieces everywhere and killing those who happened to be nearby." The largest of their cannon was the Great Turkish Bombard, which required an operating crew of 200 men and 70 oxen, and 10,000 men to transport it. Gunpowder made the formerly devastating Greek fire obsolete, and with the final fall of Constantinople—which was protected by what were once the strongest walls in Europe—on 29 May 1453, "it was the end of an era in more ways than one."
Early modern period.
By the 16th century, cannon were made in a great variety of lengths and bore diameters, but the general rule was that the longer the barrel, the longer the range. Some cannon made during this time had barrels exceeding in length, and could weigh up to . Consequently, large amounts of gunpowder were needed, to allow them to fire stone balls several hundred yards. By mid-century, European monarchs began to classify cannon to reduce the confusion. Henry II of France opted for six sizes of cannon, but others settled for more; the Spanish used twelve sizes, and the English sixteen. Better powder had been developed by this time as well. Instead of the finely ground powder used by the first bombards, powder was replaced by a "corned" variety of coarse grains. This coarse powder had pockets of air between grains, allowing fire to travel through and ignite the entire charge quickly and uniformly.
The end of the Middle Ages saw the construction of larger, more powerful cannon, as well their spread throughout the world. As they were not effective at breaching the newer fortifications resulting from the development of cannon, siege engines—such as siege towers and trebuchets—became less widely used. However, wooden "battery-towers" took on a similar role as siege towers in the gunpowder age—such as that used at siege of Kazan in 1552, which could hold ten large-calibre cannon, in addition to 50 lighter pieces. Another notable effect of cannon on warfare during this period was the change in conventional fortifications. Niccolò Machiavelli wrote, "There is no wall, whatever its thickness that artillery will not destroy in only a few days." Although castles were not immediately made obsolete by cannon, their use and importance on the battlefield rapidly declined. Instead of majestic towers and merlons, the walls of new fortresses were thick, angled, and sloped, while towers became low and stout; increasing use was also made of earth and brick in breastworks and redoubts. These new defences became known as "star forts", after their characteristic shape which attempted to force any advance toward it directly into the firing line of the guns. A few of these featured cannon batteries, such as the Tudors' Device Forts, in England. Star forts soon replaced castles in Europe, and, eventually, those in the Americas, as well.
By the end of the 15th century, several technological advancements made cannon more mobile. Wheeled gun carriages and trunnions became common, and the invention of the limber further facilitated transportation. As a result, field artillery became more viable, and began to see more widespread use, often alongside the larger cannon intended for sieges. Better gunpowder, cast-iron projectiles (replacing stone), and the standardization of calibres meant that even relatively light cannon could be deadly. In "The Art of War", Niccolò Machiavelli observed that "It is true that the arquebuses and the small artillery do much more harm than the heavy artillery." This was the case at Flodden, in 1513: the English field guns outfired the Scottish siege artillery, firing two or three times as many rounds. Despite the increased maneuverability, however, cannon were still the slowest component of the army: a heavy English cannon required 23 horses to transport, while a culverin needed nine. Even with this many animals pulling, they still moved at a walking pace. Due to their relatively slow speed, and lack of organization, and undeveloped tactics, the combination of pike and shot still dominated the battlefields of Europe.
Innovations continued, notably the German invention of the mortar, a thick-walled, short-barrelled gun that blasted shot upward at a steep angle. Mortars were useful for sieges, as they could hit targets behind walls or other defences. This cannon found more use with the Dutch, who learned to shoot bombs filled with powder from them. Setting the bomb fuse was a problem. "Single firing" was first used to ignite the fuse, where the bomb was placed with the fuse down against the cannon's propellant. This often resulted in the fuse being blown into the bomb, causing it to blow up as it left the mortar. Because of this, "double firing" was tried where the gunner lit the fuse and then the touch hole. This, however, required considerable skill and timing, and was especially dangerous if the gun misfired, leaving a lighted bomb in the barrel. Not until 1650 was it accidentally discovered that double-lighting was superfluous as the heat of firing would light the fuse.
Gustavus Adolphus of Sweden emphasized the use of light cannon and mobility in his army, and created new formations and tactics that revolutionized artillery. He discontinued using all 12 pounder—or heavier—cannon as field artillery, preferring, instead, to use cannon that could be manned by only a few men. One obsolete type of gun, the "leatheren" was replaced by 4 pounder and 9 pounder demi-culverins. These could be operated by three men, and pulled by only two horses. Adolphus's army was also the first to use a cartridge that contained both powder and shot which sped up reloading, increasing the rate of fire. Finally, against infantry he pioneered the use of canister shot - essentially a tin can filled with musket balls. Until then there was no more than one cannon for every thousand infantrymen on the battlefield but Gustavus Adolphus increased the number of cannon sixfold. Each regiment was assigned two pieces, though he often arranged then into batteries instead of distributing them piecemeal. He used these batteries to break his opponent's infantry line, while his cavalry would outflank their heavy guns.
At the Battle of Breitenfeld, in 1631, Adolphus proved the effectiveness of the changes made to his army, by defeating Johann Tserclaes, Count of Tilly. Although severely outnumbered, the Swedes were able to fire between three and five times as many volleys of artillery, and their infantry's linear formations helped ensure they didn't lose any ground. Battered by cannon fire, and low on morale, Tilly's men broke ranks and fled.
In England cannon were being used to besiege various fortified buildings during the English Civil War. Nathaniel Nye is recorded as testing a Birmingham cannon in 1643 and experimenting with a saker in 1645. From 1645 he was the master gunner to the Parliamentarian garrison at Evesham and in 1646 he successfully directed the artillery at the Siege of Worcester, detailing his experiences and in his 1647 book "The Art of Gunnery". Believing that war was as much a science as an art, his explanations focused on triangulation, arithmetic, theoretical mathematics, and cartography as well as practical considerations such as the ideal specification for gunpowder or slow matches. His book acknowledged mathematicians such as Robert Recorde and Marcus Jordanus as well as earlier military writers on artillery such as Niccolò Tartaglia and Thomas Malthus.
Around this time also came the idea of aiming the cannon to hit a target. Gunners controlled the range of their cannon by measuring the angle of elevation, using a "gunner's quadrant." Cannon did not have sights, therefore, even with measuring tools, aiming was still largely guesswork.
In the latter half of the 17th century, the French engineer Vauban introduced a more systematic and scientific approach to attacking gunpowder fortresses, in a time when many field commanders "were notorious dunces in siegecraft." Careful sapping forward, supported by enfilading ricochet fire, was a key feature of this system, and it even allowed Vauban to calculate the length of time a siege would take. He was also a prolific builder of star forts, and did much to popularize the idea of "depth in defence" in the face of cannon. These principles were followed into the mid-19th century, when changes in armaments necessitated greater depth defence than Vauban had provided for. It was only in the years prior to World War I that new works began to break radically away from his designs.
18th and 19th centuries.
The lower tier of 17th-century English ships of the line were usually equipped with demi-cannon, guns that fired a solid shot, and could weigh up to . Demi-cannon were capable of firing these heavy metal balls with such force that they could penetrate more than a metre of solid oak, from a distance of , and could dismast even the largest ships at close range. Full cannon fired a shot, but were discontinued by the 18th century, as they were too unwieldy. By the end of the 18th century, principles long adopted in Europe specified the characteristics of the Royal Navy's cannon, as well as the acceptable defects, and their severity. The United States Navy tested guns by measuring them, firing them two or three times—termed "proof by powder"—and using pressurized water to detect leaks.
The carronade was adopted by the Royal Navy in 1779; the lower muzzle velocity of the round shot when fired from this cannon was intended to create more wooden splinters when hitting the structure of an enemy vessel, as they were believed to be more deadly than the ball by itself. The carronade was much shorter, and weighed between a third to a quarter of the equivalent long gun; for example, a 32 pounder carronade weighed less than a ton, compared with a 32 pounder long gun, which weighed over 3 tons. The guns were, therefore, easier to handle, and also required less than half as much gunpowder, allowing fewer men to crew them. Carronades were manufactured in the usual naval gun calibres, but were not counted in a ship of the line's rated number of guns. As a result, the classification of Royal Navy vessels in this period can be misleading, as they often carried more cannon than were listed.
In the 1810s and 1820s, greater emphasis was placed on the accuracy of long-range gunfire, and less on the weight of a broadside. The carronade, although initially very successful and widely adopted, disappeared from the Royal Navy in the 1850s after the development of wrought-iron-jacketed steel cannon by William George Armstrong and Joseph Whitworth. Nevertheless, carronades were used in the American Civil War.
Western cannon during the 19th century became larger, more destructive, more accurate, and could fire at longer range. One example is the American wrought-iron, muzzle-loading rifle, or Griffen gun, used during the American Civil War, which had an effective range of over . Another is the smoothbore 12 pounder Napoleon, which was renowned for its sturdiness, reliability, firepower, flexibility, relatively light weight, and range of .
Cannon were crucial in Napoleon Bonaparte's rise to power, and continued to play an important role in his army in later years. During the French Revolution, the unpopularity of the Directory led to riots and rebellions. When over 25,000 royalists led by General Danican assaulted Paris, Paul François Jean Nicolas, vicomte de Barras was appointed to defend the capital; outnumbered five to one and disorganized, the Republicans were desperate. When Napoleon arrived, he reorganized the defences but realized that without cannon the city could not be held. He ordered Joachim Murat to bring the guns from the Sablons artillery park; the Major and his cavalry fought their way to the recently captured cannon, and brought them back to Napoleon. When Danican's poorly trained men attacked, on 13 Vendémiaire, 1795 — 5 October 1795, in the calendar used in France at the time — Napoleon ordered his cannon to fire grapeshot into the mob, an act that became known as the "whiff of grapeshot". The slaughter effectively ended the threat to the new government, while, at the same time, made Bonaparte a famous—and popular—public figure. Among the first generals to recognize that artillery was not being used to its full potential, Napoleon often massed his cannon into batteries and introduced several changes into the French artillery, improving it significantly and making it among the finest in Europe. Such tactics were successfully used by the French, for example, at the Battle of Friedland, when sixty-six guns fired a total of 3,000 roundshot and 500 rounds of grapeshot, inflicting severe casualties to the Russian forces, whose losses numbered over 20,000 killed and wounded, in total. At the Battle of Waterloo—Napoleon's final battle—the French army had many more artillery pieces than either the British or Prussians. As the battlefield was muddy, recoil caused cannon to bury themselves into the ground after firing, resulting in slow rates of fire, as more effort was required to move them back into an adequate firing position; also, roundshot did not ricochet with as much force from the wet earth. Despite the drawbacks, sustained artillery fire proved deadly during the engagement, especially during the French cavalry attack. The British infantry, having formed infantry squares, took heavy losses from the French guns, while their own cannon fired at the cuirassiers and lancers, when they fell back to regroup. Eventually, the French ceased their assault, after taking heavy losses from the British cannon and musket fire.
The practice of rifling—casting spiralling lines inside the cannon's barrel—was applied to artillery more frequently by 1855, as it gave cannon projectiles gyroscopic stability, which improved their accuracy. One of the earliest rifled cannon was the breech-loading Armstrong Gun—also invented by William George Armstrong—which boasted significantly improved range, accuracy, and power than earlier weapons. The projectile fired from the Armstrong gun could reportedly pierce through a ship's side, and explode inside the enemy vessel, causing increased damage, and casualties. The British military adopted the Armstrong gun, and was impressed; the Duke of Cambridge even declared that it "could do everything but speak." Despite being significantly more advanced than its predecessors, the Armstrong gun was rejected soon after its integration, in favour of the muzzle-loading pieces that had been in use before. While both types of gun were effective against wooden ships, neither had the capability to pierce the armour of ironclads; due to reports of slight problems with the breeches of the Armstrong gun, and their higher cost, the older muzzle-loaders were selected to remain in service instead. Realizing that iron was more difficult to pierce with breech-loaded cannon, Armstrong designed rifled muzzle-loading guns, which proved successful; "The Times" reported: "even the fondest believers in the invulnerability of our present ironclads were obliged to confess that against such artillery, at such ranges, their plates and sides were almost as penetrable as wooden ships."
The superior cannon of the Western world brought them tremendous advantages in warfare. For example, in the Opium War in China, during the 19th century, British battleships bombarded the coastal areas and fortifications from afar, safe from the reach of the Chinese cannon. Similarly, the shortest war in recorded history, the Anglo-Zanzibar War of 1896, was brought to a swift conclusion by shelling from British cruisers. The cynical attitude toward recruited infantry in the face of ever more powerful field artillery is the source of the term "cannon fodder", first used by François-René de Chateaubriand, in 1814; however, the concept of regarding soldiers as nothing more than "food for powder" was mentioned by William Shakespeare as early as 1598, in Henry IV, Part 1.
20th and 21st centuries.
Cannon in the 20th and 21st centuries are usually divided into sub-categories and given separate names. Some of the most widely used types of modern cannon are howitzers, mortars, guns, and autocannon, although a few superguns—extremely large, custom-designed cannon—have also been constructed. Nuclear artillery was experimented with, but was abandoned as impractical. Modern artillery is used in a variety of roles, depending on its type. According to NATO, the general role of artillery is to provide fire support, which is defined as "the application of fire, coordinated with the manoeuvre of forces to destroy, neutralize, or suppress the enemy."
When referring to cannon, the term "gun" is often used incorrectly. In military usage, a gun is a cannon with a high muzzle velocity and a flat trajectory, useful for hitting the sides of targets such as walls, as opposed to howitzers or mortars, which have lower muzzle velocities, and fire indirectly, lobbing shells up and over obstacles to hit the target from above.
By the early 20th century, infantry weapons had become more powerful, forcing most artillery away from the front lines. Despite the change to indirect fire, cannon proved highly effective during World War I, directly or indirectly causing over 75% of casualties. The onset of trench warfare after the first few months of World War I greatly increased the demand for howitzers, as they were more suited at hitting targets in trenches. Furthermore, their shells carried more explosives than those of guns, and caused considerably less barrel wear. The German army had the advantage here as they began the war with many more howitzers than the French. World War I also saw the use of the Paris Gun, the longest-ranged gun ever fired. This calibre gun was used by the Germans against Paris and could hit targets more than away.
The Second World War sparked new developments in cannon technology. Among them were sabot rounds, hollow-charge projectiles, and proximity fuses, all of which increased the effectiveness of cannon against specific target. The proximity fuse emerged on the battlefields of Europe in late December 1944. Used to great effect in anti-aircraft projectiles, proximity fuses were fielded in both the European and Pacific Theatres of Operations; they were particularly useful against V-1 flying bombs and kamikaze planes. Although widely used in naval warfare, and in anti-air guns, both the British and Americans feared unexploded proximity fuses would be reverse engineered leading to them limiting its use in continental battles. During the Battle of the Bulge, however, the fuses became known as the American artillery's "Christmas present" for the German army because of their effectiveness against German personnel in the open, when they frequently dispersed attacks. Anti-tank guns were also tremendously improved during the war: in 1939, the British used primarily 2 pounder and 6 pounder guns. By the end of the war, 17 pounders had proven much more effective against German tanks, and 32 pounders had entered development. Meanwhile, German tanks were continuously upgraded with better main guns, in addition to other improvements. For example, the Panzer III was originally designed with a 37 mm gun, but was mass-produced with a 50 mm cannon. To counter the threat of the Russian T-34s, another, more powerful 50 mm gun was introduced, only to give way to a larger 75 mm cannon, which was in a fixed mount as the StuG III, the most-produced German World War II armoured fighting vehicle of any type. Despite the improved guns, production of the Panzer III was ended in 1943, as the tank still could not match the T-34, and was replaced by the Panzer IV and Panther tanks. In 1944, the 8.8 cm KwK 43 and many variations, entered service with the Wehrmacht, and was used as both a tank main gun, and as the PaK 43 anti-tank gun. One of the most powerful guns to see service in World War II, it was capable of destroying any Allied tank at very long ranges.
Despite being designed to fire at trajectories with a steep angle of descent, howitzers can be fired directly, as was done by the 11th Marine Regiment at the Battle of Chosin Reservoir, during the Korean War. Two field batteries fired directly upon a battalion of Chinese infantry; the Marines were forced to brace themselves against their howitzers, as they had no time to dig them in. The Chinese infantry took heavy casualties, and were forced to retreat.
The tendency to create larger calibre cannon during the World Wars has reversed since. The United States Army, for example, sought a lighter, more versatile howitzer, to replace their ageing pieces. As it could be towed, the M198 was selected to be the successor to the World War II–era cannon used at the time, and entered service in 1979. Still in use today, the M198 is, in turn, being slowly replaced by the M777 Ultralightweight howitzer, which weighs nearly half as much and can be more easily moved. Although land-based artillery such as the M198 are powerful, long-ranged, and accurate, naval guns have not been neglected, despite being much smaller than in the past, and, in some cases, having been replaced by cruise missiles. However, the 's planned armament includes the Advanced Gun System (AGS), a pair of 155 mm guns, which fire the Long Range Land-Attack Projectile. The warhead, which weighs , has a circular error of probability of , and will be mounted on a rocket, to increase the effective range to , further than that of the Paris Gun. The AGS's barrels will be water cooled, and will fire 10 rounds per minute, per gun. The combined firepower from both turrets will give a "Zumwalt"-class destroyer the firepower equivalent to 18 conventional M-198 howitzers. The reason for the re-integration of cannon as a main armament in United States Navy ships is because satellite-guided munitions fired from a gun are less expensive than a cruise missile but have a similar guidance capability.
Autocannon.
Autocannon have an automatic firing mode, similar to that of a machine gun. They have mechanisms to automatically load their ammunition, and therefore have a higher rate of fire than artillery, often approaching, or, in the case of Gatling guns, even surpassing the firing rate of a machine gun. While there is no minimum bore for autocannon, they are generally larger than machine guns, typically 20 mm or greater since World War II and are usually capable of using explosive ammunition even if it isn't always used. Machine guns in contrast are usually too small to use explosive ammunition.
Most nations use rapid-fire cannon on light vehicles, replacing a more powerful, but heavier, tank gun. A typical autocannon is the 25 mm "Bushmaster" chain gun, mounted on the LAV-25 and M2 Bradley armored vehicles. Cannon may be capable of a very high rate of fire, but ammunition is heavy and bulky, limiting the amount carried. For this reason, both the 25 mm Bushmaster and the 30 mm RARDEN are deliberately designed with relatively low rates of fire. The typical rate of fire for modern autocannon ranges from 90 to 1,800 rounds per minute. Systems with multiple barrels such as Gatling guns can have rates of fire of more than several thousand rounds per minute. The fastest of these is the GSh-6-23, which has a rate of fire of over 10,000 rounds per minute.
Autocannon are often found in aircraft, where they replaced machine guns, and as shipboard anti-aircraft weapons, as they provide greater destructive power than machine guns.
Aircraft use.
The first documented installation of a cannon on an aircraft was on the Voisin Canon in 1911, displayed at the Paris Exposition that year.
By World War I, all of the major powers were experimenting with aircraft mounted cannons; however their low rate of fire and great size and weight precluded any of them from being anything other than experimental. The most successful (or least unsuccessful) was the SPAD 12 Ca.1 with a single 37mm Puteaux mounted to fire between the cylinder banks and through the propeller boss of the aircraft's Hispano-Suiza 8C. The pilot (by necessity an ace) had to manually reload each round.
The first autocannons were developed during World War I as anti-aircraft guns, and one of these - the Coventry Ordnance Works "COW 37 mm gun" was installed in an aircraft but the war ended before it could be given a field trial and never became standard equipment in a production aircraft. Later trials had it fixed at a steep angle upwards in both the Vickers Type 161 and the Westland C.O.W. Gun Fighter, an idea that would return later.
During this period autocannons became available and several fighters of the German "Luftwaffe" and the Imperial Japanese Navy Air Service were fitted with 20mm cannons. They continued to be installed as an adjunct to machine guns rather than as a replacement, as the rate of fire was still too low and the complete installation too heavy. There was a some debate in the RAF as to whether the greater number of possible rounds being fired from a machine gun, or a smaller number of explosive rounds from a cannon was preferable. Improvements during the war in regards to rate of fire allowed the cannon to displace the machine gun almost entirely. The cannon was more effective against armour so they were increasingly used during the course of World War II, and newer fighters such as the Hawker Tempest usually carried two or four versus the six .50 Browning machine guns for US aircraft or eight to twelve M1919 Browning machine guns on British aircraft. The Hispano-Suiza HS.404, Oerlikon 20 mm cannon, MG FF, and their numerous variants became among the most widely used autocannon in the war. Cannons, as with machine guns, were generally fixed to fire forwards (mounted in the wings, in the nose or fuselage, or in a pannier under either); or were mounted in gun turrets on heavier aircraft. Both the Germans and Japanese mounted cannons to fire upwards and forwards for use against heavy bombers, with the Germans calling guns so-installed "Schräge Musik" . Schräge Musik derives from the German colloquialism for Jazz Music (the German word schräg literally means slanted or oblique)
Preceding the Vietnam War the high speeds aircraft were attaining led to a move to remove the cannon due to the mistaken belief that they would be useless in a dogfight, but combat experience during the Vietnam War showed conclusively that despite advances in missiles, there was still a need for them. Nearly all modern fighter aircraft are armed with an autocannon and they are also commonly found on ground-attack aircraft. One of the most powerful examples is the 30mm GAU-8/A Avenger Gatling-type rotary cannon, mounted exclusively on the Fairchild Republic A-10 Thunderbolt II The Lockheed AC-130 gunship (a converted transport) can carry a 105mm howitzer as well as a variety of autocannons ranging up to 40mm. Both are used in the close air support role.
Operation.
In the 1770s, cannon operation worked as follows: each cannon would be manned by two gunners, six soldiers, and four officers of artillery. The right gunner was to prime the piece and load it with powder, and the left gunner would fetch the powder from the magazine and be ready to fire the cannon at the officer's command. On each side of the cannon, three soldiers stood, to ram and sponge the cannon, and hold the ladle. The second soldier on the left tasked with providing 50 bullets.
Before loading, the cannon would be cleaned with a wet sponge to extinguish any smouldering material from the last shot. Fresh powder could be set off prematurely by lingering ignition sources. The powder was added, followed by wadding of paper or hay, and the ball was placed in and rammed down. After ramming the cannon would be aimed with the elevation set using a quadrant and a plummet. At 45 degrees, the ball had the utmost range: about ten times the gun's level range. Any angle above a horizontal line was called random-shot. Wet sponges were used to cool the pieces every ten or twelve rounds.
During the Napoleonic Wars, a British gun team consisted of five gunners to aim it, clean the bore with a damp sponge to quench any remaining embers before a fresh charge was introduced, and another to load the gun with a bag of powder and then the projectile. The fourth gunner pressed his thumb on the vent hole, to prevent a draught that might fan a flame. The charge loaded, the fourth would prick the bagged charge through the vent hole, and fill the vent with powder. On command, the fifth gunner would fire the piece with a slowmatch.
When a cannon had to be abandoned such as in a retreat or surrender, the touch hole of the cannon would be plugged flush with an iron spike, disabling the cannon (at least until metal boring tools could be used to remove the plug). This was called "spiking the cannon".
Deceptive use.
Historically, logs or poles have been used as decoys to mislead the enemy as to the strength of an emplacement. The "Quaker gun trick" was used by Colonel William Washington's Continentals, during the American Revolutionary War; in 1780, approximately 100 Loyalists surrendered to them, rather than face bombardment. During the American Civil War, Quaker guns were also used by the Confederates, to compensate for their shortage of artillery. The decoy cannon were painted black at the "muzzle", and positioned behind fortifications to delay Union attacks on those positions. On occasion, real gun carriages were used to complete the deception.
In popular culture.
Music.
Cannon sounds have sometimes been used in classical pieces with a military theme. Giuseppe Sarti is believed to be the first composer to orchestrate real cannon in a musical work. His "Te Deum" celebrates the Russian victory at Ochakov (1789) with the firing of a real cannon and the use of fireworks, to heighten the martial effect of the music.
One of the best known examples of such a piece is another Russian work, Pyotr Ilyich Tchaikovsky's "1812 Overture". The overture is properly performed using an artillery section together with the orchestra, resulting in noise levels requiring musicians to wear ear protection. The cannon fire simulates Russian artillery bombardments of the Battle of Borodino, a critical battle in Napoleon's invasion of Russia, whose defeat the piece celebrates. When the overture was first performed, the cannon were fired by an electric current triggered by the conductor. However, the overture was not recorded with real cannon fire until Mercury Records and conductor Antal Doráti's 1958 recording of the Minnesota Orchestra. Cannon fire is also frequently used annually in presentations of the "1812" on the American Independence Day, a tradition started by Arthur Fiedler of the Boston Pops in 1974.
The hard rock band AC/DC also used cannon in their song "For Those About to Rock (We Salute You)", and in live shows replica Napoleonic cannon and pyrotechnics were used to perform the piece.
Restoration.
Cannon recovered from the sea are often extensively damaged from exposure to salt water; because of this, electrolytic reduction treatment is required to forestall the process of corrosion. The cannon is then washed in deionized water to remove the electrolyte, and is treated in tannic acid, which prevents further rust and gives the metal a bluish-black colour. After this process, cannon on display may be protected from oxygen and moisture by a wax sealant. A coat of polyurethane may also be painted over the wax sealant, to prevent the wax-coated cannon from attracting dust in outdoor displays.
Recently archaeologists say six cannon recovered from a river in Panama that could have belonged to legendary pirate Henry Morgan are being studied and could eventually be displayed after going through a restoration process.

</doc>
<doc id="7056" url="http://en.wikipedia.org/wiki?curid=7056" title="Mouse (computing)">
Mouse (computing)

In computing, a mouse is a pointing device that detects two-dimensional motion relative to a surface. This motion is typically translated into the motion of a pointer on a display, which allows for fine control of a graphical user interface.
Physically, a mouse consists of an object held in one's hand, with one or more buttons. Mice often also feature other elements, such as touch surfaces and "wheels", which enable additional control and dimensional input.
Naming.
The earliest known publication of the term "mouse" as a computer pointing device is in Bill English's 1965 publication "Computer-Aided Display Control".
The online "Oxford Dictionaries" entry for "mouse" states the plural for the small rodent is "mice", while the plural for the small computer connected device is either "mice" or "mouses". However, in the use section of the entry it states that the more common plural is "mice", and that the first recorded use of the term in the plural is "mice" as well (though it cites a 1984 use of "mice" when there were actually several earlier ones, such as J. C. R. Licklider's "The Computer as a Communication Device" of 1968). According to the fifth edition of "The American Heritage Dictionary of the English Language" the plural can be either "mice" or "mouses".
History.
The trackball, a related pointing device, was invented in 1946 by Ralph Benjamin as part of a post-World War II-era radar plotting system called Comprehensive Display System (CDS). Benjamin was then working for the British Royal Navy Scientific Service. Benjamin's project used analog computers to calculate the future position of target aircraft based on several initial input points provided by a user with a joystick. Benjamin felt that a more elegant input device was needed and invented a ball tracker called "roller ball" for this purpose.
The device was patented in 1947, but only a prototype using a metal ball rolling on two rubber-coated wheels was ever built and the device was kept as a military secret.
Another early trackball was built by British electrical engineer Kenyon Taylor in collaboration with Tom Cranston and Fred Longstaff. Taylor was part of the original Ferranti Canada, working on the Royal Canadian Navy's DATAR (Digital Automated Tracking and Resolving) system in 1952.
DATAR was similar in concept to Benjamin's display. The trackball used four disks to pick up motion, two each for the X and Y directions. Several rollers provided mechanical support. When the ball was rolled, the pickup discs spun and contacts on their outer rim made periodic contact with wires, producing pulses of output with each movement of the ball. By counting the pulses, the physical movement of the ball could be determined. A digital computer calculated the tracks, and sent the resulting data to other ships in a task force using pulse-code modulation radio signals. This trackball used a standard Canadian five-pin bowling ball. It was not patented, as it was a secret military project as well.
Independently, Douglas Engelbart at the Stanford Research Institute (now SRI International) invented his first mouse prototype in the 1960s with the assistance of his lead engineer Bill English. They christened the device the "mouse" as early models had a cord attached to the rear part of the device looking like a tail and generally resembling the common mouse. Engelbart never received any royalties for it, as his employer SRI held the patent, which ran out before it became widely used in personal computers. The invention of the mouse was just a small part of Engelbart's much larger project, aimed at augmenting human intellect via the Augmentation Research Center.
Several other experimental pointing-devices developed for Engelbart's oN-Line System (NLS) exploited different body movementsbut ultimately the mouse won out because of its speed and convenience. The first mouse, a bulky device (pictured) used two wheels perpendicular to each other: the rotation of each wheel translated into motion along one axis.
On 2 October 1968, just a few months before Engelbart released his demo on 9 December 1968, a mouse device named ' (German for "rolling ball") was released that had been developed and published by the German company Telefunken. As the name suggests and unlike Engelbart's mouse, the Telefunken model already had a ball. It was based on an earlier trackball-like device (also named ') that was embedded into radar flight control desks. This had been developed around 1965 by a team led by Rainer Mallebrein at Telefunken for the German "Bundesanstalt für Flugsicherung" as part of their TR 86 process computer system with its SIG 100-86 vector graphics terminal.
When the development for the Telefunken main frame began in 1965, and his team came up with the idea of "reversing" the existing into a moveable mouse-like device, so that customers did not have to be bothered with mounting holes for the earlier trackball device. Together with light pens and trackballs, it was offered as optional input device for their system since 1968. Some samples, installed at the in Munich in 1972, are still well preserved. Telefunken considered the invention too small to apply for a patent on their device.
The Xerox Alto was one of the first computers designed for individual use in 1973, and is regarded as the grandfather of computers that utilize the mouse. Inspired by PARC's Alto, the Lilith, a computer, which had been developed by a team around at ETH Zürich between 1978 and 1980, provided a mouse as well. The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star Information System in 1981. In 1982, Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning Microsoft Hardware. However, the mouse remained relatively obscure until the 1984 appearance of the Macintosh 128K, which included an updated version of the original Lisa Mouse.
Operation.
A mouse typically controls the motion of a pointer in two dimensions in a graphical user interface (GUI). The mouse turns movements of the hand backward and forward, left and right into equivalent electronic signals that in turn are used to move the pointer.
The relative movements of the mouse on the surface are applied to the position of the pointer on the screen, which signals the point where actions of the user take place, so that the hand movements are replicated by the pointer. Clicking or hovering (stopping movement while the cursor is within the bounds of an area) can select files, programs or actions from a list of names, or (in graphical interfaces) through small images called "icons" and other elements. For example, a text file might be represented by a picture of a paper notebook, and clicking while the cursor hovers this icon might cause a text editing program to open the file in a window.
Different ways of operating the mouse cause specific things to happen in the GUI:
Mouse gestures.
Users can also employ mice "gesturally"; meaning that a stylized motion of the mouse cursor itself, called a "gesture", can issue a command or map to a specific action. For example, in a drawing program, moving the mouse in a rapid "x" motion over a shape might delete the shape.
Gestural interfaces occur more rarely than plain pointing-and-clicking; and people often find them more difficult to use, because they require finer motor-control from the user. However, a few gestural conventions have become widespread, including the drag and drop gesture, in which:
For example, a user might drag-and-drop a picture representing a file onto a picture of a trash can, thus instructing the system to delete the file.
Standard semantic gestures include:
Specific uses.
Other uses of the mouse's input occur commonly in special application-domains. In interactive three-dimensional graphics, the mouse's motion often translates directly into changes in the virtual camera's orientation. For example, in the first-person shooter genre of games (see below), players usually employ the mouse to control the direction in which the virtual player's "head" faces: moving the mouse up will cause the player to look up, revealing the view above the player's head. A related function makes an image of an object rotate, so that all sides can be examined.
When mice have more than one button, software may assign different functions to each button. Often, the primary (leftmost in a right-handed configuration) button on the mouse will select items, and the secondary (rightmost in a right-handed) button will bring up a menu of alternative actions applicable to that item. For example, on platforms with more than one button, the Mozilla web browser will follow a link in response to a primary button click, will bring up a contextual menu of alternative actions for that link in response to a secondary-button click, and will often open the link in a new tab or window in response to a click with the tertiary (middle) mouse button.
Variants.
Mechanical mice.
The German company Telefunken published on their early ball mouse on October 2, 1968. Telefunken's mouse was sold as optional equipment for their computer systems. Bill English, builder of Engelbart's original mouse, created a ball mouse in 1972 while working for Xerox PARC.
The ball mouse replaced the external wheels with a single ball that could rotate in any direction. It came as part of the hardware package of the Xerox Alto computer. Perpendicular chopper wheels housed inside the mouse's body chopped beams of light on the way to light sensors, thus detecting in their turn the motion of the ball. This variant of the mouse resembled an inverted trackball and became the predominant form used with personal computers throughout the 1980s and 1990s. The Xerox PARC group also settled on the modern technique of using both hands to type on a full-size keyboard and grabbing the mouse when required.
The ball mouse has two freely rotating rollers. They are located 90 degrees apart. One roller detects the forward–backward motion of the mouse and other the left–right motion. Opposite the two rollers is a third one (white, in the photo, at 45 degrees) that is spring-loaded to push the ball against the other two rollers. Each roller is on the same shaft as an encoder wheel that has slotted edges; the slots interrupt infrared light beams to generate electrical pulses that represent wheel movement. Each wheel's disc, however, has a pair of light beams, located so that a given beam becomes interrupted, or again starts to pass light freely, when the other beam of the pair is about halfway between changes.
Simple logic circuits interpret the relative timing to indicate which direction the wheel is rotating. This incremental rotary encoder scheme is sometimes called quadrature encoding of the wheel rotation, as the two optical sensor produce signals that are in approximately quadrature phase. The mouse sends these signals to the computer system via the mouse cable, directly as logic signals in very old mice such as the Xerox mice, and via a data-formatting IC in modern mice. The driver software in the system converts the signals into motion of the mouse cursor along X and Y axes on the computer screen.
The ball is mostly steel, with a precision spherical rubber surface. The weight of the ball, given an appropriate working surface under the mouse, provides a reliable grip so the mouse's movement is transmitted accurately. Ball mice and wheel mice were manufactured for Xerox by Jack Hawley, doing business as The Mouse House in Berkeley, California, starting in 1975. Based on another invention by Jack Hawley, proprietor of the Mouse House, Honeywell produced another type of mechanical mouse. Instead of a ball, it had two wheels rotating at off axes. Key Tronic later produced a similar product.
Modern computer mice took form at the École Polytechnique Fédérale de Lausanne (EPFL) under the inspiration of Professor Jean-Daniel Nicoud and at the hands of engineer and watchmaker André Guignard. This new design incorporated a single hard rubber mouseball and three buttons, and remained a common design until the mainstream adoption of the scroll-wheel mouse during the 1990s. In 1985, René Sommer added a microprocessor to Nicoud's and Guignard's design. Through this innovation, Sommer is credited with inventing a significant component of the mouse, which made it more "intelligent;" though optical mice from Mouse Systems had incorporated microprocessors by 1984.
Another type of mechanical mouse, the "analog mouse" (now generally regarded as obsolete), uses potentiometers rather than encoder wheels, and is typically designed to be plug compatible with an analog joystick. The "Color Mouse", originally marketed by RadioShack for their Color Computer (but also usable on MS-DOS machines equipped with analog joystick ports, provided the software accepted joystick input) was the best-known example.
Optical and laser mice.
Optical mice make use of one or more light-emitting diodes (LEDs) and an imaging array of photodiodes to detect movement relative to the underlying surface, rather than internal moving parts as does a mechanical mouse. A laser mouse is an optical mouse that uses coherent (laser) light.
The earliest optical mice detected movement on pre-printed mousepad surfaces, whereas the modern optical mouse works on most opaque surfaces; it is usually unable to detect movement on specular surfaces like glass. Laser diodes are also used for better resolution and precision. Battery powered, wireless optical mice flash the LED intermittently to save power, and only glow steadily when movement is detected.
Inertial and gyroscopic mice.
Often called "air mice" since they do not require a surface to operate, inertial mice use a tuning fork or other accelerometer (, published in 1988) to detect rotary movement for every axis supported. The most common models (manufactured by Logitech and Gyration) work using 2 degrees of rotational freedom and are insensitive to spatial translation. The user requires only small wrist rotations to move the cursor, reducing user fatigue or "gorilla arm".
Usually cordless, they often have a switch to deactivate the movement circuitry between use, allowing the user freedom of movement without affecting the cursor position. A patent for an inertial mouse claims that such mice consume less power than optically based mice, and offer increased sensitivity, reduced weight and increased ease-of-use. In combination with a wireless keyboard an inertial mouse can offer alternative ergonomic arrangements which do not require a flat work surface, potentially alleviating some types of repetitive motion injuries related to workstation posture.
3D mice.
Also known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion/Logitech's SpaceMouse from the early 1990s. In the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.
A recent consumer 3D pointing device is the Wii Remote. While primarily a motion-sensing device (that is, it can determine its orientation and direction of movement), Wii Remote can also detect its spatial position by comparing the distance and position of the lights from the IR emitter using its integrated IR camera (since the nunchuk accessory lacks a camera, it can only tell its current heading and orientation). The obvious drawback to this approach is that it can only produce spatial coordinates while its camera can see the sensor bar.
A mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each. In November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.
Tactile mice.
In 2000, Logitech introduced a "tactile mouse" that contained a small actuator to make the mouse vibrate. Such a mouse can augment user-interfaces with haptic feedback, such as giving feedback when crossing a window boundary. To surf by touch requires the user to be able to feel depth or hardness; this ability was realized with the first electrorheological tactile mice but never marketed.
Ergonomic mice.
As the name suggests, this type of mouse is intended to provide optimum comfort and avoid injuries such as carpal tunnel syndrome, arthritis and other repetitive strain injuries. It is designed to fit natural hand position and movements, to reduce discomfort.
When holding a typical mouse, ulna and radius bones on the arm are crossed. Some designs attempt to place the palm more vertically, so the bones take more natural parallel position. Some limit wrist movement, encouraging to use arm instead that may be less precise but more optimal from the health point of view. A mouse may be angled from the thumb downward to the opposite side - this is known to reduce wrist pronation. However such optimizations make the mouse right or left hand specific, making more problematic to change the tired hand.
Another solution is a pointing bar device. The so-called "roller bar mouse" is positioned snuggly in front of the keyboard, thus allowing bi-manual accessibility.
Gaming mice.
These mice are specifically designed for use in computer games. They typically employ a wide array of controls and buttons and have designs that differ radically from traditional mice. It is also common for gaming mice, especially those designed for use in real-time strategy games such as "StarCraft", or in multiplayer online battle arena games such as "Dota 2" and "League of Legends", to have a relatively high sensitivity, measured in dots per inch (DPI). Some advanced mice from gaming manufacturers also allow users to customize the weight of the mouse by adding or subtracting weights to allow for easier control. Ergonomic quality is also an important factor in gaming mice, as extended gameplay times may render further use of the mouse to be uncomfortable. Gaming mice are held by gamers in three styles of grip:
Connectivity and communication protocols.
To transmit their input, typical cabled mice use a thin electrical cord terminating in a standard connector, such as RS-232C, PS/2, ADB or USB. Cordless mice instead transmit data via infrared radiation (see IrDA) or radio (including Bluetooth), although many such cordless interfaces are themselves connected through the aforementioned wired serial buses.
While the electrical interface and the format of the data transmitted by commonly available mice is currently standardized on USB, in the past it varied between different manufacturers. A bus mouse used a dedicated interface card for connection to an IBM PC or compatible computer.
Mouse use in DOS applications became more common after the introduction of the Microsoft mouse, largely because Microsoft provided an open standard for communication between applications and mouse driver software. Thus, any application written to use the Microsoft standard could use a mouse with a driver that implements the same API, even if the mouse hardware itself was incompatible with Microsoft's. This driver provides the state of the buttons and the distance the mouse has moved in units that its documentation calls "mickeys", as does the Allegro library.
Serial interface and protocol.
Standard PC mice once used the RS-232C serial port via a D-subminiature connector, which provided power to run the mouse's circuits as well as data on mouse movements. The Mouse Systems Corporation version used a five-byte protocol and supported three buttons. The Microsoft version used a three-byte protocol and supported two buttons. Due to the incompatibility between the two protocols, some manufacturers sold serial mice with a mode switch: "PC" for MSC mode, "MS" for Microsoft mode.
PS/2 interface and protocol.
With the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 interface for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin connector. In default mode (called "stream mode") a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets. For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:
Here, XS and YS represent the sign bits of the movement vectors, XV and YV indicate an overflow in the respective vector component, and LB, MB and RB indicate the status of the left, middle and right mouse buttons (1 = pressed). PS/2 mice also understand several commands for reset and self-test, switching between different operating modes, and changing the resolution of the reported motion vectors.
A Microsoft IntelliMouse relies on an extension of the PS/2 protocol: the ImPS/2 or IMPS/2 protocol (the abbreviation combines the concepts of "IntelliMouse" and "PS/2"). It initially operates in standard PS/2 format, for backwards compatibility. After the host sends a special command sequence, it switches to an extended format in which a fourth byte carries information about wheel movements. The IntelliMouse Explorer works analogously, with the difference that its 4-byte packets also allow for two additional buttons (for a total of five).
Mouse vendors also use other extended formats, often without providing public documentation. The Typhoon mouse uses 6-byte packets which can appear as a sequence of two standard 3-byte packets, such that an ordinary PS/2 driver can handle them. For 3-D (or 6-degree-of-freedom) input, vendors have made many extensions both to the hardware and to software. In the late 1990s Logitech created ultrasound based tracking which gave 3D input to a few millimetres accuracy, which worked well as an input device but failed as a profitable product. In 2008, Motion4U introduced its "OptiBurst" system using IR tracking for use as a Maya (graphics software) plugin.
Apple Desktop Bus.
In 1986 Apple first implemented the Apple Desktop Bus allowing the daisy-chaining together of up to 16 devices, including arbitrarily many mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to computer/mouse communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when iMac joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.
USB.
The industry-standard USB (Universal Serial Bus) protocol and its connector have become widely used for mice; it is among the most popular types.
Cordless or wireless.
Cordless or wireless mice transmit data via infrared radiation (see IrDA) or radio (including Bluetooth and Wi-Fi). The receiver is connected to the computer through a serial or USB port, or can be built in (as is sometimes the case with Bluetooth and WiFi).
Modern non-Bluetooth and non-WiFi wireless mice use USB receivers. Some of these can be stored inside the mouse for safe transport while not in use, while other, newer mice use newer "nano" receivers, designed to be small enough to remain plugged into a laptop during transport, while still being large enough to easily remove.
Atari standard joystick connectivity.
The Amiga and the Atari ST use an Atari standard DE-9 connector for mice, the same connector that is used for joysticks on the same computers and numerous 8-bit systems, such as the Commodore 64 and the Atari 2600. However, the signals used for mice are different from those used for joysticks. As a result, plugging a mouse into a joystick port causes the "joystick" to continuously move in some direction, even if the mouse stays still, whereas plugging a joystick into a mouse port causes the "mouse" to only be able to move a single pixel in each direction.
Multiple-mouse systems.
Some systems allow two or more mice to be used at once as input devices. 16-bit era home computers such as the Amiga used this to allow computer games with two players interacting on the same computer (Lemmings and The Settlers for example). The same idea is sometimes used in collaborative software, e.g. to simulate a whiteboard that multiple users can draw on without passing a single mouse around.
Microsoft Windows, since Windows 98, has supported multiple simultaneous pointing devices. Because Windows only provides a single screen cursor, using more than one device at the same time requires cooperation of users or applications designed for multiple input devices.
Multiple mice are often used in multi-user gaming in addition to specially designed devices that provide several input interfaces.
Windows also has full support for multiple input/mouse configurations for multiuser environments.
Starting with Windows XP, Microsoft introduced a SDK for developing applications that allow multiple input devices to be used at the same time with independent cursors and independent input points.
The introduction of Vista and Microsoft Surface (now known as Microsoft PixelSense) introduced a new set of input APIs that were adopted into Windows 7, allowing for 50 points/cursors, all controlled by independent users. The new input points provide traditional mouse input; however, are designed for more advanced input technology like touch and image. They inherently offer 3D coordinates along with pressure, size, tilt, angle, mask, and even an image bitmap to see and recognize the input point/object on the screen.
As of 2009, Linux distributions and other operating systems that use X.Org, such as OpenSolaris and FreeBSD, support 255 cursors/input points through Multi-Pointer X. However, currently no window managers support Multi-Pointer X leaving it relegated to custom software usage.
There have also been propositions of having a single operator use two mice simultaneously as a more sophisticated means of controlling various graphics and multimedia applications.
Buttons.
Mouse buttons are microswitches which can be pressed to select or interact with an element of a graphical user interface, producing a distinctive clicking sound.
The three-button scrollmouse has become the most commonly available design. As of 2007 (and roughly since the late 1990s), users most commonly employ the second button to invoke a contextual menu in the computer's software user interface, which contains options specifically tailored to the interface element over which the mouse cursor currently sits. By default, the primary mouse button sits located on the left-hand side of the mouse, for the benefit of right-handed users; left-handed users can usually reverse this configuration via software.
Mouse speed.
Mickeys per second is a unit of measurement for the speed and movement direction of a computer mouse. But speed can also refer to the ratio between how many pixels the cursor moves on the screen and how far the mouse moves on the mouse pad, which may be expressed as pixels per Mickey, or pixels per inch, or pixels per cm. The directional movement is called the horizontal mickey count and the vertical mickey count.
The computer industry often measures mouse sensitivity in terms of counts per inch (CPI), commonly expressed as dots per inch (DPI)the number of steps the mouse will report when it moves one inch. In early mice, this specification was called pulses per inch (ppi). The Mickey originally referred to one of these counts, or one resolvable step of motion. If the default mouse-tracking condition involves moving the cursor by one screen-pixel or dot on-screen per reported step, then the CPI does equate to DPI: dots of cursor motion per inch of mouse motion. The CPI or DPI as reported by manufacturers depends on how they make the mouse; the higher the CPI, the faster the cursor moves with mouse movement. However, software can adjust the mouse sensitivity, making the cursor move faster or slower than its CPI. software can change the speed of the cursor dynamically, taking into account the mouse's absolute speed and the movement from the last stop-point. In most software, an example being the Windows platforms, this setting is named "speed" referring to "cursor precision". However, some operating systems name this setting "acceleration", the typical Apple OS designation. This term is in fact incorrect. The mouse acceleration, in the majority of mouse software, refers to the setting allowing the user to modify the cursor acceleration: the change in speed of the cursor over time while the mouse movement is constant.
For simple software, when the mouse starts to move, the software will count the number of "counts" or "mickeys" received from the mouse and will move the cursor across the screen by that number of pixels (or multiplied by a rate factor, typically less than 1). The cursor will move slowly on the screen, having a good precision. When the movement of the mouse passes the value set for "threshold", the software will start to move the cursor more quickly, with a greater rate factor. Usually, the user can set the value of the second rate factor by changing the "acceleration" setting.
Operating systems sometimes apply acceleration, referred to as "ballistics", to the motion reported by the mouse. For example, versions of Windows prior to Windows XP doubled reported values above a configurable threshold, and then optionally doubled them again above a second configurable threshold. These doublings applied separately in the X and Y directions, resulting in very nonlinear response.
Mousepads.
Engelbart's original mouse did not require a mousepad; the mouse had two large wheels which could roll on virtually any surface. However, most subsequent mechanical mice starting with the steel roller ball mouse have required a mousepad for optimal performance.
The mousepad, the most common mouse accessory, appears most commonly in conjunction with mechanical mice, because to roll smoothly the ball requires more friction than common desk surfaces usually provide. So-called "hard mousepads" for gamers or optical/laser mice also exist.
Most optical and laser mice do not require a pad. Whether to use a hard or soft mousepad with an optical mouse is largely a matter of personal preference. One exception occurs when the desk surface creates problems for the optical or laser tracking, for example, a transparent or reflective surface.
In the marketplace.
Around 1981 Xerox included mice with its Xerox Star, based on the mouse used in the 1970s on the Alto computer at Xerox PARC. Sun Microsystems, Symbolics, Lisp Machines Inc., and Tektronix also shipped workstations with mice, starting in about 1981. Later, inspired by the Star, Apple Computer released the Apple Lisa, which also used a mouse. However, none of these products achieved large-scale success. Only with the release of the Apple Macintosh in 1984 did the mouse see widespread use.
The Macintosh design, commercially successful and technically influential, led many other vendors to begin producing mice or including them with their other computer products (by 1986, Atari ST, Amiga, Windows 1.0, GEOS for the Commodore 64, and the Apple IIGS).
The widespread adoption of graphical user interfaces in the software of the 1980s and 1990s made mice all but indispensable for controlling computers. In November 2008, Logitech built their billionth mouse.
Use in games.
Mice often function as an interface for PC-based computer games and sometimes for video game consoles.
First-person shooters.
First-person shooters naturally lend themselves to separate and simultaneous control of the player's movement and aim, and on computers this has traditionally been achieved with a combination of keyboard and mouse. Players use the X-axis of the mouse for looking (or turning) left and right, and the Y-axis for looking up and down; the keyboard is used for movement and supplemental inputs.
Many first person gamers prefer a mouse over a gamepad or joystick because the mouse is a linear input device, which allows for fast and precise control. Holding a gamepad or joystick in a given position produces a corresponding constant movement or rotation, i.e. the output is an integral of the user's input; in contrast, the output of a mouse directly corresponds to how far it is moved in a given direction (often multiplied by an "acceleration" factor derived from how quickly the mouse is moved). The effect of this is that a mouse is well suited to small, precise movements as well as large, quick movements, both of which are important in first person gaming. This advantage also extends in varying degrees to other game styles, notably real-time strategy.
The left button usually controls primary fire. If the game supports multiple fire modes, the right button often provides secondary fire from the selected weapon. Games with only a single fire mode will generally map secondary fire to "ironsights". In some games, the right button may also provide bonus options for a particular weapon, such as allowing access to the scope of a sniper rifle or allowing the mounting of a bayonet or silencer.
Gamers can use a scroll wheel for changing weapons (or for controlling scope-zoom magnification, in older games). On most first person shooter games, programming may also assign more functions to additional buttons on mice with more than three controls. A keyboard usually controls movement (for example, WASD for moving forward, left, backward and right, respectively) and other functions such as changing posture. Since the mouse serves for aiming, a mouse that tracks movement accurately and with less lag (latency) will give a player an advantage over players with less accurate or slower mice.
Many games provide players with the option of mapping their own choice of a key or button to a certain control.
An early technique of players, circle strafing, saw a player continuously strafing while aiming and shooting at an opponent by walking in circle around the opponent with the opponent at the center of the circle. Players could achieve this by holding down a key for strafing while continuously aiming the mouse towards the opponent.
Games using mice for input are so popular that many manufacturers make mice specifically for gaming. Such mice may feature adjustable weights, high-resolution optical or laser components, additional buttons, ergonomic shape, and other features such as adjustable CPI.
Many games, such as first- or third-person shooters, have a setting named "invert mouse" or similar (not to be confused with "button inversion", sometimes performed by left-handed users) which allows the user to look downward by moving the mouse forward and upward by moving the mouse backward (the opposite of non-inverted movement). This control system resembles that of aircraft control sticks, where pulling back causes pitch up and pushing forward causes pitch down; computer joysticks also typically emulate this control-configuration.
After id Software's "Doom", the game that popularized first person shooter games but which did not support vertical aiming with a mouse (the y-axis served for forward/backward movement), competitor 3D Realms' "Duke Nukem 3D" became one of the first games that supported using the mouse to aim up and down. This and other games using the Build engine had an option to invert the Y-axis. The "invert" feature actually made the mouse behave in a manner that users regard as non-inverted (by default, moving mouse forward resulted in looking down). Soon after, id Software released "Quake", which introduced the invert feature as users know it. Other games using the Quake engine have come on the market following this standard, likely due to the overall popularity of "Quake".
Home consoles.
In 1988, the educational video game system VTech Socrates featured a wireless mouse with an attached mouse pad as an optional controller used for some games. In the early 1990s the Super Nintendo Entertainment System video game system featured a mouse in addition to its controllers. The "Mario Paint" game in particular used the mouse's capabilities, as did its successor on the Nintendo 64. Sega released official mice for their Genesis/Mega Drive, Saturn and Dreamcast consoles. NEC sold official mice for its PC Engine and PC-FX consoles. Sony Computer Entertainment released an official mouse product for the PlayStation console, and included one along with the Linux for PlayStation 2 kit. However, users can attach virtually any USB mouse to the PlayStation 2 console. In addition the PlayStation 3 also fully supports USB mice. Recently the Wii also has this latest development added on in a recent software update.

</doc>
<doc id="7059" url="http://en.wikipedia.org/wiki?curid=7059" title="Civil defense">
Civil defense

Civil defense, civil defence (see spelling differences) or civil protection is an effort to protect the citizens of a state (generally non-combatants) from military attack. It uses the principles of emergency operations: prevention, mitigation, preparation, response, or emergency evacuation and recovery. Programs of this sort were initially discussed at least as early as the 1920s and were implemented in some countries during the 1930s as the threat of war and aerial bombardment grew. It became widespread after the threat of nuclear weapons was realized.
Since the end of the Cold War, the focus of civil defense has largely shifted from military attack to emergencies and disasters in general. The new concept is described by a number of terms, each of which has its own specific shade of meaning, such as crisis management, emergency management, emergency preparedness, contingency planning, emergency services, and civil protection.
In some countries, civil defense is seen as a key part of "total defense". For example in Sweden, the Swedish word "totalförsvar" refers to the commitment of a wide range of resources of the nation to its defense - including to civil protection. Respectively, some countries (notably the Soviet Union) may have or have had military-organized civil defense units (Civil Defense Troops) as part of their armed forces or as a paramilitary service.
History.
Origins.
The advent of civil defence was stimulated by the experience of the bombing of civilian areas during the First World War.The bombing of Britain began on 19 January 1915 when zeppelins dropped bombs on the Great Yarmouth area, killing six people. German bombing operations of the First World War were surprisingly effective, especially after the Gotha bombers surpassed the zeppelins. The most devastating raids inflicted 121 casualties for each ton of bombs dropped and it was this figure that was used as a basis for predictions.
After the war, attention was turned toward civil defence in the event of war, and the Air Raid Precautions Committee was established in 1924 to investigate ways for ensuring the protection of civilians from the danger of air-raids.
The Committee produced figures estimating that in London there would be 9,000 casualties in the first two days and then a continuing rate of 17,500 casualties a week. These rates were thought conservative. It was believed that there would be "total chaos and panic" and hysterical neurosis as the people of London would try to flee the city. To control the population harsh measures were proposed—bringing London under almost military control; physically cordoning London with 120,000 troops to force people back to work. A different government department proposed setting up camps for refugees for a few days before sending them back to London.
A special government department, the Civil Defence Service was established by the Home Office in 1935. Its remit included the pre-existing ARP as well as wardens, firemen (initially the Auxiliary Fire Service (AFS) and latterly the National Fire Service (NFS)), fire watchers, rescue, first aid post, stretcher party and industry. Over 1.9 million people served within the CD and nearly 2,400 lost their lives to enemy action.
The organisation of civil defence was the responsibility of the local authority. Volunteers were ascribed to different units depending on experience or training. Each local civil defence service were divided into several sections. Wardens were responsible for local reconnaissance and reporting, and leadership, organisation, guidance and control of the general public. Wardens would also advise survivors of the locations of rest and food centres, and other welfare facilities.
Rescue Parties were required to assess and then access bombed out buildings and retrieve injured or dead people. In addition they would turn off gas, electricity and water supplies, and repair or pull down unsteady buildings. Medical services, including First Aid Parties, provided on the spot medical assistance.
The expected stream of information that would be generated during an attack was handled by 'Report and Control' teams. A local headquarters would have an ARP controller that would direct rescue, first aid and decontamination teams to the scenes of reported bombing. If local services were deemed insufficient to deal with the incident then the controller could request assistance from surrounding boroughs.
Fire Guards were responsible for a designated area/building and required to monitor the fall of incendiary bombs and pass on news of any fires that had broken out to the NFS. They could deal with an individual magnesium electron incendiary bomb by dousing them in buckets of sand, water or by smothering. Additionally, 'Gas Decontamination Teams' were kitted out with gas-tight and waterproof protective clothing and were to deal with any gas attacks. They were trained to decontaminate buildings, roads, rail and other material that had been contaminated by liquid or jelly gases.
Little progress was made over the issue of air-raid shelters, because of the apparently irreconcilable conflict between the need to send the public underground for shelter and the need to keep them above ground for protection against gas attacks. In February 1936 the Home Secretary appointed a technical Committee on Structural Precautions against Air Attack. During the Munich crisis, local authorities dug trenches to provide shelter. After the crisis, the British Government decided to make these a permanent feature, with a standard design of precast concrete trench lining. They also decided to issue free to poorer households the Anderson shelter and to provide steel props to create shelters in suitable basements.
During the Second World War, the ARP was responsible for the issuing of gas masks, pre-fabricated air-raid shelters (such as Anderson shelters, as well as Morrison shelters), the upkeep of local public shelters, and the maintenance of the blackout. The ARP also helped rescue people after air raids and other attacks, and some women became ARP Ambulance Attendants whose job was to help administer first aid to casualties, search for survivors, and in many grim instances, help recover bodies, sometimes those of their own colleagues.
As the war progressed, the effectiveness of aerial bombardment was, beyond the destruction of property, very limited. There were less than three casualties for each ton of bombs dropped by the Luftwaffe in many British cities and the expected social consequences hardly happened. The morale of the British people remained high, 'shell-shock' was not at all common, and the rates of other nervous and mental ailments declined.
In the United States, the Office of Civil Defense was established in May 1941 to coordinate civilian defense efforts. It coordinated with the Department of the Army and established similar groups to the British ARP. One of these groups that still exists today is the Civil Air Patrol, which was originally created as a civilian auxiliary to the Army. The CAP was created on December 1, 1941, with the main civil defense mission of search and rescue. The CAP also sunk 2 Axis submarines and provided aerial reconnaissance for Allied and neutral merchant ships. In 1946, the Civil Air Patrol was barred from combat by Public Law 79-476. The CAP then received its current mission: search and rescue for downed aircraft. When the Air Force was created, in 1947, the Civil Air Patrol became the auxiliary of the Air Force.
Atomic Age.
In most of the NATO states, such as the United States, the United Kingdom and West Germany as well as the Soviet Bloc, and especially in the neutral countries, such as Switzerland and in Sweden during the 1950s and 1960s, many civil defense practices took place to prepare for the aftermath of a nuclear war, which seemed quite likely at that time.
In the UK, the Civil Defence Service was disbanded in 1945, followed by the ARP in 1946. With the onset of the growing tensions between East and West, the service was revived in 1949 as the Civil Defence Corps. As a civilian volunteer organisation, it was tasked take control in the aftermath of a major national emergency, principally envisaged as being a Cold War nuclear attack. Although under the authority of the Home Office, with a centralised administrative establishment, the corps was administered locally by Corps Authorities. In general every county was a Corps Authority, as were most county boroughs in England and Wales and large burghs in Scotland.
Each division was divided into several sections, including the Headquarters, Intelligence and Operations, Scientific and Reconnaissance, Warden & Rescue, Ambulance and First Aid and Welfare.
In the United States, the sheer power of nuclear weapons and the perceived likelihood of such an attack, precipitated a greater response than had yet been required of civil defense. Civil defense, something previously considered an important and common sense step, also became divisive and controversial in the charged atmosphere of the Cold War. In 1950, the National Security Resources Board created a 162 page document outlining a model civil defense structure for the U.S. Called the "Blue Book" by civil defense professionals in reference to its solid blue cover, it was the template for legislation and organization that occurred over the next 40 years.
Perhaps the most memorable aspect of the Cold War civil defense effort was the educational effort made or promoted by the government. In "Duck and Cover", Bert the Turtle advocated that children "duck and cover" when they "see the flash." Booklets were also common place such as "Survival Under Atomic Attack", "Fallout Protection" and "Nuclear War Survival Skills". The transcribed radio program Stars for Defense combined hit music with civil defense advice. Public service announcements including children's songs were created by government institutes and then distributed and released by radio stations to educate the public in case of nuclear attack.
President Kennedy launched an ambitious effort to install fallout shelters throughout the United States. These shelters would not protect against the blast and heat effects of nuclear weapons, but would provide some protection against the radiation effects that would last for weeks and even affect areas distant from a nuclear explosion. In order for most of these preparations to be effective, there had to be some degree of warning. In 1951, Control of Electromagnetic Radiation was established. Under the system, a few primary stations would be alerted of an emergency and would broadcast an alert. All broadcast stations throughout the country would be constantly listening to an upstream station and repeat the message, thus passing it from station to station.
Contrary to the approach taken in the West, military strategy in the USSR held that winning a nuclear war was possible. To this effect the Soviets planned to minimize, as far as possible, the effects of nuclear weapon strikes on its territory and therefore spent considerably more thought on civil defense preparations than in the U.S., with defense plans that have been assessed to be far more effective than those in the U.S.
Soviet Civil Defense Troops played the main role in the massive disaster relief operation following the 1986 Chernobyl nuclear accident. Defense Troops reservists were officially mobilized (as in a case of war) from throughout the USSR to join the Chernobyl task force formed on basis of the Kiev Civil Defense Brigade. The task force performed some high-risk tasks, including manual removal of lethally-radioactive debris.
Decline.
In Western countries, strong civil defense policies were never properly implemented, because it was fundamentally at odds, with the doctrine of "mutual assured destruction" (MAD) by making provisions for survivors. It was also considered that a fully fledged total defense would have not been worth the very large expense. For whatever reason, the public saw efforts at civil defense as fundamentally ineffective against the powerful destructive forces of nuclear weapons, and therefore a waste of time and money. Detailed scientific research programmes did lay behind the much-mocked government civil defence pamphlets of the 1950s and 1960s.
Governments in most Western countries, with the sole exception of Switzerland, generally sought to underfund Civil Defense due to its perceived pointlessness. Nevertheless effective, but commonly dismissed civil defense measures against nuclear attack were implemented, in the face of popular apathy and scepticism of authority. After the end of the Cold War, the focus moved from defense against nuclear war to defense against a terrorist attack possibly involving chemical or biological weapons.
The Civil Defence Corps was stood down in Great Britain in 1968 with the tacit realization that nothing practical could be done in the event of an unrestricted nuclear attack. However, two Civil Defence Corps still operate within the British Isles, namely the Isle of Man Civil Defence Corps and Civil Defence Ireland (Republic of Ireland).
In the United States, the various civil defense agencies were replaced with the Federal Emergency Management Agency (FEMA) in 1979. In 2002 this became part of the Department of Homeland Security. The focus was shifted from nuclear war to an "all-hazards" approach of Comprehensive Emergency Management. Natural disasters and the emergence of new threats such as terrorism have caused attention to be focused away from traditional civil defense and into new forms of civil protection such as emergency management and homeland security.
Today.
Many countries still maintain a national Civil Defence Corps, usually having a wide brief for assisting in large scale civil emergencies such as flood, earthquake, invasion, or civil disorder.
After the September 11 attacks in 2001, in the United States the concept of civil defense has been revisited under the umbrella term of homeland security and all-hazards emergency management.
In Europe, the triangle CD logo continues to be widely used. The old U.S. civil defense logo was used in the FEMA logo until 2006 and is hinted at in the United States Civil Air Patrol logo. Created in 1939 by Charles Coiner of the N. W. Ayer Advertising Agency, it was used throughout World War II and the Cold War era. In 2006, the National Emergency Management Association—a U.S. organisation made up of state emergency managers—"officially" retired the Civil Defense triangle logo, replacing it with a stylised EM (standing for Emergency management). The name and logo, however, continue to be used by Hawaii State Civil Defense and Guam Homeland Security/Office of Civil Defense.
The term "civil protection" is currently widely used within the European Union to refer to government-approved systems and resources tasked with protecting the non-combat population, primarily in the event of natural and technological disasters. In recent years there has been emphasis on preparedness for technological disasters resulting from terrorist attack. Within EU countries the term "crisis-management" emphasises the political and security dimension rather than measures to satisfy the immediate needs of the population.
In Australia, civil defence is the responsibility of the volunteer-based State Emergency Service.
In Russia, and most former Soviet countries, civil defence is the responsibility of Governmental Ministry, Like the Ministry of Emergency Situations in Russia.
Importance.
Relatively small investments in preparation can speed up recovery by months or years and thereby prevent millions of deaths by hunger, cold and disease. According to human capital theory in economics, a country's population is more valuable than all of the land, factories and other assets that it possesses. People rebuild a country after its destruction, and it is therefore important for the economic security of a country that it protect its people. According to psychology, it is important for people to feel like they are in control of their own destiny, and preparing for uncertainty via civil defense may help to achieve this.
In the United States, the federal civil defense program was authorised by statute and ran from 1951 to 1994. Originally authorised by Public Law 920 of the 81st Congress it was repealed by Public Law 93-337 in 1994. Small portions of that statutory scheme were incorporated into the Robert T. Stafford Disaster Relief and Emergency Assistance Act (Public Law 100-707) which superseded in part, amended in part, and supplemented in part the Disaster Relief Act of 1974 (Public Law 93-288). In the portions of the civil defense statute incorporated into the Stafford Disaster Relief and Emergency Assistance Act the primary modification was to use the term "Emergency Preparedness" wherever the term "Civil Defence" previously appeared in the statutory language.
An important concept initiated by President Jimmy Carter was the so-called "Crisis Relocation Program" administered as part of the federal civil defense program. That effort largely lapsed under President Ronald Reagan who discontinued the Carter initiative because of opposition from areas potentially hosting the relocated population. SEE Presidential Review Memorandum/NSC-32 (September 30, 1977) and Presidential Decision Memorandum 42 .
Threat assessment.
Threats to civilians and civilian life include NBC (Nuclear, Biological, and Chemical warfare) and others, like the more modern term CBRN, Chemical Biological Radiological and Nuclear. Threat assessment involves studying each threat so that preventative measures can be built into civilian life.
This would be conventional explosives. A blast shelter designed to protect only from radiation and fall-out, however, would be much more vulnerable to conventional explosives,also see fallout shelter.
Shelter intended to protect against nuclear blast effects would include thick concrete and other sturdy elements which are resistant to conventional explosives. The biggest threats from a nuclear attack are effects from the blast, fires and radiation. One of the most prepared countries for a nuclear attack is Switzerland. Almost every building in Switzerland has an "abri" (shelter) against the initial nuclear bomb and explosion followed by the fall-out. Because of this, many people use it as a safe to protect valuables, photos, financial information and so on. Switzerland also has air-raid and nuclear-raid sirens in every village.
A "radiologically enhanced weapon," or "dirty bomb" uses an explosive to spread radioactive material. This is a theoretical risk, and such weapons have not been used by terrorists. Depending on the quantity of the radioactive material, the dangers may be mainly psychological. Toxic effects can be managed by standard hazmat techniques.
The threat here is primarily from disease-causing microorganisms such as bacteria and viruses.
Various chemical agents are a threat such as nerve gas (VX, Sarin, and so on.).
Stages.
Mitigation.
Mitigation is the process of actively preventing the war or the release of nuclear weapons. It includes policy analysis, diplomacy, political measures, nuclear disarmament and more military responses such as a National Missile Defense and air defense artillery. In the case of counter-terrorism, mitigation would include diplomacy, intelligence gathering and direct action against terrorist groups. Mitigation may also be reflected in long-term planning such as the design of the interstate highway system and the placement of military bases further away from populated areas.
Preparation.
Preparation consists of building blast shelters, and pre-positioning information, supplies and emergency infrastructure. For example, most larger cities in the U.S. now have underground emergency operations centres that can perform civil defense coordination. FEMA also has many underground facilities located near major railheads such as the one in Denton, Texas and Mount Weather, Virginia for the same purpose.
Other measures would include continuous government inventories of grain silos, the Strategic National Stockpile, the uncapping of the Strategic Petroleum Reserve, the dispersal of lorry-transportable bridges, water purification, mobile refineries, mobile de-contamination facilities, mobile general and special purpose disaster mortuary facilities such as Disaster Mortuary Operational Response Team (DMORT) and DMORT-WMD, and other aids such as temporary housing to speed civil recovery.
On an individual scale, one means of preparation for exposure to nuclear fallout is to obtain potassium iodide (KI) tablets as a safety measure to protect the human thyroid gland from the uptake of dangerous radioactive iodine. Another measure is to cover the nose, mouth and eyes with a piece of cloth and sunglasses to protect against alpha particles, which are only an internal hazard.
To support and supplement efforts at national, regional and local level with regard to disaster prevention, the preparedness of those responsible for civil protection and the intervention in the event of disaster
Preparing also includes sharing information:
Response.
Response consists first of warning civilians so they can enter Fallout Shelters and protect assets.
Staffing a response is always full of problems in a civil defense emergency. After an attack, conventional full-time emergency services are dramatically overloaded, with conventional fire fighting response times often exceeding several days. Some capability is maintained by local and state agencies, and an emergency reserve is provided by specialised military units, especially civil affairs, Military Police, Judge Advocates and combat engineers.
However, the traditional response to massed attack on civilian population centres is to maintain a mass-trained force of volunteer emergency workers. Studies in World War II showed that lightly trained (40 hours or less) civilians in organised teams can perform up to 95% of emergency activities when trained, liaised and supported by local government. In this plan, the populace rescues itself from most situations, and provides information to a central office to prioritize professional emergency services.
In the 1990s, this concept was revived by the Los Angeles Fire Department to cope with civil emergencies such as earthquakes. The program was widely adopted, providing standard terms for organization. In the U.S., this is now official federal policy, and it is implemented by community emergency response teams, under the Department of Homeland Security, which certifies training programmes by local governments, and registers "certified disaster service workers" who complete such training.
Recovery.
Recovery consists of rebuilding damaged infrastructure, buildings and production. The recovery phase is the longest and ultimately most expensive phase. Once the immediate "crisis" has passed, cooperation fades away and recovery efforts are often politicised or seen as economic opportunities.
Preparation for recovery can be very helpful. If mitigating resources are dispersed before the attack, cascades of social failures can be prevented. One hedge against bridge damage in riverine cities is to subsidise a "tourist ferry" that performs scenic cruises on the river. When a bridge is down, the ferry takes up the load.
Implementation.
Some advocates believe that government should change building codes to require autonomous buildings in order to reduce civil societies' dependence on complex, fragile networks of social services.
An example of a crucial need after a general nuclear attack would be the fuel required to transport every other item for recovery. However, oil refineries are large, immobile, and probable targets. One proposal is to preposition truck-mounted fuel refineries near oil fields and bulk storage depots. Other critical infrastructure needs would include road and bridge repair, communications, electric power, food production, and potable water.
Civil defense organizations.
Civil Defense is also the name of a number of organizations around the world dedicated to protecting civilians from military attacks, as well as to providing rescue services after natural and human-made disasters alike.
Worldwide is managed by the UN United Nations Office for the Coordination of Humanitarian Affairs (OCHA).
In a few countries such as Jordan and Singapore (see Singapore Civil Defence Force), civil defense is essentially the same organisation as the fire brigade. In most countries however, civil defense is a government-managed, volunteer-staffed organisation, separate from the fire brigade and the ambulance service.
As the threat of Cold War eased, a number of such civil defense organisations have been disbanded or mothballed (as in the case of the Royal Observer Corps in the United Kingdom and the United States civil defense), while others have changed their focuses into providing rescue services after natural disasters (as for the State Emergency Service in Australia). However the ideals of Civil Defense have been brought back in the United States under FEMA's Citizen Corps and Community Emergency Response Team (CERT).
In the United Kingdom Civil Defence work is carried out by Emergency Responders under the Civil Contingencies Act 2004, with assistance from voluntary groups such as RAYNET, Search and Rescue Teams and 4x4 Response. In Ireland, the Civil Defence is still very much an active organisation and is occasionally called upon for its Auxiliary Fire Service and ambulance/rescue services when emergencies such as flash flooding occur and require additional manpower. The organisation has units of trained firemen and medical responders based in key areas around the country.
By country.
UK:
US:
See also.
General:

</doc>
<doc id="7060" url="http://en.wikipedia.org/wiki?curid=7060" title="Chymotrypsin">
Chymotrypsin

Chymotrypsin (, "chymotrypsins A and B", "alpha-chymar ophth", "avazyme", "chymar", "chymotest", "enzeon", "quimar", "quimotrase", "alpha-chymar", "alpha-chymotrypsin A", "alpha-chymotrypsin") is a digestive enzyme component of pancreatic juice acting in the duodenum where it performs proteolysis, the breakdown of proteins and polypeptides. Chymotrypsin preferentially cleaves peptide amide bonds where the carboxyl side of the amide bond (the P1 position) is a large hydrophobic amino acid (tyrosine, tryptophan, and phenylalanine). These amino acids contain an aromatic ring in their sidechain that fits into a 'hydrophobic pocket' (the S1 position) of the enzyme. It is activated in the presence of trypsin. The hydrophobic and shape complementarity between the peptide substrate P1 sidechain and the enzyme S1 binding cavity accounts for the substrate specificity of this enzyme. Chymotrypsin also hydrolyzes other amide bonds in peptides at slower rates, particularly those containing leucine and methionine at the P1 position.
Structurally, it is the archetypal structure for its superfamily, the PA clan of proteases.
Activation.
Chymotrypsin is synthesized in the pancreas by protein biosynthesis as a precursor called chymotrypsinogen that is enzymatically inactive. On cleavage by trypsin into two parts that are still connected via an S-S bond, cleaved chymotrypsinogen molecules can activate each other by removing two small peptides in a "trans"-proteolysis. The resulting molecule is active chymotrypsin, a three-polypeptide molecule interconnected via disulfide bonds.
Mechanism of action and kinetics.
"In vivo", chymotrypsin is a proteolytic enzyme (Serine protease) acting in the digestive systems of many organisms. It facilitates the cleavage of peptide bonds by a hydrolysis reaction, which despite being thermodynamically favorable occurs extremely slowly in the absence of a catalyst. The main substrates of chymotrypsin include tryptophan, tyrosine, phenylalanine, leucine, and methionine, which are cleaved at the carboxyl terminal. Like many proteases, chymotrypsin will also hydrolyse amide bonds "in vitro", a virtue that enabled the use of substrate analogs such as N-acetyl-L-phenylalanine p-nitrophenyl amide for enzyme assays.
Chymotrypsin cleaves peptide bonds by attacking the unreactive carbonyl group with a powerful nucleophile, the serine 195 residue located in the active site of the enzyme, which briefly becomes covalently bonded to the substrate, forming an enzyme-substrate intermediate. Along with histidine 57 and aspartic acid 102, this serine residue constitutes the catalytic triad of the active site.
These findings rely on inhibition assays and the study of the kinetics of cleavage of the aforementioned substrate, exploiting the fact that the enzyme-substrate intermediate "p"-nitrophenolate has a yellow colour, enabling us to measure its concentration by measuring light absorbance at 410 nm.
It was found that the reaction of chymotrypsin with its substrate takes place in two stages, an initial “burst” phase at the beginning of the reaction and a steady-state phase following Michaelis-Menten kinetics. It is also called "ping-pong" mechanism. The mode of action of chymotrypsin explains this as hydrolysis takes place in two steps. First acylation of the substrate to form an acyl-enzyme intermediate and then deacylation in order to return the enzyme to its original state. This occurs via the concerted action of the three amino acid residues in the catalytic triad. Aspartate hydrogen bonds to the N-δ hydrogen of histidine, increasing the pKa of its ε nitrogen and thus making it able to deprotonate serine. It is this deprotonation that allows the serine side chain to act as a nucleophile and bind to the electron-deficient carbonyl carbon of the protein main chain. Ionization of the carbonyl oxygen is stabilized by formation of two hydrogen bonds to adjacent main chain N-hydrogens. This occurs in the oxyanion hole. This forms a tetrahedral adduct and breakage of the peptide bond. An acyl-enzyme intermediate, bound to the serine, is formed, and the newly formed amino terminus of the cleaved protein can dissociate. In the second reaction step, a water molecule is activated by the basic histidine, and acts as a nucleophile. The oxygen of water attacks the carbonyl carbon of the serine-bound acyl group, resulting in formation of a second tetrahedral adduct, regeneration of the serine -OH group, and release of a proton, as well as the protein fragment with the newly formed carboxyl terminus 
See also.
Trypsin
PA clan of proteases

</doc>
<doc id="7061" url="http://en.wikipedia.org/wiki?curid=7061" title="Community emergency response team">
Community emergency response team

In the United States a Community Emergency Response Team (CERT) can refer to
Sometimes programs and organizations take different names, such as Neighborhood Emergency Response Team (NERT), or Neighborhood Emergency Team (NET).
The concept of civilian auxiliaries is similar to civil defense, which has a longer history. The CERT concept differs because it includes nonmilitary emergencies, and is coordinated with all levels of emergency authorities, local to national, via an overarching incident command system.
History.
The concept of widespread local volunteer emergency responders was implemented and developed by the Los Angeles Fire Department in 1985. The 1987 Whittier Narrows earthquake showed the need for preparing citizens to take care of themselves and their loved ones after a disaster.
In the 1989 Loma Prieta earthquake, residents of San Francisco's Marina District help run lengths of fire hose from a fireboat to firefighters ashore after the hydrant system failed. Later, the Fire Department worked with the community to form the City's NERT program (Neighborhood Emergency Response Team).
By 1993, the Federal Emergency Management Agency had made the program available nationwide; by 2012, CERT programs were offered in all 50 states, the District of Columbia, Puerto Rico and the Northern Mariana Islands.
CERT and Citizen Corps were transferred to the Office of Domestic Preparedness (now the Office of Grants and Training) in August 2004.
CERT Organization.
A local government agency, often a fire department or emergency management agency, agrees to sponsor CERT within its jurisdiction. The sponsoring agency liases with, deploys and may train or supervise the training of CERT members. The sponsoring agency receives and disburses federal and state Citizen Corps grant funds allocated to its CERT program. Many sponsoring agencies employ a full-time community-service person as liaison to the CERT members. In some communities, the liaison is a volunteer and CERT member. 
As people are trained and agree to join the community emergency response effort, a CERT is formed. Initial efforts may result in a team with only a few members from across the community. As the number of members grow, a single community-wide team may subdivide. Multiple CERTs are organized into a hierarchy of teams consistent with ICS principles. This follows the Incident Command System (ICS) principle of Span of control until the ideal distribution is achieved: one or more teams are formed at each neighborhood within a community.
A Teen Community Emergency Response Team (TEEN CERT), or Student Emergency Response Team (SERT), can be formed as a school club, service organization, Venturing Crew, Explorer Post, or added to a school's graduation curriculum. Some CERTs form a club or service corporation, and recruit volunteers to perform training on behalf of the sponsoring agency. This reduces the financial and human resource burden on the sponsoring agency.
When not responding to disasters, CERTs may
Some sponsoring agencies use Citizen Corps grant funds to purchase response tools and equipment for their members and team(s) (subject to Stafford Act limitations). Most CERTs also acquire their own supplies, tools, and equipment. As community members, CERTs are aware of the specific needs of their community and equip the teams accordingly.
CERT Response.
The basic idea is to use CERT to perform the large number of tasks needed in emergencies. This frees highly trained professional responders for more technical tasks. Much of CERT training concerns the Incident Command System and organization, so CERT members fit easily into larger command structures.
A team may self-activate (self-deploy) when their own neighborhood is affected by disaster. An effort is made to report their response status to the sponsoring agency. A self-activated team will size-up the loss in their neighborhood and begin performing the skills they have learned to minimize further loss of life, property, and environment. They will continue to respond safely until redirected or relieved by the sponsoring agency or professional responders on-scene.
Teams in neighborhoods not affected by disaster may be deployed or activated by the sponsoring agency. The sponsoring agency may communicate with neighborhood CERT leaders through an organic communication team. In some areas the communications may be by amateur radio, FRS, GMRS or MURS radio, dedicated telephone or fire-alarm networks. In other areas, relays of bicycle-equipped runners can effectively carry messages between the teams and the local emergency operations center.
The sponsoring agency may activate and dispatch teams in order to gather or respond to intelligence about an incident. Teams may be dispatched to affected neighborhoods, or organized to support operations. CERT members may augment support staff at an Incident Command Post or Emergency Operations Center. Additional teams may also be created to guard a morgue, locate supplies and food, convey messages to and from other CERT teams and local authorities, and other duties on an as-needed basis as identified by the team leader.
In the short term, CERTs perform data gathering, especially to locate mass-casualties requiring professional response, or situations requiring professional rescues, simple fire-fighting tasks (for example, small fires, turning off gas), light search and rescue, damage evaluation of structures, triage and first aid. In the longer term, CERTs may assist in the evacuation of residents, or assist with setting up a neighborhood shelter.
While responding, CERT members are temporary volunteer government workers. In some areas, (such as California, Hawaii and Kansas) registered, activated CERT members are eligible for worker's compensation for on-the-job injuries during declared disasters.
CERT Team Member Roles.
The Federal Emergency Management Agency (FEMA) recommends that the standard, ten-person team be comprised as follows:
Because every CERT member in a community receives the same core instruction, any team member has the training necessary to assume any of these roles. This is important during a disaster response because not all members of a regular team may be available to respond. Hasty teams may be formed by whichever members are responding at the time. Additionally, members may need to adjust team roles due to stress, fatigue, injury, or other circumstances.
CERT Training.
While state and local jurisdictions will implement training in the manner that best suits the community, the Citizen Corps CERT program has an established curriculum. Jurisdictions may augment the training, but are strongly encouraged to deliver the entire core content. The Citizen Corps CERT core curriculum for the basic course is composed of the following nine units (time is instructional hours):
Citizen Corps CERT training emphasizes safely "doing the most good for the most people as quickly as possible" when responding to a disaster. For this reason, cardiopulmonary resuscitation (CPR) training is not included in the core curriculum, as it is time and responder intensive in a mass-casualty incident. However, many jurisdictions encourage or require CERT members to obtain CPR training. Many CERT programs provide or encourage members to take additional first aid training. Some CERT members may also take training to become a certified first responder or emergency medical technician.
Each unit of Citizen Corps CERT training is ideally delivered by professional responders or other experts in the field addressed by the unit. This is done to help build unity between CERT members and responders, keep the attention of students, and help the professional response organizations be comfortable with the training which CERT members receive. 
Each course of instruction is ideally facilitated by one or more instructors certified in the CERT curriculum by the state or sponsoring agency. Facilitating instructors provide continuity between units, and help ensure that the CERT core curriculum is being delivered successfully. Facilitating instructors also perform set-up and tear-down of the classroom, provide instructional materials for the course, record student attendance and other tasks which assist the professional responder in delivering their unit as efficiently as possible.
Citizen Corps CERT training is provided free to interested members of the community, and is delivered in a group classroom setting. People may complete the training without obligation to join a CERT. Citizen Corps grant funds can be used to print and provide each student with a printed manual. Some sponsoring agencies use Citizen Corps grant funds to purchase disaster response tool kits. These kits are offered as an incentive to join a CERT, and must be returned to the sponsoring agency when members resign from CERT.
Because uniformed volunteer disaster responders are accorded a higher level of trust than unaffiliated volunteers when responding in a disaster, many sponsoring agencies require a criminal background-check of all trainees before allowing them to participate on a CERT. 
The Citizen Corps CERT curriculum (including the Train-the-Trainer course) was updated during the last half of 2008 to reflect feedback from instructors across the nation. The update is in final review, and is scheduled for release during the first quarter of 2009.
More information: 

</doc>
<doc id="7063" url="http://en.wikipedia.org/wiki?curid=7063" title="Catapult">
Catapult

A catapult is a ballistic device used to launch a projectile a great distance without the aid of explosive devices—particularly various types of ancient and medieval siege engines. Although the catapult has been used since ancient times, it has proven to be one of the most effective mechanisms during warfare. The word 'catapult' comes from the Latin 'catapulta', which in turn comes from the Greek καταπέλτης ("katapeltēs"), itself from ("kata"), "downwards" + πάλλω ("pallō"), "to toss, to hurl". Catapults were invented by the ancient Greeks.
Greek and Roman catapults.
The catapult and crossbow in Greece are closely intertwined. Primitive catapults were essentially “the product of relatively straightforward attempts to increase the range and penetrating power of missiles by strengthening the bow which propelled them”. The historian Diodorus Siculus (fl. 1st century BC), described the invention of a mechanical arrow-firing catapult ("katapeltikon") by a Greek task force in 399 BC. The weapon was soon after employed against Motya (397 BC), a key Carthaginian stronghold in Sicily. Diodorus is assumed to have drawn his description from the highly rated history of Philistus, a contemporary of the events then. The introduction of crossbows however, can be dated further back: according to the inventor Hero of Alexandria (fl. 1st century AD), who referred to the now lost works of the 3rd-century BC engineer Ctesibius, this weapon was inspired by an earlier foot-held crossbow, called the "gastraphetes", which could store more energy than the Greek bows. A detailed description of the "gastraphetes", or the “belly-bow”, along with a watercolor drawing, is found in Heron's technical treatise "Belopoeica".
A third Greek author, Biton (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the "gastraphetes", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421 BC and 401 BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.
Philo of Byzantium provides probably the most detailed account on the establishment of a theory of belopoietics (“belos” = projectile; “poietike” = (art) of making) circa 200 BC. The central principle to this theory was that “all parts of a catapult, including the weight or length of the projectile, were proportional to the size of the torsion springs”. This kind of innovation is indicative of the increasing rate at which geometry and physics were being assimilated into military enterprises.
From the mid-4th century BC onwards, evidence of the Greek use of arrow-shooting machines becomes more dense and varied: arrow firing machines ("katapaltai") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350 BC. An extant inscription from the Athenian arsenal, dated between 338 and 326 BC, lists a number of stored catapults with shooting bolts of varying size and springs of sinews. The later entry is particularly noteworthy as it constitutes the first clear evidence for the switch to torsion catapults which are more powerful than the flexible crossbows and came to dominate Greek and Roman artillery design thereafter. This move to torsion springs was likely spurred by the engineers of Philip II of Macedonia. Another Athenian inventory from 330 to 329 BC includes catapult bolts with heads and flights. As the use of catapults became more commonplace, so did the training required to operate them. Many Greek children were instructed in catapult usage, as evidenced by “a 3rd Century B.C. inscription from the island of Ceos in the Cyclades [regulating] catapult shooting competitions for the young”. Arrow firing machines in action are reported from Philip II's siege of Perinth (Thrace) in 340 BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, which could have been used to house anti-personnel arrow shooters, as in Aigosthena. Projectiles included both arrows and (later) stones that were sometimes lit on fire. Onomarchus of Phocis first used catapults on the battlefield against Philip II of Macedon. Philip's son, Alexander the Great, was the next commander in recorded history to make such use of catapults on the battlefield as well as to use them during sieges.
The Romans started to use catapults as arms for their wars against Syracuse, Macedon, Sparta and Aetolia (3rd and 2nd centuries BC). The Roman machine known as an arcuballista was similar to a large crossbow. Later the Romans used ballista catapults on their warships.
Other ancient catapults.
Ajatshatru is recorded in Jaina texts as having used a catapult in his campaign against the Licchavis.
Medieval catapults.
Castles and fortified walled cities were common during this period – and catapults were used as a key siege weapon against them. As well as attempting to breach the walls, incendiary missiles could be thrown inside—or early biological warfare attempted with diseased carcasses or putrid garbage catapulted over the walls.
Defensive techniques in the Middle Ages progressed to a point that rendered catapults ineffective for the most part. The Viking siege of Paris (885–6 A.D.) “saw the employment by both sides of virtually every instrument of siege craft known to the classical world, including a variety of catapults,” to little effect, resulting in failure.
The most widely used catapults throughout the Middle Ages were as follows:
Modern use.
The last large scale military use of catapults was during the trench warfare of World War I. During the early stages of the war, catapults were used to throw hand grenades across no man's land into enemy trenches. They were eventually replaced by small mortars.
Special variants called "aircraft catapults" are used to launch planes from land bases and sea carriers when the takeoff runway is too short for a powered takeoff or simply impractical to extend. Ships also use them to launch torpedoes and deploy bombs against submarines. Small catapults, referred to as "traps", are still widely used to launch clay targets into the air in the sport of clay pigeon shooting.
Until recently, catapults were used by thrill-seekers to experience being catapulted through the air. The practice has been discontinued due to fatalities, when the participants failed to land onto the safety net.
"Pumpkin chunking" is another widely popularized use, in which people compete to see who can launch a pumpkin the farthest by mechanical means (although the world record is held by a pneumatic air cannon).
In January 2011, PopSci.com, the news blog version of "Popular Science" magazine, reported that a group of smugglers used a homemade catapult to deliver marijuana into the United States from Mexico. The machine was found 20 feet from the border fence with bales of marijuana ready to launch.
Models.
In the US, catapults of all types and sizes are being built for school science and history fairs, competitions or as a hobby. Catapult projects can inspire students to study different subjects including physics, engineering, science, math and history. These kits can be purchased from Renaissance Fairs, or from several online stores.

</doc>
<doc id="7066" url="http://en.wikipedia.org/wiki?curid=7066" title="Cinquain">
Cinquain

Cinquain is a class of poetic forms that employ a 5-line pattern. Earlier used to describe any five-line form, it now refers to one of several forms that are defined by specific rules and guidelines.
American cinquain.
The modern form, known as American Cinquain inspired by Japanese haiku and tanka, akin in spirit to that of the Imagists.
In her 1915 collection titled "Verse", published one year after her death, Crapsey included 28 cinquains.
Crapsey's American Cinquain form developed in two stages. The first, fundamental form is a stanza of five lines of accentual verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses. Then Crapsey decided to make the criterion a stanza of five lines of accentual-syllabic verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses and 2, 4, 6, 8, and 2 syllables. Iambic feet were meant to be the standard for the cinquain, which made the dual criteria match perfectly. Some resource materials define classic cinquains as solely iambic, but that is not necessarily so. In contrast to the Eastern forms upon which she based them, Crapsey always titled her cinquains, effectively utilizing the title as a sixth line. Crapsey's cinquain depends on strict structure and intense physical imagery to communicate a mood or feeling.
The form is illustrated by Crapsey's "November Night":
Listen... <br>
With faint dry sound, <br>
Like steps of passing ghosts, <br>
The leaves, frost-crisp'd, break from the trees <br>
And fall.
The Scottish poet William Soutar also wrote over one hundred American Cinquains (he labelled them Epigrams) between 1933 and 1940.
Cinquain variations.
The Crapsey cinquain has subsequently seen a number of variations by modern poets, including:
Didactic cinquain.
The didactic cinquain is closely related to the Crapsey cinquain. It is an informal cinquain widely taught in elementary schools and has been featured in, and popularized by, children's media resources, including Junie B. Jones and PBS Kids. This form is also embraced by young adults and older poets for its expressive simplicity. The prescriptions of this type of cinquain refer to word count, not syllables and stresses. Ordinarily, the first line is a one-word title, the subject of the poem; the second line is a pair of adjectives describing that title; the third line is a three-word phrase that gives more information about the subject (often a list of three gerunds); the fourth line consists of four words describing feelings related to that subject; and the fifth line is a single word synonym or other reference for the subject from line one.
For example:
Snow
Silent, white
Dancing, falling, drifting
Covering everything it touches
Blanket

</doc>
<doc id="7067" url="http://en.wikipedia.org/wiki?curid=7067" title="Cook Islands">
Cook Islands

The Cook Islands (; Cook Islands Māori: "Kūki 'Āirani") is an island country in the South Pacific Ocean in free association with New Zealand. It comprises 15 small islands whose total land area is . The Cook Islands' Exclusive Economic Zone (EEZ), however, covers of ocean.
The Cook Islands' defence and foreign affairs are the responsibility of New Zealand, which is exercised in consultation with the Cook Islands. In recent times, the Cook Islands have adopted an increasingly independent foreign policy. Although Cook Islanders are citizens of New Zealand, they have the status of Cook Islands nationals, which is not given to other New Zealand citizens.
The Cook Islands' main population centres are on the island of Rarotonga (14,153 in 2006), where there is an international airport. There is a much larger population of Cook Islanders in New Zealand, particularly the North Island. In the 2006 census, 58,008 self-identified as being of ethnic Cook Islands Māori descent.
With about 100,000 visitors travelling to the islands in the 2010–11 financial year, tourism is the country's main industry, and the leading element of the economy, far ahead of offshore banking, pearls, and marine and fruit exports.
Geography.
The Cook Islands are in the South Pacific Ocean, northeast of New Zealand, between French Polynesia and American Samoa. There are 15 major islands spread over 2.2 million km2 of ocean, divided into two distinct groups: the Southern Cook Islands and the Northern Cook Islands of coral atolls.
The islands were formed by volcanic activity; the northern group is older and consists of six atolls, which are sunken volcanoes topped by coral growth. The climate is moderate to tropical.
The 15 islands and two reefs are grouped as follows:
History.
The Cook Islands were first settled in the 6th century CE by Polynesian people who migrated from Tahiti, an island 1154 km to the northeast of Cook Islands.
Spanish ships visited the islands in the 16th century; the first written record of contact with the islands came with the sighting of Pukapuka by Spanish sailor Álvaro de Mendaña de Neira in 1595 who called it "San Bernardo" (Saint Bernard). Pedro Fernandes de Queirós, a Portuguese captain working for the Spanish crown, made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling it "Gente Hermosa" (Beautiful People).
British navigator Captain James Cook arrived in 1773 and 1777 and named the islands the "Hervey Islands"; the name "Cook Islands", in honour of Cook, appeared on a Russian naval chart published in the 1820s.
In 1813 John Williams, a missionary on the "Endeavour" (not the same ship as Cook's) made the first recorded sighting of Rarotonga. The first recorded landing on Rarotonga by Europeans was in 1814 by the "Cumberland"; trouble broke out between the sailors and the Islanders and many were killed on both sides. The islands saw no more Europeans until missionaries arrived from England in 1821. Christianity quickly took hold in the culture and many islanders continue to be Christian believers today.
The Cook Islands became a British protectorate in 1888, due largely to community fears that France might occupy the territory as it had Tahiti. In 1901 the New Zealand Government decided to annexe the country despite opposition from the country's traditional chiefs. As many of the islands were independent and ruled by local chiefs, the Cook Islands had no federal statutory law to decide the constitutional constraints regarding whether to agree to the country's annexation. When the British Nationality and New Zealand Citizenship Act 1948 came into effect on 1 January 1949, Cook Islanders who were British subjects gained New Zealand citizenship. The country remained a New Zealand protectorate until 1965, when the New Zealand Government decided to offer self-governing status to its colony. In that year, Albert Henry of the Cook Islands Party was elected as the first Prime Minister. Henry led the country until he was accused of vote-rigging. He was succeeded in 1978 by Tom Davis of the Democratic Party.
Politics and foreign relations.
The Cook Islands is a representative democracy with a parliamentary system in an associated state relationship with New Zealand. Executive power is exercised by the government, with the Chief Minister as head of government. Legislative power is vested in both the government and the Parliament of the Cook Islands. There is a pluriform multi-party system. The Judiciary is independent of the executive and the legislature. The Head of State is the Queen of New Zealand, who is represented in the Cook Islands by the Queen's Representative.
The islands are self-governing in "free association" with New Zealand. New Zealand retains primary responsibility for external affairs, with consultation with the Cook Islands government. Cook Islands nationals are citizens of New Zealand and can receive New Zealand government services, but the reverse is not true; New Zealand citizens are not Cook Islands nationals. Despite this, as of 2013, the Cook Islands had diplomatic relations in its own name with 41 other countries. The Cook Islands is not a United Nations member state, but, along with Niue, has had their "full treaty-making capacity" recognised by United Nations Secretariat, and is a full member of the WHO and UNESCO UN specialised agencies, is an associate member of the Economic and Social Commission for Asia and the Pacific (UNESCAP) and a Member of the Assembly of States of the International Criminal Court.
On 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing any United States claim to the islands of Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990 the Cook Islands and France signed a treaty that delimited the boundary between the Cook Islands and French Polynesia. As competition between the US and China heated up in the South China Sea and other areas closer to the mainland, even the far off Cook Islands began to feel the results. In late August 2012, for instance, United States Secretary of State Hillary Clinton visited the islands.
Administrative subdivisions.
There are Island Councils on all of the inhabited outer islands (Outer Islands Local Government Act 1987 with amendments up to 2004, and Palmerston Island Local Government Act 1993) except Nassau, which is governed by Pukapuka (Suwarrow, with only one caretaker living on the island, also governed by Pukapuka, is not counted with the inhabited islands in this context). Each council is headed by a Mayor.
The three "Vaka" councils of Rarotonga established in 1997 ("Rarotonga Local Government Act 1997"), also headed by mayors, were abolished in February 2008, despite much controversy.
On the lowest level, there are village committees. Nassau, which is governed by Pukapuka, has an island committee (Nassau Island Committee), which advises the Pukapuka Island Council on matters concerning its own island.
Economy.
The economy is strongly affected by geography. It is isolated from foreign markets, and has some inadequate infrastructure; it lacks major natural resources, has limited manufacturing and suffers moderately from natural disasters. Tourism provides the economic base which makes up approximately 67.5% of GDP.
Additionally, the economy is supported by foreign aid, largely from New Zealand. The Peoples' Republic of China has also contributed foreign aid which has resulted in, among other projects, the Police Headquarters building. The Cook Islands is expanding its agriculture, mining and fishing sectors, with varying success.
Since approximately 1989, the Cook Islands have become a location specialising in so-called asset protection trusts, by which investors shelter assets from the reach of creditors and legal authorities. According to "The New York Times", the Cooks have "laws devised to protect foreigners' assets from legal claims in their home countries" which were apparently crafted specifically to thwart the long arm of American justice; creditors must travel to the Cook Islands and argue their cases under Cooks law, often at prohibitive expense. Unlike other foreign jurisdictions such as the British Virgin Islands, the Cayman Islands and Switzerland, the Cooks "generally disregard foreign court orders" and do not require that bank accounts, real estate, or other assets protected from scrutiny (it is illegal to disclose names or any information about Cooks trusts) be physically located within the archipelago. Taxes on trusts and trust employees account for some 8% of the Cook Islands economy, behind tourism but ahead of fishing.
Culture.
Language.
The languages of the Cook Islands include English, Cook Islands Māori, or "Rarotongan," and Pukapukan. Dialects of Cook Islands Maori include Penrhyn; Rakahanga-Manihiki; the Ngaputoru dialect of Atiu, Mitiaro, and Mauke; the Aitutaki dialect; and the Mangaian dialect. Cook Islands Maori and its dialectic variants are closely related to both Tahitian and to New Zealand Māori. Pukapukan is considered closely related to the Samoan language. English and Cook Islands Maori are official languages of the Cook Islands.
Music.
Music in the Cook Islands is varied, with Christian songs being quite popular, but traditional dancing and songs in Polynesian languages remain popular.
Art.
Carving.
Woodcarving is a common art form in the Cook Islands. The proximity of islands in the southern group helped produce a homogeneous style of carving but which had special developments in each island. Rarotonga is known for its fisherman's gods and staff-gods, Atiu for its wooden seats, Mitiaro, Mauke and Atiu for mace and slab gods and Mangaia for its ceremonial adzes. Most of the original wood carvings were either spirited away by early European collectors or were burned in large numbers by missionary zealots. Today, carving is no longer the major art form with the same spiritual and cultural emphasis given to it by the Maori in New Zealand. However, there are continual efforts to interest young people in their heritage and some good work is being turned out under the guidance of older carvers. Atiu, in particular, has a strong tradition of crafts both in carving and local fibre arts such as tapa. Mangaia is the source of many fine adzes carved in a distinctive, idiosyncratic style with the so-called double-k design. Mangaia also produces food pounders carved from the heavy calcite found in its extensive limestone caves.
Weaving.
The outer islands produce traditional weaving of mats, basketware and hats. Particularly fine examples of rito hats are worn by women to church. They are made from the uncurled immature fibre of the coconut palm and are of very high quality. The Polynesian equivalent of Panama hats, they are highly valued and are keenly sought by Polynesian visitors from Tahiti. Often, they are decorated with hatbands made of minuscule pupu shells which are painted and stitched on by hand. Although pupu are found on other islands the collection and use of them in decorative work has become a speciality of Mangaia. The weaving of rito is a speciality of the northern island of Penrhyn.
Tivaevae.
A major art form in the Cook Islands is tivaevae. This is, in essence, the art of handmade Island scenery patchwork quilts. Introduced by the wives of missionaries in the 19th century, the craft grew into a communal activity and is probably one of the main reasons for its popularity.
Contemporary art.
The Cook Islands has produced internationally recognised contemporary artists, especially in the main island of Rarotonga. Artists include painter (and photographer) Mahiriki Tangaroa, sculptors Eruera (Ted) Nia (originally a film maker) and master carver Mike Tavioni, painter (and Polynesian tattoo enthusiast) Upoko'ina Ian George, Aitutakian-born painter Tim Manavaroa Buchanan, Loretta Reynolds, Judith Kunzlé, Joan Rolls Gragg, Kay George (who is also known for her fabric designs), Apii Rongo, Varu Samuel, and multi-media, installation and community-project artist Ani O'Neil, all of whom currently live on the main island of Rarotonga. Atiuan-based Andrea Eimke is an artist who works in the medium of tapa and other textiles, and also co-authored the book 'Tivaivai – The Social Fabric of the Cook Islands' with British academic Susanne Kuechler. Many of these artists have studied at university art schools in New Zealand and continue to enjoy close links with the New Zealand art scene.
New Zealand-based Cook Islander artists include Michel Tuffrey, print-maker David Teata, Richard Shortland Cooper, Sylvia Marsters and Jim Vivieaere.
On Rarotonga, the main commercial galleries are Beachcomber Contemporary Art (Taputapuatea, Avarua) run by Ben & Trevon Bergman, and The Art Gallery ('Arorangi). The Cook Islands National Museum also exhibits art.
Sport.
Rugby league is the most popular sport in the Cook Islands. Association football (soccer), Netball, and Cricket are also popular.

</doc>
<doc id="7068" url="http://en.wikipedia.org/wiki?curid=7068" title="History of the Cook Islands">
History of the Cook Islands

The Cook Islands are named after Captain James Cook, who visited the islands in 1773 and 1777. The Cook Islands became a British protectorate in 1888.
By 1900, administrative control was transferred to New Zealand; in 1965 residents chose self-government in free association with New Zealand.
The Cook Islands contain 15 islands in the group spread over a vast area in the South Pacific. The majority of islands are low coral atolls in the Northern Group, with Rarotonga, a volcanic island in the Southern Group, as the main administration and government centre. The main Cook Islands language is Rarotongan Māori. There are some variations in dialect in the 'outer' islands.
History.
The Cook Islands were first settled around 600 AD by Polynesian people who migrated from nearby Tahiti to the northeast. Overpopulation on many of the tiny islands of Polynesia led to these oceanic migrations. Tradition has it that this was the reason for the expedition of Ru, from Tupua'i in French Polynesia, who landed on Aitutaki, and Tangiia, also from French Polynesia, both of whom are believed to have arrived on Rarotonga around 800 AD. These arrivals are evidenced by an older road in Toi, the "Ara Metua", which runs around most of Rarotonga, and is believed to be at least 1200 years old. This 29 km long, paved road is a considerable achievement of ancient engineering, possibly unsurpassed elsewhere in Polynesia. The islands of Manihiki and Rakahanga trace their origins to the arrival of Toa (an outcast from Rarotonga) and Tupaeru (a woman of high-ranking in the Puaikura tribe of Rarotonga)The remainder of the northern islands were probably settled by expeditions from Samoa and Tonga.
Spanish ships visited the islands in the 16th century; the first written record of contact from Europeans with the native inhabitants of the Cook Islands came with the sighting of Pukapuka by Spanish sailor Álvaro de Mendaña in 1595 who called it "San Bernardo" (Saint Bernard). Portuguese-Spaniard Pedro Fernández de Quirós made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling it "Gente Hermosa" (Beautiful People).
British navigator Captain James Cook arrived in 1773 and 1777; Cook named the islands the 'Hervey Islands' to honour a British Lord of the Admiralty; Half a century later the Russian Baltic German Admiral Adam Johann von Krusenstern published the "Atlas de l'Ocean Pacifique", in which he renamed the islands the Cook Islands to honour Cook. Captain Cook navigated and mapped much of the group. Surprisingly, Cook never sighted the largest island, Rarotonga, and the only island that he personally set foot on was tiny, uninhabited Palmerston Atoll.
 The first recorded landing by Europeans was in 1814 by the Cumberland; trouble broke out between the sailors and the Islanders and many were killed on both sides.
The islands saw no more Europeans until missionaries arrived from England in 1821. Christianity quickly took hold in the culture and retains that grip today.
In 1823, Captain John Dibbs of the colonial barque Endeavour made the first official sighting of the island Rarotonga. The Endeavour was transporting Rev. John Williams on a missionary voyage to the islands.
Brutal Peruvian slave traders, known as blackbirders, took a terrible toll on the islands of the Northern Group in 1862 and 1863. At first the traders may have genuinely operated as labour recruiters, but they quickly turned to subterfuge and outright kidnapping to round up their human cargo. The Cook Islands was not the only island group visited by the traders, but Penrhyn Atoll was their first port of call and it has been estimated that three-quarters of the population was taken to Callao, Peru. Rakahanga and Pukapuka also suffered tremendous losses.
The Kingdom of Rarotonga was established in 1858 and in 1888 it became a British protectorate by the request of Queen Makea Takau, mainly to thwart French expansionism. Then later were transferred to New Zealand in 1901. They remained a New Zealand protectorate until 1965, at which point they became a self-governing territory in free association with New Zealand. The first Prime Minister Sir Albert Henry led the county until 1978 when he was accused of vote-rigging.
Today, the Cook Islands are essentially independent (self-governing in free association with New Zealand), New Zealand is tasked with overseeing the country's foreign relations and defense. The Cook Islands, Niue, New Zealand (and its territories: Tokelau and the Ross Dependency) make up the Realm of New Zealand.
After achieving autonomy in 1965, the Cook Islands elected Albert Henry of the Cook Islands Party as their first Prime Minister. He was succeeded in 1978 by Tom Davis of the Democratic Party.
On 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing its claim to the islands of Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990 the Cook Islands signed a treaty with France which delimited the maritime boundary between the Cook Islands and French Polynesia.
On June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. "Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers," chief Makea Vakatini Joseph Ariki explained. The "Cook Islands Herald" suggested that the "ariki" were attempting thereby to regain some of their traditional prestige or "mana". Prime Minister Jim Marurai described the take-over move as "ill-founded and nonsensical". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.
The emigration of skilled workers to New Zealand and government deficits are continuing problems.
Timeline.
1595 — Spaniard Álvaro de Mendaña de Neira is the first European to sight the islands.
1606 — Portuguese-Spaniard Pedro Fernández de Quirós made the first recorded European landing in the islands when he set foot on Rakahanga.
1773 — Captain James Cook explores the islands and names them the Hervey Islands. Fifty years later they are renamed in his honour by Russian Admiral Adam Johann von Krusenstern.
1821 — English and Tahitian missionaries land in Aitutaki, become the first non-Polynesian settlers.
1823 — English missionary John Williams lands in Rarotonga, converting Makea Pori Ariki to Christianity.
1858 — The Cook Islands become united as a state, the Kingdom of Rarotonga.
1862 — Peruvian slave traders took a terrible toll on the islands of Penrhyn, Rakahanga and Pukapuka in 1862 and 1863.
1888 — Cook Islands are proclaimed a British protectorate and a single federal parliament is established.
1901 — The Cook Islands are annexed to New Zealand.
1924 — The All Black "Invincibles" stop in Rarotonga on their way to the United Kingdom and play a friendly match against a scratch Rarotongan team.
1946 — Legislative Council is established. For the first time since 1912, the territory has direct representation.
1965 — The Cook Islands become a self-governing territory in free association with New Zealand. Albert Henry, leader of the Cook Islands Party, is elected as the territory's first prime minister.
1974 — Albert Henry is knighted by Queen Elizabeth II
1979 — Sir Albert Henry is found guilty of electoral fraud and stripped of his premiership and his knighthood. Tom Davis becomes Premier.
1980 — Cook Islands – United States Maritime Boundary Treaty establishes the Cook Islands – American Samoa boundary
1981 — Constitution is amended. Parliament grows from 22 to 24 seats and the parliamentary term is extended from four to five years. Tom Davis is knighted.
1984 — The country's first coalition government, between Sir Thomas and Geoffrey Henry, is signed in the lead up to hosting regional Mini Games in 1985. Shifting coalitions saw ten years of political instability. At one stage, all but two MPs were in government.
1985 — Rarotonga Treaty is opened for signing in the Cook Islands, creating a nuclear free zone in the South Pacific.
1986 — In January 1986, following the rift between New Zealand and the USA in respect of the ANZUS security arrangements Prime Minister Tom Davis declared the Cook Islands a neutral country, because he considered that New Zealand (which has control over the islands' defence and foreign policy) was no longer in a position to defend the islands. The proclamation of neutrality meant that the Cook Islands would not enter into a military relationship with any foreign power, and, in particular, would prohibit visits by US warships. Visits by US naval vessels were allowed to resume by Henry's Government.
1990 — Cook Islands – France Maritime Delimitation Agreement establishes the Cook Islands – French Polynesia boundary
1991 — The Cook Islands signed a treaty of friendship and co-operation with France, covering economic development, trade and surveillance of the islands' EEZ. The establishment of closer relations with France was widely regarded as an expression of the Cook Islands' Government's dissatisfaction with existing arrangements with New Zealand which was no longer in a position to defend the Cook Islands.
1995 — The French Government resumed its Programme of nuclear-weapons testing at Mururoa Atoll in September 1995 upsetting the Cook Islands. New Prime Minister Geoffrey Henry was fiercely critical of the decision and dispatched a vaka (traditional voyaging canoe) with a crew of Cook Islands' traditional warriors to protest near the test site. The tests were concluded in January 1996 and a moratorium was placed on future testing by the French government.
1997 — Full diplomatic relations established with China.
1997 — In November, Cyclone Martin in Manihiki kills at least six people; 80% of buildings are damaged and the black pearl industry suffered severe losses.
1999 — A second era of political instability begins, starting with five different coalitions in less than nine months, and at least as many since then.
2000 — Full diplomatic relations concluded with France.
2002 — Prime Minister Terepai Maoate is ousted from government following second vote of no-confidence in his leadership.
2004 — Prime Minister Robert Woonton visits China; Chinese Premier Wen Jiabao grants $16 million in development aid.
2006 — Parliamentary elections held. The Democratic Party keeps majority of seats in parliament, but is unable to command a majority for confidence, forcing a coalition with breakaway MPs who left, then rejoined the "Demos."
2008 — Pacific Island nations imposed a series of measures aimed at halting overfishing. 8 of 17 members of the Western and Central Pacific Fisheries Commission agreed on the measures In December the 9 remaining members joined in imposing a conservation agenda on 20 million square miles to take effect on January 1, 2010. The 17 members included Australia, Cook Islands, Federated States of Micronesia, Fiji, Kiribati, Marshall Islands, Nauru, New Zealand, Niue, Palau, Papua New Guinea, Samoa, Solomon Islands, Tokelau, Tonga, Tuvalu, Vanuatu.
2012 — The world's largest marine park, a vast swathe of ocean almost twice the size of France, was unveiled by the Cook Islands at the opening of the Pacific Islands Forum.
2013 — China’s Vice Premier Wang Yang said China will provide a concessionary loan of up to $1 billion to Pacific island nations to support construction projects in a part of the world where Beijing and Taiwan compete for influence. He made the announcement at a forum with Pacific island nations in Guangzhou at a meeting attended by representatives from Micronesia, Samoa, Papua New Guinea, Vanuatu, the Cook Islands, Tonga, Niue and Fiji.

</doc>
<doc id="7069" url="http://en.wikipedia.org/wiki?curid=7069" title="Geography of the Cook Islands">
Geography of the Cook Islands

The Cook Islands can be divided into two groups: the Southern Cook Islands and the Northern Cook Islands.
Location.
Oceania, group of islands in the South Pacific Ocean, about one-half of the way from Hawaii to New Zealand

</doc>
<doc id="7070" url="http://en.wikipedia.org/wiki?curid=7070" title="Demographics of the Cook Islands">
Demographics of the Cook Islands

This article is about the demographic features of the population of the Cook Islands, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

</doc>
<doc id="7071" url="http://en.wikipedia.org/wiki?curid=7071" title="Politics of the Cook Islands">
Politics of the Cook Islands

The politics of the Cook Islands, an associated state, takes place in a framework of a parliamentary representative democracy within a constitutional monarchy. The Queen of New Zealand, represented in the Cook Islands by the Queen's Representative, is the Head of State; the Chief Minister is the head of government and of a multi-party system. The Islands are self-governing in free association with New Zealand and are fully responsible for internal affairs. New Zealand retains some responsibility for external affairs, in consultation with the Cook Islands. In recent years, the Cook Islands have taken on more of its own external affairs; as of 2005, it has diplomatic relations in its own name with eighteen other countries. Executive power is exercised by the government, while legislative power is vested in both the government and the islands' parliament. The judiciary is independent of the executive and the legislatures.
Constitution.
The Constitution of the Cook Islands took effect on August 4, 1965, when the Cook Islands became a self-governing territory in free association with New Zealand. The anniversary of these events in 1965 is commemorated annually on Constitution Day, with week long activities known as "Te Maevea Nui Celebrations" locally.
Executive.
The monarch is hereditary; her representative is appointed by the monarch on the recommendation of the Cook Islands Government. The cabinet is chosen by the prime minister and collectively responsible to Parliament.
Ten years of rule by the Cook Islands Party (CIP) came to an end 18 November 1999 with the resignation of Prime Minister Joe Williams. Williams had led a minority government since October 1999 when the New Alliance Party (NAP) left the government coalition and joined the main opposition Democratic Party (DAP). On 18 November 1999, DAP leader Dr. Terepai Maoate was sworn in as prime minister. He was succeeded by his co-partisan Robert Woonton. When Dr Woonton lost his seat in the 2004 elections, Jim Marurai took over. In the 2010 elections, the CIP regained power and Henry Puna was sworn in as prime minister on 30 November 2010.
Following uncertainty about the ability of the government to maintain its majority, the Queen's representative dissolved parliament mid-way through its term and a 'snap' election was held on 26 September 2006. Jim Marurai's Democratic Party retained the Treasury benches with an increased majority.
The New Zealand High Commissioner is appointed by the New Zealand Government.
Legislature.
The Parliament of the Cook Islands has 24 members, elected for a five-year term in single-seat constituencies. There is also a House of Ariki, composed of chiefs, which has a purely advisory role. The Koutu Nui is a similar organization consisting of sub-chiefs. It was established by an amendment in 1972 of the 1966 House of Ariki Act. The current President is Te Tika Mataiapo Dorice Reid.
On June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. "Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers," chief Makea Vakatini Joseph Ariki explained. The "Cook Islands Herald" suggested that the "ariki" were attempting thereby to regain some of their traditional prestige or "mana". Prime Minister Jim Marurai described the take-over move as "ill-founded and nonsensical". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.
Recent political history.
The 1999 election produced a hung Parliament. Cook Islands Party leader Geoffrey Henry remained Prime Minister, but was replaced after a month by Joe Williams following a coalition realignment. A further realignment three months later saw Williams replaced by Democratic Party leader Terepai Maoate. A third realignment saw Maoate replaced mid-term by his deputy Robert Woonton in 2002, who ruled with the backing of the CIP.
The Democratic Party won a majority in the 2004 election, but Woonton lost his seat, and was replaced by Jim Marurai. In 2005 Marurai left the Democrats due to an internal disputes, founding his own Cook Islands First Party. He continued to govern with the support of the CIP, but in 2005 returned to the Democrats. The loss of several by-elections forced a snap-election in 2006, which produced a solid majority for the Democrats and saw Marurai continue as Prime Minister.
In December 2009, Marurai sacked his Deputy Prime Minister, Terepai Maoate, sparking a mass-resignation of Democratic Party cabinet members He and new Deputy Prime Minister Robert Wigmore were subsequently expelled from the Democratic Party. Marurai appointed three junior members of the Democratic party to Cabinet, but on 31 December 2009 the party withdrew its support.

</doc>
<doc id="7072" url="http://en.wikipedia.org/wiki?curid=7072" title="Economy of the Cook Islands">
Economy of the Cook Islands

The economy of the Cook Islands, as in many other South Pacific island nations, is hindered by the isolation of the country from foreign markets, lack of natural resources, periodic devastation from natural disasters, and inadequate infrastructure.
Agriculture provides the economic base with major exports made up of copra and citrus fruit. Manufacturing activities are limited to fruit-processing, clothing, and handicrafts.
Trade deficits are made up for by remittances from emigrants and by foreign aid, overwhelmingly from New Zealand. Efforts to exploit tourism potential, encourage offshore banking, and expand the mining and fishing industries have been partially successful in stimulating investment and growth.
Banking and finance.
The Cook Islands has "Home Rule" with respect to banking, similar to Guernsey, Jersey and the Isle of Man.
This "Home Rule" banking confuses New Zealanders on vacation in the Cooks. Cook automated teller machines often fail to fully disclose the fact that the Cooks are not part of the New Zealand banking system, thus legally requiring banks to charge the same fees for withdrawing or transferring money as if the person was in Australia or the EU. The New Zealand dollar is the official currency of the Cook Islands, adding to the confusion. Cook Islanders are NZ citizens.
The banking and incorporation laws of the Cook Islands make it an important centre for setting up companies that are involved in global trade.
Telecommunications.
Telecom Cook Islands Ltd (TCI) is the sole provider of telecommunications in the Cook Islands. TCI is a private company owned by Spark New Zealand Ltd (60%) and the Cook Islands Government (40%). In operation since July 1991, TCI provides local, national and international telecommunications as well as internet access on all islands except Suwarrow. Communications to Suwarrow is via HF radio.

</doc>
<doc id="7073" url="http://en.wikipedia.org/wiki?curid=7073" title="Telecommunications in the Cook Islands">
Telecommunications in the Cook Islands

Like most countries and territories in Oceania, telecommunications in the Cook Islands is limited by its isolation and low population, with only one major television broadcasting station and six radio stations. However, most residents have a main line or mobile phone. Its telecommunications are mainly provided by Telecom Cook Islands, who is currently working with O3b Networks, Ltd. for faster Internet connection.
Telephone.
In July 2012, there were about 7,500 main line telephones, which covers about 98% of the country's population, while there were approximately 7,800 mobile phones in 2009. Telecom Cook Islands, owned by Spark New Zealand, is the islands' main telephone system and offers international direct dialling, Internet, email, fax, and Telex. The individual islands are connected by a combination of satellite earth stations, microwave systems, and very high frequency and high frequency radiotelephone; within the islands, service is provided by small exchanges connected to subscribers by open wire, cable, and fibre optic cable. For international communication, they rely on the satellite earth station Intelsat.
In 2003, the largest island of Rarotonga started using a GSM/GPRS mobile data service system with GSM 900/1900 networks by 2006.
The Cook Islands uses the country calling code +682.
Broadcasting.
Radio.
There are six radio stations in the Cook Islands, with one reaching all islands. there were 14,000 radios.
Television.
Cook Islands Television broadcasts from Rarotonga, the capital of the Cook Islands, providing a mix of local news and overseas-sourced programs. there were 4,000 television sets.
Internet.
There were 6,000 Internet users in 2009 and 3,562 Internet hosts as of 2012. The country code top-level domain for the Cook Islands is .ck.
In June 2010, Telecom Cook Islands partnered with O3b Networks, Ltd. to provide faster Internet connection to the Cook Islands. On 25 June 2013 the O3b satellite constellation was launched from an Arianespace Soyuz ST-B rocket in French Guinea. The medium Earth orbit satellite orbits at and uses the Ka band. It has a latency of about 100 milliseconds because it is much closer to Earth than standard geostationary satellites, whose latencies can be over 600 milliseconds. Although the initial launch consisted of 4 satellites, as many as 20 may be launched eventually to serve various areas with little or no optical fibre service, the first of which is the Cook Islands.

</doc>
<doc id="7074" url="http://en.wikipedia.org/wiki?curid=7074" title="Transport in the Cook Islands">
Transport in the Cook Islands

This article lists transport in the Cook Islands.

</doc>
<doc id="7077" url="http://en.wikipedia.org/wiki?curid=7077" title="Computer file">
Computer file

A computer file is a resource for storing information, which is available to a computer program and is usually based on some kind of durable storage. A file is "durable" in the sense that it remains available for other programs to use after the program that created it has finished executing. Computer files can be considered as the modern counterpart of paper documents which traditionally are kept in office and library files, and this is the source of the term.
Etymology.
The word "file" was used publicly in the context of computer storage as early as February, 1950. In an RCA (Radio Corporation of America) advertisement in "Popular Science Magazine" describing a new "memory" vacuum tube it had developed, RCA stated:
In 1952, "file" was used in referring to information stored on punched cards.
In early usage, people regarded the underlying hardware (rather than the contents) as the file. For example, the IBM 350 disk drives were called "disk files". In about 1961 the Burroughs MCP and the MIT Compatible Time-Sharing System introduced the concept of a "file system", which managed several virtual "files" on one storage device, giving the term its present-day meaning. Although the current term "register file" shows the early concept of files, it has largely disappeared.
File contents.
On most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its content since a file is solely a container for data, although, on some platforms the format is usually indicated by its filename extension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file (.txt in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic information about itself.
Some file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to the file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific databases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.
File size.
At any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage device. In such systems, software employed other methods to track the exact byte count (e.g., CP/M used a special control character, Ctrl-Z, to signal the end of text files).
The general definition of a file does not require that its size has any real meaning, however, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a zero byte file; these files can be newly created file that have not yet had any data written to them, or may serve as some kind of flag in the file system, or are accidents (the results of aborted disk operations). For example, the file to which the link /bin/ls points in a typical Unix-like system probably has a defined size that seldom changes. Compare this with /dev/null which is also a file, but its size may be obscure.
Organizing the data in a file.
Information in a computer file can consist of smaller packets of information (often called "records" or "lines") that are individually different but share some common traits. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll—this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a BLOB) or it may contain an executable.
The way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by computer programs which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.
In some cases, computer programs manipulate files that are made visible to the computer user. For example, in a word-processing program, the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.
Many applications pack all their data files into a single file called archive file, using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.
File operations.
The most basic operations that programs can perform on a file are:
Files on a computer can be created, moved, modified, grown, shrunk, and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on Windows computers) or by command lines (CLI).
In Unix-like systems, user-space processes do not normally deal with files at all; the operating system provides a level of abstraction which means that almost all interaction with files from user-space is through hard links. For example, a user space program cannot delete a file; it can delete a link to a file, and if the kernel determines that there are no hard links to the file, it may then allow the memory location for the deleted file to be allocated for another file. The resulting free space, is commonly considered a security risk due to the existence of file recovery software. Such a risk has given rise to secure deletion programs. Only the kernel deals with files, but it handles all user-space interaction with (virtual) files in a manner that is transparent to the user-space programs.
Special file type error-ability.
Normally if an object in a file is being moved, and if for some reason if the process was interrupted, and the process not cancelled,(i.e.the power goes out, etc.), Microsoft is programmed in a special way that it moves the file by first copying it to the destination, then after copying, deletes the file from the original location. This is done so that a person doesn't run into a problem where the file is corrupted due to half of the information already having been moved to a document but the other half still being in the original location. This is not the case in a .zip file. If in any of the above scenarios where the process of moving documents into a folder were to fail completion, most of the time not only does that document become corrupted but the rest of the .zip file may become corrupted as well. A corrupt .zip file cannot even get its contents extracted.
Identifying and organizing files.
In modern computer systems, files are typically accessed using names (filenames). In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.
Files (or links to files) can be located in directories. However, more generally, a directory can contain either a list of files or a list of links to files. Within this definition, it is of paramount importance that the term "file" includes directories. This permits the existence of directory hierarchies, i.e., directories containing sub-directories. A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory. However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.
In environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system—no two files can have the same name and path. Where a file is anonymous, named references to it will exist within a namespace. In most cases, any name within the namespace will refer to exactly zero or one file. However, any file may be represented within any namespace by zero, one or more names.
Any string of characters may or may not be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of unicode letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the file system. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. Microsoft Windows supports multiple file systems, each with different policies regarding case-sensitivity. The common FAT file system can have multiple files whose names differ only in case if the user uses a disk editor to edit the file names in the directory entries. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.
Most computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one "master folder" (or "root folder" — the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.
When a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character—such as a slash—is used to separate the file and folder names. For example, in the illustration shown in this article, the path /Payroll/Salaries/Managers uniquely identifies a file called Managers in a folder called Salaries, which in turn is contained in a folder called Payroll. The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).
Many (but not all) computer systems use extensions in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of .txt identifies a text file; a .doc extension identifies any type of document or documentation, commonly in the Microsoft Word file format; and so on. Even when extensions are used in a computer system, the degree to which the computer system recognizes 
and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.
Protecting files.
Many modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement file permissions to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.
Another protection mechanism implemented in many computers is a "read-only flag." When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a "hidden flag" to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.
Storing files.
The discussion above describes a file as a concept presented to a user or a high-level operating system. However, any file that has any useful purpose, outside of a thought experiment, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.
In physical terms, most computer files are stored on some type of data storage device. For example, there is a hard disk, from which most operating systems run and on which most store their files. Hard disks have been the ubiquitous form of non-volatile storage since the early 1960s. Where files contain only temporary information, they may be stored in RAM. Computer files can be also stored on other media in some cases, such as magnetic tapes, compact discs, Digital Versatile Discs, Zip drives, USB flash drives, etc. The use of solid state drives is also beginning to rival the hard
disk drive.
In Unix-like operating systems, many files have no direct association with a physical storage device: /dev/null is a prime example, as are just about all files under /dev, /proc and /sys. These can be accessed as files in user space. They are really virtual files that exist, in reality, as objects within the operating system kernel.
As seen by a running user program, files are usually represented either by a File Control Block or by a file handle. A File Control Block (FCB) is an area of memory which is manipulated to establish a filename etc. and then passed to the operating system as a parameter, it was used by older IBM operating systems and by early PC operating systems including CP/M and early versions of MS-DOS. A file handle is generally either an opaque data type or an integer, it was introduced in around 1961 by the ALGOL-based Burroughs MCP running on the Burroughs B5000 but is now ubiquitous.
Backing up files.
When computer files contain information that is extremely important, a "back-up" process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.
There are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.
The grandfather-father-son backup method automatically makes three back-ups; the grandfather file is the oldest copy of the file and the son is the current copy.
File systems and file managers.
The way a computer organizes, names, stores and manipulates files is globally referred to as its "file system." Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of MS-DOS and old versions of Windows are supported, in addition to the NTFS file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file "Payroll records" in NTFS, but in FAT you would be restricted to something like payroll.dat (unless you were using VFAT, a FAT extension allowing long file names).
File manager programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. Under Windows, the most commonly used file manager program is Windows Explorer.

</doc>
<doc id="7079" url="http://en.wikipedia.org/wiki?curid=7079" title="CID">
CID

CID may refer to:

</doc>
<doc id="7080" url="http://en.wikipedia.org/wiki?curid=7080" title="Christian Doppler">
Christian Doppler

Christian Andreas Doppler (; 29 November 1803 – 17 March 1853) was an Austrian mathematician and physicist. He is celebrated for his principle — known as the Doppler effect — that the observed frequency of a wave depends on the relative speed of the source and the observer. He used this concept to explain the color of binary stars.
Biography.
Doppler was raised in Salzburg, Austria, the son of a stonemason. He could not work in his father's business because of his generally weak physical condition. After completing high school, Doppler studied philosophy in Salzburg and mathematics and physics at the "k. k. Polytechnisches Institut" (now Vienna University of Technology) where he began work as an assistant in 1829. In 1835 he began work at the "Prague Polytechnic" (now Czech Technical University), where he received an appointment in 1841.
Only a year later, at the age of 38, Doppler gave a lecture to the Royal Bohemian Society of Sciences and subsequently published his most notable work, "Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels" "(On the coloured light of the binary stars and some other stars of the heavens)". There is a facsimile edition with an English translation by Alec Eden. In this work, Doppler postulated his principle (later coined the Doppler effect) that the observed frequency of a wave depends on the relative speed of the source and the observer, and he tried to use this concept for explaining the colour of binary stars. In Doppler's time in Prague as a professor he published over 50 articles on mathematics, physics and astronomy. In 1847 he left Prague for the professorship of mathematics, physics, and mechanics at the Academy of Mines and Forests (its successor is the present day University of Miskolc) in Selmecbánya (then Kingdom of Hungary, now Banská Štiavnica, Slovakia), and in 1849 he moved to Vienna.
Doppler's research in Prague was interrupted by the revolutionary incidents of March 1848, when he fled to Vienna. There he was appointed head of the Institute for Experimental Physics at the University of Vienna in 1850. During his time there, Doppler, along with Franz Unger, played an influential role in the development of young Gregor Mendel, known as the founding father of genetics, who was a student at the University of Vienna from 1851 to 1853.
Doppler died on 18 March 1853 at age 49 from a pulmonary disease in Venice (also at that time part of the Austrian Empire). His tomb, found by Dr. Peter M. Schuster is just inside the entrance of the Venetian island cemetery of San Michele.
Full name.
Some confusion exists about Doppler's full name. Doppler referred to himself as Christian Doppler. The records of his birth and baptism stated Christian "Andreas" Doppler. Forty years after Doppler's death the misnomer "Johann" Christian Doppler was introduced by the astronomer Julius Scheiner. Scheiner's mistake has since been copied by many.

</doc>
<doc id="7081" url="http://en.wikipedia.org/wiki?curid=7081" title="Clerihew">
Clerihew

A clerihew is a whimsical, four-line biographical poem invented by Edmund Clerihew Bentley. The first line is the name of the poem's subject, usually a famous person put in an absurd light. The rhyme scheme is AABB, and the rhymes are often forced. The line length and meter are irregular. Bentley invented the clerihew in school and then popularized it in books. One of his best known is this (1905):
Form.
A clerihew has the following properties:
Clerihews are not satirical or abusive, but they target famous individuals and reposition them in an absurd, anachronistic or commonplace setting, often giving them an over-simplified and slightly garbled description (not unlike the schoolboy style of "1066 and All That)".
The unbalanced and unpolished poetic meter and line length parody the limerick, and the clerihew in form also parodies the eulogy.
Practitioners.
The form was invented by and is named after Edmund Clerihew Bentley. When he was a 16-year-old pupil at St Paul's School in London, the lines about Humphry Davy came into his head during a science class. Together with his schoolfriends, he filled a notebook with examples. The first use of the word in print was in 1928. Clerihew published three volumes of his own clerihews: "Biography for Beginners" (1905), published as "edited by E. Clerihew"; "More Biography" (1929); and "Baseless Biography" (1939), a compilation of clerihews originally published in "Punch" illustrated by the author's son Nicolas Bentley.
Bentley's friend, G. K. Chesterton, was also a practitioner of the clerihew and one of the sources of its popularity. Chesterton provided verses and illustrations for the original schoolboy notebook and illustrated "Biography for Beginners". Other serious authors also produced clerihews, including W. H. Auden, and it remains a popular humorous form among other writers and the general public. Among contemporary writers, the satirist Craig Brown has made considerable use of the clerihew in his columns for "The Daily Telegraph".
Examples.
Bentley's first clerihew, published in 1905, was written about Sir Humphry Davy:
The original poem had the second line “Was not fond of gravy"; but the better-known published version has the more terse and succinct “Abominated gravy”.
Other classic clerihews by Bentley include:
Auden's "Literary Graffiti" includes:
The subject of a clerihew written by the students of his alma mater, Sherborne School in England, was one of the founders of computing:
A clerihew much appreciated by chemists is cited in "Dark Sun" by Richard Rhodes, and regards the inventor of the thermos bottle (or Dewar flask): 
In 1983, "Games" Magazine ran a contest titled "Do You Clerihew?" The winning entry was:
Other uses of the form.
The clerihew form has also occasionally been used for non-biographical verses. Bentley opened his "Biography for Beginners" with an example, entitled "Introductory Remarks", on the theme of biography itself:
The third edition of the same work, published in 1925, included a "Preface to the New Edition" in 11 stanzas, each in clerihew form. One stanza ran:

</doc>
<doc id="7085" url="http://en.wikipedia.org/wiki?curid=7085" title="Civil war">
Civil war

A civil war is a war between organized groups within the same state or republic, or, less commonly, between two countries created from a formerly united state.
The aim of one side may be to take control of the country or a region, to achieve independence for a region, or to change government policies.
The term is a calque of the Latin "bellum civile" which was used to refer to the various civil wars of the Roman Republic in the 1st century BC.
A civil war is a high-intensity conflict, often involving regular armed forces, that is sustained, organized and large-scale. Civil wars may result in large numbers of casualties and the consumption of significant resources.
Civil wars since the end of World War II have lasted on average just over four years, a dramatic rise from the one-and-a-half year average of the 1900-1944 period. While the rate of emergence of new civil wars has been relatively steady since the mid-19th century, the increasing length of those wars resulted in increasing numbers of wars ongoing at any one time. For example, there were no more than five civil wars underway simultaneously in the first half of the 20th century while over 20 concurrent civil wars were occurring close to the end of the Cold War. Since 1945, civil wars have resulted in the deaths of over 25 million people, as well as the forced displacement of millions more. Civil wars have further resulted in economic collapse; Somalia, Burma (Myanmar), Uganda and Angola are examples of nations that were considered to have promising futures before being engulfed in civil wars.
Formal classification.
James Fearon, a scholar of civil wars at Stanford University, defines a civil war as "a violent conflict within a country fought by organized groups that aim to take power at the center or in a region, or to change government policies". Ann Hironaka further specifies that one side of a civil war is the state. The intensity at which a civil disturbance becomes a civil war is contested by academics. Some political scientists define a civil war as having more than 1000 casualties, while others further specify that at least 100 must come from each side. The Correlates of War, a dataset widely used by scholars of conflict, classifies civil wars as having over 1000 war-related casualties per year of conflict. This rate is a small fraction of the millions killed in the Second Sudanese Civil War and Cambodian Civil War, for example, but excludes several highly publicized conflicts, such as The Troubles of Northern Ireland and the struggle of the African National Congress in Apartheid-era South Africa.
Based on the 1000 casualties per year criterion, there were 213 civil wars from 1816 to 1997, 104 of which occurred from 1944 to 1997. If one uses the less-stringent 1000 casualties total criterion, there were over 90 civil wars between 1945 and 2007, with 20 ongoing civil wars as of 2007.
The Geneva Conventions do not specifically define the term "civil war", nevertheless they do outline the responsibilities of parties in "armed conflict not of an international character". This includes civil wars, however no specific definition of civil war is provided in the text of the Conventions.
Nevertheless the International Committee of the Red Cross has sought to provide some clarification through its commentaries on the Geneva Conventions, noting that the Conventions are "so general, so vague, that many of the delegations feared that it might be taken to cover any act committed by force of arms". Accordingly the commentaries provide for different 'conditions' on which the application of the Geneva Convention would depend, the commentary however points out that these should not be interpreted as rigid conditions. The conditions listed by the ICRC in its commentary are as follows:
That the Party in revolt against the de jure Government possesses an organized military force, an authority responsible for its acts, acting within a determinate territory and having the means of respecting and ensuring respect for the Convention.
That the legal Government is obliged to have recourse to the regular military forces against insurgents organized as military and in possession of a part of the national territory.
(a) That the de jure Government has recognized the insurgents as belligerents; or
(b) That it has claimed for itself the rights of a belligerent; or
(c) That it has accorded the insurgents recognition as belligerents for the purposes only of the present Convention; or
(d) That the dispute has been admitted to the agenda of the Security
Council or the General Assembly of the United Nations as being a
threat to international peace, a breach of the peace, or an act
of aggression.
(a) That the insurgents have an organization purporting to have the
characteristics of a State.
(b) That the insurgent civil authority exercises de facto authority
over the population within a determinate portion of the national
territory.
(c) That the armed forces act under the direction of an organized
authority and are prepared to observe the ordinary laws of war.
(d) That the insurgent civil authority agrees to be bound by the
provisions of the Convention.
Causes of civil war in the Collier–Hoeffler Model.
Scholars investigating the cause of civil war are attracted by two opposing theories, greed versus grievance. Roughly stated: are conflicts caused by who people are, whether that be defined in terms of ethnicity, religion or other social affiliation, or do conflicts begin because it is in the economic best interests of individuals and groups to start them? Scholarly analysis supports the conclusion that economic and structural factors are more important than those of identity in predicting occurrences of civil war.
A comprehensive study of civil war was carried out by a team from the World Bank in the early 21st century. The study framework, which came to be called the Collier–Hoeffler Model, examined 78 five-year increments when civil war occurred from 1960 to 1999, as well as 1,167 five-year increments of "no civil war" for comparison, and subjected the data set to regression analysis to see the effect of various factors. The factors that were shown to have a statistically significant effect on the chance that a civil war would occur in any given five-year period were:
A high proportion of primary commodities in national exports significantly increases the risk of a conflict. A country at "peak danger", with commodities comprising 32% of gross domestic product, has a 22% risk of falling into civil war in a given five-year period, while a country with no primary commodity exports has a 1% risk. When disaggregated, only petroleum and non-petroleum groupings showed different results: a country with relatively low levels of dependence on petroleum exports is at slightly less risk, while a high level of dependence on oil as an export results in slightly more risk of a civil war than national dependence on another primary commodity. The authors of the study interpreted this as being the result of the ease by which primary commodities may be extorted or captured compared to other forms of wealth; for example, it is easy to capture and control the output of a gold mine or oil field compared to a sector of garment manufacturing or hospitality services.
A second source of finance is national diasporas, which can fund rebellions and insurgencies from abroad. The study found that statistically switching the size of a country's diaspora from the smallest found in the study to the largest resulted in a sixfold increase in the chance of a civil war.
Higher male secondary school enrollment, per capita income and economic growth rate all had significant effects on reducing the chance of civil war. Specifically, a male secondary school enrollment 10% above the average reduced the chance of a conflict by about 3%, while a growth rate 1% higher than the study average resulted in a decline in the chance of a civil war of about 1%. The study interpreted these three factors as proxies for earnings forgone by rebellion, and therefore that lower forgone earnings encourage rebellion. Phrased another way: young males (who make up the vast majority of combatants in civil wars) are less likely to join a rebellion if they are getting an education or have a comfortable salary, and can reasonably assume that they will prosper in the future.
Low per capita income has been proposed as a cause for grievance, prompting armed rebellion. However, for this to be true, one would expect economic inequality to also be a significant factor in rebellions, which it is not. The study therefore concluded that the economic model of opportunity cost better explained the findings.
High levels of population dispersion and, to a lesser extent, the presence of mountainous terrain, increased the chance of conflict. Both of these factors favor rebels, as a population dispersed outward toward the borders is harder to control than one concentrated in a central region, while mountains offer terrain where rebels can seek sanctuary.
Most proxies for "grievance" – the theory that civil wars begin because of issues of identity, rather than economics – were statistically insignificant, including economic equality, political rights, ethnic polarization and religious fractionalization. Only ethnic dominance, the case where the largest ethnic group comprises a majority of the population, increased the risk of civil war. A country characterized by ethnic dominance has nearly twice the chance of a civil war. However, the combined effects of ethnic and religious fractionalization, i.e. the greater chance that any two randomly chosen people will be from separate ethnic or religious groups, the less chance of a civil war, were also significant and positive, as long as the country avoided ethnic dominance. The study interpreted this as stating that minority groups are more likely to rebel if they feel that they are being dominated, but that rebellions are more likely to occur the more homogeneous the population and thus more cohesive the rebels. These two factors may thus be seen as mitigating each other in many cases.
The various factors contributing to the risk of civil war rise increase with population size. The risk of a civil war rises approximately proportionately with the size of a country's population.
The more time that has elapsed since the last civil war, the less likely it is that a conflict will recur. The study had two possible explanations for this: one opportunity-based and the other grievance-based. The elapsed time may represent the depreciation of whatever capital the rebellion was fought over and thus increase the opportunity cost of restarting the conflict. Alternatively, elapsed time may represent the gradual process of healing of old hatreds. The study found that the presence of a diaspora substantially reduced the positive effect of time, as the funding from diasporas offsets the depreciation of rebellion-specific capital.
Other causes.
Evolutionary psychologist Satoshi Kanazawa has argued that an important cause of intergroup conflict may be the relative availability of women of reproductive age. He found that polygyny greatly increased the frequency of civil wars but not interstate wars. Gleditsch et al. did not find a relationship between ethnic groups with polygyny and increased frequency of civil wars but nations having legal polygamy may have more civil wars. They argued that misogyny is a better explanation than polygyny. They found that increased women's rights were are associated with less civil wars and that legal polygamy had no effect after women’s rights were controlled for.
Duration of civil wars.
Ann Hironaka, author of "Neverending Wars", divides the modern history of civil wars into the pre-19th century, 19th century to early 20th century, and late 20th century. In 19th-century Europe, the length of civil wars fell significantly, largely due to the nature of the conflicts as battles for the power center of the state, the strength of centralized governments, and the normally quick and decisive intervention by other states to support the government. Following World War II the duration of civil wars grew past the norm of the pre-19th century, largely due to weakness of the many postcolonial states and the intervention by major powers on both sides of conflict. The most obvious commonality to civil wars are that they occur in fragile states.
Civil wars in the 19th and early 20th centuries.
Civil wars in the 19th century and in the early 20th century tended to be short; civil wars between 1900 and 1944 lasted on average one and half years. The state itself formed the obvious center of authority in the majority of cases, and the civil wars were thus fought for control of the state. This meant that whoever had control of the capital and the military could normally crush resistance. A rebellion which failed to quickly seize the capital and control of the military for itself normally found itself doomed to rapid destruction. For example, the fighting associated with the 1871 Paris Commune occurred almost entirely in Paris, and ended quickly once the military sided with the government at Versailles and conquered Paris.
The power of non-state actors resulted in a lower value placed on sovereignty in the 18th and 19th centuries, which further reduced the number of civil wars. For example, the pirates of the Barbary Coast were recognized as "de facto" states because of their military power. The Barbary pirates thus had no need to rebel against the Ottoman Empire - their nominal state government - to gain recognition for their sovereignty. Conversely, states such as Virginia and Massachusetts in the United States of America did not have sovereign status, but had significant political and economic independence coupled with weak federal control, reducing the incentive to secede.
The two major global ideologies, monarchism and democracy, led to several civil wars. However, a bi-polar world, divided between the two ideologies, did not develop, largely due to the dominance of monarchists through most of the period. The monarchists would thus normally intervene in other countries to stop democratic movements taking control and forming democratic governments, which were seen by monarchists as being both dangerous and unpredictable. The Great Powers (defined in the 1815 Congress of Vienna as the United Kingdom, Habsburg Austria, Prussia, France, and Russia) would frequently coordinate interventions in other nations' civil wars, nearly always on the side of the incumbent government. Given the military strength of the Great Powers, these interventions nearly always proved decisive and quickly ended the civil wars.
There were several exceptions from the general rule of quick civil wars during this period. The American Civil War (1861–1865) was unusual for at least two reasons: it was fought around regional identities as well as political ideologies, and it ended through a war of attrition, rather than with a decisive battle over control of the capital, as was the norm. The Spanish Civil War (1936–1939) proved exceptional because "both" sides in the struggle received support from intervening great powers: Germany, Italy, and Portugal supported opposition leader Francisco Franco, while France and the Soviet Union supported the government (see proxy war).
Civil wars since 1945.
In the 1990s, about twenty civil wars were occurring concurrently during an average year, a rate about ten times the historical average since the 19th century. However, the rate of new civil wars had not increased appreciably; the drastic rise in the number of ongoing wars after World War II was a result of the tripling of the average duration of civil wars to over four years. This increase was a result of the increased number of states, the fragility of states formed after 1945, the decline in interstate war, and the Cold War rivalry.
Following World War II, the major European powers divested themselves of their colonies at an increasing rate: the number of ex-colonial states jumped from about 30 to almost 120 after the war. The rate of state formation leveled off in the 1980s, at which point few colonies remained. More states also meant more states in which to have long civil wars. Hironaka statistically measures the impact of the increased number of ex-colonial states as increasing the post-WWII incidence of civil wars by +165% over the pre-1945 number.
While the new ex-colonial states appeared to follow the blueprint of the idealized state - centralized government, territory enclosed by defined borders, and citizenry with defined rights -, as well as accessories such as a national flag, an anthem, a seat at the United Nations and an official economic policy, they were in actuality far weaker than the Western states they were modeled after. In Western states, the structure of governments closely matched states' actual capabilities, which had been arduously developed over centuries. The development of strong administrative structures, in particular those related to extraction of taxes, is closely associated with the intense warfare between predatory European states in the 17th and 18th centuries, or in Charles Tilly's famous formulation: "War made the state and the state made war". For example, the formation of the modern states of Germany and Italy in the 19th century is closely associated with the wars of expansion and consolidation led by Prussia and Sardinia, respectively. The Western process of forming effective and impersonal bureaucracies, developing efficient tax systems, and integrating national territory continued into the 20th century. Nevertheless, Western states that survived into the latter half of the 20th century were considered "strong" by simple reason that they had managed to develop the institutional structures and military capability required to survive predation by their fellow states.
In sharp contrast, decolonization was an entirely different process of state formation. Most imperial powers had not foreseen a need to prepare their colonies for independence; for example, Britain had given limited self-rule to India and Sri Lanka, while treating British Somaliland as little more than a trading post, while all major decisions for French colonies were made in Paris and Belgium prohibited any self-government up until it suddenly granted independence to its colonies in 1960. Like Western states of previous centuries, the new ex-colonies lacked autonomous bureaucracies, which would make decisions based on the benefit to society as a whole, rather than respond to corruption and nepotism to favor a particular interest group. In such a situation, factions manipulate the state to benefit themselves or, alternatively, state leaders use the bureaucracy to further their own self-interest. The lack of credible governance was compounded by the fact that most colonies were economic loss-makers at independence, lacking both a productive economic base and a taxation system to effectively extract resources from economic activity. Among the rare states profitable at decolonization was India, to which scholars credibly argue that Uganda, Malaysia and Angola may be included. Neither did imperial powers make territorial integration a priority, and may have discouraged nascent nationalism as a danger to their rule. Many newly independent states thus found themselves impoverished, with minimal administrative capacity in a fragmented society, while faced with the expectation of immediately meeting the demands of a modern state. Such states are considered "weak" or "fragile". The "strong"-"weak" categorization is not the same as "Western"-"non-Western", as some Latin American states like Argentina and Brazil and Middle Eastern states like Egypt and Israel are considered to have "strong" administrative structures and economic infrastructure.
Historically, the international community would have targeted weak states for territorial absorption or colonial domination or, alternatively, such states would fragment into pieces small enough to be effectively administered and secured by a local power. However, international norms towards sovereignty changed in the wake of WWII in ways that support and maintain the existence of weak states. Weak states are given "de jure" sovereignty equal to that of other states, even when they do not have "de facto" sovereignty or control of their own territory, including the privileges of international diplomatic recognition and an equal vote in the United Nations. Further, the international community offers development aid to weak states, which helps maintain the facade of a functioning modern state by giving the appearance that the state is capable of fulfilling its implied responsibilities of control and order. The formation of a strong international law regime and norms against territorial aggression is strongly associated with the dramatic drop in the number of interstate wars, though it has also been attributed to the effect of the Cold War or to the changing nature of economic development. Consequently, military aggression that results in territorial annexation became increasingly likely to prompt international condemnation, diplomatic censure, a reduction in international aid or the introduction of economic sanction, or, as in the case of 1990 invasion of Kuwait by Iraq, international military intervention to reverse the territorial aggression. Similarly, the international community has largely refused to recognize secessionist regions, while keeping some secessionist self-declared states such as Somaliland in diplomatic recognition limbo. While there is not a large body of academic work examining the relationship, Hironaka's statistical study found a correlation that suggests that every major international anti-secessionist declaration increased the number of ongoing civil wars by +10%, or a total +114% from 1945 to 1997. The diplomatic and legal protection given by the international community, as well as economic support to weak governments and discouragement of secession, thus had the unintended effect of encouraging civil wars.
There has been an enormous amount of international intervention in civil wars since 1945 that some have argued served to extend wars. While intervention has been practiced since the international system has existed, its nature changed substantially. It became common for both the state and opposition group to receive foreign support, allowing wars to continue well past the point when domestic resources had been exhausted. Superpowers, such as the European great powers, had always felt no compunction in intervening in civil wars that affected their interests, while distant regional powers such as the United States could declare the interventionist Monroe Doctrine of 1821 for events in its Central American "backyard". However, the large population of weak states after 1945 allowed intervention by former colonial powers, regional powers and neighboring states who themselves often had scarce resources. On average, a civil war with interstate intervention was 300% longer than those without. When disaggregated, a civil war with intervention on only one side is 156% longer, while when intervention occurs on both sides the average civil war is longer by an additional 92%. If one of the intervening states was a superpower, a civil war is a further 72% longer; a conflict such as the Angolan Civil War, in which there is two-sided foreign intervention, including by a superpower (actually, two superpowers in the case of Angola), would be 538% longer on average than a civil war without any international intervention.
Effect of the Cold War.
The Cold War (1945–1989) provided a global network of material and ideological support that often helped perpetuate civil wars, which were mainly fought in weak ex-colonial states rather than the relatively strong states that were aligned with the Warsaw Pact and North Atlantic Treaty Organization. In some cases, superpowers would superimpose Cold War ideology onto local conflicts, while in others local actors using Cold War ideology would attract the attention of a superpower to obtain support. Using a separate statistical evaluation than used above for interventions, civil wars that included pro- or anti-communist forces lasted 141% longer than the average non-Cold War conflict, while a Cold War civil war that attracted superpower intervention resulted in wars typically lasting over three times as long as other civil wars. Conversely, the end of the Cold War marked by the fall of the Berlin Wall in 1989 resulted in a reduction in the duration of Cold War civil wars of 92% or, phrased another way, a roughly ten-fold increase in the rate of resolution of Cold War civil wars. Lengthy Cold War-associated civil conflicts that ground to a halt include the wars of Guatemala (1960–1996), El Salvador (1979–1991) and Nicaragua (1970–1990).

</doc>
